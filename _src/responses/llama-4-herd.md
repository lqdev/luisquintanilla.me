---
title: "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation"
targeturl: https://ai.meta.com/blog/llama-4-multimodal-intelligence/
response_type: reshare
dt_published: "2025-04-08 08:30"
dt_updated: "2025-04-08 08:30 -05:00"
tags: ["llama","meta","ai","llm"]
---

> We’re introducing Llama 4 Scout and Llama 4 Maverick, the first open-weight natively multimodal models with unprecedented context length support and our first built using a mixture-of-experts (MoE) architecture. We’re also previewing Llama 4 Behemoth, one of the smartest LLMs in the world and our most powerful yet to serve as a teacher for our new models.

> Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3

> Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks

After tinkering with Mistral Small 3.1, I'm excited to try Llama 4 Scout once it lands in GitHub Models and Ollama. 

I don't think that Mistral Small 3.1 uses MoE architecture which should help a ton considering the 10 million token context size. 