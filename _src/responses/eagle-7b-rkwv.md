---
title: "Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages (RWKV-v5)"
targeturl: https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers
response_type: bookmark
dt_published: "2024-01-29 20:27"
dt_updated: "2024-01-29 20:27 -05:00"
tags: ["ai","llm","rwkv","deeplearning","neuralnetwork"]
---

> Eagle 7B is a 7.52B parameter model that:  
>  <br>
>  Built on the [RWKV-v5 architecture](https://wiki.rwkv.com/) (a linear transformer with 10-100x+ lower inference cost)  
>  <br>
>  Ranks as the [world’s greenest 7B model (per token)](https://blog.rwkv.com/p/the-worlds-greenest-ai-model-rwkvs)  
>  <br>
>  Trained on 1.1 Trillion Tokens across 100+ languages  
>  <br>
>  Outperforms all 7B class models in multi-lingual benchmarks  
>  <br>
>  Approaches Falcon (1.5T), LLaMA2 (2T), Mistral (>2T?) level of performance in English evals  
>  <br>
>  Trade blows with MPT-7B (1T) in English evals  
>  <br>
>  [All while being an “Attention-Free Transformer”](https://www.isattentionallyouneed.com/)  
>  <br>
>  Is a foundation model, with a very small instruct tune - further fine-tuning is required for various use cases!  

> We are releasing RWKV-v5 Eagle 7B, [licensed as Apache 2.0 license, under the Linux Foundation](https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as), and can be used personally or commercially without restrictions

[Download from HuggingFace](https://huggingface.co/RWKV/v5-Eagle-7B/)