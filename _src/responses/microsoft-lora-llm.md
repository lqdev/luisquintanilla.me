---
title: "LoRA: Low-Rank Adaptation of Large Language Models"
targeturl: https://github.com/microsoft/LoRA
response_type: bookmark
dt_published: "2023-06-01 22:45"
dt_updated: "2023-06-01 22:45 -05:00"
tags: ["ai","finetune","llm"]
---

> LoRA reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights. This vastly reduces the storage requirement for large language models adapted to specific tasks and enables efficient task-switching during deployment all without introducing inference latency. LoRA also outperforms several other adaptation methods including adapter, prefix-tuning, and fine-tuning.

[Paper](https://arxiv.org/abs/2106.09685)