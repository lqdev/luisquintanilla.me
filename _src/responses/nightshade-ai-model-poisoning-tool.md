---
title: "NightShade"
targeturl: https://nightshade.cs.uchicago.edu/
response_type: reshare
dt_published: "2024-01-23 20:57"
dt_updated: "2024-01-23 20:57 -05:00"
tags: ["research","tools",ai","generativeai","llm","computervision","cv"]
---

> Nightshade, a tool that turns any image into a data sample that is unsuitable for model training. More precisely, Nightshade transforms images into "poison" samples, so that models training on them without consent will see their models learn unpredictable behaviors that deviate from expected norms, e.g. a prompt that asks for an image of a cow flying in space might instead get an image of a handbag floating in space. 

[What is NightShade?](https://nightshade.cs.uchicago.edu/whatis.html)
