---
title: "nanoGPT - The simplest, fastest repository for training/finetuning medium-sized GPTs."
targeturl: https://github.com/karpathy/nanoGPT 
response_type: bookmark
dt_published: "2023-01-03 22:35"
dt_updated: "2023-01-03 22:35 -05:00"
---

> The simplest, fastest repository for training/finetuning medium-sized GPTs. It's a re-write of minGPT, which I think became too complicated, and which I am hesitant to now touch. Still under active development, currently working to reproduce GPT-2 on OpenWebText dataset. The code itself aims by design to be plain and readable: train.py is a ~300-line boilerplate training loop and model.py a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it.
