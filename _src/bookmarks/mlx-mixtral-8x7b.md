---
title: "Mixtral 8x7B on Apple Silicon with MLX"
targeturl: https://github.com/ml-explore/mlx-examples/tree/main/mixtral
response_type: bookmark
dt_published: "2023-12-12 19:19 -05:00"
dt_updated: "2023-12-12 19:19 -05:00"
tags: ["mlx","ai","frameworks"]
---

> Run the Mixtral1 8x7B mixture-of-experts (MoE) model in MLX on Apple silicon.