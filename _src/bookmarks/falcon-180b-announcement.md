---
title: "Spread Your Wings: Falcon 180B is here "
targeturl: https://huggingface.co/blog/falcon-180b
response_type: bookmark
dt_published: "2023-09-12 10:35 -05:00"
dt_updated: "2023-09-12 10:35 -05:00"
tags: ["ai","llm","opensource","huggingface"]
---

> Today, we're excited to welcome TII's Falcon 180B to HuggingFace! Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII's RefinedWeb dataset. This represents the longest single-epoch pretraining for an open model. 