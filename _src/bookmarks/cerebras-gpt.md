---
title: "Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models"
targeturl: https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/ 
response_type: bookmark
dt_published: "2023-03-28 21:53 -05:00"
dt_updated: "2023-03-28 21:53 -05:00"
---

> Cerebras open sources seven GPT-3 models from 111 million to 13 billion parameters. Trained using the Chinchilla formula, these models set new benchmarks for accuracy and compute efficiency.

> Todayâ€™s release is designed to be used by and reproducible by anyone. All models, weights, and checkpoints are available on Hugging Face and GitHub under the Apache 2.0 license. Additionally, we provide detailed information on our training methods and performance results in our forthcoming paper. The Cerebras CS-2 systems used for training are also available on-demand via Cerebras Model Studio.