<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - Responses</title>
    <link>https://www.luisquintanilla.me/responses</link>
    <description>IndieWeb responses by Luis Quintanilla</description>
    <lastBuildDate>2025-08-20 23:59 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Omnineural 4B</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;OmniNeural-4B is the first multimodal AI model built natively for NPUs ‚Äî handling text, images, and audio in one model.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/omnineural-4b</link>
      <guid>https://www.luisquintanilla.me/responses/omnineural-4b</guid>
      <pubDate>2025-08-20 23:59 -05:00</pubDate>
      <category>ai</category>
      <category>npu</category>
      <category>multimodal</category>
    </item>
    <item>
      <title>Agents.md</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;A simple, open format for guiding coding agents&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Think of AGENTS.md as a README for agents: a dedicated, predictable place to provide the context and instructions to help AI coding agents work on your project.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/agentsmd</link>
      <guid>https://www.luisquintanilla.me/responses/agentsmd</guid>
      <pubDate>2025-08-20 22:53 -05:00</pubDate>
      <category>ai</category>
      <category>agents</category>
      <category>openai</category>
    </item>
    <item>
      <title>Seemingly Conscious AI</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;We must build AI for people; not to be a person&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;üíØüíØüíØüíØüíØ&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/seemingly-conscious-ai</link>
      <guid>https://www.luisquintanilla.me/responses/seemingly-conscious-ai</guid>
      <pubDate>2025-08-20 19:46 -05:00</pubDate>
      <category>ai</category>
      <category>consciousness</category>
      <category>humanism</category>
    </item>
    <item>
      <title>Reflections on the social web</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The work of a product team, when working with new technology, is to abstract away as much of this complexity as possible, so that it feels friendly and approachable to new people.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;My strongest belief about the social web is that if we want it to succeed, we have to keep lowering the barrier to entry.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/reflections-on-social-web</link>
      <guid>https://www.luisquintanilla.me/responses/reflections-on-social-web</guid>
      <pubDate>2025-08-18 19:31 -05:00</pubDate>
      <category>ghost</category>
      <category>socialweb</category>
      <category>indieweb</category>
    </item>
    <item>
      <title>Ghost 6.0</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;In Ghost 6.0 we're introducing another new distribution channel: The social web. Now, millions of people can discover, follow, like and reply to your posts from any supported social web client - including Bluesky, Mastodon, Threads, Flipboard, Ghost, WordPress, Surf, WriteFreely, and many more.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Just as people can visit your Ghost website in any browser, subscribe with any feed reader, or receive newsletters in any email client, they can now follow your Ghost publication from any social web client.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Unlike closed platforms with restrictive algorithms, open web protocols create a direct connection between you and your subscribers which you have full control over.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;If you've been around on the web for while, and you can remember back that far... you might even call it the return of the blogosphere.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Love the introduction of social components to the Ghost platform. I'm so ready for the return of the blogosphere and the return to open protocols.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/ghost-6-changelog</link>
      <guid>https://www.luisquintanilla.me/responses/ghost-6-changelog</guid>
      <pubDate>2025-08-18 19:24 -05:00</pubDate>
      <category>ghost</category>
      <category>fediverse</category>
      <category>socialweb</category>
      <category>publishing</category>
    </item>
    <item>
      <title>The newly redesigned Blogroll Club</title>
      <description>&lt;![CDATA[[star] ]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/newly-redesigned-blogroll</link>
      <guid>https://www.luisquintanilla.me/responses/newly-redesigned-blogroll</guid>
      <pubDate>2025-08-18 19:21 -05:00</pubDate>
      <category>blogrollclub</category>
      <category>blogroll</category>
      <category>indieweb</category>
      <category>socialweb</category>
    </item>
    <item>
      <title>FediUrbanism ‚Äî Johanna Botari ‚Äî FediCon 2025</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;I really enjoyed this talk by &lt;a href="https://cosocial.ca/@johannab"&gt;@johannab&lt;/a&gt; on the concept of FediUrbanism.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://spectra.video/w/f7GBsBf3nAhF8WfTMWUEXm" title="Thumbnail of FediUrbanism Talk Recording"&gt;&lt;img src="https://spectra.video/lazy-static/previews/40a5e4bd-f8d7-438a-b35c-0c81e753ddbd.jpg" class="img-fluid" alt="Thumbnail of FediUrbanism Talk Recording" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/fediurbanism-fedicon-2025</link>
      <guid>https://www.luisquintanilla.me/responses/fediurbanism-fedicon-2025</guid>
      <pubDate>2025-08-18 18:36 -05:00</pubDate>
      <category>fediurbanism</category>
      <category>fedicon2025</category>
      <category>fediverse</category>
      <category>thirdspace</category>
      <category>community</category>
    </item>
    <item>
      <title>Introducing Gemma 3 270M</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we're adding a new, highly specialized tool to the Gemma 3 toolkit: Gemma 3 270M, a compact, 270-million parameter model designed from the ground up for task-specific fine-tuning with strong instruction-following and text structuring capabilities already trained in.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/introducing-gemma-3-270m</link>
      <guid>https://www.luisquintanilla.me/responses/introducing-gemma-3-270m</guid>
      <pubDate>2025-08-14 12:40 -05:00</pubDate>
      <category>ai</category>
      <category>google</category>
      <category>gemma</category>
      <category>slm</category>
      <category>genai</category>
    </item>
    <item>
      <title>I Deleted My Second Brain</title>
      <description>&lt;![CDATA[[star] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;I don‚Äôt build a second brain. I inhabit the first.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/i-deleted-my-second-brain</link>
      <guid>https://www.luisquintanilla.me/responses/i-deleted-my-second-brain</guid>
      <pubDate>2025-08-13 08:03 -05:00</pubDate>
      <category>pkm</category>
      <category>notes</category>
      <category>secondbrain</category>
    </item>
    <item>
      <title>Engineering for Slow Internet</title>
      <description>&lt;![CDATA[[reply] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Does this webapp really need to be 20 MB? What all is being loaded that could be deferred until it is needed, or included in an ‚Äúoptional‚Äù add-on bundle? Is there a possibility of a ‚Äúlite‚Äù version, for bandwidth-constrained users?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While Antarctica is an edge case, this article illustrates some of the motivations behind my &lt;a href="https://www.luisquintanilla.me/text"&gt;text-first website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;By trimming the excess not only do you get to the core of the app or website, but it also loads faster.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/engineering-for-slow-internet</link>
      <guid>https://www.luisquintanilla.me/responses/engineering-for-slow-internet</guid>
      <pubDate>2025-08-12 08:41 -05:00</pubDate>
      <category>slowweb</category>
      <category>performance</category>
      <category>internet</category>
      <category>computing</category>
    </item>
    <item>
      <title>Stop saving everything</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;If your read-it-later list isn‚Äôt getting cleared weekly, perhaps it‚Äôs time to delete the lot.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Save from a mindset of abundance, rather than scarcity, and process the things you‚Äôve saved each week (or month, at the most). If you are worried that something you deleted truly would have changed your life, just stop.&lt;/p&gt;
&lt;p&gt;STOP.&lt;/p&gt;
&lt;p&gt;You can‚Äôt read it all, do it all, be it all. Trust that those potentially life-changing ideas will come around again, when you are ready for them&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Good reminder. Lately I've been sending a lot of notes to myself with stuff to read but just haven't had the time to get to it. That said, the act of sending myself those notes is low friction that I don't feel FOMO when I don't get to read the articles and consume the media.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/stop-saving-everything</link>
      <guid>https://www.luisquintanilla.me/responses/stop-saving-everything</guid>
      <pubDate>2025-08-11 23:00 -05:00</pubDate>
      <category>pkm</category>
      <category>productivity</category>
      <category>notetaking</category>
    </item>
    <item>
      <title>BLM Lands Marked for Potential Sale (OnX)</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;Sad to see what's happening with our public lands. Great to see folks staying vigilant and providing information of what's happening on the ground.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/blm-lands-marked-for-potential-sale-onx</link>
      <guid>https://www.luisquintanilla.me/responses/blm-lands-marked-for-potential-sale-onx</guid>
      <pubDate>2025-08-10 21:13 -05:00</pubDate>
      <category>onx</category>
      <category>bureaulandmanagement</category>
      <category>nature</category>
    </item>
    <item>
      <title>Tumblr‚Äôs move to WordPress and fediverse integration is ‚Äòon hold‚Äô</title>
      <description>&lt;![CDATA[[reply] &lt;p&gt;That's unfortunate. Having Tumblr as your frontend for posting and curating feeds, with WordPress as the backend and integrated with decentralized networks like the Fediverse for distribution would've been amazing. It sounds like Tumblr is still strategic to Automattic and as more people move away from the centralized social platforms and towards their own corners of the internet, having these differente building blocks in place will be even more important.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/tumblr-wordpress-fediverse-integration-pause</link>
      <guid>https://www.luisquintanilla.me/responses/tumblr-wordpress-fediverse-integration-pause</guid>
      <pubDate>2025-06-30 20:21 -05:00</pubDate>
      <category>tumblr</category>
      <category>wordpress</category>
      <category>fediverse</category>
      <category>automattic</category>
    </item>
    <item>
      <title>Do not privatize federal land</title>
      <description>&lt;![CDATA[[reply] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;I‚Äôve long advocated selling off some federal land...Most of this ‚Äúpublic land‚Äù is never used by the public. Selling some of it would actually make it more accessible and useful to real people.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How about no. If privatization means, John and Jane Doe can purchase some land for their homestead is one thing. We've tried that before by the way - &lt;a href="https://en.wikipedia.org/wiki/Homestead_Acts"&gt;Homestead Act&lt;/a&gt;. In reality, it means some private equity firm or hedge fund buying the land, building expensive and shoddy cookit-cutter condos, and &lt;a href="https://www.nar.realtor/magazine/real-estate-news/commercial/build-to-rent-homes"&gt;selling you a subscription for a roof over your head&lt;/a&gt;. I say condos because it's hot and there's not a lot of water in Vegas, so keeping those brand new AI data centers cool might be a problem.&lt;/p&gt;
&lt;p&gt;Also, if you haven't seen that area that Alex highlights in his post, it seems like a deserted landscape. And yes, the terrain can be rough, but actually, it's a beautiful landscape that is best left undisturbed.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/do-not-privatize-federal-land</link>
      <guid>https://www.luisquintanilla.me/responses/do-not-privatize-federal-land</guid>
      <pubDate>2025-06-29 17:26 -05:00</pubDate>
      <category>bureaulandmanagement</category>
      <category>uspol</category>
      <category>land</category>
    </item>
    <item>
      <title>Self-Adapting Language Models (SEAL)</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. &lt;strong&gt;We introduce Self-Adapting LLMs (SEAL) ü¶≠, a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives.&lt;/strong&gt; Given a new input, the model produces a self-edit ‚Äî a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop, using the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's generation to parameterize and control its own adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation in response to new data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We demonstrate SEAL in two domains: (1) &lt;strong&gt;Knowledge Incorporation&lt;/strong&gt;, where the model integrates new factual information by generating logical implications as synthetic data, and (2) &lt;strong&gt;Few-Shot Learning&lt;/strong&gt;, where the model autonomously selects data augmentations and training hyperparameters to adapt to new abstract reasoning tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Continual-Intelligence"&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2506.10943"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/self-adapting-language-models-seal</link>
      <guid>https://www.luisquintanilla.me/responses/self-adapting-language-models-seal</guid>
      <pubDate>2025-06-16 20:33 -05:00</pubDate>
      <category>agent</category>
      <category>ai</category>
      <category>llm</category>
      <category>mit</category>
      <category>research</category>
    </item>
    <item>
      <title>How we built our multi-agent research system - Anthropic</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;Great article.&lt;/p&gt;
&lt;p&gt;TLDR: These are largely engineering problems.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The journey of this multi-agent system from prototype to production taught us critical lessons about system architecture, tool design, and prompt engineering. A multi-agent system consists of multiple agents (&lt;strong&gt;LLMs autonomously using tools in a loop&lt;/strong&gt;) working together. Our Research feature involves an agent that plans a research process based on user queries, and then uses tools to create parallel agents that search for information simultaneously. Systems with multiple agents introduce new challenges in agent coordination, evaluation, and reliability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Benefits of a multi-agent system&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The essence of search is compression: distilling insights from a vast corpus. &lt;strong&gt;Subagents facilitate compression by operating in parallel&lt;/strong&gt; with their own context windows, exploring different aspects of the question simultaneously before condensing the most important tokens for the lead research agent. Each subagent also provides &lt;strong&gt;separation of concerns&lt;/strong&gt;‚Äîdistinct tools, prompts, and exploration trajectories‚Äîwhich reduces path dependency and enables thorough, independent investigations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Once intelligence reaches a threshold, multi-agent systems become a vital way to scale performance. For instance, although individual humans have become more intelligent in the last 100,000 years, human societies have become exponentially more capable in the information age because of our collective intelligence and ability to coordinate. Even generally-intelligent agents face limits when operating as individuals; &lt;strong&gt;groups of agents can accomplish far more&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our internal evaluations show that multi-agent research systems excel especially for breadth-first queries that involve pursuing multiple independent directions simultaneously.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Multi-agent systems work mainly because they help spend enough tokens to solve the problem...Multi-agent architectures effectively scale token usage for tasks that exceed the limits of single agents.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôve found that multi-agent systems excel at valuable tasks that involve heavy parallelization, information that exceeds single context windows, and interfacing with numerous complex tools.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Architecture overview&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our Research system uses a multi-agent architecture with an orchestrator-worker pattern, where a lead agent coordinates the process while delegating to specialized subagents that operate in parallel.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Traditional approaches using Retrieval Augmented Generation (RAG) use static retrieval. That is, they fetch some set of chunks that are most similar to an input query and use these chunks to generate a response. In contrast, our architecture uses a &lt;strong&gt;multi-step search that dynamically finds relevant information, adapts to new findings, and analyzes results to formulate high-quality answers&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Prompt engineering and evaluations for research agents&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Think like your agents.&lt;/strong&gt; To iterate on prompts, you must understand their effects. To help us do this, we built simulations using our Console with the exact prompts and tools from our system, then watched agents work step-by-step.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Teach the orchestrator how to delegate.&lt;/strong&gt; In our system, the lead agent decomposes queries into subtasks and describes them to subagents. Each subagent needs an &lt;strong&gt;objective&lt;/strong&gt;, an &lt;strong&gt;output format&lt;/strong&gt;, &lt;strong&gt;guidance on the tools and sources to use&lt;/strong&gt;, and &lt;strong&gt;clear task boundaries&lt;/strong&gt;. Without detailed task descriptions, agents duplicate work, leave gaps, or fail to find necessary information.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Scale effort to query complexity.&lt;/strong&gt; Agents struggle to judge appropriate effort for different tasks, so we embedded scaling rules in the prompts...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Tool design and selection are critical.&lt;/strong&gt; Agent-tool interfaces are as critical as human-computer interfaces. Using the right tool is efficient‚Äîoften, it‚Äôs strictly necessary...Bad tool descriptions can send agents down completely wrong paths, so each tool needs a distinct purpose and a clear description.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Let agents improve themselves.&lt;/strong&gt;...When given a prompt and a failure mode, they are able to diagnose why the agent is failing and suggest improvements.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Start wide, then narrow down.&lt;/strong&gt; Search strategy should mirror expert human research: explore the landscape before drilling into specifics.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Guide the thinking process.&lt;/strong&gt;...Our testing showed that extended thinking improved instruction-following, reasoning, and efficiency. Subagents also plan, then use interleaved thinking after tool results to evaluate quality, identify gaps, and refine their next query. This makes subagents more effective in adapting to any task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Parallel tool calling transforms speed and performance.&lt;/strong&gt;...For speed, we introduced two kinds of parallelization: (1) the lead agent spins up 3-5 subagents in parallel rather than serially; (2) the subagents use 3+ tools in parallel.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our prompting strategy focuses on instilling &lt;strong&gt;good heuristics rather than rigid rules&lt;/strong&gt;. We studied how skilled humans approach research tasks and encoded these strategies in our prompts‚Äîstrategies like &lt;strong&gt;decomposing difficult questions into smaller tasks&lt;/strong&gt;, &lt;strong&gt;carefully evaluating the quality of sources&lt;/strong&gt;, &lt;strong&gt;adjusting search approaches based on new information&lt;/strong&gt;, and &lt;strong&gt;recognizing when to focus on depth (investigating one topic in detail) vs. breadth (exploring many topics in parallel)&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Effective evaluation of agents&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...evaluating multi-agent systems presents unique challenges...Because we don‚Äôt always know what the right steps are, we usually can't just check if agents followed the ‚Äúcorrect‚Äù steps we prescribed in advance. Instead, we need flexible evaluation methods that judge whether agents achieved the right outcomes while also following a reasonable process.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Start evaluating immediately with small samples.&lt;/strong&gt;...it‚Äôs best to start with small-scale testing right away with a few examples, rather than delaying until you can build more thorough evals.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;LLM-as-judge evaluation scales when done well.&lt;/strong&gt;...We used an LLM judge that evaluated each output against criteria in a rubric: &lt;strong&gt;factual accuracy&lt;/strong&gt; (do claims match sources?), &lt;strong&gt;citation accuracy&lt;/strong&gt; (do the cited sources match the claims?), &lt;strong&gt;completeness&lt;/strong&gt; (are all requested aspects covered?), &lt;strong&gt;source quality&lt;/strong&gt; (did it use primary sources over lower-quality secondary sources?), and &lt;strong&gt;tool efficiency&lt;/strong&gt; (did it use the right tools a reasonable number of times?)...[we] found that a single LLM call with a single prompt outputting scores from 0.0-1.0 and a pass-fail grade was the most consistent and aligned with human judgements.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Human evaluation catches what automation misses.&lt;/strong&gt;...Adding source quality heuristics to our prompts helped resolve this issue. Even in a world of automated evaluations, manual testing remains essential.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;the best prompts for these agents are not just strict instructions, but &lt;strong&gt;frameworks for collaboration&lt;/strong&gt; that define the &lt;strong&gt;division of labor&lt;/strong&gt;, &lt;strong&gt;problem-solving approaches&lt;/strong&gt;, and &lt;strong&gt;effort budgets&lt;/strong&gt;. Getting this right relies on &lt;strong&gt;careful prompting and tool design&lt;/strong&gt;, &lt;strong&gt;solid heuristics&lt;/strong&gt;, &lt;strong&gt;observability&lt;/strong&gt;, and &lt;strong&gt;tight feedback loops&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Production reliability and engineering challenges&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Agents are stateful and errors compound.&lt;/strong&gt;...Agents can run for long periods of time, maintaining state across many tool calls. This means we need to &lt;strong&gt;durably execute code and handle errors along the way&lt;/strong&gt;. Without effective mitigations, minor system failures can be catastrophic for agents. When errors occur, we can't just restart from the beginning...We combine the adaptability of AI agents built on Claude with deterministic safeguards like retry logic and regular checkpoints.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Debugging benefits from new approaches&lt;/strong&gt;...Adding full production tracing let us diagnose why agents failed and fix issues systematically. Beyond standard observability, we monitor agent decision patterns and interaction structures‚Äîall without monitoring the contents of individual conversations, to maintain user privacy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Deployment needs careful coordination&lt;/strong&gt;...we use rainbow deployments to avoid disrupting running agents, by gradually shifting traffic from old to new versions while keeping both running simultaneously.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Synchronous execution creates bottlenecks.&lt;/strong&gt; Currently, our lead agents execute subagents synchronously, waiting for each set of subagents to complete before proceeding. This simplifies coordination, but creates bottlenecks in the information flow between agents. Asynchronous execution would enable additional parallelism: agents working concurrently and creating new subagents when needed. But this asynchronicity adds challenges in result coordination, state consistency, and error propagation across the subagents. As models can handle longer and more complex research tasks, we expect the performance gains will justify the complexity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;For all the reasons described in this post, the gap between prototype and production is often wider than anticipated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Multi-agent research systems can operate reliably at scale with &lt;strong&gt;careful engineering&lt;/strong&gt;, &lt;strong&gt;comprehensive testing&lt;/strong&gt;, &lt;strong&gt;detail-oriented prompt and tool design&lt;/strong&gt;, &lt;strong&gt;robust operational practices&lt;/strong&gt;, and &lt;strong&gt;tight collaboration between research, product, and engineering teams&lt;/strong&gt; who have a strong understanding of current agent capabilities.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/anthropic-how-we-built-multi-agent-system</link>
      <guid>https://www.luisquintanilla.me/responses/anthropic-how-we-built-multi-agent-system</guid>
      <pubDate>2025-06-16 17:39 -05:00</pubDate>
      <category>agent</category>
      <category>ai</category>
      <category>anthropic</category>
      <category>engineering</category>
    </item>
    <item>
      <title>MLS over ActivityPub Draft</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;a href="https://messaginglayersecurity.rocks/"&gt;Messaging Layer Security (MLS)&lt;/a&gt; is an IETF standard for end-to-end encrypted (E2EE) messaging. It lets people on laptops and phones communicate with each other in a secure way that no one in between can see.&lt;br /&gt;
&lt;br&gt;
MLS is designed to use pluggable lower-level protocols. This specification defines an envelope format for distributing MLS messages through the network, and an Activity Streams 2.0 profile for the packets of application data stored inside the messages.&lt;br /&gt;
&lt;br&gt;
This specification is ready for review from both ActivityPub developers and security analysts. It‚Äôs time to start making proof-of-concept implementations and testing interoperability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Source: https://socialwebfoundation.org/2025/06/13/mls-over-activitypub-draft/&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/mls-over-activitypub-draft</link>
      <guid>https://www.luisquintanilla.me/responses/mls-over-activitypub-draft</guid>
      <pubDate>2025-06-14 10:47 -05:00</pubDate>
      <category>protocol</category>
      <category>socialweb</category>
      <category>activitypub</category>
      <category>w3c</category>
    </item>
    <item>
      <title>Faircamp - A static site generator for audio producers</title>
      <description>&lt;![CDATA[[star] &lt;p&gt;This is such a cool project. Looks like it's super simple to get started with and it's packed with a ton of features so creators get to stay in their flow.&lt;/p&gt;
&lt;p&gt;I also like how creators are showcased.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/faircamp-static-site-audio-producers</link>
      <guid>https://www.luisquintanilla.me/responses/faircamp-static-site-audio-producers</guid>
      <pubDate>2025-06-05 20:05 -05:00</pubDate>
      <category>faircamp</category>
      <category>music</category>
      <category>socialweb</category>
      <category>indieweb</category>
      <category>staticsite</category>
      <category>technology</category>
      <category>tools</category>
      <category>openweb</category>
      <category>opensource</category>
    </item>
    <item>
      <title>State-Of-The-Art Prompting For AI Agents</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=DL82mGde6wo" title="YouTube Thumbnail for State-of-the-art prompting for ai agents"&gt;&lt;img src="http://img.youtube.com/vi/DL82mGde6wo/0.jpg" class="img-fluid" alt="YouTube Thumbnail for State-of-the-art prompting for ai agents" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/state-of-the-art-prompting-ai-agents-y-combinator</link>
      <guid>https://www.luisquintanilla.me/responses/state-of-the-art-prompting-ai-agents-y-combinator</guid>
      <pubDate>2025-06-03 20:23 -05:00</pubDate>
      <category>ai</category>
      <category>agents</category>
      <category>prompting</category>
    </item>
    <item>
      <title>Introducing ElevenLabs Conversational AI 2.0</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Conversational AI 2.0 launches with advanced features and enterprise readiness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Natural turn-taking to understand the flow of conversation.&lt;/li&gt;
&lt;li&gt;Multilingual communication with integrated language detection&lt;/li&gt;
&lt;li&gt;Integrated RAG: knowledgeable agents, minimum latency, maximum privacy&lt;/li&gt;
&lt;li&gt;Multimodality&lt;/li&gt;
&lt;li&gt;Batch calls&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/responses/elevenlabs-conversational-ai-2-0</link>
      <guid>https://www.luisquintanilla.me/responses/elevenlabs-conversational-ai-2-0</guid>
      <pubDate>2025-06-03 20:19 -05:00</pubDate>
      <category>ai</category>
      <category>speech</category>
      <category>elevenlabs</category>
    </item>
  </channel>
</rss>