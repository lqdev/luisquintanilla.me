<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - huggingface</title>
    <link>https://www.lqdev.me/tags/huggingface</link>
    <description>All content tagged with 'huggingface' by Luis Quintanilla</description>
    <lastBuildDate>2025-02-10 17:20 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>HuggingFace AI Agents Course</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;This free course will take you on a journey, from beginner to expert, in understanding, using and building AI agents.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;At the end of this course you‚Äôll understand how Agents work and how to build your own Agents using the latest libraries and tools.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/huggingface-ai-agents-course</link>
      <guid>https://www.lqdev.me/bookmarks/huggingface-ai-agents-course</guid>
      <pubDate>2025-02-10 17:20 -05:00</pubDate>
      <category>huggingface</category>
      <category>ai</category>
      <category>agents</category>
      <category>course</category>
    </item>
    <item>
      <title>These models are too damn big!</title>
      <description>&lt;![CDATA[&lt;p&gt;While the size of these smaller language models is significantly less than the trillion parameter models like GPT, they still take up a lot of storage space. Playing around with &lt;a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2"&gt;Mistral 7B Instruct v0.2&lt;/a&gt;, the safetensor files containing the weights take up roughly 15GB of space. I'm thinking of playing around with &lt;a href="https://learn.microsoft.com/azure/storage/blobs/blobfuse2-what-is"&gt;blobfuse&lt;/a&gt; to mount a storage container to my local file system. That way, I'm only caching and accessing the models I'm working with at any given time.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/slm-too-big</link>
      <guid>https://www.lqdev.me/notes/slm-too-big</guid>
      <pubDate>2024-04-08 23:43 -05:00</pubDate>
      <category>ai</category>
      <category>slm</category>
      <category>huggingface</category>
      <category>llm</category>
      <category>opensource</category>
    </item>
    <item>
      <title>Releasing Common Corpus: the largest public domain dataset for training LLMs</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We announce today the release of Common Corpus on HuggingFace:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Common Corpus is the largest public domain dataset released for training LLMs.&lt;/li&gt;
&lt;li&gt;Common Corpus includes 500 billion words from a wide diversity of cultural heritage initiatives.&lt;/li&gt;
&lt;li&gt;Common Corpus is multilingual and the largest corpus to date in English, French, Dutch, Spanish, German and Italian.&lt;/li&gt;
&lt;li&gt;Common Corpus shows it is possible to train fully open LLMs on sources without copyright concerns.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/common-corpus-llm-training-dataset</link>
      <guid>https://www.lqdev.me/bookmarks/common-corpus-llm-training-dataset</guid>
      <pubDate>2024-03-20 22:12 -05:00</pubDate>
      <category>ai</category>
      <category>data</category>
      <category>llm</category>
      <category>huggingface</category>
      <category>nlp</category>
    </item>
    <item>
      <title>Machine Learning for Games Course - HuggingFace</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Welcome to the course that will teach you the most fascinating topic in game development: how to use powerful AI tools and models to create unique game experiences.&lt;br /&gt;
&lt;br&gt;
New AI models are revolutionizing the Game Industry in two impactful ways:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On how we make games:
&lt;ul&gt;
&lt;li&gt;Generate textures using AI&lt;/li&gt;
&lt;li&gt;Using AI voice actors for the voices.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How we create gameplay:
&lt;ul&gt;
&lt;li&gt;Crafting smart Non-Playable Characters (NPCs) using large language models.&lt;br /&gt;
&lt;br&gt;
This course will teach you:&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How to integrate AI models for innovative gameplay, featuring intelligent NPCs.&lt;/li&gt;
&lt;li&gt;How to use AI tools to help your game development pipeline.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/machine-learning-for-games-course-huggingface</link>
      <guid>https://www.lqdev.me/bookmarks/machine-learning-for-games-course-huggingface</guid>
      <pubDate>2024-03-19 22:35 -05:00</pubDate>
      <category>machinelearning</category>
      <category>games</category>
      <category>huggingface</category>
      <category>course</category>
      <category>ai</category>
      <category>ml</category>
    </item>
    <item>
      <title>Quanto: a PyTorch quantization toolkit </title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Quantization is a technique to reduce the computational and memory costs of evaluating Deep Learning Models by representing their weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we are excited to introduce quanto, a versatile pytorch quantization toolkit, that provides several unique features:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;available in eager mode (works with non-traceable models)&lt;/li&gt;
&lt;li&gt;quantized models can be placed on any device (including CUDA and MPS),&lt;/li&gt;
&lt;li&gt;automatically inserts quantization and dequantization stubs,&lt;/li&gt;
&lt;li&gt;automatically inserts quantized functional operations,&lt;/li&gt;
&lt;li&gt;automatically inserts quantized modules (see below the list of supported modules),&lt;/li&gt;
&lt;li&gt;provides a seamless workflow for a float model, going from a dynamic to a static quantized model,&lt;/li&gt;
&lt;li&gt;supports quantized model serialization as a state_dict,&lt;/li&gt;
&lt;li&gt;supports not only int8 weights, but also int2 and int4,&lt;/li&gt;
&lt;li&gt;supports not only int8 activations, but also float8.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/quanto-pytorch-quantization-toolkit</link>
      <guid>https://www.lqdev.me/responses/quanto-pytorch-quantization-toolkit</guid>
      <pubDate>2024-03-19 22:00 -05:00</pubDate>
      <category>huggingface</category>
      <category>quantization</category>
      <category>pytorch</category>
      <category>tools</category>
      <category>ai</category>
    </item>
    <item>
      <title>The Tokenizer Playground</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Experiment with different tokenizers (running locally in your browser).
I really love doing this thing&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/tokenizer-playground</link>
      <guid>https://www.lqdev.me/responses/tokenizer-playground</guid>
      <pubDate>2024-03-18 22:35 -05:00</pubDate>
      <category>ai</category>
      <category>tokenizer</category>
      <category>huggingface</category>
      <category>playground</category>
      <category>aiplayground</category>
    </item>
    <item>
      <title>HuggingChat</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The goal of this app is to showcase that it is now possible to build an open source alternative to ChatGPT.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/huggingface/chat-ui"&gt;GitHub repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huggingface/text-generation-inference"&gt;Text Generation Inference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/huggingchat</link>
      <guid>https://www.lqdev.me/responses/huggingchat</guid>
      <pubDate>2024-02-21 14:38 -05:00</pubDate>
      <category>huggingface</category>
      <category>assistants</category>
      <category>ai</category>
      <category>chat</category>
      <category>opensource</category>
    </item>
    <item>
      <title>Cosmopedia v0.1</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Cosmopedia is a dataset of synthetic textbooks, blogposts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1.The dataset contains over 30 million files and 25 billion tokens, making it the largest open synthetic dataset to date.&lt;br /&gt;
&lt;br&gt;
It covers a variety of topics; we tried to map world knowledge present in Web datasets like RefinedWeb and RedPajama, and generate synthetic content that covers them. This is the v0.1 of Cosmopedia, with ample room for improvement and topics to be more comprehensively covered. We hope this dataset will help the community's research efforts in the increasingly intriguing domain of synthetic data.&lt;br /&gt;
&lt;br&gt;
This work is inspired by the great work of Phi1.5.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/cosmopedia-ai-synthetic-dataset</link>
      <guid>https://www.lqdev.me/responses/cosmopedia-ai-synthetic-dataset</guid>
      <pubDate>2024-02-20 16:50 -05:00</pubDate>
      <category>cosmopedia</category>
      <category>dataset</category>
      <category>huggingface</category>
      <category>mixtral</category>
      <category>ai</category>
      <category>genai</category>
      <category>llm</category>
    </item>
    <item>
      <title>HuggingFace - Open Source AI Cookbook</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Open-Source AI Cookbook is a collection of notebooks illustrating practical aspects of building AI applications and solving various machine learning tasks using open-source tools and models.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/open-source-ai-cookbook-hf</link>
      <guid>https://www.lqdev.me/bookmarks/open-source-ai-cookbook-hf</guid>
      <pubDate>2024-02-16 22:36 -05:00</pubDate>
      <category>ai</category>
      <category>tutorial</category>
      <category>huggingface</category>
      <category>opensource</category>
      <category>ml</category>
    </item>
    <item>
      <title>Google‚Äôs Hugging Face deal puts ‚Äòsupercomputer‚Äô power behind open-source AI</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Google Cloud‚Äôs new partnership with AI model repository Hugging Face is letting developers build, train, and deploy AI models without needing to pay for a Google Cloud subscription. Now, outside developers using Hugging Face‚Äôs platform will have ‚Äúcost-effective‚Äù access to Google‚Äôs tensor processing units (TPU) and GPU supercomputers, which will include thousands of Nvidia‚Äôs in-demand and export-restricted H100s.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Google said that Hugging Face users can begin using the AI app-building platform Vertex AI and the Kubernetes engine that helps train and fine-tune models ‚Äúin the first half of 2024.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://www.googlecloudpresscorner.com/2024-01-25-Google-Cloud-and-Hugging-Face-Announce-Strategic-Partnership-to-Accelerate-Generative-AI-and-ML-Development"&gt;Press Release&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/google-cloud-huggingface-deal</link>
      <guid>https://www.lqdev.me/responses/google-cloud-huggingface-deal</guid>
      <pubDate>2024-01-25 21:23 -05:00</pubDate>
      <category>google</category>
      <category>huggingface</category>
      <category>ai</category>
      <category>cloud</category>
      <category>llm</category>
      <category>ml</category>
      <category>developers</category>
    </item>
    <item>
      <title>Phi-2 now on HuggingFace</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;When &lt;a href="https://www.lqdev.me/responses/microsoft-phi-2"&gt;Phi-2 initially released&lt;/a&gt;, it was on the Azure AI Studio Model Catalog. It's nice to see it's now in HuggingFace as well.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/phi-2-huggingface</link>
      <guid>https://www.lqdev.me/responses/phi-2-huggingface</guid>
      <pubDate>2023-12-20 18:37 -05:00</pubDate>
      <category>opensource</category>
      <category>ai</category>
      <category>phi</category>
      <category>languagemodel</category>
      <category>smalllanguagemodel</category>
      <category>huggingface</category>
      <category>transformers</category>
      <category>microsoft</category>
    </item>
    <item>
      <title>Coqui üê∏ XTTS</title>
      <description>&lt;![CDATA[[bookmark] &lt;p&gt;&lt;a href="https://huggingface.co/coqui/XTTS-v1"&gt;XTTS&lt;/a&gt; is a Voice generation model that lets you clone voices into different languages by using just a quick 3-second audio clip.&lt;/p&gt;
&lt;p&gt;XTTS is built on previous research, like Tortoise, with additional architectural innovations and training to make cross-language voice cloning and multilingual speech generation possible.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/coqui-xtts-text-to-speech</link>
      <guid>https://www.lqdev.me/bookmarks/coqui-xtts-text-to-speech</guid>
      <pubDate>2023-09-17 12:25 -05:00</pubDate>
      <category>ai</category>
      <category>generativeai</category>
      <category>speech</category>
      <category>huggingface</category>
      <category>ml</category>
      <category>tts</category>
      <category>opensource</category>
    </item>
    <item>
      <title>Introducing W√ºrstchen: Fast Diffusion for Image Generation </title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;W√ºrstchen is a diffusion model, whose text-conditional component works in a highly compressed latent space of images. Why is this important? Compressing data can reduce computational costs for both training and inference by orders of magnitude. Training on 1024√ó1024 images is way more expensive than training on 32√ó32. Usually, other works make use of a relatively small compression, in the range of 4x - 8x spatial compression. W√ºrstchen takes this to an extreme. Through its novel design, it achieves a 42x spatial compression! This had never been seen before, because common methods fail to faithfully reconstruct detailed images after 16x spatial compression. W√ºrstchen employs a two-stage compression, what we call Stage A and Stage B. Stage A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found in the  paper). Together Stage A and B are called the Decoder, because they decode the compressed images back into pixel space. A third model, Stage C, is learned in that highly compressed latent space. This training requires fractions of the compute used for current top-performing models, while also allowing cheaper and faster inference. We refer to Stage C as the Prior.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/wurstchen-fast-image-generation</link>
      <guid>https://www.lqdev.me/bookmarks/wurstchen-fast-image-generation</guid>
      <pubDate>2023-09-17 12:15 -05:00</pubDate>
      <category>ai</category>
      <category>generativeai</category>
      <category>imagegeneration</category>
      <category>huggingface</category>
      <category>diffusion</category>
      <category>ml</category>
    </item>
    <item>
      <title>Efficient Controllable Generation for SDXL with T2I-Adapters </title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;a href="https://huggingface.co/papers/2302.08453"&gt;T2I-Adapter&lt;/a&gt; is an efficient plug-and-play model that provides extra guidance to pre-trained text-to-image models while freezing the original large text-to-image models. T2I-Adapter aligns internal knowledge in T2I models with external control signals. We can train various adapters according to different conditions and achieve rich control and editing effects.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Over the past few weeks, the Diffusers team and the T2I-Adapter authors have been collaborating to bring the support of T2I-Adapters for Stable Diffusion XL (SDXL) in diffusers. In this blog post, we share our findings from training T2I-Adapters on SDXL from scratch, some appealing results, and, of course, the T2I-Adapter checkpoints on various conditionings (sketch, canny, lineart, depth, and openpose)!&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/t2i-adapters-efficient-controllable-generation</link>
      <guid>https://www.lqdev.me/bookmarks/t2i-adapters-efficient-controllable-generation</guid>
      <pubDate>2023-09-17 12:11 -05:00</pubDate>
      <category>ai</category>
      <category>generativeai</category>
      <category>imagegeneration</category>
      <category>deeplearning</category>
      <category>ml</category>
      <category>huggingface</category>
    </item>
    <item>
      <title>Spread Your Wings: Falcon 180B is here </title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we're excited to welcome TII's Falcon 180B to HuggingFace! Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII's RefinedWeb dataset. This represents the longest single-epoch pretraining for an open model.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/falcon-180b-announcement</link>
      <guid>https://www.lqdev.me/bookmarks/falcon-180b-announcement</guid>
      <pubDate>2023-09-12 10:35 -05:00</pubDate>
      <category>ai</category>
      <category>llm</category>
      <category>opensource</category>
      <category>huggingface</category>
    </item>
    <item>
      <title>HuggingFace Candle</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) and ease of use.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/huggingface-candle</link>
      <guid>https://www.lqdev.me/bookmarks/huggingface-candle</guid>
      <pubDate>2023-08-16 21:54 -05:00</pubDate>
      <category>ai</category>
      <category>machinelearning</category>
      <category>opensource</category>
      <category>huggingface</category>
    </item>
  </channel>
</rss>