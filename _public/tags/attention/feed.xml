<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - attention</title>
    <link>https://www.lqdev.me/tags/attention</link>
    <description>All content tagged with 'attention' by Luis Quintanilla</description>
    <lastBuildDate>2025-01-20 21:21 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Short Long Form</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;This is something I've been dealing with on my site. I separate my &lt;a href="https://www.lqdev.me/posts/1"&gt;long-form&lt;/a&gt; from &lt;a href="https://www.lqdev.me/feed"&gt;short-form&lt;/a&gt; posts. However, I don't have a hard limit that clearly defines which is which. An initial reason for the separation was, I wanted to have a feed that was easy to scroll through. That was harder to do if I was rendering an entire long-form post as part of the feed. In the end though, most of my posts are all markdown files and their main distinction is how they are rendered.&lt;/p&gt;
&lt;p&gt;As I work on my website redesign, I want to change how I think about posts. I want to separate the post content from the rendering. A post is a post regardless of content and length. Regardless of whether it's a response, image, video, note, or anything in between, I don't want to constrain the content of my post. I can choose to render posts differently depending on whether it's being viewed in a feed or individual post page. The content driving those different views though shouldn't change.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/short-long-form-moreale</link>
      <guid>https://www.lqdev.me/responses/short-long-form-moreale</guid>
      <pubDate>2025-01-20 21:21 -05:00</pubDate>
      <category>instagram</category>
      <category>posts</category>
      <category>attention</category>
    </item>
    <item>
      <title>Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/griffin-mix-linear-recurrence-local-attention-language-models</link>
      <guid>https://www.lqdev.me/bookmarks/griffin-mix-linear-recurrence-local-attention-language-models</guid>
      <pubDate>2024-04-10 22:02 -05:00</pubDate>
      <category>griffin</category>
      <category>ai</category>
      <category>research</category>
      <category>architecture</category>
      <category>rnn</category>
      <category>attention</category>
      <category>transformers</category>
      <category>llm</category>
    </item>
  </channel>
</rss>