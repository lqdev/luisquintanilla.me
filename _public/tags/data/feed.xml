<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - data</title>
    <link>https://www.lqdev.me/tags/data</link>
    <description>All content tagged with 'data' by Luis Quintanilla</description>
    <lastBuildDate>2024-12-18 21:43 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>MarkItDown - Convert files to Markdown</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;MarkItDown is a utility for converting various files to Markdown (e.g., for indexing, text analysis, etc). It supports:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PDF&lt;/li&gt;
&lt;li&gt;PowerPoint&lt;/li&gt;
&lt;li&gt;Word&lt;/li&gt;
&lt;li&gt;Excel&lt;/li&gt;
&lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt;
&lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt;
&lt;li&gt;HTML&lt;/li&gt;
&lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt;
&lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/microsoft-markitdown</link>
      <guid>https://www.lqdev.me/responses/microsoft-markitdown</guid>
      <pubDate>2024-12-18 21:43 -05:00</pubDate>
      <category>markdown</category>
      <category>etl</category>
      <category>data</category>
      <category>python</category>
      <category>ai</category>
      <category>microsoft</category>
    </item>
    <item>
      <title>Reddit says companies must pay for data access</title>
      <description>&lt;![CDATA[[reply] &lt;p&gt;&lt;img src="https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExejNzaDF5aHZ6dHRlenl6bnh4c2h6YWoxbnRuOXBqZnlmOGVlbWlwOCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/xT9DPzhNGA8MKjxwFG/giphy.gif" class="img-fluid" alt="GIF of Snoopy laughing" /&gt;&lt;/p&gt;
&lt;p&gt;I'm all for compensation. Remind me again, how much are the communities that create the content and make Reddit what it is getting out of these licensing deals?&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/reddit-companies-pay-data-access</link>
      <guid>https://www.lqdev.me/responses/reddit-companies-pay-data-access</guid>
      <pubDate>2024-07-31 16:23 -05:00</pubDate>
      <category>ai</category>
      <category>reddit</category>
      <category>data</category>
      <category>licensing</category>
      <category>microsoft</category>
      <category>anthropic</category>
      <category>perplexity</category>
    </item>
    <item>
      <title>DE-COP: Detecting Copyrighted Content in Language Models Training Data</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give ≈ 4% accuracy. Our code and datasets are available at this https URL&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/avduarte333/DE-COP_Method"&gt;Repo&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/decop-detecting-copyright-llm-training-data</link>
      <guid>https://www.lqdev.me/bookmarks/decop-detecting-copyright-llm-training-data</guid>
      <pubDate>2024-04-10 22:00 -05:00</pubDate>
      <category>ai</category>
      <category>copyright</category>
      <category>research</category>
      <category>llm</category>
      <category>data</category>
    </item>
    <item>
      <title>Releasing Common Corpus: the largest public domain dataset for training LLMs</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We announce today the release of Common Corpus on HuggingFace:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Common Corpus is the largest public domain dataset released for training LLMs.&lt;/li&gt;
&lt;li&gt;Common Corpus includes 500 billion words from a wide diversity of cultural heritage initiatives.&lt;/li&gt;
&lt;li&gt;Common Corpus is multilingual and the largest corpus to date in English, French, Dutch, Spanish, German and Italian.&lt;/li&gt;
&lt;li&gt;Common Corpus shows it is possible to train fully open LLMs on sources without copyright concerns.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/common-corpus-llm-training-dataset</link>
      <guid>https://www.lqdev.me/bookmarks/common-corpus-llm-training-dataset</guid>
      <pubDate>2024-03-20 22:12 -05:00</pubDate>
      <category>ai</category>
      <category>data</category>
      <category>llm</category>
      <category>huggingface</category>
      <category>nlp</category>
    </item>
    <item>
      <title>Observable 2.0 - Announcing Observable Framework</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we’re launching Observable 2.0 with a bold new vision: an open-source static site generator for building fast, beautiful data apps, dashboards, and reports.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our mission is to help teams communicate more effectively with data. Effective presentation of data is critical for deep insight, nuanced understanding, and informed decisions. Observable notebooks are great for ephemeral, ad hoc data exploration. But notebooks aren’t well-suited for polished dashboards and apps.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/observable-2-0</link>
      <guid>https://www.lqdev.me/responses/observable-2-0</guid>
      <pubDate>2024-02-15 20:48 -05:00</pubDate>
      <category>data</category>
      <category>analytics</category>
      <category>observable</category>
    </item>
    <item>
      <title>GraphRAG: Unlocking LLM discovery on narrative private data </title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Perhaps the greatest challenge – and opportunity – of LLMs is extending their powerful capabilities to solve problems beyond the data on which they have been trained, and to achieve comparable results with data the LLM has never seen.  This opens new possibilities in data investigation, such as identifying themes and semantic concepts with context and grounding on datasets.  In this post, we introduce GraphRAG, created by Microsoft Research, as a significant advance in enhancing the capability of LLMs.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/ms-research-graphrag</link>
      <guid>https://www.lqdev.me/bookmarks/ms-research-graphrag</guid>
      <pubDate>2024-02-13 21:50 -05:00</pubDate>
      <category>research</category>
      <category>ai</category>
      <category>rag</category>
      <category>llm</category>
      <category>data</category>
      <category>microsoft</category>
    </item>
    <item>
      <title>Willow Protocol Specifications</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;A protocol for peer-to-peer data stores. The best parts? Fine-grained permissions, a keen approach to privacy, destructive edits, and a dainty bandwidth and memory footprint.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/willow-protocol</link>
      <guid>https://www.lqdev.me/bookmarks/willow-protocol</guid>
      <pubDate>2024-01-17 20:38 -05:00</pubDate>
      <category>willow</category>
      <category>protocol</category>
      <category>p2p</category>
      <category>privacy</category>
      <category>data</category>
    </item>
    <item>
      <title>My AI Timelines Have Sped Up (Again)</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;...computers are useful, ML models are useful, and even if models fail to scale, people will want to fit GPT-4 sized models on their phone. It seems reasonable to assume the competing factions will figure something out.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Data seems like the harder question. (Or at least the one I feel qualified talking about.) We have already crossed the event horizon of trying to train on everything on the Internet. It’s increasingly difficult for labs to differentiate themselves on publicly available data. Differentiation is instead coming from non-public high-quality data to augment public low-quality data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;All the scaling laws have followed power laws so far, including dataset size. Getting more data by hand doesn’t seem good enough to cross to the next thresholds. We need better means to get good data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A long time ago, when OpenAI still did RL in games / simulation, they were very into self-play. You run agents against copies of themselves, score their interactions, and update the models towards interactions with higher reward. Given enough time, they learn complex strategies through competition.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I think it’s possible we’re at the start of a world where self-play or self-play-like ideas work to improve LLM capabilities. Drawing an analogy, the environment is the dialogue, actions are text generated from an LLM, and the reward is from whatever reward model you have. Instead of using ground truth data, our models may be at a point where they can generate data that’s good enough to train on.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/ai-pipelines-sped-up-again-alexirpan</link>
      <guid>https://www.lqdev.me/bookmarks/ai-pipelines-sped-up-again-alexirpan</guid>
      <pubDate>2024-01-11 10:07 -05:00</pubDate>
      <category>ai</category>
      <category>predictions</category>
      <category>agi</category>
      <category>data</category>
      <category>llm</category>
      <category>technology</category>
      <category>ml</category>
      <category>computervision</category>
    </item>
    <item>
      <title>Software²</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;A new generation of AIs that become increasingly general by producing their own training data&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are currently at the cusp of transitioning from “learning from data” to “learning what data to learn from” as the central focus of AI research.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;If deep learning can be described as “Software 2.0”—software that programs itself based on example inputs/output pairs, then this promising, data-centric paradigm, in which software effectively improves itself by searching for its own training data, can be described as a kind of “Software²”. This paradigm inherits the benefits of Software 2.0 while improving on its core, data-bound weaknesses: While deep learning (Software 2.0) requires the programmer to manually provide training data for each new task, Software² recasts data as software that models or searches the world to produce its own, potentially unlimited, training tasks and data.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/software-squared-jiang</link>
      <guid>https://www.lqdev.me/bookmarks/software-squared-jiang</guid>
      <pubDate>2023-09-17 12:54 -05:00</pubDate>
      <category>ai</category>
      <category>software</category>
      <category>data</category>
    </item>
  </channel>
</rss>