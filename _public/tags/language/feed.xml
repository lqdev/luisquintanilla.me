<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - language</title>
    <link>https://www.lqdev.me/tags/language</link>
    <description>All content tagged with 'language' by Luis Quintanilla</description>
    <lastBuildDate>2025-09-11 15:48 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>The mistake music teachers make</title>
      <description>&lt;![CDATA[[star] &lt;p&gt;I like this analogy.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Uc_bPtUltDU" title="The mistake music teachers make"&gt;&lt;img src="http://img.youtube.com/vi/Uc_bPtUltDU/0.jpg" class="img-fluid" alt="The mistake music teachers make" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/mistake-music-teachers-make-ted-talk</link>
      <guid>https://www.lqdev.me/responses/mistake-music-teachers-make-ted-talk</guid>
      <pubDate>2025-09-11 15:48 -05:00</pubDate>
      <category>music</category>
      <category>language</category>
    </item>
    <item>
      <title>LLaVA: Large Language and Vision Assistant</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;LLaVA represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks in the language domain, but the idea is less explored in the multimodal field.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Multimodal Instruct Data. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data.
2. LLaVA Model. We introduce LLaVA (Large Language-and-Vision Assistant), an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.
3. Performance. Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%.
4. Open-source. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/llava-language-vision-assistant</link>
      <guid>https://www.lqdev.me/bookmarks/llava-language-vision-assistant</guid>
      <pubDate>2023-04-18 21:34 -05:00</pubDate>
      <category>ai</category>
      <category>language</category>
      <category>vision</category>
    </item>
  </channel>
</rss>