<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - gpu</title>
    <link>https://www.lqdev.me/tags/gpu</link>
    <description>All content tagged with 'gpu' by Luis Quintanilla</description>
    <lastBuildDate>2024-04-04 11:05 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>IPEX-LLM</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/intel-ipex-llm</link>
      <guid>https://www.lqdev.me/responses/intel-ipex-llm</guid>
      <pubDate>2024-04-04 11:05 -05:00</pubDate>
      <category>intel</category>
      <category>llm</category>
      <category>pytorch</category>
      <category>cpu</category>
      <category>gpu</category>
    </item>
    <item>
      <title>Nvidia reveals Blackwell B200 GPU</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Nvidia reveals Blackwell B200 GPU, the ‘world’s most powerful chip’ for AI&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;‘Built to democratize trillion-parameter AI.’&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Nvidia says the new B200 GPU offers up to 20 petaflops of FP4 horsepower from its 208 billion transistors. Also, it says, a GB200 that combines two of those GPUs with a single Grace CPU can offer 30 times the performance for LLM inference workloads while also potentially being substantially more efficient. It “reduces cost and energy consumption by up to 25x” over an H100, says Nvidia.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/nvidia-blackwell-b200-gpu</link>
      <guid>https://www.lqdev.me/responses/nvidia-blackwell-b200-gpu</guid>
      <pubDate>2024-03-18 21:09 -05:00</pubDate>
      <category>ai</category>
      <category>nvidia</category>
      <category>gpu</category>
      <category>hardware</category>
      <category>pchardware</category>
      <category>pc</category>
    </item>
    <item>
      <title>Ollama now supports AMD graphics cards</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated by AMD graphics cards on Ollama for Linux and Windows.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/ollama-supports-amd-gpus</link>
      <guid>https://www.lqdev.me/responses/ollama-supports-amd-gpus</guid>
      <pubDate>2024-03-17 20:43 -05:00</pubDate>
      <category>ollama</category>
      <category>gpu</category>
      <category>amd</category>
      <category>ai</category>
      <category>llm</category>
      <category>opensource</category>
    </item>
    <item>
      <title>You can now train a 70b language model at home</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we’re releasing Answer.AI’s first project: a fully open source system that, for the first time, can efficiently train a 70b large language model on a regular desktop computer with two or more standard gaming GPUs (RTX 3090 or 4090). This system, which combines FSDP and QLoRA, is the result of a collaboration between Answer.AI, Tim Dettmers (U Washington), and Hugging Face’s Titus von Koeller and Sourab Mangrulkar.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/answer-ai-train-70b-llm-at-home</link>
      <guid>https://www.lqdev.me/responses/answer-ai-train-70b-llm-at-home</guid>
      <pubDate>2024-03-08 12:13 -05:00</pubDate>
      <category>ai</category>
      <category>llm</category>
      <category>qlora</category>
      <category>gpu</category>
      <category>nvidia</category>
      <category>finetuning</category>
    </item>
    <item>
      <title>Training great LLMs entirely from ground up in the wilderness as a startup</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Given that we’ve successfully trained pretty strong multimodal language models at Reka, many people have been particularly curious about the experiences of building infrastructure and training large language &amp;amp; multimodal models from scratch from a completely clean slate.&lt;br /&gt;
&lt;br&gt;
I complain a lot about external (outside Google) infrastructure and code on my social media, leading people to really be curious about what are the things I miss and what I hate/love in the wilderness. So here’s a post (finally). This blogpost sheds light on the challenges and lessons learned.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Figuring out things in the wilderness was an interesting experience. It was unfortunately not painless. Compute scarcity and also unreliable compute providers made things significantly harder than expected but we’re glad we pulled through with brute technical strength.&lt;br /&gt;
&lt;br&gt;
All in all, this is only a small part of the story of how we started a company, raised some money, bought some chips and matched Gemini pro/GPT 3.5 and outperformed many others in less than a year having to build everything from scratch.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/training-llms-ground-up-wilderness-startup</link>
      <guid>https://www.lqdev.me/responses/training-llms-ground-up-wilderness-startup</guid>
      <pubDate>2024-03-07 21:10 -05:00</pubDate>
      <category>llm</category>
      <category>ai</category>
      <category>startup</category>
      <category>compute</category>
      <category>gpu</category>
      <category>training</category>
    </item>
    <item>
      <title>NVIDIA Chat with RTX</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Chat With RTX is a demo app that lets you personalize a GPT large language model (LLM) connected to your own content—docs, notes, videos, or other data. Leveraging retrieval-augmented generation (RAG), TensorRT-LLM, and RTX acceleration, you can query a custom chatbot to quickly get contextually relevant answers. And because it all runs locally on your Windows RTX PC or workstation, you’ll get fast and secure results.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/nvidia-chat-rtx</link>
      <guid>https://www.lqdev.me/responses/nvidia-chat-rtx</guid>
      <pubDate>2024-02-13 21:08 -05:00</pubDate>
      <category>nvidia</category>
      <category>chat</category>
      <category>ai</category>
      <category>rag</category>
      <category>chatbot</category>
      <category>gpu</category>
      <category>llm</category>
    </item>
    <item>
      <title>Getting Started With CUDA for Python Programmers</title>
      <description>&lt;![CDATA[[bookmark] &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=nOxKexn3iBo" title="A YouTube Video Tutorial by Jeremy Howard to help Python programmers get started with CUDA"&gt;&lt;img src="http://img.youtube.com/vi/nOxKexn3iBo/0.jpg" class="img-fluid" alt="A YouTube Video Tutorial by Jeremy Howard to help Python programmers get started with CUDA" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In this comprehensive video tutorial, Jeremy Howard from answer.ai demystifies the process of programming NVIDIA GPUs using CUDA, and simplifies the perceived complexities of CUDA programming. Jeremy emphasizes the accessibility of CUDA, especially when combined with PyTorch's capabilities, allowing for programming directly in notebooks rather than traditional compilers and terminals. To make CUDA more approachable to Python programmers, Jeremy shows step by step how to start with Python implementations, and then convert them largely automatically to CUDA. This approach, he argues, simplifies debugging and development.&lt;br /&gt;
&lt;br&gt;
The tutorial is structured in a hands-on manner, encouraging viewers to follow along in a Colab notebook. Jeremy uses practical examples, starting with converting an RGB image to grayscale using CUDA, demonstrating the process step-by-step. He further explains the memory layout in GPUs, emphasizing the differences from CPU memory structures, and introduces key CUDA concepts like streaming multi-processors and CUDA cores.&lt;br /&gt;
&lt;br&gt;
Jeremy then delves into more advanced topics, such as matrix multiplication, a critical operation in deep learning. He demonstrates how to implement matrix multiplication in Python first and then translates it to CUDA, highlighting the significant performance gains achievable with GPU programming. The tutorial also covers CUDA's intricacies, such as shared memory, thread blocks, and optimizing CUDA kernels.&lt;br /&gt;
&lt;br&gt;
The tutorial also includes a section on setting up the CUDA environment on various systems using Conda, making it accessible for a wide range of users.&lt;br /&gt;
&lt;br&gt;
This is lecture 3 of the &amp;quot;CUDA Mode&amp;quot; series (but you don't need to watch the others first). The notebook is available in the lecture3 folder here: &lt;a href="https://github.com/cuda-mode/lecture2"&gt;https://github.com/cuda-mode/lecture2&lt;/a&gt;...&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/get-started-cuda-python-programmers</link>
      <guid>https://www.lqdev.me/bookmarks/get-started-cuda-python-programmers</guid>
      <pubDate>2024-01-29 20:23 -05:00</pubDate>
      <category>gpu</category>
      <category>cuda</category>
      <category>python</category>
      <category>programming</category>
    </item>
    <item>
      <title>Creating the First Confidential GPUs</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The team at NVIDIA brings confidentiality and integrity to user code and data for accelerated computing.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/creating-first-confidential-gpus</link>
      <guid>https://www.lqdev.me/bookmarks/creating-first-confidential-gpus</guid>
      <pubDate>2023-09-28 20:59 -05:00</pubDate>
      <category>ai</category>
      <category>security</category>
      <category>privacy</category>
      <category>gpu</category>
    </item>
    <item>
      <title>Intel GPU Tools - Linux</title>
      <description>&lt;![CDATA[&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;This article talks about Intel GPU tools for Linux to monitor usage of integrated Intel GPUs&lt;/p&gt;
&lt;h2&gt;When to use&lt;/h2&gt;
&lt;p&gt;The scenarios in which I've used this tool is to confirm that my browser is leveraging hardware acceleration. By default, Firefox doesn't enable hardware acceleration on Linux. As a result, everything runs on the CPU causing unnecessary load and faster battery drain. To alleviate that, you can enable hardware acceleration and make sure that the GPU is being used by running tools like &lt;code&gt;intel_gpu_top&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Install&lt;/h2&gt;
&lt;p&gt;To install the tools, run the following command&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo pacman -S intel-gpu-tools
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Run intel_gpu_top&lt;/h2&gt;
&lt;p&gt;To view GPU activity, run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo intel_gpu_top
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will display process graphs of GPU usage.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://wiki.archlinux.org/title/Intel_graphics"&gt;Intel Grapics - Arch Wiki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://archlinux.org/packages/community/x86_64/intel-gpu-tools/"&gt;Intel GPU Tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://support.mozilla.org/en-US/kb/upgrade-graphics-drivers-use-hardware-acceleration"&gt;Firefox hardware acceleration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://support.mozilla.org/en-US/kb/performance-settings"&gt;Firefox Performance Settings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/resources/wiki/intel-gpu-tools</link>
      <guid>https://www.lqdev.me/resources/wiki/intel-gpu-tools</guid>
      <pubDate>12/11/2022 18:05 -05:00</pubDate>
      <category>linux</category>
      <category>intel</category>
      <category>gpu</category>
      <category>manjaro</category>
      <category>performances</category>
    </item>
  </channel>
</rss>