<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - ml</title>
    <link>https://www.lqdev.me/tags/ml</link>
    <description>All content tagged with 'ml' by Luis Quintanilla</description>
    <lastBuildDate>2025-01-20 21:09 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Semantic Search and On-Device ML in Emacs</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;This is a cool walk-through of how to add semantic search to Emacs using local ML models.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;With ONNX.el you can run any ML model inside Emacs, including ones for images, audios, etc. In case you are running embedding models, combine this with sem.el to perform semantic searches. Depending on your input modality, you might have to figure out preprocessing. For text, tokenizers.el should cover all of what you need in modern NLP for preprocessing, though you will have to do something on your own for anything non-text or multimodal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;By default sem runs a local all-MiniLM-L6-v2 model for text embeddings. This model is small and runs fast on CPU. Additionally, we load an O2 optimized variant which helps further. The vectors are stored in a lancedb database on file system which runs without a separate process. I was originally writing the vector db core myself to learn more of Rust, but then stopped doing that since lancedb gives all that I needed, out of the box.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/semantic-search-on-device-ml-emacs</link>
      <guid>https://www.lqdev.me/responses/semantic-search-on-device-ml-emacs</guid>
      <pubDate>2025-01-20 21:09 -05:00</pubDate>
      <category>onnx</category>
      <category>emacs</category>
      <category>ai</category>
      <category>ml</category>
    </item>
    <item>
      <title>Tensors from scratch series</title>
      <description>&lt;![CDATA[[star] &lt;p&gt;I've been enjoying reading through this Tensors series.&lt;/p&gt;
&lt;p&gt;If you're interested, here's also the link to &lt;a href="https://maharshi.bearblog.dev/tensors-from-scratch-part-1/"&gt;part 1&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/tensors-from-scratch-blog-series</link>
      <guid>https://www.lqdev.me/responses/tensors-from-scratch-blog-series</guid>
      <pubDate>2024-07-29 22:26 -05:00</pubDate>
      <category>tensors</category>
      <category>math</category>
      <category>ai</category>
      <category>c</category>
      <category>machinelearning</category>
      <category>ml</category>
      <category>artificialintelligence</category>
    </item>
    <item>
      <title>Machine Learning for Games Course - HuggingFace</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Welcome to the course that will teach you the most fascinating topic in game development: how to use powerful AI tools and models to create unique game experiences.&lt;br /&gt;
&lt;br&gt;
New AI models are revolutionizing the Game Industry in two impactful ways:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On how we make games:
&lt;ul&gt;
&lt;li&gt;Generate textures using AI&lt;/li&gt;
&lt;li&gt;Using AI voice actors for the voices.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How we create gameplay:
&lt;ul&gt;
&lt;li&gt;Crafting smart Non-Playable Characters (NPCs) using large language models.&lt;br /&gt;
&lt;br&gt;
This course will teach you:&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How to integrate AI models for innovative gameplay, featuring intelligent NPCs.&lt;/li&gt;
&lt;li&gt;How to use AI tools to help your game development pipeline.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/machine-learning-for-games-course-huggingface</link>
      <guid>https://www.lqdev.me/bookmarks/machine-learning-for-games-course-huggingface</guid>
      <pubDate>2024-03-19 22:35 -05:00</pubDate>
      <category>machinelearning</category>
      <category>games</category>
      <category>huggingface</category>
      <category>course</category>
      <category>ai</category>
      <category>ml</category>
    </item>
    <item>
      <title>Predictive Human Preference: From Model Ranking to Model Routing</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Human preference has emerged to be both the Northstar and a powerful tool for AI model development. Human preference guides post-training techniques including RLHF and DPO. Human preference is also used to rank AI models, as used by LMSYS’s Chatbot Arena.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Chatbot Arena aims to determine which model is generally preferred. I wanted to see if it’s possible to do predictive human preference: determine which model is preferred for each query.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This post first discusses the correctness of Chatbot Arena, which will then be used as a baseline to evaluate the correctness of preference predictions. It then discusses how to build a preference predictor and the initial results.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/predictive-human-preference-model-ranking-routing</link>
      <guid>https://www.lqdev.me/responses/predictive-human-preference-model-ranking-routing</guid>
      <pubDate>2024-02-29 10:43 -05:00</pubDate>
      <category>ai</category>
      <category>llm</category>
      <category>evaluation</category>
      <category>ml</category>
    </item>
    <item>
      <title>Stable Diffusion 3 - Early Preview</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Announcing Stable Diffusion 3 in early preview, our most capable text-to-image model with greatly improved performance in multi-subject prompts, image quality, and spelling abilities.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Stable Diffusion 3 suite of models currently range from 800M to 8B parameters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable Diffusion 3 combines a diffusion transformer architecture and flow matching.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/stable-diffusion-3-early-preview</link>
      <guid>https://www.lqdev.me/responses/stable-diffusion-3-early-preview</guid>
      <pubDate>2024-02-22 20:59 -05:00</pubDate>
      <category>stabilityai</category>
      <category>ai</category>
      <category>stablediffusion</category>
      <category>imagegeneration</category>
      <category>ml</category>
      <category>image</category>
      <category>genai</category>
      <category>generativeai</category>
    </item>
    <item>
      <title>The AI Study Guide: Azure Machine Learning Edition</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The AI Study Guide: Discover Machine Learning with these free Azure resources&lt;br /&gt;
&lt;br&gt;
Welcome to the February edition of the Azure AI Study Guide. Every month I’ll be spilling the tea on the best and newest tools for skilling up on Azure AI. This month we’re putting on our thinking caps to investigate Azure Machine Learning (ML). I’ll give you a quick breakdown of what it is, then we’ll explore a four-week roadmap of our top FREE resources for you to continue your AI learning journey! And as a bonus, stay tuned to the end to see what makes machine learning and generative AI a dynamic duo.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/ai-study-guide-azure-ml-edition</link>
      <guid>https://www.lqdev.me/bookmarks/ai-study-guide-azure-ml-edition</guid>
      <pubDate>2024-02-21 15:18 -05:00</pubDate>
      <category>azure</category>
      <category>tutorial</category>
      <category>machinelearning</category>
      <category>ml</category>
      <category>ai</category>
      <category>azureml</category>
    </item>
    <item>
      <title>MLX Swift - On-device ML research with MLX and Swift</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Swift programming language has a lot of potential to be used for machine learning research because it combines the ease of use and high-level syntax of a language like Python with the speed of a compiled language like C++.&lt;br /&gt;
&lt;br&gt;
&lt;a href="https://github.com/ml-explore/mlx"&gt;MLX&lt;/a&gt; is an array framework for machine learning research on Apple silicon. MLX is intended for research and not for production deployment of models in apps.&lt;br /&gt;
&lt;br&gt;
&lt;a href="https://github.com/ml-explore/mlx-swift/"&gt;MLX Swift&lt;/a&gt; expands MLX to the Swift language, making experimentation on Apple silicon easier for ML researchers.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/swift-mlx-ml-research</link>
      <guid>https://www.lqdev.me/responses/swift-mlx-ml-research</guid>
      <pubDate>2024-02-20 16:25 -05:00</pubDate>
      <category>apple</category>
      <category>mlx</category>
      <category>swift</category>
      <category>opensource</category>
      <category>ml</category>
      <category>ai</category>
    </item>
    <item>
      <title>Ollama - Windows Preview</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Ollama is now available on Windows in preview, making it possible to pull, run and create large language models in a new native Windows experience. Ollama on Windows includes built-in GPU acceleration, access to the full model library, and the Ollama API including OpenAI compatibility.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://ollama.com/download/windows"&gt;Download (https://ollama.com/download/windows)&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/ollama-windows-preview</link>
      <guid>https://www.lqdev.me/responses/ollama-windows-preview</guid>
      <pubDate>2024-02-19 16:13 -05:00</pubDate>
      <category>ollama</category>
      <category>windows</category>
      <category>llm</category>
      <category>opensource</category>
      <category>localmodels</category>
      <category>ml</category>
      <category>ai</category>
    </item>
    <item>
      <title>HuggingFace - Open Source AI Cookbook</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Open-Source AI Cookbook is a collection of notebooks illustrating practical aspects of building AI applications and solving various machine learning tasks using open-source tools and models.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/open-source-ai-cookbook-hf</link>
      <guid>https://www.lqdev.me/bookmarks/open-source-ai-cookbook-hf</guid>
      <pubDate>2024-02-16 22:36 -05:00</pubDate>
      <category>ai</category>
      <category>tutorial</category>
      <category>huggingface</category>
      <category>opensource</category>
      <category>ml</category>
    </item>
    <item>
      <title>V-JEPA: The next step toward Yann LeCun’s vision of advanced machine intelligence (AMI)</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we’re publicly releasing the Video Joint Embedding Predictive Architecture (V-JEPA) model, a crucial step in &lt;a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"&gt;advancing machine intelligence&lt;/a&gt; with a more grounded understanding of the world.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This early example of a physical world model excels at detecting and understanding highly detailed interactions between objects.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In the spirit of responsible open science, we’re releasing this model under a Creative Commons NonCommercial license for researchers to further explore.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/v-jepa-meta</link>
      <guid>https://www.lqdev.me/responses/v-jepa-meta</guid>
      <pubDate>2024-02-15 20:20 -05:00</pubDate>
      <category>meta</category>
      <category>ai</category>
      <category>ami</category>
      <category>ml</category>
      <category>opensource</category>
      <category>multimodal</category>
    </item>
    <item>
      <title>Introducing Nomic Embed: A Truly Open Embedding Model</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We're excited to announce the release of Nomic Embed, the first&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open source&lt;/li&gt;
&lt;li&gt;Open data&lt;/li&gt;
&lt;li&gt;Open training code&lt;/li&gt;
&lt;li&gt;Fully reproducible and auditable&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/nomic-embed-open-embedding-model</link>
      <guid>https://www.lqdev.me/responses/nomic-embed-open-embedding-model</guid>
      <pubDate>2024-02-03 20:37 -05:00</pubDate>
      <category>embedding</category>
      <category>ai</category>
      <category>opensource</category>
      <category>ml</category>
      <category>model</category>
    </item>
    <item>
      <title>Google’s Hugging Face deal puts ‘supercomputer’ power behind open-source AI</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Google Cloud’s new partnership with AI model repository Hugging Face is letting developers build, train, and deploy AI models without needing to pay for a Google Cloud subscription. Now, outside developers using Hugging Face’s platform will have “cost-effective” access to Google’s tensor processing units (TPU) and GPU supercomputers, which will include thousands of Nvidia’s in-demand and export-restricted H100s.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Google said that Hugging Face users can begin using the AI app-building platform Vertex AI and the Kubernetes engine that helps train and fine-tune models “in the first half of 2024.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://www.googlecloudpresscorner.com/2024-01-25-Google-Cloud-and-Hugging-Face-Announce-Strategic-Partnership-to-Accelerate-Generative-AI-and-ML-Development"&gt;Press Release&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/google-cloud-huggingface-deal</link>
      <guid>https://www.lqdev.me/responses/google-cloud-huggingface-deal</guid>
      <pubDate>2024-01-25 21:23 -05:00</pubDate>
      <category>google</category>
      <category>huggingface</category>
      <category>ai</category>
      <category>cloud</category>
      <category>llm</category>
      <category>ml</category>
      <category>developers</category>
    </item>
    <item>
      <title>My AI Timelines Have Sped Up (Again)</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;...computers are useful, ML models are useful, and even if models fail to scale, people will want to fit GPT-4 sized models on their phone. It seems reasonable to assume the competing factions will figure something out.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Data seems like the harder question. (Or at least the one I feel qualified talking about.) We have already crossed the event horizon of trying to train on everything on the Internet. It’s increasingly difficult for labs to differentiate themselves on publicly available data. Differentiation is instead coming from non-public high-quality data to augment public low-quality data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;All the scaling laws have followed power laws so far, including dataset size. Getting more data by hand doesn’t seem good enough to cross to the next thresholds. We need better means to get good data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A long time ago, when OpenAI still did RL in games / simulation, they were very into self-play. You run agents against copies of themselves, score their interactions, and update the models towards interactions with higher reward. Given enough time, they learn complex strategies through competition.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I think it’s possible we’re at the start of a world where self-play or self-play-like ideas work to improve LLM capabilities. Drawing an analogy, the environment is the dialogue, actions are text generated from an LLM, and the reward is from whatever reward model you have. Instead of using ground truth data, our models may be at a point where they can generate data that’s good enough to train on.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/ai-pipelines-sped-up-again-alexirpan</link>
      <guid>https://www.lqdev.me/bookmarks/ai-pipelines-sped-up-again-alexirpan</guid>
      <pubDate>2024-01-11 10:07 -05:00</pubDate>
      <category>ai</category>
      <category>predictions</category>
      <category>agi</category>
      <category>data</category>
      <category>llm</category>
      <category>technology</category>
      <category>ml</category>
      <category>computervision</category>
    </item>
    <item>
      <title>Ferret: Refer and Ground Anything Anywhere at Any Granularity</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination. Code and data will be available at this https URL&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/apple/ml-ferret"&gt;Code&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/apple-ml-ferret</link>
      <guid>https://www.lqdev.me/bookmarks/apple-ml-ferret</guid>
      <pubDate>2023-12-25 20:12 -05:00</pubDate>
      <category>ai</category>
      <category>ml</category>
      <category>llm</category>
      <category>mllm</category>
      <category>largelanguagemodel</category>
      <category>multimodal</category>
      <category>multimodallargelanguagemodel</category>
      <category>apple</category>
      <category>opensource</category>
    </item>
    <item>
      <title>HuggingFace: Text Embeddings Inference</title>
      <description>&lt;![CDATA[[bookmark] &lt;p&gt;A blazing fast inference solution for text embeddings models.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/hf-text-embedding-inference</link>
      <guid>https://www.lqdev.me/bookmarks/hf-text-embedding-inference</guid>
      <pubDate>2023-10-14 12:54 -05:00</pubDate>
      <category>ai</category>
      <category>embeddings</category>
      <category>text</category>
      <category>inference</category>
      <category>ml</category>
    </item>
    <item>
      <title>Carton - Run any ML model from any programming language</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Carton makes it easy to run any ML model from any programming language.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/carton-ml-models</link>
      <guid>https://www.lqdev.me/bookmarks/carton-ml-models</guid>
      <pubDate>2023-09-28 10:03 -05:00</pubDate>
      <category>ai</category>
      <category>ml</category>
      <category>mlops</category>
    </item>
    <item>
      <title>ChatGPT can now see, hear, and speak</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are beginning to roll out new voice and image capabilities in ChatGPT. They offer a new, more intuitive type of interface by allowing you to have a voice conversation or show ChatGPT what you’re talking about.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/chatgpt-voice-image-capabilities</link>
      <guid>https://www.lqdev.me/bookmarks/chatgpt-voice-image-capabilities</guid>
      <pubDate>2023-09-25 10:12 -05:00</pubDate>
      <category>ai</category>
      <category>chatgpt</category>
      <category>openai</category>
      <category>llm</category>
      <category>machinelearning</category>
      <category>ml</category>
    </item>
    <item>
      <title>DALL·E 3</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;DALL·E 3 understands significantly more nuance and detail than our previous systems, allowing you to easily translate your ideas into exceptionally accurate images.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/dall-e-3</link>
      <guid>https://www.lqdev.me/bookmarks/dall-e-3</guid>
      <pubDate>2023-09-22 10:18 -05:00</pubDate>
      <category>dalle</category>
      <category>ai</category>
      <category>openai</category>
      <category>images</category>
      <category>llm</category>
      <category>ml</category>
    </item>
    <item>
      <title>Coqui 🐸 XTTS</title>
      <description>&lt;![CDATA[[bookmark] &lt;p&gt;&lt;a href="https://huggingface.co/coqui/XTTS-v1"&gt;XTTS&lt;/a&gt; is a Voice generation model that lets you clone voices into different languages by using just a quick 3-second audio clip.&lt;/p&gt;
&lt;p&gt;XTTS is built on previous research, like Tortoise, with additional architectural innovations and training to make cross-language voice cloning and multilingual speech generation possible.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/coqui-xtts-text-to-speech</link>
      <guid>https://www.lqdev.me/bookmarks/coqui-xtts-text-to-speech</guid>
      <pubDate>2023-09-17 12:25 -05:00</pubDate>
      <category>ai</category>
      <category>generativeai</category>
      <category>speech</category>
      <category>huggingface</category>
      <category>ml</category>
      <category>tts</category>
      <category>opensource</category>
    </item>
    <item>
      <title>Introducing Würstchen: Fast Diffusion for Image Generation </title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Würstchen is a diffusion model, whose text-conditional component works in a highly compressed latent space of images. Why is this important? Compressing data can reduce computational costs for both training and inference by orders of magnitude. Training on 1024×1024 images is way more expensive than training on 32×32. Usually, other works make use of a relatively small compression, in the range of 4x - 8x spatial compression. Würstchen takes this to an extreme. Through its novel design, it achieves a 42x spatial compression! This had never been seen before, because common methods fail to faithfully reconstruct detailed images after 16x spatial compression. Würstchen employs a two-stage compression, what we call Stage A and Stage B. Stage A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found in the  paper). Together Stage A and B are called the Decoder, because they decode the compressed images back into pixel space. A third model, Stage C, is learned in that highly compressed latent space. This training requires fractions of the compute used for current top-performing models, while also allowing cheaper and faster inference. We refer to Stage C as the Prior.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/wurstchen-fast-image-generation</link>
      <guid>https://www.lqdev.me/bookmarks/wurstchen-fast-image-generation</guid>
      <pubDate>2023-09-17 12:15 -05:00</pubDate>
      <category>ai</category>
      <category>generativeai</category>
      <category>imagegeneration</category>
      <category>huggingface</category>
      <category>diffusion</category>
      <category>ml</category>
    </item>
  </channel>
</rss>