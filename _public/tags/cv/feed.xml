<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - cv</title>
    <link>https://www.lqdev.me/tags/cv</link>
    <description>All content tagged with 'cv' by Luis Quintanilla</description>
    <lastBuildDate>2024-07-29 22:22 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Meta AI's Segment Anything Model (SAM) 2</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;h2&gt;Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Following up on the success of the Meta Segment Anything Model (SAM) for images, we're releasing SAM 2, a unified model for real-time promptable object segmentation in images and videos that achieves state-of-the-art performance.&lt;/li&gt;
&lt;li&gt;In keeping with our approach to open science, we're sharing the code and model weights with a permissive Apache 2.0 license.&lt;/li&gt;
&lt;li&gt;We're also sharing the SA-V dataset, which includes approximately 51,000 real-world videos and more than 600,000 masklets (spatio-temporal masks).&lt;/li&gt;
&lt;li&gt;SAM 2 can segment any object in any video or image - even for objects and visual domains it has not seen previously, enabling a diverse range of use cases without custom adaptation.&lt;/li&gt;
&lt;li&gt;SAM 2 has many potential real-world applications. For example, the outputs of SAM 2 can be used with a generative video model to create new video effects and unlock new creative applications. SAM 2 could also aid in faster annotation tools for visual data to build better computer vision systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/meta-ai-segment-anything-model-2</link>
      <guid>https://www.lqdev.me/responses/meta-ai-segment-anything-model-2</guid>
      <pubDate>2024-07-29 22:22 -05:00</pubDate>
      <category>meta</category>
      <category>ai</category>
      <category>computervision</category>
      <category>sam2</category>
      <category>segmentanythingmodel</category>
      <category>aimodel</category>
      <category>cv</category>
    </item>
    <item>
      <title>NightShade</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Nightshade, a tool that turns any image into a data sample that is unsuitable for model training. More precisely, Nightshade transforms images into &amp;quot;poison&amp;quot; samples, so that models training on them without consent will see their models learn unpredictable behaviors that deviate from expected norms, e.g. a prompt that asks for an image of a cow flying in space might instead get an image of a handbag floating in space.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://nightshade.cs.uchicago.edu/whatis.html"&gt;What is NightShade?&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/nightshade-ai-model-poisoning-tool</link>
      <guid>https://www.lqdev.me/responses/nightshade-ai-model-poisoning-tool</guid>
      <pubDate>2024-01-23 20:57 -05:00</pubDate>
      <category>research</category>
      <category>tools</category>
      <category>ai</category>
      <category>generativeai</category>
      <category>llm</category>
      <category>computervision</category>
      <category>cv</category>
    </item>
  </channel>
</rss>