<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - pytorch</title>
    <link>https://www.lqdev.me/tags/pytorch</link>
    <description>All content tagged with 'pytorch' by Luis Quintanilla</description>
    <lastBuildDate>2024-04-04 11:05 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>IPEX-LLM</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/intel-ipex-llm</link>
      <guid>https://www.lqdev.me/responses/intel-ipex-llm</guid>
      <pubDate>2024-04-04 11:05 -05:00</pubDate>
      <category>intel</category>
      <category>llm</category>
      <category>pytorch</category>
      <category>cpu</category>
      <category>gpu</category>
    </item>
    <item>
      <title>Quanto: a PyTorch quantization toolkit </title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Quantization is a technique to reduce the computational and memory costs of evaluating Deep Learning Models by representing their weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we are excited to introduce quanto, a versatile pytorch quantization toolkit, that provides several unique features:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;available in eager mode (works with non-traceable models)&lt;/li&gt;
&lt;li&gt;quantized models can be placed on any device (including CUDA and MPS),&lt;/li&gt;
&lt;li&gt;automatically inserts quantization and dequantization stubs,&lt;/li&gt;
&lt;li&gt;automatically inserts quantized functional operations,&lt;/li&gt;
&lt;li&gt;automatically inserts quantized modules (see below the list of supported modules),&lt;/li&gt;
&lt;li&gt;provides a seamless workflow for a float model, going from a dynamic to a static quantized model,&lt;/li&gt;
&lt;li&gt;supports quantized model serialization as a state_dict,&lt;/li&gt;
&lt;li&gt;supports not only int8 weights, but also int2 and int4,&lt;/li&gt;
&lt;li&gt;supports not only int8 activations, but also float8.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/quanto-pytorch-quantization-toolkit</link>
      <guid>https://www.lqdev.me/responses/quanto-pytorch-quantization-toolkit</guid>
      <pubDate>2024-03-19 22:00 -05:00</pubDate>
      <category>huggingface</category>
      <category>quantization</category>
      <category>pytorch</category>
      <category>tools</category>
      <category>ai</category>
    </item>
    <item>
      <title>Gemma PyTorch</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Gemma is a family of lightweight, state-of-the art open models built from research and technology used to create Google Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This is the official PyTorch implementation of Gemma models. We provide model and inference implementations using both PyTorch and PyTorch/XLA, and support running inference on CPU, GPU and TPU.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/gemma-pytorch</link>
      <guid>https://www.lqdev.me/responses/gemma-pytorch</guid>
      <pubDate>2024-03-04 21:29 -05:00</pubDate>
      <category>google</category>
      <category>gemma</category>
      <category>pytorch</category>
      <category>ai</category>
      <category>llm</category>
    </item>
    <item>
      <title>Introducing Keras Core: Keras for TensorFlow, JAX, and PyTorch.</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We're excited to share with you a new library called Keras Core, a preview version of the future of Keras. In Fall 2023, this library will become Keras 3.0. Keras Core is a full rewrite of the Keras codebase that rebases it on top of a modular backend architecture. It makes it possible to run Keras workflows on top of arbitrary frameworks — starting with TensorFlow, JAX, and PyTorch.&lt;/p&gt;
&lt;p&gt;Keras Core is also a drop-in replacement for tf.keras, with near-full backwards compatibility with tf.keras code when using the TensorFlow backend. In the vast majority of cases you can just start importing it via import keras_core as keras in place of from tensorflow import keras and your existing code will run with no issue — and generally with slightly improved performance, thanks to XLA compilation.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/keras-core-announcement</link>
      <guid>https://www.lqdev.me/bookmarks/keras-core-announcement</guid>
      <pubDate>2023-07-11 16:22 -05:00</pubDate>
      <category>deeplearning</category>
      <category>ai</category>
      <category>keras</category>
      <category>jax</category>
      <category>tensorflow</category>
      <category>pytorch</category>
    </item>
  </channel>
</rss>