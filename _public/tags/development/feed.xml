<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - development</title>
    <link>https://www.lqdev.me/tags/development</link>
    <description>All content tagged with 'development' by Luis Quintanilla</description>
    <lastBuildDate>06/29/2024 14:48 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>DevContainer configurations</title>
      <description>&lt;![CDATA[&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;A collection of DevContainer configurations&lt;/p&gt;
&lt;h2&gt;Base Debian Image&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Base Debian DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Python&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Python DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/python:1&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;3.11&amp;quot;
        },
        &amp;quot;ghcr.io/va-h/devcontainers-features/uv:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;                
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Python (GPU)&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Python (GPU) DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/python:1&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;3.11&amp;quot;
        },
        &amp;quot;ghcr.io/devcontainers/features/nvidia-cuda:1&amp;quot;: {},
        &amp;quot;ghcr.io/va-h/devcontainers-features/uv:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;                
            ]
        }
    },
    &amp;quot;runArgs&amp;quot;: [
        &amp;quot;--gpus&amp;quot;, 
        &amp;quot;all&amp;quot;
    ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;.NET&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me .NET DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/dotnet:2&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;9.0&amp;quot;
        }
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;Ionide.Ionide-fsharp&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;,
                &amp;quot;ms-dotnettools.csdevkit&amp;quot;                                
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;.NET (GPU)&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me .NET (GPU) DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/dotnet:2&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;9.0&amp;quot;
        },
        &amp;quot;ghcr.io/devcontainers/features/nvidia-cuda:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;Ionide.Ionide-fsharp&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;,
                &amp;quot;ms-dotnettools.csdevkit&amp;quot;                
            ]
        }
    },
    &amp;quot;runArgs&amp;quot;: [
        &amp;quot;--gpus&amp;quot;, 
        &amp;quot;all&amp;quot;
    ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Python and .NET&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Python and .NET DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/python:1&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;3.11&amp;quot;
        },        
        &amp;quot;ghcr.io/devcontainers/features/dotnet:2&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;9.0&amp;quot;
        },
        &amp;quot;ghcr.io/va-h/devcontainers-features/uv:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;Ionide.Ionide-fsharp&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;,
                &amp;quot;ms-dotnettools.csdevkit&amp;quot;                
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Python and .NET (GPU)&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Python and .NET (GPU) DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/python:1&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;3.11&amp;quot;
        },        
        &amp;quot;ghcr.io/devcontainers/features/dotnet:2&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;9.0&amp;quot;
        },
        &amp;quot;ghcr.io/devcontainers/features/nvidia-cuda:1&amp;quot;: {},
        &amp;quot;ghcr.io/va-h/devcontainers-features/uv:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;Ionide.Ionide-fsharp&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;,
                &amp;quot;ms-dotnettools.csdevkit&amp;quot;               
            ]
        }
    },
    &amp;quot;runArgs&amp;quot;: [
        &amp;quot;--gpus&amp;quot;, 
        &amp;quot;all&amp;quot;
    ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Additional Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://code.visualstudio.com/docs/devcontainers/containers"&gt;Developing inside a DevContainer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/devcontainers/images"&gt;Pre-built DevContainer images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/devcontainers/features"&gt;Pre-built DevContainer features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.github.com/en/codespaces/overview"&gt;GitHub Codespaces overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://marketplace.visualstudio.com/vscode"&gt;VS Code Extensions Marketplace&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/resources/wiki/devcontainers-configurations</link>
      <guid>https://www.lqdev.me/resources/wiki/devcontainers-configurations</guid>
      <pubDate>06/29/2024 14:48 -05:00</pubDate>
      <category>devcontainer</category>
      <category>vscode</category>
      <category>codespaces</category>
      <category>development</category>
      <category>software</category>
      <category>tech</category>
      <category>programming</category>
      <category>python</category>
      <category>dotnet</category>
      <category>csharp</category>
      <category>fsharp</category>
      <category>docker</category>
      <category>git</category>
      <category>debian</category>
    </item>
    <item>
      <title>Configure Ollama on Dev Containers and VS Code</title>
      <description>&lt;![CDATA[
The source for my website is [hosted on GitHub](/github/luisquintanilla.me). As a result, I use VS Code as the text editor to author my posts. Typically, I use [Codespaces or github.dev](/colophon) to quickly draft and publish articles to simplify the process. 

As part of my authoring and publishing posts, there's a few things I do like:

1. Coming up with a title
2. Creating relevant tags to help with discovery and connecting related information on my website.
3. For long-form blog posts, I also include a description. 

This is generally relatively easy to do. However, sometimes I spend too much time coming up with something that truly summarizes and condenses the information in the post. It'd be nice if I had an assistant to help me brainstorm and workshop some of these items.

In comes AI. Now, I could use something like [Copilot](https://code.visualstudio.com/docs/copilot/overview) which would work wonderfully and easily plug into my workflow. However, my website is a labor of love and I don't make any money from it. In many instances, I've designed [various components to be as low-cost as possible](/posts/receive-webmentions-fsharp-az-functions-fsadvent). 

Recently, I created a post which showed [how to get started with Ollama on Windows](/posts/getting-started-ollama-windows). In this post, I'll show how you can do the same for your Dev Container environments. 

## Install Ollama

Assuming you already have a [Dev Container configuration file](https://code.visualstudio.com/docs/devcontainers/create-dev-container), add the following line to it:

```json
"postCreateCommand": "curl -fsSL https://ollama.com/install.sh | sh"
```

This command triggers when the container environment is created. This will install Ollama in your development environment. For more details, see the [Ollama download instructions](https://ollama.com/download/linux).

## Start Ollama

Now that Ollama is installed, it's time to start the service so you can get models and use them. To start the Ollama service on your Dev Conatiner environment, add the following line to your Dev Container configuration file.

```json
"postStartCommand": "ollama serve"
```

This command will run `ollama serve` when the Dev Container environment starts.

## Pull a model

To use Ollama, you're going to need a model. In my case, I went with [Phi-2](https://ollama.com/library/phi) because it's lightweight and space on Codespaces is limited.  

To get the model:

1. Open the terminal
1. Enter the following command:

    ```bash
    ollama pull phi
    ```

    After a few minutes, the model is downloaded and ready to use. 

1. Enter the following command to ensure that your model is now available

    ```bash
    ollama list
    ```

    The result should look like the following:

    ```text
    NAME            ID              SIZE    MODIFIED      
    phi:latest      e2fd6321a5fe    1.6 GB  6 seconds ago
    ```

For more details, see the [Ollama model library](https://ollama.com/library). 

## Conclusion

In this post, I showed how you can configure Ollama with Dev Containers to use AI models locally in your projects. In subsequent posts, I'll show how once the service is running, I use it as part of my authoring and publishing posts. You can see a preview of it in my [scripts directory](https://github.com/lqdev/luisquintanilla.me/blob/main/Scripts/ai.fsx). Happy coding! ]]&gt;</description>
      <link>https://www.lqdev.me/posts/install-ollama-vscode-devcontainer</link>
      <guid>https://www.lqdev.me/posts/install-ollama-vscode-devcontainer</guid>
      <pubDate>2024-03-06 13:50 -05:00</pubDate>
      <category>ollama</category>
      <category>vscode</category>
      <category>devcontainer</category>
      <category>ai</category>
      <category>llm</category>
      <category>opensource</category>
      <category>development</category>
    </item>
    <item>
      <title>Podcast - localfirst.fm</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;A podcast about local-first software development&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/localfirstfm-podcast</link>
      <guid>https://www.lqdev.me/responses/localfirstfm-podcast</guid>
      <pubDate>2024-02-20 21:38 -05:00</pubDate>
      <category>development</category>
      <category>software</category>
      <category>podcast</category>
      <category>tech</category>
      <category>podcast</category>
      <category>localfirst</category>
      <category>local</category>
    </item>
    <item>
      <title>Jupyter AI Brings Generative AI to Notebooks</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The open-source Project Jupyter, used by millions for data science and machine learning, has released Jupyter AI, a free tool bringing powerful generative AI capabilities to Jupyter notebooks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://jupyter-ai.readthedocs.io/en/latest/"&gt;https://jupyter-ai.readthedocs.io/en/latest/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Jupyter AI, which brings generative AI to Jupyter. Jupyter AI provides a user-friendly and powerful way to explore generative AI models in notebooks and improve your productivity in JupyterLab and the Jupyter Notebook. More specifically, Jupyter AI offers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;An %%ai magic that turns the Jupyter notebook into a reproducible generative AI playground. This works anywhere the IPython kernel runs (JupyterLab, Jupyter Notebook, Google Colab, VSCode, etc.).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A native chat UI in JupyterLab that enables you to work with generative AI as a conversational assistant.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Support for a wide range of generative model providers and models (AI21, Anthropic, Cohere, Hugging Face, OpenAI, SageMaker, etc.).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/jupyter-ai-notebooks</link>
      <guid>https://www.lqdev.me/bookmarks/jupyter-ai-notebooks</guid>
      <pubDate>2023-08-16 21:43 -05:00</pubDate>
      <category>ai</category>
      <category>notebooks</category>
      <category>development</category>
    </item>
    <item>
      <title>Configured GitHub Codespaces Dev Container with .NET 7</title>
      <description>&lt;![CDATA[&lt;p&gt;Soon after .NET 7 was released, I &lt;a href="https://www.lqdev.me/notes/net7-website-update"&gt;upgraded&lt;/a&gt; the static site generator I use as well as the GitHub Actions that build and publish my website. Having upgraded last year from .NET 6, the process was as smooth as I had expected with no code refactoring required.&lt;/p&gt;
&lt;p&gt;When it comes to development environments, for quick status updates like the ones on my &lt;a href="https://www.lqdev.me/feed"&gt;feed&lt;/a&gt; or minor edits, I've been using &lt;a href="https://www.lqdev.me/notes/surface-duo-blogging-github-dev"&gt;github.dev&lt;/a&gt;. However, there's been times where I've needed to run and debug code to confirm that my changes work. This is where I hit some of the limitation of github.dev which means unless I set up a Codespace, I have to save my work and move offline to my PC. Codespaces are great, but given that Codespaces are nice to haves not a requirement for my workflow at this time, it didn't make sense for me to pay for them. That's why I was excited to learn that GitHub is providing up to &lt;a href="https://github.blog/2022-11-10-whats-new-with-codespaces-from-github-universe-2022/"&gt;60 hours per month of free Codespace usage&lt;/a&gt; to all developers. That's more than enough for me.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://cdn.lqdev.tech/files/images/net7-gh-codespaces.png" class="img-fluid" alt="Blog post authored in GitHub Codespaces with integrated terminal open" /&gt;&lt;/p&gt;
&lt;p&gt;By default, Codespace images come preinstalled with the .NET 6 SDK which makes sense considering it's the latest LTS. However, since my static site generator targets .NET 7, I had to configure my Codespace to use .NET 7. This was just as easy as upgrading to .NET 7. All I had to do was provide the .NET 7 SDK Docker image as part of my &lt;a href="https://github.com/lqdev/luisquintanilla.me/blob/main/.devcontainer.json"&gt;devcontainer.json&lt;/a&gt; configuration file. From there, Codespaces takes care of the rest. As a result, I can now run and debug my code all in one place without interrupting my workflow.&lt;/p&gt;
&lt;p&gt;PS: This post was authored in GitHub Codespaces 🙂&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/net7-devcontainer-gh-codespaces</link>
      <guid>https://www.lqdev.me/notes/net7-devcontainer-gh-codespaces</guid>
      <pubDate>2022-11-21 11:28 -05:00</pubDate>
      <category>dotnet</category>
      <category>github</category>
      <category>codespaces</category>
      <category>development</category>
      <category>devcontainer</category>
      <category>website</category>
      <category>internet</category>
    </item>
    <item>
      <title>Impressed by Azure Cloud Shell</title>
      <description>&lt;![CDATA[&lt;p&gt;I can probably count the number of times I've used the Azure Cloud Shell inside the Azure Portal with one hand. However, I recently had a need to use it and was impressed by the experience.&lt;/p&gt;
&lt;p&gt;I wanted to create an Azure Function app but didn't want to install all the tools required. Currently I'm running Manjaro on an ASUS L210MA. That's my daily driver and although the specs are incredibly low I like the portability. The Azure set of tools run on Linux but are easier to install on Debian distributions. The low specs plus the Arch distro didn't make me want to go through the process of installing the Azure tools locally for a throwaway app.&lt;/p&gt;
&lt;p&gt;That's when I thought about using Azure Cloud Shell. Setup was relatively quick and easy. It was also great to see the latest version of .NET, Azure Functions Core Tools, and Azure CLI were already installed. Even Git was already there. After creating the function app, I was able to make some light edits using the built-in Monaco editor. When I was ready to test the app, I was able to open up a port, create a proxy, and test the function locally. It's not a high-end machine so I won't be training machine learning models on it anytime soon. However, for experimentation and working with my Azure account it's more than I need.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/impressed-az-cloud-shell</link>
      <guid>https://www.lqdev.me/notes/impressed-az-cloud-shell</guid>
      <pubDate>2022-10-13 22:45 -05:00</pubDate>
      <category>azure</category>
      <category>cloud</category>
      <category>tools</category>
      <category>development</category>
    </item>
    <item>
      <title>TIL: VS Code Live Preview Extension</title>
      <description>&lt;![CDATA[&lt;p&gt;Today I Learned about the &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode.live-server"&gt;VS Code Live Preview extension&lt;/a&gt;. While reading the &lt;a href="https://code.visualstudio.com/updates/v1_71"&gt;release notes&lt;/a&gt; for the latest version of VS Code, I noticed a &lt;a href="https://code.visualstudio.com/updates/v1_71#_live-preview"&gt;note&lt;/a&gt; on the Live Preview extension. Usually to preview static content, I use Python's HTTP server with the &lt;code&gt;python -m http.server&lt;/code&gt; command. This extension simplifies the process because it's built into Visual Studio Code. If you do any kind of web development, it's definitely worth a look.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/til-vs-code-live-preview-extension</link>
      <guid>https://www.lqdev.me/notes/til-vs-code-live-preview-extension</guid>
      <pubDate>09/03/2022 18:47 -05:00</pubDate>
      <category>til</category>
      <category>vscode</category>
      <category>vs</category>
      <category>extension</category>
      <category>development</category>
      <category>web</category>
    </item>
    <item>
      <title>Operationalizing Machine Learning with ML.NET, Azure DevOps and Azure Container Instances</title>
      <description>&lt;![CDATA[

## Introduction

Azure DevOps, formerly known as Visual Studio Team Services (VSTS), helps individuals and organizations plan, collaborate and ship products faster. One if its noteworthy services, Azure Pipelines, helps developers build Continuous Integration (CI) and Continuous Delivery (CD) pipelines that automate and standardize the build, test and deploy phases of the software development process. In addition, Azure Pipelines provides native container support and works with any language, platform and cloud. Machine learning like software development is also a process that includes a build, test and deploy phase which makes it a good candidate for automation and standardization. At Build 2018, Microsoft announced [ML.NET](https://github.com/dotnet/machinelearning), an open-source, cross-plaform machine learning framework for .NET. If we were to put all of these tools and services together, it means that we can automate and standardize the training of a machine learning model built with ML.NET, package it into a Docker container and deploy it to Azure Container Instances (ACI). In this writeup, I will go through the process of building a CI/CD pipeline in Azure Devops that trains, packages and deploys an ML.NET machine learning model to predict which class an Iris flower belongs to using a variety of measurements. Source code for this project can be found at this [link](https://github.com/lqdev/mlnetazdevopssample).

## Prerequisites

- [Git](https://git-scm.com/)
- [GitHub Account](https://github.com/)
- [.NET Core SDK](https://www.microsoft.com/net/download)  
- [Azure Account](https://azure.microsoft.com/en-us/free/)

## The Application

Because the purpose of this post is to demonstrate the functionality of Azure Devops and not that of ML.NET, I'll start with a pre-built application. For some more information and detail into the functionality of ML.NET, check out the official documentation [page](https://docs.microsoft.com/en-us/dotnet/machine-learning/) as well as some of my previous posts: 

- [Serverless Machine Learning with ML.NET and Azure Functions](http://luisquintanilla.me/2018/08/21/serverless-machine-learning-mlnet-azure-functions/)
- [Deploy .NET Machine Learning Models with ML.NET, ASP.NET Core, Docker and Azure Container Instances](http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/).

The application used in this writeup contains three .NET Core projects within it. One is a class library which is what we'll use to wrap ML.NET functionality for training models as well as loading pre-trained models that will then be used to make predictions. Another is a .NET Core console application which references the class library to train and persist an ML.NET model. Finally, there's the ASP.NET Core Web API which also references the class library application to load the pre-trained model created by the console application and then makes predictions via HTTP. This application can be utilized and deployed standalone but in this writeup it will be packaged into a Docker image that will then be deployed to Azure Container Instances.  

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-1.png)

### Class Library

The class library can be found in the `MLModel` directory. The class library defines the observation and prediction data classes which can be found in the `IrisData.cs` and `IrisPrediction.cs` files respectively. Additionally, the `Model` class contains helper methods that train and save machine learning models, load pre-trained models and use these models to make predictions. 

### Console Application

In the solution directory we also have a console application in the `ModelTrainer` directory. This application references the class library in the `MLModel` directory to train and persist the machine learning model. 

### API

The `ModelApi` directory contains an ASP.NET Core Web API application that references the `MLModel` class library project to load the pre-trained model that is trained by the `ModelTrainer` console application and makes predictions via HTTP. The logic for making predictions can be found in the `PredictController.cs` class in the `Controllers` directory of the `ModelApi` application. 

## CI/CD Pipeline Flow

Conceptually, when the application is built and deployed manually, the machine learning model is defined and developed inside the `MLModel` class library. Once satisfied with the model, the class library is built. The console application which references the `MLModel` class library is built as well as run to train and persist a classification model in a file called `model.zip`. The `MLModel` class library is also referenced in the `ModelApi` ASP.NET Core project. Because `ModelApi` is the application we're looking to deploy in order to expose our pre-trained machine learning model, we need to find a way to package it for deployment. We'll be deploying `ModelApi` using Azure Container Instances which means we need to create a Docker image of the project that will then be pushed to a Docker registry where it will be made available for public consumption. The building of multiple projects as well as the building, publishing and deployment of the Docker image to Azure Container Instances can be standardized and automated using Azure DevOps. The rest of this write-up will focus on demonstrating step-by-step how to operationalize this machine learning application via CI/CD pipelines in Azure DevOps using Azure Pipelines.   

### Getting The Code

Before getting started, the first thing you'll want to do is fork the [mlnetazdevopssample](https://github.com/lqdev/mlnetazdevopssample) GitHub repository into your own GitHub account.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-2.png)

### Creating the Project

Navigate to [https://devops.azure.com](https://devops.azure.com), click `Start Free` and follow the prompts to either create a new account or sign into your existing account.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-3.png)

Once logged in, click `Create Project`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-4.png)

Enter the name of your project as well as a short description. Then, click `Create`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-5.png)

## The Continuous Integration (CI) Pipeline

Using Azure Pipelines, we'll configure a CI pipeline for the build and packaging steps of our application. Below is an illustration of all the steps involved in our CI pipeline:

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-6.png)

1. Build the class library application
2. Build the console application
3. Train and persist the ML.NET Model by running the console application.
4. Copy ML.NET model file created by console application into ASP.NET Core Web API application directory
5. Build ASP.NET Core Web API application
6. Build Docker image 
7. Push Docker image to Docker Hub

### CI Pipeline Setup

Once the project is created, in the main project page, hover over `Pipelines` and click on `Builds`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-7.png)

In the `Builds` pipeline page, click `New pipeline`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-8.png)

Select GitHub as the source and connect your GitHub account with Azure DevOps.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-9.png)

Once you have authorized Azure DevOps to use your GitHub account, select the repository and branch that will be used for this build pipeline. In our case, we'll be using the master branch of the `mlnetazdevopssample` repository. When finished configuring, click `Continue`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-10.png)

The next step will be to select the jobs to execute in our pipeline. Because there are multiple steps in this build pipeline, let's start with an `Empty Job` and customize it to our needs.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-11.png)

From inside the build pipeline page, before we start adding jobs, lets select the agent that will execute the jobs. For this pipeline, select the `Hosted Ubuntu 1604` option from the dropdown.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-12.png)

### 1. Build the Class Library Application

The first step in our CI Pipeline will be to build our class library which contains methods that wrap the training, loading and prediction functionality of the ML.NET framework and persisted models.

To achieve that, we'll add a .NET Core task to our `Agent Job 1`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-13.png)

Once added to the pipeline, let's configure this task. To make it more descriptive, we can give it a name such as `Build Class Library`. Because this task will be responsible for building the .NET Core class library, we'll leave the default `build` Command setting as is. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-14.png)

The other setting we'll want to configure is the `Working Directory`. We can do so by clicking the `Advanced` tab. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-15.png)

For this task we'll use the `MLModel` directory.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-16.png)

When finished with the configuration, click `Save &amp; Queue` -&gt; `Save` on the top toolbar.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-17.png)

Enter a detailed comment describing the change to the pipeline and click `Save`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-18.png)

### 2. Building The Console Application

Once we've built the class library application which we'll reference from the .NET Core console and ASP.NET Core Web API applications, it's time to build the console application which will serve the purpose of training and persisting the ML.NET model.

Similar to the previous step, add a new .NET Core *build* task to the pipeline. The only setting that will change for this task is the `Working Directory` which will have the value of `ModelTrainer`. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-19.png)

Although not required, when finished configuring the task, click `Save &amp; Queue` -&gt; `Save` to save and comment the changes to the pipeline. 

### 3. Train and persist the ML.NET Model

Now that our console application is built, it's time to run it in order to train and persist the ML.NET model. To do so, we'll add another .NET Core task. The difference is that the `Command` setting will now be configured with the `run` value. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-20.png)

The `Working Directory` will be set to `ModelTrainer` like in the previous task.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-21.png)

Remember to save and comment the new changes to the pipeline.

### 4. Copy ML.NET Model to Web API Directory

After the console application is run and the ML.NET model is trained, it is persisted in a file called `model.zip` inside the `ModelTrainer` directory. We can use this persisted version of the model to make predictions from both the console application or any other application of our choice. In this case, we'll be making predictions via an ASP.NET Core Web API. In order for our API to reference this file, we need to copy it into the root directory of our `ModelApi` directory. A way to perform that task is via bash script. To add a bash script to our pipeline, all we need to do is add a Bash task to it. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-22.png)

Once added to our pipeline, it's time to configure the task. We'll set the `Type` setting to `Inline` which will bring up a text box for us to type in the script. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-23.png)

Inside of the text box, enter the following content:

```bash
# Write your commands here

cp ../ModelTrainer/model.zip .

# Use the environment variables input below to pass secret variables to this script
```

This command will copy the `model.zip` file from the `ModelTrainer` directory to the `ModelApi` directory.

We can set the `Working Directory` of this step to `ModelApi`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-24.png)

Once finished, save and comment the new changes to the pipeline. 

### 5. Build ASP.NET Core Web API application

Now that we have the necessary files inside our `ModelApi` application, it's time to build it. We'll add a .NET Core task to our pipeline and set the `Command` to `build`. The `Working Directory` will be `ModelApi` like the previous task.

Save and comment the new changes to the pipeline when finished.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-25.png)

### 6. Build ASP.NET Core Web API Docker Image

The method of deployment for the ASP.NET Core Web API application is via containers. Therefore, after building the application, we have to build a Docker image for it that can then be pushed to a Docker registry of your choice. To build a Docker image, we'll add a Docker task to our pipeline.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-26.png)

When we configure the task, we'll start off by setting the `Container Registry Type` to `Container Registry`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-27.png)

This will prompt the setup of a service connection to a Docker registry if one does not already exist.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-28.png)

The Docker registry type we'll be using is Docker Hub. Give the connection a name, enter the credentials to your Docker Hub account and click `Verify this connection` to make sure that your credentials are valid and a connection can be established with Docker Hub. When finished click `OK`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-29.png)

The `Command` setting will be `build` so we can leave the default as is as well as the `Dockerfile` setting which will use the Dockerfile in the root `mlnetazdevopssample` directory.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-30.png)

Finally, we'll configure the `Image name` setting. The convention we'll use is `&lt;docker-hub-username&gt;/&lt;image-name&gt;`. In my case, `lqdev` is my Docker Hub username and I'll name my image `mlnetazdevopssample` resulting in `lqdev/mlnetazdevopssample`. Additionally, check the `Include latest tag` checkbox to have every build be the latest as opposed to tagging it with versions numbers. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-31.png)

Remember to save and comment the recent changes to the pipeline.

### 7. Push Docker Image to Docker Hub

The last step in our CI pipeline is to push our newly built image to Docker Hub. To do so we'll use anoter Docker task. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-32.png)

Like in the previous task, we'll set the `Container registry type` to `Container Registry`. Set the `Docker registry service connection` to the most recently created connection by selecting it from the dropdown. We'll be changing our `Command` to `push` and set the `Image name` to the name of the image built in the previous step. The naming convention is `&lt;docker-hub-username&gt;/&lt;image-name&gt;:latest`. The latest tag was added by our previous Docker build task so make sure that you include it in this task.

Once finished, click `Save &amp; Queue` -&gt; `Save &amp; Queue`. As opposed to only clicking `Save`, this action will manually trigger the CI pipeline.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-33.png)

Don't forget to comment your changes and click `Save &amp; queue` to kick off the CI pipeline.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-34.png)

### Monitoring the Build

When the build starts, you can click on `Builds` under the `Pipelines` section on the left pane.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-35.png)

Select the first build from the list to get more details on the build.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-36.png)

This will take you to the logs which show the status of the pipeline near real-time.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-37.png)

### Confirming CI Pipeline Success

If the build is successful, navigate to [https://hub.docker.com/](https://hub.docker.com/) to check whether the Docker image was pushed to the registry.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-38.png)

## The Continuous Delivery (CD) Pipeline

Now that we have our CI pipeline set up which will build and package our application, it's time to deploy it. We could do this ourselves or automate it using a CD pipeline. Our application wil be deployed to Azure Container Instances which is an Azure service that offers a quick way to run containers without having to worry about the management of virtual machines or orchestration services. The steps involved in our CD pipeline are the following:

1. Create Azure Resource Group for deployment
2. Deploy application to Azure Container Instances.

### CD Pipeline Setup

To get started setting up a CD pipeline, from the Azure DevOps project main page, hover over `Pipelines` and click on `Releases`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-39.png)

Once in that page, click on `New pipeline`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-40.png)

As with our CI pipeline, we'll start off with an `Empty Job` which we'll configure at a later time.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-41.png)

### Triggering Deployments

Once our pipeline is created, it's time to configure it. The first thing we'll want to do is add an artifact. An artifact can be a variety of things including the output of our build pipeline. In our case, the end our CI pipeline will be the trigger for our CD pipeline. To add an artifact, click `Add an artifact`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-42.png)

In the configuration form, set the `Source type` to `Build` and the `Source` to the name of the CI pipeline created in the previous steps. When finished, click `Add`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-43.png)


After configuring our artifact, it's time to configure the steps in the CD pipeline. To do so, click on the `Stage 1` option in the `Stages` section of the release pipeline page and change the name to something more descriptive.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-44.png)

When finished, close out the form and click on the hyperlink below the stage title. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-45.png)

You should now be on a page similar to the CI pipeline job configuration page. On this page, we'll want to click on the `Agent Job` panel to set the `Agent pool` setting to `Hosted Ubuntu 1604`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-46.png)

Once that is complete, it's time to configure the tasks in the CD pipeline.

### 1. Create Azure Resource Group

Start off adding an `Azure CLI` task to the pipeline. In this task we'll create a resource group in Azure to which we'll deploy our application to. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-47.png)

Before doing anything else, link DevOps to an Azure Subscription by selecting one from the dropdown and clicking `Authorize` which will prompt you to authenticate your subscription. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-48.png)

Once an Azure subscription has been linked, let's change the `Script Location` setting to `Inline Script`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-49.png)

In the `Inline Script` text box enter the following:

```bash
az group create --name mlnetazdevopssampleresourcegroup --location eastus
```
This script will create a resource group in Azure called `mlnetazdevopssampleresourcegroup` that is located in `eastus`. Both of these are configurable to your preference. 

### 2. Deploy Docker Image to Azure Container Instances

The next and final step in our CD pipeline is the deployment to Azure Container Instances. To deploy our application, we'll add another `Azure CLI` task. This time, since we already configured our `Azure subscription` in the previous task, we can select the service connection as opposed to a subscription from the dropdown.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-50.png)

Like in the previous task, our script will be inline. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-51.png)

In the `Inline Script` text box enter the following:

```bash
az container create --resource-group mlnetazdevopssampleresourcegroup --name mlnetcontainer --image lqdev/mlnetazdevopssample:latest --ports 80 --ip-address public
```

This script creates a container in the resource group created by the previous task of the pipeline with the name `mlnetcontainer` from the Docker image that was pushed to Docker Hub by the CI pipeline. Additionally, it opens up port 80 and assigns a publicly accessible IP address for the container to be accessed externally. 

Once this step has been configured, make sure to save and comment all your changes by clicking `Save`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-52.png)

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-53.png)

Then, to make it easily recognizable, edit the name of the pipeline by hovering near `New release pipeline` and clicking on the pencil icon.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-54.png)

Make sure to save and comment your changes.

## Automating CI/CD Pipelines

In the previous steps, we configured CI and CD pipelines. However, we have still not fully automated the triggers that kick off both of these. 

### CI Pipeline Trigger

First, lets start off by automating the CI pipeline. To do so, go the project's main page, hover over `Pipelines` and click on `Builds`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-55.png)

This will take you to the CI pipeline page. While on this page, click `Edit`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-56.png)

Then, click on `Triggers`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-57.png)

Once on this page, check the `Enable continous integration` checkbox and save and comment your changes by clicking `Save &amp; Queue` -&gt; `Save`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-58.png)


### CD Pipeline Trigger

To automate the CD pipeline trigger, click on `Releases` under the `Pipelines` page to automate the CD pipeline.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-59.png)

Once on the CD pipeline's page, click `Edit`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-60.png)

Then, click on the lightning icon in the Artifacts section which will show a configuration form. In this form, toggle the `Continuous deployment trigger` setting to `Enabled`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-61.png)

When finished, save and comment your changes. 

## Running CI/CD Pipelines

Although going forward builds and deployments will be started when new changes are checked into the master branch of the `mlnetazdevopssample` repository, for demonstration purposes we will manually kick off the CI/CD pipelines we have just configured. To do so, click on `Builds` under the `Pipelines` section on the left pane.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-62.png)

From the CI pipeline page click `Queue`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-63.png)

This will prompt a modal to show up in which you can just click `Queue` to start the build.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-64.png)

This will kick off a new CI build which subsequently will also kick off the CD pipeline of your application. 

## Testing The Deployment

If all is successful, a Docker image of an ASP.NET Core Web API application will be deployed to Azure Container Instances which can be accessed via a public IP address. 

To see whether the deployment worked, navigate to [https://portal.azure.com/](https://portal.azure.com/) and click on `Resource groups`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-65.png)

At this point, you should see the resource group that was created by the CD pipeline. If that's the case, click on it. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-66.png)

This will then show a page that displays the container that was deployed to this resource group. Click on that.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-67.png)

The container page will display diagnostic and configuration information about the container. The information we're interested in is the `IP address`. Hover to the right of it and click on the icon that says `Click to copy`. This will copy the address to the clipboard.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-68.png)

In an application like Postman or Insomnia, make an HTTP POST request to `http://&lt;ip-address&gt;/api/predict` where `ip-address` is the public IP address of the container in Azure with the following body.

```json
{
    "SepalLength":3.3,
    "SepalWidth":1.6,
    "PetalLength":0.2,
    "PetalWidth":5.1
}
```

If successful, the response will be `Iris-virginica`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-69.png)

## Conclusion

In this writeup, we operationalized the building, packaging and deployment of an ML.NET application that predicts the class of an Iris flower using a variety of mesurements with Azure DevOps. We created both a Continous Integration as well as a Continous Delivery pipeline which deploys the Docker image of an ASP.NET Core Web API to Azure Container Instances. Keep in mind this is just one way of doing it and Azure DevOps is flexible in how all of these tasks and workflows are configured to meet your requirements. Happy coding!

###### Resources

[ML.NET Samples](https://github.com/dotnet/machinelearning-samples)
[DevOps for Data Science](https://www.youtube.com/watch?v=bUTBBS1TECc)]]&gt;</description>
      <link>https://www.lqdev.me/posts/azdevops-mlnet-aci</link>
      <guid>https://www.lqdev.me/posts/azdevops-mlnet-aci</guid>
      <pubDate>2018-11-26 23:50:23 -05:00</pubDate>
      <category>mlnet</category>
      <category>machinelearning</category>
      <category>ai</category>
      <category>dotnet</category>
      <category>dotnetcore</category>
      <category>devops</category>
      <category>azure</category>
      <category>docker</category>
      <category>containers</category>
      <category>microsoft</category>
      <category>artificialintelligence</category>
      <category>programming</category>
      <category>webapi</category>
      <category>aci</category>
      <category>development</category>
    </item>
    <item>
      <title>Blogging Tools</title>
      <description>&lt;![CDATA[
## TLDR

These are the tools I use to write this blog:

- **Editor**: [Visual Studio Code](https://code.visualstudio.com/)
- **Markup**: [Markdown](https://daringfireball.net/projects/markdown/)
- **Static Site Generator**: [Hexo](https://hexo.io/) 
- **Blog Theme**: [Cactus](https://github.com/probberechts/hexo-theme-cactus)
- **Hosting**: [Namecheap](https://www.namecheap.com/)

## Introduction

I have gotten a few questions regarding what I use to write this blog so I thought it would be a good idea to write a post about it. In this writeup I'll discuss what I use for writing posts as well as building and hosting the site. 

## Writing

### Editor

For writing, I use Visual Studio Code. Visual Studio Code is an open source, cross-platform text editor. One of the nice features it includes is Markdown Preview. Markdown Preview allows you to see how the content would look when rendered as a web page. Additionally, Visual Studio Code allows you to split panes. What that provides is the ability to write and preview the content side-by-side. 

![](http://cdn.lqdev.tech/files/images/blog-tools-1.PNG)

### Markup 

I write my content in Markdown. Markdown is a plain text formatting syntax that allows authors to write easy-to-read and easy-to-write plain text formats that are then converted to HTML. It can take a while to get used to the syntax especially when including images, links and code snippets but once you get comfortable with it, it becomes natural and no different than writing sentences in plain text. While this is not a requirement for everyone, since a fair amount of my content includes code snippets, Markdown makes it easy to include such content in the language of choice. 

## Website

### Static Site Generation

While Markdown helps me format my plain text, at some point it needs to be converted into HTML or some format that can be rendered on the web. To help with that, I use Hexo. Hexo is an open source blog framework that allows you to write content using Markdown or other languages and generates static files which have a theme applied to them. Hexo is built on JavaScript, therefore you'll need NodeJS to configure it. 

### Theme 

Hexo has numerous themes that can be used to style your site. The one I use is the `Cactus` theme. 

## Hosting

For hosting, I use Namecheap. There is no special reason for this other than I had a slot available in an existing hosting plan. My method of deployment is via FTP. Since my site is all static assets, I upload them into a folder that is publicly accessible. With that in mind, by having my site made up of static assets I have the flexibility of hosting it on an S3 bucket or Azure Storage. This is the method I'm looking to use for deployments and hosting in the near future. Here is a [link](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-static-website) that details how to host a static site on Azure Storage.]]&gt;</description>
      <link>https://www.lqdev.me/posts/blog-tools</link>
      <guid>https://www.lqdev.me/posts/blog-tools</guid>
      <pubDate>2018-11-18 00:07:36 -05:00</pubDate>
      <category>markdown</category>
      <category>vscode</category>
      <category>tools</category>
      <category>development</category>
      <category>blogging</category>
    </item>
    <item>
      <title>Hacker News Vue Top Stories Client</title>
      <description>&lt;![CDATA[
# Introduction

Over the past few years, I've used AngularJS as my front-end framework for most web projects. With the release of Angular and React, I tried both of them and while both are great tools the learning took significantly longer because I also had to learn the bundler systems and in the case of Angular, TypeScript. Although the CLI tools go a long way towards automating a lot of the setup process, building an application from scratch in these frameworks is not intuitive especially for a newcomer. In the early days of Vue, I was able to attend a Meetup presentation by [Evan You](https://twitter.com/youyuxi) which went over the philosophy of Vue as well as its capabilities. I was instantly interested because of how easy it was to get started. Since that presentation, I have not had time to try it but with AngularJS being phased out, I felt it was about time to start exploring another framework. To get a better understanding of the framework's features prior to using CLI tools, I created a standalone application that relies only on NPM packages. In this writeup, I will create a Hacker News client with Vue that calls the Hacker News API and displays the top 50 stories. Source code for this project can be found in the following [repository](https://github.com/lqdev/hnvuedemo).

## Prerequisites

- [Node](https://nodejs.org/en/)

## Initialize Project

The first thing we want to do is create a directory for our project and initialize it with NPM

```bash
mkdir hnvuedemo
cd hnvuedemo
npm init -y
```

## Install Dependencies

Next, we want to install our dependencies. (Express is optional)

```bash
npm install --save vue axios express
```

## Create Server (Optional)

We can set up a server to host and serve our content. This is optional though because the application should still work when the `index.html` file is opened in the browser.

In our root project directory, we can create the `server.js` file and add the following content:

```javascript
var express = require("express");

const PORT = 3000 || process.env.PORT;

var app = express();

app.use(express.static("."));

app.get("/", (req, res) =&gt; {
  res.sendFile("index.html");
});

app.listen(PORT, () =&gt; {
  console.log(`Listening on port ${PORT}`);
});
```

## Create Application

### Create The View

Now it's time to create our application. Let's start by scaffolding the `index.html` which is where our application will be displayed.

```html
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;script src="./node_modules/vue/dist/vue.min.js"&gt;&lt;/script&gt;
    &lt;script src="./node_modules/axios/dist/axios.min.js"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;!--App Content Here--&gt;

    &lt;!--App Scripts Here--&gt;
&lt;/body&gt;
&lt;/html&gt;
```

We want to add the scripts to the `Vue` and `axios` packages in our `head` element. We'll hold off on the content for now.

### Get Data

Our data source for this project will be the Hacker News [API](https://github.com/HackerNews/API). To interact with it, we'll be using the `axios` NPM package. The data collection happens over several HTTP requests, therefore to help with it we'll be creating a file called `apihelpers.js` in our root project directory that will contain functions to get and manipulate the data.

#### Get Top Stories

The first thing we want to do is get a list of top stories. We can do so via the `/topstories` endpoint. The response of this request returns a list of ids which can then be used in conjunction with the `/item/{id}` endpoint to get the individual story data. We can then validate the story object to make sure it is a story as opposed to a user or job posting and return a list of all the story objects. The functions that will help us with that are `getIds`, `isStory` and `extractStories`.

```javascript
/**
 * Checks whether the item is a story
 * @param {Object} story - Story object
 */
function isStory(story) {
  return story.type == "story";
}

/**
 * Gets ids of stories
 * @param {string} url - Url to fetch story ids
 */
function getIds(url) {
  return axios.get(url);
}

/**
 * 
 * @param {Array&lt;Object&gt;} - List of resolved promises 
 */
function extractStories(...responses) {
    var stories = responses.map(story =&gt; story = story.data);
    return stories.filter(isStory);
}
```

### Create Vue App

Now that we have our helper methods to make calls to the API, we can create our application.

We can start by creating our Vue instance which is where our application will live. We can create a file called `app.js` in our root project directory and add the following content:

```javascript
//Components Go Here

new Vue({
  el: "#app",
  data: {
    title: "Hacker News Vue Simple Reader",
    loading: true,
    topStories: []
  },
  methods: {
    getTopStories: function() {
      getIds(
        "https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty"
      )
        .then(ids =&gt; buildRequest(ids.data.splice(0, 50))) //Only get first 50
        .then(getStories)
        .then(axios.spread(extractStories))
        .then(stories =&gt; {
          this.topStories = stories;
          this.loading = false;
        })
        .catch(e =&gt; console.log(e));
    }
  },
  created: function() {
    this.getTopStories();
  }
});
```

There's a lot happening here, so let's break it down based on the properties of the object passed to the Vue constructor.

The `el` property is like a selector which tells the application which element it should operate on. The value `#app` tells it to look for an element where the `id` attribute is `app`. This of course can be anything of your choosing.

The `data` property specifies the properties that contain the data of our application. These properties are reactive which means that whenever they change, that change is reflected in our view. As such, even if they have no value when the application starts, in order for them to automatically change when data is passed to them, they need to be declared in the `data` property. In our application, we have a `title` property which will be the title of our web page, `topStories` which is where we'll store the list of Hacker News stories and `loading` which we'll use to let us know when data is being loaded into our application.

In the `methods` property, we define functions that we want our application to use. In this case, I created the `getTopStories` method which chains together all the functions defined in our `apihelpers.js` file to return the top 50 stories on Hacker News.

Finally, Vue has instance lifecycle hooks. The `created` property defines what should happen when our instance is created. In our case, we want to call the `getTopProperties` method which is defined in our `methods` property to load the data, update our `topStories` data property and set `loading` to `false` because our data has loaded successfully.

### Add App to View

Now that we have created the logic of our application, it's time to add it to our view. We can do so by adding the `apihelpers.js` and `app.js` files to our `index.html` file via `script` elements.

```html
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;script src="./node_modules/vue/dist/vue.min.js"&gt;&lt;/script&gt;
    &lt;script src="./node_modules/axios/dist/axios.min.js"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;!--App Content Here--&gt;

    &lt;!--App Scripts Here--&gt;
    &lt;script src="apihelpers.js"&gt;&lt;/script&gt;
    &lt;script src="app.js"&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
```

### Display Data

Although we can now use our application logic inside of our view, we still can't see any of it because we have not added elements to display it. To view our application data, we can add the following code to our `index.html` file below the `&lt;!--App Content Here--&gt;` section.

```html
&lt;div id="app"&gt;
    &lt;h2&gt;{{title}}&lt;/h2&gt;
    &lt;h5 v-if="loading"&gt;Loading...&lt;/h5&gt;
    &lt;story v-for="story in topStories" :key="story.id" :story="story"&gt;&lt;/story&gt;
&lt;/div&gt;
```

The final `index.html` contents should look like the content below:

```html
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;script src="./node_modules/vue/dist/vue.min.js"&gt;&lt;/script&gt;
    &lt;script src="./node_modules/axios/dist/axios.min.js"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div id="app"&gt;
        &lt;h2&gt;{{title}}&lt;/h2&gt;
        &lt;h5 v-if="loading"&gt;Loading...&lt;/h5&gt;
        &lt;story v-for="story in topStories" :key="story.id" :story="story"&gt;&lt;/story&gt;
    &lt;/div&gt;
    &lt;script src="apihelpers.js"&gt;&lt;/script&gt;
    &lt;script src="app.js"&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
```

As mentioned earlier, we created an element, in this case a `div` that has the `id` attribute with a value of `app`. This is how our application knows where to display our content. Inside of an `h2` element, we display our `title` data property. Below it, we have an `h5` element that displays the text "Loading...". However, this is to only be displayed when our `loading` data property is `true`. If not, it should not be visible. We can achieve this conditional rendering via the `v-if` directive. This directive evaluates the expression inside of it and renders content based on its truthiness. 

Finally, there's one last piece that looks like an element, but not one of the built-in HTML elements. So then, what is it? It's a component. The Vue website defines a component as "...a reusable Vue instance with a name: in this case, `&lt;story&gt;`. We can use this component as a custom element inside a root Vue instance...". In our component, the `v-for` directive is what it sounds like. It creates a sequence of `story` components based on a list of objects defined in our `Vue` instance's `data` property. Our `story` component iterates over the `topStories` data property and assigns the value of the individual object in the list to the variable `story`. We bind the `id` property of the `story` object to the `key` attribute of the component and pass in the entire object to the component via the `story` prop. A prop is a custom attribute that you can register for the component. We can use props to pass data into the component. In all cases, the `:` prefix on the attributes and props of the component are shorthand for the `v-bind` directive which dynamically binds an expression to an attribute or component prop.

With all that being said your next question might be, how does the view know about this component? The answer is it doesn't at least not until you define it which is what we'll do next in our `app.js` file. In order for our `story` component to be usable, we need to define it above the instantiation of our `Vue` instance. The definition looks like the following:

```javascript
Vue.component('story',{
    props: ['story'],
    template: `
        &lt;div&gt;
            &lt;h3&gt;&lt;a :href="story.url" target="_blank"&gt;{{story.title}}&lt;/a&gt;&lt;/h3&gt;
        &lt;/div&gt;
    `
});
```

Like our `Vue` instance, let's unpack what's happening here. The first parameter is a string with the name of our component. The `props` property is a list of the props or custom attributes that are accepted by our component. The `template` property is where we set up the template of what will be rendered in place of our component in the view. In our case, we'll have an `h3` element with a nested `a` element whose `href` attribute is the `url` property of our `story` object and the display text is the `title` property of our `story` object.

## Run Application

At this point, our application should be ready to run. You can either start the server with the following command `npm start`  and navigate to `http://localhost:3000` or open the `index.html` page in the browser of your choice. The result should look like the screenshot below:

![](http://cdn.lqdev.tech/files/images/hnvueclient.png)

## Conclusion

In this writeup, I built a standalone Hacker News client that displays the top 50 stories using Vue while also highlighting some of its main features. Overall, I really enjoyed building this application. The setup process was extremely simple and after a few hours looking through the excellent documentation and working through some bugs, it took less than two hours to get this application up and running from start to finish. For prototyping and learning purposes, Vue is great because you're able to take advantage of the core features of the framework without having too much overhead. Although this may not be the most appropriate way to build production-ready applications, it's nice to know you can have a modern web application with minimal setup required. My next steps will be to continue learning some of the other features the framework provides and eventually build up to learning how to use the CLI tools.
]]&gt;</description>
      <link>https://www.lqdev.me/posts/hn-vue-topstories-client</link>
      <guid>https://www.lqdev.me/posts/hn-vue-topstories-client</guid>
      <pubDate>2018-06-21 18:30:38 -05:00</pubDate>
      <category>vue</category>
      <category>web development</category>
      <category>programming</category>
      <category>api</category>
      <category>development</category>
      <category>frontend</category>
      <category>js</category>
    </item>
    <item>
      <title>Classification with F# ML.NET Models</title>
      <description>&lt;![CDATA[
# Introduction

In a previous [post](http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/), I detailed how to build and deploy C# `ML.NET` models with `Docker` and `ASP.NET Core`. With inspiration from [Jeff Fritz](https://twitter.com/csharpfritz), I have been learning F# for the past week and a half or so. When trying to think of projects to start practicing my F#, porting over the code I had built in C# naturally came to mind. After overcoming many obstacles and with much guidance from [Alan Ball](https://github.com/voronoipotato) and [Isaac Abraham](https://twitter.com/isaac_abraham) whose F# [book](https://www.amazon.com/Get-Programming-guide-NET-developers/dp/1617293997/ref=sr_1_1?ie=UTF8&amp;qid=1528929802&amp;sr=8-1&amp;keywords=get+programming+with+F%23) I highly recommend, I was able to successfully port over the main parts of my code which highlight `ML.NET` functionality. In this writeup, I will port a C# `ML.NET` classification model to F# which predicts the type of flower based on four numerical measurement inputs. I tried to keep the organization of this post nearly identical to that of the C# article where possible. Sample code for this project can be found at the following [link](https://github.com/lqdev/fsmlnetdemo).

## Prerequisites

This project was built on a Linux PC but should work cross-platform on Mac and Windows.

- [.NET Core SDK 2.0+](https://www.microsoft.com/net/download/linux)
- [Ionide Extension - Option 1](https://fsharp.org/use/linux/)
- [ML.NET v 0.2.0](https://www.nuget.org/packages/Microsoft.ML/)

## Setting Up The Project

The first thing we want to do is create a folder for our solution.

```bash
mkdir fsharpmlnetdemo
```

Then, we want to create a solution inside our newly created folder.

```bash
cd fsharpmlnetdemo
dotnet new sln
```

## Building The Model

### Setting Up The Model Project

First, we want to create the project. From the solution folder enter:

```bash
dotnet new console -o model -lang f#
```

Now we want to add this new project to our solution.

```bash
dotnet sln add model/model.fsproj
```

### Adding Dependencies

Since we’ll be using the `ML.NET` framework, we need to add it to our `model` project.

```bash
dotnet add model/model.fsproj package Microsoft.ML
```

### Download The Data

Before we start training the model, we need to download the data we’ll be using to train. We do so by downloading the data file into our root solution directory.

```bash
curl -o iris-data.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
```

If we take a look at the data file, it should look something like this:

```bash
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa
5.0,3.4,1.5,0.2,Iris-setosa
4.4,2.9,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
```

### Train Model

Now that we have all our dependencies set up, it’s time to build our model. I leveraged the demo that is used on the `ML.NET` Getting-Started [website](https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet/get-started/linux/ubuntu16-04).

#### Defining Data Structures

At the time of this writing, `ML.NET` version `0.2.0` does not fully support F# Records. A workaround for this are mutable classes. Not really inline with F# paradigms, but it should be good enough.

In the `Program.fs` file of our `model` project directory, let’s create two mutable classes called `IrisData` and `IrisPrediction` which will define our features and predicted attribute respectively. Both of them will use `Microsoft.ML.Runtime.Api` to add the property attributes.

Here is what our `IrisData` class looks like:

```fsharp
type IrisData() =
    [&lt;Column(ordinal = "0");DefaultValue&gt;]
    val mutable public SepalLength: float32

    [&lt;Column(ordinal = "1");DefaultValue&gt;]
    val mutable public SepalWidth: float32

    [&lt;Column(ordinal = "2");DefaultValue&gt;]
    val mutable public PetalLength:float32

    [&lt;Column(ordinal = "3");DefaultValue&gt;]
    val mutable public PetalWidth:float32

    [&lt;Column(ordinal = "4",name="Label");DefaultValue&gt;]
    val mutable public Label: string
```

Similarly, here is the `IrisPrediction` class:

```fsharp
type IrisPrediction() =
    [&lt;ColumnName "PredictedLabel";DefaultValue&gt;] val mutable public PredictedLabel : string
```

#### Building Training Pipeline

The way the `ML.NET` computations process is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. We can add that logic inside the `main` function of our `Program.fs` file.

```fsharp
let dataPath = "./iris-data.txt"

// Initialize Compute Graph
let pipeline = new LearningPipeline()

// Load Data
pipeline.Add((new TextLoader(dataPath).CreateFrom&lt;IrisData&gt;separator=','))

// Transform Data
// Assign numeric values to text in the "Label" column, because
// only numbers can be processed during model training
pipeline.Add(new Transforms.Dictionarizer("Label"))

// Vectorize Features
pipeline.Add(new ColumnConcatenator("Features","SepalLength", "SepalWidth", "PetalLength", "PetalWidth"))

// Add Learner
pipeline.Add(new StochasticDualCoordinateAscentClassifier())

// Convert Label back to text
pipeline.Add(new ransforms.PredictedLabelColumnOriginalValueConverterPredictedLabelColumn = "PredictedLabel"))

//Train the model
let model = pipeline.Train&lt;IrisData, IrisPrediction&gt;()
```

#### Testing Our Model

Now that we have our data structures and model trained, it’s time to test it to make sure it's working. Following our training operation, we can add the following code.

```fsharp
// Test data for prediction
let testInstance = IrisData()
testInstance.SepalLength &lt;- 3.3f
testInstance.SepalWidth &lt;- 1.6f
testInstance.PetalLength &lt;- 0.2f
testInstance.PetalWidth &lt;- 5.1f

//Get Prediction
let prediction = model.Predict(testInstance)

//Output Prediction
printfn "Predicted flower type is: %s" prediction.PredictedLabel
```

Our final `Program.fs` file should contain content similar to that below:

```fsharp
open System
open Microsoft.ML
open Microsoft.ML.Runtime
open Microsoft.ML.Runtime.Api
open Microsoft.ML.Data
open Microsoft.ML.Transforms
open Microsoft.ML.Trainers

type IrisData() =

    [&lt;Column(ordinal = "0");DefaultValue&gt;] val mutable public SepalLength: float32
    [&lt;Column(ordinal = "1");DefaultValue&gt;] val mutable public SepalWidth: float32
    [&lt;Column(ordinal = "2");DefaultValue&gt;] val mutable public PetalLength:float32
    [&lt;Column(ordinal = "3");DefaultValue&gt;] val mutable public PetalWidth:float32
    [&lt;Column(ordinal = "4",name="Label");DefaultValue&gt;] val mutable public Label: string


type IrisPrediction() =

    [&lt;ColumnName "PredictedLabel";DefaultValue&gt;] val mutable public PredictedLabel : string

[&lt;EntryPoint&gt;]
let main argv =

    let dataPath = "./iris-data.txt"

    // Initialize Compute Graph
    let pipeline = new LearningPipeline()

    // Load Data
    pipeline.Add((new TextLoader(dataPath)).CreateFrom&lt;IrisData&gt;(separator=','))

    // Transform Data
    // Assign numeric values to text in the "Label" column, because
    // only numbers can be processed during model training
    pipeline.Add(new Transforms.Dictionarizer("Label"))

    // Vectorize Features
    pipeline.Add(new ColumnConcatenator("Features","SepalLength", "SepalWidth", "PetalLength", "PetalWidth"))

    // Add Learner
    pipeline.Add(new StochasticDualCoordinateAscentClassifier())

    // Convert Label back to text
    pipeline.Add(new Transforms.PredictedLabelColumnOriginalValueConverter(PredictedLabelColumn = "PredictedLabel"))

    //Train the model
    let model = pipeline.Train&lt;IrisData, IrisPrediction&gt;()

    // Test data for prediction
    let testInstance = IrisData()
    testInstance.SepalLength &lt;- 3.3f
    testInstance.SepalWidth &lt;- 1.6f
    testInstance.PetalLength &lt;- 0.2f
    testInstance.PetalWidth &lt;- 5.1f

    //Get Prediction
    let prediction = model.Predict(testInstance)

    //Output Prediction
    printfn "Predicted flower type is: %s" prediction.PredictedLabel
    0 // return an integer exit code
```

All set to run. We can do so by entering the following command from our solution directory:

```fsharp
dotnet run -p model/model.fsproj
```

Once the application has been run, the following output should display on the console.

```bash
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Using 2 threads to train.
Automatically choosing a check frequency of 2.
Auto-tuning parameters: maxIterations = 9998.
Auto-tuning parameters: L2 = 2.667734E-05.Auto-tuning parameters: L1Threshold (L1/L2) = 0.
Using best model from iteration 1066.Not training a calibrator because it is not needed.
Predicted flower type is: Iris-virginica
```

## Conclusion

In this post, we ported over a C# `ML.NET` classification model to F# which predicts the class of flower based on numerical measurement inputs. While several workarounds needed to be made, `ML.NET` is still in its infancy. As more people become involved and provide feedback hopefully in the near future, F# support and functionality will become more stable. Happy coding!
]]&gt;</description>
      <link>https://www.lqdev.me/posts/mlnet-classification-fsharp</link>
      <guid>https://www.lqdev.me/posts/mlnet-classification-fsharp</guid>
      <pubDate>2018-06-13 18:19:05 -05:00</pubDate>
      <category>fsharp</category>
      <category>dotnet</category>
      <category>dotnetcore</category>
      <category>programming</category>
      <category>development</category>
      <category>mlnet</category>
      <category>machinelearning</category>
      <category>artificialintelligence</category>
      <category>functionalprogramming</category>
    </item>
    <item>
      <title>Organizing F# Modules Into Separate Files</title>
      <description>&lt;![CDATA[
# Introduction

After watching [Jeff Fritz's](https://twitter.com/csharpfritz) F# Friday [Stream](https://www.twitch.tv/videos/268107540), I was inspired to do some tinkering of my own. I have looked at F# in the past, but often times have not done much with it. After the stream, I decided to keep plugging away at it and one of the first things I thought about doing was organizing my code into separate files. As simple as it may seem, it was no easy task and while there are many different ways of doing it, the method described in this writeup is one of the easiest I found. Keep in mind though that this may not be the appropriate way of doing it although it seems to work nicely for our purposes. In this solution I will create a set of mathematical operations that will be organized into a single module that will then be imported and used inside of my console application. Source code for this sample solution can be found at this [link](https://github.com/lqdev/fsharpmoduledemo).

## Prerequisites

This solution was built on a Mac, but should work on both Windows and Linux environments.

- [.NET Core SDK](https://www.microsoft.com/net/download/macos)
- [Ionide Extension - Option 1](https://fsharp.org/use/mac/)

## Create Project

Once you have everything installed, you can create a new F# project by entering the following command into the console.

```bash
dotnet new console -lang f#
```

## Create Module

The goal of this solution will be to create functions that add, subtract, multiply and divide two numbers. Since all of these functions are mathematical operations, we can organize them inside a single module. Therefore, we can create a new file inside of our project called `Math.fs` which will contain all of our functions.

```fsharp
module Math
    let add x y = x + y
    let subtract x y = x - y
    let multiply x y = x * y
    let divide x y = x / y
```

## Compile File

Now that our file has been created, we need to add it to our list of files to be compiled by our project. To do so, we can add an entry for our `Math.fs` file to the `ItemGroup` property inside our `fsproj` file. The contents of the file should look as follows after the addition:

```xml
&lt;Project Sdk="Microsoft.NET.Sdk"&gt;

  &lt;PropertyGroup&gt;
    &lt;OutputType&gt;Exe&lt;/OutputType&gt;
    &lt;TargetFramework&gt;netcoreapp2.0&lt;/TargetFramework&gt;
  &lt;/PropertyGroup&gt;

  &lt;ItemGroup&gt;
    &lt;Compile Include="Math.fs" /&gt;
    &lt;Compile Include="Program.fs" /&gt;
  &lt;/ItemGroup&gt;

&lt;/Project&gt;
```

## Import Module Into Console Application

Once we have our module created and our `fsproj` file configured, we can call the module from our console application. To do so, we need to first import it into our `Program.fs` file.

```fsharp
open Math
```

Once imported, we can call the `Math` module functions from our `main` function in the `Program.fs` file.

```fsharp
    printfn "Add: %i" (add 1 2)
    printfn "Subtract: %i" (subtract 1 2)
    printfn "Multiply: %i" (multiply 1 2)
    printfn "Divide: %i" (divide 4 2)
```

## Run Application

Now that everything is set up, we can build and run our console application with the following commands:

```bash
dotnet build
dotnet run
```

The output should be the following:

```bash
Add: 3
Subtract: -1
Multiply: 2
Divide: 2
```

## Conclusion

In this writeup, I went over how to create an F# console application that organizes a set of mathematical functions into a module whose contents are stored in a separate file. This is good for organization and to reduce clutter. Happy coding!


]]&gt;</description>
      <link>https://www.lqdev.me/posts/fsharpmodulesample</link>
      <guid>https://www.lqdev.me/posts/fsharpmodulesample</guid>
      <pubDate>2018-06-05 20:56:52 -05:00</pubDate>
      <category>dotnet</category>
      <category>fsharp</category>
      <category>programming</category>
      <category>development</category>
      <category>functionalprogramming</category>
      <category>dotnetcore</category>
    </item>
    <item>
      <title>Deploy .NET Machine Learning Models with ML.NET, ASP.NET Core, Docker and Azure Container Instances</title>
      <description>&lt;![CDATA[

# Introduction

Leading up to and during MS Build 2018 Microsoft has released a wide range of products that reduce the complexity that comes with building and deploying software. The focus this year was on Machine Learning and Artificial Intelligence. Some of the products I found particularly interesting are [Azure Container Instances](https://azure.microsoft.com/en-us/services/container-instances/) which makes it easier to run containerized applications without provisioning or managing servers and [ML.NET](https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet) which is a .NET cross-platform machine learning framework. In this writeup, I will make use of both these products by creating a machine learning classification model with `ML.NET`, exposing it via an ASP.NET Core Web API, packaging it into a Docker container and deploying it to the cloud via Azure Container Instances. Source code for this project can be found [here](https://github.com/lqdev/mlnetacidemo).

## Prerequisites

This writeup assumes that you have some familiarity with Docker. The following software/dependencies are also required to build and deploy the sample application. It's important to note the application was built on a Ubuntu 16.04 PC, but all the software is cross-platform and should work on any environment.

- [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/)
- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest)
- [.NET Core 2.0](https://www.microsoft.com/net/download/linux)
- [Docker Hub Account](https://hub.docker.com/)

## Setting Up The Project

The first thing we want to do is create a folder for our solution.

```bash
mkdir mlnetacidemo
```

Then, we want to create a solution inside our newly created folder.

```bash
cd mlnetacidemo
dotnet new sln
```

## Building The Model

Inside our solution folder, we want to create a new console application which is where we'll build and test our machine learning model.

### Setting Up the Model Project

First, we want to create the project. From the solution folder enter:

```bash
dotnet new console -o model
```

Now we want to add this new project to our solution.

```bash
dotnet sln mlnetacidemo.sln add model/model.csproj
```

### Adding Dependencies

Since we'll be using the `ML.NET` framework, we need to add it to our `model` project.

```bash
cd model
dotnet add package Microsoft.ML
dotnet restore
```

### Download The Data

Before we start training the model, we need to download the data we'll be using to train. We do so by creating a directory called `data` and downloading the data file onto there.

```bash
mkdir data
curl -o data/iris.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
```

If we take a look at the data file, it should look something like this:

```text
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa
5.0,3.4,1.5,0.2,Iris-setosa
4.4,2.9,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
```

### Train Model

Now that we have all our dependencies set up, it's time to build our model. I leveraged the demo that is used on the [ML.NET Getting-Started website](https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet/get-started/linux/ubuntu16-04).

#### Defining Data Structures

In the root directory of our `model` project, let's create two classes called `IrisData` and `IrisPrediction` which will define our features and predicted attribute respectively. Both of them will use `Microsoft.ML.Runtime.Api` to add the property attributes.

Here is what our `IrisData` class looks like: 
```csharp
using Microsoft.ML.Runtime.Api;

namespace model
{
public class IrisData
    {
        [Column("0")]
        public float SepalLength;

        [Column("1")]
        public float SepalWidth;

        [Column("2")]
        public float PetalLength;
        
        [Column("3")]
        public float PetalWidth;

        [Column("4")]
        [ColumnName("Label")]
        public string Label;
    }       
}
```

Similarly, here is the `IrisPrediction` class:

```csharp
using Microsoft.ML.Runtime.Api;

namespace model
{
    public class IrisPrediction
    {
        [ColumnName("PredictedLabel")]
        public string PredictedLabels;
    }
}
```

#### Building Training Pipeline

The way the `ML.NET` computations process is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. Therefore, we can create a class called `Model` to perform all of these tasks for us.

```csharp
using Microsoft.ML.Data;
using Microsoft.ML;
using Microsoft.ML.Runtime.Api;
using Microsoft.ML.Trainers;
using Microsoft.ML.Transforms;
using Microsoft.ML.Models;
using System;
using System.Threading.Tasks;

namespace model
{
    class Model
    {
        
        public static async Task&lt;PredictionModel&lt;IrisData,IrisPrediction&gt;&gt; Train(LearningPipeline pipeline, string dataPath, string modelPath)
        {
            // Load Data
            pipeline.Add(new TextLoader(dataPath).CreateFrom&lt;IrisData&gt;(separator:',')); 

            // Transform Data
            // Assign numeric values to text in the "Label" column, because 
            // only numbers can be processed during model training   
            pipeline.Add(new Dictionarizer("Label"));

            // Vectorize Features
            pipeline.Add(new ColumnConcatenator("Features", "SepalLength", "SepalWidth", "PetalLength", "PetalWidth"));

            // Add Learner
            pipeline.Add(new StochasticDualCoordinateAscentClassifier());

            // Convert Label back to text 
            pipeline.Add(new PredictedLabelColumnOriginalValueConverter() {PredictedLabelColumn = "PredictedLabel"});

            // Train Model
            var model = pipeline.Train&lt;IrisData,IrisPrediction&gt;();

            // Persist Model
            await model.WriteAsync(modelPath);

            return model;
        }
    }
}
```

In addition to building our pipeline and training our machine learning model, the `Model` class also serialized and persisted the model for future use in a file called `model.zip`.

#### Testing Our Model

Now that we have our data structures and model training pipeline set up, it's time to test everything to make sure it's working. We'll put our logic inside of our `Program.cs` file.

```csharp
using System;
using Microsoft.ML;

namespace model
{
    class Program
    {
        static void Main(string[] args)
        {

            string dataPath = "model/data/iris.txt";

            string modelPath = "model/model.zip";

            var model = Model.Train(new LearningPipeline(),dataPath,modelPath).Result;

            // Test data for prediction
            var prediction = model.Predict(new IrisData() 
            {
                SepalLength = 3.3f,
                SepalWidth = 1.6f,
                PetalLength = 0.2f,
                PetalWidth = 5.1f
            });

            Console.WriteLine($"Predicted flower type is: {prediction.PredictedLabels}");
        }
    }
}
```

All set to run. We can do so by entering the following command from our solution directory:

```bash
dotnet run -p model/model.csproj
```

Once the application has been run, the following output should display on the console. 

```text
Automatically adding a MinMax normalization transform, use 'norm=Warn' or
'norm=No' to turn this behavior off.Using 2 threads to train.
Automatically choosing a check frequency of 2.Auto-tuning parameters: maxIterations = 9998.
Auto-tuning parameters: L2 = 2.667734E-05.
Auto-tuning parameters: L1Threshold (L1/L2) = 0.Using best model from iteration 882.
Not training a calibrator because it is not needed.
Predicted flower type is: Iris-virginica
```

Additionally, you'll notice that a file called `model.zip` was created in the root directory of our `model` project. This persisted model can now be used outside of our application to make predictions, which is what we'll do next via an API.

## Exposing The Model

Once a machine learning model is built, you want to deploy it so it can start making predictions. One way to do that is via a REST API. At it's core, all our API needs to do is accept data input from the client and respond back with a prediction. To help us do that, we'll be using an ASP.NET Core API.

### Setting Up The API Project

The first thing we want to do is create the project.

```bash
dotnet new webapi -o api
```

Then we want to add this new project to our solution

```bash
dotnet sln mlnetacidemo.sln add api/api.csproj
```

### Adding Dependencies

Because we'll be loading our model and making predictions via our API, we need to add the `ML.NET` package to our `api` project.

```bash
cd api
dotnet add package Microsoft.ML
dotnet restore
```

### Referencing Our Model

In the previous step when we built our machine learning model, it was saved to a file called `model.zip`. This is the file we'll be referencing in our API to help us make predictions. To reference it in our API, simply copy it from the model project directory into our `api` project directory.

### Creating Data Models

Our model was built using data structures `IrisData` and `IrisPrediction` to define the features as well as the predicted attribute. Therefore, when our model makes predictions via our API, it needs to reference these data types as well. As a result, we need to define `IrisData` and `IrisPrediction` classes inside of our `api` project. The contents of the classes will be nearly identical to those in the `model` project with the only exception of our namespace changing from `model` to `api`.

```csharp
using Microsoft.ML.Runtime.Api;

namespace api
{
    public class IrisData
    {
        [Column("0")]
        public float SepalLength;

        [Column("1")]
        public float SepalWidth;

        [Column("2")]
        public float PetalLength;
        
        [Column("3")]
        public float PetalWidth;

        [Column("4")]
        [ColumnName("Label")]
        public string Label;
    }    
}
```

```csharp
using Microsoft.ML.Runtime.Api;

namespace api
{
    public class IrisPrediction
    {
        [ColumnName("PredictedLabel")]
        public string PredictedLabels;
    }
}
```

### Building Endpoints

Now that our project is set up, it's time to add a controller that will handle prediction requests from the client. In the `Controllers` directory of our `api` project we can create a new class called `PredictController` with a single `POST` endpoint. The contents of the file should look like the code below:

```csharp
using System;
using System.Collections.Generic;
using System.Linq;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Mvc;
using Microsoft.ML;

namespace api.Controllers
{
    [Route("api/[controller]")]
    public class PredictController : Controller
    {
        // POST api/predict
        [HttpPost]
        public string Post([FromBody] IrisData instance)
        {
            var model = PredictionModel.ReadAsync&lt;IrisData,IrisPrediction&gt;("model.zip").Result;
            var prediction = model.Predict(instance);
            return prediction.PredictedLabels;
        }
    }
}
```

### Testing The API

Once our `predict` endpoint is set up, it's time to test it. From the root directory of our `mlnetacidemo` solution, enter the following command.

```bash
dotnet run -p api/api.csproj
```

In a client like POSTMAN or Insomnia, send an HHTP POST request to the endpoint `http://localhost:5000/api/predict`.

The body our request should look similar to the snippet below:

```json
{
	"SepalLength": 3.3,
	"SepalWidth": 1.6,
	"PetalLength": 0.2,
	"PetalWidth": 5.1,
}
```

If successful, the output returned should equal `Iris-virginica` just like our console application.

## Packaging The Application

Great! Now that our application is successfully running locally, it's time to package it up into a Docker container and push it to Docker Hub.

### Creating The Dockerfile

In our `mlnetacidemo` solution directory, create a `Dockerfile` with the following content:

```Dockerfile
FROM microsoft/dotnet:2.0-sdk AS build
WORKDIR /app

# copy csproj and restore as distinct layers
COPY *.sln .
COPY api/*.csproj ./api/
RUN dotnet restore

# copy everything else and build app
COPY api/. ./api/
WORKDIR /app/api
RUN dotnet publish -c release -o out


FROM microsoft/aspnetcore:2.0 AS runtime
WORKDIR /app
COPY api/model.zip .
COPY --from=build /app/api/out ./
ENTRYPOINT ["dotnet", "api.dll"]
```

### Building Our Image

To build the image, we need to enter the following command into the command prompt. This make take a while because it needs to download the .NET Core SDK and ASP.NET Core runtime Docker images.

```bash
docker build -t &lt;DOCKERUSERNAME&gt;/&lt;IMAGENAME&gt;:latest .
```

### Test Image Locally

We need to test our image locally to make sure it can run on the cloud. To do so, we can use the `docker run` command. 

```bash
docker run -d -p 5000:80 &lt;DOCKERUSERNAME&gt;/&lt;IMAGENAME&gt;:latest
```

Although the API is exposing port 80, we bind it to the local port 5000 just to keep our prior API request intact. When sending a POST request to `http://localhost:5000/api/predict` with the appropriate body, the response should again equal `Iris-virginica`.

To stop the container, use `Ctrl + C`.

### Push to Docker Hub

Now that the Docker image is successfully running locally, it's time to push to Docker Hub. Again, we use the Docker CLI to do this.

```bash
docker login
docker push &lt;DOCKERUSERNAME&gt;/&lt;IMAGENAME&gt;:latest
```

## Deploying To The Cloud

Now comes the final step which is to deploy and expose our machine learning model and API to the world. Our deployment will occur via Azure Container Instances because it requires almost no provisioning or management of servers. 

### Prepare Deployment Manifest

Although deployments can be performed inline in the command line, it's usually best to place all the configurations in a file for documentation and to save time not having to type in the parameters every time. With Azure, we can do that via a JSON file. 

```json
{
  "$schema":
    "https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
  "contentVersion": "1.0.0.0",
  "parameters": {
    "containerGroupName": {
      "type": "string",
      "defaultValue": "mlnetacicontainergroup",
      "metadata": {
        "description": "Container Group name."
      }
    }
  },
  "variables": {
    "containername": "mlnetacidemo",
    "containerimage": "&lt;DOCKERUSERNAME&gt;/&lt;IMAGENAME&gt;:latest"
  },
  "resources": [
    {
      "name": "[parameters('containerGroupName')]",
      "type": "Microsoft.ContainerInstance/containerGroups",
      "apiVersion": "2018-04-01",
      "location": "[resourceGroup().location]",
      "properties": {
        "containers": [
          {
            "name": "[variables('containername')]",
            "properties": {
              "image": "[variables('containerimage')]",
              "resources": {
                "requests": {
                  "cpu": 1,
                  "memoryInGb": 1.5
                }
              },
              "ports": [
                {
                  "port": 80
                }
              ]
            }
          }
        ],
        "osType": "Linux",
        "ipAddress": {
          "type": "Public",
          "ports": [
            {
              "protocol": "tcp",
              "port": "80"
            }
          ]
        }
      }
    }
  ],
  "outputs": {
    "containerIPv4Address": {
      "type": "string",
      "value":
        "[reference(resourceId('Microsoft.ContainerInstance/containerGroups/', parameters('containerGroupName'))).ipAddress.ip]"
    }
  }
}
```

It's a lot to look at but for now we can use this template and save it to the file `azuredeploy.json` in the root directory of our `mlnetacidemo` solution. The only thing that needs to be changed is the value of the `containerimage` property. Replace it with your Docker Hub username and the name of the image you just pushed to Docker Hub.

### Deploy

In order to deploy our application we need to make sure to log into our Azure account. To do so via the Azure CLI, type into the command prompt:

```bash
az login
```

Follow the prompts to log in. Once logged in, it's time to create a resource group for our container.

```bash
az group create --name mlnetacidemogroup --location eastus
```

After the group has been successfully created it's time to deploy our application.

```bash
az group deployment create --resource-group mlnetacidemogroup --template-file azuredeploy.json
```

Give it a few minutes for your deployment to initialize. If the deployment was successful, you should see some output on the command line. Look for the `ContainerIPv4Address` property. This is the IP Address where your container is accessible. In POSTMAN or Insomnia, replace the URL to which you previously made a POST request to with `http://&lt;ContainerIPv4Address&gt;/api/predict` where `ContainerIPv4Address` is the value that was returned to the command line after the deployment. If successful, the response should be just like previous requests `Iris-virginica`.

Once you're finished, you can clean up resources with the following command:

```bash
az group delete --name mlnetacidemogroup
```

## Conclusion

In this writeup, we built a classification machine learning model using `ML.NET` that predicts the class of an iris plant given four measurement features, exposed it via an ASP.NET Core REST API, packaged it into a container and deployed it to the cloud using Azure Container Instances. As the model changes and becomes more complex, the process is standardized enough that extending this example would require minimal changes to our existing application. Happy Coding!
]]&gt;</description>
      <link>https://www.lqdev.me/posts/deploy-netml-docker-aci</link>
      <guid>https://www.lqdev.me/posts/deploy-netml-docker-aci</guid>
      <pubDate>2018-05-11 21:17:00 -05:00</pubDate>
      <category>azure</category>
      <category>devops</category>
      <category>dotnet</category>
      <category>ml</category>
      <category>ai</category>
      <category>microsoft</category>
      <category>programming</category>
      <category>development</category>
      <category>csharp</category>
      <category>aci</category>
      <category>docker</category>
      <category>mlnet</category>
      <category>webapi</category>
      <category>aspnetcore</category>
      <category>aspnet</category>
      <category>machinelearning</category>
      <category>artificialintelligence</category>
    </item>
    <item>
      <title>E2E Dockerizing a MEAN Stack Application</title>
      <description>&lt;![CDATA[

## Introduction

Lately I've been getting familiar with [Docker](https://docker.com). I have built single container applications using a `Dockerfile` and run them locally. This works fine, especially for deployment purposes to a production VM but it's no different than setting up the VM with all the required dependencies and pushing updates via FTP or source control. However, one of the features that I have found extremely useful is multi-container building and deployment via `docker-compose`. With `docker-compose`, not only can I build and run my application, but also dependent services like databases, caches, proxies, etc. Best of all, the builds are standardized and initialized at once without having to individually install the dependencies and components. This writeup explores how to containerize a MEAN stack application and set up a `docker-compose.yml` file for it to build and start the server and database services defined within it.

## Requirements

This writeup assumes that `Docker`, `Docker-Compose` and `Node` are installed on your PC. 

## The Application

The application is a CRUD todo MEAN stack application. The repo for this application can be found [here](https://github.com/lqdev/todomeandockerdemo).

### Project Structure

```text
|_models (Mongoose models)
| |_todo.model.js
|_public
| |_scripts
| | |_controllers
| | | |_main.controller.js
| | |_services
| |   |_todo.service.js
| |_views
| | |_main.html
| |_app.js (front-end application)
| |_index.html
|_Dockerfile (server service)
|_docker-compose.yml
|_api.js (todo api routes)
|_config.js
|_server.js (back-end application)
```

The front-end is built with `AngularJS` and the back-end is built with `NodeJS` using the `Express` web framework and `MongoDB` database. `MongoDB` models are defined with the `Mongoose` package. In the application, users can create, view, update and delete todo tasks. The `Dockerfile` is used to define the container for the web application and the `docker-compose.yml` defines both the `MongoDB` database container as well as the web application container defined in the `Dockerfile`.

## The Docker File

```docker
#Define base image
FROM node:8

#Set Working Directory
WORKDIR /app

#Copy pakage.json file from current directory to working directory
ADD package.json /app

#Install npm packages
RUN npm install

#Copy all application files from local directory to working directory
ADD . /app

#Open port where app will be listening
EXPOSE 3000

#Start application
CMD ['npm','start']
```

### Define Docker Image and Application Directory

The `Dockerfile` has no extension and the syntax is like a standard text file. `#` characters denote comments. `Docker` works based off images which are basically pre-built packages that are stored in one of many registries such as `DockerHub`. `DockerHub` can be thought of as a package repository/manager like `npm` or `dpkg`. In the first two lines of the file we define which base image we want to create our container with. Since this is a MEAN stack application built entirely in `JavaScript`, we'll be using the `node` version 8 image. Then we want set the directory in which our application will reside. We do this by using the `WORKDIR` command and setting `/app` as our application directory, but any directory of your choosing is valid.

### Install Dependecies

All of our dependencies should be defined in our `package.json` file. In order to install these dependencies in our container, we need to copy our local `package.json` file into our container application directory. This can be done with the `ADD` command by passing the `package.json` and application directory `/app` as arguments. Once that file has been copied, it's time to install the dependencies. To run commands through the build process of the application we use the `RUN` command. The command is no different than the one you'd use on your local machine. Therefore, to install the dependencies defined in the `package.json` file we use the command `RUN npm install`.

### Copy Application Files

Once our dependencies are installed, we need to copy the rest of the files in our local project directory to our container application directory. Like with the `package.json` file we use the `ADD` command and pass `.` and `/app` as our source and destination arguments respectively.

#### .dockerignore

Something to keep in mind is that locally we have a `node_modules` directory containing our installed dependencies. In the previous step, we ran the `npm install` command which will create the `node_modules` directory inside our container. Therefore, there is no need to copy all these files over. Like `git`, we can set up a `.dockerignore` file which will contain the files and directory to be ignored by `Docker` when packaging and building the container. The `.dockerignore` file looks like the following.

```text
node_modules/*
```

### Opening Ports

Our application will be listening for connections on a port. This particular application will use port 3000. We need to define the port to listen on in the `Dockerfile` as well. To do so, we'll use the `EXPOSE` command and pass the port(s) that the application will listen on. (MongoDB listens on 27017, but since the `Dockerfile` only deals with the web application and not the database we only need to specify the web application's port).

### Starting the Application

After our container is set up, dependencies are installed and port is defined, it's time to start our application. Unlike the process of running commands while building the container using the `RUN` command, we'll use the `CMD` command to start our application. The arguments accepted by this are an array of strings. In this case, we start our application like we would locally by typing in `npm start`. The `Dockerfile` command to start our application is the following `CMD ['npm','start']`.

## The docker-compose.yml File

```yaml
version: '2'
services:
  db:
    image: mongo
    ports: 
      - 27017:27017
  web:
    build: .
    ports:
      - 3000:3000
    links:
      - "db"
```

The `docker-compose.yml` file is a way of defining, building and starting multi-container applications using `docker-compose`. In our case we have a two container application, one of the containers is the web application we defined and built in the `Dockerfile` and the other is a `MongoDB` database. The `docker-compose.yml` file can take many options, but the only ones we'll be using are the `version` and `services` option. The `version` option defines which syntax version of the `docker-compose.yml` file we'll be using. In our case we'll be using version 2. The `services` option defines the individual containers to be packaged and initialized.

### Services

As mentioned, we have two containers. The names of our containers are `web` and `db`. These names can be anything you want, as long as they're descriptive and make sense to you. The `web` container will be our web application and the `db` container will be our `MongoDB` database. Notice that we have listed our `db` service first and then our `web` service. The reason for this is we want to build and initialize our database prior to our application so that by the time that the web application is initialized, it's able to successfully connect to the database. If done the other way around, an error will be thrown because the database will not be listening for connections and the web application won't be able to connect. Another way to ensure that our database is initialized prior to our web application is to use the `links` option in our `web` service and add the name of the database service `db` to the list of dependent services. The `ports` option like in our `Dockerfile` defines which ports that container will need to operate. In this case, our `web` app listens on port 3000 and the `db` service will listen on port 27017.

#### Container Images

The `docker-compose.yml` can build containers based on images hosted in a registry as well as those defined by a `Dockerfile`. To use images from a registry, we use the `image` option inside of our service. Take the `db` service for example. `MongoDB` already has an image in the `DockerHub` registry which we will use to build the container. Our `web` container does not have an image that is listed in a registry. However, we can still build an image based off a `Dockerfile`. To do this, we use the `build` option inside our service and pass the directory of the respective `Dockerfile` containing the build instructions for the container. 

## Building and Running Containers

Now that our container definitions and files are set up, we're ready to build and run our application. This can be done by typing `docker-compose up -d` in the terminal from inside our local project directory. The `-d` option runs the command detached allowing us to continue using the terminal. This command will both build and start our containers simultaneously. Once the `web` and `db` containers are up and running, we can visit `http://localhost:3000` from our browser to view and test our application. To stop the application, inside the local project directory, we can type `docker-compose stop` in our terminal to stop both containers.

## Conclusion

This writeup uses a pre-configured MEAN stack CRUD todo application and explores how to define a single container `Dockerfile` as well as a multi-container application using `docker-compose`. Docker streamlines how applications are built and deployed while `docker-compose` allows more complex multi-container applications to be orchestrated, linked, deployed and managed simultaneously allowing developers to spend more time  developing solutions and less time managing infrastructure and dependencies.

###### Links/Resources

[Docker Community Edition](https://www.docker.com/community-edition#/download)  
[Docker: Getting Started](https://docs.docker.com/get-started/)  
[NodeJS](https://nodejs.org/en/)]]&gt;</description>
      <link>https://www.lqdev.me/posts/e2e-mean-docker-build</link>
      <guid>https://www.lqdev.me/posts/e2e-mean-docker-build</guid>
      <pubDate>2018-04-29 20:10:09 -05:00</pubDate>
      <category>devops</category>
      <category>MEAN</category>
      <category>mongodb</category>
      <category>docker</category>
      <category>angular</category>
      <category>angularjs</category>
      <category>nodejs</category>
      <category>javascript</category>
      <category>programming</category>
      <category>development</category>
      <category>docker-compose</category>
    </item>
    <item>
      <title>Type Driven Development - Scaling Safely with Python</title>
      <description>&lt;![CDATA[
# Introduction

Python is a great language. The syntax is readable and allows pseudocode to be converted into code nearly verbatim. While this is great for prototyping and moving fast, scaling can become an issue. One of the issues is with regards to documentation. In statically typed languages, even if there's no documentation, types help provide some sort of documentation that allow new contributors as well current developers to remember where they left off and what their code does. There are ways around this using docstrings as well as unit tests. However, this often involves performing tasks outside of writing code which can be time consuming. In Python 3.5, type hints or optional static typing are allowed and tools like `mypy` help write safer, more scalable code. The best part is, if the code already has docstrings and unit tests, optional static typing adds an additional layer of safety and documentation to existing projects. This writeup explores practices for documenting and developing scalable Python code as well as illustrating how to use optional static types and type checkers.

## Prerequisites

This writeup assumes that Python 3.5 or greater is being used and both `mypy` and `pytest` packages are installed. To install them using `pip` we can type the following command in the shell:

```bash
pip install pytest mypy
```

## Docstrings

[PEP 257](https://www.python.org/dev/peps/pep-0257/) provides a great overview of what docstrings are and how to use them. The summarized version of it is a string literal in classes and functions that allows developers to document logic, inputs and outputs of those particular sections of code. Below are examples of code with and without docstrings:

### No Docstring

```python
def combine(a,b):
    return a + b
```

### With Docstring

```python
def combine(a,b):
    """
    Returns the sum of two numbers

    Keyword arguments:
    a -- the first number
    b -- the second number

    Returns:
    Sum of a and b
    """
    return a + b
```

As we can see, the string literal or docstring allows individuals who are looking at the code for the first time as well as someone who worked on it and has forgotten the logic of a program to easily decipher what the code does.

Something else to notice is the function name. In this case, the logic is relatively simple and the name may make some sense at the time of writing the code. However, this simple logic is dangerous. Without knowing what is expected as input and output, there's not a clear way of knowing what this code should do. For example, someone might try to run the undocumented version of this function with parameter `a` having the value 'Hello' and `b` with the value 'World'. The function would not throw any errors and return 'HelloWorld' as a result. However, if we look at the intented logic as well as expected input and output provided by the docstring, we'd know that `a` and `b` are both supposed to be numerical values, not strings or any other type.

It's clear that writing a docstring can become tedious and take up a substantial amount of time as a project grows. However, the benefits are reaped when extending the code and using it in production because more time is spent being productive rather than figuring out what the code should do and whether it's being done correctly. Docstrings however are not a panacea since there is no way to enforce what is documented in the code and serves as more of an FYI for developers using and extending the code.

## Unit Testing

One way to prevent code from being misused is by writing tests. By writing unit tests and making sure that they pass, developers can test edge cases such as passing a string and immediately getting feedback through failing tests. Here's an example of what a unit test would look like for the `combine` function written above.

In the file `main.py`, we can write the logic for our `combine` function. However, keeping in mind the docstring, we might want to add some exception handling. 

```python
"""
Module Containing Logic
"""

def combine(a,b):
    """                    
    Returns the sum of two numbers                                          
    Keyword arguments:                 
    a -- the first number  
    b -- the second number                                                      
    Returns:             
    Sum of a and b       
    """
    if(type(a) == str or type(b) == str)
        return a + b
    else:
        raise TypeError
```

In another file called `test_main.py`, we can write our tests. Our test file will look like the code below:

```python
import pytest
from main import combine

testparams = [(1,2,3),(2,4,6)]
@pytest.mark.parametrize("a,b,expected",testparams)
def test_combine(a,b,expected): 
    assert combine(a,b) == expected

testparams = [('a','b'),('a','b')]
@pytest.mark.parametrize("a,b",testparams)
def test_combine_exception(a,b):
    with pytest.raises(TypeError):
        combine(a,b)
```

In our shell we can enter the `pytest` command inside of our project directory and get the following output.

```bash
pytest
```

![](http://cdn.lqdev.tech/files/images/typedrivendevelopment1.png)

The results from `pytest` ensure that passing the expected parameters returns the expected output which is the sum of two numbers and passing in the wrong parameters such as those of type string return a `TypeError`. This gets us closer to where we want to be where we're able to test whether the functionality of our application does what it's supposed to. Like docstrings, there is additional work and time that needs to be accounted for when writing tests. However, this is a practice that should be taking place already and in the case of Python which does not provide the type checking or compilation is a way to if not ensure that our logic is sound, at least it provides us with an additional form of documentation and peace of mind that the code is being used accurately. 

## Type Hints (Optional Static Types)

Good practice would have us write docstrings to document our code and unit tests to ensure the soundness of our logic and code. However, what if that seems like too much work or there's not much time to perform those tasks. Is there a shorthand way that we can both document our code for posterity as well as ensure that we can only use the code as intended. That's where type hints comes in and starting with Python 3.5 have been accepted by the Python community per [PEP 484](https://www.python.org/dev/peps/pep-0484/). With type hints our code would not change much and with a few extra characters, we can write safer code. Our `combine` function from previous examples would look as follows with type hints:

```python
def combine(a:float,b:float) -&gt; float:
    return a + b    
```

If we run this, it should run as expected given the appropriate parameters. That being said, as with the undocumented example, if we pass in parameters 'Hello' and 'World', it should work as well and we get the result 'HelloWorld'. If we still don't get the result we want and our code is still unsafe, then what's the point? One of the benefits is the documentation piece. In the event that we had no docstring, we could still tell that `a` and `b` are both of type `float` and return a `float`. The second benefit comes from the use of `mypy`, a type checker for Python. To see it in action, we can create a script called `mainmypy.py` and add the following code:

```python
def combine(a:float,b:float) -&gt; float:
    return a + b

combine(1,2)
combine('Hello','World')
```

In the shell, we can then use the `mypy` command on our script to check types.

```bash
mypy mainmypy.py
```

The output is the following:

![](http://cdn.lqdev.tech/files/images/typedrivendevelopment2.png)

As we can see, without having to run our code, `mypy` checks the types and throws errors that we would not have found unless we ran our code. Therefore, we get both documentation by defining the types of parameters and outputs we expect which make it easier for individuals using or writing code to safely do so without having to write long, descriptive docstrings. With `mypy`, we enforce the good use of code by checking that the correct parameters are being passed in and the correct results are being returned prior to runtime making it safe to scale and write correct code most of the time. 

## Conclusion

Python is a very expressive language that allows applications to be prototyped in no time. However, the tradeoff is that writing new code or returning to it at a later time without documenting it, particularly the types needed by functions or classes to produce an accurate result can be unsafe. Some existing practices such as docstrings and unit tests can help with documenting and writing safe code. However, tools like `mypy` and the recently introduced type hints achieve what both docstrings and unit tests do in less time and code. This is not to say that these tools are perfect and ideally, unit tests, docstrings and type hints are all integrated to make developers more productive and create safe, scalable code. Happy coding!


]]&gt;</description>
      <link>https://www.lqdev.me/posts/type-driven-development-scaling-safely-with-python</link>
      <guid>https://www.lqdev.me/posts/type-driven-development-scaling-safely-with-python</guid>
      <pubDate>2018-03-24 18:16:31 -05:00</pubDate>
      <category>python</category>
      <category>static typing</category>
      <category>productivity</category>
      <category>programming</category>
      <category>development</category>
    </item>
    <item>
      <title>Scaffolding A Web Page In One Line</title>
      <description>&lt;![CDATA[
## Introduction

A few months ago, I saw VS Code released an update that had [Emmet](https://www.emmet.io/) abbreviations built in. At the time, I saw a demo and it looked like an interesting tool but I knew very little about it. This past week, while practicing `css` and scaffolding a blog-like web page for practice I decided to give Emmet a try. After reading some of the documentation and trying it out, within five minutes I was able to replicate what I had done manually with one line of "code". The entire process is described below. 

## What We'll Build

Displayed below is the scaffold of the webpage I built. As it can be seen, in the `body` element there is a `nav` element and four `div` elements that will contain our articles and their metadata.

```html
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;&lt;/title&gt;
    &lt;link rel="stylesheet" href=""&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;nav&gt;
        &lt;ul&gt;
            &lt;li class="nav-item"&gt;&lt;/li&gt;
            &lt;li class="nav-item"&gt;&lt;/li&gt;
            &lt;li class="nav-item"&gt;&lt;/li&gt;
        &lt;/ul&gt;
    &lt;/nav&gt;
    &lt;div&gt;
        &lt;div class="article"&gt;
            &lt;h2 class="article-title"&gt;&lt;/h2&gt;
            &lt;h6 class="article-date"&gt;&lt;/h6&gt;
            &lt;p class="article-description"&gt;&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
        &lt;div class="article"&gt;
            &lt;h2 class="article-title"&gt;&lt;/h2&gt;
            &lt;h6 class="article-date"&gt;&lt;/h6&gt;
            &lt;p class="article-description"&gt;&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
        &lt;div class="article"&gt;
            &lt;h2 class="article-title"&gt;&lt;/h2&gt;
            &lt;h6 class="article-date"&gt;&lt;/h6&gt;
            &lt;p class="article-description"&gt;&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
        &lt;div class="article"&gt;
            &lt;h2 class="article-title"&gt;&lt;/h2&gt;
            &lt;h6 class="article-date"&gt;&lt;/h6&gt;
            &lt;p class="article-description"&gt;&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
```

## How We'll Build It

The `hmtl` above can be programatically built using the abbreviation below.

```text
html&gt;(head&gt;title+link)+(body&gt;(nav&gt;ul&gt;li.nav-item*3)+(div&gt;div.article&gt;h2.article-title+h6.article-date+p.article-description)*4
```

To break down some of the things that are going on here, we can start with the syntax. The `&gt;` character denotes that the item on the left is the parent of the item(s) on the right. Similarly, the `+` character is used to describe siblings. Similar to math operations, parentheses evaluate what's inside them. Elements themselves can contain additional data such as class names and ids. This data can be appended to the element name just like `css` with `.` for classes and `#` for ids. 

With that knowledge, we can see that we have an `html` root element with `head` and `body` children. The `head` element has a `title` and `link` child elements. The `body` element has a `nav` element with a `ul` child which contains three `li` elements. The `nav` element has a `div` sibling which contains four `div` children that serve as the containers for our articles and their metadata.  

## Adding some style

To help visualize what the page looks like we can add some `css`. 

```css
* {
    margin: 0;
}

nav {
    padding: 0;
    background-color: black;
    color: white;
}

.nav-item {
    margin: 20px;
    display: inline-block;
    font-weight: bold;
    font-size: 20px;
}

.nav-item:hover {
    opacity: 0.7;
}

.article {
    display: block;
    margin: 20px;
    padding-left: 5px;
    background-color: lightgray;
    border: black solid 2px;
}

.article-title {
    display: block;
    font-family: serif;
    font-size: 30px;
    text-decoration: underline;
    display: inline;
}

.article-date {
    font-size: 20px;
    font-family: fantasy;
    float: right;
    margin-right: 5px;
}

.article-description {
    display: block;
    font-size: 18px;
    font-family: sans-serif;
    margin-top: 5px;
}
```

## Result

After linking our `css` with the webpage as well as adding some content to the page, the result will look as follows. 

![](http://cdn.lqdev.tech/files/images/scaffoldawebpageoneline1.png)


```html
&lt;html&gt;
    &lt;head&gt;
        &lt;link rel="stylesheet" type="text/css" href="main.css"/&gt;
        &lt;title&gt;HTML CSS Practice&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;nav&gt;
            &lt;ul&gt;
                &lt;li class="nav-item"&gt;Home&lt;/li&gt;
                &lt;li class="nav-item"&gt;About&lt;/li&gt;
                &lt;li class="nav-item"&gt;Contact&lt;/li&gt;
            &lt;/ul&gt;
        &lt;/nav&gt;
        &lt;div&gt;
            &lt;div class="article"&gt;
                &lt;h2 class="article-title"&gt;Hello World&lt;/h2&gt;
                &lt;h6 class="article-date"&gt;January 8, 2017&lt;/h6&gt;
                &lt;p class="article-description"&gt;This is a short description of my program. There are more things than this.&lt;/p&gt;
            &lt;/div&gt;
            &lt;div class="article"&gt;
                &lt;h2 class="article-title"&gt;Second Post&lt;/h2&gt;
                &lt;h6 class="article-date"&gt;February 8, 2017&lt;/h6&gt;
                &lt;p class="article-description"&gt;I think that I got my feet wet a little bit. Maybe there are other things that I can write about&lt;/p&gt;
            &lt;/div&gt;
            &lt;div class="article"&gt;
                &lt;h2 class="article-title"&gt;Third Post&lt;/h2&gt;
                &lt;h6 class="article-date"&gt;March 8, 2017&lt;/h6&gt;
                &lt;p class="article-description"&gt;This is the third go around. I think I got pretty good at this design stuff.&lt;/p&gt;
            &lt;/div&gt;
            &lt;div class="article"&gt;
                &lt;h2 class="article-title"&gt;Fourth Post&lt;/h2&gt;
                &lt;h6 class="article-date"&gt;June 8, 2017&lt;/h6&gt;
                &lt;p class="article-description"&gt;I really like my description of the things that are happening here and there is another post on the way very soon.&lt;/p&gt;
            &lt;/div&gt;                        
        &lt;/div&gt;
    &lt;/body&gt;    
&lt;/html&gt;
```

###### Sources
[Emmet](https://www.emmet.io/)

]]&gt;</description>
      <link>https://www.lqdev.me/posts/scaffold-a-webpage-in-one-line</link>
      <guid>https://www.lqdev.me/posts/scaffold-a-webpage-in-one-line</guid>
      <pubDate>2018-03-10 16:07:00 -05:00</pubDate>
      <category>html</category>
      <category>web development</category>
      <category>development</category>
      <category>tools</category>
      <category>emmet</category>
    </item>
    <item>
      <title>Testing and Deploying Python Projects with Travis CI</title>
      <description>&lt;![CDATA[
# Introduction

When working on projects, especially those that others or yourself may depend upon, it is important to test to make sure that everything is working as expected. Furthermore, being able to deploy your code/packages to a central repository with a package manager makes distribution easier. Although this can all be done manually, it can also be automated. Both testing and deployment can be automated using Travis CI which makes the entire process as easy as pushing your most recent changes to GitHub. In this writeup, I will go over how to create a Python project with unit tests and deploy to PyPI. A sample project can be found at this [link](https://github.com/lqdev/TravisTest)

## Prerequisites

- [GitHub Login](https://github.com/)
- [PyPI Login](https://pypi.python.org/pypi)
- [virtualenv](https://virtualenv.pypa.io/en/stable/)

### Install virtualenv

```bash
sudo pip install virtualenv
```

# Create The Project

For the test project, I will create a module that performs adding, subtracting, increment and decrement operations.

## Define Folder Structure

We start out by creating a directory for our project.

```bash
mkdir travistest
```

Inside that directory, we want to have have a directory for our module as well as for our tests. Therefore, we need to create a directory for both of them.

```bash
mkdir travistest
mkdir test
```

Finally, we want to initialize the virtual environment for our project. To do so, we can use the `virtualenv` package. For the project `python 3.5` is the version that will be used.

```bash
virtualenv -p python3.5 ENV
```

After installation, a folder with the name `ENV` should appear in the root directory of the project. The final directory structure should look like so:

```text
travistest
|_ENV
|_travistest
|_test
```

## Install Modules

For this project, I'll be using `pytest` for unit testing. Before installing anything however, I'll need to activate the virtual environment.

```bash
source ENV/bin/activate
```

Once our virtual environment is activated, we can install `pytest`.

```bash
pip install pytest
```

After installation, we can persist installed packages inside a `requirements.txt` file with the `freeze` command.

```bash
pip freeze &gt; requirements.txt
```

## Create The Module

Inside the `travistest` module directory, the easiest way to create a module is to include an `__init__.py` file inside the directory. It's okay if it's empty.

Therefore, we can start by creating the `__init__` file in that directory.

```bash
touch __init__.py
```

Once that's created, we can begin writing the main functionality of our module. Inside a file called `Operations.py`, we can put the following code in.

```python
class Operations:
    def __init__(self):
        pass
    
    def add(self,x,y):
        return x + y

    def subtract(self,x,y):
        return x - y

    def increment(self,x):
        return x + 1

    def decrement(self,x):
    	return x - 1
```

## Unit Test

Once we have our code, we need to write tests for it. Navigating to the `test` directory, we can add the following code to the `test_operations.py` file.

```python
from pytest import fixture

@fixture
def op():
    from travistest.Operations import Operations
    return Operations()

def test_add(op):
    assert op.add(1,2) == 3

def test_subtract(op):
    assert op.subtract(2,1) == 1

def test_increment(op):
    assert op.increment(1) == 2

def test_decrement(op):
assert op.decrement(2) == 1
```

To make sure everything is working correctly, from the project's root directory, we can run the `pytest` command. If all goes well, an output similar to the one below should appear.

```bash
============================= test session starts ==============================
platform linux -- Python 3.5.2, pytest-3.4.0, py-1.5.2, pluggy-0.6.0

collected 4 items                                                              

test/test_operations.py ....                                             [100%]

=========================== 4 passed in 0.04 seconds ===========================
```

# Prepare For Deployment

To prepare for deployment and uploading to PyPI, we need to add a `setup.py` file to the root directory of our project. The contents of this file for our purposes are mostly metadata that will populate information in PyPI.

```python
from distutils.core import setup

setup(
    name='travistest',
    packages=['travistest'],
    version='0.0.7',
    description='Test project to get acquainted with TravisCI',
    url='https://github.com/lqdev/TravisTest',    
)
```

## Setup Travis

### Enable Repository

Assuming that you have a `GitHub` login and a repository has been created for your project:

1. Log into [travis-ci.org](https://travis-ci.org/) with your GitHub credentials.
2. Once all of your repositores are synced, toggle the switch next to the repository containing your project.

### Configure .travis.yml

Once the project has been enabled, we need fo configure Travis. This is all done using the `.travis.yml` file.

In this file we'll tell Travis that the language of our project is Python version 3.5 and that we'll be using a virtual environment. Additionally we'll require sudo priviliges and target the Ubuntu Xenial 16.04 distribution. All of these configurations can be done as follows.

```yaml
sudo: required
dist: xenial
language: python
virtualenv:
  system_site_packages: true
python:
- '3.5'
```

Once that is set up, we can tell it to install all of the dependencies stored in our `requirements.txt` file.

```yaml
install:
- pip install -r requirements.txt
```

After this, we need to tell Travis to run our tests just like we would on our local machine.

```yaml
script: pytest
```

Once our tests have run, we need to make sure we are back in the root directory of our project for deployment.

```yaml
after_script: cd ~
```

We're done with the automated testing script portion of our project. Now we need to setup deployment options. This section will mainly contain the credentials of your PyPI account.

```yaml
deploy:
  provider: pypi
  user: "YOURUSERNAME"
```

After setting the provider and user, we need to set the password. Because this will be a public repository DO NOT enter your password on this file. Instead, we can set an encrypted version that only Travis can decrypt. To do so, while in the root directory of our project, we can enter the following command into the terminal.

```bash
travis encrypt --add deployment.password
```

Type your password into the terminal and press `Ctrl + D`.

If you take a look at your `.travis.yml` file you should see something like the following in your `deploy` options

```bash
deploy:
  provider: pypi
  user: "YOURUSERNAME"
  password:
    secure: "YOURPASSWORD"
```

The final `.travis.yml` file should like like so

```yaml
sudo: required
dist: xenial
language: python
virtualenv:
  system_site_packages: true
python:
- '3.5'
install:
- pip install -r requirements.txt
script: pytest
after_script: cd ~
deploy:
  provider: pypi
  user: "YOURUSERNAME"
  password:
    secure: "YOURPASSWORD"
```

# Deploy

Now that we have everything set up, deployment should be relatively easy. A build is triggered when changes are pushed to the repository on GitHub. Therefore, pushing your local changes to the remote GitHub repository should initialize the build. To track progress, visit the project's page on Travis CI.

**NOTE: WHEN PUSHING NEW CHANGES, INCREMENT VERSION NUMBER IN THE `SETUP.PY` FILE SO THAT EXISTING FILE ERRORS DO NOT CRASH BUILDS**

# Conclusion

In this writeup, we created a Python package that performs basic operations and set up automated testing and deployment with Travis CI. Configurations can be further customized and refined to adapt more complex build processes.]]&gt;</description>
      <link>https://www.lqdev.me/posts/testing-deploying-python-projects-travisci</link>
      <guid>https://www.lqdev.me/posts/testing-deploying-python-projects-travisci</guid>
      <pubDate>2018-02-18 20:51:32 -05:00</pubDate>
      <category>devops</category>
      <category>travisci</category>
      <category>python</category>
      <category>ci/cd</category>
      <category>programming</category>
      <category>development</category>
    </item>
    <item>
      <title>The Case for .NET in the Classroom</title>
      <description>&lt;![CDATA[
# Introduction

I recently finished by M.S. Computer Science degree and throughout the course, I used a variety of programming languages to complete assignments such as Matlab (Octave), Python, R, Java, Scala, Scheme (Lisp), and C. While all powerful in their own way, in my personal time (if and when I had any), I tried to learn about .NET tools and the ecosystem as a whole mainly because I have been a long time user of Microsoft products that include but are not limited to Zune, Kin, Surface and Windows Phone. Coincidentally, around the time I started my program, Microsoft was in the process of making an aggressive push to be more open and return to its roots of making tools and services that developers love. That being said, this is not exactly something that many people are aware of and the company's image in the eyes of consumers and developers using open source technologies has only recently started to improve. With recent acquisitions of companies like Skype and Xamarin as well as products like HoloLens, the opportunities for developers to build transformative solutions for consumers and businesses across a wide range of devices is limitless. Equally exciting and advantageous is the fact that the underlying ecosystem many of these technologies rely on is .NET. Keeping all that in mind, I strongly believe that there is no better time to introduce .NET into the classroom, especially at the university level in order to equip the next generation of developers with the necessary tools to achieve more in a wide range of environments. 

## Microsoft Past

### Enterprise and Licensing

For the longest time, Microsoft has been synonymous with enterprise and rightfully so as most businesses run Windows and Office. What this also meant was licensing. Many of the products and services that Microsoft provided for businesses and developers were licensed products. Even though there were in some instances free versions of the products, they had their limitations. As a result, this naturally created a barrier to entry to try and experiment with their products.

### Overhead

Even when trying to do the simplest thing with .NET Framework (particularly C#, ASP.NET or F#), you needed to run Visual Studio. Like the enterprise and licensing, this provided a barrier to entry for someone trying to learn the language. Conversely, in the most closely related language Java, you could've gotten started with a text editor and command line (among other things, this might be one reason Java has been widely used in the classroom).

### Platform Specific

Although most of the world ran Windows, developer tools and products like the .NET Framework targeted Windows only. While this was acceptable a few years ago, the world is much different today than it was then. The current state of computing, while not entirely a statement on Microsoft and their products is one that Microft has astutely recognized. People want their services and experiences to travel with them regardless of the platform or products they use.

Along these lines is the type of devices individuals performed their computing on. A few years ago, the Windows PC was the main device for productivity. Therefore, there was little to no need to focus on mobile, let alone cross-platform capabilities. Now, individuals can perform elaborate and deeply complex workflows from the palm of their hand. Therefore, the focus has shifted from a single device category to multi-device and multi-platform one.

## Microsoft Today

Microsoft has significantly transformed their business practices and refocused on what at one time made them one of the top companies in the world by making experiences and products consumers and more importantly developers love.

### Mobile-First, Cloud-First

This was the strategy Satya Nadella tried to get the entire team to rally around when he became CEO of the company. At the time, Microsoft had recently purchased Nokia, yet its mobile phone efforts were non-existent aside from the three percent. Therefore, this strategy seemed ambiguous and unattainable at least from a consumer perspective.

Upon further inspection, this strategy is not limited to mobile phones. Instead it refers to the ability of individuals' computing experiences, with the aid of the cloud, to go wherever they can be most productive. The hardware and to some extent, the software in terms of the platform no longer bind individuals as the services are available nearly everywhere.

A shining example of this is Xamarin. Xamarin is a mobile app development platform built on the .NET ecosystem. With this purchase, Microsoft allowed developers who were native .NET developers to work in the same environment they were used to, while targeting a new family of devices running Android or iOS. Aside from learning the some of the inner workings the Xamarin framework, at the core, individuals can leverage the .NET tools they're used to using. 

### Emerging Technologies
#### Artificial Intelligence

Artificial intelligence, data science and machine learning have been at the forefront of discussions involving technology both in the enterprise as well as academia. Microsoft has made strides on many fronts to position themselves and provide the means for individuals to use the Microsoft set of tools to create solutions. 
One of the ways Microsoft has worked on providing developers and individuals with tools for deep learning is the Microsoft Cognitive Toolkit (CNTK). This framework is very similar to Tensorflow. Although models can be built using BrainScript or Python, the models themselves can be easily plugged into .NET solutions seamlessly. 
Another way in which Microsoft is providing tools for artificial intelligence is through its Bot Framework. The Bot Framework has different parts. One of them is the creation of chat bots using either NodeJS or C#. These bots can target different chat applications that include but are not limited to Skype, Facebook Messenger, Slack and many others. These bots can be made even more intelligent with Microsoft's Language Understanding Service (LUIS) which takes care of the natural language processing the bot might need to process requests adequately.

The second part of the Bot Framework is Cortana Skills. Using LUIS and the Bot Framework, individuals can create speech and text-enabled solutions for individuals that interact with Cortana enabled devices which include but are not limited to smart speakers, PCs and mobile devices. This new kind of experiences again is entirely backed by the .NET ecosystem many developers already use today. 

#### Virtual and Mixed Reality

While still in its infancy, virtual and mixed reality have received tons of support from technology companies like Facebook with Oculus, Google with Google VR, Apple with AR Kit. Likewise, Microsoft's solution is HoloLens. Applications for HoloLens can be developed with .NET tools and Unity. Yet again, developers can leverage their .NET skills to build applications for yet another family of devices to build new immersive experiences that not too long ago were closer to science-fiction than reality. 

### .NET Core and .NET Standard

A lot of computing functions overall are fairly standard. For example, reading from the console is a relatively universal procedure regardless of the platform. Microsoft has found a way to take advantage of the power of .NET for these scenarios and released .NET Standard and .NET Core which are cross-platform environments to develop C#, ASP.NET and F# solutions. These applications can run on the web as well as any OS platform further expanding the capabilities of the ecosystem as well as extending the number of solutions developers can create.

Furthermore, Visual Studio, while helpful is not a requirement to start creating applications. Most things if not all can be done via the command line and a text editor. Therefore, the barrier to entry both in terms of cost and start time has been greatly reduced and individuals get started building applications in little to no time for free. 

## Conclusion

While this is not the entire gamut of tools and services Microsoft provides, it is a good demonstration of how learning a single set of skills like C# and F# in conjunction with the standard curriculum can greatly empower the next generation of computer scientists and developers by expanding the number of devices and platforms they can target across a wide variety of industries and disciplines making the number of applications nearly limitless.]]&gt;</description>
      <link>https://www.lqdev.me/posts/dotnet-in-the-classroom</link>
      <guid>https://www.lqdev.me/posts/dotnet-in-the-classroom</guid>
      <pubDate>2017-12-14 16:57:39 -05:00</pubDate>
      <category>.net</category>
      <category>microsoft</category>
      <category>thoughts</category>
      <category>programming</category>
      <category>development</category>
    </item>
  </channel>
</rss>