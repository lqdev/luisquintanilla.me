<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - llmc</title>
    <link>https://www.lqdev.me/tags/llmc</link>
    <description>All content tagged with 'llmc' by Luis Quintanilla</description>
    <lastBuildDate>2024-05-28 20:33 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the TLDR is that we're training a 12-layer GPT-2 (124M), from scratch, on 10B tokens of FineWeb, with max sequence length of 1024 tokens.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The 124M model is the smallest model in the GPT-2 series released by OpenAI in 2019, and is actually quite accessible today, even for the GPU poor. With llm.c, which is quite efficient at up to ~60% model flops utilization, reproducing this model on one 8X A100 80GB SXM node takes ~90 minutes. For example, on Lambda this node goes for ~$14/hr, so the total cost of reproducing this model today is about $20. You can train the model with a single GPU too, it would just take proportionally longer (e.g. ~4-24 hours depending on the GPU).&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/repro-gpt-2-llm-c-90-min-20-dollars-karpathy</link>
      <guid>https://www.lqdev.me/responses/repro-gpt-2-llm-c-90-min-20-dollars-karpathy</guid>
      <pubDate>2024-05-28 20:33 -05:00</pubDate>
      <category>ai</category>
      <category>llm</category>
      <category>gpt</category>
      <category>gpt2</category>
      <category>llmc</category>
      <category>c</category>
      <category>slm</category>
    </item>
  </channel>
</rss>