<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - wwdc</title>
    <link>https://www.lqdev.me/tags/wwdc</link>
    <description>All content tagged with 'wwdc' by Luis Quintanilla</description>
    <lastBuildDate>2024-06-11 20:42 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Apple - Private Cloud Compute</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We set out to build Private Cloud Compute with a set of core requirements:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Stateless computation on personal user data.&lt;/strong&gt; ...we want a strong form of stateless data processing where personal data leaves no trace in the PCC system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enforceable guarantee.&lt;/strong&gt; Security and privacy guarantees are strongest when they are entirely technically enforceable, which means it must be possible to constrain and analyze all the components that critically contribute to the guarantees of the overall Private Cloud Compute system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No privileged runtime access.&lt;/strong&gt; Private Cloud Compute must not contain privileged interfaces that would enable Apple’s site reliability staff to bypass PCC privacy guarantees, even when working to resolve an outage or other severe incident.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-targetability.&lt;/strong&gt; An attacker should not be able to attempt to compromise personal data that belongs to specific, targeted Private Cloud Compute users without attempting a broad compromise of the entire PCC system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Verifiable transparency.&lt;/strong&gt; Security researchers need to be able to verify, with a high degree of confidence, that our privacy and security guarantees for Private Cloud Compute match our public promises.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/apple-private-cloud-compute</link>
      <guid>https://www.lqdev.me/responses/apple-private-cloud-compute</guid>
      <pubDate>2024-06-11 20:42 -05:00</pubDate>
      <category>ai</category>
      <category>pcc</category>
      <category>privacy</category>
      <category>cloud</category>
      <category>privatecloudcompute</category>
      <category>wwdc</category>
    </item>
    <item>
      <title>Introducing Apple’s On-Device and Server Foundation Models</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Apple Intelligence is comprised of multiple highly-capable generative models that are specialized for our users’ everyday tasks, and can adapt on the fly for their current activity. The foundation models built into Apple Intelligence have been fine-tuned for user experiences such as writing and refining text, prioritizing and summarizing notifications, creating playful images for conversations with family and friends, and taking in-app actions to simplify interactions across apps.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In the following overview, we will detail how two of these models — a ~3 billion parameter on-device language model, and a larger server-based language model available with Private Cloud Compute and running on Apple silicon servers — have been built and adapted to perform specialized tasks efficiently, accurately, and responsibly. These two foundation models are part of a larger family of generative models created by Apple to support users and developers; this includes a coding model to build intelligence into Xcode, as well as a diffusion model to help users express themselves visually, for example, in the Messages app.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Pre-training&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our foundation models are trained on Apple's AXLearn framework, an open-source project we released in 2023. It builds on top of JAX and XLA, and allows us to train the models with high efficiency and scalability on various training hardware and cloud platforms, including TPUs and both cloud and on-premise GPUs. We used a combination of data parallelism, tensor parallelism, sequence parallelism, and Fully Sharded Data Parallel (FSDP) to scale training along multiple dimensions such as data, model, and sequence length.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Post-training&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We find that data quality is essential to model success, so we utilize a hybrid data strategy in our training pipeline, incorporating both human-annotated and synthetic data, and conduct thorough data curation and filtering procedures. We have developed two novel algorithms in post-training: (1) a rejection sampling fine-tuning algorithm with teacher committee, and (2) a reinforcement learning from human feedback (RLHF) algorithm with mirror descent policy optimization and a leave-one-out advantage estimator. We find that these two algorithms lead to significant improvement in the model’s instruction-following quality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Optimization&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Both the on-device and server models use grouped-query-attention. We use shared input and output vocab embedding tables to reduce memory requirements and inference cost. These shared embedding tensors are mapped without duplications. The on-device model uses a vocab size of 49K, while the server model uses a vocab size of 100K, which includes additional language and technical tokens.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;For on-device inference, we use low-bit palletization, a critical optimization technique that achieves the necessary memory, power, and performance requirements. To maintain model quality, we developed a new framework using LoRA adapters that incorporates a mixed 2-bit and 4-bit configuration strategy — averaging 3.5 bits-per-weight — to achieve the same accuracy as the uncompressed models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Additionally, we use an interactive model latency and power analysis tool, Talaria, to better guide the bit rate selection for each operation. We also utilize activation quantization and embedding quantization, and have developed an approach to enable efficient Key-Value (KV) cache update on our neural engines.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Model Adaptation&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our foundation models are fine-tuned for users’ everyday activities, and can dynamically specialize themselves on-the-fly for the task at hand...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We represent the values of the adapter parameters using 16 bits, and for the ~3 billion parameter on-device model, the parameters for a rank 16 adapter typically require 10s of megabytes. The adapter models can be dynamically loaded, temporarily cached in memory, and swapped — giving our foundation model the ability to specialize itself on the fly for the task at hand while efficiently managing memory and guaranteeing the operating system's responsiveness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Performance and Evaluation&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We compare our models with both open-source models (Phi-3, Gemma, Mistral, DBRX) and commercial models of comparable size (GPT-3.5-Turbo, GPT-4-Turbo)1. We find that our models are preferred by human graders over most comparable competitor models. On this benchmark, our on-device model, with ~3B parameters, outperforms larger models including Phi-3-mini, Mistral-7B, and Gemma-7B. Our server model compares favorably to DBRX-Instruct, Mixtral-8x22B, and GPT-3.5-Turbo while being highly efficient.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;To further evaluate our models, we use the Instruction-Following Eval (IFEval) benchmark to compare their instruction-following capabilities with models of comparable size. The results suggest that both our on-device and server model follow detailed instructions better than the open-source and commercial models of comparable size.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/introducing-apple-on-device-server-foundational-models</link>
      <guid>https://www.lqdev.me/responses/introducing-apple-on-device-server-foundational-models</guid>
      <pubDate>2024-06-11 20:26 -05:00</pubDate>
      <category>ai</category>
      <category>apple</category>
      <category>slm</category>
      <category>edge</category>
      <category>wwdc</category>
    </item>
    <item>
      <title>The Verge - Apple WWDC 2024 keynote in 18 minutes</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;I always look forward to watching these recap videos from the Verge.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=sBXdyUA6A88" title="Thumbnail of Verge WWDC 2024 Keynote recap video"&gt;&lt;img src="http://img.youtube.com/vi/sBXdyUA6A88/0.jpg" class="img-fluid" alt="Thumbnail of Verge WWDC 2024 Keynote recap video" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/verge-wwdc-2024-18-minute-recap-video</link>
      <guid>https://www.lqdev.me/responses/verge-wwdc-2024-18-minute-recap-video</guid>
      <pubDate>2024-06-11 20:05 -05:00</pubDate>
      <category>apple</category>
      <category>wwdc</category>
      <category>keynote</category>
      <category>wwdc2024</category>
      <category>ai</category>
      <category>ios</category>
      <category>ipad</category>
      <category>iphone</category>
      <category>watchos</category>
      <category>ipados</category>
      <category>macos</category>
      <category>visionos</category>
    </item>
  </channel>
</rss>