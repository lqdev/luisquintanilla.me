<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - generativeai</title>
    <link>https://www.lqdev.me/tags/generativeai</link>
    <description>All content tagged with 'generativeai' by Luis Quintanilla</description>
    <lastBuildDate>2025-02-15 12:14 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Generative AI for Beginners (.NET) is now available</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;If you're a .NET developer looking to get started with Generative AI, there's &lt;a href="https://devblogs.microsoft.com/dotnet/announcing-generative-ai-for-beginners-dotnet/"&gt;a new course for beginners&lt;/a&gt; that just launched.&lt;/p&gt;
&lt;p&gt;Through a series of self-paced lessons, you'll learn the foundations to help you start using AI to augment the capabilities of your .NET applications.&lt;/p&gt;
&lt;p&gt;Check out the &lt;a href="https://aka.ms/genainet"&gt;course&lt;/a&gt; and give us feedback!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/generative-ai-for-beginners-dotnet-course</link>
      <guid>https://www.lqdev.me/responses/generative-ai-for-beginners-dotnet-course</guid>
      <pubDate>2025-02-15 12:14 -05:00</pubDate>
      <category>ai</category>
      <category>dotnet</category>
      <category>course</category>
      <category>generativeai</category>
    </item>
    <item>
      <title>The Illustrated DeepSeek-R1</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Just like most existing LLMs, DeepSeek-R1 generates one token at a time, except it excels at solving math and reasoning problems because it is able to spend more time processing a problem through the process of generating thinking tokens that explain its chain of thought.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png" class="img-fluid" alt="DeepSeek R1 Training Recipe" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/illustrated-deepseek-r1</link>
      <guid>https://www.lqdev.me/bookmarks/illustrated-deepseek-r1</guid>
      <pubDate>2025-02-03 20:20 -05:00</pubDate>
      <category>deepseek</category>
      <category>ai</category>
      <category>visualization</category>
      <category>generativeai</category>
      <category>genai</category>
      <category>llm</category>
      <category>learning</category>
    </item>
    <item>
      <title>Spotify Wrapped 2024 AI Generated Podcast</title>
      <description>&lt;![CDATA[&lt;p&gt;While looking at &lt;a href="https://www.lqdev.me/notes/spotify-wrapped-2024"&gt;Spotify Wrapped this year&lt;/a&gt;, I completely missed the AI-generated podcast that came along with it.&lt;/p&gt;
&lt;p&gt;As cool as this is, &lt;strong&gt;it should've been opt-in&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Although I would hope that when I clicked &amp;quot;I Agree&amp;quot; on the user agreement, the use of my data in this way is covered (for Spotify's sake), it's not clear or easy to find:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What data was shared with Google's NotebookLM&lt;/li&gt;
&lt;li&gt;What Google will do with that data&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There's a reason I purposefully haven't linked Spotify to other accounts.&lt;/p&gt;
&lt;p&gt;I know there's probably hundreds of worse ways my data is being used I don't know about. This one though was hard to miss and a sad reminder that often you don't learn about how your data is being used until after the fact. Cases like this are not really a big deal. So what if you know I was going through a &amp;quot;Witchy Jam Band Phsychedelic Rock&amp;quot; phase in September? Where it does matter though is when it involves more personal and sensitive data. Again, because we don't realize it until after the fact, it's not until then that we learn what we're giving up when we make the simple, often uninformed, decision to select &amp;quot;I Agree&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://wrappedaipodcast.spotifycdn.com/bDEn3i-uWoItOTYMmcRcNGKBqZ18M9ygFTAY0X8YHXk.mp4" class="img-fluid" alt="AI Genereated Spotify Wrapped Podcast" /&gt;&lt;/p&gt;
&lt;!-- &lt;video controls="" autoplay="" name="media"&gt;&lt;source src="https://wrappedaipodcast.spotifycdn.com/bDEn3i-uWoItOTYMmcRcNGKBqZ18M9ygFTAY0X8YHXk.mp4" type="video/mp4"&gt;&lt;/video&gt; --&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/spotify-wrapped-2024-podcast-notebook-lm</link>
      <guid>https://www.lqdev.me/notes/spotify-wrapped-2024-podcast-notebook-lm</guid>
      <pubDate>2024-12-05 09:02 -05:00</pubDate>
      <category>spotify</category>
      <category>ai</category>
      <category>spotifywrapped</category>
      <category>google</category>
      <category>notebooklm</category>
      <category>privacy</category>
      <category>podcast</category>
      <category>generativeai</category>
      <category>genai</category>
    </item>
    <item>
      <title>Generative AI Handbook: A Roadmap for Learning Resources</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;This document aims to serve as a handbook for learning the key concepts underlying modern artificial intelligence systems. Given the speed of recent development in AI, there really isn’t a good textbook-style source for getting up-to-speed on the latest-and-greatest innovations in LLMs or other generative models, yet there is an abundance of great explainer resources (blog posts, videos, etc.) for these topics scattered across the internet. My goal is to organize the “best” of these resources into a textbook-style presentation, which can serve as a roadmap for filling in the prerequisites towards individual AI-related learning goals. My hope is that this will be a “living document”, to be updated as new innovations and paradigms inevitably emerge, and ideally also a document that can benefit from community input and contribution. This guide is aimed at those with a technical background of some kind, who are interested in diving into AI either out of curiosity or for a potential career. I’ll assume that you have some experience with coding and high-school level math, but otherwise will provide pointers for filling in any other prerequisites.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/genai-handbook-learning-resource-brown</link>
      <guid>https://www.lqdev.me/bookmarks/genai-handbook-learning-resource-brown</guid>
      <pubDate>2024-06-06 22:18 -05:00</pubDate>
      <category>generativeai</category>
      <category>genai</category>
      <category>handbook</category>
      <category>learning</category>
      <category>resource</category>
    </item>
    <item>
      <title>Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;A practical guide to constructing and retrieving information from knowledge graphs in RAG applications with Neo4j and LangChain&lt;br /&gt;
&lt;br&gt;
Graph retrieval augmented generation (Graph RAG) is gaining momentum and emerging as a powerful addition to traditional vector search retrieval methods. This approach leverages the structured nature of graph databases, which organize data as nodes and relationships, to enhance the depth and contextuality of retrieved information.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/enhace-rag-application-accuracy-knowledge-graphs</link>
      <guid>https://www.lqdev.me/responses/enhace-rag-application-accuracy-knowledge-graphs</guid>
      <pubDate>2024-03-17 21:29 -05:00</pubDate>
      <category>knowledgegraph</category>
      <category>rag</category>
      <category>genai</category>
      <category>langchain</category>
      <category>generativeai</category>
      <category>retrievalaugmentedgeneration</category>
      <category>patterns</category>
      <category>applicationpatterns</category>
    </item>
    <item>
      <title>Building Meta’s GenAI Infrastructure</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;ul&gt;
&lt;li&gt;Marking a major investment in Meta’s AI future, we are announcing two 24k GPU clusters. We are sharing details on the hardware, network, storage, design, performance, and software that help us extract high throughput and reliability for various AI workloads. We use this cluster design for Llama 3 training.&lt;/li&gt;
&lt;li&gt;We are strongly committed to open compute and open source. We built these clusters on top of Grand Teton, OpenRack, and PyTorch and continue to push open innovation across the industry.&lt;/li&gt;
&lt;li&gt;This announcement is one step in our ambitious infrastructure roadmap. By the end of 2024, we’re aiming to continue to grow our infrastructure build-out that will include 350,000 NVIDIA H100 GPUs as part of a portfolio that will feature compute power equivalent to nearly 600,000 H100s.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/building-meta-genai-infrastructure</link>
      <guid>https://www.lqdev.me/responses/building-meta-genai-infrastructure</guid>
      <pubDate>2024-03-17 21:14 -05:00</pubDate>
      <category>meta</category>
      <category>ai</category>
      <category>infrastraucture</category>
      <category>generativeai</category>
      <category>genai</category>
      <category>facebook</category>
    </item>
    <item>
      <title>Stable Diffusion 3: Research Paper</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Key Takeaways:&lt;br /&gt;
&lt;br&gt;
Today, we’re publishing our research paper that dives into the underlying technology powering Stable Diffusion 3.&lt;br /&gt;
&lt;br&gt;
Stable Diffusion 3 outperforms state-of-the-art text-to-image generation systems such as DALL·E 3, Midjourney v6, and Ideogram v1 in typography and prompt adherence, based on human preference evaluations.&lt;br /&gt;
&lt;br&gt;
Our new Multimodal Diffusion Transformer (MMDiT) architecture uses separate sets of weights for image and language representations, which improves text understanding and spelling capabilities compared to previous versions of SD3.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/stable-diffusion-3-research-paper</link>
      <guid>https://www.lqdev.me/responses/stable-diffusion-3-research-paper</guid>
      <pubDate>2024-03-05 12:55 -05:00</pubDate>
      <category>stabilityai</category>
      <category>stablediffusion</category>
      <category>ai</category>
      <category>generativeai</category>
      <category>genai</category>
      <category>images</category>
    </item>
    <item>
      <title>Getting started with Ollama on Windows</title>
      <description>&lt;![CDATA[
Recently [Ollama announced support for Windows](/responses/ollama-windows-preview) in preview. In doing so, people who want to use AI models like Llama, Phi, and many others can do so locally on their PC. In this post, I'll go over how you can get started with Ollama on Windows. 

## Install Ollama

The first thing you'll want to do is install Ollama.

You can do so by [downloading the installer from the website](https://ollama.com/download/windows) and following the installation prompts. 

## Get a model

Once you've installed Ollama, it's time to get a model.

1. Open PowerShell
1. Run the following command

    ```powershell
    ollama pull llama2
    ```

In this case, I'm using llama2. However, you can choose another model. You could even download many models at once and switch between them. For a full list of supported models, see the [Ollama model documentation](https://ollama.com/library).

## Use the model

Now that you have the model, it's time to use it. The easiest way to use the model is using the REST API. When you install Ollama, it starts up a server to host your model. One other neat thing is, the REST API is [OpenAI API compatible](https://ollama.com/blog/openai-compatibility).

1. Open PowerShell
1. Send the following request:

    ```powershell
    (Invoke-WebRequest -method POST -Body '{"model":"llama2", "prompt":"Why is the sky blue?", "stream": false}' -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json
    ```

    This command will issue an HTTP POST request to the server listening on port 11434.

    The main things to highlight in the body:

    - *model*: The model you'll use. Make sure this is one of the models you pulled. 
    - *prompt*: The input to the model
    - *stream*: Whether to stream responses back to the client

    For more details on the REST API, see the [Ollama REST API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md). 

## Conclusion

In this post, I went over how you can quickly install Ollama to start using generative AI models like Llama and Phi locally on your Windows PC. If you use Mac or Linux, you can perform similar steps as those outlined in this guide to get started on those operating systems. Happy coding! ]]&gt;</description>
      <link>https://www.lqdev.me/posts/getting-started-ollama-windows</link>
      <guid>https://www.lqdev.me/posts/getting-started-ollama-windows</guid>
      <pubDate>2024-03-05 10:32 -05:00</pubDate>
      <category>ai</category>
      <category>ollama</category>
      <category>windows</category>
      <category>llm</category>
      <category>opensource</category>
      <category>llama</category>
      <category>openai</category>
      <category>generativeai</category>
      <category>genai</category>
    </item>
    <item>
      <title>Stable Diffusion 3 - Early Preview</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Announcing Stable Diffusion 3 in early preview, our most capable text-to-image model with greatly improved performance in multi-subject prompts, image quality, and spelling abilities.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Stable Diffusion 3 suite of models currently range from 800M to 8B parameters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable Diffusion 3 combines a diffusion transformer architecture and flow matching.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/stable-diffusion-3-early-preview</link>
      <guid>https://www.lqdev.me/responses/stable-diffusion-3-early-preview</guid>
      <pubDate>2024-02-22 20:59 -05:00</pubDate>
      <category>stabilityai</category>
      <category>ai</category>
      <category>stablediffusion</category>
      <category>imagegeneration</category>
      <category>ml</category>
      <category>image</category>
      <category>genai</category>
      <category>generativeai</category>
    </item>
    <item>
      <title>OpenAI Microscope</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We’re introducing OpenAI Microscope, a collection of visualizations of every significant layer and neuron of eight vision “model organisms” which are often studied in interpretability. Microscope makes it easier to analyze the features that form inside these neural networks, and we hope it will help the research community as we move towards understanding these complicated systems.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/openai-microscope</link>
      <guid>https://www.lqdev.me/bookmarks/openai-microscope</guid>
      <pubDate>2024-01-23 22:15 -05:00</pubDate>
      <category>openai</category>
      <category>interpretability</category>
      <category>ai</category>
      <category>generativeai</category>
      <category>visualization</category>
    </item>
    <item>
      <title>NightShade</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Nightshade, a tool that turns any image into a data sample that is unsuitable for model training. More precisely, Nightshade transforms images into &amp;quot;poison&amp;quot; samples, so that models training on them without consent will see their models learn unpredictable behaviors that deviate from expected norms, e.g. a prompt that asks for an image of a cow flying in space might instead get an image of a handbag floating in space.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://nightshade.cs.uchicago.edu/whatis.html"&gt;What is NightShade?&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/nightshade-ai-model-poisoning-tool</link>
      <guid>https://www.lqdev.me/responses/nightshade-ai-model-poisoning-tool</guid>
      <pubDate>2024-01-23 20:57 -05:00</pubDate>
      <category>research</category>
      <category>tools</category>
      <category>ai</category>
      <category>generativeai</category>
      <category>llm</category>
      <category>computervision</category>
      <category>cv</category>
    </item>
    <item>
      <title>Sampling for Text Generation</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;ML models are probabilistic. Imagine that you want to know what’s the best cuisine in the world. If you ask someone this question twice, a minute apart, their answers both times should be the same. If you ask a model the same question twice, its answer can change.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This probabilistic nature makes AI great for creative tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;However, this probabilistic nature also causes inconsistency and hallucinations. It’s fatal for tasks that depend on factuality. Recently, I went over 3 months’ worth of customer support requests of an AI startup I advise and found that ⅕ of the questions are because users don’t understand or don’t know how to work with this probabilistic nature.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;To understand why AI’s responses are probabilistic, we need to understand how models generate responses, a process known as sampling (or decoding). This post consists of 3 parts.
&lt;br&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Sampling: sampling strategies and sampling variables including temperature, top-k, and top-p.
2. Test time sampling: sampling multiple outputs to help improve a model’s performance.
3. Structured outputs: how to get models to generate outputs in a certain format.
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/sampling-text-generation-huyen</link>
      <guid>https://www.lqdev.me/bookmarks/sampling-text-generation-huyen</guid>
      <pubDate>2024-01-17 20:32 -05:00</pubDate>
      <category>ai</category>
      <category>generativeai</category>
      <category>statistics</category>
      <category>llm</category>
    </item>
    <item>
      <title>SingSong - Generating musical accompaniments from singing</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We present SingSong, a system that generates instrumental music to accompany input vocals, potentially offering musicians and non-musicians alike an intuitive new way to create music featuring their own voice. To accomplish this, we build on recent developments in musical source separation and audio generation. Specifically, we apply a state-of-the-art source separation algorithm to a large corpus of music audio to produce aligned pairs of vocals and instrumental sources. Then, we adapt AudioLM (Borsos et al., 2022) -- a state-of-the-art approach for unconditional audio generation -- to be suitable for conditional &amp;quot;audio-to-audio&amp;quot; generation tasks, and train it on the source-separated (vocal, instrumental) pairs. In a pairwise comparison with the same vocal inputs, listeners expressed a significant preference for instrumentals generated by SingSong compared to those from a strong retrieval baseline.
Sound examples at &lt;a href="https://g.co/magenta/singsong"&gt;this https URL&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;AI can now help you create a backing track to all the songs you make up about your pets.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/singsong-ai-music-accompaniment</link>
      <guid>https://www.lqdev.me/bookmarks/singsong-ai-music-accompaniment</guid>
      <pubDate>2024-01-17 19:39 -05:00</pubDate>
      <category>ai</category>
      <category>generativeai</category>
      <category>music</category>
    </item>
    <item>
      <title>What about instrumentals? AI Generated Spotify Clips Addendum</title>
      <description>&lt;![CDATA[&lt;p&gt;In my recent post, &lt;a href="https://www.lqdev.me/posts/generative-ai-spotify-clips"&gt;Using Generative AI to produce Spotify Clips&lt;/a&gt; I listed a bunch of challenges but realized I missed a big one. While listening to &lt;a href="https://www.youtube.com/watch?v=G0CqHxDG74k"&gt;IJ by Sam Gendel&lt;/a&gt;, I realized, what do you do about instrumental songs, solos, or songs with few lyrics?&lt;/p&gt;
&lt;p&gt;Some sort of audio-to-video technique?&lt;/p&gt;
&lt;p&gt;Maybe using metadata like title, genre, beats-per-minute (BPM), spectrogram, and musical notes would work?&lt;/p&gt;
&lt;p&gt;Or generating an image and animating it or its background to add depth similar to how in the &lt;a href="https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html"&gt;VideoPoet blog post&lt;/a&gt;, the image of the Mona Lisa is animated.&lt;/p&gt;
&lt;p&gt;It would be a fun experiment.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/genai-spotify-clips-instrumental</link>
      <guid>https://www.lqdev.me/notes/genai-spotify-clips-instrumental</guid>
      <pubDate>2023-12-23 19:32 -05:00</pubDate>
      <category>ai</category>
      <category>genai</category>
      <category>music</category>
      <category>instrumental</category>
      <category>spotify</category>
      <category>generativeai</category>
      <category>llm</category>
    </item>
    <item>
      <title>VideoPoet: A large language model for zero-shot video generation</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;VideoPoet is a simple modeling method that can convert any autoregressive language model or large language model (LLM) into a high-quality video generator. It contains a few simple components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A pre-trained MAGVIT V2 video tokenizer and a SoundStream audio tokenizer transform images, video, and audio clips with variable lengths into a sequence of discrete codes in a unified vocabulary. These codes are compatible with text-based language models, facilitating an integration with other modalities, such as text.&lt;/li&gt;
&lt;li&gt;An autoregressive language model learns across video, image, audio, and text modalities to autoregressively predict the next video or audio token in the sequence.&lt;/li&gt;
&lt;li&gt;A mixture of multimodal generative learning objectives are introduced into the LLM training framework, including text-to-video, text-to-image, image-to-video, video frame continuation, video inpainting and outpainting, video stylization, and video-to-audio. Furthermore, such tasks can be composed together for additional zero-shot capabilities (e.g., text-to-audio).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This simple recipe shows that language models can synthesize and edit videos with a high degree of temporal consistency. VideoPoet demonstrates state-of-the-art video generation, in particular in producing a wide range of large, interesting, and high-fidelity motions. The VideoPoet model supports generating videos in square orientation, or portrait to tailor generations towards short-form content, as well as supporting audio generation from a video input.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html"&gt;Blog Post&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/videopoet-llm-zero-shot-video-generation</link>
      <guid>https://www.lqdev.me/bookmarks/videopoet-llm-zero-shot-video-generation</guid>
      <pubDate>2023-12-23 19:06 -05:00</pubDate>
      <category>ai</category>
      <category>genai</category>
      <category>videopoet</category>
      <category>generativeai</category>
      <category>google</category>
    </item>
    <item>
      <title>Using Generative AI to produce Spotify Clips</title>
      <description>&lt;![CDATA[
## Introduction

While listening to music on Spotify last night, this thought hit me. It's been cooking for some time so now you get to read about it. Spotify and YouTube, if you build this into your platforms for songs or podcasts, [contact me](/contact) for info on where to send the check :joy:. As the title of the post suggests, this is a thought experiment on how you might use Generative AI to produce Spotify Clips. This is not limited to songs but could also be extended to audio-only podcasts. However, given the long-form nature of podcasts it might be extremely resource intensive to produce clips per episode. This post is not exactly a how-to guide, though there are some practical examples of how some of the steps in the workflow might work using [Microsoft Copilot (formerly Bing Chat)](https://copilot.microsoft.com/).

## Spotify Clips

Let's start off by talking about what Spotify Clips are. Back in March of 2023, [Spotify introduced Clips to their platform](https://artists.spotify.com/blog/spotify-clips-get-started-short-form-video-stream-on). Spotify describes [Clips](https://clips.byspotify.com/) as "short, under-30-second vertical videos that are uploaded directly to Spotify for Artists. They’re designed to put you and your music front-and-center, so they can be attached to your artist profile, to a song, to an album, or to an upcoming release.". When paired with a song, you can think of them as short music videos. Music videos being the videos made to accompany songs that played on TV channels like Music Television (MTV) before Jersey Shore and Catfish. Music videos are still in production today and they generally serve the same purpose. They provide a visual representation of the story being told by the song.  

## Generative AI for video

An overwhelming majority of Generative AI models only generate text. With models like DALL-E and Stable Diffusion, you can also generate images. Other models to generate audio and video have started coming on the scene. Most recently, Google announced [VideoPoet](https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html), a large language model for zero-shot video generation. There's many interesting techniques that VideoPoet introduces but I'll keep it to one of the more simple scenarios and the technique used in this post, text-to-image. Given a text prompt, VideoPoet can generate by default, a short 2-second clip. You can use VideoPoet as well to extend a clip so you could go well beyond the default 2 seconds. That however is outside the scope of this post. With that in mind, let's talk about how we might use a Generative AI model and apply it to the context of Spotify Clips.

## Generative AI Spotify Clips workflow

The following is a rough idea of how you might use song information like lyrics to 

```mermaid
flowchart TD
  A["Get song lyrics"] --&gt; B["Generate scenes and script"]
  B --&gt; C["Generate prompts for each scene"]
  C --&gt; D["Generate video clips"]
  D --&gt; E["(Optional) Edit clips"]
  D --&gt; F["Stitch clips together"]
  E--&gt;F
```

This is roughly what Google did in their VideoPoet samples. They "...produced a short movie composed of many short clips generated by the model. For the script, we asked Bard to write a series of prompts to detail a short story about a traveling raccoon. We then generated video clips for each prompt, and stitched together all resulting clips". The resulting video looks like the following:

[![An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities](http://img.youtube.com/vi/70wZKfx6Ylk/0.jpg)](https://www.youtube.com/watch?v=70wZKfx6Ylk "An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities")

One thing to note is that the videos generated are YouTube Shorts which are vertical as well. This makes it easier to produce videos in the format Spotify expects Clips to be in.

## AI Generated Spotify Clips in practice

Now that we know the general process, what does it actually look like in practice? To help me with that, I used Microsoft Copilot.

### Generate scenes and script

In the example shared by Google, they used a prompt to generate a script made up of a series of prompts. In the case of songs on Spotify, we can use that as context and the starting point for our script. From there, we can ask Copilot to compose scenes for the verses in the song. We can also make sure as these scenes are generated, that there is a smooth transition between them and the storytelling is consistent. 

Let's take a look at what this might look like using the Chicano Batman song "A Hundred Dead and Loving Souls". Why that one? Just check out my [Spotify Wrapped](/notes/spotify-wrapped-2023). The general theme of that song is about the cycles of life experienced across millenia and generations. 

**Prompt**

Given the following prompt:

```text
Using the song information below as context, generate a script for a story that visually represents the song lyrics. Ensure that the script is broken down into individual scenes and the scenes are linear and cohesively transition.

&lt;song&gt;
&lt;title&gt;A Hundred Dead and Loving Souls&lt;/title&gt;
&lt;lyrics&gt;
I am the sand stuck under miles of rock
watching the dry air sweep away my thoughts
as we travel protected by shards of glass
driving through the history of a million memories

In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
Climbing a thousand unclimbed trees
I don't know I want to know how it would feel
to be on top of one of these!

I am the root
quickly ascending
up through the surrounding dirt
quickly devoured by a hungry sun
quickly devoured by a hungry son
who is my father
and we are alike
we look alike we talk alike
we are alike
we are alike
we are alike we look alike
we talk alike

Thats why Im hungry
like the bellowing sky
this is no lie
as sure as light
entering eye
want to stay high
everything's cool too cool so cool
and I remember that everything falls

So I take a look around me now, as I speak
and I imagine a disinterested crowd
talking aloud
and I think of what I want most
and the crowd disappears
and a fixated gaze envelopes my wondering mind
and then she recruits and takes me to this place
I've been there before and that's where I saw her face

In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
Climbing a thousand unclimbed trees
I don't know I want to know how it would feel
to be on top of one of these!
&lt;/lyrics&gt;
&lt;/song&gt;

Story: 
```

**Response**

The response looks like the following:

```text
# Script for A Hundred Dead and Loving Souls

## Scene 1: The Sand
- A close-up shot of a grain of sand under a microscope, showing its intricate structure and texture.
- The camera zooms out to reveal that the sand is part of a vast desert landscape, with dunes and rocks stretching to the horizon.
- A voice-over of the protagonist (a young man) narrates the first verse of the song, expressing his feelings of being trapped and forgotten under the weight of the world.
- The camera pans to show a car driving on a dusty road, with the protagonist and his father inside. They are both wearing sunglasses and hats to protect themselves from the sun.
- The car is filled with boxes and bags, indicating that they are moving to a new place.
- The father is driving and listening to the radio, while the protagonist is looking out the window, bored and restless.
- The radio plays a news report about a nuclear fusion experiment in South Korea, followed by a song by a popular band.
- The father comments on the news, saying that it is a breakthrough for humanity and that he hopes to see it in his lifetime.
- The protagonist does not respond, but thinks to himself that he does not care about anything anymore.

## Scene 2: The Sea
- A montage of shots showing the protagonist and his father arriving at their new home, a small town by the sea.
- The father is excited and optimistic, while the protagonist is indifferent and detached.
- The father unpacks their belongings and sets up their new house, while the protagonist wanders around the town, exploring the beach, the pier, and the streets.
- The protagonist sees a group of teenagers hanging out at a skate park, laughing and having fun. He feels out of place and lonely.
- The protagonist walks to the edge of the pier, where he sees a sign that says "No diving".
- He looks down at the water, where he sees hundreds of dead fish floating on the surface, victims of a recent oil spill.
- He hears the chorus of the song in his head, imagining himself diving into the sea of a hundred dead and loving souls, and climbing a thousand unclimbed trees.
- He wonders what it would feel like to be free and alive, instead of being stuck and dead inside.

## Scene 3: The Root
- A flashback scene, showing the protagonist as a child, playing in a garden with his father.
- The father is a botanist, and he teaches his son about different plants and flowers, showing him how to care for them and appreciate their beauty.
- The son is curious and eager, and he asks his father many questions about the natural world.
- The father tells his son that he is a root, quickly ascending up through the surrounding dirt, and that he will grow into a strong and beautiful tree one day.
- The son smiles and hugs his father, saying that he loves him and that he wants to be like him.
- The father smiles and hugs his son back, saying that he loves him too and that he is proud of him.

## Scene 4: The Sun
- A flash-forward scene, showing the protagonist as a young adult, working in a laboratory with his father.
- The father is a nuclear physicist, and he is working on a project to harness the power of nuclear fusion, the same process that fuels the sun.
- The son is his assistant, and he helps his father with the experiments and calculations, following his instructions and commands.
- The father tells his son that he is a hungry sun, quickly devoured by a hungry son, and that he will make a great contribution to science and humanity one day.
- The son nods and agrees, but he does not look happy or satisfied. He feels pressured and obligated to follow his father's footsteps, even though he does not share his passion or vision.
- The son thinks to himself that he is hungry, like the bellowing sky, and that he wants to stay high, away from the reality and expectations that weigh him down.

## Scene 5: The Crowd
- A present-day scene, showing the protagonist at a party, surrounded by a crowd of people.
- The party is hosted by one of the teenagers that he saw at the skate park, who invited him out of pity and curiosity.
- The party is loud and chaotic, with music, drinks, and drugs everywhere.
- The protagonist feels uncomfortable and anxious, and he does not fit in with the crowd. He does not know anyone, and no one knows him.
- He tries to make small talk with some of the guests, but they are either rude, bored, or high, and they do not pay attention to him.
- He feels invisible and insignificant, and he wishes he could disappear.
- He takes a look around him, as he speaks, and he imagines a disinterested crowd, talking aloud, and he thinks of what he wants most.

## Scene 6: The Gaze
- A close-up shot of a pair of eyes, staring intently at the protagonist.
- The eyes belong to a girl, who is also at the party, but who is different from the rest of the crowd. She is quiet and mysterious, and she has a captivating and mysterious aura.
- The girl notices the protagonist, and she is intrigued by him. She sees something in him that no one else sees, something that she relates to and understands.
- She walks up to him, and she introduces herself. She says her name is Luna, and that she likes his song. She says she heard him humming it to himself, and that she knows the lyrics by heart.
- The protagonist is surprised and confused, and he asks her how she knows his song. He says he wrote it himself, and that he never shared it with anyone.
- The girl smiles and says that she knows his song, because she wrote it too. She says she has a gift, or a curse, depending on how you look at it. She says she can hear the songs that people sing in their souls, and that she can sing them back to them.
- The protagonist is shocked and amazed, and he asks her to prove it. He asks her to sing his song, or any song that he knows.
- The girl nods and agrees, and she sings his song, word for word, note for note, perfectly. She sings it with emotion and expression, as if she wrote it herself.
- The protagonist is speechless and mesmerized, and he feels a connection with the girl that he never felt with anyone before. He feels like she knows him, and he wants to know her.
- The girl stops singing, and she looks at him with a fixated gaze, that envelopes his wondering mind. She says she likes him, and she asks him if he wants to go with her to a place that she knows.
- The protagonist nods and agrees, and he follows her out of the party, leaving the crowd behind.

## Scene 7: The Place
- A montage of shots showing the protagonist and the girl driving to a secluded spot in the woods, where they park their car and get out.
- The spot is a clearing, where there is a large tree, with a rope swing hanging from one of its branches.
- The tree is the only one of its kind in the area, and it stands out from the rest of the forest. It is tall and majestic, with green leaves and red flowers.
- The girl tells the protagonist that this is her place, and that she comes here often to escape from the world. She says she loves this tree, and that she calls it Luna, after herself.
- The protagonist tells the girl that this is his place too, and that he has been here before. He says he saw this tree in his dreams, and that he always wanted to climb it.
- The girl smiles and says that they have more in common than they thought, and that they are meant to be together. She says she wants to show him something, and she asks him to follow her.
- The girl climbs the tree, using the rope swing as a support. She is agile and graceful, and she reaches the top in no time.
- The protagonist follows her, using the rope swing as well. He is clumsy and nervous, and he struggles to keep up with her.
- The girl reaches the top of the tree, where she finds a small platform, made of wood and rope. She sits on the platform, and she looks at the view.
- The view is breathtaking, and it shows the sea, the town, and the sky, all in one panorama. The sun is setting, and it casts a golden glow on everything.
- The girl waves at the protagonist, and she invites him to join her. She says this is the best place to watch the sunset, and that she wants to share it with him.
- The protagonist reaches the top of the tree, where he joins the girl on the platform. He sits next to her, and he looks at the view. He is awestruck, and he agrees that this is the best place to watch the sunset.
- The girl leans in and kisses the protagonist, and he kisses her back. They embrace each other, and they watch the sunset together.
- The chorus of the song plays in the background, as the scene fades to black.
```

Not too bad for a first pass. From here you might want to continue tweaking the different scenes but at least you have a starting point. 

### Generate prompts for each scene

Using the text-to-image technique, I first need to generate prompts to guide the model in the video generation process. In the case of this sample, you could take each of the bullet points in the scenes and generate a video for each of them. 

At the time of this writing, I didn't have access to a video generation model and I don't think VideoPoet is open to the public yet. Therefore, I had to slightly modify what I was generating. Instead of generating a video, I used Copilot to generate images using DALL-E. The concept is the same, except instead of generating videos, I'm generating still images. 

I could've applied the same concept of one image per bullet point. However, for brevity, I chose to summarize all of the bullet points in a scene into a single prompt. 

**Prompt**

Given the following prompt:

```text
Using storyboarding techniques, for each scene, generate a single prompt to use as input for DALL-E to generate an image. Since there are 7 scenes, there should be only 7 prompts. Distill as much of the concepts from each of the bullet points in each scene.
```

**Response**

The results from that prompt are the following:

```text
Scene 1: The Sand
    A grain of sand under a microscope and a car driving on a dusty road in a desert
Scene 2: The Sea
    A boy standing on a pier looking at a sea of dead fish and a sign that says “No diving”
Scene 3: The Root
    A father and a son playing in a garden with different plants and flowers
Scene 4: The Sun
    A father and a son working in a laboratory with a nuclear fusion reactor
Scene 5: The Crowd
    A boy at a party surrounded by a crowd of people with music, drinks, and drugs
Scene 6: The Gaze
    A girl with a pair of eyes that can hear the songs that people sing in their souls
Scene 7: The Place
    A boy and a girl sitting on a platform on top of a tree with a rope swing and a view of the sea, the town, and the sunset
```

Again, not too bad for a first pass. The main points of the scene are captured. I can already foresee there are a few scenes like "The Sand" which might be difficult to illustrate in a single image. 

### Generate images (video clips)

Now that we have individual prompts, it's time to generate some images. Again, I'm doing this because I don't have access to a video generation model but the process would be generally the same. 

**Prompt**

```text
Use the prompts for each of the scenes to generate images with DALL-E. Render them as pencil art.
```

**Response**

For brevity, I'll only include some of the images generated:

"The Sand"

![AI genreated image of a grain of sand under a microscope and a car driving on a dusty road in a desert](http://cdn.lqdev.tech/files/images/sand-car-dirt-road.jpg)

"The Sea"

![AI generated image of a boy standing on a pier looking at a sea of dead fish and a sign that says “No diving”](http://cdn.lqdev.tech/files/images/child-dock.jpg)

"The Root"

![AI generated image of a father and a son playing in a garden with different plants and flowers”](http://cdn.lqdev.tech/files/images/father-son-playing.jpg)

The pencil art part was something I thought would look nice to illustrate this instead of digital art or photorealism. That was a personal choice though. 

As we can see from the images, in general they're not too bad. When not condensed into a single image though and you have an image or video per frame in a scene, the generated content can be richer. 

### Edit and stitch videos

For the image generation example, this seems like a natural stopping point. However, with videos, you can imagine using AI to edit these videos. The VideoPoet post mentions how you might be able to do that. Part of that editing might also include lengthening the video clips. Once you're happy with your videos, you would then stitch them together to compose a larger clip. 

## Improvements

Tons! I used the simplest tools available at hand to validate whether my idea is feasible.  

### Videos

The most obvious improvement here would be to actually generate the videos. Whether that's using VideoPoet or another video generation model, the whole point is to end up with a video clip to upload to Spotify.

### Prompts

The prompts I used were not complex at all. I tried to keep them as simple as possible and in many cases, I wasn't the one providing the prompts, Copilot generated them for me. I would try to use more complex prompts to further guide the model in the formats I expect the outputs to be in and give more details for the model to generate better results. 

### Code

All of this was done using Copilot in the browser. As a starting point to validate my ideas this was fine. However, this approach is not composable, testable, reproducible, or scalable. I'd convert these steps to code in order to get all of the things Copilot doesn't give me. 

### Timestamp sync

One of the features Spotify offers is lyrics that are synced to the song. Similarly, being able to sync these generated clips to the song would be important so the visuals match the 

## Challenges

### Consistency

One challenge that someone brought up when I was explaining this idea was, how do you ensure that the visuals are consistent throughout all of the video clips.

That is a great question I don't have a great answer for, though I have ideas. 

If there aren't many changes throughout scenes, you could just opt to keep lengthening the video clip. 

If there are flashbacks or different visuals between scenes, you might achieve consistency by  using the image-to-video modality. Using a snapshot from a frame in a previous video clip, you might try to edit that snapshot to use in between transitions and then lengthen the video to produce the next scene. 

### Power Consumption

As fascinating as this is, is it worth the energy and cooling demands it requires when using large models? Maybe not in its current form, but maybe it makes sense when using smaller models or more power efficient compute. 

### People

For music videos, there's a human element to it both from the performers and the people who produce them. Again, the Generative AI is impressive and for someone like me without any artistic skills, it can be an empowering tool. Although I'm tempted to see what an [AI Rickroll](https://www.youtube.com/watch?v=dQw4w9WgXcQ) might look like, I prefer the human version. :slightly_smiling_face:

## Conclusion

In this post I discussed how you might use Generative AI models like VideoPoet to generate short videos for Spotify Clips. Since I don't have acess to VideoPoet or other video generation models at this time, I decided to show practical image generation examples instead using models like DALL-E to illustrate the general concepts. It's not wihtout its challenges and there's a ton of improvements I can think of but it was a fun experiment regardless. What else would you use these Generative AI models for? 

You can find an addendum to this post [here](/notes/genai-spotify-clips-instrumental).]]&gt;</description>
      <link>https://www.lqdev.me/posts/generative-ai-spotify-clips</link>
      <guid>https://www.lqdev.me/posts/generative-ai-spotify-clips</guid>
      <pubDate>2023-12-23 18:30 -05:00</pubDate>
      <category>ai</category>
      <category>video</category>
      <category>chicanobatman</category>
      <category>generativeai</category>
      <category>genai</category>
      <category>spotify</category>
      <category>google</category>
      <category>microsoft</category>
      <category>bing</category>
      <category>copilot</category>
      <category>dalle</category>
      <category>image</category>
      <category>prompt</category>
      <category>projectidea</category>
      <category>music</category>
      <category>podcasts</category>
      <category>llm</category>
    </item>
    <item>
      <title>PointLLM: Empowering Large Language Models to Understand Point Clouds</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We introduce PointLLM, a multi-modal large language model capable of understanding colored point clouds of objects. It perceives object types, geometric structures, and appearance without concerns for ambiguous depth, occlusion, or viewpoint dependency. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establish two benchmarks: Generative 3D Object Classification and 3D Object Captioning, assessed through three different evaluation methods.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/point-llm-point-clouds</link>
      <guid>https://www.lqdev.me/bookmarks/point-llm-point-clouds</guid>
      <pubDate>2023-09-17 12:52 -05:00</pubDate>
      <category>ai</category>
      <category>llm</category>
      <category>generativeai</category>
      <category>multimodal</category>
      <category>research</category>
    </item>
    <item>
      <title>How consumers are using Generative AI</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;1. Most leading products are built from the “ground up” around generative AI&lt;/strong&gt;&lt;br /&gt;
Like ChatGPT, the majority of products on this list didn’t exist a year ago—80% of these websites are new
Of the 50 companies on the list, only 5 are products of, or acquisitions by, pre-existing big tech companies...
Of the remaining list members, a whopping 48% are completely bootstrapped, with no outside funding, according to PitchBook data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;2. ChatGPT has a massive lead, for now…&lt;/strong&gt;&lt;br /&gt;
ChatGPT represents 60% of monthly traffic to the entire top 50 list, with an estimated 1.6 billion monthly visits and 200 million monthly users (as of June 2023). This makes ChatGPT the 24th most visited website globally.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;3. LLM assistants (like ChatGPT) are dominant, but companionship and creative tools are on the rise&lt;/strong&gt;&lt;br /&gt;
General LLM chatbots represent 68% of total consumer traffic to the top 50 list.
However, two other categories have started to drive significant usage in recent months—AI companions (such as CharacterAI) and content generation tools (such as Midjourney and ElevenLabs). Within the broader content generation category, image generation is the top use case with 41% of traffic, followed by prosumer writing tools at 26%, and video generation at 8%.
Another category worth mentioning? Model hubs. There are only 2 on the list, but they drive significant traffic—Civitai (for images) and Hugging Face both rank in the top 10. This is especially impressive because consumers are typically visiting these sites to download models to run locally, so web traffic is likely an underestimate of actual usage.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;4. Early “winners” have emerged, but most product categories are up for grabs&lt;/strong&gt;&lt;br /&gt;
Good news for builders: despite the surge in interest in generative AI, in many categories there is not yet a runway success.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;5. Acquisition for top products is entirely organic—and consumers are willing to pay!&lt;/strong&gt;&lt;br /&gt;
The majority of companies on this list have no paid marketing (at least, that SimilarWeb is able to attribute). There is significant free traffic “available” via X, Reddit, Discord, and email, as well as word of mouth and referral growth.
And consumers are willing to pay for GenAI. 90% of companies on the list are already monetizing, nearly all of them via a subscription model. The average product on the list makes $21/month (for users on monthly plans)—yielding $252 annually.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;6. Mobile apps are still emerging as a GenAI platform&lt;/strong&gt;&lt;br /&gt;
Consumer AI products have, thus far, been largely browser-first, rather than app-first. Even ChatGPT took 6 months to launch a mobile app!
Why aren’t more AI companies building on mobile? The browser is a natural starting place to reach the broadest base of consumers. Many AI companies have small teams and likely don’t want to fragment their focus and resources across Web, iOS, and Android.
Given that the average consumer now spends 36 minutes more per day on mobile than desktop (4.1 hours vs. 3.5 hours), we expect to see more mobile-first GenAI products emerge as the technology matures.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/a16z-how-consumers-using-generative-ai</link>
      <guid>https://www.lqdev.me/bookmarks/a16z-how-consumers-using-generative-ai</guid>
      <pubDate>2023-09-17 12:40 -05:00</pubDate>
      <category>ai</category>
      <category>generativeai</category>
      <category>product</category>
      <category>business</category>
      <category>venturecapital</category>
      <category>vc</category>
    </item>
    <item>
      <title>Coqui 🐸 XTTS</title>
      <description>&lt;![CDATA[[bookmark] &lt;p&gt;&lt;a href="https://huggingface.co/coqui/XTTS-v1"&gt;XTTS&lt;/a&gt; is a Voice generation model that lets you clone voices into different languages by using just a quick 3-second audio clip.&lt;/p&gt;
&lt;p&gt;XTTS is built on previous research, like Tortoise, with additional architectural innovations and training to make cross-language voice cloning and multilingual speech generation possible.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/coqui-xtts-text-to-speech</link>
      <guid>https://www.lqdev.me/bookmarks/coqui-xtts-text-to-speech</guid>
      <pubDate>2023-09-17 12:25 -05:00</pubDate>
      <category>ai</category>
      <category>generativeai</category>
      <category>speech</category>
      <category>huggingface</category>
      <category>ml</category>
      <category>tts</category>
      <category>opensource</category>
    </item>
    <item>
      <title>Introducing Würstchen: Fast Diffusion for Image Generation </title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Würstchen is a diffusion model, whose text-conditional component works in a highly compressed latent space of images. Why is this important? Compressing data can reduce computational costs for both training and inference by orders of magnitude. Training on 1024×1024 images is way more expensive than training on 32×32. Usually, other works make use of a relatively small compression, in the range of 4x - 8x spatial compression. Würstchen takes this to an extreme. Through its novel design, it achieves a 42x spatial compression! This had never been seen before, because common methods fail to faithfully reconstruct detailed images after 16x spatial compression. Würstchen employs a two-stage compression, what we call Stage A and Stage B. Stage A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found in the  paper). Together Stage A and B are called the Decoder, because they decode the compressed images back into pixel space. A third model, Stage C, is learned in that highly compressed latent space. This training requires fractions of the compute used for current top-performing models, while also allowing cheaper and faster inference. We refer to Stage C as the Prior.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/wurstchen-fast-image-generation</link>
      <guid>https://www.lqdev.me/bookmarks/wurstchen-fast-image-generation</guid>
      <pubDate>2023-09-17 12:15 -05:00</pubDate>
      <category>ai</category>
      <category>generativeai</category>
      <category>imagegeneration</category>
      <category>huggingface</category>
      <category>diffusion</category>
      <category>ml</category>
    </item>
  </channel>
</rss>