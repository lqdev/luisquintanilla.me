<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - evaluation</title>
    <link>https://www.lqdev.me/tags/evaluation</link>
    <description>All content tagged with 'evaluation' by Luis Quintanilla</description>
    <lastBuildDate>2024-02-29 10:43 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Predictive Human Preference: From Model Ranking to Model Routing</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Human preference has emerged to be both the Northstar and a powerful tool for AI model development. Human preference guides post-training techniques including RLHF and DPO. Human preference is also used to rank AI models, as used by LMSYS’s Chatbot Arena.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Chatbot Arena aims to determine which model is generally preferred. I wanted to see if it’s possible to do predictive human preference: determine which model is preferred for each query.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This post first discusses the correctness of Chatbot Arena, which will then be used as a baseline to evaluate the correctness of preference predictions. It then discusses how to build a preference predictor and the initial results.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/predictive-human-preference-model-ranking-routing</link>
      <guid>https://www.lqdev.me/responses/predictive-human-preference-model-ranking-routing</guid>
      <pubDate>2024-02-29 10:43 -05:00</pubDate>
      <category>ai</category>
      <category>llm</category>
      <category>evaluation</category>
      <category>ml</category>
    </item>
    <item>
      <title>Evaluating LLMs is a minefield</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;...many things can go wrong when we are trying to evaluate LLMs’ performance on a certain task or behavior in a certain scenario.&lt;/p&gt;
&lt;p&gt;It has big implications for reproducibility: both for research on LLMs and research that uses LLMs to answer a question in social science or any other field.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/evaluating-llms-minefield</link>
      <guid>https://www.lqdev.me/bookmarks/evaluating-llms-minefield</guid>
      <pubDate>2023-12-11 20:08 -05:00</pubDate>
      <category>ai</category>
      <category>llms</category>
      <category>evaluation</category>
    </item>
    <item>
      <title>Best Practices for LLM Evaluation of RAG Applications</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;This blog represents the first in a series of investigations we’re running at Databricks to provide learnings on LLM evaluation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Recently, the LLM community has been exploring the use of “LLMs as a judge” for automated evaluation with many using powerful LLMs such as GPT-4 to do the evaluation for their LLM outputs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Using the Few Shots prompt with GPT-4 didn’t make an obvious difference in the consistency of results.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Including few examples for GPT-3.5-turbo-16k significantly improves the consistency of the scores, and makes the result usable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...evaluation results can’t be transferred between use cases and we need to build use-case-specific benchmarks in order to properly evaluate how good a model can meet customer needs.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/best-practices-rag-application-evaluation-databricks</link>
      <guid>https://www.lqdev.me/bookmarks/best-practices-rag-application-evaluation-databricks</guid>
      <pubDate>2023-12-11 19:54 -05:00</pubDate>
      <category>ai</category>
      <category>rag</category>
      <category>evaluation</category>
      <category>llm</category>
    </item>
    <item>
      <title>MLflow 2.8 with LLM-as-a-judge metrics and Best Practices for LLM Evaluation of RAG Applications</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;LLM-as-a-judge is one promising tool in the suite of evaluation techniques necessary to measure the efficacy of LLM-based applications.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/mlflow-2-8-llm-as-judge-rag-evaluation</link>
      <guid>https://www.lqdev.me/bookmarks/mlflow-2-8-llm-as-judge-rag-evaluation</guid>
      <pubDate>2023-12-11 19:51 -05:00</pubDate>
      <category>mlflow</category>
      <category>ai</category>
      <category>llm</category>
      <category>evaluation</category>
    </item>
  </channel>
</rss>