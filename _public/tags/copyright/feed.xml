<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - copyright</title>
    <link>https://www.lqdev.me/tags/copyright</link>
    <description>All content tagged with 'copyright' by Luis Quintanilla</description>
    <lastBuildDate>2024-04-10 22:00 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>DE-COP: Detecting Copyrighted Content in Language Models Training Data</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give â‰ˆ 4% accuracy. Our code and datasets are available at this https URL&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/avduarte333/DE-COP_Method"&gt;Repo&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/decop-detecting-copyright-llm-training-data</link>
      <guid>https://www.lqdev.me/bookmarks/decop-detecting-copyright-llm-training-data</guid>
      <pubDate>2024-04-10 22:00 -05:00</pubDate>
      <category>ai</category>
      <category>copyright</category>
      <category>research</category>
      <category>llm</category>
      <category>data</category>
    </item>
  </channel>
</rss>