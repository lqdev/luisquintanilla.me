<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - math</title>
    <link>https://www.lqdev.me/tags/math</link>
    <description>All content tagged with 'math' by Luis Quintanilla</description>
    <lastBuildDate>2024-07-29 22:26 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Tensors from scratch series</title>
      <description>&lt;![CDATA[[star] &lt;p&gt;I've been enjoying reading through this Tensors series.&lt;/p&gt;
&lt;p&gt;If you're interested, here's also the link to &lt;a href="https://maharshi.bearblog.dev/tensors-from-scratch-part-1/"&gt;part 1&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/tensors-from-scratch-blog-series</link>
      <guid>https://www.lqdev.me/responses/tensors-from-scratch-blog-series</guid>
      <pubDate>2024-07-29 22:26 -05:00</pubDate>
      <category>tensors</category>
      <category>math</category>
      <category>ai</category>
      <category>c</category>
      <category>machinelearning</category>
      <category>ml</category>
      <category>artificialintelligence</category>
    </item>
    <item>
      <title>Calculus Made Easy</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Calculus Made Easy is a book on calculus originally published in 1910 by Silvanus P. Thompson, considered a classic and elegant introduction to the subject.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.gutenberg.org/ebooks/33283"&gt;Project Gutenberg PDF&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/calculus-made-easy</link>
      <guid>https://www.lqdev.me/bookmarks/calculus-made-easy</guid>
      <pubDate>2024-04-25 23:01 -05:00</pubDate>
      <category>calculus</category>
      <category>book</category>
      <category>math</category>
    </item>
    <item>
      <title>Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We introduce a new algorithm called the Free-pipeline Fast Inner Product (FFIP) and its hardware architecture that improve an under-explored fast inner-product algorithm (FIP) proposed by Winograd in 1968. Unlike the unrelated Winograd minimal filtering algorithms for convolutional layers, FIP is applicable to all machine learning (ML) model layers that can mainly decompose to matrix multiplication, including fully-connected, convolutional, recurrent, and attention/transformer layers. We implement FIP for the first time in an ML accelerator then present our FFIP algorithm and generalized architecture which inherently improve FIP's clock frequency and, as a consequence, throughput for a similar hardware cost. Finally, we contribute ML-specific optimizations for the FIP and FFIP algorithms and architectures. We show that FFIP can be seamlessly incorporated into traditional fixed-point systolic array ML accelerators to achieve the same throughput with half the number of multiply-accumulate (MAC) units, or it can double the maximum systolic array size that can fit onto devices with a fixed hardware budget. Our FFIP implementation for non-sparse ML models with 8 to 16-bit fixed-point inputs achieves higher throughput and compute efficiency than the best-in-class prior solutions on the same type of compute platform.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/trevorpogue/algebraic-nnhw"&gt;GitHub&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/fast-inner-product-dnn-accelerators</link>
      <guid>https://www.lqdev.me/bookmarks/fast-inner-product-dnn-accelerators</guid>
      <pubDate>2024-03-17 20:47 -05:00</pubDate>
      <category>ai</category>
      <category>math</category>
      <category>acceleration</category>
      <category>neuralnetworks</category>
      <category>dnn</category>
    </item>
  </channel>
</rss>