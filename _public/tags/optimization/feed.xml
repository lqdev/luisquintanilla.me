<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - optimization</title>
    <link>https://www.lqdev.me/tags/optimization</link>
    <description>All content tagged with 'optimization' by Luis Quintanilla</description>
    <lastBuildDate>2024-04-25 22:48 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>SAMMO: A general-purpose framework for prompt optimization</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large language models (LLMs) have revolutionized a wide range of tasks and applications that were previously reliant on manually crafted machine learning (ML) solutions, streamlining through automation. However, despite these advances, a notable challenge persists: the need for extensive prompt engineering to adapt these models to new tasks. New generations of language models like GPT-4 and Mixtral 8x7B advance the capability to process long input texts. This progress enables the use of longer inputs, providing richer context and detailed instructions to language models. A common technique that uses this enhanced capacity is the Retrieval Augmented Generation (RAG) approach. RAG dynamically incorporates information into the prompt based on the specific input example.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;To address these challenges, we developed the Structure-Aware Multi-objective Metaprompt Optimization (SAMMO) framework. SAMMO is a new open-source tool that streamlines the optimization of prompts, particularly those that combine different types of structural information like in the RAG example above. It can make structural changes, such as removing entire components or replacing them with different ones. These features enable AI practitioners and researchers to efficiently refine their prompts with little manual effort.&lt;br /&gt;
&lt;br&gt;
Central to SAMMO’s innovation is its approach to treating prompts not just as static text inputs but as dynamic, programmable entities—metaprompts. SAMMO represents these metaprompts as function graphs, where individual components and substructures can be modified to optimize performance, similar to the optimization process that occurs during traditional program compilation.&lt;br /&gt;
&lt;br&gt;
The following key features contribute to SAMMO’s effectiveness:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Structured optimization: Unlike current methods that focus on text-level changes, SAMMO focuses on optimizing the structure of metaprompts. This granular approach facilitates precise modifications and enables the straightforward integration of domain knowledge, for instance, through rewrite operations targeting specific stylistic objectives.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Multi-objective search: SAMMO’s flexibility enables it to simultaneously address multiple objectives, such as improving accuracy and computational efficiency. Our paper illustrates how SAMMO can be used to compress prompts without compromising their accuracy.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;General purpose application: SAMMO has proven to deliver significant performance improvements across a variety of tasks, including instruction tuning, RAG, and prompt compression.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/sammo-prompt-optimization-framework</link>
      <guid>https://www.lqdev.me/responses/sammo-prompt-optimization-framework</guid>
      <pubDate>2024-04-25 22:48 -05:00</pubDate>
      <category>ai</category>
      <category>microsoft</category>
      <category>research</category>
      <category>prompt</category>
      <category>optimization</category>
      <category>sammo</category>
    </item>
    <item>
      <title>Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;In &lt;a href="https://arxiv.org/abs/2305.02301"&gt;“Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes”&lt;/a&gt;, presented at ACL2023, we set out to tackle this trade-off between model size and training data collection cost. We introduce distilling step-by-step, a new simple mechanism that allows us to train smaller task-specific models with much less training data than required by standard fine-tuning or distillation approaches that outperform few-shot prompted LLMs’ performance. We demonstrate that the distilling step-by-step mechanism enables a 770M parameter T5 model to outperform the few-shot prompted 540B PaLM model using only 80% of examples in a benchmark dataset, which demonstrates a more than 700x model size reduction with much less training data required by standard approaches.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/distilling-step-x-step-outperforming-large-models-small-data</link>
      <guid>https://www.lqdev.me/bookmarks/distilling-step-x-step-outperforming-large-models-small-data</guid>
      <pubDate>2023-09-25 20:45 -05:00</pubDate>
      <category>ai</category>
      <category>llm</category>
      <category>slm</category>
      <category>optimization</category>
    </item>
  </channel>
</rss>