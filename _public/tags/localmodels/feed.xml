<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - localmodels</title>
    <link>https://www.lqdev.me/tags/localmodels</link>
    <description>All content tagged with 'localmodels' by Luis Quintanilla</description>
    <lastBuildDate>2024-02-19 16:13 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Ollama - Windows Preview</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Ollama is now available on Windows in preview, making it possible to pull, run and create large language models in a new native Windows experience. Ollama on Windows includes built-in GPU acceleration, access to the full model library, and the Ollama API including OpenAI compatibility.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://ollama.com/download/windows"&gt;Download (https://ollama.com/download/windows)&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/ollama-windows-preview</link>
      <guid>https://www.lqdev.me/responses/ollama-windows-preview</guid>
      <pubDate>2024-02-19 16:13 -05:00</pubDate>
      <category>ollama</category>
      <category>windows</category>
      <category>llm</category>
      <category>opensource</category>
      <category>localmodels</category>
      <category>ml</category>
      <category>ai</category>
    </item>
  </channel>
</rss>