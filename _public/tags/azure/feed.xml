<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - azure</title>
    <link>https://www.lqdev.me/tags/azure</link>
    <description>All content tagged with 'azure' by Luis Quintanilla</description>
    <lastBuildDate>2025-08-14 13:35 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Deploy Owncast to Azure Container Apps with Persistent Storage</title>
      <description>&lt;![CDATA[&lt;h2&gt;Description&lt;/h2&gt;
&lt;p&gt;This guide shows how to deploy Owncast to Azure Container Apps with persistent storage, scale-to-zero capability, and proper SQLite compatibility to minimize costs while ensuring reliable operation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ultra-cost-optimized configuration (~$0.60/month for 4 hours of streaming)&lt;/li&gt;
&lt;li&gt;Full data persistence (users, chat, federation, configuration)&lt;/li&gt;
&lt;li&gt;Scale-to-zero capability when not streaming&lt;/li&gt;
&lt;li&gt;SQLite + Azure Files compatibility fixes&lt;/li&gt;
&lt;li&gt;RTMP + HTTP dual-port configuration for OBS Studio&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Azure CLI installed and logged in&lt;/li&gt;
&lt;li&gt;An Azure subscription&lt;/li&gt;
&lt;li&gt;A resource group created&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Step 1: Create Required Resources&lt;/h2&gt;
&lt;h3&gt;Create Storage Account for Persistent Data&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# Set variables (choose cheapest regions)
RESOURCE_GROUP=&amp;quot;your-resource-group&amp;quot;
LOCATION=&amp;quot;centralus&amp;quot;  # Often cheaper than eastus
STORAGE_ACCOUNT=&amp;quot;owncaststorage$(date +%s)&amp;quot;  # Must be globally unique
CONTAINER_APP_ENV=&amp;quot;owncast-env&amp;quot;
CONTAINER_APP_NAME=&amp;quot;owncast-app&amp;quot;

# Create MINIMAL cost storage account
az storage account create \
  --name $STORAGE_ACCOUNT \
  --resource-group $RESOURCE_GROUP \
  --location $LOCATION \
  --sku Standard_LRS \
  --kind StorageV2 \
  --access-tier Cool \
  --allow-blob-public-access false \
  --https-only true \
  --min-tls-version TLS1_2

# Create file share with minimal provisioned size
az storage share create \
  --name &amp;quot;owncast-data&amp;quot; \
  --account-name $STORAGE_ACCOUNT \
  --quota 1  # Start with 1GB, scales automatically
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create Container Apps Environment&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create the Container Apps environment
az containerapp env create \
  --name $CONTAINER_APP_ENV \
  --resource-group $RESOURCE_GROUP \
  --location $LOCATION
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Step 2: Configure Storage Mount&lt;/h2&gt;
&lt;p&gt;Get the storage account key and create the storage mount:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get storage account key
STORAGE_KEY=$(az storage account keys list \
  --account-name $STORAGE_ACCOUNT \
  --resource-group $RESOURCE_GROUP \
  --query &amp;quot;[0].value&amp;quot; -o tsv)

# Create storage mount in the environment
az containerapp env storage set \
  --name $CONTAINER_APP_ENV \
  --resource-group $RESOURCE_GROUP \
  --storage-name &amp;quot;owncast-storage&amp;quot; \
  --azure-file-account-name $STORAGE_ACCOUNT \
  --azure-file-account-key $STORAGE_KEY \
  --azure-file-share-name &amp;quot;owncast-data&amp;quot; \
  --access-mode ReadWrite
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Step 3: Deploy Owncast Container App&lt;/h2&gt;
&lt;p&gt;Create the container app with persistent storage:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az containerapp create \
  --name $CONTAINER_APP_NAME \
  --resource-group $RESOURCE_GROUP \
  --environment $CONTAINER_APP_ENV \
  --image &amp;quot;owncast/owncast:latest&amp;quot; \
  --target-port 8080 \
  --ingress external \
  --min-replicas 0 \
  --max-replicas 1 \
  --cpu 0.5 \
  --memory 1Gi \
  --volume-mount &amp;quot;data:/app/data&amp;quot; \
  --volume-name &amp;quot;data&amp;quot; \
  --volume-storage-name &amp;quot;owncast-storage&amp;quot; \
  --volume-storage-type AzureFile
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Step 4: Configure Dual-Port Ingress (HTTP + RTMP)&lt;/h2&gt;
&lt;p&gt;For Owncast to work properly, you need both HTTP (8080) and RTMP (1935) ports. This requires a &lt;strong&gt;Virtual Network (VNet)&lt;/strong&gt; integration:&lt;/p&gt;
&lt;h3&gt;Create VNet and Subnet&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create MINIMAL virtual network (smallest possible address space)
az network vnet create \
  --name &amp;quot;owncast-vnet&amp;quot; \
  --resource-group $RESOURCE_GROUP \
  --location $LOCATION \
  --address-prefix &amp;quot;10.0.0.0/24&amp;quot;  # Smaller than default /16

# Create minimal subnet
az network vnet subnet create \
  --name &amp;quot;container-apps-subnet&amp;quot; \
  --resource-group $RESOURCE_GROUP \
  --vnet-name &amp;quot;owncast-vnet&amp;quot; \
  --address-prefix &amp;quot;10.0.0.0/27&amp;quot;  # Only 32 IPs instead of /23 (512 IPs)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Recreate Container Apps Environment with VNet&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get subnet ID
SUBNET_ID=$(az network vnet subnet show \
  --name &amp;quot;container-apps-subnet&amp;quot; \
  --vnet-name &amp;quot;owncast-vnet&amp;quot; \
  --resource-group $RESOURCE_GROUP \
  --query id -o tsv)

# Delete existing environment and recreate with VNet
az containerapp env delete \
  --name $CONTAINER_APP_ENV \
  --resource-group $RESOURCE_GROUP \
  --yes

# Create Container Apps environment with workload profiles DISABLED (cheapest option)
az containerapp env create \
  --name $CONTAINER_APP_ENV \
  --resource-group $RESOURCE_GROUP \
  --location $LOCATION \
  --infrastructure-subnet-resource-id $SUBNET_ID \
  --enable-workload-profiles false  # Forces consumption-only pricing
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Deploy Container App with MINIMAL Resources&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az containerapp create \
  --name $CONTAINER_APP_NAME \
  --resource-group $RESOURCE_GROUP \
  --environment $CONTAINER_APP_ENV \
  --image &amp;quot;owncast/owncast:latest&amp;quot; \
  --target-port 8080 \
  --exposed-port 1935 \
  --ingress external \
  --transport auto \
  --min-replicas 0 \
  --max-replicas 1 \
  --cpu 0.25 \
  --memory 0.5Gi \
  --volume-mount &amp;quot;data:/app/data&amp;quot; \
  --volume-name &amp;quot;data&amp;quot; \
  --volume-storage-name &amp;quot;owncast-storage&amp;quot; \
  --volume-storage-type AzureFile
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Cost-Optimized Resource Allocation:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 0.25 cores (minimum allowed, sufficient for small streams)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 0.5Gi (minimum allowed, will work for basic streaming)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scaling&lt;/strong&gt;: Aggressive scale-to-zero with max 1 replica&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Ultra Low-Cost Alternative YAML Configuration&lt;/h2&gt;
&lt;p&gt;For maximum cost optimization, use this YAML approach with the smallest possible resource allocation. This configuration includes critical SQLite compatibility fixes for Azure Files:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;# owncast-production-ready.yaml
properties:
  configuration:
    ingress:
      external: true
      targetPort: 8080
      additionalPortMappings:
      - external: true
        targetPort: 1935
        exposedPort: 1935
        transport: tcp
    secrets: []
  template:
    containers:
    - image: owncast/owncast:latest
      name: owncast-app
      resources:
        cpu: 0.25
        memory: 0.5Gi
      volumeMounts:
      - mountPath: /app/data
        volumeName: data
      env:
      - name: OWNCAST_RTMP_PORT
        value: &amp;quot;1935&amp;quot;
      - name: OWNCAST_WEBSERVER_PORT  
        value: &amp;quot;8080&amp;quot;
      - name: OWNCAST_DATABASE_FILE
        value: &amp;quot;/app/data/db/owncast.db&amp;quot;
      - name: OWNCAST_DATABASE_JOURNAL_MODE
        value: &amp;quot;DELETE&amp;quot;
      - name: OWNCAST_LOG_DIRECTORY
        value: &amp;quot;/app/data/logs&amp;quot;
      - name: OWNCAST_DATA_DIRECTORY
        value: &amp;quot;/app/data&amp;quot;
      - name: OWNCAST_HLS_DIRECTORY
        value: &amp;quot;/app/data/hls&amp;quot;
    scale:
      minReplicas: 0
      maxReplicas: 1
      rules:
      - name: &amp;quot;http-rule&amp;quot;
        http:
          metadata:
            concurrentRequests: &amp;quot;10&amp;quot;
    volumes:
    - name: data
      storageType: AzureFile
      storageName: owncast-storage
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Deploy with:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az containerapp create \
  --name $CONTAINER_APP_NAME \
  --resource-group $RESOURCE_GROUP \
  --yaml owncast-production-ready.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Critical Configuration Notes&lt;/h2&gt;
&lt;h3&gt;SQLite + Azure Files Compatibility Fix&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: The default Owncast configuration can cause &amp;quot;database is locked&amp;quot; errors when using Azure Files storage due to SQLite's WAL (Write-Ahead Logging) mode being incompatible with network file systems. The configuration above includes these critical fixes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Database Subdirectory&lt;/strong&gt;: &lt;code&gt;OWNCAST_DATABASE_FILE=/app/data/db/owncast.db&lt;/code&gt; - Places database in a subdirectory which improves Azure Files compatibility&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Journal Mode Override&lt;/strong&gt;: &lt;code&gt;OWNCAST_DATABASE_JOURNAL_MODE=DELETE&lt;/code&gt; - Forces SQLite to use DELETE mode instead of WAL mode to prevent file locking issues&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Full Data Mount&lt;/strong&gt;: Mounts entire &lt;code&gt;/app/data&lt;/code&gt; directory for complete persistence of logs, database, and HLS content&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;What Gets Persisted&lt;/h3&gt;
&lt;p&gt;With this configuration, the following data persists across container restarts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User accounts and chat history&lt;/strong&gt; (critical for community continuity)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stream keys and admin credentials&lt;/strong&gt; (prevents reconfiguration)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ActivityPub federation data&lt;/strong&gt; (maintains Mastodon/Fediverse followers)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Server customization and branding&lt;/strong&gt; (logos, site name, descriptions)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;API tokens and webhook configurations&lt;/strong&gt; (third-party integrations)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complete logs and HLS video segments&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Without proper persistence, you would lose all users, followers, and configuration on every container restart.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Cost Optimization Features&lt;/h2&gt;
&lt;h3&gt;Scale-to-Zero Configuration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Min Replicas&lt;/strong&gt;: Set to 0 to completely scale down when not in use&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Max Replicas&lt;/strong&gt;: Set to 1 (Owncast doesn't need horizontal scaling)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale Rules&lt;/strong&gt;: Container Apps will automatically scale up when requests arrive&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Resource Limits (Ultra Cost-Optimized)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 0.25 cores (absolute minimum, sufficient for 1-2 viewer streams)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: 0.5Gi (minimum allowed by Azure Container Apps)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: Cool tier with 1GB initial quota (auto-scales as needed)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Network&lt;/strong&gt;: Minimal VNet addressing to reduce overhead&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;OBS Configuration&lt;/h2&gt;
&lt;p&gt;After deployment, configure OBS for streaming:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Server Settings&lt;/strong&gt;: Use &lt;code&gt;rtmp://your-app-url:1935/live&lt;/code&gt; (note: &lt;code&gt;rtmp://&lt;/code&gt; not &lt;code&gt;https://&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Stream Key&lt;/strong&gt;: Use the key from Owncast admin panel (Configuration &amp;gt; Server Setup &amp;gt; Stream Keys)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Owncast Web Interface&lt;/strong&gt;: Access at &lt;code&gt;https://your-app-url&lt;/code&gt; (port 8080 is handled automatically by ingress)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Persistent Data&lt;/strong&gt;: All Owncast configuration, database, and uploaded files are stored in Azure Files and persist across container restarts and scale-to-zero events. &lt;strong&gt;Critical&lt;/strong&gt;: Uses SQLite DELETE journal mode to prevent &amp;quot;database is locked&amp;quot; errors with Azure Files.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Cold Start&lt;/strong&gt;: When scaling from zero, there will be a 10-15 second cold start delay as the container initializes. This is normal and acceptable for personal streaming.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;VNet Requirement&lt;/strong&gt;: For dual-port access (HTTP + RTMP), you &lt;strong&gt;must&lt;/strong&gt; use a Virtual Network integration. This is a requirement for exposing additional TCP ports in Azure Container Apps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Database Compatibility&lt;/strong&gt;: The configuration includes specific environment variables (&lt;code&gt;OWNCAST_DATABASE_JOURNAL_MODE=DELETE&lt;/code&gt;) to ensure SQLite works properly with Azure Files network storage. Without these settings, you'll experience database crashes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Security Configuration&lt;/strong&gt;: After deployment, immediately change the default admin credentials:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Navigate to &lt;code&gt;https://your-app-url/admin&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Default login: &lt;code&gt;admin&lt;/code&gt; / &lt;code&gt;abc123&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Go to Configuration &amp;gt; Server Setup and change the admin password&lt;/li&gt;
&lt;li&gt;Create/copy stream keys from Configuration &amp;gt; Server Setup &amp;gt; Stream Keys tab&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Custom Domain&lt;/strong&gt;: You can configure a custom domain using:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az containerapp hostname add \
  --name $CONTAINER_APP_NAME \
  --resource-group $RESOURCE_GROUP \
  --hostname &amp;quot;your-domain.com&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;SSL Certificate&lt;/strong&gt;: Azure Container Apps provides automatic SSL certificates for custom domains.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Monitoring and Troubleshooting&lt;/h2&gt;
&lt;p&gt;Check your deployment:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get the URL
az containerapp show \
  --name $CONTAINER_APP_NAME \
  --resource-group $RESOURCE_GROUP \
  --query properties.configuration.ingress.fqdn

# Test RTMP port connectivity (should return TcpTestSucceeded: True)
Test-NetConnection -ComputerName &amp;quot;your-app-url&amp;quot; -Port 1935 -InformationLevel Detailed

# Test web interface
curl -I https://your-app-url/

# View logs
az containerapp logs show \
  --name $CONTAINER_APP_NAME \
  --resource-group $RESOURCE_GROUP
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Common Issues and Solutions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: Container logs show &amp;quot;database is locked&amp;quot; errors and crashes
&lt;strong&gt;Solution&lt;/strong&gt;: Ensure you're using the configuration above with &lt;code&gt;OWNCAST_DATABASE_JOURNAL_MODE=DELETE&lt;/code&gt; and database in a subdirectory&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: OBS shows &amp;quot;Failed to connect to server&amp;quot;&lt;br /&gt;
&lt;strong&gt;Solution&lt;/strong&gt;: Verify RTMP port 1935 is accessible using &lt;code&gt;Test-NetConnection&lt;/code&gt; and ensure you're using VNet integration&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: Admin panel won't load or shows errors
&lt;strong&gt;Solution&lt;/strong&gt;: Database persistence issues - check that the &lt;code&gt;/app/data&lt;/code&gt; mount is working and SQLite journal mode is set correctly&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: Container scales to zero but doesn't start properly
&lt;strong&gt;Solution&lt;/strong&gt;: Cold start delay is normal (10-15 seconds). If it doesn't start, check environment variable configuration&lt;/p&gt;
&lt;h2&gt;Cost Estimation (Ultra-Optimized)&lt;/h2&gt;
&lt;p&gt;With these optimizations, your monthly costs should be:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When Streaming (4 hours/month example):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Compute&lt;/strong&gt;: ~$0.50/month (0.25 CPU + 0.5Gi RAM × 4 hours)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Container Apps Environment&lt;/strong&gt;: ~$0.00 (consumption plan, no dedicated resources)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Networking&lt;/strong&gt;: ~$0.05/month (minimal VNet overhead)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;When Idle (Scale-to-Zero):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Compute&lt;/strong&gt;: $0.00 (scaled to zero)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt;: $0.00 (consumption plan)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Always-On Costs:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: ~$0.05-0.10/month (1-2GB in Cool tier)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;VNet&lt;/strong&gt;: ~$0.00 (no gateways or dedicated resources)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Total Monthly Cost: ~$0.60-0.65/month&lt;/strong&gt; (assuming 4 hours of streaming)&lt;/p&gt;
&lt;h3&gt;Performance Expectations at Minimal Resources:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;0.25 CPU + 0.5Gi RAM&lt;/strong&gt;: Suitable for 480p-720p streams with 1-5 concurrent viewers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale-up Path&lt;/strong&gt;: Monitor performance and increase to 0.5 CPU + 1Gi if needed&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cold Start&lt;/strong&gt;: ~10-15 seconds when scaling from zero (acceptable for personal streaming)&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/resources/snippets/owncast-optimized-azure-deployment-guide</link>
      <guid>https://www.lqdev.me/resources/snippets/owncast-optimized-azure-deployment-guide</guid>
      <pubDate>2025-08-14 13:35 -05:00</pubDate>
      <category>owncast</category>
      <category>fediverse</category>
      <category>azure</category>
      <category>livestream</category>
    </item>
    <item>
      <title>Mobile-First Static Site Publishing: Discord Bot Pipeline via Azure and GitHub</title>
      <description>&lt;![CDATA[
## The problem

Ever since I published my [first note](/notes/hello-world)([microblog post](https://indieweb.org/note)) on my website, I've always wanted a way to quickly publish while on the go. Unfortunately, I never found a good solution. 

Because my website is statically generated and the source is hosted on GitHub(check out the [colophon](/colophon) for more details), there is no backend for me to talk to. At the same time, I didn't want to build an entire backend to support my website because I want to keep things as lean and cost-efficient as possible. 

Since my posts are just frontmatter and Markdown, I [use VS Code as my editor](/uses). For some time, back when I used to have a Surface Duo, I [authored posts from mobile using the github.dev experience](https://www.lqdev.me/notes/surface-duo-blogging-github-dev). On two screens, while not ideal, it was manageable. After switching devices (because sadly there were no more security updates on the Surface Duo) and upgrading to a dumbphone and later a single screen smartphone, that workflow wasn't feasible.

At that point, what I resorted to was sending messages to myself via [Element](https://element.io/). The message would contain a link I wanted to check out later. Once I was on my laptop, I would check out the link and if I wanted to post about it on my website, I'd do so then. 

That process, while it worked, wasn't necessarily scalable. In part that's a feature because I could spend more time digesting the content and writing a thoughtful article. However, it stopped me from sharing more in the moment and there were posts that were never authored or bookmarks that weren't captured because eventually that link got lost in the river of other links. 

Basically what I wanted to replicate was the more instant posting that social media gives you, but do so on my own site. 

That led me to doing some thinking and requirement gathering around the type of experience I wanted to have.

## Requirements

When it came to requirements for my solution, I was focused more on the workflow and experience rather on technical details. 

Here is a list of those solution requirements:

- Mobile is the primary publishing interface. Desktop publishing is a nice to have.
- Be as low-friction as sharing a link via Element or posting on social media
- Doesn't require implementing my own client or frontend
- Doesn't require me to use existing micropub clients
- Handles short-form and media posts supported by my website
    - Notes
    - Responses
        - Repost
        - Reply
        - Like
    - Bookmark
    - Media
        - Image
        - Audio (not used as often but technically supported)
        - Video (not used as often but technically supported)
- Low-cost

## The solution

For years, I had been struggling with actually implementing this system. The main part that gave me pause was not implementing my own client or relying on existing micropub clients. 

Eventually, I just accepted that it might never happen. 

One day, it eventually hit me. If the notes to self Element workflow worked so well, why not use a chat client as the frontend for publishing. At least have it serve as the capture system that would then format the content into a post that gets queued for publishing on GitHub. I'd seen [Benji do something similar with his micropub endpoint](https://github.com/benjifs/serverless-micropub). 

While I could've used Element since that's my preferred platform, I've been contemplating no longer hosting my own Matrix server. So if I went through with this, I'd want something that I didn't feel bad about investing the time on this solution if that chat client went away. 

That then left me with Discord as the next best option. Primarily because of its support for bots as well as its cross-platform support across mobile and desktop. 

In the end, the solution then ended up being fairly straightforward. 

More importantly, with the help of AI, I wrote none of the code. 

Using Copilot and Claude Sonnet 4, I was able to go from idea to deployment in 1-2 days. At that time the solution supported all of the posts except for media which I hadn't figured out what the best way of uploading media through Discord was. Figuring that out, implementing it, and deploying it took another day or two. 

Since I wanted for my solution to be as low-cost as possible, serverless seemed like a good option. I only pay for compute when it's actually being used which can be infrequent in my case. I don't need the server running 24/7 or even to be powerful. However, I didn't want to write my system as an Azure Function. I wanted the flexibility of deploying on a shared VM or container. A VM though wasn't an option since it's running 24/7. Keeping all of that in mind, my choice was narrowed down to Azure Container Apps which gave me the characteristics I was looking for. Serverless containers. 

Once that decision was made, I used Copilot again to figure out how to optimize my container image so that it's space and resource efficient. And while at it, I used Copilot again to figure out the right incantations to get the container deployed to Azure Container Apps. 

All-in-all, the solution had been staring at me in the face since I already had a workflow that for the most part worked for me, it just needed some optimizations and with the help of AI, I was able to quickly build and deploy something I'd been ruminating over for years.  

## Workflow

The workflow for publishing is as follows:

1. Invoke the bot in Discord to capture my input using slash command `/post` and the respective post type.

    ![Using slash commands to invoke discord publishing bot](http://cdn.lqdev.tech/files/images/invoke-discord-publishing-bot.png)

1. Provide post details. For media posts, I can provide an attachment which gets uploaded to Azure Blob Storage.

    ![A modal in discord with note post fields filled in](http://cdn.lqdev.tech/files/images/discord-publishing-client-modal.png)

1. [Bot creates a branch and PR in my repo](https://github.com/lqdev/luisquintanilla.me/pull/152) with the post content
1. While logged into GitHub from my phone, if everything looks good, I merge the PR which kicks off my GitHub Actions workflow to build and publish the site including the new post. 
1. Post [displays on my website](/media/test-media-post-from-mobile/).

## Challenges

The solution is not perfect. 

One of the problems I've run into is cold-start. Since I scale my solution down to zero when it's not being used to save on costs, I suffer from the cold start problem. Therefore, when I first invoke the bot, it fails. I have to give it a few seconds and retry the invocation. It's usually about 5 seconds so it's not a huge issue but it does add some friction. 

## Next steps

Overall I'm happy with my solution but there are a few improvements I'd like to make.

- **Open-source the repo** - Currently I've kept the repo private since it was all AI generated. Since my system is already in production and processes were documented, I need to do a more thorough pass to make sure that no secrets or credentials are checked in or documented anywhere.
- **Improve UX** - Discord limits modal fields to 5. Therefore, I'm playing around with the right balance between how much of the input should come from slash commands and how much should come from the modal.
- **Expand supported post types** - I'd like to expand the number of posts supported by my publishing client. Reviews are a good example of the type of post I'd like to support as well as RSVPs. [Reviews I already support](https://www.lqdev.me/reviews) on my website but RSVPs I don't yet. Also, I'd have to fix my Webmentions which are currently broken after [upgrading my website](/notes/hello-world-new-site-2025-08/). 
- **Make it generator agnostic** - Currently this only works for my website. With a few tweaks and refactoring, I think I can get the project to a place where it should work with other popular static site generators. 
- **One-click deployment** - Currently the solution is packaged up as a container so it can be deployed from anywhere. I want to make it even simpler to deploy. One click if possible. 

]]&gt;</description>
      <link>https://www.lqdev.me/posts/website-mobile-publishing-discord-client</link>
      <guid>https://www.lqdev.me/posts/website-mobile-publishing-discord-client</guid>
      <pubDate>2025-08-14 01:45 -05:00</pubDate>
      <category>discord</category>
      <category>azure</category>
      <category>indieweb</category>
      <category>mobile</category>
      <category>microblogging</category>
      <category>web</category>
      <category>bot</category>
      <category>aca</category>
      <category>azurecontainerapps</category>
    </item>
    <item>
      <title>How to Multistream Using Azure, Azure VM, MonaServer 2, and FFmpeg with OBS Studio</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;In this guide, we will set up a multistreaming workflow using Azure, an Azure VM, MonaServer 2, and FFmpeg to broadcast to multiple platforms like LinkedIn Live and YouTube Live. We’ll use OBS Studio as the main streaming software. Let’s get started.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Love this detailed writeup from &lt;a href="https://maho.dev/"&gt;Maho&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When I originally experimented with something similar using my Owncast instance, I was running ffmpeg locally but I like his solution better.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/multistream-azure-ffmpeg-obs-studio-maho</link>
      <guid>https://www.lqdev.me/responses/multistream-azure-ffmpeg-obs-studio-maho</guid>
      <pubDate>2025-02-25 20:33 -05:00</pubDate>
      <category>livestream</category>
      <category>azure</category>
      <category>obs</category>
    </item>
    <item>
      <title>Redirect URLs working again</title>
      <description>&lt;![CDATA[&lt;p&gt;Thanks to the kind e-mail from &lt;a href="https://www.benji.dog/"&gt;benji.dog&lt;/a&gt;, I realized people couldn't subscribe to my RSS feeds using the links provided in the &lt;a href="https://www.lqdev.me/feed"&gt;/feed&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;I'd noticed something weird had been going on for the past few days but since the rest of my site was working, I didn't pay attention to it.&lt;/p&gt;
&lt;p&gt;When I went in earlier today to fix the issue, I realized because of changes to &lt;a href="https://learn.microsoft.com/azure/cdn/edgio-retirement-faq"&gt;Azure CDN&lt;/a&gt;, none of my redirects were working.&lt;/p&gt;
&lt;p&gt;My site kept working fine, but that's because the migration was autmatically done by Azure.&lt;/p&gt;
&lt;p&gt;What wasn't automatically migrated were my CDN rules which I'd set up to &lt;a href="https://www.lqdev.me/notes/new-rss-feed-links"&gt;simplify URLs&lt;/a&gt; and &lt;a href="https://www.lqdev.me/responses/own-your-rss-links"&gt;own my links&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/cdn-issue-fixed-redirects-working</link>
      <guid>https://www.lqdev.me/notes/cdn-issue-fixed-redirects-working</guid>
      <pubDate>2025-02-08 14:38 -05:00</pubDate>
      <category>cdn</category>
      <category>azure</category>
      <category>rss</category>
      <category>ownyourlinks</category>
      <category>cdn</category>
      <category>frontdoor</category>
      <category>community</category>
    </item>
    <item>
      <title>Tinkering with DeepSeek R1, GitHub Models, and .NET on stream</title>
      <description>&lt;![CDATA[&lt;p&gt;I saw that the &lt;a href="https://azure.microsoft.com/en-us/blog/deepseek-r1-is-now-available-on-azure-ai-foundry-and-github/"&gt;DeepSeek R1 model is now on GitHub Models and Azure AI Foundry&lt;/a&gt;, so I decided to start a stream and play around with it in the GitHub Models playground as well as a .NET application.&lt;/p&gt;
&lt;p&gt;Here's the recording.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=8Z6iFALi8kM" title="YouTube Video Thumbnail Tinkering on Stream with DeepSeek R1, GitHub Models, and .NET"&gt;&lt;img src="http://img.youtube.com/vi/8Z6iFALi8kM/0.jpg" class="img-fluid" alt="YouTube Video Thumbnail Tinkering on Stream with DeepSeek R1, GitHub Models, .NET" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I rambled on stream since it's my stream of consciousness as I'm tinkering with these technologies.&lt;/p&gt;
&lt;p&gt;If you're mainly interested in the code, here's the GitHub repo: &lt;a href="https://github.com/lqdev/DeepSeekDotnetSample"&gt;lqdev/DeepSeekDotnetSample&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/tinkering-deepseek-r1-dotnet-github-models</link>
      <guid>https://www.lqdev.me/notes/tinkering-deepseek-r1-dotnet-github-models</guid>
      <pubDate>2025-01-29 19:59 -05:00</pubDate>
      <category>deepseek</category>
      <category>ai</category>
      <category>dotnet</category>
      <category>github</category>
      <category>azure</category>
      <category>livestream</category>
    </item>
    <item>
      <title>First live stream using Owncast done</title>
      <description>&lt;![CDATA[&lt;p&gt;Earlier today I tested out my &lt;a href="https://www.lqdev.me/posts/deploy-owncast-azure/"&gt;self-hosted Owncast setup&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=3opjC7fEAJs" title="Video Thumbnail from live stream"&gt;&lt;img src="http://img.youtube.com/vi/3opjC7fEAJs/0.jpg" class="img-fluid" alt="Video Thumbnail from live stream" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Overall, I think it went well considering I was using the built-in camera and mic on my laptop. Despite sharing my screen, using the browser, broadcasting and recording the stream, the laptop seemed to handle it just fine. I didn't get a chance to test writing and running code which is going to make up a bulk of the content on stream.&lt;/p&gt;
&lt;p&gt;The flow on OBS is straightforward and simple. I might automate it either using my Stream Deck or just mapping hotkeys. It's simple enough that I only need to transition between a few scenes.&lt;/p&gt;
&lt;p&gt;As far as hosting the recordings, the process of uploading to YouTube was straightforward. It took a little over 10 minutes, but that's something that I can just do in the background so not really worried about that. For the time being, I think I'll just use YouTube to host the videos and mark them as unlisted. No particular reason behind that other than I only want YouTube to serve as my video host. The main place I plan on showcasing and organizing videos is on my website.&lt;/p&gt;
&lt;p&gt;For the time being, I'll just create notes like these. Eventually though, I'd like to have a video post type which shows up on my feed. The post card will only display the video but the detailed view, I'd like for it to be like my &lt;a href="https://www.lqdev.me/resources/presentations/hello-world"&gt;presentation pages&lt;/a&gt;. Except, instead of a presentation, it'd contain the video, show notes, maybe the transcript, and links mentioned during the stream. Also, I'd like video posts to have their own RSS feed just like other posts on this page so folks who are interested in following along can via their RSS reader.&lt;/p&gt;
&lt;p&gt;A few other things I want to figure out are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Setting up a custom domain that points to my Owncast server rather than the default one provided by Azure Container Apps.&lt;/li&gt;
&lt;li&gt;Displaying and managing chat outside of the built-in Owncast frontend.&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/first-owncast-stream</link>
      <guid>https://www.lqdev.me/notes/first-owncast-stream</guid>
      <pubDate>2025-01-20 18:33 -05:00</pubDate>
      <category>owncast</category>
      <category>livestream</category>
      <category>selfhost</category>
      <category>opensource</category>
      <category>fediverse</category>
      <category>video</category>
      <category>azure</category>
    </item>
    <item>
      <title>Workaround for Azure CLI docker image azcopy issue in GitHub Actions</title>
      <description>&lt;![CDATA[&lt;p&gt;Publishing on my site was broken since yesterday. I thought it was only me but after doing some digging looks like it was an issue with Azure CLI and the link it uses to download azcopy.&lt;/p&gt;
&lt;p&gt;Looks like the team is already working on a fix and you can track progress using this issue&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/Azure/azure-cli/issues/30635"&gt;https://github.com/Azure/azure-cli/issues/30635&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you're using Azure CLI in GitHub Actions and running into this problem, you can unblock yourself by installing azcopy manually. Here's the snippet that worked for me.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;- name: Upload to blob storage
  uses: azure/CLI@v1
  with:
    azcliversion: 2.67.0
    # azcopy workadound https://github.com/Azure/azure-cli/issues/30635
    inlineScript: |
        tdnf install -y azcopy;
        # YOUR SCRIPT
&lt;/code&gt;&lt;/pre&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/gh-actions-az-cli-docker-azcopy-workaround</link>
      <guid>https://www.lqdev.me/notes/gh-actions-az-cli-docker-azcopy-workaround</guid>
      <pubDate>2025-01-10 21:12 -05:00</pubDate>
      <category>azure</category>
      <category>github</category>
      <category>cicd</category>
    </item>
    <item>
      <title>Deploy your own Owncast server</title>
      <description>&lt;![CDATA[
I'm a fan of learning in public. One way I do that is writing these types of posts. Another is live-streaming. It's been about four years since the last time I streamed. You can check out some of those recordings on my [YouTube channel](/youtube) if you're interested.

Lately, I've been wanting to get back into live-streaming, but like many parts of my digital presence, it's an area I want ownership over. That means, I don't want to use Twitch, YouTube, Kick, or any other streaming platform. I want to self-host my own livestream server. 

For years, I've been following [Owncast](https://owncast.online/). I'm a big fan. Aside from being open-source and allowing me to self-host, it has [Fediverse](https://owncast.online/docs/social/) and [IndieAuth](https://owncast.online/docs/chat/chat-authentication/#indieauth) integrations.

Because I don't plan to monetize my streams, I want to make sure I keep my costs down. Owncast offers a container deployment option. That means, I can deploy my application to [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview) using a [consumption plan](https://learn.microsoft.com/azure/container-apps/plans#consumption). By doing so, I only pay for compute and networking when I'm streaming. The rest of the time, I can scale down the application to zero and I'm not charged for it. 

In this post, I'll go over how to set up a self-hosted Owncast instance on Azure Container Apps and once deployed, how to configure it with [OBS](https://obsproject.com/). 

## Deploy owncast server

The deployment is fairly straightforward. You can use the ["Provide a virtual network to an Azure Container Apps environment"](https://learn.microsoft.com/azure/container-apps/vnet-custom?tabs=bash&amp;pivots=azure-portal) tutorial. I'd recommend going through the portal if you don't want to install the Azure CLI or are not comfortable with the command line. 

The few configurations that tutorial doesn't guide you through is setting up your container and ingress since those are application specific. 

After you configure your environment and virtual network in the *Basics* tab, continue your setup using the steps below. 

### Container

To set up Owncast, you can use the [public docker image](https://hub.docker.com/r/owncast/owncast/tags).

When you get to the *Container* tab in the setup process, fill in the fields with the following values:

| Field | Value |
| --- | --- |
| Name | Enter a name for your container |
| Image source | Docker Hub or other registies |
| Image type | Public |
| Registry login server | docker.io |
| Image and tag | owncast/owncast:latest |
| Command override | You can leave this blank |
| Arguments override | You can leave this blank |
| Deployment stack | Generic |
| Workload profile | Consumption |
| CPU and Memory | 2 CPU Cores, 4Gi Memory |

### Ingress 

By setting up ingress on your deployment, you'll expose the ports Owncast listens on. 

The reason you need a virtual network is because Owncast listens on two ports:

- **8080** - The web application and admin portal
- **1935** - The RMTP server used for streaming

By default, Azure Container Apps only allows you to expose one port. For more details, see the [Azure Container Apps documentation](https://learn.microsoft.com/azure/container-apps/ingress-overview#additional-tcp-ports). 

In the *Ingress* tab, check the box that says **Enabled**.

Then, fill in the fields with the following values:

| Field | Value |
| --- | --- |
| Ingress traffic | Accepting traffic from anywhere |
| Ingress type | HTTP |
| Transport | Auto |
| Insecure connections | Leave unchecked |
| Target port | 8080 |
| Session affinity | Leave unchecked | 

Expand the Additional TCP ports section and configure the RMTP port

| Field | Value |
| --- | --- |
| Target port | 1935 |
| Exposed port | 1935 |
| Ingress traffic | Accepting traffic from anywhere |

Once you've configured ingress, you can select on **Review + create**.

Review your configurations in the *Review + create* tab. If everything looks good, select **Create**. 

## **IMPORTANT** Change admin password

Once your application deploys, make sure to change your admin password. 

1. In the portal, go to your container app resource and copy your application URL.

    ![Screenshot of Azure Container Apps resource in Azure Portal highlighting Application URL](http://cdn.lqdev.tech/files/images/portal-container-resource.png)

    Make sure to keek your application URL around since you'll also be using it when configuring OBS. 

1. Access the admin portal at the following URL - `&lt;YOUR-APPLICATION-URL&gt;/admin`. Replace `&lt;YOUR-APPLICATION-URL&gt;` with your application URL.
1. Log in with the default credentials.

| Field | Value |
| Username | admin |
| Password | abc123 |

1. In the Owncast admin portal, select **Configuration &gt; Server Setup**
1. Change your admin password.

For more details, see the [Owncast documentation](https://owncast.online/docs/configuration/). 

## Create stream key

Once you've changed your admin password, create a stream key. This will enable you to log into your server through OBS and begin streaming. 

1. In the Owncast admin portal, select **Configuration &gt; Server Setup**
1. Select the **Stream Keys** tab.
1. Copy the default stream key or create a new one.

## Configure OBS

Now that everything is configured, it's time to set up OBS. 

To do so, [follow the guidance in the Owncast documentattion](https://owncast.online/docs/broadcasting/obs/).

Replace the server address with your application URL and use the stream key you copied or created. Note that the protocol is not `http` or `https`. **Use `rmtp`**.

That's it! If everything is configured correctly, you should be able to select **Start Streaming** in OBS. 

Go to your Owncast instance using your application URL and in a few seconds, your broadcast should start. 

## Conclusion

I want to end by thanking the Owncast contributors and community for making self-hosting a live-streaming server so easy. 

My plan in the next couple of weeks is to:

1. Embed my stream on my website. Most likely, I'll make it a dedicated page. Something like `lqdev.me/live` or something like that.
1. Do a few pilot streams to make sure my stream quality and configurations are all set up correctly.
1. Evaluate what the costs are for this solution and whethere it's sustainable.

If all goes as expected, I'll start streaming more regularly. As mentioned, I want to keep my costs down and video hosting is expensive, so I'll use YouTube to post recordings. I'll also keep backups though in case any of those videos get taken down, I can always have a local copy still available. ]]&gt;</description>
      <link>https://www.lqdev.me/posts/deploy-owncast-azure</link>
      <guid>https://www.lqdev.me/posts/deploy-owncast-azure</guid>
      <pubDate>2025-01-09 20:47 -05:00</pubDate>
      <category>owncast</category>
      <category>azure</category>
      <category>selfhost</category>
      <category>fediverse</category>
      <category>obs</category>
      <category>livestream</category>
    </item>
    <item>
      <title>Got an Owncast instance deployed to Azure!</title>
      <description>&lt;![CDATA[&lt;p&gt;A few months ago, I started tinkering with Owncast. In that case, I was figuring out how I could use .NET Aspire and Blazor to &lt;a href="https://www.lqdev.me/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire/"&gt;configure an Owncast server and embed the stream on a website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Although I didn't go all the way through creating the deployment, doing so with &lt;a href="https://learn.microsoft.com/dotnet/aspire/deployment/azure/aca-deployment"&gt;Aspire would've been fairly straightforward&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since I already have a website, all I care about deploying is the Owncast server.&lt;/p&gt;
&lt;p&gt;After spending about an hour on it today, I was able deploy and Owncast server to Azure Container Apps using the &lt;a href="https://learn.microsoft.com/en-us/azure/container-apps/vnet-custom?tabs=bash&amp;amp;pivots=azure-portal"&gt;virtual network environment setup tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The hardest part was figuring out that I needed to set up a virtual network in order to publicly expose multiple ports.&lt;/p&gt;
&lt;p&gt;Once the server was up and running, configuring OBS was relatively easy as well.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://cdn.lqdev.tech/files/images/owncast-obs-azure-container-apps-deployment.png" class="img-fluid" alt="AI Generated image of two aliens enjoying a beer in the middle of the desert by a campfire on a starry night" /&gt;&lt;/p&gt;
&lt;p&gt;I'll try to put together a more detailed writeup so I remember what I did for next time as well as help others who may be interested in self-hosting their own livestream server.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/owncast-azure-container-apps-deployment</link>
      <guid>https://www.lqdev.me/notes/owncast-azure-container-apps-deployment</guid>
      <pubDate>2025-01-07 19:52 -05:00</pubDate>
      <category>owncast</category>
      <category>livestream</category>
      <category>azure</category>
      <category>selfhost</category>
      <category>fediverse</category>
      <category>obs</category>
      <category>containers</category>
      <category>aca</category>
    </item>
    <item>
      <title>GitHub Actions for Azure</title>
      <description>&lt;![CDATA[&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;How to use GitHub Actions with Azure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://learn.microsoft.com/azure/developer/github/github-actions"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Authenticate to Azure from GitHub&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://learn.microsoft.com/azure/developer/github/connect-from-azure?tabs=azure-cli%2Clinux"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This workflow is useful when using the &lt;a href="https://github.com/azure/login?tab=readme-ov-file#github-actions-for-deploying-to-azure"&gt;az login action in GitHub Actions&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/resources/wiki/github-actions-azure</link>
      <guid>https://www.lqdev.me/resources/wiki/github-actions-azure</guid>
      <pubDate>04/23/2024 23:04 -05:00</pubDate>
      <category>github</category>
      <category>cicd</category>
      <category>githubactions</category>
      <category>azure</category>
    </item>
    <item>
      <title>GitHub Actions Publishing Workflow Fixed</title>
      <description>&lt;![CDATA[&lt;p&gt;The website had been out of comission for a few days because my credentials had expired. I could still add posts and make updates. However, none of them were published to Azure.&lt;/p&gt;
&lt;p&gt;Figuring out the right incantations wasn't straightforward, so &lt;a href="https://www.lqdev.me/resources/wiki/github-actions-azure/"&gt;saving the fix&lt;/a&gt; for future reference.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/gha-publishing-workflow-fixed</link>
      <guid>https://www.lqdev.me/notes/gha-publishing-workflow-fixed</guid>
      <pubDate>2024-04-23 22:51 -05:00</pubDate>
      <category>github</category>
      <category>blogging</category>
      <category>azure</category>
      <category>githubactions</category>
      <category>cicd</category>
    </item>
    <item>
      <title>Introducing Command R+: A Scalable LLM Built for Business</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Command R+ is a state-of-the-art RAG-optimized model designed to tackle enterprise-grade workloads, and is available first on Microsoft Azure&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Command R+, like our recently launched Command R model, features a 128k-token context window and is designed to offer best-in-class:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Advanced Retrieval Augmented Generation (RAG) with citation to reduce hallucinations&lt;/li&gt;
&lt;li&gt;Multilingual coverage in 10 key languages to support global business operations&lt;/li&gt;
&lt;li&gt;Tool Use to automate sophisticated business processes&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/introducing-cohere-comandr-plus</link>
      <guid>https://www.lqdev.me/responses/introducing-cohere-comandr-plus</guid>
      <pubDate>2024-04-04 22:46 -05:00</pubDate>
      <category>cohere</category>
      <category>llm</category>
      <category>comandr</category>
      <category>azure</category>
      <category>comandrplus</category>
    </item>
    <item>
      <title>Announcing Mistral Large</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mistral Large is our flagship model, with top-tier reasoning capacities. It is also available on Azure.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mistral Large comes with new capabilities and strengths:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is natively fluent in English, French, Spanish, German, and Italian, with a nuanced understanding of grammar and cultural context.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Its 32K tokens context window allows precise information recall from large documents.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Its precise instruction-following enables developers to design their moderation policies – we used it to set up the system-level moderation of le Chat.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;It is natively capable of function calling. This, along with constrained output mode, implemented on la Plateforme, enables application development and tech stack modernisation at scale.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;At Mistral, our mission is to make frontier AI ubiquitous. This is why we’re announcing today that we’re bringing our open and commercial models to Azure.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/announcing-mistral-large</link>
      <guid>https://www.lqdev.me/responses/announcing-mistral-large</guid>
      <pubDate>2024-02-26 15:39 -05:00</pubDate>
      <category>ai</category>
      <category>mistral</category>
      <category>llm</category>
      <category>opensource</category>
      <category>azure</category>
    </item>
    <item>
      <title>The AI Study Guide: Azure Machine Learning Edition</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The AI Study Guide: Discover Machine Learning with these free Azure resources&lt;br /&gt;
&lt;br&gt;
Welcome to the February edition of the Azure AI Study Guide. Every month I’ll be spilling the tea on the best and newest tools for skilling up on Azure AI. This month we’re putting on our thinking caps to investigate Azure Machine Learning (ML). I’ll give you a quick breakdown of what it is, then we’ll explore a four-week roadmap of our top FREE resources for you to continue your AI learning journey! And as a bonus, stay tuned to the end to see what makes machine learning and generative AI a dynamic duo.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/ai-study-guide-azure-ml-edition</link>
      <guid>https://www.lqdev.me/bookmarks/ai-study-guide-azure-ml-edition</guid>
      <pubDate>2024-02-21 15:18 -05:00</pubDate>
      <category>azure</category>
      <category>tutorial</category>
      <category>machinelearning</category>
      <category>ml</category>
      <category>ai</category>
      <category>azureml</category>
    </item>
    <item>
      <title>POSSE to Mastodon using RSS and Azure Logic Apps</title>
      <description>&lt;![CDATA[
## Introduction

[Over the past year, I've made this website the main place where I post content](/notes/weblogging-rewind-2023/). The easiest way to [subscribe to content on my website](/feed) is through the various RSS feeds. However, I have accounts on other platforms like X (formerly Twitter), Bluesky, and Mastodon where I'd still like to repost my content to. Since the [changes to the Twitter API](https://techcrunch.com/2023/03/29/twitter-announces-new-api-with-only-free-basic-and-enterprise-levels/), the only place I cross-post to is Mastodon. The main reason behind it is, as of now, it's one of the few platforms that allows me to automate post creation via its REST APIs without restrictions. A large part of that is I self-host my own Mastodon instance but I assume there aren't as many restrictions when using other instances like mastodon.social. The way I automate posting is by setting up workflows using Azure Logic Apps. These workflows subscribe to my various RSS feeds and whenever a new post is published, they make an HTTP request to my Mastodon instance to create a new post. I've been doing this for some time but never got around to documenting it. This blog post goes into more details about how to set up these workflows.  

## What is POSSE

[POSSE](https://indieweb.org/POSSE) is short for "Post on your Own Site Syndicate Elsewhere". It's one of the patterns adopted within IndiWeb communities and projects. The main idea is, your website or a website you own or have administrative rights to becomes the main platform where you publish your content. Effectively, it's the cannonical version of your content. Once your content is on your website, you can optionally choose to distribute it on other platforms. 

## What is RSS

[RSS](https://en.wikipedia.org/wiki/RSS) is short for Really Simple Syndication. Accorting to Wikipedia, this protocol "allows users and applications to access updates to websites in a standardized, computer-readable format". 

## What is Mastodon

[Mastodon](https://joinmastodon.org/) is an [open-source](https://github.com/mastodon/mastodon) decentralized microblogging platform built on the [ActivityPub](https://activitypub.rocks/) protocol and part of the larger collective of federated systems known as the [Fediverse](https://fediverse.info/). 

## What are Azure Logic Apps

If you've used workflow automation systems like [IFTTT](https://ifttt.com/), then you generally know what Azure Logic Apps are. 

A more formal definition from the Azure documentation - "Azure Logic Apps is a cloud platform where you can create and run automated workflows with little to no code. By using the visual designer and selecting from prebuilt operations, you can quickly build a workflow that integrates and manages your apps, data, services, and systems."

For more details, see the [Azure Logic Apps documentation](https://learn.microsoft.com/azure/logic-apps/logic-apps-overview).

## Prerequisites

Since you'll be creating Logic App Resources on Azure, you'll need an [Azure account](https://aka.ms/free).

## Create Consumption Azure Logic App Resource

There's various ways to create an Azure Logic App Resource but the easiest one for this relatively simple workflow is using the Azure Portal. For more details, see the [create a consumption logic app resource documentation](https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#create-a-consumption-logic-app-resource).

For the most part you can leave the defaults as is. Since the intended use for this workflow is personal and I don't need enterprise features, I chose to create my logic app using the consumption plan. 

When prompted to choose **Plan Type**, select **Consumption**. 

## The Workflow

The workflow is relatively simple. Whenever a new item is posted to an RSS feed, make an HTTP POST request to the Mastodon API

```mermaid
flowchart TD
    A["RSS"] --&gt; B["HTTP (Mastodon API)"]
```

Once your Logic App resource deploys to Azure, create a new logic app using the Blank Template. For more details, see the [select a blank template documentation](https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#select-the-blank-template).

This will launch you into the Logic app designer UI where you can begin to configure your workflow.

### RSS trigger

The first thing you'll want to do is set up the trigger that initiates the workflow. The trigger in this case will be new posts on an RSS feed.

In the Logic app designer, search for *RSS* and add it as a trigger.

Once the RSS trigger is added to your workflow, configure it as follows:

- **The RSS feed URL**: The feed you want want to get posts from. In this case, I'm using my blog posts [feed](/posts/feed.xml). 

    **TIP: The RSS feed can be ANY feed. So if you wanted to subscribe and repost items from your favorite blog or website, you can as well. Just note though that if you don't own the content it might not be something the author wants you doing and for websites that update multiple times a day, it might produce a lot of noise in your feed. Other than that though, subscribe and post away!**

- **How often do you want to check for items?**: The frequency at which you want to poll the RSS feed and check for new posts. In my case, I don't post very often but I do want my posts to be published on Mastodon within an hour of publishing. Therefore, I chose 1 hour as the update frequency. 

### HTTP action

Now that your trigger is configured, it's time to do someting with the latest posts. In this case, since I want to create a new post on Mastodon, I can do so via their REST API. 

#### Get Mastodon credentials

One thing that you'll need to publish posts to Mastodon is an application token. The easiest way to get one is using the Web UI. 

To get your app credentials through the web UI:

1. In the Mastodon Web UI, select **Preferences**.
1. In the preferences page, select **Development**.
1. In the development preferences tab, select **New application**.
1. In the new application page:
  - Provide a name for your application
  - Choose the **write:statuses** scope checkbox. For details on required scopes to post statuses, see the [Post a new status REST API documentation](https://docs.joinmastodon.org/methods/statuses/#create)
  - Select **Submit**

If successful, this will create credentials and a token you can use to send authenticated requests to the Mastodon API.

#### Configure HTTP action

Now that you have your credentials, you can configure your HTTP action in the Logic Apps workflow.

In the Logic App designer:

1. Select **+ New Step**.
1. Search for *HTTP* and add it as an action. 
1. Once the HTTP action is added, configure it as follows:

- **Method**: The HTTP Method. To create statuses, choose **POST** from the dropdown.
- **URI**: The endpoint to make the requests to. For mastodon, it's `https://&lt;HOST&gt;/api/v1/statuses`. Make sure to replace `&lt;HOST&gt;` with your Mastodon instance. In my case, it's `toot.lqdev.tech` since that's where I host my mastodon instance. 
- **Headers**: HTTP Headers to use. In this case, set the following:

    | Key | Value |
    | --- | --- |
    | Content-Type | application/x-www-form-urlencoded |

- **Queries**: URL Query Parameters to add to the request. This is where you'll set your access token.

    | Key | Value |
    | --- | --- |
    | access_token | `[YOUR-ACCESS-TOKEN]` |

    Make sure to replace `[YOUR-ACCESS-TOKEN]` with the token credential generated in the Mastodon Web UI.
- **Body**: The content to be published in your Mastodon post. This is entirely up to you though at minumum, you'll add the following to the text feed. `status=&lt;YOUR-CONTENT&gt;`. `&lt;YOUR-CONTENT&gt;` is what will be displayed in the Mastodon post. 

    One of the nice things about Azure Logic Apps is, properties from previous steps are available to you in subsequent steps. Since our trigger is an RSS feed, we can get access to the feed and item properties of our feed in the HTTP action. If all you wanted was to post the tile and URL, you can do that using the **Feed title** and **Primary Feed Link** properties. For more details, see the [RSS connector documentation](https://learn.microsoft.com/connectors/rss/). 

## Save and run

That's it! Now you just need to select **Save** in the Logic app designer page. 

Once it's saved, click **Run trigger** which will kick off your trigger. If you have anything recent to publish and everything is configured correctly, it should show up in your Mastodon feed.

## Conclusion

By publishing content on your own website first, you're in full control of your content. Regardless of which platforms come and go, you won't have to adjust to those changes because your content is not locked in to those platforms. However, that doesn't mean you can't also publish your content there. Using protocols like RSS make it easy to subscribe to updates on your website. Using REST APIs provided by the respective platforms, you can automate publishing these updates. To further automate and simplify this process, you can use services like Azure Logic Apps to make publishing to all places easy. ]]&gt;</description>
      <link>https://www.lqdev.me/posts/rss-to-mastodon-posse-azure-logic-apps</link>
      <guid>https://www.lqdev.me/posts/rss-to-mastodon-posse-azure-logic-apps</guid>
      <pubDate>2023-12-24 16:40 -05:00</pubDate>
      <category>azure</category>
      <category>mastodon</category>
      <category>indieweb</category>
      <category>logicapps</category>
      <category>fediverse</category>
      <category>posse</category>
      <category>internet</category>
      <category>web</category>
      <category>blogging</category>
      <category>blog</category>
      <category>automation</category>
      <category>programming</category>
      <category>rss</category>
    </item>
    <item>
      <title>Generative AI and .NET - Part 2 SDK</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;It’s time to have a look at how we can build the basics of an application using Azure OpenAI Services and the .NET SDK.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/generative-ai-dotnet-pt-2-powell</link>
      <guid>https://www.lqdev.me/responses/generative-ai-dotnet-pt-2-powell</guid>
      <pubDate>2023-09-06 09:29 -05:00</pubDate>
      <category>ai</category>
      <category>dotnet</category>
      <category>generativeai</category>
      <category>llm</category>
      <category>azure</category>
    </item>
    <item>
      <title>Use Azure CDN from Verizon Premium for Regex URL redirects</title>
      <description>&lt;![CDATA[&lt;p&gt;Well, that's time that I'm never going to get back.&lt;/p&gt;
&lt;p&gt;Imagine that you wanted to redirect a URL,&lt;/p&gt;
&lt;p&gt;From: &lt;code&gt;https://mydomain/github/repo-name&lt;/code&gt;&lt;br /&gt;
To: &lt;code&gt;https://github.com/username/repo-name&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You could use regular expressions to parse out the &lt;code&gt;repo-name&lt;/code&gt; part of the URL path from the source URL to the destination URL. This is what it might look like using something like NGINX.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;location ~ ^/github/(.*) {
     return 307 https://github.com/yourGHusername/$1;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unless I wasn't looking at the right documentation, it's not obvious that the rules engine for most Azure CDN plans don't support this feature, except for the &lt;a href="https://learn.microsoft.com/azure/cdn/cdn-verizon-premium-rules-engine-reference-features"&gt;Verizon Premium&lt;/a&gt;. While the standard rules engine supports wildcards for source URLs, lack of regular expresssions support means there's no way to parameterize the values of the destination URL. If this is something  you need, make sure to use the Verizon Premium plan.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/azure-cdn-regex-url-redirect</link>
      <guid>https://www.lqdev.me/notes/azure-cdn-regex-url-redirect</guid>
      <pubDate>2022-12-21 15:09 -05:00</pubDate>
      <category>azure</category>
      <category>azurecdn</category>
      <category>redirect</category>
      <category>indieweb</category>
      <category>selfhosting</category>
    </item>
    <item>
      <title>Now accepting webmentions</title>
      <description>&lt;![CDATA[&lt;p&gt;It took me a while to get all the pieces working but I'm excited that my website now accepts &lt;a href="https://www.w3.org/TR/webmention/"&gt;webmentions&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;A few months ago I implemented &lt;a href="https://www.lqdev.me/notes/webmentions-partially-implemented/"&gt;sending webmentions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Accepting them was a bit tricky because there were a few design decisions I was optimizing for.&lt;/p&gt;
&lt;p&gt;In the meantime, you can check out &lt;a href="https://github.com/lqdev/WebmentionFs"&gt;WebmentionFs&lt;/a&gt;, an F# library I built for validating and receiving webmentions along with the &lt;a href="https://github.com/lqdev/WebmentionService"&gt;WebmentionService&lt;/a&gt; Azure Functions backend that I'm using to process webmentions for my website.&lt;/p&gt;
&lt;p&gt;Also, feel free to send me a webmention. Here's my endpoint for now &lt;a href="https://lqdevwebmentions.azurewebsites.net/api/inbox"&gt;https://lqdevwebmentions.azurewebsites.net/api/inbox&lt;/a&gt;. Check out this &lt;a href="https://www.lqdev.me/posts/sending-webmentions-fsharp-fsadvent/"&gt;blog post&lt;/a&gt; where I show how you can send webmentions using F#. You can use any HTTP Client of your choosing to send an HTTP POST request to that endpoint with the body containing a link to one of my articles (target) and the link to your post (source).&lt;/p&gt;
&lt;p&gt;Stay tuned for a more detailed post coming on December 21st!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/now-accepting-webmentions</link>
      <guid>https://www.lqdev.me/notes/now-accepting-webmentions</guid>
      <pubDate>2022-12-18 22:41 -05:00</pubDate>
      <category>indieweb</category>
      <category>webmentions</category>
      <category>community</category>
      <category>fsharp</category>
      <category>azurefunctions</category>
      <category>azure</category>
    </item>
    <item>
      <title>Impressed by Azure Cloud Shell</title>
      <description>&lt;![CDATA[&lt;p&gt;I can probably count the number of times I've used the Azure Cloud Shell inside the Azure Portal with one hand. However, I recently had a need to use it and was impressed by the experience.&lt;/p&gt;
&lt;p&gt;I wanted to create an Azure Function app but didn't want to install all the tools required. Currently I'm running Manjaro on an ASUS L210MA. That's my daily driver and although the specs are incredibly low I like the portability. The Azure set of tools run on Linux but are easier to install on Debian distributions. The low specs plus the Arch distro didn't make me want to go through the process of installing the Azure tools locally for a throwaway app.&lt;/p&gt;
&lt;p&gt;That's when I thought about using Azure Cloud Shell. Setup was relatively quick and easy. It was also great to see the latest version of .NET, Azure Functions Core Tools, and Azure CLI were already installed. Even Git was already there. After creating the function app, I was able to make some light edits using the built-in Monaco editor. When I was ready to test the app, I was able to open up a port, create a proxy, and test the function locally. It's not a high-end machine so I won't be training machine learning models on it anytime soon. However, for experimentation and working with my Azure account it's more than I need.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/impressed-az-cloud-shell</link>
      <guid>https://www.lqdev.me/notes/impressed-az-cloud-shell</guid>
      <pubDate>2022-10-13 22:45 -05:00</pubDate>
      <category>azure</category>
      <category>cloud</category>
      <category>tools</category>
      <category>development</category>
    </item>
    <item>
      <title>A year's worth of Azure hosting costs</title>
      <description>&lt;![CDATA[&lt;p&gt;It's been a little over a year since I started hosting my website on Azure. The resources I use include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Azure Storage: Where all my site assets are stored.&lt;/li&gt;
&lt;li&gt;Azure CDN: Content delivery&lt;/li&gt;
&lt;li&gt;Azure Logic Apps: Event-based jobs to syndicate new posts on Twitter and Mastodon.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I haven't tracked traffic for the entire year but since October of last year the site has gotten roughly 5,000 visitors.&lt;/p&gt;
&lt;p&gt;Taking all that into account, I've only had to pay $2.06 most of it being the storage I use for the site. In comparison, I was previously hosting my website through my domain provider in a shared server where it cost about $60!&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/166848883-8d6cfe2c-5ff3-468c-b95f-b0ecab69595a.png" class="img-fluid" alt="Azure portal billing cost" /&gt;&lt;/p&gt;
&lt;p&gt;If you're interested in how this site is built, check out the &lt;a href="https://www.lqdev.me/colophon"&gt;colophon&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/azure-costs-website-hosting-2021</link>
      <guid>https://www.lqdev.me/notes/azure-costs-website-hosting-2021</guid>
      <pubDate>2022-05-04 20:36 -05:00</pubDate>
      <category>azure</category>
      <category>website</category>
      <category>cost</category>
      <category>hosting</category>
      <category>selfhosting</category>
      <category>cloud</category>
      <category>indieweb</category>
    </item>
  </channel>
</rss>