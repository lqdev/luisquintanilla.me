<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - quantization</title>
    <link>https://www.lqdev.me/tags/quantization</link>
    <description>All content tagged with 'quantization' by Luis Quintanilla</description>
    <lastBuildDate>2025-05-09 20:45 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Introducing AutoRound</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;As large language models (LLMs) and vision-language models (VLMs) continue to grow in size and complexity, deploying them efficiently becomes increasingly challenging. Quantization offers a solution by reducing model size and inference latency. Intel's AutoRound emerges as a cutting-edge quantization tool that balances accuracy, efficiency, and compatibility.&lt;br /&gt;
&lt;br&gt;
AutoRound is a weight-only post-training quantization (PTQ) method developed by Intel. It uses signed gradient descent to jointly optimize weight rounding and clipping ranges, enabling accurate low-bit quantization (e.g., INT2 - INT8) with minimal accuracy loss in most scenarios. For example, at INT2, it outperforms popular baselines by up to 2.1x higher in relative accuracy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2309.05516"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/intel/auto-round"&gt;Repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/intel-autoround</link>
      <guid>https://www.lqdev.me/bookmarks/intel-autoround</guid>
      <pubDate>2025-05-09 20:45 -05:00</pubDate>
      <category>quantization</category>
      <category>intel</category>
      <category>ai</category>
    </item>
    <item>
      <title>Quanto: a PyTorch quantization toolkit </title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Quantization is a technique to reduce the computational and memory costs of evaluating Deep Learning Models by representing their weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we are excited to introduce quanto, a versatile pytorch quantization toolkit, that provides several unique features:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;available in eager mode (works with non-traceable models)&lt;/li&gt;
&lt;li&gt;quantized models can be placed on any device (including CUDA and MPS),&lt;/li&gt;
&lt;li&gt;automatically inserts quantization and dequantization stubs,&lt;/li&gt;
&lt;li&gt;automatically inserts quantized functional operations,&lt;/li&gt;
&lt;li&gt;automatically inserts quantized modules (see below the list of supported modules),&lt;/li&gt;
&lt;li&gt;provides a seamless workflow for a float model, going from a dynamic to a static quantized model,&lt;/li&gt;
&lt;li&gt;supports quantized model serialization as a state_dict,&lt;/li&gt;
&lt;li&gt;supports not only int8 weights, but also int2 and int4,&lt;/li&gt;
&lt;li&gt;supports not only int8 activations, but also float8.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/quanto-pytorch-quantization-toolkit</link>
      <guid>https://www.lqdev.me/responses/quanto-pytorch-quantization-toolkit</guid>
      <pubDate>2024-03-19 22:00 -05:00</pubDate>
      <category>huggingface</category>
      <category>quantization</category>
      <category>pytorch</category>
      <category>tools</category>
      <category>ai</category>
    </item>
  </channel>
</rss>