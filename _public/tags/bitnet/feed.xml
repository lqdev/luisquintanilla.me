<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - bitnet</title>
    <link>https://www.lqdev.me/tags/bitnet</link>
    <description>All content tagged with 'bitnet' by Luis Quintanilla</description>
    <lastBuildDate>2024-02-28 22:03 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/era-1-bit-llms</link>
      <guid>https://www.lqdev.me/responses/era-1-bit-llms</guid>
      <pubDate>2024-02-28 22:03 -05:00</pubDate>
      <category>llm</category>
      <category>ai</category>
      <category>bitnet</category>
      <category>research</category>
    </item>
  </channel>
</rss>