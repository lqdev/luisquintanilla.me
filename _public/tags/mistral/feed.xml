<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - mistral</title>
    <link>https://www.lqdev.me/tags/mistral</link>
    <description>All content tagged with 'mistral' by Luis Quintanilla</description>
    <lastBuildDate>2025-04-08 08:19 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Ollama Adds Mistral 3.1 Support</title>
      <description>&lt;![CDATA[&lt;p&gt;In the latest &lt;a href="https://github.com/ollama/ollama/releases/tag/v0.6.5"&gt;Ollama 0.6.5 release&lt;/a&gt; support was added for &lt;a href="https://ollama.com/library/mistral-small3.1"&gt;Mistral Small 3.1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In an earlier post, I &lt;a href="https://www.lqdev.me/responses/mistral-small-3-1"&gt;highlighted the announcement and summarized the key features&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I've been using it with &lt;a href="https://github.com/marketplace/models/azureml-mistral/mistral-small-2503"&gt;GitHub Models&lt;/a&gt; and based on vibes I found it was more reliable at structured output compared to Gemma 3, especially when considering the model and context window size.&lt;/p&gt;
&lt;p&gt;Now that it's on Ollama thought, I can use it offline as well. However, I'm not sure how well that'll work on my ARM64 device, which only has 16GB of RAM.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/ollama-adds-support-mistral-small-3-1</link>
      <guid>https://www.lqdev.me/notes/ollama-adds-support-mistral-small-3-1</guid>
      <pubDate>2025-04-08 08:19 -05:00</pubDate>
      <category>mistral</category>
      <category>ollama</category>
      <category>ai</category>
      <category>llm</category>
    </item>
    <item>
      <title>Mistral Small 3.1</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Building on Mistral Small 3, this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini, while delivering inference speeds of 150 tokens per second.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Lightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/mistral-small-3-1</link>
      <guid>https://www.lqdev.me/responses/mistral-small-3-1</guid>
      <pubDate>2025-03-18 20:31 -05:00</pubDate>
      <category>mistral</category>
      <category>ai</category>
      <category>opensource</category>
    </item>
    <item>
      <title>Announcing Mistral Large</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mistral Large is our flagship model, with top-tier reasoning capacities. It is also available on Azure.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mistral Large comes with new capabilities and strengths:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is natively fluent in English, French, Spanish, German, and Italian, with a nuanced understanding of grammar and cultural context.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Its 32K tokens context window allows precise information recall from large documents.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Its precise instruction-following enables developers to design their moderation policies – we used it to set up the system-level moderation of le Chat.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;It is natively capable of function calling. This, along with constrained output mode, implemented on la Plateforme, enables application development and tech stack modernisation at scale.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;At Mistral, our mission is to make frontier AI ubiquitous. This is why we’re announcing today that we’re bringing our open and commercial models to Azure.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/announcing-mistral-large</link>
      <guid>https://www.lqdev.me/responses/announcing-mistral-large</guid>
      <pubDate>2024-02-26 15:39 -05:00</pubDate>
      <category>ai</category>
      <category>mistral</category>
      <category>llm</category>
      <category>opensource</category>
      <category>azure</category>
    </item>
  </channel>
</rss>