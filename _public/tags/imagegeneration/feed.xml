<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - imagegeneration</title>
    <link>https://www.lqdev.me/tags/imagegeneration</link>
    <description>All content tagged with 'imagegeneration' by Luis Quintanilla</description>
    <lastBuildDate>2024-11-04 21:28 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>In-Context LoRA for Diffusion Transformers</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20∼100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Repo: https://github.com/ali-vilab/In-Context-LoRA&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/in-context-lora</link>
      <guid>https://www.lqdev.me/responses/in-context-lora</guid>
      <pubDate>2024-11-04 21:28 -05:00</pubDate>
      <category>ai</category>
      <category>transformers</category>
      <category>diffusers</category>
      <category>imagegeneration</category>
      <category>lora</category>
      <category>finetuning</category>
    </item>
    <item>
      <title>Stable Diffusion 3 - Early Preview</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Announcing Stable Diffusion 3 in early preview, our most capable text-to-image model with greatly improved performance in multi-subject prompts, image quality, and spelling abilities.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Stable Diffusion 3 suite of models currently range from 800M to 8B parameters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable Diffusion 3 combines a diffusion transformer architecture and flow matching.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/stable-diffusion-3-early-preview</link>
      <guid>https://www.lqdev.me/responses/stable-diffusion-3-early-preview</guid>
      <pubDate>2024-02-22 20:59 -05:00</pubDate>
      <category>stabilityai</category>
      <category>ai</category>
      <category>stablediffusion</category>
      <category>imagegeneration</category>
      <category>ml</category>
      <category>image</category>
      <category>genai</category>
      <category>generativeai</category>
    </item>
    <item>
      <title>Introducing Würstchen: Fast Diffusion for Image Generation </title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Würstchen is a diffusion model, whose text-conditional component works in a highly compressed latent space of images. Why is this important? Compressing data can reduce computational costs for both training and inference by orders of magnitude. Training on 1024×1024 images is way more expensive than training on 32×32. Usually, other works make use of a relatively small compression, in the range of 4x - 8x spatial compression. Würstchen takes this to an extreme. Through its novel design, it achieves a 42x spatial compression! This had never been seen before, because common methods fail to faithfully reconstruct detailed images after 16x spatial compression. Würstchen employs a two-stage compression, what we call Stage A and Stage B. Stage A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found in the  paper). Together Stage A and B are called the Decoder, because they decode the compressed images back into pixel space. A third model, Stage C, is learned in that highly compressed latent space. This training requires fractions of the compute used for current top-performing models, while also allowing cheaper and faster inference. We refer to Stage C as the Prior.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/wurstchen-fast-image-generation</link>
      <guid>https://www.lqdev.me/bookmarks/wurstchen-fast-image-generation</guid>
      <pubDate>2023-09-17 12:15 -05:00</pubDate>
      <category>ai</category>
      <category>generativeai</category>
      <category>imagegeneration</category>
      <category>huggingface</category>
      <category>diffusion</category>
      <category>ml</category>
    </item>
    <item>
      <title>Efficient Controllable Generation for SDXL with T2I-Adapters </title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;a href="https://huggingface.co/papers/2302.08453"&gt;T2I-Adapter&lt;/a&gt; is an efficient plug-and-play model that provides extra guidance to pre-trained text-to-image models while freezing the original large text-to-image models. T2I-Adapter aligns internal knowledge in T2I models with external control signals. We can train various adapters according to different conditions and achieve rich control and editing effects.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Over the past few weeks, the Diffusers team and the T2I-Adapter authors have been collaborating to bring the support of T2I-Adapters for Stable Diffusion XL (SDXL) in diffusers. In this blog post, we share our findings from training T2I-Adapters on SDXL from scratch, some appealing results, and, of course, the T2I-Adapter checkpoints on various conditionings (sketch, canny, lineart, depth, and openpose)!&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/t2i-adapters-efficient-controllable-generation</link>
      <guid>https://www.lqdev.me/bookmarks/t2i-adapters-efficient-controllable-generation</guid>
      <pubDate>2023-09-17 12:11 -05:00</pubDate>
      <category>ai</category>
      <category>generativeai</category>
      <category>imagegeneration</category>
      <category>deeplearning</category>
      <category>ml</category>
      <category>huggingface</category>
    </item>
  </channel>
</rss>