<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - documentation</title>
    <link>https://www.lqdev.me/tags/documentation</link>
    <description>All content tagged with 'documentation' by Luis Quintanilla</description>
    <lastBuildDate>2024-08-08 20:16 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>GPT-4o System Card</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;GPT-4o is an autoregressive omni model, which accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It\u2019s trained end-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network.&lt;br /&gt;
&lt;br&gt;
GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window)2 in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.&lt;br /&gt;
&lt;br&gt;
In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House3, we are sharing the GPT-4o System Card, which includes our Preparedness Framework(opens in a new window)5 evaluations. In this System Card, we provide a detailed look at GPT-4o\u2019s capabilities, limitations, and safety evaluations across multiple categories, with a focus on speech-to-speech (voice)A while also evaluating text and image capabilities, and the measures we\u2019ve taken to enhance safety and alignment. We also include third party assessments on general autonomous capabilities, as well as discussion of potential societal impacts of GPT-4o text and vision capabilities.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/gpt4o-system-card</link>
      <guid>https://www.lqdev.me/bookmarks/gpt4o-system-card</guid>
      <pubDate>2024-08-08 20:16 -05:00</pubDate>
      <category>ai</category>
      <category>openai</category>
      <category>gpt-4o</category>
      <category>documentation</category>
      <category>llm</category>
    </item>
    <item>
      <title>Copilot for Docs</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Whether you’re learning a new library or API or you’ve been using it for years, it can feel like the documentation gets in your way more than it helps. Maybe the tutorials are too basic, or the reference manual is too sketchy, or the relevant information is split across multiple pages full of irrelevant details.&lt;/p&gt;
&lt;p&gt;We’re exploring a way to get you the information you need, faster. By surfacing the most relevant content for questions with tailored summaries that help connect the dots, Copilot for docs saves developers from scouring reams of documentation.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/copilot-docs</link>
      <guid>https://www.lqdev.me/bookmarks/copilot-docs</guid>
      <pubDate>2023-05-11 20:32 -05:00</pubDate>
      <category>ai</category>
      <category>documentation</category>
      <category>developers</category>
    </item>
  </channel>
</rss>