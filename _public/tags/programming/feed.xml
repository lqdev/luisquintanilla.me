<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - programming</title>
    <link>https://www.lqdev.me/tags/programming</link>
    <description>All content tagged with 'programming' by Luis Quintanilla</description>
    <lastBuildDate>2024-11-15 11:52 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>.NET Conf 2024 - Session Recordings</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;The recordings are out for .NET Conf 2024 in case you missed any of the sessions.&lt;/p&gt;
&lt;p&gt;Here are the recordings from the sessions I participated in.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://youtu.be/7Rw_ciSh2Wk"&gt;Building AI Applications From Scratch: A Hands-On Guide for .NET Developers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://youtu.be/MdM8G2e7jOg"&gt;Building the Foundation: AI Fundamentals in .NET&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, there's a &lt;a href="https://www.youtube.com/playlist?list=PLdo4fOcmZ0oXU8g5XIKiOPX4tgIqG2Ddc"&gt;Premier Bonus playlist&lt;/a&gt; which has a ton of amazing bonus content.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/dotnet-conf-2024-session-playlist</link>
      <guid>https://www.lqdev.me/responses/dotnet-conf-2024-session-playlist</guid>
      <pubDate>2024-11-15 11:52 -05:00</pubDate>
      <category>dotnet</category>
      <category>dotnetconf</category>
      <category>net9</category>
      <category>dotnet9</category>
      <category>programming</category>
      <category>dev</category>
    </item>
    <item>
      <title>NotebookLlama: An Open Source version of NotebookLM</title>
      <description>&lt;![CDATA[[bookmark] &lt;p&gt;This is cool! Already looking at how I can repurpose this sample for a project I have in mind.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/notebook-llama-oss-notebook-lm</link>
      <guid>https://www.lqdev.me/bookmarks/notebook-llama-oss-notebook-lm</guid>
      <pubDate>2024-11-01 09:32 -05:00</pubDate>
      <category>ai</category>
      <category>sample</category>
      <category>postcast</category>
      <category>texttospeech</category>
      <category>programming</category>
      <category>sample</category>
      <category>tutorial</category>
    </item>
    <item>
      <title>Note to Self: Useful Emacs Commands Pt. 1</title>
      <description>&lt;![CDATA[&lt;p&gt;I still need to set up an org-capture template for snippets, so writing a note to myself to remember these commands I'll eventually want to come back to.&lt;/p&gt;
&lt;h2&gt;Capture elfeed link&lt;/h2&gt;
&lt;p&gt;In this custom function, &lt;code&gt;elfeed-show-yank&lt;/code&gt; extracts the link element in an elfeed entry. &lt;code&gt;org-capture&lt;/code&gt; then just invokes the org-capture template selection prompt. At this point, I can move forward with creating a response entry on the website and since the link to the entry I was viewing in elfeed is in the kill-ring, I can easily paste the URL while filling out the org-capture template.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-lisp"&gt;(defun capture-elfeed-entry ()
  (elfeed-show-yank)
  (org-capture))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Org capture contexts&lt;/h2&gt;
&lt;p&gt;I recently found out, I can add filters to org-capture templates based on the mode I'm in Emacs. Here's an example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-lisp"&gt;(setq org-capture-templates-contexts
      '((&amp;quot;wrn&amp;quot; ((in-mode . &amp;quot;elfeed-show-mode&amp;quot;)))))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;org-capture-templates-contexts&lt;/code&gt; defines a set of conditions that defines which contexts certain org-capture templates appear under.&lt;/p&gt;
&lt;p&gt;For example, the org-capture template mapped to &lt;code&gt;wrn&lt;/code&gt; will only be visible when org-capture is invoked from a buffer in &lt;code&gt;elfeed-show-mode&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;More information can be found in the &lt;a href="https://orgmode.org/manual/Templates-in-contexts.html"&gt;org-capture templates in context documentation&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/useful-emacs-commands-pt-1</link>
      <guid>https://www.lqdev.me/notes/useful-emacs-commands-pt-1</guid>
      <pubDate>2024-07-28 17:00 -05:00</pubDate>
      <category>emacs</category>
      <category>lisp</category>
      <category>elisp</category>
      <category>programming</category>
      <category>code</category>
      <category>notetoself</category>
    </item>
    <item>
      <title>Home-Cooked Software and Barefoot Developers</title>
      <description>&lt;![CDATA[[star] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The emerging golden age of home-cooked software, barefoot developers, and why the local-first community should help build it&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This is a talk I presented Local-first Conference in Berlin, May 2024. It's specifically directed at the local-first
community, but its relevant to anyone involved in building software.&lt;br /&gt;
&lt;/br&gt;
For the last ~year I've been keeping a close eye on how language models capabilities meaningfully change the speed, ease, and accessibility of software development. The slightly bold theory I put forward in this talk is that we're on a verge of a golden age of local, home-cooked software and a new kind of developer – what I've called the barefoot developer.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/home-cooked-software-barefoot-developers-appleton</link>
      <guid>https://www.lqdev.me/responses/home-cooked-software-barefoot-developers-appleton</guid>
      <pubDate>2024-07-06 22:02 -05:00</pubDate>
      <category>localfirst</category>
      <category>smallweb</category>
      <category>sofware</category>
      <category>llm</category>
      <category>programming</category>
      <category>talk</category>
      <category>indieweb</category>
    </item>
    <item>
      <title>DevContainer configurations</title>
      <description>&lt;![CDATA[&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;A collection of DevContainer configurations&lt;/p&gt;
&lt;h2&gt;Base Debian Image&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Base Debian DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Python&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Python DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/python:1&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;3.11&amp;quot;
        },
        &amp;quot;ghcr.io/va-h/devcontainers-features/uv:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;                
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Python (GPU)&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Python (GPU) DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/python:1&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;3.11&amp;quot;
        },
        &amp;quot;ghcr.io/devcontainers/features/nvidia-cuda:1&amp;quot;: {},
        &amp;quot;ghcr.io/va-h/devcontainers-features/uv:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;                
            ]
        }
    },
    &amp;quot;runArgs&amp;quot;: [
        &amp;quot;--gpus&amp;quot;, 
        &amp;quot;all&amp;quot;
    ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;.NET&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me .NET DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/dotnet:2&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;9.0&amp;quot;
        }
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;Ionide.Ionide-fsharp&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;,
                &amp;quot;ms-dotnettools.csdevkit&amp;quot;                                
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;.NET (GPU)&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me .NET (GPU) DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/dotnet:2&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;9.0&amp;quot;
        },
        &amp;quot;ghcr.io/devcontainers/features/nvidia-cuda:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;Ionide.Ionide-fsharp&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;,
                &amp;quot;ms-dotnettools.csdevkit&amp;quot;                
            ]
        }
    },
    &amp;quot;runArgs&amp;quot;: [
        &amp;quot;--gpus&amp;quot;, 
        &amp;quot;all&amp;quot;
    ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Python and .NET&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Python and .NET DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/python:1&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;3.11&amp;quot;
        },        
        &amp;quot;ghcr.io/devcontainers/features/dotnet:2&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;9.0&amp;quot;
        },
        &amp;quot;ghcr.io/va-h/devcontainers-features/uv:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;Ionide.Ionide-fsharp&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;,
                &amp;quot;ms-dotnettools.csdevkit&amp;quot;                
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Python and .NET (GPU)&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Python and .NET (GPU) DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/python:1&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;3.11&amp;quot;
        },        
        &amp;quot;ghcr.io/devcontainers/features/dotnet:2&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;9.0&amp;quot;
        },
        &amp;quot;ghcr.io/devcontainers/features/nvidia-cuda:1&amp;quot;: {},
        &amp;quot;ghcr.io/va-h/devcontainers-features/uv:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;Ionide.Ionide-fsharp&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;,
                &amp;quot;ms-dotnettools.csdevkit&amp;quot;               
            ]
        }
    },
    &amp;quot;runArgs&amp;quot;: [
        &amp;quot;--gpus&amp;quot;, 
        &amp;quot;all&amp;quot;
    ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Additional Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://code.visualstudio.com/docs/devcontainers/containers"&gt;Developing inside a DevContainer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/devcontainers/images"&gt;Pre-built DevContainer images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/devcontainers/features"&gt;Pre-built DevContainer features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.github.com/en/codespaces/overview"&gt;GitHub Codespaces overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://marketplace.visualstudio.com/vscode"&gt;VS Code Extensions Marketplace&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/resources/wiki/devcontainers-configurations</link>
      <guid>https://www.lqdev.me/resources/wiki/devcontainers-configurations</guid>
      <pubDate>06/29/2024 14:48 -05:00</pubDate>
      <category>devcontainer</category>
      <category>vscode</category>
      <category>codespaces</category>
      <category>development</category>
      <category>software</category>
      <category>tech</category>
      <category>programming</category>
      <category>python</category>
      <category>dotnet</category>
      <category>csharp</category>
      <category>fsharp</category>
      <category>docker</category>
      <category>git</category>
      <category>debian</category>
    </item>
    <item>
      <title>Deep Dive into Ownership in Mojo</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;In the second part of the ownership series in Mojo, we built on the mental model developed in the &lt;a href="http://www.modular.com/blog/what-ownership-is-really-about-a-mental-model-approach"&gt;first part&lt;/a&gt; and provided practical examples to illustrate how ownership works in Mojo. We covered the different kinds of values (BValue, LValue, and RValue) and how they propagate through expressions. We also explained the function argument conventions (borrowed, inout, owned) and demonstrated how these conventions help manage memory safely and efficiently. We concluded with three fundamental rules:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Rule 1&lt;/strong&gt;: Owned arguments take RValue on the caller side but are LValue on the callee side.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rule 2&lt;/strong&gt;: Owned arguments own the type if the transfer operator ^ is used; otherwise, they copy the type if it is Copyable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rule 3&lt;/strong&gt;: Copy operations are optimized to move operations if the type is Copyable and Movable and isn’t used anymore, reducing unnecessary overhead.&lt;br /&gt;
&lt;br&gt;
Lastly, we emphasized that the main goals of ownership in Mojo are:&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory Safety&lt;/strong&gt;: Enforcing exclusive ownership and proper lifetimes to prevent memory errors such as use-after-free and double-free.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance Optimization&lt;/strong&gt;: Converting unnecessary copy operations into move operations to reduce overhead and enhance performance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ease of Use&lt;/strong&gt;: Automating memory management through ownership rules and the transfer operator, simplifying development.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compile-Time Guarantees&lt;/strong&gt;: Providing strong compile-time guarantees through type-checking and dataflow lifetime analysis, catching errors early in the development process.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/deep-dive-ownership-mojo</link>
      <guid>https://www.lqdev.me/responses/deep-dive-ownership-mojo</guid>
      <pubDate>2024-06-11 21:20 -05:00</pubDate>
      <category>mojo</category>
      <category>python</category>
      <category>c</category>
      <category>cpp</category>
      <category>c++</category>
      <category>pl</category>
      <category>programming</category>
      <category>programminglanguage</category>
    </item>
    <item>
      <title>LLM training in simple, raw C/CUDA </title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;LLM training in simple, pure C/CUDA. There is no need for 245MB of PyTorch or 107MB of cPython. For example, training GPT-2 (CPU, fp32) is ~1,000 lines of clean code in a single file. It compiles and runs instantly, and exactly matches the PyTorch reference implementation. I chose GPT-2 as the first working example because it is the grand-daddy of LLMs, the first time the modern stack was put together.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/llm-c-karpathy</link>
      <guid>https://www.lqdev.me/responses/llm-c-karpathy</guid>
      <pubDate>2024-04-09 22:15 -05:00</pubDate>
      <category>llm</category>
      <category>gpt</category>
      <category>c</category>
      <category>programming</category>
      <category>learning</category>
      <category>tutorial</category>
    </item>
    <item>
      <title>The missing graph datatype already exists. It was invented in the '70s</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The datatype for a graph is a relation, and graph algorithms are queries on the relation. But modern languages need better support for the relational model.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This post is a response to/inspired by &lt;a href="https://www.hillelwayne.com/post/graph-types/"&gt;The Hunt for the Missing Data Type (HN) by Hillel Wayne&lt;/a&gt;. I suggest reading his article first.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I claim the reason why it is so difficult to support graphs in languages nowadays is because the imperative/structured programming model of modern programming languages is ill-suited for graph algorithms. As Wayne correctly points out, the core problem is that when you write a graph algorithm in an imperative language like Python or Rust, you have to choose some explicit representation for the graph. Then, your traversal algorithm is dependent on the representation you chose. If you find out later that your representation is no longer efficient, it is a lot of work to adapt your algorithms for a new representation.&lt;br /&gt;
&lt;br&gt;
So what if we just, like, didn’t do this?&lt;br /&gt;
&lt;br&gt;
We already have a declarative programming language where expressing graph algorithms is extremely natural—Datalog, whose semantics are based on* the relational algebra, which was developed in the 1970s.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Wonderful! Except for the “writing Datalog” part.&lt;br /&gt;
&lt;br&gt;
If Datalog is so great, why hasn’t it seen more adoption?&lt;br /&gt;
&lt;br&gt;
The short answer is that Datalog is relatively esoteric outside of academia and some industry applications and, as a result, is not a great language from a “software engineering” perspective. It is hard for programmers accustomed to imperative code to write Datalog programs, and large Datalog programs can be hard to write and understand.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/datalog-the-missing-graph-data-type-already-exists</link>
      <guid>https://www.lqdev.me/responses/datalog-the-missing-graph-data-type-already-exists</guid>
      <pubDate>2024-03-06 21:39 -05:00</pubDate>
      <category>datalog</category>
      <category>programming</category>
      <category>graphs</category>
      <category>datatypes</category>
      <category>programminglanguages</category>
      <category>software</category>
      <category>database</category>
      <category>algorithms</category>
    </item>
    <item>
      <title>Setting up your Rust development environment using Dev Containers</title>
      <description>&lt;![CDATA[
Setting up a new development environment that's configured with all the SDKs and tools you need to get started working can be a multi-step process that is error-prone. Especially when you don't know whether you want to commit to a specific technology or don't have too much space on your device, you might not want to install everything on your device. Additionally, cleaning up and removing everything you installed isn't always 100%. This is where development environments like Dev Containers can come in handy. 

## What are Dev Containers

A development container (Dev Container) lets you use a container as a full development environment. 

Because your development environment is in a container, the environment is isolated. The builds are reproducible and disposable. This means you can quickly stand up a new clean development environments and dispose of them just as easily. 

For more details, see the [Dev Containers website](https://containers.dev/).

If you have Docker installed, you can run them locally. Otherwise, you can run Dev Containers using GitHub Codespaces

## What are GitHub Codespaces

A codespace is a development environment that's hosted in the cloud.

For more details, see the [GitHub Codespaces website](https://github.com/features/codespaces).

## Set up Dev Container for Rust Development

1. Create a new directory.
1. Add a Dev Container configuration file called *.devcontainer.json* that contains the following content:

    ```json
    {
        "name": "Rust Development Environment",
        "image": "mcr.microsoft.com/devcontainers/rust:latest",
        "customizations": {
            "vscode": {
                "extensions": [
                    "rust-lang.rust-analyzer"
                ]
            }
        }
    }
    ```

    - **name**: A human friendly name to identify your development environment. It's not as important when you only have one environment but if you have different environments, it can be useful for differentiating between the various configurations. 
    - **image**: For the most customization, you can provide the path to a Docker file. However, to make things easier, there are a set of prebuilt container images. In this case, I'm using the [Rust image](https://github.com/devcontainers/images/tree/main/src/rust).
    - **customizations.vscode.extensions**: A list of Visual Studio Code extensions that can help you with your development. In this case, it's the [*rust-analyzer*](https://marketplace.visualstudio.com/items?itemName=rust-lang.rust-analyzer).

At minimum, this is all you need to get started. 

## Start your environment 

[Visual Studio Code provides good support for Dev Containers](https://code.visualstudio.com/docs/devcontainers/tutorial). 

1. Install [VS Code](https://code.visualstudio.com/download)
1. Open your directory where you configured your Dev Container in Visual Studio Code
1. Install the [Dev Containers VS Code extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers).
1. Open the command palette in VS Code. In the menu bar, select **View &gt; Command Palette.**
1. Enter the following command **&gt;Dev Containers: Open Dev Container**. This will start building your dev container. Wait for it to start.

## Check your environment

Once your Dev Container starts, check that `rustc` is installed.

1. Open the terminal.
1. Run `rustc -h`. A help message should output to the console for the Rust compiler. 

## Build your app

For simplicity, I've used the [Hello World sample from the Rust By Example website](https://doc.rust-lang.org/rust-by-example/hello.html).

1. Create a new file called *hello.rs*.
1. Add the following code:

    ```rust
    fn main ()
    {
        println!("Hello World!");
    }
    ```
1. Compile your program

    ```bash
    rustc hello.rs
    ```

    This will generate an executable called *hello*

1. Run your app

    ```bash
    ./hello
    ```

    The application should print out the string "Hello World" to the console. 

That's it! You now have a Rust development environment to get started learning and building apps. 

## Conclusion

Dev Containers make it easy for you to configure full-featured development environments. When paired with Visual Studio Code and GitHub Codespaces, they can help you focus on learning and building applications rather than setting up your environment. You can find a [final version of the configured environment and app on GitHub](https://github.com/lqdev/rust-codespace-sandbox). Happy coding! 

## Additional Resources

- [Rust in Visual Studio Code](https://code.visualstudio.com/docs/languages/rust)
- [Rust by Example](https://doc.rust-lang.org/rust-by-example/index.html)
- [Developing Inside a Container](https://code.visualstudio.com/docs/devcontainers/containers)]]&gt;</description>
      <link>https://www.lqdev.me/posts/setting-up-rust-dev-env-devcontainers-vscode</link>
      <guid>https://www.lqdev.me/posts/setting-up-rust-dev-env-devcontainers-vscode</guid>
      <pubDate>2024-01-31 21:05 -05:00</pubDate>
      <category>rust</category>
      <category>vscode</category>
      <category>devcontainers</category>
      <category>codespaces</category>
      <category>programming</category>
      <category>technology</category>
      <category>docker</category>
      <category>containers</category>
      <category>github</category>
    </item>
    <item>
      <title>Getting Started With CUDA for Python Programmers</title>
      <description>&lt;![CDATA[[bookmark] &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=nOxKexn3iBo" title="A YouTube Video Tutorial by Jeremy Howard to help Python programmers get started with CUDA"&gt;&lt;img src="http://img.youtube.com/vi/nOxKexn3iBo/0.jpg" class="img-fluid" alt="A YouTube Video Tutorial by Jeremy Howard to help Python programmers get started with CUDA" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In this comprehensive video tutorial, Jeremy Howard from answer.ai demystifies the process of programming NVIDIA GPUs using CUDA, and simplifies the perceived complexities of CUDA programming. Jeremy emphasizes the accessibility of CUDA, especially when combined with PyTorch's capabilities, allowing for programming directly in notebooks rather than traditional compilers and terminals. To make CUDA more approachable to Python programmers, Jeremy shows step by step how to start with Python implementations, and then convert them largely automatically to CUDA. This approach, he argues, simplifies debugging and development.&lt;br /&gt;
&lt;br&gt;
The tutorial is structured in a hands-on manner, encouraging viewers to follow along in a Colab notebook. Jeremy uses practical examples, starting with converting an RGB image to grayscale using CUDA, demonstrating the process step-by-step. He further explains the memory layout in GPUs, emphasizing the differences from CPU memory structures, and introduces key CUDA concepts like streaming multi-processors and CUDA cores.&lt;br /&gt;
&lt;br&gt;
Jeremy then delves into more advanced topics, such as matrix multiplication, a critical operation in deep learning. He demonstrates how to implement matrix multiplication in Python first and then translates it to CUDA, highlighting the significant performance gains achievable with GPU programming. The tutorial also covers CUDA's intricacies, such as shared memory, thread blocks, and optimizing CUDA kernels.&lt;br /&gt;
&lt;br&gt;
The tutorial also includes a section on setting up the CUDA environment on various systems using Conda, making it accessible for a wide range of users.&lt;br /&gt;
&lt;br&gt;
This is lecture 3 of the &amp;quot;CUDA Mode&amp;quot; series (but you don't need to watch the others first). The notebook is available in the lecture3 folder here: &lt;a href="https://github.com/cuda-mode/lecture2"&gt;https://github.com/cuda-mode/lecture2&lt;/a&gt;...&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/get-started-cuda-python-programmers</link>
      <guid>https://www.lqdev.me/bookmarks/get-started-cuda-python-programmers</guid>
      <pubDate>2024-01-29 20:23 -05:00</pubDate>
      <category>gpu</category>
      <category>cuda</category>
      <category>python</category>
      <category>programming</category>
    </item>
    <item>
      <title>Marimo</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;marimo is an open-source reactive notebook for Python — reproducible, git-friendly, executable as a script, and shareable as an app.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/marimo-notebook</link>
      <guid>https://www.lqdev.me/bookmarks/marimo-notebook</guid>
      <pubDate>2024-01-13 09:38 -05:00</pubDate>
      <category>notebook</category>
      <category>programming</category>
      <category>literateprogramming</category>
    </item>
    <item>
      <title>Python 3.13 gets a JIT</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;In late December 2023 (Christmas Day to be precise), CPython core developer &lt;a href="https://github.com/python/cpython/pull/113465"&gt;Brandt Bucher submitted a little pull-request&lt;/a&gt; to the Python 3.13 branch adding a JIT compiler.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/python-3-13-gets-jit</link>
      <guid>https://www.lqdev.me/bookmarks/python-3-13-gets-jit</guid>
      <pubDate>2024-01-09 09:53 -05:00</pubDate>
      <category>python</category>
      <category>programming</category>
      <category>compiler</category>
    </item>
    <item>
      <title>TIOBE Index for January 2024</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;For the first time in the history of the TIOBE index, C# has won the programming language of the year award. Congratulations! C# has been a top 10 player for more than 2 decades and now that it is catching up with the big 4 languages, it won the well-deserved award by being the language with the biggest uptick in one year (+1.43%).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Exciting to see F# almost break into the Top 20 at number 22 with 0.77%.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/tiobe-index-january-2024</link>
      <guid>https://www.lqdev.me/bookmarks/tiobe-index-january-2024</guid>
      <pubDate>2024-01-07 22:52 -05:00</pubDate>
      <category>programming</category>
      <category>dotnet</category>
      <category>technology</category>
      <category>fsharp</category>
      <category>csharp</category>
      <category>programminglanguages</category>
    </item>
    <item>
      <title>POSSE to Mastodon using RSS and Azure Logic Apps</title>
      <description>&lt;![CDATA[
## Introduction

[Over the past year, I've made this website the main place where I post content](/notes/weblogging-rewind-2023/). The easiest way to [subscribe to content on my website](/feed) is through the various RSS feeds. However, I have accounts on other platforms like X (formerly Twitter), Bluesky, and Mastodon where I'd still like to repost my content to. Since the [changes to the Twitter API](https://techcrunch.com/2023/03/29/twitter-announces-new-api-with-only-free-basic-and-enterprise-levels/), the only place I cross-post to is Mastodon. The main reason behind it is, as of now, it's one of the few platforms that allows me to automate post creation via its REST APIs without restrictions. A large part of that is I self-host my own Mastodon instance but I assume there aren't as many restrictions when using other instances like mastodon.social. The way I automate posting is by setting up workflows using Azure Logic Apps. These workflows subscribe to my various RSS feeds and whenever a new post is published, they make an HTTP request to my Mastodon instance to create a new post. I've been doing this for some time but never got around to documenting it. This blog post goes into more details about how to set up these workflows.  

## What is POSSE

[POSSE](https://indieweb.org/POSSE) is short for "Post on your Own Site Syndicate Elsewhere". It's one of the patterns adopted within IndiWeb communities and projects. The main idea is, your website or a website you own or have administrative rights to becomes the main platform where you publish your content. Effectively, it's the cannonical version of your content. Once your content is on your website, you can optionally choose to distribute it on other platforms. 

## What is RSS

[RSS](https://en.wikipedia.org/wiki/RSS) is short for Really Simple Syndication. Accorting to Wikipedia, this protocol "allows users and applications to access updates to websites in a standardized, computer-readable format". 

## What is Mastodon

[Mastodon](https://joinmastodon.org/) is an [open-source](https://github.com/mastodon/mastodon) decentralized microblogging platform built on the [ActivityPub](https://activitypub.rocks/) protocol and part of the larger collective of federated systems known as the [Fediverse](https://fediverse.info/). 

## What are Azure Logic Apps

If you've used workflow automation systems like [IFTTT](https://ifttt.com/), then you generally know what Azure Logic Apps are. 

A more formal definition from the Azure documentation - "Azure Logic Apps is a cloud platform where you can create and run automated workflows with little to no code. By using the visual designer and selecting from prebuilt operations, you can quickly build a workflow that integrates and manages your apps, data, services, and systems."

For more details, see the [Azure Logic Apps documentation](https://learn.microsoft.com/azure/logic-apps/logic-apps-overview).

## Prerequisites

Since you'll be creating Logic App Resources on Azure, you'll need an [Azure account](https://aka.ms/free).

## Create Consumption Azure Logic App Resource

There's various ways to create an Azure Logic App Resource but the easiest one for this relatively simple workflow is using the Azure Portal. For more details, see the [create a consumption logic app resource documentation](https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#create-a-consumption-logic-app-resource).

For the most part you can leave the defaults as is. Since the intended use for this workflow is personal and I don't need enterprise features, I chose to create my logic app using the consumption plan. 

When prompted to choose **Plan Type**, select **Consumption**. 

## The Workflow

The workflow is relatively simple. Whenever a new item is posted to an RSS feed, make an HTTP POST request to the Mastodon API

```mermaid
flowchart TD
    A["RSS"] --&gt; B["HTTP (Mastodon API)"]
```

Once your Logic App resource deploys to Azure, create a new logic app using the Blank Template. For more details, see the [select a blank template documentation](https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#select-the-blank-template).

This will launch you into the Logic app designer UI where you can begin to configure your workflow.

### RSS trigger

The first thing you'll want to do is set up the trigger that initiates the workflow. The trigger in this case will be new posts on an RSS feed.

In the Logic app designer, search for *RSS* and add it as a trigger.

Once the RSS trigger is added to your workflow, configure it as follows:

- **The RSS feed URL**: The feed you want want to get posts from. In this case, I'm using my blog posts [feed](/posts/feed.xml). 

    **TIP: The RSS feed can be ANY feed. So if you wanted to subscribe and repost items from your favorite blog or website, you can as well. Just note though that if you don't own the content it might not be something the author wants you doing and for websites that update multiple times a day, it might produce a lot of noise in your feed. Other than that though, subscribe and post away!**

- **How often do you want to check for items?**: The frequency at which you want to poll the RSS feed and check for new posts. In my case, I don't post very often but I do want my posts to be published on Mastodon within an hour of publishing. Therefore, I chose 1 hour as the update frequency. 

### HTTP action

Now that your trigger is configured, it's time to do someting with the latest posts. In this case, since I want to create a new post on Mastodon, I can do so via their REST API. 

#### Get Mastodon credentials

One thing that you'll need to publish posts to Mastodon is an application token. The easiest way to get one is using the Web UI. 

To get your app credentials through the web UI:

1. In the Mastodon Web UI, select **Preferences**.
1. In the preferences page, select **Development**.
1. In the development preferences tab, select **New application**.
1. In the new application page:
  - Provide a name for your application
  - Choose the **write:statuses** scope checkbox. For details on required scopes to post statuses, see the [Post a new status REST API documentation](https://docs.joinmastodon.org/methods/statuses/#create)
  - Select **Submit**

If successful, this will create credentials and a token you can use to send authenticated requests to the Mastodon API.

#### Configure HTTP action

Now that you have your credentials, you can configure your HTTP action in the Logic Apps workflow.

In the Logic App designer:

1. Select **+ New Step**.
1. Search for *HTTP* and add it as an action. 
1. Once the HTTP action is added, configure it as follows:

- **Method**: The HTTP Method. To create statuses, choose **POST** from the dropdown.
- **URI**: The endpoint to make the requests to. For mastodon, it's `https://&lt;HOST&gt;/api/v1/statuses`. Make sure to replace `&lt;HOST&gt;` with your Mastodon instance. In my case, it's `toot.lqdev.tech` since that's where I host my mastodon instance. 
- **Headers**: HTTP Headers to use. In this case, set the following:

    | Key | Value |
    | --- | --- |
    | Content-Type | application/x-www-form-urlencoded |

- **Queries**: URL Query Parameters to add to the request. This is where you'll set your access token.

    | Key | Value |
    | --- | --- |
    | access_token | `[YOUR-ACCESS-TOKEN]` |

    Make sure to replace `[YOUR-ACCESS-TOKEN]` with the token credential generated in the Mastodon Web UI.
- **Body**: The content to be published in your Mastodon post. This is entirely up to you though at minumum, you'll add the following to the text feed. `status=&lt;YOUR-CONTENT&gt;`. `&lt;YOUR-CONTENT&gt;` is what will be displayed in the Mastodon post. 

    One of the nice things about Azure Logic Apps is, properties from previous steps are available to you in subsequent steps. Since our trigger is an RSS feed, we can get access to the feed and item properties of our feed in the HTTP action. If all you wanted was to post the tile and URL, you can do that using the **Feed title** and **Primary Feed Link** properties. For more details, see the [RSS connector documentation](https://learn.microsoft.com/connectors/rss/). 

## Save and run

That's it! Now you just need to select **Save** in the Logic app designer page. 

Once it's saved, click **Run trigger** which will kick off your trigger. If you have anything recent to publish and everything is configured correctly, it should show up in your Mastodon feed.

## Conclusion

By publishing content on your own website first, you're in full control of your content. Regardless of which platforms come and go, you won't have to adjust to those changes because your content is not locked in to those platforms. However, that doesn't mean you can't also publish your content there. Using protocols like RSS make it easy to subscribe to updates on your website. Using REST APIs provided by the respective platforms, you can automate publishing these updates. To further automate and simplify this process, you can use services like Azure Logic Apps to make publishing to all places easy. ]]&gt;</description>
      <link>https://www.lqdev.me/posts/rss-to-mastodon-posse-azure-logic-apps</link>
      <guid>https://www.lqdev.me/posts/rss-to-mastodon-posse-azure-logic-apps</guid>
      <pubDate>2023-12-24 16:40 -05:00</pubDate>
      <category>azure</category>
      <category>mastodon</category>
      <category>indieweb</category>
      <category>logicapps</category>
      <category>fediverse</category>
      <category>posse</category>
      <category>internet</category>
      <category>web</category>
      <category>blogging</category>
      <category>blog</category>
      <category>automation</category>
      <category>programming</category>
      <category>rss</category>
    </item>
    <item>
      <title>fsharpConf 2023</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;I'm really enjoying the sessions. Kudos to the team who put it together and presenters delivering great content.&lt;/p&gt;
&lt;p&gt;If you're interested, check out the stream at &lt;a href="http://fsharpconf.com/"&gt;http://fsharpconf.com/&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/fsharp-conf-2023</link>
      <guid>https://www.lqdev.me/responses/fsharp-conf-2023</guid>
      <pubDate>2023-06-26 13:02 -05:00</pubDate>
      <category>fsharp</category>
      <category>fsharpConf</category>
      <category>dotnet</category>
      <category>programming</category>
      <category>community</category>
      <category>opensource</category>
    </item>
    <item>
      <title>Serverless Racket Applications Using Azure Functions Custom Handlers</title>
      <description>&lt;![CDATA[
## Introduction

[Racket](https://racket-lang.org/) is a fun and powerful general purpose programming language based on the Scheme dialect of Lisp that provides tools and packages that allow individuals to quickly be productive. Although you can build traditional web applications with it, it would be nice to use it with cloud-native technologies like serverless. Azure Functions is designed for these types of event-driven workflows but unfortunately does not officially support Racket. Recently, a new feature called custom handlers was announced which allows individuals to run web applications written in any language that supports HTTP primitives as an Azure Function. When I learned of this feature, my immediate thought was, challenge accepted!

Custom handlers require the following:

1. Write a web server to process requests
2. Define the bindings for the request and response function payloads
3. Configure the Azure Functions host to send request to the web server

In this writeup, I'll show how to set up a Racket web server that processes `GET` requests running as an Azure Function. The source code for this project can be found in the [RacketAzureFunctionsCustomHandlerSample GitHub repository](https://github.com/lqdev/RacketAzureFunctionsCustomHandlerSample)

## Prerequisites

This project was built on a Windows 10 PC, but it should work cross-platform on Mac and Linux.

- [Node.js](https://nodejs.org/en/)
- [Racket](https://download.racket-lang.org/)
- [Azure Functions Core Tools](https://docs.microsoft.com/azure/azure-functions/functions-run-local). This sample uses v2.x of the tool.

## Create Azure Functions project

Create a new directory called `RacketAzureFunctionsCustomHandlerSample` and navigate to it.

## Create Racket server

The way Azure Functions custom handlers work is by having the Azure Functions host proxy requests to a web server written in the language of choice which processes the request and sends the response back to the Azure Functions host.

Start by setting the environment variable where Azure Functions and the server listen on. Create and environment variable called `FUNCTIONS_HTTPWORKER_PORT`. In this example, I set the variable to `7071`.

Inside of your application directory, create a file called *server.rkt* which will contain the server logic.

Open the *server.rkt* file. Define the language and import the required packages.

```racket
#lang racket
(require json)
(require web-server/servlet)
(require web-server/servlet-env)
```

Then, get the port where the server listens on from the `FUNCTIONS_HTTPWORKER_PORT` environment variable.

```racket
(define PORT (string-&gt;number (getenv "FUNCTIONS_HTTPWORKER_PORT")))
```

Next, create a function called `get-values` to process your request. In this case, the function receives a `GET` request that returns a JSON object containing a list of integers.

```racket
(define (get-values req)
    (response/full
        200
        #"OK"
        (current-seconds)
        #"application/json;charset=utf-8"
        empty
        (list (jsexpr-&gt;bytes #hasheq((value . (1 2 3)))))))
```

After that, define the routes so your server dispatches requests to the appropriate endpoint. In this case, `GET` requests to the `/values` endpoint are sent to and processed by the `get-values` function.

```racket
(define-values (dispatch req)
    (dispatch-rules
        [("values") #:method "get" get-values]
        [else (error "Route does not exist")]))
```

The final *server.rkt* file should contains content similar to the one below:

```racket
;; Define language and import packages
#lang racket
(require json)
(require web-server/servlet)
(require web-server/servlet-env)

;; Get port where server listens on
(define PORT (string-&gt;number (getenv "FUNCTIONS_HTTPWORKER_PORT")))

;; Create function to handle GET /values request
(define (get-values req)
    (response/full
        200
        #"OK"
        (current-seconds)
        #"application/json;charset=utf-8"
        empty
        (list (jsexpr-&gt;bytes #hasheq((value . (1 2 3)))))))

;; Define routes
(define-values (dispatch req)
    (dispatch-rules
        [("values") #:method "get" get-values]
        [else (error "Route does not exist")]))

;; Define and start server
(serve/servlet
    (lambda (req) (dispatch req))
    #:launch-browser? #f
    #:quit? #f
    #:port PORT
    #:servlet-path "/"
    #:servlet-regexp #rx"")
```

## Test the Racket server

Start the server by running the following command:

```bash
racket --require server.rkt
```

Then, using an application like Postman or Insomina, make a `GET` request to `http://localhost:7071/values`.

The response should look like the following:

```json
{
    "value": [
        1,
        2,
        3
    ]
}
```

## Define function bindings

The way Azure Functions discovers functions is through subdirectories containing a binding definition called *function.json*. The name of the subdirectories must match the name of your function's route path. For example if the route path is `/values`, then the name of the subdirectory is  `values`.

Create a subdirectory inside the main application directory called *values*

Inside the *values* subdirectory, create a file called *function.json* and add the following content to it.

```json
{
    "bindings": [
      {
        "type": "httpTrigger",
        "direction": "in",
        "name": "req",
        "methods": ["get"]
      },
      {
        "type": "http",
        "direction": "out",
        "name": "res"
      }
    ]
}
```

The *function.json* file defines the request and response payloads. In this case, the incoming request is an `HttpTrigger` that only handles `GET` requests and returns an HTTP response.

## Create server executable

Package your server application into a single executable by entering the following command into the command prompt:

```bash
raco exe server.rkt
```

Once your application is packaged, an executable with the name *server.exe* should be created in your application directory.

## Configure Azure Functions host

In your application directory, create a file called *host.json* and add the following contents:

```json
{
  "version": "2.0",
  "httpWorker": {
      "description": {
          "defaultExecutablePath": "server.exe"
      }
  }
}
```

This *host.json* configuration file tells the Azure Functions host where to find the web server executable.

## Run the Azure Functions application

Inside the root application directory, enter the following command into the command prompt.

```bash
func start
```

Using an application like Postman or Insomnia, make a `GET` request to `localhost:7071/api/values`.

The response should look like the following:

```json
{
    "value": [
        1,
        2,
        3
    ]
}
```

## Conclusion

In this writeup, I showed how to create a Racket serverless application that runs on Azure Functions by using custom handlers. Doing so requires you to:

1. Write a web server to process requests
2. Define the bindings for the request and response function payloads
3. Configure the Azure Functions host to send request to the web server

Although in this example, the server was written in Racket, the same process is applicable to other languages. Keep in mind that at the time of this writing, custom handlers are preview and may change. Happy coding!

## Resources

- [Azure Functions Custom Handlers](https://docs.microsoft.com/azure/azure-functions/functions-custom-handlers)]]&gt;</description>
      <link>https://www.lqdev.me/posts/serverless-racket-azure-functions-custom-handlers</link>
      <guid>https://www.lqdev.me/posts/serverless-racket-azure-functions-custom-handlers</guid>
      <pubDate>2020-03-21 13:45:43 -05:00</pubDate>
      <category>serverless</category>
      <category>racket</category>
      <category>lisp</category>
      <category>azure-functions</category>
      <category>azure</category>
      <category>programming</category>
      <category>web</category>
    </item>
    <item>
      <title>The Case for Doing Machine Learning with F#</title>
      <description>&lt;![CDATA[
## Introduction

This post is part of the 2018 [FsAdvent](https://sergeytihon.com/tag/fsadvent/) series organized by [Sergey Tihon](https://twitter.com/sergey_tihon).

When searching for tools and languages to implement machine learning applications, there are numerous options to choose from each with their own set of advantages and disadvantages. Out of all, however, Python seems to be the most salient. Not only is it a popular language but also many of the tools available for machine learning are either implemented in Python or support it in some capacity whether it's native or through community libraries. Very rarely though is F# mentioned in these discussions despite having many of the features that make languages like Python so loved and extending them to empower users. In this writeup, I will do a short review of many of the advantages of Python such as succinctness, platform support, library availability as well as many others and compare it to F#'s capabilities.

![2017 Top Languages](http://cdn.lqdev.tech/files/images/case-fsharp-ml-0.PNG)

## Python's Advantages

### Learning Curve

One of the reasons why Python is so widely adopted is its learning curve. Whether an individual knows how to program or not, at times, Python can look like pseudocode making it accessible to not only readers but also writers. As with anything the more complex the task, the steeper the learning curve. However, at a simpler level, Python makes it as easy as possible to get started. 

Although at first it may not appear to be the case with F#, the learning curve is not much steeper than that of Python. The syntax can sometimes look intimidating to individuals, but the steepest part of the learning curve doesn't necessarily come from the language itself but rather from the way of reasoning about the logic of the programs. As a functional language, there is somewhat of a paradigm shift from that of a procedural execution model. Below is an example that defines a function that doubles an integer in both languages.  

##### Python

```python
def double(x):
    return x * 2
```

##### FSharp

```fsharp
let double x = 
    x * 2
```

As it can be seen, despite some minor syntax and character differences the functions are essentially the same.  

### Intended Purpose

Depending on the task at hand, some languages are more adept for handling respective tasks. For example, R and Julia are excellent languages when performing statistical tasks. However, outside of those types of tasks their abilities are more limited. Python, being a general-purpose language means that not only can you use it for machine learning tasks but also to build n-tier applications entirely in Python without having to worry about integrations, plugins or having to learn an entirely different language to perform such actions. 

Similarly, F# is a general-purpose language which allows you to build web and console applications as well as machine learning applications all from the comfort of the same ecosystem.

### Strong Library Support

When performing data analysis and machine learning, practicioners use a variety of libraries for their development such as [NumPy](http://www.numpy.org/) and [Pandas](https://pandas.pydata.org/) for data wrangling, [scikit-learn](https://scikit-learn.org/stable/index.html) for machine learning algorithms and [matplotlib](https://matplotlib.org/) for creating visualizations. Although all of these tasks could most certainly be implemented from scratch, libraries speed up the development process allowing practitioners to focus more on the domain and experiment with the models that best solve the respective problem they are facing.  

F#, like Python has exceptional library support, specifically as it regards data science and machine learning. [FsLab](https://fslab.org/) is a collection of open source F# packages for machine learning that contain libraries such as [FSharp.Data](https://fsharp.github.io/FSharp.Data/) and [Deedle](https://fslab.org/Deedle/) for data wrangling, [Math.NET Numerics](https://numerics.mathdotnet.com/) for machine learning algorithms and [XPlot](https://fslab.org/XPlot/) to help with data visualization. Furthermore, at Build 2018, Microsoft released [ML.NET](https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet), an open-source, cross-platform machine learning framework for .NET which allows .NET users to not only perform machine learning tasks within .NET but also allows extensibility via Tensorflow, ONNX and Python extensions. For a more detailed writup on using ML.NET with F#, check out the post [A Christmas Classifier](https://towardsdatascience.com/f-advent-calendar-a-christmas-classifier-72fb9e9d8f23) by Willie Tetlow or my post [Classification with F# ML.NET Models](http://luisquintanilla.me/2018/06/13/mlnet-classification-fsharp/).

Libraries for F# are also not confined only to those written in F#. In many instances, because F# is a .NET language, there is interoperability with C# libraries further extending the capabilities of F#.

### Platform Support

One of the things that makes Python so attractive is that it runs cross-platform. It does not matter whether you're on Windows, Mac or Linux; Python code runs the same. That being said though, not all platforms are created equal and although it is possible to run Python code on all platforms, essential libraries such as NumPy, Pandas and scikit-learn run best on Unix environments. It is possible to run them on Windows but the set up is not as straightforward. 

As a .NET Language, F# runs on Windows. With the help of the Mono runtime and most recently .NET Core, it also runs on Mac and Linux. Like Python, depending on the runtime and dependencies used by the respective software packages there may be limitations as to which platform code can be run on. However, from my experience most of the FsLab set of packages work cross-platform. 

### Succintness

Productivity is an important measure of a language. One way to achieve it is to write logic using the least number of characters. This is where Python shines. As a dynamically typed, whitespace ruled language, Python does not require developers to declare types associated with the objects and variables defined nor does it require the use of brackets and any other special characters. As expected, this allows for developers to write the same logic with less characters much faster. 

Unlike Python, F# is a statically typed language. However, thanks to type inference and the help of the compiler, writing programs often does not require developers to explicitly define what types objects and variables are. Additionally, it is a whitespace ruled language therefore removing the need for brackets and additional characters further speeding up the development process in a safe manner.

### Immediate Feedback

Writing safe and effective code takes time and experience. However, even the most experienced developers often makes mistakes. Therefore, getting immediate feedback before adding certain logic to programs goes a long way to making code that is safe, efficient and tested.  

#### Read-Evaluate-Print Loop (REPL)

One way in which both Python and F# provide immediate feedback is via the command line using the Read-Evaluate-Print Loop (REPL). The REPL is a programming environment that reads the user input, evaluates it and prints out the results, hence the name. Below are screenshots of what that environment looks like using both Python and F#.

##### Python

![](http://cdn.lqdev.tech/files/images/case-fsharp-ml-1.png)

##### FSharp

![](http://cdn.lqdev.tech/files/images/case-fsharp-ml-2.png)

As it can be seen, getting that immediate feedback makes it easier to see whether the code is behaving the way it should. Although useful, this environment is not ideal when experimenting and making tweaks to the code. Additionally, because of the lack of a graphical user interface, the navigation can be less than ideal. Fortunately there is a solution out there that provides the same level of interactivity along with a graphical user interface that allows for ad-hoc experimentation and re-running of code at different points in time which is essential when developing machine learning applications.

#### Jupyter Notebooks

Much of machine learning deals with experimentation. Experimentation involves having a way to tweak parameters and evaluate the results. In addition, experiments should have a way of being documented and reproduced. As alluded to previously, such development environment and capabilities can be found in [Jupyter Notebook](https://jupyter.org/). 

As mentioned on the project's website: 

&gt; The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more.

Jupyter Notebooks work with various languages which include but are not limited to Python and F# and can be run both on local machines as well as via a hosted service. One such service is Microsoft's [Azure Notebooks](https://notebooks.azure.com/) which allows you to use Jupyter Notebooks in the cloud for free. All you need is to have a Microsoft account. Below are screenshots of what that environment looks like in both Python and F#.   

##### Python

![](http://cdn.lqdev.tech/files/images/case-fsharp-ml-3.png)

##### FSharp

![](http://cdn.lqdev.tech/files/images/case-fsharp-ml-4.png)

## Beyond Python

F#, is comparable to Python on many of the features that make Python a great language for both development and machine learning. However, there are some areas where F# provides additional safety and functionality that greatly improve the way in which machine learning applications are built. 

### Typing

As previously mentioned, F# is a statically typed language. However, it also makes use of type inference which means that declaring types is not always required. With the help of the compiler, based on the structure of functions and variables it is possible to determine which type an object or variable is. This has several advantages. One advantage of being strongly typed is that when the types are declared the code becomes self-documented because it is easier to deduce what functions are doing based on the types being passed in and returned. Another advantage is that it makes it harder to write bad code. Having the compiler help you when you write your code allows you to find errors prior to compilation or running the code. This along with the REPL gives you additional reassurance that your code is executing the intended logic. 

Python is making strides in acquiring some of that functionality with the introduction of type hints in version 3.6. However, this has not always been a core feature of the language and is only in its nascent stages. 

### Immutability

As a functional language, immutability is something that is a native part of the language. While in some cases it can change the way in which code is written, immutability has one advantage, especially when it comes to machine learning. With immutability, parallelization can be fully exploited for those algorithms which take advantage of it. 

## Conclusion

In this writeup, I went over how F# is comparable to Python on many of the features that make it such a popular language such as library support, succinctness, general purpose and interactivity. However, F# has additional capabilities such as static typing and immutability that further enhance its capabilities as a language for machine learning. That is not to say that one is better than the other as they both are more adept for performing certain tasks. When it comes to machine learning it does become a matter of choice as they are both robust, powerful and strongly supported languages. Therefore next time you're looking to build a machine learning application, hopefully you give F# a try. Happy coding!

###### Resources

[2018 Top Programming Languages](https://spectrum.ieee.org/at-work/innovation/the-2018-top-programming-languages)  
[Get Programming with F#](https://www.manning.com/books/get-programming-with-f-sharp)  
[ML.NET Machine Learning Samples](https://github.com/dotnet/machinelearning-samples)]]&gt;</description>
      <link>https://www.lqdev.me/posts/case-fsharp-machine-learning</link>
      <guid>https://www.lqdev.me/posts/case-fsharp-machine-learning</guid>
      <pubDate>2018-12-14 23:34:50 -05:00</pubDate>
      <category>mlnet</category>
      <category>machine learning</category>
      <category>fsharp</category>
      <category>dotnet</category>
      <category>programming</category>
      <category>python</category>
      <category>ai</category>
      <category>dotnetcore</category>
      <category>artificial intelligence</category>
      <category>fsadvent</category>
      <category>artificialintelligence</category>
      <category>machinelearning</category>
      <category>data science</category>
      <category>functional programming</category>
    </item>
    <item>
      <title>Operationalizing Machine Learning with ML.NET, Azure DevOps and Azure Container Instances</title>
      <description>&lt;![CDATA[

## Introduction

Azure DevOps, formerly known as Visual Studio Team Services (VSTS), helps individuals and organizations plan, collaborate and ship products faster. One if its noteworthy services, Azure Pipelines, helps developers build Continuous Integration (CI) and Continuous Delivery (CD) pipelines that automate and standardize the build, test and deploy phases of the software development process. In addition, Azure Pipelines provides native container support and works with any language, platform and cloud. Machine learning like software development is also a process that includes a build, test and deploy phase which makes it a good candidate for automation and standardization. At Build 2018, Microsoft announced [ML.NET](https://github.com/dotnet/machinelearning), an open-source, cross-plaform machine learning framework for .NET. If we were to put all of these tools and services together, it means that we can automate and standardize the training of a machine learning model built with ML.NET, package it into a Docker container and deploy it to Azure Container Instances (ACI). In this writeup, I will go through the process of building a CI/CD pipeline in Azure Devops that trains, packages and deploys an ML.NET machine learning model to predict which class an Iris flower belongs to using a variety of measurements. Source code for this project can be found at this [link](https://github.com/lqdev/mlnetazdevopssample).

## Prerequisites

- [Git](https://git-scm.com/)
- [GitHub Account](https://github.com/)
- [.NET Core SDK](https://www.microsoft.com/net/download)  
- [Azure Account](https://azure.microsoft.com/en-us/free/)

## The Application

Because the purpose of this post is to demonstrate the functionality of Azure Devops and not that of ML.NET, I'll start with a pre-built application. For some more information and detail into the functionality of ML.NET, check out the official documentation [page](https://docs.microsoft.com/en-us/dotnet/machine-learning/) as well as some of my previous posts: 

- [Serverless Machine Learning with ML.NET and Azure Functions](http://luisquintanilla.me/2018/08/21/serverless-machine-learning-mlnet-azure-functions/)
- [Deploy .NET Machine Learning Models with ML.NET, ASP.NET Core, Docker and Azure Container Instances](http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/).

The application used in this writeup contains three .NET Core projects within it. One is a class library which is what we'll use to wrap ML.NET functionality for training models as well as loading pre-trained models that will then be used to make predictions. Another is a .NET Core console application which references the class library to train and persist an ML.NET model. Finally, there's the ASP.NET Core Web API which also references the class library application to load the pre-trained model created by the console application and then makes predictions via HTTP. This application can be utilized and deployed standalone but in this writeup it will be packaged into a Docker image that will then be deployed to Azure Container Instances.  

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-1.png)

### Class Library

The class library can be found in the `MLModel` directory. The class library defines the observation and prediction data classes which can be found in the `IrisData.cs` and `IrisPrediction.cs` files respectively. Additionally, the `Model` class contains helper methods that train and save machine learning models, load pre-trained models and use these models to make predictions. 

### Console Application

In the solution directory we also have a console application in the `ModelTrainer` directory. This application references the class library in the `MLModel` directory to train and persist the machine learning model. 

### API

The `ModelApi` directory contains an ASP.NET Core Web API application that references the `MLModel` class library project to load the pre-trained model that is trained by the `ModelTrainer` console application and makes predictions via HTTP. The logic for making predictions can be found in the `PredictController.cs` class in the `Controllers` directory of the `ModelApi` application. 

## CI/CD Pipeline Flow

Conceptually, when the application is built and deployed manually, the machine learning model is defined and developed inside the `MLModel` class library. Once satisfied with the model, the class library is built. The console application which references the `MLModel` class library is built as well as run to train and persist a classification model in a file called `model.zip`. The `MLModel` class library is also referenced in the `ModelApi` ASP.NET Core project. Because `ModelApi` is the application we're looking to deploy in order to expose our pre-trained machine learning model, we need to find a way to package it for deployment. We'll be deploying `ModelApi` using Azure Container Instances which means we need to create a Docker image of the project that will then be pushed to a Docker registry where it will be made available for public consumption. The building of multiple projects as well as the building, publishing and deployment of the Docker image to Azure Container Instances can be standardized and automated using Azure DevOps. The rest of this write-up will focus on demonstrating step-by-step how to operationalize this machine learning application via CI/CD pipelines in Azure DevOps using Azure Pipelines.   

### Getting The Code

Before getting started, the first thing you'll want to do is fork the [mlnetazdevopssample](https://github.com/lqdev/mlnetazdevopssample) GitHub repository into your own GitHub account.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-2.png)

### Creating the Project

Navigate to [https://devops.azure.com](https://devops.azure.com), click `Start Free` and follow the prompts to either create a new account or sign into your existing account.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-3.png)

Once logged in, click `Create Project`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-4.png)

Enter the name of your project as well as a short description. Then, click `Create`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-5.png)

## The Continuous Integration (CI) Pipeline

Using Azure Pipelines, we'll configure a CI pipeline for the build and packaging steps of our application. Below is an illustration of all the steps involved in our CI pipeline:

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-6.png)

1. Build the class library application
2. Build the console application
3. Train and persist the ML.NET Model by running the console application.
4. Copy ML.NET model file created by console application into ASP.NET Core Web API application directory
5. Build ASP.NET Core Web API application
6. Build Docker image 
7. Push Docker image to Docker Hub

### CI Pipeline Setup

Once the project is created, in the main project page, hover over `Pipelines` and click on `Builds`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-7.png)

In the `Builds` pipeline page, click `New pipeline`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-8.png)

Select GitHub as the source and connect your GitHub account with Azure DevOps.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-9.png)

Once you have authorized Azure DevOps to use your GitHub account, select the repository and branch that will be used for this build pipeline. In our case, we'll be using the master branch of the `mlnetazdevopssample` repository. When finished configuring, click `Continue`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-10.png)

The next step will be to select the jobs to execute in our pipeline. Because there are multiple steps in this build pipeline, let's start with an `Empty Job` and customize it to our needs.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-11.png)

From inside the build pipeline page, before we start adding jobs, lets select the agent that will execute the jobs. For this pipeline, select the `Hosted Ubuntu 1604` option from the dropdown.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-12.png)

### 1. Build the Class Library Application

The first step in our CI Pipeline will be to build our class library which contains methods that wrap the training, loading and prediction functionality of the ML.NET framework and persisted models.

To achieve that, we'll add a .NET Core task to our `Agent Job 1`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-13.png)

Once added to the pipeline, let's configure this task. To make it more descriptive, we can give it a name such as `Build Class Library`. Because this task will be responsible for building the .NET Core class library, we'll leave the default `build` Command setting as is. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-14.png)

The other setting we'll want to configure is the `Working Directory`. We can do so by clicking the `Advanced` tab. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-15.png)

For this task we'll use the `MLModel` directory.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-16.png)

When finished with the configuration, click `Save &amp; Queue` -&gt; `Save` on the top toolbar.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-17.png)

Enter a detailed comment describing the change to the pipeline and click `Save`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-18.png)

### 2. Building The Console Application

Once we've built the class library application which we'll reference from the .NET Core console and ASP.NET Core Web API applications, it's time to build the console application which will serve the purpose of training and persisting the ML.NET model.

Similar to the previous step, add a new .NET Core *build* task to the pipeline. The only setting that will change for this task is the `Working Directory` which will have the value of `ModelTrainer`. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-19.png)

Although not required, when finished configuring the task, click `Save &amp; Queue` -&gt; `Save` to save and comment the changes to the pipeline. 

### 3. Train and persist the ML.NET Model

Now that our console application is built, it's time to run it in order to train and persist the ML.NET model. To do so, we'll add another .NET Core task. The difference is that the `Command` setting will now be configured with the `run` value. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-20.png)

The `Working Directory` will be set to `ModelTrainer` like in the previous task.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-21.png)

Remember to save and comment the new changes to the pipeline.

### 4. Copy ML.NET Model to Web API Directory

After the console application is run and the ML.NET model is trained, it is persisted in a file called `model.zip` inside the `ModelTrainer` directory. We can use this persisted version of the model to make predictions from both the console application or any other application of our choice. In this case, we'll be making predictions via an ASP.NET Core Web API. In order for our API to reference this file, we need to copy it into the root directory of our `ModelApi` directory. A way to perform that task is via bash script. To add a bash script to our pipeline, all we need to do is add a Bash task to it. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-22.png)

Once added to our pipeline, it's time to configure the task. We'll set the `Type` setting to `Inline` which will bring up a text box for us to type in the script. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-23.png)

Inside of the text box, enter the following content:

```bash
# Write your commands here

cp ../ModelTrainer/model.zip .

# Use the environment variables input below to pass secret variables to this script
```

This command will copy the `model.zip` file from the `ModelTrainer` directory to the `ModelApi` directory.

We can set the `Working Directory` of this step to `ModelApi`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-24.png)

Once finished, save and comment the new changes to the pipeline. 

### 5. Build ASP.NET Core Web API application

Now that we have the necessary files inside our `ModelApi` application, it's time to build it. We'll add a .NET Core task to our pipeline and set the `Command` to `build`. The `Working Directory` will be `ModelApi` like the previous task.

Save and comment the new changes to the pipeline when finished.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-25.png)

### 6. Build ASP.NET Core Web API Docker Image

The method of deployment for the ASP.NET Core Web API application is via containers. Therefore, after building the application, we have to build a Docker image for it that can then be pushed to a Docker registry of your choice. To build a Docker image, we'll add a Docker task to our pipeline.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-26.png)

When we configure the task, we'll start off by setting the `Container Registry Type` to `Container Registry`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-27.png)

This will prompt the setup of a service connection to a Docker registry if one does not already exist.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-28.png)

The Docker registry type we'll be using is Docker Hub. Give the connection a name, enter the credentials to your Docker Hub account and click `Verify this connection` to make sure that your credentials are valid and a connection can be established with Docker Hub. When finished click `OK`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-29.png)

The `Command` setting will be `build` so we can leave the default as is as well as the `Dockerfile` setting which will use the Dockerfile in the root `mlnetazdevopssample` directory.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-30.png)

Finally, we'll configure the `Image name` setting. The convention we'll use is `&lt;docker-hub-username&gt;/&lt;image-name&gt;`. In my case, `lqdev` is my Docker Hub username and I'll name my image `mlnetazdevopssample` resulting in `lqdev/mlnetazdevopssample`. Additionally, check the `Include latest tag` checkbox to have every build be the latest as opposed to tagging it with versions numbers. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-31.png)

Remember to save and comment the recent changes to the pipeline.

### 7. Push Docker Image to Docker Hub

The last step in our CI pipeline is to push our newly built image to Docker Hub. To do so we'll use anoter Docker task. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-32.png)

Like in the previous task, we'll set the `Container registry type` to `Container Registry`. Set the `Docker registry service connection` to the most recently created connection by selecting it from the dropdown. We'll be changing our `Command` to `push` and set the `Image name` to the name of the image built in the previous step. The naming convention is `&lt;docker-hub-username&gt;/&lt;image-name&gt;:latest`. The latest tag was added by our previous Docker build task so make sure that you include it in this task.

Once finished, click `Save &amp; Queue` -&gt; `Save &amp; Queue`. As opposed to only clicking `Save`, this action will manually trigger the CI pipeline.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-33.png)

Don't forget to comment your changes and click `Save &amp; queue` to kick off the CI pipeline.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-34.png)

### Monitoring the Build

When the build starts, you can click on `Builds` under the `Pipelines` section on the left pane.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-35.png)

Select the first build from the list to get more details on the build.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-36.png)

This will take you to the logs which show the status of the pipeline near real-time.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-37.png)

### Confirming CI Pipeline Success

If the build is successful, navigate to [https://hub.docker.com/](https://hub.docker.com/) to check whether the Docker image was pushed to the registry.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-38.png)

## The Continuous Delivery (CD) Pipeline

Now that we have our CI pipeline set up which will build and package our application, it's time to deploy it. We could do this ourselves or automate it using a CD pipeline. Our application wil be deployed to Azure Container Instances which is an Azure service that offers a quick way to run containers without having to worry about the management of virtual machines or orchestration services. The steps involved in our CD pipeline are the following:

1. Create Azure Resource Group for deployment
2. Deploy application to Azure Container Instances.

### CD Pipeline Setup

To get started setting up a CD pipeline, from the Azure DevOps project main page, hover over `Pipelines` and click on `Releases`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-39.png)

Once in that page, click on `New pipeline`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-40.png)

As with our CI pipeline, we'll start off with an `Empty Job` which we'll configure at a later time.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-41.png)

### Triggering Deployments

Once our pipeline is created, it's time to configure it. The first thing we'll want to do is add an artifact. An artifact can be a variety of things including the output of our build pipeline. In our case, the end our CI pipeline will be the trigger for our CD pipeline. To add an artifact, click `Add an artifact`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-42.png)

In the configuration form, set the `Source type` to `Build` and the `Source` to the name of the CI pipeline created in the previous steps. When finished, click `Add`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-43.png)


After configuring our artifact, it's time to configure the steps in the CD pipeline. To do so, click on the `Stage 1` option in the `Stages` section of the release pipeline page and change the name to something more descriptive.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-44.png)

When finished, close out the form and click on the hyperlink below the stage title. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-45.png)

You should now be on a page similar to the CI pipeline job configuration page. On this page, we'll want to click on the `Agent Job` panel to set the `Agent pool` setting to `Hosted Ubuntu 1604`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-46.png)

Once that is complete, it's time to configure the tasks in the CD pipeline.

### 1. Create Azure Resource Group

Start off adding an `Azure CLI` task to the pipeline. In this task we'll create a resource group in Azure to which we'll deploy our application to. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-47.png)

Before doing anything else, link DevOps to an Azure Subscription by selecting one from the dropdown and clicking `Authorize` which will prompt you to authenticate your subscription. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-48.png)

Once an Azure subscription has been linked, let's change the `Script Location` setting to `Inline Script`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-49.png)

In the `Inline Script` text box enter the following:

```bash
az group create --name mlnetazdevopssampleresourcegroup --location eastus
```
This script will create a resource group in Azure called `mlnetazdevopssampleresourcegroup` that is located in `eastus`. Both of these are configurable to your preference. 

### 2. Deploy Docker Image to Azure Container Instances

The next and final step in our CD pipeline is the deployment to Azure Container Instances. To deploy our application, we'll add another `Azure CLI` task. This time, since we already configured our `Azure subscription` in the previous task, we can select the service connection as opposed to a subscription from the dropdown.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-50.png)

Like in the previous task, our script will be inline. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-51.png)

In the `Inline Script` text box enter the following:

```bash
az container create --resource-group mlnetazdevopssampleresourcegroup --name mlnetcontainer --image lqdev/mlnetazdevopssample:latest --ports 80 --ip-address public
```

This script creates a container in the resource group created by the previous task of the pipeline with the name `mlnetcontainer` from the Docker image that was pushed to Docker Hub by the CI pipeline. Additionally, it opens up port 80 and assigns a publicly accessible IP address for the container to be accessed externally. 

Once this step has been configured, make sure to save and comment all your changes by clicking `Save`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-52.png)

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-53.png)

Then, to make it easily recognizable, edit the name of the pipeline by hovering near `New release pipeline` and clicking on the pencil icon.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-54.png)

Make sure to save and comment your changes.

## Automating CI/CD Pipelines

In the previous steps, we configured CI and CD pipelines. However, we have still not fully automated the triggers that kick off both of these. 

### CI Pipeline Trigger

First, lets start off by automating the CI pipeline. To do so, go the project's main page, hover over `Pipelines` and click on `Builds`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-55.png)

This will take you to the CI pipeline page. While on this page, click `Edit`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-56.png)

Then, click on `Triggers`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-57.png)

Once on this page, check the `Enable continous integration` checkbox and save and comment your changes by clicking `Save &amp; Queue` -&gt; `Save`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-58.png)


### CD Pipeline Trigger

To automate the CD pipeline trigger, click on `Releases` under the `Pipelines` page to automate the CD pipeline.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-59.png)

Once on the CD pipeline's page, click `Edit`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-60.png)

Then, click on the lightning icon in the Artifacts section which will show a configuration form. In this form, toggle the `Continuous deployment trigger` setting to `Enabled`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-61.png)

When finished, save and comment your changes. 

## Running CI/CD Pipelines

Although going forward builds and deployments will be started when new changes are checked into the master branch of the `mlnetazdevopssample` repository, for demonstration purposes we will manually kick off the CI/CD pipelines we have just configured. To do so, click on `Builds` under the `Pipelines` section on the left pane.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-62.png)

From the CI pipeline page click `Queue`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-63.png)

This will prompt a modal to show up in which you can just click `Queue` to start the build.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-64.png)

This will kick off a new CI build which subsequently will also kick off the CD pipeline of your application. 

## Testing The Deployment

If all is successful, a Docker image of an ASP.NET Core Web API application will be deployed to Azure Container Instances which can be accessed via a public IP address. 

To see whether the deployment worked, navigate to [https://portal.azure.com/](https://portal.azure.com/) and click on `Resource groups`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-65.png)

At this point, you should see the resource group that was created by the CD pipeline. If that's the case, click on it. 

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-66.png)

This will then show a page that displays the container that was deployed to this resource group. Click on that.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-67.png)

The container page will display diagnostic and configuration information about the container. The information we're interested in is the `IP address`. Hover to the right of it and click on the icon that says `Click to copy`. This will copy the address to the clipboard.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-68.png)

In an application like Postman or Insomnia, make an HTTP POST request to `http://&lt;ip-address&gt;/api/predict` where `ip-address` is the public IP address of the container in Azure with the following body.

```json
{
    "SepalLength":3.3,
    "SepalWidth":1.6,
    "PetalLength":0.2,
    "PetalWidth":5.1
}
```

If successful, the response will be `Iris-virginica`.

![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-69.png)

## Conclusion

In this writeup, we operationalized the building, packaging and deployment of an ML.NET application that predicts the class of an Iris flower using a variety of mesurements with Azure DevOps. We created both a Continous Integration as well as a Continous Delivery pipeline which deploys the Docker image of an ASP.NET Core Web API to Azure Container Instances. Keep in mind this is just one way of doing it and Azure DevOps is flexible in how all of these tasks and workflows are configured to meet your requirements. Happy coding!

###### Resources

[ML.NET Samples](https://github.com/dotnet/machinelearning-samples)
[DevOps for Data Science](https://www.youtube.com/watch?v=bUTBBS1TECc)]]&gt;</description>
      <link>https://www.lqdev.me/posts/azdevops-mlnet-aci</link>
      <guid>https://www.lqdev.me/posts/azdevops-mlnet-aci</guid>
      <pubDate>2018-11-26 23:50:23 -05:00</pubDate>
      <category>mlnet</category>
      <category>machinelearning</category>
      <category>ai</category>
      <category>dotnet</category>
      <category>dotnetcore</category>
      <category>devops</category>
      <category>azure</category>
      <category>docker</category>
      <category>containers</category>
      <category>microsoft</category>
      <category>artificialintelligence</category>
      <category>programming</category>
      <category>webapi</category>
      <category>aci</category>
      <category>development</category>
    </item>
    <item>
      <title>Hacker News Vue Top Stories Client</title>
      <description>&lt;![CDATA[
# Introduction

Over the past few years, I've used AngularJS as my front-end framework for most web projects. With the release of Angular and React, I tried both of them and while both are great tools the learning took significantly longer because I also had to learn the bundler systems and in the case of Angular, TypeScript. Although the CLI tools go a long way towards automating a lot of the setup process, building an application from scratch in these frameworks is not intuitive especially for a newcomer. In the early days of Vue, I was able to attend a Meetup presentation by [Evan You](https://twitter.com/youyuxi) which went over the philosophy of Vue as well as its capabilities. I was instantly interested because of how easy it was to get started. Since that presentation, I have not had time to try it but with AngularJS being phased out, I felt it was about time to start exploring another framework. To get a better understanding of the framework's features prior to using CLI tools, I created a standalone application that relies only on NPM packages. In this writeup, I will create a Hacker News client with Vue that calls the Hacker News API and displays the top 50 stories. Source code for this project can be found in the following [repository](https://github.com/lqdev/hnvuedemo).

## Prerequisites

- [Node](https://nodejs.org/en/)

## Initialize Project

The first thing we want to do is create a directory for our project and initialize it with NPM

```bash
mkdir hnvuedemo
cd hnvuedemo
npm init -y
```

## Install Dependencies

Next, we want to install our dependencies. (Express is optional)

```bash
npm install --save vue axios express
```

## Create Server (Optional)

We can set up a server to host and serve our content. This is optional though because the application should still work when the `index.html` file is opened in the browser.

In our root project directory, we can create the `server.js` file and add the following content:

```javascript
var express = require("express");

const PORT = 3000 || process.env.PORT;

var app = express();

app.use(express.static("."));

app.get("/", (req, res) =&gt; {
  res.sendFile("index.html");
});

app.listen(PORT, () =&gt; {
  console.log(`Listening on port ${PORT}`);
});
```

## Create Application

### Create The View

Now it's time to create our application. Let's start by scaffolding the `index.html` which is where our application will be displayed.

```html
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;script src="./node_modules/vue/dist/vue.min.js"&gt;&lt;/script&gt;
    &lt;script src="./node_modules/axios/dist/axios.min.js"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;!--App Content Here--&gt;

    &lt;!--App Scripts Here--&gt;
&lt;/body&gt;
&lt;/html&gt;
```

We want to add the scripts to the `Vue` and `axios` packages in our `head` element. We'll hold off on the content for now.

### Get Data

Our data source for this project will be the Hacker News [API](https://github.com/HackerNews/API). To interact with it, we'll be using the `axios` NPM package. The data collection happens over several HTTP requests, therefore to help with it we'll be creating a file called `apihelpers.js` in our root project directory that will contain functions to get and manipulate the data.

#### Get Top Stories

The first thing we want to do is get a list of top stories. We can do so via the `/topstories` endpoint. The response of this request returns a list of ids which can then be used in conjunction with the `/item/{id}` endpoint to get the individual story data. We can then validate the story object to make sure it is a story as opposed to a user or job posting and return a list of all the story objects. The functions that will help us with that are `getIds`, `isStory` and `extractStories`.

```javascript
/**
 * Checks whether the item is a story
 * @param {Object} story - Story object
 */
function isStory(story) {
  return story.type == "story";
}

/**
 * Gets ids of stories
 * @param {string} url - Url to fetch story ids
 */
function getIds(url) {
  return axios.get(url);
}

/**
 * 
 * @param {Array&lt;Object&gt;} - List of resolved promises 
 */
function extractStories(...responses) {
    var stories = responses.map(story =&gt; story = story.data);
    return stories.filter(isStory);
}
```

### Create Vue App

Now that we have our helper methods to make calls to the API, we can create our application.

We can start by creating our Vue instance which is where our application will live. We can create a file called `app.js` in our root project directory and add the following content:

```javascript
//Components Go Here

new Vue({
  el: "#app",
  data: {
    title: "Hacker News Vue Simple Reader",
    loading: true,
    topStories: []
  },
  methods: {
    getTopStories: function() {
      getIds(
        "https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty"
      )
        .then(ids =&gt; buildRequest(ids.data.splice(0, 50))) //Only get first 50
        .then(getStories)
        .then(axios.spread(extractStories))
        .then(stories =&gt; {
          this.topStories = stories;
          this.loading = false;
        })
        .catch(e =&gt; console.log(e));
    }
  },
  created: function() {
    this.getTopStories();
  }
});
```

There's a lot happening here, so let's break it down based on the properties of the object passed to the Vue constructor.

The `el` property is like a selector which tells the application which element it should operate on. The value `#app` tells it to look for an element where the `id` attribute is `app`. This of course can be anything of your choosing.

The `data` property specifies the properties that contain the data of our application. These properties are reactive which means that whenever they change, that change is reflected in our view. As such, even if they have no value when the application starts, in order for them to automatically change when data is passed to them, they need to be declared in the `data` property. In our application, we have a `title` property which will be the title of our web page, `topStories` which is where we'll store the list of Hacker News stories and `loading` which we'll use to let us know when data is being loaded into our application.

In the `methods` property, we define functions that we want our application to use. In this case, I created the `getTopStories` method which chains together all the functions defined in our `apihelpers.js` file to return the top 50 stories on Hacker News.

Finally, Vue has instance lifecycle hooks. The `created` property defines what should happen when our instance is created. In our case, we want to call the `getTopProperties` method which is defined in our `methods` property to load the data, update our `topStories` data property and set `loading` to `false` because our data has loaded successfully.

### Add App to View

Now that we have created the logic of our application, it's time to add it to our view. We can do so by adding the `apihelpers.js` and `app.js` files to our `index.html` file via `script` elements.

```html
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;script src="./node_modules/vue/dist/vue.min.js"&gt;&lt;/script&gt;
    &lt;script src="./node_modules/axios/dist/axios.min.js"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;!--App Content Here--&gt;

    &lt;!--App Scripts Here--&gt;
    &lt;script src="apihelpers.js"&gt;&lt;/script&gt;
    &lt;script src="app.js"&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
```

### Display Data

Although we can now use our application logic inside of our view, we still can't see any of it because we have not added elements to display it. To view our application data, we can add the following code to our `index.html` file below the `&lt;!--App Content Here--&gt;` section.

```html
&lt;div id="app"&gt;
    &lt;h2&gt;{{title}}&lt;/h2&gt;
    &lt;h5 v-if="loading"&gt;Loading...&lt;/h5&gt;
    &lt;story v-for="story in topStories" :key="story.id" :story="story"&gt;&lt;/story&gt;
&lt;/div&gt;
```

The final `index.html` contents should look like the content below:

```html
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;script src="./node_modules/vue/dist/vue.min.js"&gt;&lt;/script&gt;
    &lt;script src="./node_modules/axios/dist/axios.min.js"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div id="app"&gt;
        &lt;h2&gt;{{title}}&lt;/h2&gt;
        &lt;h5 v-if="loading"&gt;Loading...&lt;/h5&gt;
        &lt;story v-for="story in topStories" :key="story.id" :story="story"&gt;&lt;/story&gt;
    &lt;/div&gt;
    &lt;script src="apihelpers.js"&gt;&lt;/script&gt;
    &lt;script src="app.js"&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
```

As mentioned earlier, we created an element, in this case a `div` that has the `id` attribute with a value of `app`. This is how our application knows where to display our content. Inside of an `h2` element, we display our `title` data property. Below it, we have an `h5` element that displays the text "Loading...". However, this is to only be displayed when our `loading` data property is `true`. If not, it should not be visible. We can achieve this conditional rendering via the `v-if` directive. This directive evaluates the expression inside of it and renders content based on its truthiness. 

Finally, there's one last piece that looks like an element, but not one of the built-in HTML elements. So then, what is it? It's a component. The Vue website defines a component as "...a reusable Vue instance with a name: in this case, `&lt;story&gt;`. We can use this component as a custom element inside a root Vue instance...". In our component, the `v-for` directive is what it sounds like. It creates a sequence of `story` components based on a list of objects defined in our `Vue` instance's `data` property. Our `story` component iterates over the `topStories` data property and assigns the value of the individual object in the list to the variable `story`. We bind the `id` property of the `story` object to the `key` attribute of the component and pass in the entire object to the component via the `story` prop. A prop is a custom attribute that you can register for the component. We can use props to pass data into the component. In all cases, the `:` prefix on the attributes and props of the component are shorthand for the `v-bind` directive which dynamically binds an expression to an attribute or component prop.

With all that being said your next question might be, how does the view know about this component? The answer is it doesn't at least not until you define it which is what we'll do next in our `app.js` file. In order for our `story` component to be usable, we need to define it above the instantiation of our `Vue` instance. The definition looks like the following:

```javascript
Vue.component('story',{
    props: ['story'],
    template: `
        &lt;div&gt;
            &lt;h3&gt;&lt;a :href="story.url" target="_blank"&gt;{{story.title}}&lt;/a&gt;&lt;/h3&gt;
        &lt;/div&gt;
    `
});
```

Like our `Vue` instance, let's unpack what's happening here. The first parameter is a string with the name of our component. The `props` property is a list of the props or custom attributes that are accepted by our component. The `template` property is where we set up the template of what will be rendered in place of our component in the view. In our case, we'll have an `h3` element with a nested `a` element whose `href` attribute is the `url` property of our `story` object and the display text is the `title` property of our `story` object.

## Run Application

At this point, our application should be ready to run. You can either start the server with the following command `npm start`  and navigate to `http://localhost:3000` or open the `index.html` page in the browser of your choice. The result should look like the screenshot below:

![](http://cdn.lqdev.tech/files/images/hnvueclient.png)

## Conclusion

In this writeup, I built a standalone Hacker News client that displays the top 50 stories using Vue while also highlighting some of its main features. Overall, I really enjoyed building this application. The setup process was extremely simple and after a few hours looking through the excellent documentation and working through some bugs, it took less than two hours to get this application up and running from start to finish. For prototyping and learning purposes, Vue is great because you're able to take advantage of the core features of the framework without having too much overhead. Although this may not be the most appropriate way to build production-ready applications, it's nice to know you can have a modern web application with minimal setup required. My next steps will be to continue learning some of the other features the framework provides and eventually build up to learning how to use the CLI tools.
]]&gt;</description>
      <link>https://www.lqdev.me/posts/hn-vue-topstories-client</link>
      <guid>https://www.lqdev.me/posts/hn-vue-topstories-client</guid>
      <pubDate>2018-06-21 18:30:38 -05:00</pubDate>
      <category>vue</category>
      <category>web development</category>
      <category>programming</category>
      <category>api</category>
      <category>development</category>
      <category>frontend</category>
      <category>js</category>
    </item>
    <item>
      <title>Classification with F# ML.NET Models</title>
      <description>&lt;![CDATA[
# Introduction

In a previous [post](http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/), I detailed how to build and deploy C# `ML.NET` models with `Docker` and `ASP.NET Core`. With inspiration from [Jeff Fritz](https://twitter.com/csharpfritz), I have been learning F# for the past week and a half or so. When trying to think of projects to start practicing my F#, porting over the code I had built in C# naturally came to mind. After overcoming many obstacles and with much guidance from [Alan Ball](https://github.com/voronoipotato) and [Isaac Abraham](https://twitter.com/isaac_abraham) whose F# [book](https://www.amazon.com/Get-Programming-guide-NET-developers/dp/1617293997/ref=sr_1_1?ie=UTF8&amp;qid=1528929802&amp;sr=8-1&amp;keywords=get+programming+with+F%23) I highly recommend, I was able to successfully port over the main parts of my code which highlight `ML.NET` functionality. In this writeup, I will port a C# `ML.NET` classification model to F# which predicts the type of flower based on four numerical measurement inputs. I tried to keep the organization of this post nearly identical to that of the C# article where possible. Sample code for this project can be found at the following [link](https://github.com/lqdev/fsmlnetdemo).

## Prerequisites

This project was built on a Linux PC but should work cross-platform on Mac and Windows.

- [.NET Core SDK 2.0+](https://www.microsoft.com/net/download/linux)
- [Ionide Extension - Option 1](https://fsharp.org/use/linux/)
- [ML.NET v 0.2.0](https://www.nuget.org/packages/Microsoft.ML/)

## Setting Up The Project

The first thing we want to do is create a folder for our solution.

```bash
mkdir fsharpmlnetdemo
```

Then, we want to create a solution inside our newly created folder.

```bash
cd fsharpmlnetdemo
dotnet new sln
```

## Building The Model

### Setting Up The Model Project

First, we want to create the project. From the solution folder enter:

```bash
dotnet new console -o model -lang f#
```

Now we want to add this new project to our solution.

```bash
dotnet sln add model/model.fsproj
```

### Adding Dependencies

Since we’ll be using the `ML.NET` framework, we need to add it to our `model` project.

```bash
dotnet add model/model.fsproj package Microsoft.ML
```

### Download The Data

Before we start training the model, we need to download the data we’ll be using to train. We do so by downloading the data file into our root solution directory.

```bash
curl -o iris-data.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
```

If we take a look at the data file, it should look something like this:

```bash
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa
5.0,3.4,1.5,0.2,Iris-setosa
4.4,2.9,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
```

### Train Model

Now that we have all our dependencies set up, it’s time to build our model. I leveraged the demo that is used on the `ML.NET` Getting-Started [website](https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet/get-started/linux/ubuntu16-04).

#### Defining Data Structures

At the time of this writing, `ML.NET` version `0.2.0` does not fully support F# Records. A workaround for this are mutable classes. Not really inline with F# paradigms, but it should be good enough.

In the `Program.fs` file of our `model` project directory, let’s create two mutable classes called `IrisData` and `IrisPrediction` which will define our features and predicted attribute respectively. Both of them will use `Microsoft.ML.Runtime.Api` to add the property attributes.

Here is what our `IrisData` class looks like:

```fsharp
type IrisData() =
    [&lt;Column(ordinal = "0");DefaultValue&gt;]
    val mutable public SepalLength: float32

    [&lt;Column(ordinal = "1");DefaultValue&gt;]
    val mutable public SepalWidth: float32

    [&lt;Column(ordinal = "2");DefaultValue&gt;]
    val mutable public PetalLength:float32

    [&lt;Column(ordinal = "3");DefaultValue&gt;]
    val mutable public PetalWidth:float32

    [&lt;Column(ordinal = "4",name="Label");DefaultValue&gt;]
    val mutable public Label: string
```

Similarly, here is the `IrisPrediction` class:

```fsharp
type IrisPrediction() =
    [&lt;ColumnName "PredictedLabel";DefaultValue&gt;] val mutable public PredictedLabel : string
```

#### Building Training Pipeline

The way the `ML.NET` computations process is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. We can add that logic inside the `main` function of our `Program.fs` file.

```fsharp
let dataPath = "./iris-data.txt"

// Initialize Compute Graph
let pipeline = new LearningPipeline()

// Load Data
pipeline.Add((new TextLoader(dataPath).CreateFrom&lt;IrisData&gt;separator=','))

// Transform Data
// Assign numeric values to text in the "Label" column, because
// only numbers can be processed during model training
pipeline.Add(new Transforms.Dictionarizer("Label"))

// Vectorize Features
pipeline.Add(new ColumnConcatenator("Features","SepalLength", "SepalWidth", "PetalLength", "PetalWidth"))

// Add Learner
pipeline.Add(new StochasticDualCoordinateAscentClassifier())

// Convert Label back to text
pipeline.Add(new ransforms.PredictedLabelColumnOriginalValueConverterPredictedLabelColumn = "PredictedLabel"))

//Train the model
let model = pipeline.Train&lt;IrisData, IrisPrediction&gt;()
```

#### Testing Our Model

Now that we have our data structures and model trained, it’s time to test it to make sure it's working. Following our training operation, we can add the following code.

```fsharp
// Test data for prediction
let testInstance = IrisData()
testInstance.SepalLength &lt;- 3.3f
testInstance.SepalWidth &lt;- 1.6f
testInstance.PetalLength &lt;- 0.2f
testInstance.PetalWidth &lt;- 5.1f

//Get Prediction
let prediction = model.Predict(testInstance)

//Output Prediction
printfn "Predicted flower type is: %s" prediction.PredictedLabel
```

Our final `Program.fs` file should contain content similar to that below:

```fsharp
open System
open Microsoft.ML
open Microsoft.ML.Runtime
open Microsoft.ML.Runtime.Api
open Microsoft.ML.Data
open Microsoft.ML.Transforms
open Microsoft.ML.Trainers

type IrisData() =

    [&lt;Column(ordinal = "0");DefaultValue&gt;] val mutable public SepalLength: float32
    [&lt;Column(ordinal = "1");DefaultValue&gt;] val mutable public SepalWidth: float32
    [&lt;Column(ordinal = "2");DefaultValue&gt;] val mutable public PetalLength:float32
    [&lt;Column(ordinal = "3");DefaultValue&gt;] val mutable public PetalWidth:float32
    [&lt;Column(ordinal = "4",name="Label");DefaultValue&gt;] val mutable public Label: string


type IrisPrediction() =

    [&lt;ColumnName "PredictedLabel";DefaultValue&gt;] val mutable public PredictedLabel : string

[&lt;EntryPoint&gt;]
let main argv =

    let dataPath = "./iris-data.txt"

    // Initialize Compute Graph
    let pipeline = new LearningPipeline()

    // Load Data
    pipeline.Add((new TextLoader(dataPath)).CreateFrom&lt;IrisData&gt;(separator=','))

    // Transform Data
    // Assign numeric values to text in the "Label" column, because
    // only numbers can be processed during model training
    pipeline.Add(new Transforms.Dictionarizer("Label"))

    // Vectorize Features
    pipeline.Add(new ColumnConcatenator("Features","SepalLength", "SepalWidth", "PetalLength", "PetalWidth"))

    // Add Learner
    pipeline.Add(new StochasticDualCoordinateAscentClassifier())

    // Convert Label back to text
    pipeline.Add(new Transforms.PredictedLabelColumnOriginalValueConverter(PredictedLabelColumn = "PredictedLabel"))

    //Train the model
    let model = pipeline.Train&lt;IrisData, IrisPrediction&gt;()

    // Test data for prediction
    let testInstance = IrisData()
    testInstance.SepalLength &lt;- 3.3f
    testInstance.SepalWidth &lt;- 1.6f
    testInstance.PetalLength &lt;- 0.2f
    testInstance.PetalWidth &lt;- 5.1f

    //Get Prediction
    let prediction = model.Predict(testInstance)

    //Output Prediction
    printfn "Predicted flower type is: %s" prediction.PredictedLabel
    0 // return an integer exit code
```

All set to run. We can do so by entering the following command from our solution directory:

```fsharp
dotnet run -p model/model.fsproj
```

Once the application has been run, the following output should display on the console.

```bash
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Using 2 threads to train.
Automatically choosing a check frequency of 2.
Auto-tuning parameters: maxIterations = 9998.
Auto-tuning parameters: L2 = 2.667734E-05.Auto-tuning parameters: L1Threshold (L1/L2) = 0.
Using best model from iteration 1066.Not training a calibrator because it is not needed.
Predicted flower type is: Iris-virginica
```

## Conclusion

In this post, we ported over a C# `ML.NET` classification model to F# which predicts the class of flower based on numerical measurement inputs. While several workarounds needed to be made, `ML.NET` is still in its infancy. As more people become involved and provide feedback hopefully in the near future, F# support and functionality will become more stable. Happy coding!
]]&gt;</description>
      <link>https://www.lqdev.me/posts/mlnet-classification-fsharp</link>
      <guid>https://www.lqdev.me/posts/mlnet-classification-fsharp</guid>
      <pubDate>2018-06-13 18:19:05 -05:00</pubDate>
      <category>fsharp</category>
      <category>dotnet</category>
      <category>dotnetcore</category>
      <category>programming</category>
      <category>development</category>
      <category>mlnet</category>
      <category>machinelearning</category>
      <category>artificialintelligence</category>
      <category>functionalprogramming</category>
    </item>
  </channel>
</rss>