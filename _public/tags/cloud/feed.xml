<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - cloud</title>
    <link>https://www.lqdev.me/tags/cloud</link>
    <description>All content tagged with 'cloud' by Luis Quintanilla</description>
    <lastBuildDate>2024-06-11 20:42 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Apple - Private Cloud Compute</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We set out to build Private Cloud Compute with a set of core requirements:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Stateless computation on personal user data.&lt;/strong&gt; ...we want a strong form of stateless data processing where personal data leaves no trace in the PCC system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enforceable guarantee.&lt;/strong&gt; Security and privacy guarantees are strongest when they are entirely technically enforceable, which means it must be possible to constrain and analyze all the components that critically contribute to the guarantees of the overall Private Cloud Compute system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No privileged runtime access.&lt;/strong&gt; Private Cloud Compute must not contain privileged interfaces that would enable Apple’s site reliability staff to bypass PCC privacy guarantees, even when working to resolve an outage or other severe incident.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-targetability.&lt;/strong&gt; An attacker should not be able to attempt to compromise personal data that belongs to specific, targeted Private Cloud Compute users without attempting a broad compromise of the entire PCC system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Verifiable transparency.&lt;/strong&gt; Security researchers need to be able to verify, with a high degree of confidence, that our privacy and security guarantees for Private Cloud Compute match our public promises.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/apple-private-cloud-compute</link>
      <guid>https://www.lqdev.me/responses/apple-private-cloud-compute</guid>
      <pubDate>2024-06-11 20:42 -05:00</pubDate>
      <category>ai</category>
      <category>pcc</category>
      <category>privacy</category>
      <category>cloud</category>
      <category>privatecloudcompute</category>
      <category>wwdc</category>
    </item>
    <item>
      <title>Grow Your Own Services</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;This is a site encouraging non-technical people and organisations to create their own online services such as websites, social networks, personal clouds, instant messaging etc.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/grow-your-own-services</link>
      <guid>https://www.lqdev.me/bookmarks/grow-your-own-services</guid>
      <pubDate>2024-04-25 22:53 -05:00</pubDate>
      <category>selfhosting</category>
      <category>indieweb</category>
      <category>openweb</category>
      <category>internet</category>
      <category>community</category>
      <category>cloud</category>
      <category>messaging</category>
      <category>website</category>
      <category>socialnetworks</category>
      <category>websites</category>
      <category>online</category>
    </item>
    <item>
      <title>Ente - Private cloud storage for photos, videos, etc.</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Store, share, and rediscover your memories with absolute privacy&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/ente-private-media-cloud-storage</link>
      <guid>https://www.lqdev.me/bookmarks/ente-private-media-cloud-storage</guid>
      <pubDate>2024-03-02 15:49 -05:00</pubDate>
      <category>e2ee</category>
      <category>photos</category>
      <category>cloud</category>
      <category>storage</category>
      <category>product</category>
    </item>
    <item>
      <title>Google’s Hugging Face deal puts ‘supercomputer’ power behind open-source AI</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Google Cloud’s new partnership with AI model repository Hugging Face is letting developers build, train, and deploy AI models without needing to pay for a Google Cloud subscription. Now, outside developers using Hugging Face’s platform will have “cost-effective” access to Google’s tensor processing units (TPU) and GPU supercomputers, which will include thousands of Nvidia’s in-demand and export-restricted H100s.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Google said that Hugging Face users can begin using the AI app-building platform Vertex AI and the Kubernetes engine that helps train and fine-tune models “in the first half of 2024.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://www.googlecloudpresscorner.com/2024-01-25-Google-Cloud-and-Hugging-Face-Announce-Strategic-Partnership-to-Accelerate-Generative-AI-and-ML-Development"&gt;Press Release&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/google-cloud-huggingface-deal</link>
      <guid>https://www.lqdev.me/responses/google-cloud-huggingface-deal</guid>
      <pubDate>2024-01-25 21:23 -05:00</pubDate>
      <category>google</category>
      <category>huggingface</category>
      <category>ai</category>
      <category>cloud</category>
      <category>llm</category>
      <category>ml</category>
      <category>developers</category>
    </item>
    <item>
      <title>Amazon and Anthropic announce strategic collaboration to advance generative AI </title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Anthropic selects AWS as its primary cloud provider and will train and deploy its future foundation models on AWS Trainium and Inferentia chips, taking advantage of AWS’s high-performance, low-cost machine learning accelerators.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/aws-anthropic-partnership</link>
      <guid>https://www.lqdev.me/bookmarks/aws-anthropic-partnership</guid>
      <pubDate>2023-09-25 10:11 -05:00</pubDate>
      <category>ai</category>
      <category>cloud</category>
    </item>
    <item>
      <title>Impressed by Azure Cloud Shell</title>
      <description>&lt;![CDATA[&lt;p&gt;I can probably count the number of times I've used the Azure Cloud Shell inside the Azure Portal with one hand. However, I recently had a need to use it and was impressed by the experience.&lt;/p&gt;
&lt;p&gt;I wanted to create an Azure Function app but didn't want to install all the tools required. Currently I'm running Manjaro on an ASUS L210MA. That's my daily driver and although the specs are incredibly low I like the portability. The Azure set of tools run on Linux but are easier to install on Debian distributions. The low specs plus the Arch distro didn't make me want to go through the process of installing the Azure tools locally for a throwaway app.&lt;/p&gt;
&lt;p&gt;That's when I thought about using Azure Cloud Shell. Setup was relatively quick and easy. It was also great to see the latest version of .NET, Azure Functions Core Tools, and Azure CLI were already installed. Even Git was already there. After creating the function app, I was able to make some light edits using the built-in Monaco editor. When I was ready to test the app, I was able to open up a port, create a proxy, and test the function locally. It's not a high-end machine so I won't be training machine learning models on it anytime soon. However, for experimentation and working with my Azure account it's more than I need.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/impressed-az-cloud-shell</link>
      <guid>https://www.lqdev.me/notes/impressed-az-cloud-shell</guid>
      <pubDate>2022-10-13 22:45 -05:00</pubDate>
      <category>azure</category>
      <category>cloud</category>
      <category>tools</category>
      <category>development</category>
    </item>
    <item>
      <title>A year's worth of Azure hosting costs</title>
      <description>&lt;![CDATA[&lt;p&gt;It's been a little over a year since I started hosting my website on Azure. The resources I use include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Azure Storage: Where all my site assets are stored.&lt;/li&gt;
&lt;li&gt;Azure CDN: Content delivery&lt;/li&gt;
&lt;li&gt;Azure Logic Apps: Event-based jobs to syndicate new posts on Twitter and Mastodon.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I haven't tracked traffic for the entire year but since October of last year the site has gotten roughly 5,000 visitors.&lt;/p&gt;
&lt;p&gt;Taking all that into account, I've only had to pay $2.06 most of it being the storage I use for the site. In comparison, I was previously hosting my website through my domain provider in a shared server where it cost about $60!&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/166848883-8d6cfe2c-5ff3-468c-b95f-b0ecab69595a.png" class="img-fluid" alt="Azure portal billing cost" /&gt;&lt;/p&gt;
&lt;p&gt;If you're interested in how this site is built, check out the &lt;a href="https://www.lqdev.me/colophon"&gt;colophon&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/azure-costs-website-hosting-2021</link>
      <guid>https://www.lqdev.me/notes/azure-costs-website-hosting-2021</guid>
      <pubDate>2022-05-04 20:36 -05:00</pubDate>
      <category>azure</category>
      <category>website</category>
      <category>cost</category>
      <category>hosting</category>
      <category>selfhosting</category>
      <category>cloud</category>
      <category>indieweb</category>
    </item>
    <item>
      <title>Automating Resource Provisioning for Machine Learning in Azure with Cognitive Services and Terraform</title>
      <description>&lt;![CDATA[
## Introduction

Resources in the cloud, particularly Azure can be created in several ways some of which include Powershell, Azure CLI, Azure Portal and programatically through code. Another tool that can be leveraged to create resources in Azure is [Terraform](https://www.terraform.io/). Terraform via configuration files can build and manage infrastructure across various providers such as AWS, Azure and Google. One of the benefits of Terraform is that it allows infastructure and resources to be defined as code. The configuration files serve as a blueprint as well as an execution plan of what needs to be provisioned in the cloud. Not only does this allow for automation and removal of human error in configurations but it also serves as a way to document the different changes infrastructure has gone through thoroughout the application lifecycle. The purpose of this writeup is to show how to create an environment using Terraform for an application that utilizes Azure Cognitive Services. Our application will create a Computer Vision service that we can then use via HTTP requests. Source code for this writeup can be found at the following [link](https://github.com/lqdev/azcognitiveserviceterraformsample).

# Prerequisites

This writeup was built and tested using a PC running Ubuntu 18.04 but should work on both Windows and Mac. It also assumes that you have an Azure account as well as Azure CLI and Terraform CLI installed. Below are links to get all of the resources needed:

- [Azure Account](https://azure.microsoft.com/en-us/free/)
- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest)
- [Terraform CLI](https://www.terraform.io/downloads.html)

## Log into Azure CLI

The first thing we need to do is authenticate with Azure CLI so that Terraform can use your account information to create services. In the terminal, enter the following command: 

```bash
az login
```

## Create Terraform Resource Configuration Script

Once we're logged in, it's time to create the configuration file that Terraform will use to provision our services. First we'll need to create a directory for our application:

```bash
mkdir azcognitiveserviceterraformsample
```

Then, we'll enter that directory and create our configuration file:

```bash
cd azcognitiveserviceterraformsample
touch azcomputervision.tf
```

Using your preferred text editor, open the newly created `azcomputervision.tf` file and begin editing.

### Defining the Provider

At the top of our file we'll want to configure our provider. This tells Terraform where our resources will be deployed to. In the `azcomputervision.tf` file enter the following:

```bash
# Configure Azure Provider
provider "azurerm" {
  version="=1.20.0"
}
```

Because we've already authenticated with the Azure CLI, there's no need to provide credentials. The only thing we need to do is specify the provider name and the version of the provider that will be used by the configuration file. For more details on how to configure the Azure provider, visit this [link](https://www.terraform.io/docs/providers/azurerm/index.html).

### Create Resource Group

After defining the provider, it's time to create a Resource Group that will contain the resources we create. In the configuration file, under the provider definition, enter the following:

```bash
# Create Resource Group
resource "azurerm_resource_group" "terraform_cognitive_sample" {
  name="terraform_cognitive_sample"
  location="East US"
}
```

This defines the name of our resource groups as well as where it is hosted. The syntax for resources looks like the snippet below where the `type` is the type of resources as required by Terraform and the `name` is any value of your choosing.

```bash
resource "type" "name" {
# Properties
}
```

For more details on Resource Group configuration visit this [link](https://www.terraform.io/docs/providers/azurerm/d/resource_group.html).

### Create Azure Cognitive Service

With our resource group defined, it's time to define our Cognitive Service resource. Below our Resource Group definition, enter the following:

```bash
resource "azurerm_cognitive_account" "computer_vision_service" {
  name="computer_vision_service"
  resource_group_name="${azurerm_resource_group.terraform_cognitive_sample.name}"
  location="${azurerm_resource_group.terraform_cognitive_sample.location}"
  kind="ComputerVision"
  
  sku {
      name="F0"
      tier="Free"
  }
}
```

Like our Resource Group, we provide a name for our service. Additional properties we need to provide are the name of the Resource Group and the location of where to deploy our service to. Because this has been previously defined in our Resource Group, Terraform allows us to access the configuration values as variables. Additionally, we need to specify the `kind` of cognitive service to deploy. In our case it will be the Computer Vision service so we use `ComputerVision`. The last thing we need to do is specify the pricing tier to use. We'll be using the free tier for this project. There are several services and tiers to choose from. To get more details on acceptable values for these properties visit the following [link](https://www.terraform.io/docs/providers/azurerm/r/cognitive_account.html).

### Store Output Variables

When our application is deployed, in order to use it we'll need the endpoint to where we will be making HTTP requests to. This can be persisted by Terraform by defining an output variable inside of the configuration file. To get the enpoint of our cognitive service, enter the following in the `azcomputervision.tf` file:

```bash
output "computer_vision_endpoint" {
    value="${azurerm_cognitive_account.computer_vision_service.endpoint}"
}
```

Once finished, the `azcomputervision.tf` file should look like this:

```bash
# Configure Azure Provider
provider "azurerm" {
  version="=1.20.0"
}

# Create Resource Group
resource "azurerm_resource_group" "terraform_cognitive_sample" {
  name="terraform_cognitive_sample"
  location="East US"
}

# Create Cognitive Service
resource "azurerm_cognitive_account" "computer_vision_service" {
  name="computer_vision_service"
  resource_group_name="${azurerm_resource_group.terraform_cognitive_sample.name}"
  location="${azurerm_resource_group.terraform_cognitive_sample.location}"
  kind="ComputerVision"
  
  sku {
      name="F0"
      tier="Free"
  }
}

# Output Cognitive Services Endpoint
output "computer_vision_endpoint" {
    value="${azurerm_cognitive_account.computer_vision_service.endpoint}"
}
```

## Provision Resources

### Initialize Terraform Resources

When provisioning resources we first want to get the necessary plugins needed by Terraform to create such resources. Using Terraform CLI, enter the following in the terminal:

```bash
terraform init
```

### Check Execution Plan

Although this is a single resource provision, it's always a good idea to see the execution plan of the resources that will be created. To do so, enter the following command in the terminal:

```bash
terraform plan
```

The output should look similar to the content below:

```text
Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.


------------------------------------------------------------------------

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  + azurerm_cognitive_account.computer_vision_service
      id:                  &lt;computed&gt;
      endpoint:            &lt;computed&gt;
      kind:                "ComputerVision"
      location:            "eastus"
      name:                "computer_vision_service"
      resource_group_name: "terraform_cognitive_sample"
      sku.#:               "1"
      sku.0.name:          "F0"
      sku.0.tier:          "Free"
      tags.%:              &lt;computed&gt;

  + azurerm_resource_group.terraform_cognitive_sample
      id:                  &lt;computed&gt;
      location:            "eastus"
      name:                "terraform_cognitive_sample"
      tags.%:              &lt;computed&gt;


Plan: 2 to add, 0 to change, 0 to destroy.
```

## Create Resources

It's now time to create our Computer Vision resource. Enter the following command in the terminal:

```bash
terraform apply
```

You will be asked to review the execution plan once again. If everything looks good, type `yes` in the terminal to continue with the creation of resources. If everything deployed successfully, you should be able to see it in Azure.

## Test Provisioned Resources

Using Azure CLI, enter the following command in the terminal to see whether your resource was deployed:

```bash
az resource list --resource-group terraform_cognitive_sample --output table
```

That command will output something similar to the content below:

```bash
Name                     ResourceGroup               Location    Type                                  Status
-----------------------  --------------------------  ----------  ------------------------------------  --------
computer_vision_service  terraform_cognitive_sample  eastus      Microsoft.CognitiveServices/accounts
```

### Get Resource Endpoint and Keys

In order to make a request to our service, we need both the endpoint and a key to authenticate our request.

To get the endpoint, we can use Terraform CLI to extract the output variable defined in our configuration file. We can do that by using the following command in the terminal:

```bash
terraform output computer_vision_endpoint
```

Save the output value somewhere because that's what will be used to make a request to the Computer Vision Cognitive Service.

To get the keys, we'll use Azure CLI. In the terminal, enter:

```bash
az cognitiveservices account keys list --resource-group terraform_cognitive_sample --name computer_vision_service
```

This will output the keys of your deployed service.

```json
{
  "key1": "&lt;YOUR-KEY-1&gt;",
  "key2": "&lt;YOUR-KEY-2&gt;"
}
```

### Make HTTP Request

Either key can be used to make requests. To test the service, I'll be using an image from the web. All you need is to get the URL for that image. The image I will be using can be found at this URL `https://upload.wikimedia.org/wikipedia/commons/1/10/Empire_State_Building_%28aerial_view%29.jpg`. Using cURL, I will make a POST request to the endpoint with the key provided.

One thing to note is that the endpoint is generic to many Cognitive Services. Therefore, we need to append to the path which service we will be using and the action we want to perform. In our case, we'll append the following path the the endpoint `/vision/v1.0/describe`. For more details, visit the Computer Vision API documentation at this [link](https://eastus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fe).

To make the request, enter the following into the terminal where `&lt;YOUR-KEY&gt;` is one of the keys of your Computer Vision service.

```bash
curl -H 'Ocp-Apim-Subscription-Key: &lt;YOUR-KEY&gt;' -H "Content-type: application/json" -d '{"url":"https://upload.wikimedia.org/wikipedia/commons/1/10/Empire_State_Building_%28aerial_view%29.jpg"}' 'https://eastus.api.cognitive.microsoft.com/vision/v1.0/describe'
```

If on Windows, you might want to try using a REST client such as POSTMAN or Insomnia.

If the request is successful, the output should be similar to the following:

```json
{
    "description":{
        "tags":[
            "mountain","building","city","sitting","table","view","full","filled","large","old","skyscraper","many","stacked","water","room","white"
        ],
        "captions":[
            {
                "text":"a view of a city","confidence":0.927147938733121
            }
        ]
    },
    "requestId":"a9aaf779-2aa1-4012-b1e0-2d2d9c20c6a5",
    "metadata":{
        "width":846,
        "height":1270,
        "format":"Jpeg"
    }
}
```

## Conclusion

In this writeup I went over how to create a Terraform configuration file for automating the creation of a Computer Vision Cognitive Service resource in Azure. Although this is a simple example, using the same concepts, more complex environments and infrastructure can be provisioned just as seamlessly in a safe and efficient manner. An example of how to extend this application would be to provision an Azure Storage Account as well as an Azure Function application that utilizes the Computer Vision Cognitive Service to automatically classify photos when they are uploaded to an Azure Storage container. In such scenario, the benefits of automation and documentation that Terraform provides can leveraged to a greater extent.]]&gt;</description>
      <link>https://www.lqdev.me/posts/automate-machine-learning-service-provisioning-azure-terraform</link>
      <guid>https://www.lqdev.me/posts/automate-machine-learning-service-provisioning-azure-terraform</guid>
      <pubDate>2019-01-05 22:58:39 -05:00</pubDate>
      <category>azure</category>
      <category>cognitive services</category>
      <category>terraform</category>
      <category>machine learning</category>
      <category>cloud</category>
    </item>
  </channel>
</rss>