<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - diffusion</title>
    <link>https://www.lqdev.me/tags/diffusion</link>
    <description>All content tagged with 'diffusion' by Luis Quintanilla</description>
    <lastBuildDate>2025-02-26 20:10 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Introducing Mercury, the first commercial-scale diffusion large language model</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are announcing the Mercury family of diffusion large language models (dLLMs), a new generation of LLMs that push the frontier of fast, high-quality text generation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mercury is up to 10x faster than frontier speed-optimized LLMs. Our models run at over 1000 tokens/sec on NVIDIA H100s, a speed previously possible only using custom chips.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/mercury-first-commercial-scale-llm</link>
      <guid>https://www.lqdev.me/responses/mercury-first-commercial-scale-llm</guid>
      <pubDate>2025-02-26 20:10 -05:00</pubDate>
      <category>diffusion</category>
      <category>llm</category>
      <category>ai</category>
    </item>
    <item>
      <title>One-step Diffusion with Distribution Matching Distillation</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Our one-step generator achieves comparable image quality with StableDiffusion v1.5 while being 30x faster.&lt;/strong&gt;&lt;br /&gt;
&lt;br&gt;
Diffusion models are known to approximate the score function of the distribution they are trained on. In other words, an unrealistic synthetic image can be directed toward higher probability density region through the denoising process (see SDS). Our core idea is training two diffusion models to estimate not only the score function of the target real distribution, but also that of the fake distribution. We construct a gradient update to our generator as the difference between the two scores, essentially nudging the generated images toward higher realism as well as lower fakeness (see VSD). Our method is similar to GANs in that a critic is jointly trained with the generator to minimize a divergence between the real and fake distributions, but differs in that our training does not play an adversarial game that may cause training instability, and our critic can fully leverage the weights of a pretrained diffusion model. Combined with a simple regression loss to match the output of the multi-step diffusion model, our method outperforms all published few-step diffusion approaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shot COCO-30k, comparable to Stable Diffusion but orders of magnitude faster. Utilizing FP16 inference, our model generates images at 20 FPS on modern hardware.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2311.18828"&gt;Paper&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/one-step-diffusion-distribution-matching-distillation</link>
      <guid>https://www.lqdev.me/responses/one-step-diffusion-distribution-matching-distillation</guid>
      <pubDate>2024-03-26 21:44 -05:00</pubDate>
      <category>ai</category>
      <category>genai</category>
      <category>diffusion</category>
      <category>dmd</category>
      <category>stablediffusion</category>
      <category>dalle</category>
    </item>
    <item>
      <title>Diffusion Models From Scratch</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Here, we'll cover the derivations from scratch to provide a rigorous understanding of the core ideas behind diffusion. What assumptions are we making? What properties arise as a result?&lt;br /&gt;
&lt;br&gt;
A reference [codebase] is written from scratch, which provides minimalist re-production of the MNIST example below. It clocks in at under 500 lines of code.&lt;br /&gt;
&lt;br&gt;
Each page takes up to an hour to read thoroughly. Approximately a lecture each.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/diffusion-models-from-scratch-duan</link>
      <guid>https://www.lqdev.me/bookmarks/diffusion-models-from-scratch-duan</guid>
      <pubDate>2024-03-17 21:03 -05:00</pubDate>
      <category>diffusion</category>
      <category>ai</category>
      <category>concepts</category>
      <category>models</category>
    </item>
    <item>
      <title>Diffusion models from scratch, from a new theoretical perspective</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;This tutorial aims to introduce diffusion models from an optimization perspective as introduced in our paper (joint work with Frank Permenter). It will go over both theory and code, using the theory to explain how to implement diffusion models from scratch. By the end of the tutorial, you will learn how to implement training and sampling code for a toy dataset, which will also work for larger datasets and models.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/diffusion-models-from-scratch-yuan</link>
      <guid>https://www.lqdev.me/bookmarks/diffusion-models-from-scratch-yuan</guid>
      <pubDate>2024-03-17 21:02 -05:00</pubDate>
      <category>diffusion</category>
      <category>ai</category>
      <category>concepts</category>
      <category>models</category>
    </item>
    <item>
      <title>Introducing Würstchen: Fast Diffusion for Image Generation </title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Würstchen is a diffusion model, whose text-conditional component works in a highly compressed latent space of images. Why is this important? Compressing data can reduce computational costs for both training and inference by orders of magnitude. Training on 1024×1024 images is way more expensive than training on 32×32. Usually, other works make use of a relatively small compression, in the range of 4x - 8x spatial compression. Würstchen takes this to an extreme. Through its novel design, it achieves a 42x spatial compression! This had never been seen before, because common methods fail to faithfully reconstruct detailed images after 16x spatial compression. Würstchen employs a two-stage compression, what we call Stage A and Stage B. Stage A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found in the  paper). Together Stage A and B are called the Decoder, because they decode the compressed images back into pixel space. A third model, Stage C, is learned in that highly compressed latent space. This training requires fractions of the compute used for current top-performing models, while also allowing cheaper and faster inference. We refer to Stage C as the Prior.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/wurstchen-fast-image-generation</link>
      <guid>https://www.lqdev.me/bookmarks/wurstchen-fast-image-generation</guid>
      <pubDate>2023-09-17 12:15 -05:00</pubDate>
      <category>ai</category>
      <category>generativeai</category>
      <category>imagegeneration</category>
      <category>huggingface</category>
      <category>diffusion</category>
      <category>ml</category>
    </item>
  </channel>
</rss>