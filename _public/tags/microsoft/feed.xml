<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - microsoft</title>
    <link>https://www.lqdev.me/tags/microsoft</link>
    <description>All content tagged with 'microsoft' by Luis Quintanilla</description>
    <lastBuildDate>2025-08-26 19:40 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Crescent library brings privacy to digital identity systems</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Digital identities, the electronic credentials embedded in phone wallets, workplace logins, and other apps, are becoming ubiquitous. While they offer unprecedented convenience, they also create new privacy risks, particularly around tracking and surveillance.&lt;br /&gt;
&lt;br&gt;
One of these risks is linkability, the ability to associate one or more uses of a credential to a specific person. Currently, when people use their mobile driver’s license or log into various apps, hidden identifiers can link these separate activities together, building detailed profiles of user behavior.&lt;br /&gt;
&lt;br&gt;
To address this, we have released &lt;a href="https://eprint.iacr.org/2024/2013"&gt;Crescent&lt;/a&gt;, a cryptographic library that adds unlinkability to widely used identity formats, protecting privacy. These include JSON Web Tokens (the authentication standard behind many app logins) and mobile driver’s licenses. Crescent also works without requiring the organizations that issue these credentials to update their systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/microsoft/crescent-credentials"&gt;GitHub Repo&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/crescent-msr-privacy-digital-identity</link>
      <guid>https://www.lqdev.me/responses/crescent-msr-privacy-digital-identity</guid>
      <pubDate>2025-08-26 19:40 -05:00</pubDate>
      <category>research</category>
      <category>privacy</category>
      <category>microsoft</category>
    </item>
    <item>
      <title>AI Dev Gallery now in the Microsoft Store</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;So excited to see the &lt;a href="https://apps.microsoft.com/detail/9n9pn1mm3bd5"&gt;AI Dev Gallery is now in the Microsoft Store&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Through various interactive samples, developers get to see how they might use AI for different tasks like text classification, object detection, and many others.&lt;/p&gt;
&lt;p&gt;Best of all, the models are all running locally and you get to see and export the C# code powering the samples to Visual Studio so you can continue tinkering on your own and integrate into your own applications.&lt;/p&gt;
&lt;p&gt;A few months ago, the team came on the .NET AI Community Standup to showcase the app. Since then, it's only kept improving and introducing new scenarios.&lt;/p&gt;
&lt;p&gt;You can check out the recording from that stream here.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=5H9TxzCQfNo" title="Thumbnail of AI Community Standup AI Dev Gallery"&gt;&lt;img src="http://img.youtube.com/vi/5H9TxzCQfNo/0.jpg" class="img-fluid" alt="Thumbnail of AI Community Standup AI Dev Gallery" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/ai-dev-gallery-windows-preview-ms-store</link>
      <guid>https://www.lqdev.me/responses/ai-dev-gallery-windows-preview-ms-store</guid>
      <pubDate>2025-02-15 12:02 -05:00</pubDate>
      <category>ai</category>
      <category>windows</category>
      <category>dotnet</category>
      <category>microsoft</category>
      <category>onnx</category>
    </item>
    <item>
      <title>MarkItDown - Convert files to Markdown</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;MarkItDown is a utility for converting various files to Markdown (e.g., for indexing, text analysis, etc). It supports:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PDF&lt;/li&gt;
&lt;li&gt;PowerPoint&lt;/li&gt;
&lt;li&gt;Word&lt;/li&gt;
&lt;li&gt;Excel&lt;/li&gt;
&lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt;
&lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt;
&lt;li&gt;HTML&lt;/li&gt;
&lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt;
&lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/microsoft-markitdown</link>
      <guid>https://www.lqdev.me/responses/microsoft-markitdown</guid>
      <pubDate>2024-12-18 21:43 -05:00</pubDate>
      <category>markdown</category>
      <category>etl</category>
      <category>data</category>
      <category>python</category>
      <category>ai</category>
      <category>microsoft</category>
    </item>
    <item>
      <title>Day 1 of .NET Conf</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;Day 1 of .NET Conf was a lot of fun. Tons of great sessions. Here's the recording of all the sessions.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=hM4ifrqF_lQ" title=".NET Conf Day 1 LiveStream"&gt;&lt;img src="http://img.youtube.com/vi/hM4ifrqF_lQ/0.jpg" class="img-fluid" alt=".NET Conf Day 1 LiveStream" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you're interested in checking out my session with Jeremy, Building AI Applications From Scratch, here's a link to it.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/live/hM4ifrqF_lQt=23407" title=".NET Conf Day 1 LiveStream Countdown"&gt;&lt;img src="http://img.youtube.com/vi/hM4ifrqF_lQ/1.jpg" class="img-fluid" alt=".NET Conf Day 1 LiveStream Countdown" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tomorrow, I'll also get a chance to share the stage with Tarek and Tanner to talk &lt;a href="https://www.dotnetconf.net/agenda#:%7E:text=Building%20the%20Foundation:%20AI%20Fundamentals%20in%20.NET"&gt;AI Fundamentals&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/dotnetconf-2024-day-1-success</link>
      <guid>https://www.lqdev.me/responses/dotnetconf-2024-day-1-success</guid>
      <pubDate>2024-11-13 01:47 -05:00</pubDate>
      <category>dotnet</category>
      <category>dotnetconf</category>
      <category>dotnet9</category>
      <category>net9</category>
      <category>conference</category>
      <category>ai</category>
      <category>aspire</category>
      <category>microsoft</category>
    </item>
    <item>
      <title>Microsoft Research: Introducing DRIFT Search</title>
      <description>&lt;![CDATA[[reshare] &lt;h2&gt;Introducing DRIFT Search: Combining global and local search methods to improve quality and efficiency&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;DRIFT search (Dynamic Reasoning and Inference with Flexible Traversal)...builds upon Microsoft’s GraphRAG technique, combining characteristics of both global and local search to generate detailed responses in a method that balances computational costs with quality outcomes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;DRIFT Search: A step-by-step process&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Primer&lt;/strong&gt;: When a user submits a query, DRIFT compares it to the top K most semantically relevant community reports. This generates an initial answer along with several follow-up questions, which act as a lighter version of global search. To do this, we expand the query using Hypothetical Document Embeddings (HyDE), to increase sensitivity (recall), embed the query, look up the query against all community reports, select the top K and then use the top K to try to answer the query. The aim is to leverage high-level abstractions to guide further exploration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Follow-Up&lt;/strong&gt;: With the primer in place, DRIFT executes each follow-up using a local search variant. This yields additional intermediate answers and follow-up questions, creating a loop of refinement that continues until the search engine meets its termination criteria, which is currently configured for two iterations (further research will investigate reward functions to guide terminations). This phase represents a globally informed query refinement. Using global data structures, DRIFT navigates toward specific, relevant information within the knowledge graph even when the initial query diverges from the indexing persona. This follow-up process enables DRIFT to adjust its approach based on emerging information.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output Hierarchy&lt;/strong&gt;: The final output is a hierarchy of questions and answers ranked on their relevance to the original query. This hierarchical structure can be customized to fit specific user needs. During benchmark testing, a naive map-reduce approach aggregated all intermediate answers, with each answer weighted equally.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/introducing-drift-search</link>
      <guid>https://www.lqdev.me/responses/introducing-drift-search</guid>
      <pubDate>2024-11-04 21:59 -05:00</pubDate>
      <category>ai</category>
      <category>search</category>
      <category>RAG</category>
      <category>llm</category>
      <category>microsoft</category>
      <category>research</category>
      <category>msr</category>
      <category>graphrag</category>
    </item>
    <item>
      <title>Microsoft Research Focus: Week of October 28, 2024</title>
      <description>&lt;![CDATA[[reshare] &lt;h2&gt;METAREFLECTION: Learning Instructions for Language Agents using Past Reflections&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Language agents are AI systems that can understand, reason and respond in natural language to complete various tasks. While the latest LLMs are capable enough to power reasonably good language agents, the closed-API model makes it hard to improve them when they perform sub-optimally. Recent studies have explored using techniques like self-reflection and prompt optimization to improve performance. Unfortunately, self-reflection can be used only during the agent’s current run, while contemporary prompt optimization techniques are designed and tested to work on simple single-step agents.&lt;/p&gt;
&lt;p&gt;In a recent paper: METAREFLECTION: Learning Instructions for Language Agents using Past Reflections, researchers from Microsoft introduce a novel offline reinforcement learning technique that enhances the performance of language agents by augmenting a semantic memory based on experiential learnings from past trials. They demonstrate the efficacy of METAREFLECTION across multiple domains, including complex logical reasoning, biomedical semantic similarity, open world question answering, and vulnerability threat detection, in Infrastructure-as-Code, spanning different agent designs. METAREFLECTION boosts language agents’ performance by 4% to 16.82% over the baseline agent implementations and performs on par with existing state-of-the-art prompt optimization techniques while requiring fewer LLM calls.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Generative AI applications rely on large, foundation models, particularly LLMs. LLMs often have tens to hundreds of billions of parameters, making them too large for a single graphics processing unit (GPU) to handle in terms of both memory and computation. Because of their size, training these models requires distributing the workload across hundreds or even thousands of GPUs. This can lead to significant communication overhead, a challenge that arises when data needs to be shared between different GPUs.&lt;/p&gt;
&lt;p&gt;In a recent paper: Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping, researchers from Microsoft introduce a system designed to enhance the efficiency of LLM training by reducing the time lost to communication between GPUs.&lt;/p&gt;
&lt;p&gt;Domino breaks down data dependencies in a single batch of training into smaller, independent pieces. These smaller pieces are processed in parallel, and communication between GPUs happens simultaneously with computation, minimizing delays.&lt;/p&gt;
&lt;p&gt;Test results comparing Domino to Megatron-LM show that Domino speeds up the training process by up to 1.3x on Nvidia DGX-H100 GPUs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;OmniParser for pure vision-based GUI agent&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large vision-language models (VLMs) such as GPT-4V and GPT-4o show promise in driving intelligent agent systems that operate within user interfaces (UI). However, VLMs’ full potential remains underexplored in real-world applications, particularly when it comes to acting as general agents across diverse operating systems and applications with only vision input. One limiting factor is the absence of a robust technique for screen parsing which is capable of 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associating the intended action with the corresponding region on the screen.&lt;/p&gt;
&lt;p&gt;In a recent article: OmniParser for pure vision-based GUI agent, researchers from Microsoft present a compact screen parsing module that can convert UI screenshots into structured elements. OmniParser can be used with a variety of models to create agents capable of taking actions on UIs. When used with GPT-4V, OmniParser significantly improves the agent capability to generate precisely grounded actions for interface regions.&lt;/p&gt;
&lt;p&gt;OmniParser with GPT-4V agent achieved the best performance on the recently released  WindowsAgentArena (opens in new tab) benchmark.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/microsoft-research-focus-oct-28-2024</link>
      <guid>https://www.lqdev.me/responses/microsoft-research-focus-oct-28-2024</guid>
      <pubDate>2024-11-04 21:39 -05:00</pubDate>
      <category>ai</category>
      <category>microsoft</category>
      <category>research</category>
      <category>msr</category>
      <category>microsoftresearch</category>
    </item>
    <item>
      <title>Reddit says companies must pay for data access</title>
      <description>&lt;![CDATA[[reply] &lt;p&gt;&lt;img src="https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExejNzaDF5aHZ6dHRlenl6bnh4c2h6YWoxbnRuOXBqZnlmOGVlbWlwOCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/xT9DPzhNGA8MKjxwFG/giphy.gif" class="img-fluid" alt="GIF of Snoopy laughing" /&gt;&lt;/p&gt;
&lt;p&gt;I'm all for compensation. Remind me again, how much are the communities that create the content and make Reddit what it is getting out of these licensing deals?&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/reddit-companies-pay-data-access</link>
      <guid>https://www.lqdev.me/responses/reddit-companies-pay-data-access</guid>
      <pubDate>2024-07-31 16:23 -05:00</pubDate>
      <category>ai</category>
      <category>reddit</category>
      <category>data</category>
      <category>licensing</category>
      <category>microsoft</category>
      <category>anthropic</category>
      <category>perplexity</category>
    </item>
    <item>
      <title>LongROPE: Extending LLM Context Window Beyond 2 Million Tokens</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/jshuadvd/LongRoPE"&gt;GitHub Repo&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/longrope-extending-llm-context-2m-tokens</link>
      <guid>https://www.lqdev.me/bookmarks/longrope-extending-llm-context-2m-tokens</guid>
      <pubDate>2024-07-30 14:16 -05:00</pubDate>
      <category>ai</category>
      <category>llm</category>
      <category>longrope</category>
      <category>microsoft</category>
      <category>research</category>
      <category>msr</category>
    </item>
    <item>
      <title>SAMMO: A general-purpose framework for prompt optimization</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large language models (LLMs) have revolutionized a wide range of tasks and applications that were previously reliant on manually crafted machine learning (ML) solutions, streamlining through automation. However, despite these advances, a notable challenge persists: the need for extensive prompt engineering to adapt these models to new tasks. New generations of language models like GPT-4 and Mixtral 8x7B advance the capability to process long input texts. This progress enables the use of longer inputs, providing richer context and detailed instructions to language models. A common technique that uses this enhanced capacity is the Retrieval Augmented Generation (RAG) approach. RAG dynamically incorporates information into the prompt based on the specific input example.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;To address these challenges, we developed the Structure-Aware Multi-objective Metaprompt Optimization (SAMMO) framework. SAMMO is a new open-source tool that streamlines the optimization of prompts, particularly those that combine different types of structural information like in the RAG example above. It can make structural changes, such as removing entire components or replacing them with different ones. These features enable AI practitioners and researchers to efficiently refine their prompts with little manual effort.&lt;br /&gt;
&lt;br&gt;
Central to SAMMO’s innovation is its approach to treating prompts not just as static text inputs but as dynamic, programmable entities—metaprompts. SAMMO represents these metaprompts as function graphs, where individual components and substructures can be modified to optimize performance, similar to the optimization process that occurs during traditional program compilation.&lt;br /&gt;
&lt;br&gt;
The following key features contribute to SAMMO’s effectiveness:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Structured optimization: Unlike current methods that focus on text-level changes, SAMMO focuses on optimizing the structure of metaprompts. This granular approach facilitates precise modifications and enables the straightforward integration of domain knowledge, for instance, through rewrite operations targeting specific stylistic objectives.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Multi-objective search: SAMMO’s flexibility enables it to simultaneously address multiple objectives, such as improving accuracy and computational efficiency. Our paper illustrates how SAMMO can be used to compress prompts without compromising their accuracy.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;General purpose application: SAMMO has proven to deliver significant performance improvements across a variety of tasks, including instruction tuning, RAG, and prompt compression.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/sammo-prompt-optimization-framework</link>
      <guid>https://www.lqdev.me/responses/sammo-prompt-optimization-framework</guid>
      <pubDate>2024-04-25 22:48 -05:00</pubDate>
      <category>ai</category>
      <category>microsoft</category>
      <category>research</category>
      <category>prompt</category>
      <category>optimization</category>
      <category>sammo</category>
    </item>
    <item>
      <title>Introducing Phi-3</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Starting today, Phi-3-mini, a 3.8B language model is available on Microsoft Azure AI Studio, Hugging Face, and Ollama.&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Phi-3-mini is available in two context-length variants—4K and 128K tokens. It is the first model in its class to support a context window of up to 128K tokens, with little impact on quality.&lt;/li&gt;
&lt;li&gt;It is instruction-tuned, meaning that it’s trained to follow different types of instructions reflecting how people normally communicate. This ensures the model is ready to use out-of-the-box.&lt;/li&gt;
&lt;li&gt;It is available on Azure AI to take advantage of the deploy-eval-finetune toolchain, and is available on Ollama for developers to run locally on their laptops.&lt;/li&gt;
&lt;li&gt;It has been optimized for ONNX Runtime with support for Windows DirectML along with cross-platform support across graphics processing unit (GPU), CPU, and even mobile hardware.&lt;/li&gt;
&lt;li&gt;It is also available as an NVIDIA NIM microservice with a standard API interface that can be deployed anywhere. And has been optimized for NVIDIA GPUs.&lt;br /&gt;
&lt;br&gt;
In the coming weeks, additional models will be added to Phi-3 family to offer customers even more flexibility across the quality-cost curve. Phi-3-small (7B) and Phi-3-medium (14B) will be available in the Azure AI model catalog and other model gardens shortly.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/introducing-phi-3</link>
      <guid>https://www.lqdev.me/responses/introducing-phi-3</guid>
      <pubDate>2024-04-23 22:47 -05:00</pubDate>
      <category>microsoft</category>
      <category>phi3</category>
      <category>ai</category>
      <category>slm</category>
      <category>llm</category>
      <category>genai</category>
    </item>
    <item>
      <title>Introducing the 100 Sways Project</title>
      <description>&lt;![CDATA[&lt;p&gt;Introducing the 100 Sways project.&lt;/p&gt;
&lt;p&gt;Over the next few months, I plan on creating artifacts like blog posts, newsletters, and websites using &lt;a href="https://sway.cloud.microsoft/"&gt;Microsoft Sway&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Bookmark the site &lt;a href="http://100sways.lqdev.tech"&gt;100sways.lqdev.tech&lt;/a&gt; and follow along!&lt;/p&gt;
&lt;p&gt;Note: If you can't access the website, you can use the &lt;a href="https://sway.cloud.microsoft/aC19McbR3xcNo6s8"&gt;direct link (https://sway.cloud.microsoft/aC19McbR3xcNo6s8)&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;What is the 100 Sways project?&lt;/h2&gt;
&lt;p&gt;This project is an attempt to show how you might use Sway as a publishing platform for content like newsletters, blog posts, and websites.&lt;/p&gt;
&lt;p&gt;This project was inspired by &lt;a href="https://jamesg.blog/"&gt;James'&lt;/a&gt; &lt;a href="https://jamesg.blog/2024/02/19/personal-website-ideas/"&gt;100 things you can do on your personal website post&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Motivations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Learn more about Microsoft Sway&lt;/li&gt;
&lt;li&gt;Show how you can make the most use out of apps and services already included with your Microsoft Account&lt;/li&gt;
&lt;li&gt;Show how easy it is to create and consume web content using Sway&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/introducing-100-sways-project</link>
      <guid>https://www.lqdev.me/notes/introducing-100-sways-project</guid>
      <pubDate>2024-03-24 16:33 -05:00</pubDate>
      <category>microsoft</category>
      <category>sway</category>
      <category>indieweb</category>
      <category>blog</category>
      <category>website</category>
      <category>microblog</category>
      <category>blogging</category>
      <category>content</category>
      <category>m365</category>
      <category>office</category>
      <category>internet</category>
      <category>web</category>
      <category>microsoftoffice</category>
      <category>microsoft365</category>
    </item>
    <item>
      <title>New Era of Work - Windows / Surface Event Blog (March 21, 2024)</title>
      <description>&lt;![CDATA[&lt;p&gt;&lt;strong&gt;12:03&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copilot is making people more productive.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;12:06&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Copilot for Microsoft 365 integrated into Windows.&lt;/li&gt;
&lt;li&gt;Demo: After coming back from vacation, you want to catch up.
&lt;ul&gt;
&lt;li&gt;Ask copilot about latest project updates&lt;/li&gt;
&lt;li&gt;Ask copilot for summary of meeting you missed which generates a summary of the discussion and action items.&lt;/li&gt;
&lt;li&gt;Copilot then can help you tackle the action items like drafting and sending e-mails.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;12:09&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demo: Using Copilot to configure Windows settings&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;12:13&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demo: Using Windows App to manage Windows 365 cloud PCs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;12:21&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Surface Pro 10 and Surface Laptop 6 announced for business. Comes with latest Intel processors, Neural Processing Units (NPUs), and Copilot.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;12:39&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The demo showing a .NET development workflow on the Surface Laptop 6 was great. Usually that same workflow longer on my Lenovo P4.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/surface-event-mar-21-24-event-blog</link>
      <guid>https://www.lqdev.me/notes/surface-event-mar-21-24-event-blog</guid>
      <pubDate>2024-03-21 12:02 -05:00</pubDate>
      <category>windows</category>
      <category>microsoft</category>
      <category>surface</category>
      <category>copilot</category>
      <category>ai</category>
      <category>pc</category>
      <category>hardware</category>
    </item>
    <item>
      <title>The latest Microsoft Copilot update on Android makes me mourn the death of Cortana</title>
      <description>&lt;![CDATA[[reply] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Microsoft Copilot will soon be able to be your default assistant app on Android.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It's a shame that Cortana never worked out for Microsoft. If things had lined up differently, we may have seen Copilot gain access to smart devices and commands like Gemini has with Google Assistant (though that setup isn't perfect). While Copilot has a place on a computer, I think an assistant on your smartphone needs to be able to do more day-to-day tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So many things were ahead of their time. I just want a &lt;a href="https://www.lqdev.me/responses/windows-12-mobile-concept/"&gt;Windows Mobile PC (Windows Phone?) with an LLM-backed Cortana&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/copilot-android-update-mourn-cortana-death</link>
      <guid>https://www.lqdev.me/responses/copilot-android-update-mourn-cortana-death</guid>
      <pubDate>2024-02-27 21:04 -05:00</pubDate>
      <category>cortana</category>
      <category>copilot</category>
      <category>ai</category>
      <category>microsoft</category>
      <category>virtualassistant</category>
      <category>assistant</category>
      <category>chatbot</category>
    </item>
    <item>
      <title>GraphRAG: Unlocking LLM discovery on narrative private data </title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Perhaps the greatest challenge – and opportunity – of LLMs is extending their powerful capabilities to solve problems beyond the data on which they have been trained, and to achieve comparable results with data the LLM has never seen.  This opens new possibilities in data investigation, such as identifying themes and semantic concepts with context and grounding on datasets.  In this post, we introduce GraphRAG, created by Microsoft Research, as a significant advance in enhancing the capability of LLMs.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/ms-research-graphrag</link>
      <guid>https://www.lqdev.me/bookmarks/ms-research-graphrag</guid>
      <pubDate>2024-02-13 21:50 -05:00</pubDate>
      <category>research</category>
      <category>ai</category>
      <category>rag</category>
      <category>llm</category>
      <category>data</category>
      <category>microsoft</category>
    </item>
    <item>
      <title>Microsoft is Dead (2007)</title>
      <description>&lt;![CDATA[[reply] &lt;p&gt;&lt;img src="https://media.giphy.com/media/b6iVj3IM54Abm/giphy.gif" class="img-fluid" alt="GIF of WWE Undertaker sitting up" /&gt;&lt;/p&gt;
&lt;p&gt;Interesting points.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/graham-microsoft-dead</link>
      <guid>https://www.lqdev.me/responses/graham-microsoft-dead</guid>
      <pubDate>2024-01-23 21:12 -05:00</pubDate>
      <category>google</category>
      <category>microsoft</category>
      <category>web</category>
    </item>
    <item>
      <title>Using Generative AI to produce Spotify Clips</title>
      <description>&lt;![CDATA[
## Introduction

While listening to music on Spotify last night, this thought hit me. It's been cooking for some time so now you get to read about it. Spotify and YouTube, if you build this into your platforms for songs or podcasts, [contact me](/contact) for info on where to send the check :joy:. As the title of the post suggests, this is a thought experiment on how you might use Generative AI to produce Spotify Clips. This is not limited to songs but could also be extended to audio-only podcasts. However, given the long-form nature of podcasts it might be extremely resource intensive to produce clips per episode. This post is not exactly a how-to guide, though there are some practical examples of how some of the steps in the workflow might work using [Microsoft Copilot (formerly Bing Chat)](https://copilot.microsoft.com/).

## Spotify Clips

Let's start off by talking about what Spotify Clips are. Back in March of 2023, [Spotify introduced Clips to their platform](https://artists.spotify.com/blog/spotify-clips-get-started-short-form-video-stream-on). Spotify describes [Clips](https://clips.byspotify.com/) as "short, under-30-second vertical videos that are uploaded directly to Spotify for Artists. They’re designed to put you and your music front-and-center, so they can be attached to your artist profile, to a song, to an album, or to an upcoming release.". When paired with a song, you can think of them as short music videos. Music videos being the videos made to accompany songs that played on TV channels like Music Television (MTV) before Jersey Shore and Catfish. Music videos are still in production today and they generally serve the same purpose. They provide a visual representation of the story being told by the song.  

## Generative AI for video

An overwhelming majority of Generative AI models only generate text. With models like DALL-E and Stable Diffusion, you can also generate images. Other models to generate audio and video have started coming on the scene. Most recently, Google announced [VideoPoet](https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html), a large language model for zero-shot video generation. There's many interesting techniques that VideoPoet introduces but I'll keep it to one of the more simple scenarios and the technique used in this post, text-to-image. Given a text prompt, VideoPoet can generate by default, a short 2-second clip. You can use VideoPoet as well to extend a clip so you could go well beyond the default 2 seconds. That however is outside the scope of this post. With that in mind, let's talk about how we might use a Generative AI model and apply it to the context of Spotify Clips.

## Generative AI Spotify Clips workflow

The following is a rough idea of how you might use song information like lyrics to 

```mermaid
flowchart TD
  A["Get song lyrics"] --&gt; B["Generate scenes and script"]
  B --&gt; C["Generate prompts for each scene"]
  C --&gt; D["Generate video clips"]
  D --&gt; E["(Optional) Edit clips"]
  D --&gt; F["Stitch clips together"]
  E--&gt;F
```

This is roughly what Google did in their VideoPoet samples. They "...produced a short movie composed of many short clips generated by the model. For the script, we asked Bard to write a series of prompts to detail a short story about a traveling raccoon. We then generated video clips for each prompt, and stitched together all resulting clips". The resulting video looks like the following:

[![An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities](http://img.youtube.com/vi/70wZKfx6Ylk/0.jpg)](https://www.youtube.com/watch?v=70wZKfx6Ylk "An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities")

One thing to note is that the videos generated are YouTube Shorts which are vertical as well. This makes it easier to produce videos in the format Spotify expects Clips to be in.

## AI Generated Spotify Clips in practice

Now that we know the general process, what does it actually look like in practice? To help me with that, I used Microsoft Copilot.

### Generate scenes and script

In the example shared by Google, they used a prompt to generate a script made up of a series of prompts. In the case of songs on Spotify, we can use that as context and the starting point for our script. From there, we can ask Copilot to compose scenes for the verses in the song. We can also make sure as these scenes are generated, that there is a smooth transition between them and the storytelling is consistent. 

Let's take a look at what this might look like using the Chicano Batman song "A Hundred Dead and Loving Souls". Why that one? Just check out my [Spotify Wrapped](/notes/spotify-wrapped-2023). The general theme of that song is about the cycles of life experienced across millenia and generations. 

**Prompt**

Given the following prompt:

```text
Using the song information below as context, generate a script for a story that visually represents the song lyrics. Ensure that the script is broken down into individual scenes and the scenes are linear and cohesively transition.

&lt;song&gt;
&lt;title&gt;A Hundred Dead and Loving Souls&lt;/title&gt;
&lt;lyrics&gt;
I am the sand stuck under miles of rock
watching the dry air sweep away my thoughts
as we travel protected by shards of glass
driving through the history of a million memories

In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
Climbing a thousand unclimbed trees
I don't know I want to know how it would feel
to be on top of one of these!

I am the root
quickly ascending
up through the surrounding dirt
quickly devoured by a hungry sun
quickly devoured by a hungry son
who is my father
and we are alike
we look alike we talk alike
we are alike
we are alike
we are alike we look alike
we talk alike

Thats why Im hungry
like the bellowing sky
this is no lie
as sure as light
entering eye
want to stay high
everything's cool too cool so cool
and I remember that everything falls

So I take a look around me now, as I speak
and I imagine a disinterested crowd
talking aloud
and I think of what I want most
and the crowd disappears
and a fixated gaze envelopes my wondering mind
and then she recruits and takes me to this place
I've been there before and that's where I saw her face

In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
Climbing a thousand unclimbed trees
I don't know I want to know how it would feel
to be on top of one of these!
&lt;/lyrics&gt;
&lt;/song&gt;

Story: 
```

**Response**

The response looks like the following:

```text
# Script for A Hundred Dead and Loving Souls

## Scene 1: The Sand
- A close-up shot of a grain of sand under a microscope, showing its intricate structure and texture.
- The camera zooms out to reveal that the sand is part of a vast desert landscape, with dunes and rocks stretching to the horizon.
- A voice-over of the protagonist (a young man) narrates the first verse of the song, expressing his feelings of being trapped and forgotten under the weight of the world.
- The camera pans to show a car driving on a dusty road, with the protagonist and his father inside. They are both wearing sunglasses and hats to protect themselves from the sun.
- The car is filled with boxes and bags, indicating that they are moving to a new place.
- The father is driving and listening to the radio, while the protagonist is looking out the window, bored and restless.
- The radio plays a news report about a nuclear fusion experiment in South Korea, followed by a song by a popular band.
- The father comments on the news, saying that it is a breakthrough for humanity and that he hopes to see it in his lifetime.
- The protagonist does not respond, but thinks to himself that he does not care about anything anymore.

## Scene 2: The Sea
- A montage of shots showing the protagonist and his father arriving at their new home, a small town by the sea.
- The father is excited and optimistic, while the protagonist is indifferent and detached.
- The father unpacks their belongings and sets up their new house, while the protagonist wanders around the town, exploring the beach, the pier, and the streets.
- The protagonist sees a group of teenagers hanging out at a skate park, laughing and having fun. He feels out of place and lonely.
- The protagonist walks to the edge of the pier, where he sees a sign that says "No diving".
- He looks down at the water, where he sees hundreds of dead fish floating on the surface, victims of a recent oil spill.
- He hears the chorus of the song in his head, imagining himself diving into the sea of a hundred dead and loving souls, and climbing a thousand unclimbed trees.
- He wonders what it would feel like to be free and alive, instead of being stuck and dead inside.

## Scene 3: The Root
- A flashback scene, showing the protagonist as a child, playing in a garden with his father.
- The father is a botanist, and he teaches his son about different plants and flowers, showing him how to care for them and appreciate their beauty.
- The son is curious and eager, and he asks his father many questions about the natural world.
- The father tells his son that he is a root, quickly ascending up through the surrounding dirt, and that he will grow into a strong and beautiful tree one day.
- The son smiles and hugs his father, saying that he loves him and that he wants to be like him.
- The father smiles and hugs his son back, saying that he loves him too and that he is proud of him.

## Scene 4: The Sun
- A flash-forward scene, showing the protagonist as a young adult, working in a laboratory with his father.
- The father is a nuclear physicist, and he is working on a project to harness the power of nuclear fusion, the same process that fuels the sun.
- The son is his assistant, and he helps his father with the experiments and calculations, following his instructions and commands.
- The father tells his son that he is a hungry sun, quickly devoured by a hungry son, and that he will make a great contribution to science and humanity one day.
- The son nods and agrees, but he does not look happy or satisfied. He feels pressured and obligated to follow his father's footsteps, even though he does not share his passion or vision.
- The son thinks to himself that he is hungry, like the bellowing sky, and that he wants to stay high, away from the reality and expectations that weigh him down.

## Scene 5: The Crowd
- A present-day scene, showing the protagonist at a party, surrounded by a crowd of people.
- The party is hosted by one of the teenagers that he saw at the skate park, who invited him out of pity and curiosity.
- The party is loud and chaotic, with music, drinks, and drugs everywhere.
- The protagonist feels uncomfortable and anxious, and he does not fit in with the crowd. He does not know anyone, and no one knows him.
- He tries to make small talk with some of the guests, but they are either rude, bored, or high, and they do not pay attention to him.
- He feels invisible and insignificant, and he wishes he could disappear.
- He takes a look around him, as he speaks, and he imagines a disinterested crowd, talking aloud, and he thinks of what he wants most.

## Scene 6: The Gaze
- A close-up shot of a pair of eyes, staring intently at the protagonist.
- The eyes belong to a girl, who is also at the party, but who is different from the rest of the crowd. She is quiet and mysterious, and she has a captivating and mysterious aura.
- The girl notices the protagonist, and she is intrigued by him. She sees something in him that no one else sees, something that she relates to and understands.
- She walks up to him, and she introduces herself. She says her name is Luna, and that she likes his song. She says she heard him humming it to himself, and that she knows the lyrics by heart.
- The protagonist is surprised and confused, and he asks her how she knows his song. He says he wrote it himself, and that he never shared it with anyone.
- The girl smiles and says that she knows his song, because she wrote it too. She says she has a gift, or a curse, depending on how you look at it. She says she can hear the songs that people sing in their souls, and that she can sing them back to them.
- The protagonist is shocked and amazed, and he asks her to prove it. He asks her to sing his song, or any song that he knows.
- The girl nods and agrees, and she sings his song, word for word, note for note, perfectly. She sings it with emotion and expression, as if she wrote it herself.
- The protagonist is speechless and mesmerized, and he feels a connection with the girl that he never felt with anyone before. He feels like she knows him, and he wants to know her.
- The girl stops singing, and she looks at him with a fixated gaze, that envelopes his wondering mind. She says she likes him, and she asks him if he wants to go with her to a place that she knows.
- The protagonist nods and agrees, and he follows her out of the party, leaving the crowd behind.

## Scene 7: The Place
- A montage of shots showing the protagonist and the girl driving to a secluded spot in the woods, where they park their car and get out.
- The spot is a clearing, where there is a large tree, with a rope swing hanging from one of its branches.
- The tree is the only one of its kind in the area, and it stands out from the rest of the forest. It is tall and majestic, with green leaves and red flowers.
- The girl tells the protagonist that this is her place, and that she comes here often to escape from the world. She says she loves this tree, and that she calls it Luna, after herself.
- The protagonist tells the girl that this is his place too, and that he has been here before. He says he saw this tree in his dreams, and that he always wanted to climb it.
- The girl smiles and says that they have more in common than they thought, and that they are meant to be together. She says she wants to show him something, and she asks him to follow her.
- The girl climbs the tree, using the rope swing as a support. She is agile and graceful, and she reaches the top in no time.
- The protagonist follows her, using the rope swing as well. He is clumsy and nervous, and he struggles to keep up with her.
- The girl reaches the top of the tree, where she finds a small platform, made of wood and rope. She sits on the platform, and she looks at the view.
- The view is breathtaking, and it shows the sea, the town, and the sky, all in one panorama. The sun is setting, and it casts a golden glow on everything.
- The girl waves at the protagonist, and she invites him to join her. She says this is the best place to watch the sunset, and that she wants to share it with him.
- The protagonist reaches the top of the tree, where he joins the girl on the platform. He sits next to her, and he looks at the view. He is awestruck, and he agrees that this is the best place to watch the sunset.
- The girl leans in and kisses the protagonist, and he kisses her back. They embrace each other, and they watch the sunset together.
- The chorus of the song plays in the background, as the scene fades to black.
```

Not too bad for a first pass. From here you might want to continue tweaking the different scenes but at least you have a starting point. 

### Generate prompts for each scene

Using the text-to-image technique, I first need to generate prompts to guide the model in the video generation process. In the case of this sample, you could take each of the bullet points in the scenes and generate a video for each of them. 

At the time of this writing, I didn't have access to a video generation model and I don't think VideoPoet is open to the public yet. Therefore, I had to slightly modify what I was generating. Instead of generating a video, I used Copilot to generate images using DALL-E. The concept is the same, except instead of generating videos, I'm generating still images. 

I could've applied the same concept of one image per bullet point. However, for brevity, I chose to summarize all of the bullet points in a scene into a single prompt. 

**Prompt**

Given the following prompt:

```text
Using storyboarding techniques, for each scene, generate a single prompt to use as input for DALL-E to generate an image. Since there are 7 scenes, there should be only 7 prompts. Distill as much of the concepts from each of the bullet points in each scene.
```

**Response**

The results from that prompt are the following:

```text
Scene 1: The Sand
    A grain of sand under a microscope and a car driving on a dusty road in a desert
Scene 2: The Sea
    A boy standing on a pier looking at a sea of dead fish and a sign that says “No diving”
Scene 3: The Root
    A father and a son playing in a garden with different plants and flowers
Scene 4: The Sun
    A father and a son working in a laboratory with a nuclear fusion reactor
Scene 5: The Crowd
    A boy at a party surrounded by a crowd of people with music, drinks, and drugs
Scene 6: The Gaze
    A girl with a pair of eyes that can hear the songs that people sing in their souls
Scene 7: The Place
    A boy and a girl sitting on a platform on top of a tree with a rope swing and a view of the sea, the town, and the sunset
```

Again, not too bad for a first pass. The main points of the scene are captured. I can already foresee there are a few scenes like "The Sand" which might be difficult to illustrate in a single image. 

### Generate images (video clips)

Now that we have individual prompts, it's time to generate some images. Again, I'm doing this because I don't have access to a video generation model but the process would be generally the same. 

**Prompt**

```text
Use the prompts for each of the scenes to generate images with DALL-E. Render them as pencil art.
```

**Response**

For brevity, I'll only include some of the images generated:

"The Sand"

![AI genreated image of a grain of sand under a microscope and a car driving on a dusty road in a desert](http://cdn.lqdev.tech/files/images/sand-car-dirt-road.jpg)

"The Sea"

![AI generated image of a boy standing on a pier looking at a sea of dead fish and a sign that says “No diving”](http://cdn.lqdev.tech/files/images/child-dock.jpg)

"The Root"

![AI generated image of a father and a son playing in a garden with different plants and flowers”](http://cdn.lqdev.tech/files/images/father-son-playing.jpg)

The pencil art part was something I thought would look nice to illustrate this instead of digital art or photorealism. That was a personal choice though. 

As we can see from the images, in general they're not too bad. When not condensed into a single image though and you have an image or video per frame in a scene, the generated content can be richer. 

### Edit and stitch videos

For the image generation example, this seems like a natural stopping point. However, with videos, you can imagine using AI to edit these videos. The VideoPoet post mentions how you might be able to do that. Part of that editing might also include lengthening the video clips. Once you're happy with your videos, you would then stitch them together to compose a larger clip. 

## Improvements

Tons! I used the simplest tools available at hand to validate whether my idea is feasible.  

### Videos

The most obvious improvement here would be to actually generate the videos. Whether that's using VideoPoet or another video generation model, the whole point is to end up with a video clip to upload to Spotify.

### Prompts

The prompts I used were not complex at all. I tried to keep them as simple as possible and in many cases, I wasn't the one providing the prompts, Copilot generated them for me. I would try to use more complex prompts to further guide the model in the formats I expect the outputs to be in and give more details for the model to generate better results. 

### Code

All of this was done using Copilot in the browser. As a starting point to validate my ideas this was fine. However, this approach is not composable, testable, reproducible, or scalable. I'd convert these steps to code in order to get all of the things Copilot doesn't give me. 

### Timestamp sync

One of the features Spotify offers is lyrics that are synced to the song. Similarly, being able to sync these generated clips to the song would be important so the visuals match the 

## Challenges

### Consistency

One challenge that someone brought up when I was explaining this idea was, how do you ensure that the visuals are consistent throughout all of the video clips.

That is a great question I don't have a great answer for, though I have ideas. 

If there aren't many changes throughout scenes, you could just opt to keep lengthening the video clip. 

If there are flashbacks or different visuals between scenes, you might achieve consistency by  using the image-to-video modality. Using a snapshot from a frame in a previous video clip, you might try to edit that snapshot to use in between transitions and then lengthen the video to produce the next scene. 

### Power Consumption

As fascinating as this is, is it worth the energy and cooling demands it requires when using large models? Maybe not in its current form, but maybe it makes sense when using smaller models or more power efficient compute. 

### People

For music videos, there's a human element to it both from the performers and the people who produce them. Again, the Generative AI is impressive and for someone like me without any artistic skills, it can be an empowering tool. Although I'm tempted to see what an [AI Rickroll](https://www.youtube.com/watch?v=dQw4w9WgXcQ) might look like, I prefer the human version. :slightly_smiling_face:

## Conclusion

In this post I discussed how you might use Generative AI models like VideoPoet to generate short videos for Spotify Clips. Since I don't have acess to VideoPoet or other video generation models at this time, I decided to show practical image generation examples instead using models like DALL-E to illustrate the general concepts. It's not wihtout its challenges and there's a ton of improvements I can think of but it was a fun experiment regardless. What else would you use these Generative AI models for? 

You can find an addendum to this post [here](/notes/genai-spotify-clips-instrumental).]]&gt;</description>
      <link>https://www.lqdev.me/posts/generative-ai-spotify-clips</link>
      <guid>https://www.lqdev.me/posts/generative-ai-spotify-clips</guid>
      <pubDate>2023-12-23 18:30 -05:00</pubDate>
      <category>ai</category>
      <category>video</category>
      <category>chicanobatman</category>
      <category>generativeai</category>
      <category>genai</category>
      <category>spotify</category>
      <category>google</category>
      <category>microsoft</category>
      <category>bing</category>
      <category>copilot</category>
      <category>dalle</category>
      <category>image</category>
      <category>prompt</category>
      <category>projectidea</category>
      <category>music</category>
      <category>podcasts</category>
      <category>llm</category>
    </item>
    <item>
      <title>Phi-2 now on HuggingFace</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;When &lt;a href="https://www.lqdev.me/responses/microsoft-phi-2"&gt;Phi-2 initially released&lt;/a&gt;, it was on the Azure AI Studio Model Catalog. It's nice to see it's now in HuggingFace as well.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/phi-2-huggingface</link>
      <guid>https://www.lqdev.me/responses/phi-2-huggingface</guid>
      <pubDate>2023-12-20 18:37 -05:00</pubDate>
      <category>opensource</category>
      <category>ai</category>
      <category>phi</category>
      <category>languagemodel</category>
      <category>smalllanguagemodel</category>
      <category>huggingface</category>
      <category>transformers</category>
      <category>microsoft</category>
    </item>
    <item>
      <title>Microsoft Forms Test Embedding</title>
      <description>&lt;![CDATA[&lt;p&gt;Testing whether my site can handle embedding &lt;a href="https://forms.office.com/"&gt;Microsoft Forms&lt;/a&gt;. Useful for polls across a variety of channels.&lt;/p&gt;
&lt;iframe width="640px" height="480px" src="https://forms.office.com/Pages/ResponsePage.aspx?id=DQSIkWdsW0yxEjajBLZtrQAAAAAAAAAAAAMAANHhN2pUQTZMR1VCUUxDUDNGSFUwWTZLWkdDQklVSC4u&amp;embed=true" frameborder="0" marginwidth="0" marginheight="0" style="border: none; max-width:100%; max-height:100vh" allowfullscreen webkitallowfullscreen mozallowfullscreen msallowfullscreen&gt; &lt;/iframe&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/ms-forms-embed-test</link>
      <guid>https://www.lqdev.me/notes/ms-forms-embed-test</guid>
      <pubDate>2023-01-30 11:46 -05:00</pubDate>
      <category>forms</category>
      <category>microsoft</category>
      <category>polls</category>
      <category>web</category>
      <category>indieweb</category>
    </item>
    <item>
      <title>Blogging on the Surface Duo with github.dev</title>
      <description>&lt;![CDATA[&lt;p&gt;Lately I've been using github.dev to author posts and make light edits on this site. One use case where it's come extremely handy has been authoring content, specifically these microblog-like posts from my Surface Duo. Instead of opening the laptop, I can open up my Surface Duo and visit my site's repo through github.dev. When I orient the device in landscape mode I have one screen display the VS Code-like web environment while the other screen displays the keyboard. Snippets, version control, extensions, and themes all carry over making the experience consistent. Committing changes through the version control utilities immediately kicks off the GitHub Action to build and publish the site with the latest changes. Pairing the GitHub online editor environment with the Duo's dual-screen form factor showcases the productivity and convenience both of these products provide.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/139595618-a2f905dd-e3a0-4f34-8702-2350f548d8c7.png" class="img-fluid" alt="Microblogging on Surface Duo in github.dev" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/surface-duo-blogging-github-dev</link>
      <guid>https://www.lqdev.me/notes/surface-duo-blogging-github-dev</guid>
      <pubDate>10/31/2021 13:51 -05:00</pubDate>
      <category>surface</category>
      <category>duo</category>
      <category>blogging</category>
      <category>github</category>
      <category>surfaceduo</category>
      <category>microsoft</category>
    </item>
    <item>
      <title>Windows 11 upgrade</title>
      <description>&lt;![CDATA[&lt;p&gt;No Windows 11 upgrade for me. 😥&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/135946651-d4d9acee-5618-4846-85aa-8387077a4785.png" class="img-fluid" alt="PC doesn't meet windows 11 upgrade requirements message" /&gt;&lt;/p&gt;
&lt;p&gt;Just because I can't doesn't mean you can't though. Go and check if you're eligible at Windows Update&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Start &amp;gt; Settings &amp;gt; Update &amp;amp; Security &amp;gt; Windows Update&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://support.microsoft.com/windows/manage-updates-in-windows-643e9ea7-3cf6-7da6-a25c-95d4f7f099fe#WindowsVersion=Windows_10"&gt;Manage updates in Windows&lt;/a&gt; for more details.&lt;/p&gt;
&lt;p&gt;For alternative install options, check out the &lt;a href="https://www.microsoft.com/software-download/windows11"&gt;Download Windows 11&lt;/a&gt; page.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/windows-11-upgrade</link>
      <guid>https://www.lqdev.me/notes/windows-11-upgrade</guid>
      <pubDate>10/05/2021 08:55 AM -05:00</pubDate>
      <category>windows10</category>
      <category>windows</category>
      <category>windows11</category>
      <category>microsoft</category>
    </item>
  </channel>
</rss>