<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - interpretability</title>
    <link>https://www.lqdev.me/tags/interpretability</link>
    <description>All content tagged with 'interpretability' by Luis Quintanilla</description>
    <lastBuildDate>2024-06-11 21:03 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Mapping the Mind of a Large Language Model</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we report a significant advance in understanding the inner workings of AI models. We have identified how millions of concepts are represented inside Claude Sonnet, one of our deployed large language models. This is the first ever detailed look inside a modern, production-grade large language model. This interpretability discovery could, in future, help us make AI models safer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Previously, we made some progress matching patterns of neuron activations, called features, to human-interpretable concepts. We used a technique called &amp;quot;dictionary learning&amp;quot;, borrowed from classical machine learning, which isolates patterns of neuron activations that recur across many different contexts. In turn, any internal state of the model can be represented in terms of a few active features instead of many active neurons. Just as every English word in a dictionary is made by combining letters, and every sentence is made by combining words, every feature in an AI model is made by combining neurons, and every internal state is made by combining features.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In October 2023, we reported success applying dictionary learning to a very small &amp;quot;toy&amp;quot; language model and found coherent features corresponding to concepts like uppercase text, DNA sequences, surnames in citations, nouns in mathematics, or function arguments in Python code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We used the same scaling law philosophy that predicts the performance of larger models from smaller ones to tune our methods at an affordable scale before launching on Sonnet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We successfully extracted millions of features from the middle layer of Claude 3.0 Sonnet, (a member of our current, state-of-the-art model family, currently available on claude.ai), providing a rough conceptual map of its internal states halfway through its computation. This is the first ever detailed look inside a modern, production-grade large language model.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/mapping-mind-large-language-model-anthropic</link>
      <guid>https://www.lqdev.me/responses/mapping-mind-large-language-model-anthropic</guid>
      <pubDate>2024-06-11 21:03 -05:00</pubDate>
      <category>ai</category>
      <category>interpretability</category>
      <category>anthropic</category>
      <category>llm</category>
    </item>
    <item>
      <title>Demystifying Embedding Spaces using Large Language Models</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing Large Language Models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decoding user preferences in recommender systems. Our work couples the immense information potential of embeddings with the interpretative power of LLMs.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/demistifying-embedding-spaces-llms</link>
      <guid>https://www.lqdev.me/responses/demistifying-embedding-spaces-llms</guid>
      <pubDate>2024-03-19 21:39 -05:00</pubDate>
      <category>ai</category>
      <category>llm</category>
      <category>embedding</category>
      <category>interpretability</category>
    </item>
    <item>
      <title>OpenAI Microscope</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We’re introducing OpenAI Microscope, a collection of visualizations of every significant layer and neuron of eight vision “model organisms” which are often studied in interpretability. Microscope makes it easier to analyze the features that form inside these neural networks, and we hope it will help the research community as we move towards understanding these complicated systems.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/openai-microscope</link>
      <guid>https://www.lqdev.me/bookmarks/openai-microscope</guid>
      <pubDate>2024-01-23 22:15 -05:00</pubDate>
      <category>openai</category>
      <category>interpretability</category>
      <category>ai</category>
      <category>generativeai</category>
      <category>visualization</category>
    </item>
    <item>
      <title>The Geometry of Truth: Dataexplorer</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;This page contains interactive charts for exploring how large language models represent truth. It accompanies the paper &lt;a href="https://arxiv.org/abs/2310.06824"&gt;The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets&lt;/a&gt; by Samuel Marks and Max Tegmark.&lt;/p&gt;
&lt;p&gt;To produce these visualizations, we first extract &lt;a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/"&gt;LLaMA-13B&lt;/a&gt; representations of factual statements. These representations live in a 5120-dimensional space, far too high-dimensional for us to picture, so we use &lt;a href="https://en.wikipedia.org/wiki/Principal_component_analysis"&gt;PCA&lt;/a&gt; to select the two directions of greatest variation for the data. This allows us to produce 2-dimensional pictures of 5120-dimensional data. See this footnote for more details.&lt;a href="https://saprmarks.github.io/geometry-of-truth/dataexplorer/#fn:1"&gt;1&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/geometry-of-truth-data-explorer</link>
      <guid>https://www.lqdev.me/bookmarks/geometry-of-truth-data-explorer</guid>
      <pubDate>2023-12-11 20:19 -05:00</pubDate>
      <category>ai</category>
      <category>interpretability</category>
      <category>llm</category>
    </item>
  </channel>
</rss>