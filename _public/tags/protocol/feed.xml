<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - protocol</title>
    <link>https://www.lqdev.me/tags/protocol</link>
    <description>All content tagged with 'protocol' by Luis Quintanilla</description>
    <lastBuildDate>2025-06-14 10:47 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>MLS over ActivityPub Draft</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;a href="https://messaginglayersecurity.rocks/"&gt;Messaging Layer Security (MLS)&lt;/a&gt; is an IETF standard for end-to-end encrypted (E2EE) messaging. It lets people on laptops and phones communicate with each other in a secure way that no one in between can see.&lt;br /&gt;
&lt;br&gt;
MLS is designed to use pluggable lower-level protocols. This specification defines an envelope format for distributing MLS messages through the network, and an Activity Streams 2.0 profile for the packets of application data stored inside the messages.&lt;br /&gt;
&lt;br&gt;
This specification is ready for review from both ActivityPub developers and security analysts. It’s time to start making proof-of-concept implementations and testing interoperability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Source: https://socialwebfoundation.org/2025/06/13/mls-over-activitypub-draft/&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/mls-over-activitypub-draft</link>
      <guid>https://www.lqdev.me/responses/mls-over-activitypub-draft</guid>
      <pubDate>2025-06-14 10:47 -05:00</pubDate>
      <category>protocol</category>
      <category>socialweb</category>
      <category>activitypub</category>
      <category>w3c</category>
    </item>
    <item>
      <title>Agent Network Protocol - The HTTP of the Agentic Web Era</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;&lt;a href="https://agent-network-protocol.com/blogs/posts/mcp-anp-comparison.html"&gt;Comparison of MCP and ANP: What Kind of Communication Protocol Do Agents Need?&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;MCP and ANP differ significantly in protocol architecture, identity authentication, and information organization.&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MCP is a typical Client-Server (CS) architecture, while &lt;strong&gt;ANP is a typical Peer-to-Peer (P2P) architecture&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;MCP's identity authentication is based on the OAuth standard, facilitating client access to current internet resources. &lt;strong&gt;ANP's identity authentication is based on the W3C DID standard&lt;/strong&gt;, focusing on cross-platform interoperability among agents, enabling seamless connectivity.&lt;/li&gt;
&lt;li&gt;MCP organizes information using JSON-RPC technology, essentially API calls. &lt;strong&gt;ANP uses semantic web Linked-Data technology&lt;/strong&gt; to build a data network that is easily accessible and understandable by AI.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;ANP is agent-centric, where each agent has equal status, forming a decentralized agent collaboration network.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/agent-network-protocol</link>
      <guid>https://www.lqdev.me/responses/agent-network-protocol</guid>
      <pubDate>2025-05-17 10:56 -05:00</pubDate>
      <category>agent</category>
      <category>ai</category>
      <category>protocol</category>
    </item>
    <item>
      <title>Vibe-Specing - From concepts to specification</title>
      <description>&lt;![CDATA[
## Introduction

Code generation is a common use case for AI. What about the design process that comes before implementation? Personally, I've found that AI excels not just at coding, but also helping formalize abstract ideas into concrete specifications. This post explores how I used AI-assisted design to transform a collection of loosely related concepts into a technical specification for a new system made up of those concepts.

## Specifications

Generally, I've had mixed success with vibe-coding (the practice of describing what you want in natural language and having AI generate the corresponding code). However, it's something that I'm constantly working on getting better at. Also, with tooling integrations like MCP, I can ground responses and supplement my prompts using external data.

What I find myself being more successful with is using AI to explore ideas and then formalizing those ideas into a specification. Even in the case of vibe-coding, what you're doing with your prompts is building a specification in real-time. 

I'd like to think that eventually I'll get to the vibe-coding part but before diving straight into the code, I'd like to spend time in the design phase. Personally, this is also the part that I find the most fun because you can throw wild things at the wall. It's not until you implement them that you actually validate whether some of those wild ideas are practical. But I find the design phase a ton of fun.

The result of my latest vibe-specing adventure is what I'm calling the [InterPlanetary Knowledge System (IPKS)](https://github.com/lqdev/IPKS).

## From concept to specification

Lately, I've been thinking a lot about knowledge. Some concepts that have been in my head are those of **non-linear publishing** (creating content that can be accessed in any order with multiple entry points, like wikis or hypertext) and **distributed cognition** (the idea that human knowledge and cognitive processes extend beyond the individual mind to include interactions with other people, tools, and environments).
Related to those concepts, I've also been thinking about how **digital gardens** (personal knowledge bases that blend note-taking, blogging, and knowledge management in a non-linear format) and **Zettelkasten** (a method of note-taking where ideas are captured as atomic notes with unique identifiers and explicit connections) are ways to capture and organize knowledge.

One other thing that I'm amazed by is the powerful concept of a **hyperlink** and how it makes the web open, decentralized, and interoperable. When paired with the **semantic web** (an extension of the web that provides a common framework for data to be shared across applications and enterprises), you have yourself a decentralized knowledgebase containing a lot of the world's knowledge.

At some point, IPFS (InterPlanetary File System, a protocol designed to create a permanent and decentralized method of storing and sharing files) joined this pool of concepts I had in my head.

These were all interesting concepts individually, but I knew there were connections but couldn't cohesively bring them together. That's where AI-assisted specification design came in.

Below is a summary of the collaborative design interaction with Claude Sonnet 3.7 (with web search) that eventually led to the generation of the IPKS specifications. I haven't combed through them in great detail, but what they're proposing seems plausible. 

Overall, I'm fascinated by this interaction. Whether or not IPKS ever becomes a reality, the process of using AI to transform abstract concepts into concrete specifications seems like a valuable and fun design approach that I'll continue to refine and include as part of my vibe-coding sessions. 

---

## Initial Exploration: IPFS and Knowledge Management

Our conversation began with exploring IPFS (InterPlanetary File System) and its fundamental capabilities as a content-addressed, distributed file system. We recognized that while IPFS excels at storing and retrieving files in a decentralized manner, it needed extensions to support knowledge representation, trust, and semantics.

Key insights from this stage:
- IPFS provides an excellent foundation with content addressing through CIDs
- Content addressing enables verification but doesn't inherently provide meaning
- Moving from document-centric to idea-centric systems requires additional layers

## Knowledge Management Concepts

We explored established knowledge management approaches, particularly:

### Zettelkasten
The Zettelkasten method contributed these important principles:
- Atomic units of knowledge (one idea per note)
- Explicit connections between ideas
- Unique identifiers for each knowledge unit
- Emergent structure through relationship networks

### Digital Gardens
The Digital Garden concept provided these insights:
- Knowledge in various stages of development
- Non-linear organization prioritizing connections
- Evolution of ideas over time
- Public visibility of work-in-progress thinking

These personal knowledge management approaches helped us envision how similar principles could work at scale in a distributed system.

## The "K" in IPKS

When we proposed replacing "IPFS" with "IPKS" (changing File → Knowledge), we recognized the need to define what makes knowledge different from files. This led to identifying several key requirements:

1. **Semantic meaning** - Knowledge needs explicit relationships and context
2. **Provenance and trust** - Knowledge requires verifiable sources and expertise
3. **Evolution** - Knowledge changes over time while maintaining continuity
4. **Governance** - Knowledge exists in various trust and privacy contexts

These requirements shaped the layered architecture of the specifications.

## Distributed Cognition and Non-Linear Publishing

Our discussions about distributed cognition highlighted how thinking processes extend beyond individual minds to include:
- Interactions with other people
- Cultural artifacts and tools
- Physical and digital environments
- Social and technological systems

This concept directly influenced the IPKS design by emphasizing:
- Knowledge as a collective, distributed resource
- The need for attribution and expertise verification
- The value of connecting knowledge across boundaries
- The role of tools in extending human cognition

Similarly, non-linear publishing concepts shaped how we approached knowledge relationships and navigation in IPKS, moving away from sequential formats toward interconnected networks of information.

## Web3 Technologies Integration

Our exploration of complementary technologies led to incorporating:

### Decentralized Identifiers (DIDs)
DIDs provided the framework for:
- Self-sovereign identity for knowledge contributors
- Cryptographic verification of authorship
- Persistent identification across systems
- Privacy-preserving selective disclosure

### Verifiable Credentials (VCs)
Verifiable Credentials offered mechanisms for:
- Expertise validation without central authorities
- Domain-specific qualification verification
- Credential-based access control
- Trust frameworks for knowledge contributors

### Semantic Web (RDF/OWL)
Semantic Web standards influenced:
- Relationship types between knowledge nodes
- Ontologies for domain knowledge representation
- Query patterns for knowledge discovery
- Interoperability with existing knowledge systems

## Business Context: Supply Chain Example

Our conversation about supply chain management provided a concrete use case that helped ground the specifications in practical application. This example demonstrated how IPKS could address real-world challenges:

- **Material Provenance**: Using DIDs and verifiable credentials to establish trusted material sources
- **Cross-Organization Collaboration**: Enabling knowledge sharing while respecting organizational boundaries
- **Regulatory Compliance**: Creating verifiable documentation of compliance requirements
- **Expertise Validation**: Ensuring contributors have appropriate qualifications for their roles
- **Selective Disclosure**: Balancing transparency with competitive confidentiality

This business context helped shape the Access Control &amp; Privacy specification in particular, highlighting the need for nuanced governance models.

## Technical Implementation Considerations

As we moved from abstract concepts to specifications, several technical considerations emerged:

1. **Building on IPLD**: Recognizing that InterPlanetary Linked Data (IPLD) already provided foundational components for structured, linked data in content-addressed systems

2. **Modular Specification Design**: Choosing to create multiple specifications rather than a monolithic standard to enable incremental implementation and adoption

3. **Backward Compatibility**: Ensuring IPKS could work with existing IPFS/IPLD infrastructure

4. **Extensibility**: Designing for future enhancements like AI integration, advanced semantic capabilities, and cross-domain knowledge mapping

## The Path Forward

The IPKS specifications represent a synthesis of our conceptual exploration, grounded in:
- Established knowledge management practices
- Decentralized web technologies
- Real-world business requirements
- Technical feasibility considerations

Moving from concept to implementation will require:
1. Reference implementations of the core specifications
2. Developer tools and libraries to simplify adoption
3. Domain-specific extensions for particular use cases
4. Community building around open standards

By building on the combined strengths of IPFS, DIDs, VCs, and semantic web technologies, IPKS creates a framework for distributed knowledge that balances openness with trust, flexibility with verification, and collaboration with governance.
]]&gt;</description>
      <link>https://www.lqdev.me/posts/vibe-specing-prompt-to-spec</link>
      <guid>https://www.lqdev.me/posts/vibe-specing-prompt-to-spec</guid>
      <pubDate>2025-05-06 20:40 -05:00</pubDate>
      <category>dweb</category>
      <category>standards</category>
      <category>ai</category>
      <category>decentralization</category>
      <category>protocol</category>
    </item>
    <item>
      <title>Willow Protocol Specifications</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;A protocol for peer-to-peer data stores. The best parts? Fine-grained permissions, a keen approach to privacy, destructive edits, and a dainty bandwidth and memory footprint.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/willow-protocol</link>
      <guid>https://www.lqdev.me/bookmarks/willow-protocol</guid>
      <pubDate>2024-01-17 20:38 -05:00</pubDate>
      <category>willow</category>
      <category>protocol</category>
      <category>p2p</category>
      <category>privacy</category>
      <category>data</category>
    </item>
    <item>
      <title>Cool URIs don't change</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Keeping URIs so that they will still be around in 2, 20 or 200 or even 2000 years is clearly not as simple as it sounds. However, all over the Web, webmasters are making decisions which will make it really difficult for themselves in the future. Often, this is because they are using tools whose task is seen as to present the best site in the moment, and no one has evaluated what will happen to the links when things change. The message here is, however, that many, many things can change and your URIs can and should stay the same. They only can if you think about how you design them.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/cool-uris-dont-change</link>
      <guid>https://www.lqdev.me/bookmarks/cool-uris-dont-change</guid>
      <pubDate>2024-01-13 10:23 -05:00</pubDate>
      <category>url</category>
      <category>http</category>
      <category>standard</category>
      <category>protocol</category>
      <category>web</category>
      <category>internet</category>
    </item>
    <item>
      <title>Accept Webmentions using F#, Azure Functions, and RSS</title>
      <description>&lt;![CDATA[
# Introduction 

&gt; This post is part of [F# Advent 2022](https://sergeytihon.com/2022/10/28/f-advent-calendar-in-english-2022/). Thanks to [Sergey Tihon](https://sergeytihon.com) for organizing it. 

While not planned this way, this post is timely considering recent developments in the social media space. While Twitter is in the headlines, it's not the only time people have questioned their engagement and relationship with these social media platforms. In the context of Twitter though, users have been looking for alternative platforms to best serve their communities. While Mastodon and the Fediverse appear to be one of those alternatives, they're not the only one. Blogosphere platforms like Tumblr and WordPress, both owned by Automattic, are also positioning themselves as viable alternatives to the point they've [considered the idea of supporting ActivityPub](https://techcrunch.com/2022/11/21/tumblr-to-add-support-for-activitypub-the-social-protocol-powering-mastodon-and-other-apps/). ActivityPub is the protocol that powers many of the platforms on the Fediverse. However, it's not the only open protocol that enables communication and engagement on the open web. Webmentions is another protocol for enabling conversations on the web which I'll talk about more in this post. 

I'll also discuss how I used F#, Azure Functions, and RSS to implement a solution that helps me and my website engage in conversations on the open web thanks to Webmentions. 

## What are Webmentions?

First, let's start by talking a bit about Webmentions. 

According to the W3C specification, Webmentions are "...a simple way to notify any URL when you mention it on your site. From the receiver's perspective, it's a way to request notifications when other sites mention it.". 

One of the goals according to the IndieWeb wiki is to enable cross-site conversations.  

Here's how a potential Webmention workflow might look like:

```mermaid
sequenceDiagram
  autonumber
  actor A as Alice
  participant AS as Alice's "Website"
  participant AW as Alice's Webmention Service
  participant BS as Bob's "Website"
  actor B as Bob
  A -&gt;&gt; AS: Publishes post
  B -&gt;&gt; AS: Reads post
  B -&gt;&gt; BS: Publishes post with link to Alice's post
  rect rgb(230,230,230)
    alt Automated
      BS -&gt;&gt; AS: Fetch Webmention endpoint
      BS -&gt;&gt; AW: Sends Webmention
    else Manual
      B -&gt;&gt; AS: Fetch Webmention endpoint
      B -&gt;&gt; AW: Sends Webmention
    end 
  end
  rect rgb(230,230,230)
      note over AW:Validates Webmention request
      note over AW: Validates Webmention
      note over AW: Accepts / processes Webmention
      alt Automated
        AW -&gt;&gt; BS: Sends back response
      else Manual
        AW -&gt;&gt; B: Sends back response
      end
  end 
  opt Display Webmentions
    AS -&gt;&gt; AW: Fetch Webmentions
    note over AS: Display / publish Webmentions
  end
```

I use the term website here very loosely because it's not only limited to your own personal website. You can technically use Webmentions through any public website including social media platforms like Twitter, Mastodon, or even GitHub.

This post will focus on the middle blocks consisting of sending and receiving Webmentions. For more information, check out the [specification](https://www.w3.org/TR/webmention/#abstract-p-1) and [Webmentions page](https://indieweb.org/Webmention) on the IndieWeb wiki. 

## My webmention solution

In September of this year, I [integrated a partial implementation of Webmentions](/notes/webmentions-partially-implemented/) into my website. This implementation only allowed me to send webmentions as part of my publishing process. If you're interested in the details, it's based on the blog post I wrote for last year's F# Advent, [Sending Webmentions with F#](/posts/sending-webmentions-fsharp-fsadvent/).

A couple of days ago, I [successfully deployed and integrated](/notes/now-accepting-webmentions) the other part of the implementation which allowed me to receive webmentions. While there are many great [services](https://indieweb.org/Webmention#Publisher_Services) which will do this for you, I decided to roll my own for fun and to learn something new. 

To help with that, I released [WebmentionFs](https://www.nuget.org/packages/lqdev.WebmentionFs), an open-source .NET library written in F# with components for sending and receiving Webmentions.

The backend service I use to receive webmentions is built on Azure Functions. 

If you're interested, here's the source code for [WebmentionFs](https://github.com/lqdev/WebmentionFs) and my [Webmention service](https://github.com/lqdev/WebmentionService). 

Before diving into the technical details of the solution, I'll first discuss some of the priorities driving my design decisions. 

### Requirements

While I don't want to trivialize it, at the end of the day, the Webmention service is just an endpoint that accepts HTTP POST requests. That means there's many ways to build it. However, here were the requirements important to me that guided my design choices:

- **Cost:** The solution should be low cost
- **Maintenance:** The solution should be easy to maintain
- **Easy to consume:** I have multiple ways to view and access my webmentions.
- **Notifications first:** Since I don't own or have responsibility over content that's not mine, prioritize Webmentions for notifications, not content.

With that in mind, I ended up choosing a solution built on Azure Functions to receive Webmentions and publish them as an RSS feed. 

### Why Azure Functions?

I don't make money from my website. It's a labor of love in which I document my experiences. Selfishly, it's for me to remind myself how to solve a problem or share something that was important to me at that time. If my posts end up helping others solve their own problems, that's a side effect and a reward unto itself. To that effect, I want my website to be as low-cost to run as possible. One way I achieve that is by using static hosting. This dramatically reduces the cost associated with my site. If you're interested in how this site is built, you can check out the [colophon](/colophon).

So where do Azure Functions come in? 

Azure Functions is a serverless solution which means there are no servers! (I kid :smirk:)

#### Cost :heavy_check_mark:

As a serverless solutions, it means web services are not constantly running. They only run whenever there is something to process so I only pay for what I need. That takes care of my low-cost requirement since processing Webmentions is not a long-running, resource-intensive operation that can easily run in a serverless environment.

#### Maintenance :heavy_check_mark:

Cost is not only monetary. There are also time costs associated with maintaining a solution. Because there is no infrastructure to maintain, that's not really a factor I need to consider. Instead I get to focus on writing the logic for accepting and processing Webmentions. Azure Functions provides a set of [prebuilt trigger and binding components](https://learn.microsoft.com/azure/azure-functions/functions-triggers-bindings) to reduce the amount of code I have to write to handle things like accepting HTTP requests or storing data in a table or blob storage. Also, in the rare instance that this blog post breaks the internet, Azure Functions enables my service to automatically handle that scale. Don't believe me? Check out this post frorm [Troy Hunt](https://www.troyhunt.com/) on [Have I Been Pwned usage of Azure Functions](https://www.troyhunt.com/serverless-to-the-max-doing-big-things-for-small-dollars-with-cloudflare-workers-and-azure-functions/).

### Why RSS?

The choice of how to display and consume Webmentions is entirely up to you. At regular intervals, my Webmention service generates an RSS feed and publishes it to a private Azure Blob Storage. Here are some of the reasons why:

#### Easy to consume :heavy_check_mark:

The decision to use RSS comes from the fact that like Webmentions, it's an open protocol that gives choice to creators and consumers. Check out my post, [Rediscovering the RSS protocol](/posts/rediscovering-rss-user-freedom) for more of my thoughts on that. Despite the appearance of decline in use, it's a protocol that's been around for over 20 years and it's one of the many reasons why podcasts continue to be one of the more open platforms despite some attempts to lock down the ecosystem.  With that it mind, by choosing RSS as the publishing format for my Webmentions feed, I can lean on the vast number of RSS readers ([NewsBlur](http://newsblur.com/) and [Elfeed](https://github.com/skeeto/elfeed) being my favorites) out there instead of having to write a custom UI. Not happy with my RSS reader? I can just add my feed to whichever other reader I move to and continue consuming my Webmentions without skipping a beat. 

### Notifications first :heavy_check_mark:
 
While not technical or specific to RSS, this is a topic I wrestled with as part of my implementation. However, choosing RSS made many parts of it simpler. 

Who owns your content? If you publish on your own website or a platform under your control, chances are you do. If I publish and redistribute your content on my website though, what parts do I own and which parts do you? I'm not sure. What about spam, harrassment, and lewd content? I'd have to put in place some sort of moderation instead of publishing directly on my site. I didn't want to have to deal with any of these questions so choosing to publish to a private RSS feed instead of directly on my site made it so Webmentions are for my eyes only. More importantly, I only capture the source and target links, not the content. Therefore, I don't store or redistribute your content. Now there's nothing stopping anyone from seeing your post since it's public but in cases where your site's content is not something I'm comfortable with, I'm not resharing or redistributing that content. If my RSS reader chooses to display the full contents of your post by using the `link` element from the RSS feed, since your post is public, that's no different than me viewing the original post on your site. If at some point I feel differently about it, the changes to capture the content in the `description` happen at the RSS file level and because the RSS is private, I'm still not redistributing your content. 

In short, with my implementation I chose to prioritize Webmentions for notification purposes rather than building a commenting and interaction (like, reshare) system for my website. 

Now that I've talked about the what and the why, in the next sections I'll get into the how. 

## Sending Webmentions

Once you've created a post linking to someone else's website, sending webmentions is a two-step process:

1. URL discovery of webmention endpoint
2. Send webmention

These steps can be automated or performed manually. 

### URL discovery of Webmention endpoint

The first thing you'll need is the endpoint where you'll be sending your Webmention to.

According to the Webmention specification, a Webmention endpoint can be found in one of three places:

- HTTP response headers
- `&lt;link&gt;` tag with `rel=webmention` attribute
- `&lt;a&gt;` tag with `rel=me` attribute

When this process is automated, the discovery is performed in order and each subsequent option works as a fallback of the other. When done manually, either the website author will publish and advertise this on their website or you can inspect the website's source code to find it.  

### Send webmention

To send a Webmention, you need to send an HTTP POST request with an `x-www-form-urlencoded` body containing two properties: `source` and `target`. Source is the URL of your article and target is the URL of the original article. Again, this too can either be automated or done manually with tools like cURL, Insomnia, or any other HTTP client.  

### Sending Webmentions in practice

Let's use a redundant example to walk through the workflow. Pretend that you created or commented on a GitHub issue and added a link to one of my articles, [Sending Webmentions with F#](/posts/sending-webmentions-fsharp-fsadvent/) for example. You then wanted to notify me either to get my attention on that issue or just as an FYI. Now I say this is a redundant example because GitHub already has notifications. However, what this does illustrate are the cross-site capabilities of Webmentions. 

![Image of GitHub Issue Post](http://cdn.lqdev.tech/files/images/send-gh-issue.png)

At this point, if GitHub implemented Webmentions, it would automatically:

1. Fetch my webpage and look for my Webmention endpoint.
1. Compose and send the HTTP POST request.  

Sadly, today that's not the case so we'll walk through the workflow manually.  

On my website, I include a `link` tag with the `rel=webmention` attribute that looks like the following:

```html
&lt;link rel="webmention" title="Luis Quintanilla Webmention Endpoint" href="https://lqdevwebmentions.azurewebsites.net/api/inbox"&gt;
```

My current Webmention endpoint is *https://lqdevwebmentions.azurewebsites.net/api/inbox*.

Now that you have the Webmention endpoint, you could use any tool of your choice to create and send the HTTP POST request. I have made it simpler though and included a form on my website for each of my posts for this purpose. 

![luisquintanilla.me send Webmention form](http://cdn.lqdev.tech/files/images/send-webmention-form.png)

The source code looks like the following:

```html
&lt;div&gt;
    &lt;script type="application/javascript"&gt;window.onload = function() { document.getElementById('webmention-target').value = window.location.href }&lt;/script&gt;
    &lt;form action="https://lqdevwebmentions.azurewebsites.net/api/inbox" method="POST" enctype="application/x-www-form-urlencoded"&gt;
        &lt;h5 class="text-center"&gt;Send me a &lt;a href="https://indieweb.org/webmentions"&gt;webmention&lt;/a&gt;&lt;/h5&gt;
        &lt;div class="form-row justify-content-center"&gt;
            &lt;div class="w-75"&gt;
                &lt;input type="text" name="source" class="form-control" placeholder="Your URL (source)"&gt;
            &lt;/div&gt;
            &lt;div class="col-auto"&gt;
                &lt;input type="submit" class="btn btn-primary" value="Send"&gt;
            &lt;/div&gt;
            &lt;input readonly="" class="form-control-plaintext" style="visibility:hidden" type="text" id="webmention-target" name="target"&gt;
        &lt;/div&gt;
    &lt;/form&gt;
&lt;/div&gt;
```

Now to send the webmention, navigate to the post you want to send a Webmention to. In this case, it's [Sending Webmentions with F#](/posts/sending-webmentions-fsharp-fsadvent/). Then, paste the link to the GitHub issue that contains the link to my site into the post's text box and hit "Send". 

![luisquintanilla.me send Webmention form filled-in](http://cdn.lqdev.tech/files/images/send-webmention-form-populated.png)

At that point, the request is sent to my Webmention service which receives and processes the request and responds with a message whether the request was successful or not. 

That's all there is to it! Before moving the the receiving process though, I'll briefly show how to send Webmentions using WebmentionFs. 

### Sending Webmentions with WebmentionFs

In WebmentionFs, you can use the [`UrlDiscoveryService`](https://github.com/lqdev/WebmentionFs/blob/main/UrlDiscoveryService.fs) and [`WebmentionSenderService`](https://github.com/lqdev/WebmentionFs/blob/main/WebmentionSenderService.fs) to send Webmentions. If you wanted to integrate sending Webmentions into your application or service, here's an example that shows how you'd use those components.

```fsharp
open System
open WebmentionFs
open WebmentionFs.Services

let ds = new UrlDiscoveryService()

let ws = new WebmentionSenderService(ds)

let data = 
    {   
        Source = new Uri("https://twitter.com/ljquintanilla/status/1603602055435894784")  
        Target = new Uri("/feed/mastodon-hashtag-rss-boffosocko")
    }

ws.SendAsync(data) |&gt; Async.AwaitTask |&gt; Async.RunSynchronously
```

The main part to pay attention to is the `SendAsync` function. This uses the `UrlDiscoveryService` to perform the Webmention endpoint discovery and send the HTTP POST request with the provided data. 

## Receiving webmentions

Now that you know how to send webmentions, let's talk about what happens on the receiver end. At its core, it's about a three-step process:

1. Validate request
1. Validate Webmention
1. Process Webmention

### Validate request

Before processing the Webmention, you want to make sure that the request is valid. To do so there are a few checks you perform:

1. Is the protocol valid? Check that the protocol for the source and target URLs in the request body are HTTP or HTTPS.
1. Are the URLs the same? Check that the source and target URLS in the request body are NOT the same. 
1. Is the target URL a valid resource? This is more loosely defined and up to you. In my case I defined valid as: 
    - I own the domain
    - The target URL doesn't return an HTTP 400 or 500 error. 

If the request is not valid, nothing is done with the Webmention and an error returns. Otherwise, it moves to the next step which is Webmention validation. 

### Validate Webmention

In the base case, Webmention validation just means you parse through the source document and make sure that the target URL is included in it. However, if you wanted to get more complex and take into account specific types of interactions such as likes, replies, reposts, bookmarks, etc. you can parse through the document looking for [microformat](http://microformats.org/wiki/h-entry#Core_Properties) annotations. For more information on these annotations, see the [Types of Posts](https://indieweb.org/posts#Types_of_Posts) or [reply](https://indieweb.org/reply#Post_a_reply) pages in the IndieWeb Wiki. 

If the target link is NOT in the source document, you return an error. Otherwise, you move to the next step of processing the Webmention

### Processing Webmentions

This step is optional and loosely defined. Once you've validated the request and Webmention, you can do with it as you please. One thing you can do is publish it (after moderation) on your website. 

In my case, I'm storing it in an Azure Tables database. Then, every morning, I generate an RSS feed of all my webmentions and save it to a private container on Azure Blob Storage.  

## Reciving Webmentions with WebmentionFs

To receive Webmentions, I use the [`RequestValidationService`](https://github.com/lqdev/WebmentionFs/blob/main/RequestValidationService.fs), [`WebmentionValidationService`](https://github.com/lqdev/WebmentionFs/blob/main/WebmentionValidationService.fs) and [`WebmentionReceiverService`](https://github.com/lqdev/WebmentionFs/blob/main/WebmentionReceiverService.fs) inside my Azure Function. 

The source code for my Azure Function is available in the [WebmentionService GitHub repo](https://github.com/lqdev/WebmentionService).

The main components are in the `Startup.fs` and `ReceiveWebmention.fs` files. 

The [`Startup.fs`](https://github.com/lqdev/WebmentionService/blob/main/Startup.fs) files registers each of the services I'll use to to receive Webmentions. By doing so, I define the initialization logic for use later in my application using dependency injection. The main thing to note here is that the `WebmentionReceiverService` is initialized with instances of a `RequestValidationService` and `WebmentionValidationService`. For the `RequestValidationService`, it's initialized with a list of host names. In this case, I'm passing it in as a comma-delimited environment variable that looks something like "lqdev.me,luisquintanilla.me". This is what the `RequestValidationService` uses to check whether I own a domain.

```fsharp
// Add request validation service
builder.Services.AddScoped&lt;RequestValidationService&gt;(fun _ -&gt; 

    let hostNames = Environment.GetEnvironmentVariable("PERSONAL_WEBSITE_HOSTNAMES")

    let hostNameList = hostNames.Split(',')

    new RequestValidationService(hostNameList)) |&gt; ignore


// Add webmention validation service
builder.Services.AddScoped&lt;WebmentionValidationService&gt;() |&gt; ignore

// Add receiver service
builder.Services.AddScoped&lt;IWebmentionReceiver&lt;Webmention&gt;,WebmentionReceiverService&gt;(fun (s:IServiceProvider) -&gt;
    let requestValidationService = s.GetRequiredService&lt;RequestValidationService&gt;()
    let webmentionValidationService = s.GetRequiredService&lt;WebmentionValidationService&gt;()
    new WebmentionReceiverService(requestValidationService,webmentionValidationService)) |&gt; ignore
```

Then, in the [`ReceiveWebmention.fs`](https://github.com/lqdev/WebmentionService/blob/main/ReceiveWebmention.fs) file, I inject my `WebmentionReceiverService` into the constructor and add all the logic for running my serverless function in the `Run` function. In this snippet, some code is ommitted for brevity.

```fsharp
type ReceiveWebmention (webmentionReceiver: IWebmentionReceiver&lt;Webmention&gt;) = 

    ///...

    [&lt;FunctionName("ReceiveWebmention")&gt;]
    member x.Run 
        ([&lt;HttpTrigger(AuthorizationLevel.Anonymous, "post", Route = "inbox")&gt;] req: HttpRequest) 
        ([&lt;Table("webmentions", Connection="AzureWebJobsStorage")&gt;] t: TableClient)
        (log: ILogger) =
        task {
            
            log.LogInformation("Processing webmention request")

            let! validationResult = x.WebmentionReceiver.ReceiveAsync(req)

            let response = 
                match validationResult with
                | ValidationSuccess m -&gt; 
                    let entity = mapMentionToTableEntity m
                    try
                        t.AddEntity(entity) |&gt; ignore
                        OkObjectResult("Webmention processed successfully") :&gt; IActionResult
                    with
                        | ex -&gt; 
                            log.LogError($"{ex}")
                            BadRequestObjectResult($"Error processing webmention. Webmention already exists") :&gt; IActionResult
                | ValidationError e -&gt; 
                    log.LogError(e)
                    BadRequestObjectResult(e) :&gt; IActionResult

            return response

        }
```

The `Run` function is triggered whenever an HTTP POST request is made to my Webmention endpoint. It then calls `ReceiveAsync` on the `WebmentionReceiverService` which under the hood uses the `RequestValidationService` and `WebmentionValidationService` for validation. For reference, here's the code that does that in WebmentionFS.

```fsharp
member x.ReceiveAsync (data:UrlData) = 
    task {
        let! requestValidationResult = 
            x.RequestValidationService.ValidateAsync data

        match requestValidationResult with
        | RequestSuccess r -&gt; 
            let! webmentionValidationResult = 
                x.WembentionValidationService.ValidateAsync r.Source r.Target

            let (result:ValidationResult&lt;Webmention&gt;) = 
                match webmentionValidationResult with
                | AnnotatedMention m -&gt; 
                    ValidationSuccess {RequestBody = r; Mentions = m}
                | UnannotatedMention -&gt; 
                    ValidationSuccess 
                        {
                            RequestBody = r
                            Mentions = 
                                {
                                    IsBookmark = false
                                    IsLike = false
                                    IsReply = false
                                    IsRepost = false        
                                }
                        }
                | MentionError e -&gt; ValidationError e
            return result
        | RequestError e -&gt; return ValidationError e
    }
```

&gt; A quick side-note. I love how elegant function composition in F# makes the request validation pipeline look. See for yourself :slightly_smiling_face:
&gt; 
&gt; ```fsharp
&gt;let validateAsync = 
&gt;    isProtocolValid &gt;&gt; isSameUrl &gt;&gt; isTargetUrlValidAsync
&gt;```

Inside my `Run` function, if the request and Webmention validation are successful, I use the Azure Table Storage table client `AddEntity` method to save the Webmention to a table. One thing to note is that `AddEntity` ensures that an entry is only added once. If an entry already exists in the table, it returns an error. 

This is what the GitHub Webmention we sent looks like in the table.

![Webmentions table on Azure Table Storage](http://cdn.lqdev.tech/files/images/webmentions-azure-table.png)

In this case, the `PartitionKey` is the target URL and `RowKey` is the source URL. The fact all other columns are set to false indicates this is an untagged mention with no special microformats annotations. Annotating your posts appropriately would set one or more of those columns to true. 

Now that we went over how I receive Webmentions, I'll show how I prepare my Webmentions for consumption. 

## Viewing Webmentions

I consume my Webmentions using RSS feeds and RSS readers. To do so, I have an Azure Function called [`WebmentionToRss`](https://github.com/lqdev/WebmentionService/blob/main/WebmentionToRss.fs) that triggers every day at 3 AM. This function queries the Azure Table Storage table containing my Webmentions and creates an RSS feed that's then stored to Azure Blob Storage. Like the other function to process my Webmentions, I:

Register the `RssService` responsible for generating the RSS feed in `Startup.fs`.

```fsharp
// Add RSS service
builder.Services.AddScoped&lt;RssService&gt;() |&gt; ignore
```

Inject the service into my `WebmentionToRss` constructor. Then in my `Run` function, I call the `getMentions` helper function to query the table, call `BuildRssFeed` to generate the feed, and write it out to Azure Blob Storage using a `Stream`.

```fsharp
type WebmentionToRss (rssService:RssService) =

    let getMentions (t:TableClient) = 

        //...

        let webmentions = t.Query&lt;WebmentionEntity&gt;()

        webmentions


    //...

    [&lt;FunctionName("WebmentionToRss")&gt;]
    member x.Run
        ([&lt;TimerTrigger("0 0 3 * * *")&gt;] info: TimerInfo)
        ([&lt;Table("webmentions",Connection="AzureWebJobsStorage")&gt;] t: TableClient)
        ([&lt;Blob("feeds/webmentions/index.xml", FileAccess.Write, Connection="AzureWebJobsStorage")&gt;] rssBlob: Stream)
        (log: ILogger) =

        task {
            let mentions = getMentions t

            let rss = x.RssService.BuildRssFeed mentions "lqdev's Webmentions" "http://lqdev.me" "lqdev's Webmentions" "en"

            log.LogInformation(rss.ToString())                

            use xmlWriter = XmlWriter.Create(rssBlob)

            rss.WriteTo(xmlWriter) |&gt; ignore
        }
```

Now, to view my feed I add the URL to the RSS file in Azure Blob Storage to my RSS feed reader. The container and blob containing the file are private though, so how does my RSS reader get access to the file? I don't use the direct link. Instead, I use a link containing a [Shared Access Signatures (SAS) token](https://learn.microsoft.com/azure/storage/common/storage-sas-overview#sas-token). 

The RSS feed reader then fetches my Webmentions feed at regular intervals. 

This is what the GitHub post looks like in my RSS reader.

![Webmentions table on Azure Table Storage](http://cdn.lqdev.tech/files/images/newsblur-webmentions-rss-feed.png)

That's all there is to it!

## What's next?

Overall I'm happy with my solution. I learned a few things along the way and got something working for myself. Publishing WebmentionFs as a library and making my Azure Function code open-source were attempts to help others looking to implement similar solutions get started. These solitions could be improved though. These are some updates I would like to make at some point:

- Upgrade my website to use WebmentionFs for sending Webmentions. You can see the [current implementation](https://github.com/lqdev/luisquintanilla.me/blob/main/Services/Webmention.fs) on my website repo which is basically what's in `UrlDiscoveryService` and `WebmentionSenderService` today. 
- Add interfaces for all services in WebmentionFs. Currently most services have concrete implementations. By adding interfaces, I can make the library extensible and you can implement your own URL discovery and validation services. 
- Implement `outbox` enpoint in WebmentionService to handle sending Webmentions.
- Create Azure ARM template for deploying WebmentionService. Currently with some small edits, you could fork and get my WebmentionService project working for your website. However, that process isn't turnkey today. Maybe using something like [Farmer](https://compositionalit.github.io/farmer/) would make onboarding easier for folks. 
 
## Conclusion

Webmentions are a great way to enable cross-site conversations on the internet. In this post, I provided a brief introduction to the general workflow for sending and receiving Webmentions. I also detailed my own implementation which I use today on my website. That implementation relies on the WebmentionFs .NET library, Azure Functions, and RSS. By using the WebmentionFs .NET library, you can more quickly implement similar solutions into your own apps and services. If you need a reference, check out my WebmentionService repository on GitHub which contains the code I use for receiving Webmentions. 

I'll leave you with three things:

1. As you consider the next platform for your next online home, in addition to considering the health of the community, take some time to also think about the protocols it's built on top of and whether they give you more or less freedom. 
1. Try out the WebmentionFs library on NuGet and add Webmentions to your site.
1. If you share this post anywhere publicly on the web, send me a Webmention.

Catch you on the world wide web!]]&gt;</description>
      <link>https://www.lqdev.me/posts/receive-webmentions-fsharp-az-functions-fsadvent</link>
      <guid>https://www.lqdev.me/posts/receive-webmentions-fsharp-az-functions-fsadvent</guid>
      <pubDate>2022-12-21 00:33 -05:00</pubDate>
      <category>fsharp</category>
      <category>indieweb</category>
      <category>protocol</category>
      <category>internet</category>
      <category>web</category>
      <category>dotnet</category>
    </item>
    <item>
      <title>Web Neural Network API - Working Draft</title>
      <description>&lt;![CDATA[&lt;p&gt;While browsing the interwebs I came across the &lt;a href="https://www.w3.org/TR/webnn/"&gt;Web Neural Network API&lt;/a&gt; spec from the &lt;a href="https://www.w3.org/groups/wg/webmachinelearning"&gt;W3C Web Machine Learning Working Group&lt;/a&gt;. The abstract defines it as &amp;quot;a dedicated low-level API for neural network inference hardware acceleration.&amp;quot;. Although there are already a few frameworks like ONNX &amp;amp; TensorFlow.js that allow you to inference in the browser, this spec looks interesting because it provides an abstraction that allows you to take advantage of hardware acceleration using the framework of your choice. As the &lt;a href="https://github.com/webmachinelearning/webnn/blob/main/explainer.md"&gt;explainer document&lt;/a&gt; mentions, &amp;quot;this architecture allows JavaScript frameworks to tap into cutting-edge machine learning innovations in the operating system and the hardware platform underneath it without being tied to platform-specific capabilities, bridging the gap between software and hardware through a hardware-agnostic abstraction layer.&amp;quot;. It's just a working draft for now but I'm looking forward to how this develops.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/notes/web-neural-network-api-working-draft</link>
      <guid>https://www.lqdev.me/notes/web-neural-network-api-working-draft</guid>
      <pubDate>10/24/2021 20:36 -05:00</pubDate>
      <category>webnn</category>
      <category>ai</category>
      <category>neuralnetwork</category>
      <category>w3c</category>
      <category>standards</category>
      <category>api</category>
      <category>protocol</category>
    </item>
  </channel>
</rss>