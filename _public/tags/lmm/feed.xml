<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - lmm</title>
    <link>https://www.lqdev.me/tags/lmm</link>
    <description>All content tagged with 'lmm' by Luis Quintanilla</description>
    <lastBuildDate>2023-12-06 20:57 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Introducing Gemini</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research. It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, operate across and combine different types of information including text, code, audio, image and video.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Gemini is also our most flexible model yet — able to efficiently run on everything from data centers to mobile devices. Its state-of-the-art capabilities will significantly enhance the way developers and enterprise customers build and scale with AI.&lt;/p&gt;
&lt;p&gt;We’ve optimized Gemini 1.0, our first version, for three different sizes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gemini Ultra — our largest and most capable model for highly complex tasks.&lt;/li&gt;
&lt;li&gt;Gemini Pro — our best model for scaling across a wide range of tasks.&lt;/li&gt;
&lt;li&gt;Gemini Nano — our most efficient model for on-device tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We designed Gemini to be natively multimodal, pre-trained from the start on different modalities. Then we fine-tuned it with additional multimodal data to further refine its effectiveness. This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models — and its capabilities are state of the art in nearly every domain.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our first version of Gemini can understand, explain and generate high-quality code in the world’s most popular programming languages, like Python, Java, C++, and Go.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;On TPUs, Gemini runs significantly faster than earlier, smaller and less-capable models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Starting today, Bard will use a fine-tuned version of Gemini Pro for more advanced reasoning, planning, understanding and more.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We’re also bringing Gemini to Pixel. Pixel 8 Pro is the first smartphone engineered to run Gemini Nano, which is powering new features like Summarize in the Recorder app and rolling out in Smart Reply in Gboard, starting with WhatsApp — with more messaging apps coming next year.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Starting on December 13, developers and enterprise customers can access Gemini Pro via the Gemini API in Google AI Studio or Google Cloud Vertex AI.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/introducing-gemini-google</link>
      <guid>https://www.lqdev.me/bookmarks/introducing-gemini-google</guid>
      <pubDate>2023-12-06 20:57 -05:00</pubDate>
      <category>ai</category>
      <category>lmm</category>
    </item>
    <item>
      <title>The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI's innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution: &lt;a href="https://cdn.openai.com/contributions/gpt-4v.pdf"&gt;this https URL&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/dawn-lmms-early-explorations-gtp4v</link>
      <guid>https://www.lqdev.me/bookmarks/dawn-lmms-early-explorations-gtp4v</guid>
      <pubDate>2023-10-14 21:32 -05:00</pubDate>
      <category>ai</category>
      <category>lmm</category>
    </item>
    <item>
      <title>Multimodality and Large Multimodal Models (LMMs)</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;This post covers multimodal systems in general, including LMMs. It consists of 3 parts.&lt;/p&gt;
&lt;p&gt;Part 1 covers the context for multimodality, including why multimodal, different data modalities, and types of multimodal tasks.&lt;br /&gt;
Part 2 discusses the fundamentals of a multimodal system, using the examples of CLIP, which lays the foundation for many future multimodal systems, and Flamingo, whose impressive performance gave rise to LMMs.&lt;br /&gt;
Part 3 discusses some active research areas for LMMs, including generating multimodal outputs and adapters for more efficient multimodal training, covering newer multimodal systems such as BLIP-2, LLaVA, LLaMA-Adapter V2, LAVIN, etc.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/multimodality-large-multimodal-models-lmm-huyen</link>
      <guid>https://www.lqdev.me/bookmarks/multimodality-large-multimodal-models-lmm-huyen</guid>
      <pubDate>2023-10-14 18:51 -05:00</pubDate>
      <category>ai</category>
      <category>lmm</category>
    </item>
  </channel>
</rss>