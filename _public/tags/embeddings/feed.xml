<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - embeddings</title>
    <link>https://www.lqdev.me/tags/embeddings</link>
    <description>All content tagged with 'embeddings' by Luis Quintanilla</description>
    <lastBuildDate>2025-02-24 20:47 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Nomic Embed Text V2: An Open Source, Multilingual, Mixture-of-Experts Embedding Model</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we're excited to announce Nomic Embed Text V2, our next-generation embedding model that brings the Mixture of Experts (MoE) architecture to text embeddings on a new expanded multilingual training dataset.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Personally, I found the part on MoE to be the most interesting about this release.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Rather than a dense model which uses all parameters on an input, the MoE architecture dynamically routes to different &amp;quot;experts&amp;quot; - sparse subsets of parameters at each layer - activating, ideally, only the parameters especially needed to process the input. This approach allows for more efficient use of compute when generating embeddings.&lt;br /&gt;
&lt;br&gt;
In our experiments, we found that alternating MoE layers with 8 experts and top-2 routing provides the optimal balance between performance and efficiency. This results in 475M total parameters in the model, but only 305M active during training and inference.&lt;br /&gt;
&lt;br&gt;
Research into embedding model architecture has significant practical implications for working with text embeddings in production:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lower latency for high-volume applications of embeddings like retrieval&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Reduced deployment costs through more efficient parameter usage&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;More accessibility to embeddings in settings with constrained compute&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/nomic-embed-text-v2</link>
      <guid>https://www.lqdev.me/responses/nomic-embed-text-v2</guid>
      <pubDate>2025-02-24 20:47 -05:00</pubDate>
      <category>nomic</category>
      <category>ai</category>
      <category>embeddings</category>
    </item>
    <item>
      <title>HuggingFace: Text Embeddings Inference</title>
      <description>&lt;![CDATA[[bookmark] &lt;p&gt;A blazing fast inference solution for text embeddings models.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/hf-text-embedding-inference</link>
      <guid>https://www.lqdev.me/bookmarks/hf-text-embedding-inference</guid>
      <pubDate>2023-10-14 12:54 -05:00</pubDate>
      <category>ai</category>
      <category>embeddings</category>
      <category>text</category>
      <category>inference</category>
      <category>ml</category>
    </item>
    <item>
      <title>ImageBind: One Embedding Space To Bind Them All</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;...ImageBind, the first AI model capable of binding information from six modalities. The model learns a single embedding, or shared representation space, not just for text, image/video, and audio, but also for sensors that record depth (3D), thermal (infrared radiation), and inertial measurement units (IMU), which calculate motion and position.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://dl.fbaipublicfiles.com/imagebind/imagebind_final.pdf"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://imagebind.metademolab.com/"&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/ImageBind"&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/imagebind</link>
      <guid>https://www.lqdev.me/bookmarks/imagebind</guid>
      <pubDate>2023-05-09 21:02 -05:00</pubDate>
      <category>ai</category>
      <category>embeddings</category>
    </item>
    <item>
      <title>Wikipedia embeddings dataset</title>
      <description>&lt;![CDATA[[bookmark] ]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/wikipedia-embeddings</link>
      <guid>https://www.lqdev.me/bookmarks/wikipedia-embeddings</guid>
      <pubDate>2023-04-20 21:23 -05:00</pubDate>
      <category>ai</category>
      <category>dataset</category>
      <category>embeddings</category>
    </item>
  </channel>
</rss>