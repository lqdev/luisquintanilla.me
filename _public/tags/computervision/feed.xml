<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - computervision</title>
    <link>https://www.lqdev.me/tags/computervision</link>
    <description>All content tagged with 'computervision' by Luis Quintanilla</description>
    <lastBuildDate>2024-07-29 22:22 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Meta AI's Segment Anything Model (SAM) 2</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;h2&gt;Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Following up on the success of the Meta Segment Anything Model (SAM) for images, we're releasing SAM 2, a unified model for real-time promptable object segmentation in images and videos that achieves state-of-the-art performance.&lt;/li&gt;
&lt;li&gt;In keeping with our approach to open science, we're sharing the code and model weights with a permissive Apache 2.0 license.&lt;/li&gt;
&lt;li&gt;We're also sharing the SA-V dataset, which includes approximately 51,000 real-world videos and more than 600,000 masklets (spatio-temporal masks).&lt;/li&gt;
&lt;li&gt;SAM 2 can segment any object in any video or image - even for objects and visual domains it has not seen previously, enabling a diverse range of use cases without custom adaptation.&lt;/li&gt;
&lt;li&gt;SAM 2 has many potential real-world applications. For example, the outputs of SAM 2 can be used with a generative video model to create new video effects and unlock new creative applications. SAM 2 could also aid in faster annotation tools for visual data to build better computer vision systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/meta-ai-segment-anything-model-2</link>
      <guid>https://www.lqdev.me/responses/meta-ai-segment-anything-model-2</guid>
      <pubDate>2024-07-29 22:22 -05:00</pubDate>
      <category>meta</category>
      <category>ai</category>
      <category>computervision</category>
      <category>sam2</category>
      <category>segmentanythingmodel</category>
      <category>aimodel</category>
      <category>cv</category>
    </item>
    <item>
      <title>Introducing Stable Video 3D: Quality Novel View Synthesis and 3D Generation from Single Images</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we are releasing Stable Video 3D (SV3D), a generative model based on Stable Video Diffusion, advancing the field of 3D technology and delivering greatly improved quality and view-consistency.&lt;br /&gt;
&lt;br&gt;
This release features two variants: SV3D_u and SV3D_p. SV3D_u generates orbital videos based on single image inputs without camera conditioning. SV3D_p extends the capability by accommodating both single images and orbital views, allowing for the creation of 3D video along specified camera paths.&lt;br /&gt;
&lt;br&gt;
Stable Video 3D can be used now for commercial purposes with a Stability AI Membership. For non-commercial use, you can download the model weights on Hugging Face and view our research paper here.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/introducing-stable-video-3d</link>
      <guid>https://www.lqdev.me/responses/introducing-stable-video-3d</guid>
      <pubDate>2024-03-18 21:29 -05:00</pubDate>
      <category>stabilityai</category>
      <category>ai</category>
      <category>video</category>
      <category>3d</category>
      <category>computervision</category>
    </item>
    <item>
      <title>NightShade</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Nightshade, a tool that turns any image into a data sample that is unsuitable for model training. More precisely, Nightshade transforms images into &amp;quot;poison&amp;quot; samples, so that models training on them without consent will see their models learn unpredictable behaviors that deviate from expected norms, e.g. a prompt that asks for an image of a cow flying in space might instead get an image of a handbag floating in space.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://nightshade.cs.uchicago.edu/whatis.html"&gt;What is NightShade?&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/nightshade-ai-model-poisoning-tool</link>
      <guid>https://www.lqdev.me/responses/nightshade-ai-model-poisoning-tool</guid>
      <pubDate>2024-01-23 20:57 -05:00</pubDate>
      <category>research</category>
      <category>tools</category>
      <category>ai</category>
      <category>generativeai</category>
      <category>llm</category>
      <category>computervision</category>
      <category>cv</category>
    </item>
    <item>
      <title>My AI Timelines Have Sped Up (Again)</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;...computers are useful, ML models are useful, and even if models fail to scale, people will want to fit GPT-4 sized models on their phone. It seems reasonable to assume the competing factions will figure something out.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Data seems like the harder question. (Or at least the one I feel qualified talking about.) We have already crossed the event horizon of trying to train on everything on the Internet. It’s increasingly difficult for labs to differentiate themselves on publicly available data. Differentiation is instead coming from non-public high-quality data to augment public low-quality data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;All the scaling laws have followed power laws so far, including dataset size. Getting more data by hand doesn’t seem good enough to cross to the next thresholds. We need better means to get good data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A long time ago, when OpenAI still did RL in games / simulation, they were very into self-play. You run agents against copies of themselves, score their interactions, and update the models towards interactions with higher reward. Given enough time, they learn complex strategies through competition.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I think it’s possible we’re at the start of a world where self-play or self-play-like ideas work to improve LLM capabilities. Drawing an analogy, the environment is the dialogue, actions are text generated from an LLM, and the reward is from whatever reward model you have. Instead of using ground truth data, our models may be at a point where they can generate data that’s good enough to train on.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/ai-pipelines-sped-up-again-alexirpan</link>
      <guid>https://www.lqdev.me/bookmarks/ai-pipelines-sped-up-again-alexirpan</guid>
      <pubDate>2024-01-11 10:07 -05:00</pubDate>
      <category>ai</category>
      <category>predictions</category>
      <category>agi</category>
      <category>data</category>
      <category>llm</category>
      <category>technology</category>
      <category>ml</category>
      <category>computervision</category>
    </item>
    <item>
      <title>Midjourney v6</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Dev Team gonna let the community test an alpha-version of Midjourney v6 model over the winter break, starting tonight, December 21st, 2023.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;What’s new with the Midjourney v6 base model?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Much more accurate prompt following as well as longer prompts,&lt;/li&gt;
&lt;li&gt;Improved coherence, and model knowledge,&lt;/li&gt;
&lt;li&gt;Improved image prompting and remix mode,&lt;/li&gt;
&lt;li&gt;Minor text drawing ability (you must write your text in “quotations” and --style raw or lower --stylize values may help)&lt;/li&gt;
&lt;li&gt;/imagine a photo of the text &amp;quot;Hello World!&amp;quot; written with a marker on a sticky note --ar 16:9 --v 6&lt;/li&gt;
&lt;li&gt;Improved upscalers, with both 'subtle‘ and 'creative‘ modes (increases resolution by 2x) (you’ll see buttons for these under your images after clicking U1/U2/U3/U4)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/midjourney-v6-release</link>
      <guid>https://www.lqdev.me/bookmarks/midjourney-v6-release</guid>
      <pubDate>2023-12-22 08:57 -05:00</pubDate>
      <category>computervision</category>
      <category>ai</category>
      <category>midjourney</category>
    </item>
  </channel>
</rss>