<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - report</title>
    <link>https://www.lqdev.me/tags/report</link>
    <description>All content tagged with 'report' by Luis Quintanilla</description>
    <lastBuildDate>2024-12-29 12:12 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>LangChain State of AI 2024 Report</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;In 2024, developers leaned into complexity with multi-step agents, sharpened efficiency by doing more with fewer LLM calls, and added quality checks to their apps using methods of feedback and evaluation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src="https://blog.langchain.dev/content/images/size/w1600/2024/12/Top-10-LLM-Providers-bar-chart.png" class="img-fluid" alt="Bar Chart Showing Top 10 LLMs Used 2024" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blog.langchain.dev/content/images/size/w1600/2024/12/Top-10-Retrievers--1-.png" class="img-fluid" alt="Graphic showing Top 10 Retrievers / Vector Stores 2024" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://blog.langchain.dev/content/images/size/w1600/2024/12/Top-evaluation-metrics--1-.png" class="img-fluid" alt="Graphic showing Top LLM Evaluation Metrics 2024" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/langchain-state-of-ai-2024</link>
      <guid>https://www.lqdev.me/responses/langchain-state-of-ai-2024</guid>
      <pubDate>2024-12-29 12:12 -05:00</pubDate>
      <category>ai</category>
      <category>2024</category>
      <category>report</category>
      <category>langchain</category>
      <category>llm</category>
    </item>
    <item>
      <title>LangChain State of AI 2023</title>
      <description>&lt;![CDATA[[bookmark] &lt;h2&gt;What are people building?&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Retrieval has emerged as the dominant way to combine your data with LLMs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...42% of complex queries involve retrieval&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...about 17% of complex queries are part of an agent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Most used LLM Providers&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;OpenAI has emerged as the leading LLM provider of 2023, and Azure (with more enterprise guarantees) has seized that momentum well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;On the open source model side, we see Hugging Face (4th), Fireworks AI (6th), and Ollama (7th) emerge as the main ways users interact with those models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;OSS Model Providers&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A lot of attention recently has been given to open source models, with more and more providers racing to host them at cheaper and cheaper costs. So how exactly are developers accessing these open source models?&lt;/p&gt;
&lt;p&gt;We see that the people are mainly running them locally, with options to do so like Hugging Face, LlamaCpp, Ollama, and GPT4All ranking high.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Most used vector stores&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Vectorstores are emerging as the primary way to retrieve relevant context.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...local vectorstores are the most used, with Chroma, FAISS, Qdrant and DocArray all ranking in the top 5.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Of the hosted offerings, Pinecone leads the pack as the only hosted vectorstore in the top 5. Weaviate follows next, showing that vector-native databases are currently more used than databases that add in vector functionality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Of databases that have added in vector functionality, we see Postgres (PGVector), Supabase, Neo4j, Redis, Azure Search, and Astra DB leading the pack.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Most used embeddings&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;OpenAI reigns supreme&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Open source providers are more used, with Hugging Face coming in 2nd most use&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;On the hosted side, we see that Vertex AI actually beats out AzureOpenAI&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Top Advanced Retrieval Strategies&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;the most common retrieval strategy we see is not a built-in one but rather a custom one.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;After that, we see more familiar names popping up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Self Query&lt;/strong&gt; - which extracts metadata filters from user's questions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hybrid Search&lt;/strong&gt; - mainly through provider specific integrations like Supabase and Pinecone&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contextual Compression&lt;/strong&gt; - which is postprocessing of base retrieval results&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi Query&lt;/strong&gt; - transforming a single query into multiple, and then retrieving results for all&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TimeWeighted VectorStore&lt;/strong&gt; - give more preference to recent documents&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2&gt;How are people testing?&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the majority of them use an LLM to evaluate the outputs. While some have expressed concern and hesitation around this, we are bullish on this as an approach and see that in practice it has emerged as the dominant way to test.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...nearly 40% of evaluators are custom evaluators. This is in line with the fact that we've observed that evaluation is often really specific to the application being worked on, and there's no one-size-fits-all evaluator to rely on.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;What are people testing?&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...most people are still primarily concerned with the correctness of their application (as opposed to toxicity, prompt leakage, or other guardrails&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...low usage of Exact Matching as an evaluation technique [suggests] that judging correctness is often quite complex (you can't just compare the output exactly as is)&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/langchain-state-of-ai-2023</link>
      <guid>https://www.lqdev.me/bookmarks/langchain-state-of-ai-2023</guid>
      <pubDate>2023-12-22 08:32 -05:00</pubDate>
      <category>ai</category>
      <category>llm</category>
      <category>opensource</category>
      <category>report</category>
    </item>
  </channel>
</rss>