<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - local</title>
    <link>https://www.lqdev.me/tags/local</link>
    <description>All content tagged with 'local' by Luis Quintanilla</description>
    <lastBuildDate>2025-01-15 20:49 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>AI Subtitles Are Coming to VLC</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Shown off at CES 2025 is VLC's upcoming automatic subtitling feature, which is local in nature, running entirely on a user's machine without needing to connect to some distant server.&lt;br /&gt;
&lt;br&gt;
According to Jean-Baptiste Kempf, the creator and lead developer of VLC, this new feature is powered by open source AI models, which carry out subtitle generation tasks in two stages.&lt;br /&gt;
&lt;br&gt;
One aspect is automatic generation of subtitles from the video, and the other is translation of those subtitles into over 100 languages.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Unfortunately, there's no word on when this feature will be arriving on VLC.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/ai-subtitles-coming-vlc</link>
      <guid>https://www.lqdev.me/responses/ai-subtitles-coming-vlc</guid>
      <pubDate>2025-01-15 20:49 -05:00</pubDate>
      <category>vlc</category>
      <category>ai</category>
      <category>opensource</category>
      <category>local</category>
    </item>
    <item>
      <title>TinyAgent: Function Calling at the Edge</title>
      <description>&lt;![CDATA[[reshare] &lt;p&gt;&lt;img src="https://bair.berkeley.edu/static/blog/tiny-agent/Figure2.png" class="img-fluid" alt="High level conceptual diagram of TinyAgent system" /&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The ability of LLMs to execute commands through plain language (e.g. English) has enabled agentic systems that can complete a user query by orchestrating the right set of tools...recent multi-modal efforts such as the GPT-4o or Gemini-1.5 model, has expanded the realm of possibilities with AI agents. While this is quite exciting, the large model size and computational requirements of these models often requires their inference to be performed on the cloud. This can create several challenges for their widespread adoption. First and foremost, uploading data such as video, audio, or text documents to a third party vendor on the cloud, can result in privacy issues. Second, this requires cloud/Wi-Fi connectivity which is not always possible...latency could also be an issue as uploading large amounts of data to the cloud and waiting for the response could slow down response time, resulting in unacceptable time-to-solution. These challenges could be solved if we deploy the LLM models locally at the edge.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...current LLMs like GPT-4o or Gemini-1.5 are too large for local deployment. One contributing factor is that a lot of the model size ends up memorizing general information about the world into its parametric memory which may not be necessary for a specialized downstream application.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...this leads to an intriguing research question:&lt;br /&gt;
&lt;br&gt;
Can a smaller language model with significantly less parametric memory emulate such emergent ability of these larger language models?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Achieving this would significantly reduce the computational footprint of agentic systems and thus enable efficient and privacy-preserving edge deployment. Our study demonstrates that this is feasible for small language models through training with specialized, high-quality data that does not require recalling generic world knowledge.&lt;br /&gt;
&lt;br&gt;
Such a system could particularly be useful for semantic systems where the AI agent’s role is to understand the user query in natural language and, instead of responding with a ChatGPT-type question answer response, orchestrate the right set of tools and APIs to accomplish the user’s command. For example, in a Siri-like application, a user may ask a language model to create a calendar invite with particular attendees. If a predefined script for creating calendar items already exists, the LLM simply needs to learn how to invoke this script with the correct input arguments (such as attendees’ email addresses, event title, and time). This process does not require recalling/memorization of world knowledge from sources like Wikipedia, but rather requires reasoning and learning to call the right functions and to correctly orchestrate them.&lt;br /&gt;
&lt;br&gt;
Our goal is to develop Small Language Models (SLM) that are capable of complex reasoning that could be deployed securely and privately at the edge. Here we will discuss the research directions that we are pursuing to that end. First, we discuss how we can enable small open-source models to perform accurate function calling, which is a key component of agentic systems. It turns out that off-the-shelf small models have very low function calling capabilities. We discuss how we address this by systematically curating high-quality data for function calling, using a specialized Mac assistant agent as our driving application. We then show that fine-tuning the model on this high quality curated dataset, can enable SLMs to even exceed GPT-4-Turbo’s function calling performance. We then show that this could be further improved and made efficient through a new Tool RAG method. Finally, we show how the final models could be deployed efficiently at the edge with real time responses.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/TinyAgent-function-calling-edge</link>
      <guid>https://www.lqdev.me/responses/TinyAgent-function-calling-edge</guid>
      <pubDate>2024-06-11 19:43 -05:00</pubDate>
      <category>agent</category>
      <category>ai</category>
      <category>slm</category>
      <category>function</category>
      <category>edge</category>
      <category>local</category>
      <category>languagemodel</category>
      <category>smalllanguagemodel</category>
      <category>research</category>
    </item>
    <item>
      <title>The Online Local Chronicle</title>
      <description>&lt;![CDATA[[reply] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;In the same way that every little place in America used to have a printed newspaper, every little place in America could have an online local chronicle.&lt;br /&gt;
&lt;br&gt;
Broadly speaking, an online local chronicle is a collection of facts organized mostly in chronological order. The “pages” of the chronicle can be thought of as subsets of a community’s universal timeline of events. These online local chronicles could become the backbone of local news operations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Nice project. Unfortunately, it's rare you get local news. I like publications / websites like &lt;a href="https://www.hobokengirl.com/"&gt;Hoboken Girl&lt;/a&gt; and &lt;a href="https://blockclubchicago.org/"&gt;Block Club Chicago&lt;/a&gt;. I wish there were more of them in more cities and towns. I know they're there in some forms like Facebook Groups. Even better, it'd be great to have the websites for these publications be the main source of truth that then syndicated their content to the various platforms out there.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/online-local-chronicle-searls</link>
      <guid>https://www.lqdev.me/responses/online-local-chronicle-searls</guid>
      <pubDate>2024-03-19 22:17 -05:00</pubDate>
      <category>community</category>
      <category>news</category>
      <category>local</category>
      <category>knowledgebase</category>
      <category>information</category>
      <category>web</category>
      <category>opensource</category>
    </item>
    <item>
      <title>Podcast - localfirst.fm</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;A podcast about local-first software development&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/localfirstfm-podcast</link>
      <guid>https://www.lqdev.me/responses/localfirstfm-podcast</guid>
      <pubDate>2024-02-20 21:38 -05:00</pubDate>
      <category>development</category>
      <category>software</category>
      <category>podcast</category>
      <category>tech</category>
      <category>podcast</category>
      <category>localfirst</category>
      <category>local</category>
    </item>
  </channel>
</rss>