<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - neuralnetworks</title>
    <link>https://www.lqdev.me/tags/neuralnetworks</link>
    <description>All content tagged with 'neuralnetworks' by Luis Quintanilla</description>
    <lastBuildDate>2025-05-05 20:03 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. &lt;strong&gt;In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications.&lt;/strong&gt; We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at &lt;a href="https://github.com/mengyuest/TeLoGraF"&gt;this https URL&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/telograf</link>
      <guid>https://www.lqdev.me/bookmarks/telograf</guid>
      <pubDate>2025-05-05 20:03 -05:00</pubDate>
      <category>ai</category>
      <category>graphs</category>
      <category>neuralnetworks</category>
      <category>research</category>
    </item>
    <item>
      <title>Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;We introduce a new algorithm called the Free-pipeline Fast Inner Product (FFIP) and its hardware architecture that improve an under-explored fast inner-product algorithm (FIP) proposed by Winograd in 1968. Unlike the unrelated Winograd minimal filtering algorithms for convolutional layers, FIP is applicable to all machine learning (ML) model layers that can mainly decompose to matrix multiplication, including fully-connected, convolutional, recurrent, and attention/transformer layers. We implement FIP for the first time in an ML accelerator then present our FFIP algorithm and generalized architecture which inherently improve FIP's clock frequency and, as a consequence, throughput for a similar hardware cost. Finally, we contribute ML-specific optimizations for the FIP and FFIP algorithms and architectures. We show that FFIP can be seamlessly incorporated into traditional fixed-point systolic array ML accelerators to achieve the same throughput with half the number of multiply-accumulate (MAC) units, or it can double the maximum systolic array size that can fit onto devices with a fixed hardware budget. Our FFIP implementation for non-sparse ML models with 8 to 16-bit fixed-point inputs achieves higher throughput and compute efficiency than the best-in-class prior solutions on the same type of compute platform.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/trevorpogue/algebraic-nnhw"&gt;GitHub&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/fast-inner-product-dnn-accelerators</link>
      <guid>https://www.lqdev.me/bookmarks/fast-inner-product-dnn-accelerators</guid>
      <pubDate>2024-03-17 20:47 -05:00</pubDate>
      <category>ai</category>
      <category>math</category>
      <category>acceleration</category>
      <category>neuralnetworks</category>
      <category>dnn</category>
    </item>
  </channel>
</rss>