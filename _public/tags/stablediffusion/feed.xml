<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - stablediffusion</title>
    <link>https://www.lqdev.me/tags/stablediffusion</link>
    <description>All content tagged with 'stablediffusion' by Luis Quintanilla</description>
    <lastBuildDate>2024-03-26 21:44 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>One-step Diffusion with Distribution Matching Distillation</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Our one-step generator achieves comparable image quality with StableDiffusion v1.5 while being 30x faster.&lt;/strong&gt;&lt;br /&gt;
&lt;br&gt;
Diffusion models are known to approximate the score function of the distribution they are trained on. In other words, an unrealistic synthetic image can be directed toward higher probability density region through the denoising process (see SDS). Our core idea is training two diffusion models to estimate not only the score function of the target real distribution, but also that of the fake distribution. We construct a gradient update to our generator as the difference between the two scores, essentially nudging the generated images toward higher realism as well as lower fakeness (see VSD). Our method is similar to GANs in that a critic is jointly trained with the generator to minimize a divergence between the real and fake distributions, but differs in that our training does not play an adversarial game that may cause training instability, and our critic can fully leverage the weights of a pretrained diffusion model. Combined with a simple regression loss to match the output of the multi-step diffusion model, our method outperforms all published few-step diffusion approaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shot COCO-30k, comparable to Stable Diffusion but orders of magnitude faster. Utilizing FP16 inference, our model generates images at 20 FPS on modern hardware.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2311.18828"&gt;Paper&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/one-step-diffusion-distribution-matching-distillation</link>
      <guid>https://www.lqdev.me/responses/one-step-diffusion-distribution-matching-distillation</guid>
      <pubDate>2024-03-26 21:44 -05:00</pubDate>
      <category>ai</category>
      <category>genai</category>
      <category>diffusion</category>
      <category>dmd</category>
      <category>stablediffusion</category>
      <category>dalle</category>
    </item>
    <item>
      <title>Stable Diffusion 3: Research Paper</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Key Takeaways:&lt;br /&gt;
&lt;br&gt;
Today, we’re publishing our research paper that dives into the underlying technology powering Stable Diffusion 3.&lt;br /&gt;
&lt;br&gt;
Stable Diffusion 3 outperforms state-of-the-art text-to-image generation systems such as DALL·E 3, Midjourney v6, and Ideogram v1 in typography and prompt adherence, based on human preference evaluations.&lt;br /&gt;
&lt;br&gt;
Our new Multimodal Diffusion Transformer (MMDiT) architecture uses separate sets of weights for image and language representations, which improves text understanding and spelling capabilities compared to previous versions of SD3.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/stable-diffusion-3-research-paper</link>
      <guid>https://www.lqdev.me/responses/stable-diffusion-3-research-paper</guid>
      <pubDate>2024-03-05 12:55 -05:00</pubDate>
      <category>stabilityai</category>
      <category>stablediffusion</category>
      <category>ai</category>
      <category>generativeai</category>
      <category>genai</category>
      <category>images</category>
    </item>
    <item>
      <title>Stable Diffusion 3 - Early Preview</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Announcing Stable Diffusion 3 in early preview, our most capable text-to-image model with greatly improved performance in multi-subject prompts, image quality, and spelling abilities.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Stable Diffusion 3 suite of models currently range from 800M to 8B parameters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable Diffusion 3 combines a diffusion transformer architecture and flow matching.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/stable-diffusion-3-early-preview</link>
      <guid>https://www.lqdev.me/responses/stable-diffusion-3-early-preview</guid>
      <pubDate>2024-02-22 20:59 -05:00</pubDate>
      <category>stabilityai</category>
      <category>ai</category>
      <category>stablediffusion</category>
      <category>imagegeneration</category>
      <category>ml</category>
      <category>image</category>
      <category>genai</category>
      <category>generativeai</category>
    </item>
    <item>
      <title>Stable Cascade</title>
      <description>&lt;![CDATA[[reshare] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable Cascade consists of three models: Stage A, Stage B and Stage C, representing a cascade for generating images, hence the name &amp;quot;Stable Cascade&amp;quot;. Stage A &amp;amp; B are used to compress images, similarly to what the job of the VAE is in Stable Diffusion. However, as mentioned before, with this setup a much higher compression of images can be achieved. Furthermore, Stage C is responsible for generating the small 24 x 24 latents given a text prompt. The following picture shows this visually. Note that Stage A is a VAE and both Stage B &amp;amp; C are diffusion models.&lt;br /&gt;
&lt;br&gt;
For this release, we are providing two checkpoints for Stage C, two for Stage B and one for Stage A. Stage C comes with a 1 billion and 3.6 billion parameter version, but we highly recommend using the 3.6 billion version, as most work was put into its finetuning. The two versions for Stage B amount to 700 million and 1.5 billion parameters. Both achieve great results, however the 1.5 billion excels at reconstructing small and fine details. Therefore, you will achieve the best results if you use the larger variant of each. Lastly, Stage A contains 20 million parameters and is fixed due to its small size.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/responses/stability-ai-stable-cascade</link>
      <guid>https://www.lqdev.me/responses/stability-ai-stable-cascade</guid>
      <pubDate>2024-02-13 21:02 -05:00</pubDate>
      <category>stabilityai</category>
      <category>ai</category>
      <category>stablediffusion</category>
    </item>
  </channel>
</rss>