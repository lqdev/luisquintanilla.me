<rss version="2.0">
  <channel>
    <title>Luis Quintanilla Blog</title>
    <link>https://www.luisquintanilla.me</link>
    <description>Luis Quintanilla's blog</description>
    <lastPubDate>2025-01-09 20:47 -05:00</lastPubDate>
    <language>en</language>
    <item>
      <title>Deploy your own Owncast server</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/deploy-owncast-azure?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/deploy-owncast-azure&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;I'm a fan of learning in public. One way I do that is writing these types of posts. Another is live-streaming. It's been about four years since the last time I streamed. You can check out some of those recordings on my &lt;a href="/youtube"&gt;YouTube channel&lt;/a&gt; if you're interested.&lt;/p&gt;
&lt;p&gt;Lately, I've been wanting to get back into live-streaming, but like many parts of my digital presence, it's an area I want ownership over. That means, I don't want to use Twitch, YouTube, Kick, or any other streaming platform. I want to self-host my own livestream server.&lt;/p&gt;
&lt;p&gt;For years, I've been following &lt;a href="https://owncast.online/"&gt;Owncast&lt;/a&gt;. I'm a big fan. Aside from being open-source and allowing me to self-host, it has &lt;a href="https://owncast.online/docs/social/"&gt;Fediverse&lt;/a&gt; and &lt;a href="https://owncast.online/docs/chat/chat-authentication/#indieauth"&gt;IndieAuth&lt;/a&gt; integrations.&lt;/p&gt;
&lt;p&gt;Because I don't plan to monetize my streams, I want to make sure I keep my costs down. Owncast offers a container deployment option. That means, I can deploy my application to &lt;a href="https://learn.microsoft.com/azure/container-apps/overview"&gt;Azure Container Apps&lt;/a&gt; using a &lt;a href="https://learn.microsoft.com/azure/container-apps/plans#consumption"&gt;consumption plan&lt;/a&gt;. By doing so, I only pay for compute and networking when I'm streaming. The rest of the time, I can scale down the application to zero and I'm not charged for it.&lt;/p&gt;
&lt;p&gt;In this post, I'll go over how to set up a self-hosted Owncast instance on Azure Container Apps and once deployed, how to configure it with &lt;a href="https://obsproject.com/"&gt;OBS&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Deploy owncast server&lt;/h2&gt;
&lt;p&gt;The deployment is fairly straightforward. You can use the &lt;a href="https://learn.microsoft.com/azure/container-apps/vnet-custom?tabs=bash&amp;amp;pivots=azure-portal"&gt;&amp;quot;Provide a virtual network to an Azure Container Apps environment&amp;quot;&lt;/a&gt; tutorial. I'd recommend going through the portal if you don't want to install the Azure CLI or are not comfortable with the command line.&lt;/p&gt;
&lt;p&gt;The few configurations that tutorial doesn't guide you through is setting up your container and ingress since those are application specific.&lt;/p&gt;
&lt;p&gt;After you configure your environment and virtual network in the &lt;em&gt;Basics&lt;/em&gt; tab, continue your setup using the steps below.&lt;/p&gt;
&lt;h3&gt;Container&lt;/h3&gt;
&lt;p&gt;To set up Owncast, you can use the &lt;a href="https://hub.docker.com/r/owncast/owncast/tags"&gt;public docker image&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When you get to the &lt;em&gt;Container&lt;/em&gt; tab in the setup process, fill in the fields with the following values:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Name&lt;/td&gt;
&lt;td&gt;Enter a name for your container&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Image source&lt;/td&gt;
&lt;td&gt;Docker Hub or other registies&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Image type&lt;/td&gt;
&lt;td&gt;Public&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Registry login server&lt;/td&gt;
&lt;td&gt;docker.io&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Image and tag&lt;/td&gt;
&lt;td&gt;owncast/owncast:latest&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Command override&lt;/td&gt;
&lt;td&gt;You can leave this blank&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Arguments override&lt;/td&gt;
&lt;td&gt;You can leave this blank&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Deployment stack&lt;/td&gt;
&lt;td&gt;Generic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Workload profile&lt;/td&gt;
&lt;td&gt;Consumption&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CPU and Memory&lt;/td&gt;
&lt;td&gt;2 CPU Cores, 4Gi Memory&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Ingress&lt;/h3&gt;
&lt;p&gt;By setting up ingress on your deployment, you'll expose the ports Owncast listens on.&lt;/p&gt;
&lt;p&gt;The reason you need a virtual network is because Owncast listens on two ports:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;8080&lt;/strong&gt; - The web application and admin portal&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1935&lt;/strong&gt; - The RMTP server used for streaming&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By default, Azure Container Apps only allows you to expose one port. For more details, see the &lt;a href="https://learn.microsoft.com/azure/container-apps/ingress-overview#additional-tcp-ports"&gt;Azure Container Apps documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the &lt;em&gt;Ingress&lt;/em&gt; tab, check the box that says &lt;strong&gt;Enabled&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Then, fill in the fields with the following values:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Ingress traffic&lt;/td&gt;
&lt;td&gt;Accepting traffic from anywhere&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ingress type&lt;/td&gt;
&lt;td&gt;HTTP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Transport&lt;/td&gt;
&lt;td&gt;Auto&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Insecure connections&lt;/td&gt;
&lt;td&gt;Leave unchecked&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Target port&lt;/td&gt;
&lt;td&gt;8080&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Session affinity&lt;/td&gt;
&lt;td&gt;Leave unchecked&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Expand the Additional TCP ports section and configure the RMTP port&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Field&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Target port&lt;/td&gt;
&lt;td&gt;1935&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exposed port&lt;/td&gt;
&lt;td&gt;1935&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ingress traffic&lt;/td&gt;
&lt;td&gt;Accepting traffic from anywhere&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Once you've configured ingress, you can select on &lt;strong&gt;Review + create&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Review your configurations in the &lt;em&gt;Review + create&lt;/em&gt; tab. If everything looks good, select &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt; Change admin password&lt;/h2&gt;
&lt;p&gt;Once your application deploys, make sure to change your admin password.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;In the portal, go to your container app resource and copy your application URL.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/deploy-owncast-azure/portal-container-resource.png" class="img-fluid" alt="Screenshot of Azure Container Apps resource in Azure Portal highlighting Application URL" /&gt;&lt;/p&gt;
&lt;p&gt;Make sure to keek your application URL around since you'll also be using it when configuring OBS.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Access the admin portal at the following URL - &lt;code&gt;&amp;lt;YOUR-APPLICATION-URL&amp;gt;/admin&lt;/code&gt;. Replace &lt;code&gt;&amp;lt;YOUR-APPLICATION-URL&amp;gt;&lt;/code&gt; with your application URL.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Log in with the default credentials.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;| Field | Value |
| Username | admin |
| Password | abc123 |&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In the Owncast admin portal, select &lt;strong&gt;Configuration &amp;gt; Server Setup&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Change your admin password.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more details, see the &lt;a href="https://owncast.online/docs/configuration/"&gt;Owncast documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Create stream key&lt;/h2&gt;
&lt;p&gt;Once you've changed your admin password, create a stream key. This will enable you to log into your server through OBS and begin streaming.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In the Owncast admin portal, select &lt;strong&gt;Configuration &amp;gt; Server Setup&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Select the &lt;strong&gt;Stream Keys&lt;/strong&gt; tab.&lt;/li&gt;
&lt;li&gt;Copy the default stream key or create a new one.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Configure OBS&lt;/h2&gt;
&lt;p&gt;Now that everything is configured, it's time to set up OBS.&lt;/p&gt;
&lt;p&gt;To do so, &lt;a href="https://owncast.online/docs/broadcasting/obs/"&gt;follow the guidance in the Owncast documentattion&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Replace the server address with your application URL and use the stream key you copied or created. Note that the protocol is not &lt;code&gt;http&lt;/code&gt; or &lt;code&gt;https&lt;/code&gt;. &lt;strong&gt;Use &lt;code&gt;rmtp&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;That's it! If everything is configured correctly, you should be able to select &lt;strong&gt;Start Streaming&lt;/strong&gt; in OBS.&lt;/p&gt;
&lt;p&gt;Go to your Owncast instance using your application URL and in a few seconds, your broadcast should start.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I want to end by thanking the Owncast contributors and community for making self-hosting a live-streaming server so easy.&lt;/p&gt;
&lt;p&gt;My plan in the next couple of weeks is to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Embed my stream on my website. Most likely, I'll make it a dedicated page. Something like &lt;code&gt;lqdev.me/live&lt;/code&gt; or something like that.&lt;/li&gt;
&lt;li&gt;Do a few pilot streams to make sure my stream quality and configurations are all set up correctly.&lt;/li&gt;
&lt;li&gt;Evaluate what the costs are for this solution and whethere it's sustainable.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If all goes as expected, I'll start streaming more regularly. As mentioned, I want to keep my costs down and video hosting is expensive, so I'll use YouTube to post recordings. I'll also keep backups though in case any of those videos get taken down, I can always have a local copy still available.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/deploy-owncast-azure?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/deploy-owncast-azure</guid>
      <pubDate>2025-01-09 20:47 -05:00</pubDate>
      <category>#owncast</category>
      <category>#azure</category>
      <category>#selfhost</category>
      <category>#fediverse</category>
      <category>#obs</category>
      <category>#livestream</category>
    </item>
    <item>
      <title>Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/listen-audiobooks-librivox-antennapod?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/listen-audiobooks-librivox-antennapod&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;Last year, I was able to complete my goal of reading 12 books or roughly one book per month. I kept track of some of the books I read in my &lt;a href="https://www.lqdev.me/library"&gt;library page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I enjoyed most of the books I read. However, despite hitting my goal and averaging a book a month, I didn't build up a habit which is what I would've preferred. Many of the books I read last year, I read them during long periods of uninterrupted time, such as cross-country flights. The topic of goal setting and measuring the right thing could be an entire post, but I'll leave that for another time.&lt;/p&gt;
&lt;p&gt;Taking the learnings from last year, this year, I'll try to focus on building a reading habit rather than reading a set number of books. While I'd still like to finish at least one book per month, that's a metric I'll use to track general progress rather than making it the goal. Unless I'm reading something like War and Peace, I'd expect that by establishing a consistent cadence, I should be able to get through at least one book every month. Also, I'm only accounting for uninterrupted reading time blocks. Using audiobooks, I can continue building my reading habit while working out or doing chores. Kind of like having TV playing in the background.&lt;/p&gt;
&lt;p&gt;There is no shortage of audiobooks out there. Audible is one of the easiest ways to access audiobooks and they have a large selection to choose from. Personally, I don't use Audible. I don't like paying for a separate subscription, getting credits to &amp;quot;buy&amp;quot; books that I don't really own and can only listen through the Audible app. Instead, I use Spotify. I get &lt;a href="https://support.spotify.com/us/article/audiobooks-access-plan/"&gt;15 hours of access to audiobooks per month&lt;/a&gt; as part of my subscription. While I still don't own the books and can only listen on the Spotify app, at least I'm not paying a separate subscription. Also, the 15 hour limit at this time is more than enough for me.&lt;/p&gt;
&lt;p&gt;The library is another amazing option. No subscription fees and you get access to your local library's entire digital catalog. Using the &lt;a href="https://www.overdrive.com/apps/libby"&gt;Libby app&lt;/a&gt;, you can borrow books and start listening for free. Selection though may be an issue here since you are limited to what's available through your local library. Pro-tip, universities and employers sometimes have libraries, so as an alumnus or employee, you can expand the number of books available to you by registering multiple libraries on the Libby app. Regardless of which library you choose though, you still have to go through the Libby app.&lt;/p&gt;
&lt;p&gt;The dream setup for me is, having direct access to the audiobook files so I can listen to them whenever and wherever I'd like. Similar to podcasts. Thanks to LibriVox and AntennaPod, that setup is possible.&lt;/p&gt;
&lt;p&gt;In case you're interested in setting up something similar, in this post, I'll briefly explain what LibriVox and AntennaPod are and show how you can use them to start listening to free audiobooks on your phone.&lt;/p&gt;
&lt;h2&gt;What is LibriVox?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://librivox.org/"&gt;LibriVox&lt;/a&gt; is a website that provides a collection of free audiobooks from the public domain. Thanks to a vibrant community of volunteers, you can enjoy classics such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://librivox.org/the-divine-comedy-by-dante-alighieri/"&gt;The Divine Comedy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://librivox.org/the-adventures-of-sherlock-holmes/"&gt;The Adventures of Sherlock Holmes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://librivox.org/discourses-of-epictetus-by-epictetus/"&gt;Discourses of Epictetus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://librivox.org/roughing-it/"&gt;Roughing It&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://librivox.org/search?primary_key=0&amp;amp;search_category=title&amp;amp;search_page=1&amp;amp;search_form=get_results&amp;amp;search_order=catalog_date"&gt;And many others...&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It's important to note that the catalog is limited to public domain works. That means you won't have access to the latest self-help, business, or current events books. However, there are still thousands of books to keep you busy for an entire lifetime and the list is constantly growining since there are new works introduced into the public domain every year.&lt;/p&gt;
&lt;p&gt;Here's &lt;a href="https://blog.archive.org/2025/01/01/welcome-to-the-public-domain-in-2025/"&gt;a few notable literary works that entered the public domain in 2025&lt;/a&gt; as well as the complete list of &lt;a href="https://archive.org/details/internetarchivebooks?tab=collection&amp;amp;query=date%3A1929"&gt;literary works from 1929&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;What is Antennapod?&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://antennapod.org/"&gt;AntennaPod&lt;/a&gt; is an open-source podcast player for Android.&lt;/p&gt;
&lt;p&gt;The app has a ton of great features, such as &lt;a href="https://podcastindex.org/"&gt;Podcast Index&lt;/a&gt; support, subscribe via RSS directly,  OPML import and export, chapters, adjustable playback speed, and for the past few years, a personalized yearly summary which is computed locally on your device &lt;a href="/feed/antennapod-echo-2024"&gt;(here's my summary from 2024)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can get AntennaPod from the Play Store or &lt;a href="https://f-droid.org/en/packages/de.danoeh.antennapod/"&gt;F-Droid&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Keep track of new releases&lt;/h2&gt;
&lt;p&gt;LibriVox provides &lt;a href="https://librivox.org/pages/librivox-feeds/"&gt;several feeds&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since podcast distribution is primarly done through RSS and you can subscribe to podcasts directly using RSS feeds in AntennaPod, we can use the app to stay up to date with new audiobook releases.&lt;/p&gt;
&lt;p&gt;To subscribe to the LibriVox New Release feed:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Copy the LibriVox New Releases RSS feed URL to your clipboard: &lt;a href="https://librivox.org/rss/latest_releases"&gt;https://librivox.org/rss/latest_releases&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In AntennaPod, open up the hamburger menu and select &lt;strong&gt;Add Podcast&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the &lt;em&gt;Add Podcast&lt;/em&gt; dialog, select &lt;strong&gt;Add podcast by RSS address&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/listen-audiobooks-librivox-antennapod/add-podcast-antennapod.png" class="img-fluid" alt="AntennaPod Add Podcast Dialog" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Paste the LibriVox New Releases RSS feed URL into the text box and select &lt;strong&gt;Confirm&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point, you should see the feed listed in your AntennaPod subscriptions.&lt;/p&gt;
&lt;p&gt;By subscribing the LibriVox New Release feed, AntennaPod treats the feed as a podcast, and each new release entry or audiobook is like a podcast episode.&lt;/p&gt;
&lt;p&gt;For more information on subscribing to podcasts, see the &lt;a href="https://antennapod.org/documentation/getting-started/subscribe"&gt;AntennaPod documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Add books to Antennapod&lt;/h2&gt;
&lt;p&gt;Subscribing to the new releases feed only notifies you when new books are added, you can't listen to or download the books. To do that, you'll need to subscribe to the RSS feed of the book you want to listen to in AntennaPod.&lt;/p&gt;
&lt;h3&gt;From New Releases Feed&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;In AntennaPod's Subscription screen, select &lt;strong&gt;LibriVox's New Releases&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;From the list of new releases, select the book you want to listen to. For example, &amp;quot;Room of One's Own&amp;quot; by Virginia Woolf.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/listen-audiobooks-librivox-antennapod/antennapod-librivox-new-releases.png" class="img-fluid" alt="AntennaPod New Releases Feed" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the audiobook's screen, select &lt;strong&gt;Visit Website&lt;/strong&gt;. This will take you to the LibriVox audiobook's page in the browser.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/listen-audiobooks-librivox-antennapod/antennapod-room-of-ones-own.png" class="img-fluid" alt="AntennaPod Audiobook Screen" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the LibriVox audiobook page, select the book's RSS Feed.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/listen-audiobooks-librivox-antennapod/librivox-room-ones-own.png" class="img-fluid" alt="A Room of One's Own by Virginia Woolf LibriVox book page" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Copy the book's feed URL to your clipboard.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In AntennaPod, open up the hamburger menu and select &lt;strong&gt;Add Podcast&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the &lt;em&gt;Add Podcast&lt;/em&gt; dialog, select &lt;strong&gt;Add podcast by RSS address&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Paste the book's RSS feed URL into the text box and select &lt;strong&gt;Confirm&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Any other books&lt;/h3&gt;
&lt;p&gt;The process works the same for other books but you'll have to use LibriVox as the entrypoint to browse books.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In your browser, navigate to LibriVox.&lt;/li&gt;
&lt;li&gt;Go to the audiobook page. For example, this is the page for &lt;a href="https://librivox.org/walden-by-henry-david-thoreau"&gt;Walden&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;In the audiobook page, select the book's RSS Feed. For Walden, it's &lt;a href="https://librivox.org/rss/549"&gt;https://librivox.org/rss/549&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Copy the book's feed URL to your clipboard.&lt;/li&gt;
&lt;li&gt;In AntennaPod, open up the hamburger menu and select &lt;strong&gt;Add Podcast&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;In the &lt;em&gt;Add Podcast&lt;/em&gt; dialog, select &lt;strong&gt;Add podcast by RSS address&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Paste the book's RSS feed URL into the text box and select &lt;strong&gt;Confirm&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Although AntennaPod wasn't designed for audiobook listening, thanks to the flexibility of RSS feeds and the thankless work of the LibriVox community, you can easily listen to audiobooks for free on your mobile device. I plan on testing out this setup and seeing whether it helps keep my reading habit on track.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/listen-audiobooks-librivox-antennapod?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/listen-audiobooks-librivox-antennapod</guid>
      <pubDate>2025-01-05 21:04 -05:00</pubDate>
      <category>#book</category>
      <category>#audiobook</category>
      <category>#rss</category>
      <category>#antennapod</category>
      <category>#librivox</category>
      <category>#android</category>
      <category>#podcast</category>
      <category>#publicdomain</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Weird, Inc. and 2025</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/weird-inc-2025?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/weird-inc-2025&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;I don't remember how I came across Weird and their projects but I'm glad I did. I appreciate their approach of using existing platforms, protocols, and practices from the yesterweb to put the personal website front and center.&lt;/p&gt;
&lt;h2&gt;Weird, Inc&lt;/h2&gt;
&lt;p&gt;First, the topic of &lt;a href="https://blog.muni.town/weird-inc/"&gt;Weird the organization and their aspirations&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We have big aspirations for Weird. At minimum, a 'social network of personal websites' can be catalyzed by simply mapping and linking existing web spaces together, playing the role of social glue. On a grander scale there's potential for a pro-social reformation of the web as we know it, putting people's websites at the center of our sociodigital interactions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By aligning to the foundations of &lt;a href="https://seths.blog/2010/05/the-modern-business-plan/"&gt;Seth Godin's Modern Business Plan&lt;/a&gt;, they aim to focus on:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Truth -  describe the world as it is&lt;/li&gt;
&lt;li&gt;Assertions - how you’re going to change things&lt;/li&gt;
&lt;li&gt;Alternatives - what you'll do if assertions don't pan out.&lt;/li&gt;
&lt;li&gt;People - who is on your team, who is going to join your team&lt;/li&gt;
&lt;li&gt;Money - how much do you need and how will you spend it&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Below are some of the points that resonated with me.&lt;/p&gt;
&lt;h3&gt;Truth&lt;/h3&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Home ownership on the web, remarkably, is just as fraught with rent-seeking monopolists as our physical spaces. No one gets to own anything anymore.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While they're mainly referring to the social web, this is is generally true across many areas of our digital spaces.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Bluesky...is bringing into mainstream a nearly forgotten cornerstone of the Indie Web: Domain sovereignty&lt;br /&gt;
&lt;br&gt;
In a reinvigorated social web where your domain-name is your universal address in the digital sphere, Weird is the virtual home to which that address is pointing&lt;br /&gt;
&lt;br&gt;
'Weird the data importer' is our wedge into the data fortresses keeping our digital identities captive. What will start as a Publish Elsewhere, Syndicate (to your) Own Site (PESOS) strategy makes way for a fully self-sovereign Publish (on your) Own Site, Syndicate Elsewhere (POSSE) end state.&lt;br /&gt;
&lt;br&gt;
Imagine a WordPress-like website engine that aggregates all of your digital personas into a unified mosaic, completely under your control by combination of cloud backing and local storage.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Alternatives&lt;/h3&gt;
&lt;p&gt;If the assertions don't pan out, the alternatives that I think might be interesting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local-first knowledge base with cloud synchronization. &lt;a href="https://anytype.io/"&gt;AnyType&lt;/a&gt; comes to mind.&lt;/li&gt;
&lt;li&gt;Cloud + local archiver for many other 3rd party accounts such as Bluesky, Mastodon, Discourse etc. This might make PESOS scenarios easier. Of course, this only works if the 3rd party services allow developer access to their APIs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Weird in 2025&lt;/h2&gt;
&lt;p&gt;Speaking specifically about &lt;a href="https://blog.muni.town/weird-2025/"&gt;Weird the product and what the team plans of shipping in 2025&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Weird is best understood as a single-page website generator&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Q1 and Q2&lt;/h3&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;'Weird the data importer' is our wedge into the data fortresses keeping our digital identities captive.&lt;br /&gt;
&lt;br&gt;
'Weird: ₱ESOS-edition' aggregates all of your digital personas into a harmonious mosaic, made completely yours by combination of cloud backing and local-first storage.&lt;br /&gt;
&lt;br&gt;
Setting up a POSSE pipeline however requires not just the right infrastructure...but also a big shift in workflow.&lt;br /&gt;
&lt;br&gt;
Most people will probably want a combination of both.&lt;br /&gt;
&lt;br&gt;
Hence our commitment to both models, starting with the least laborious one [PESOS].&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Personally, &lt;a href="https://www.lqdev.me/posts/rss-to-mastodon-posse-azure-logic-apps/"&gt;I prefer POSSE&lt;/a&gt;, but it's great to see that Weird plans on meeting people where they are.&lt;/p&gt;
&lt;h3&gt;Late 2025&lt;/h3&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Around half-way through 2025, we're hoping to shift some of our attention away from Weird as an individualistic publishing tool to Weird as a network engine.&lt;br /&gt;
&lt;br&gt;
We think there's massive untapped potential in the web 1.0 concept of webrings, and we fully intend to bring them back.&lt;br /&gt;
&lt;br&gt;
All of this culminates in one of the key motivations behind the Weird project: &lt;a href="https://github.com/muni-town/weird/issues/3"&gt;A Network of Shared Purpose&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I'm not sure yet what this looks like but making it easier to discover and build networks of personal websites is a good problem to try to solve.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I wish them luck and success in what they're aiming to accomplish. I'm excited to see more projects like this sprouting to shake up the status-quo in our digital spaces.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/weird-inc-2025?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/weird-inc-2025</guid>
      <pubDate>2024-12-17 18:41</pubDate>
      <category>#weird</category>
      <category>#cms</category>
      <category>#munitown</category>
      <category>#2025</category>
      <category>#indieweb</category>
      <category>#posse</category>
      <category>#pesos</category>
      <category>#personalweb</category>
      <category>#internet</category>
    </item>
    <item>
      <title>Starter Packs with OPML and RSS</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/rss-opml-starter-packs?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/rss-opml-starter-packs&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;One of the things I like about Bluesky is the &lt;a href="https://bsky.social/about/blog/06-26-2024-starter-packs"&gt;Starter Pack feature&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In a gist, a Starter Pack is a collection of feeds.&lt;/p&gt;
&lt;p&gt;Bluesky users can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create starter packs&lt;/li&gt;
&lt;li&gt;Share starter packs&lt;/li&gt;
&lt;li&gt;Subscribe to starter packs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unfortunately, Starter Packs are limited to Bluesky.&lt;/p&gt;
&lt;p&gt;Or are they?&lt;/p&gt;
&lt;p&gt;As mentioned, starter packs are a collection of feeds that others can create, share, and subscribe to.&lt;/p&gt;
&lt;p&gt;&lt;a href="/feed/bluesky-rss-support"&gt;Bluesky supports RSS&lt;/a&gt;, which means you could organize the feeds using an OPML file that you can share with others and others can subscribe to. The benefits of this is, you can continue to keep up with activity on Bluesky from the feed reader of your choice without being required to have an account on Bluesky.&lt;/p&gt;
&lt;p&gt;More importantly, because RSS and OPML are open standards, you're not limited to building starter packs for Bluesky. You can create, share, and subscribe to starter packs for any platform that supports RSS. That includes blogs, podcasts, forums, YouTube, Mastodon, etc. &lt;a href="https://www.manton.org/2024/12/07/with-the-success.html"&gt;Manton seems to have something similar in mind&lt;/a&gt; as a means of building on open standards that make it easy for Micro.blog to interop with various platforms.&lt;/p&gt;
&lt;p&gt;If you're interested in what that might look like in practice, check out my &amp;quot;starter packs&amp;quot; which you can subscribe to using your RSS reader of choice and the provided OPML files.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.lqdev.me/feed/blogroll"&gt;Blogs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.lqdev.me/feed/podroll"&gt;Podcasts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.lqdev.me/feed/youtube"&gt;YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.lqdev.me/feed/forums"&gt;Forums&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'm still working on similar collections for Mastodon and Bluesky but the same concept applies.&lt;/p&gt;
&lt;p&gt;Although these are just simple examples, it shows the importance of building on open standards and the open web. Doing so introduces more freedom for creators and communities.&lt;/p&gt;
&lt;p&gt;Here are other &amp;quot;starter packs&amp;quot; you might consider subscribing to.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.thingelstad.com/2024/12/03/people-blogs-list.html"&gt;People and Blogs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blogroll.club/faq/"&gt;Blogroll Club&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If this is interesting to you, &lt;a href="https://feedland.org/"&gt;Feedland&lt;/a&gt; might be a project worth checking out.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/rss-opml-starter-packs?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/rss-opml-starter-packs</guid>
      <pubDate>2024-12-08 19:40</pubDate>
      <category>#rss</category>
      <category>#opml</category>
      <category>#indieweb</category>
      <category>#starterpacks</category>
      <category>#community</category>
      <category>#bluesky</category>
      <category>#socialmedia</category>
      <category>#openweb</category>
      <category>#protocols</category>
      <category>#openstandards</category>
      <category>#feedland</category>
    </item>
    <item>
      <title>Digitize Analog Bookmarks using AI, .NET, and GitHub Models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/digitize-analog-bookmarks-with-ai-dotnet?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/digitize-analog-bookmarks-with-ai-dotnet&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This past year I've made more of an effort to read. I track books in my &lt;a href="/library"&gt;/library&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;Although I have a &lt;a href="https://shop.boox.com/products/novaair2"&gt;Nova Air 2 E-Note device&lt;/a&gt;, given the choice, I prefer physical books. Despite the conveniene of an electronic note-taking device, theres something about analog that I find hard to quit.&lt;/p&gt;
&lt;p&gt;As I read books, especially non-fiction, I annotate them in various ways. Eventually, those annotations make their way into my library page.&lt;/p&gt;
&lt;p&gt;Here's an example of those annotations for &lt;a href="/library/building-a-second-brain"&gt;Building A Second Brain by Thiago Forte&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The process of transferring notes is manual and tedious. I don't always have the discipline to transfer them at periodic intervals and what ends up happening is, I get to the end of the book without transferring any notes. To make space for new books, I donate it or resell the ones I've read. If I didn't take the time to transfer those notes, they're gone. &lt;a href="/library/slow-productivity-newport"&gt;Slow Productivity&lt;/a&gt; is an example of that.&lt;/p&gt;
&lt;p&gt;I want to find a better system that's low-maintenance for keeping more of these notes and retaining knowledge I've found valuable.&lt;/p&gt;
&lt;p&gt;Then it hit me, why not use AI? I know I could use OCR or even some of the AI models of yesteryear. The challenge is, those systems are error prone and given I don't always have the motivation to transfer notes manually, I have even less motivation to build and maintain such a system.&lt;/p&gt;
&lt;p&gt;However, vision models have advanced significantly and when paired with language models, the barrier to entry for reasoning over image data has drastically decreased.&lt;/p&gt;
&lt;p&gt;That's what led to this post. In this post, I'll show how you can use AI models like GPT-4o Mini to extract the passages I've annotated in physical books from an image. I then format those passages in markdown to make them easy to directly copy and paste them onto the website.&lt;/p&gt;
&lt;p&gt;I know there's probably a ton of services that do this for you, but it's a lot more fun to build one from scratch. With that in mind, let's get started.&lt;/p&gt;
&lt;p&gt;You can find the source for the application in the &lt;a href="/github/AIBookmarks"&gt;AIBookmarks GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Alternatively, I've configured the repo to use GitHub CodeSpaces, so you can launch the application there as well.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://codespaces.new/lqdev/AIBookmarks"&gt;&lt;img src="https://github.com/codespaces/badge.svg" class="img-fluid" alt="Open in GitHub Codespaces" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Configure access to GitHub Models&lt;/h2&gt;
&lt;p&gt;I'll be using &lt;a href="https://github.blog/news-insights/product-news/introducing-github-models/"&gt;GitHub Models&lt;/a&gt; as my AI model provider. GitHub Models provides developers with access to a catalog of AI models such as GPT-4o, Llama 3.2, Phi 3.5, and many others. Best of all, it's completely free, &lt;a href="https://docs.github.com/en/github-models/prototyping-with-ai-models#rate-limits"&gt;though restrictions apply&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I chose GitHub Models for the following reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Zero installation. Although I really like Ollama and some of the other local AI model providers, I didn't want to fill precious hard-drive space with AI models.&lt;/li&gt;
&lt;li&gt;It's free! Since I'm just prototyping, even with their limited capacity, it should be enough to prove out whether my scenario is feasible.&lt;/li&gt;
&lt;li&gt;They provide access to multi-modal models such as GPT-4o, Llama 3.2 Vision, and Phi 3.5 Vision, which can reason over text and images, which is what I need.&lt;/li&gt;
&lt;li&gt;I'm one of the GPU poor. My &lt;a href="https://www.lenovo.com/us/en/p/laptops/thinkpad/thinkpadx1/x1-titanium-g1/22tp2x1x1t1?orgRef=https%253A%252F%252Fduckduckgo.com%252F"&gt;Lenovo Thinkpad X1&lt;/a&gt; couldn't handle running one of the vision models.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Getting set up with GitHub models is fairly easy. At minimum, it requires:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A GitHub Account.&lt;/li&gt;
&lt;li&gt;A Personal Access Token. For more details, see the &lt;a href="https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens"&gt;GitHub documentation&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once you have both of those, you can get started building your application.&lt;/p&gt;
&lt;h2&gt;Create a client&lt;/h2&gt;
&lt;p&gt;The sample application is a C# console application which targets .NET 9. However, the code shown here should work on the latest LTS version as well.&lt;/p&gt;
&lt;p&gt;For this solution, I use the Azure AI Inference implementation of &lt;a href="https://www.nuget.org/packages/Microsoft.Extensions.AI.AzureAIInference/"&gt;Microsoft.Extensions.AI&lt;/a&gt;. &lt;a href="https://devblogs.microsoft.com/dotnet/introducing-microsoft-extensions-ai-preview/"&gt;Microsoft.Extensions.AI&lt;/a&gt;, M.E.AI for short, is a set of abstractions that provides a common set of interfaces for interacting with AI models.&lt;/p&gt;
&lt;p&gt;Although we're using the Azure AI Inference client SDK implementation of M.E.AI, we can still use it to work with models from the GitHub Model catalog.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;IChatClient client =
    new ChatCompletionsClient(
        endpoint: new Uri(&amp;quot;https://models.inference.ai.azure.com&amp;quot;), 
        new AzureKeyCredential(Environment.GetEnvironmentVariable(&amp;quot;GITHUB_TOKEN&amp;quot;)))
        .AsChatClient(&amp;quot;gpt-4o-mini&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Load your images&lt;/h2&gt;
&lt;p&gt;In the repo, I have a set of sample images containing annotations. These are the images I'll send to the AI model for processing.&lt;/p&gt;
&lt;p&gt;The following is an sample from &lt;a href="/library/creative-act-way-of-being-rubin"&gt;The Creative Act by Rick Rubin&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/lqdev/AIBookmarks/raw/main/data/creative-act-1.jpg" class="img-fluid" alt="An image of a book with pencil markings" /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Load the files&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var filePaths = Directory.GetFiles(&amp;quot;data&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a collection to store extracted passages&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var passages = new List&amp;lt;AIBookmark&amp;gt;();
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Process the images&lt;/h2&gt;
&lt;p&gt;Once you have the images loaded, it's time to proces them.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Start by setting the system prompt. This will provide the initial guidance for the extraction task.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var systemPrompt = 
    &amp;quot;&amp;quot;&amp;quot;
    You are an AI assistant that extracts underlined, highlighted, and marked passages from book page images.

    When passages have a natural continuation between pages, merge them and assign the page number where the first passage starts.
    &amp;quot;&amp;quot;&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then, iterate over each of the images and process them.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;foreach(var path in filePaths)
{
    var file = await File.ReadAllBytesAsync(path);
    var messages = new List&amp;lt;ChatMessage&amp;gt;
    {
        new ChatMessage(ChatRole.System, systemPrompt),
        new ChatMessage(ChatRole.User, new AIContent[] {
            new ImageContent(file, &amp;quot;image/jpeg&amp;quot;),
            new TextContent(&amp;quot;Extract the marked passages from the image&amp;quot;),
        })
    };

    var response = await client.CompleteAsync&amp;lt;List&amp;lt;AIBookmark&amp;gt;&amp;gt;(messages, options: new ChatOptions {Temperature = 0.1f});

    passages.AddRange(response.Result);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Loads the image as a &lt;code&gt;byte[]&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Composes a set of messages that include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The system prompt&lt;/li&gt;
&lt;li&gt;The image&lt;/li&gt;
&lt;li&gt;The user prompt instructing the model to extract the marked passages from the image&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sends the messages to the model for processing and returns a list of &lt;code&gt;AIBookmark&lt;/code&gt;. An &lt;code&gt;AIBookmark&lt;/code&gt; is a custom class I've defined as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;class AIBookmark
{
    public string Text {get;set;}

    public int PageNumber {get;set;}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some of the more recent AI models support structured output, which enforce a schema on AI model outputs. You can take a look at the &lt;a href="https://openai.com/index/introducing-structured-outputs-in-the-api/"&gt;OpenAI documentation&lt;/a&gt; for more details. It's important to note though that the functionality is not exclusive to OpenAI models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Adds the extracted passages to the &lt;code&gt;passages&lt;/code&gt; collection.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Format the results&lt;/h2&gt;
&lt;p&gt;Once all of the files are processed by the AI model, additional processing is done to ensure that they're in the correct page order as well as formatted as markdown blockquotes.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var sortedPassages = 
    passages
        .OrderBy(p =&amp;gt; p.PageNumber)
        .Select(p =&amp;gt; $&amp;quot;&amp;gt; {p.Text} (pg. {p.PageNumber})&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Display the results&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;foreach(var passage in sortedPassages)
{
    Console.WriteLine(passage);
    Console.WriteLine(&amp;quot;&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the images included in the repo's &lt;em&gt;data&lt;/em&gt; directory, output might look like the following.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-markdown"&gt;&amp;gt; This isn’t a matter of blind belief in yourself. It’s a matter of experimental faith. (pg. 278)

&amp;gt; When we don’t yet know where we’re going, we don’t wait. We move forward in the dark. If nothing we attempt yields progress, we rely on belief and will. We may take several steps backward in the sequence to move ahead. (pg. 278)

&amp;gt; If we try ten experiments and none of them work; we have a choice. We can take it personally, and think of ourselves as a failure and question our ability to solve the problem. (pg. 278)

&amp;gt; Staying in it means a commitment to remain open to what’s around you. Paying attention and listening. Looking for connections and relationships in the outside world. Searching for beauty. Seeking stories. Noticing what you find interesting, what makes you lean forward. And knowing all of this is available to use next time you sit down to work, where the raw data gets put into form. (pg. 296)

&amp;gt; Just as a surfer can’t control the waves, artists are at the mercy of the creative rhythms of nature. This is why it’s of such great importance to remain aware and present at all times. Watching and waiting. (pg. 296)

&amp;gt; Maybe the best idea is the one you’re going to come up with this evening. (pg. 297)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Improvements and next steps&lt;/h2&gt;
&lt;p&gt;Putting the application together took me less than an hour so this is far from done. However, it does provide me with a starting point and offers validation that this could be a way to more easily capture the knowledge I'm curating from physical books.&lt;/p&gt;
&lt;p&gt;Some improvements I can make here:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Update the system prompt with some samples to help guide the extraction. For example, in the output I shared, that last passage is not annotated. It just happens to be a mostly blank page with that quote in the center, therefore giving the illusion that the passage is important in some way.&lt;/li&gt;
&lt;li&gt;Add additional information to the AIBookmark class like &lt;code&gt;Index&lt;/code&gt; so I can ensure order within a page is preserved. Right now page number is good enough, but I can't guarantee the correct order. An index property might help here.&lt;/li&gt;
&lt;li&gt;Use a service with higher rate limits. The current rate limits wouldn't allow me to process a large number of images at once. Therefore, I'd need to use a service with higher limits. Alternatively, I could make this a job that runs in the background which abides by the rate limits but I also don't have to spend money on. Given I'm not using this for anything mission-critical, that'd be an acceptable solution as well.&lt;/li&gt;
&lt;li&gt;Refactor the solution so I can more easily swap between tasks. For example, sometimes I might want to use it with images from my bullet journal. Other times, I might want to use it with handwritten notes. Whatever the case may be, it'd be good to not have to rewrite the prompts every time.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Some ways I see myself using this project:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Periodically collect images of annotated pages and save them to cloud storage.&lt;/li&gt;
&lt;li&gt;When I'm done with the book, drop all the images in the &lt;em&gt;data&lt;/em&gt; directory.&lt;/li&gt;
&lt;li&gt;Further enrich data by condensing repetitive passages and extracting key concepts.&lt;/li&gt;
&lt;li&gt;Storing this knowledge into some sort of knowledge store to make it actionable.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Just for fun, Tyler released all of the lyrics to his &lt;a href="/feed/chromakopia-tyler-the-creator-released"&gt;latest album&lt;/a&gt; as &lt;a href="https://twitter.com/tylerthecreator/status/1852105825650708651/photo/1"&gt;images on X&lt;/a&gt;. With a few tweaks, I was able to repurpose this solution to extract the text from them and that worked relatively well.&lt;/p&gt;
&lt;p&gt;Just with this simple solution, there's a ton of other applications I can think of in my daily life to help bridge my analog and digital lives.&lt;/p&gt;
&lt;p&gt;What other use cases do you see yourself using something like this for? &lt;a href="/contact"&gt;Let me know&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/digitize-analog-bookmarks-with-ai-dotnet?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/digitize-analog-bookmarks-with-ai-dotnet</guid>
      <pubDate>2024-10-31 21:41 -05:00</pubDate>
      <category>#ai</category>
      <category>#dotnet</category>
      <category>#github</category>
      <category>#reading</category>
      <category>#bookmarks</category>
      <category>#analog</category>
      <category>#digital</category>
      <category>#reading</category>
      <category>#notes</category>
    </item>
    <item>
      <title>Build your own self-hosted live streaming application with Owncast and .NET Aspire</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;Platforms come and go. As a result, I'm a strong advocate for &lt;a href="https://indieweb.org/own_your_data"&gt;owning your data&lt;/a&gt; (when possible). Self-hosting is a way for you to do that. Owning your data could mean self-hosting &lt;a href="https://buttondown.com/ownyourweb/archive/issue-06/"&gt;your website&lt;/a&gt;, &lt;a href="https://bitwarden.com/blog/host-your-own-open-source-password-manager/"&gt;password manager&lt;/a&gt;, &lt;a href="https://jellyfin.org/docs/"&gt;media server&lt;/a&gt;, or &lt;a href="https://docs.joinmastodon.org/user/run-your-own/"&gt;social media&lt;/a&gt;. However, self-hosting comes with its own challenges, primarily cost (time and money) and in cases where software isn't provided as an appliance on your hosting provider of choice, some technical knowledge may be required.&lt;/p&gt;
&lt;p&gt;As I got more into self-hosting applications such as my &lt;a href="/mastodon"&gt;Mastodon instance&lt;/a&gt;, I came across Owncast.&lt;/p&gt;
&lt;p&gt;During peak COVID lockdowns, like many others, live streaming is one of the ways I passed the time. While Twitch and YouTube got the job done, self-hosting was always in the back of my mind.&lt;/p&gt;
&lt;p&gt;It's been a while since I've live-streamed, so I never really went through the process of evaluating the self-hosted route with Owncast.&lt;/p&gt;
&lt;p&gt;While browsing for something on YouTube the other day, I ran into some of my &lt;a href="https://www.youtube.com/playlist?list=PLsdMoYmuvh9ZtgB8U7FECR_8wKMYXJNAm"&gt;old live-stream recordings&lt;/a&gt;. This got me thinking again, how difficult would it be to put together my own live-streaming setup.&lt;/p&gt;
&lt;p&gt;This post is the result of that exploration.&lt;/p&gt;
&lt;p&gt;In this post, I'll modify a .NET Aspire Starter Application template and show how to set up a self-hosted live-streaming application using Owncast and .NET Aspire.&lt;/p&gt;
&lt;p&gt;You can find the source code in the &lt;a href="/github/BYOwncastAspire"&gt;lqdev/BYOwncastAspire&lt;/a&gt; repository.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/bdd9f901-8d8f-45be-9e37-4dce8459e481" class="img-fluid" alt="BYOwnCastAspire Web Frontend" /&gt;&lt;/p&gt;
&lt;h2&gt;What is Owncast?&lt;/h2&gt;
&lt;p&gt;The Owncast website describes the project as, &amp;quot;...a free and open source live video and web chat server for use with existing popular broadcasting software.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/769ec2d8-a3d1-4ec3-8271-c2de6e11ddd3" class="img-fluid" alt="Owncast admin server page" /&gt;&lt;/p&gt;
&lt;p&gt;It goes on to further describe some of the reasons I like Owncast, which are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Self-hosting&lt;/strong&gt; - I'm in complete control over the service and my data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open-source&lt;/strong&gt; - I can freely use and contribute to the project. Free in this case meaning both as in freedom and pizza.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Builds on top of open standards like RMTP&lt;/strong&gt; - Software that supports the Real-Time Messaging Protocol (RMTP) like OBS can immediately be leveraged.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fediverse compatible&lt;/strong&gt; - Your content and network federated across &lt;a href="https://joinfediverse.wiki/What_is_the_Fediverse%3F"&gt;the Fediverse&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To learn more, check out the &lt;a href="https://owncast.online/"&gt;Owncast website&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;What is .NET Aspire?&lt;/h2&gt;
&lt;p&gt;The .NET Aspire documentation describes it as, &amp;quot;...an opinionated, cloud ready stack for building observable, production ready, distributed applications.​ .NET Aspire is delivered through a collection of NuGet packages that handle specific cloud-native concerns.&amp;quot;&lt;/p&gt;
&lt;p&gt;Personally, the parts of .NET Aspire that matter to me are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;App composition&lt;/strong&gt; - The .NET Aspire programming model makes it easy to define and compose a variety of resources such as .NET projects, containers, and much more in a single place. In many cases, for commonly used services, these resources are exposed in the form of &lt;a href="https://learn.microsoft.com/dotnet/aspire/fundamentals/integrations-overview"&gt;integrations&lt;/a&gt;. These integrations further simplify the composition of applications.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tooling (Dashboard)&lt;/strong&gt; - .NET Aspire provides a set of tools and templates. However, my favorite is the dashboard. The dashboard provides me with a single place to view my resources, their configurations, and logs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although in this post, I don't cover deployment, there is also the provisioning component provided by .NET Aspire which in many cases can &lt;a href="https://learn.microsoft.com/dotnet/aspire/deployment/overview"&gt;simplify your application deployments&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To learn more, check out the &lt;a href="https://learn.microsoft.com/dotnet/aspire/get-started/aspire-overview"&gt;.NET Aspire documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Build your application&lt;/h2&gt;
&lt;p&gt;This application makes a few modifications to the &lt;a href="https://learn.microsoft.com/dotnet/aspire/fundamentals/setup-tooling?tabs=linux&amp;amp;pivots=dotnet-cli#net-aspire-project-templates"&gt;.NET Aspire Starter Application template&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The application consists of a few projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;BYOwncastAspire.AppHost&lt;/strong&gt; - This project is the entrypoint for .NET Aspire applications. This is where we'll configure the Owncast server as a container resource.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BYOwncastAspire.Web&lt;/strong&gt; - Blazor web application. Although Owncast provides its own page where viewers can tune into your stream, by having a separate web application, preferably your own personal website, you can further enrich and customize how and where you publish content.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BYOwncastAspire.ServiceDefaults&lt;/strong&gt; - This project contains default configurations for telemetry, health checks, resiliency, etc. No changes or updates required here.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Configure Owncast&lt;/h3&gt;
&lt;p&gt;There are many ways to host an Owncast server, with one of them being a container.&lt;/p&gt;
&lt;p&gt;In the context of Aspire which has built-in container support, you can easily add Owncast as a resource in your application.&lt;/p&gt;
&lt;p&gt;From the &lt;a href="https://owncast.online/quickstart/container/"&gt;Owncast documentation&lt;/a&gt;, once you've pulled the Owncast container image, you can start it with the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -v `pwd`/data:/app/data -p 8080:8080 -p 1935:1935 owncast/owncast:latest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This translates to the following in the &lt;em&gt;Program.cs&lt;/em&gt; of the &lt;em&gt;BYOwncastAspire.AppHost&lt;/em&gt; project.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var owncast = builder
    .AddContainer(name: &amp;quot;owncast&amp;quot;, image:&amp;quot;owncast/owncast&amp;quot;)
    .WithBindMount(&amp;quot;./data&amp;quot;,&amp;quot;/app/data&amp;quot;)
    .WithHttpEndpoint(port:8080,targetPort:8080,name:&amp;quot;admin&amp;quot;)
    .WithHttpEndpoint(port:1935,targetPort:1935,name:&amp;quot;streaming&amp;quot;)
    .WithExternalHttpEndpoints();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adds the owncast container image&lt;/li&gt;
&lt;li&gt;Mounts a local &lt;em&gt;data&lt;/em&gt; directory to the &lt;em&gt;/app/data&lt;/em&gt; directory in the container&lt;/li&gt;
&lt;li&gt;Maps port &lt;code&gt;8080&lt;/code&gt; for the Owncast admin server and &lt;code&gt;1935&lt;/code&gt; for RMTP server.&lt;/li&gt;
&lt;li&gt;Exposes the endpoints publicly&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Embed your stream&lt;/h3&gt;
&lt;p&gt;Owncast provides the ability to &lt;a href="https://owncast.online/docs/embed/"&gt;embed your video stream onto a website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Although we don't need a frontend because one is already provided by Owncast, by embedding your stream on your website you can provide a single place for your viewers to consume your content.&lt;/p&gt;
&lt;p&gt;In this case, we can treat the &lt;em&gt;BYOwncastAspire.Web&lt;/em&gt; project as our website.&lt;/p&gt;
&lt;p&gt;To embed your stream to the website, add the following code to your &lt;em&gt;Home.razor&lt;/em&gt; page.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;&amp;lt;iframe
  src=&amp;quot;http://localhost:8080/embed/video&amp;quot;
  title=&amp;quot;Owncast&amp;quot;
  height=&amp;quot;350px&amp;quot; width=&amp;quot;550px&amp;quot;
  referrerpolicy=&amp;quot;origin&amp;quot;
  allowfullscreen&amp;gt;
&amp;lt;/iframe&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, we're pointing to the endpoint listening on port &lt;code&gt;8080&lt;/code&gt; of our &lt;code&gt;localhost&lt;/code&gt;. When you deploy the application, you'd replace &lt;code&gt;src&lt;/code&gt; with your domain.&lt;/p&gt;
&lt;h2&gt;Start your application&lt;/h2&gt;
&lt;p&gt;That's all there is in terms of configuration.&lt;/p&gt;
&lt;p&gt;To start the application:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open the terminal&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Navigate to the &lt;em&gt;BYOwncastAspire.AppHost&lt;/em&gt; project and run the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet run
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This will launch you into the .NET Aspire dashboard. At this point, you can further customize your Owncast server as well as the website.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/c20d84d8-925a-4f80-9058-622466cb08e9" class="img-fluid" alt="BYOwncastAspire .NET Aspire Dashboard Resource Page" /&gt;&lt;/p&gt;
&lt;h2&gt;What next?&lt;/h2&gt;
&lt;p&gt;Now that you have your application running, in its current form, this application is only meant to serve as a sample of what you can do with Owncast and .NET Aspire.&lt;/p&gt;
&lt;p&gt;Some next steps might include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://owncast.online/docs/configuration/"&gt;Change your admin password and customize your Owncast server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://owncast.online/docs/storage/"&gt;Configure object storage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://owncast.online/docs/broadcasting/obs/"&gt;Set up your steaming software&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://learn.microsoft.com/dotnet/aspire/deployment/azure/aca-deployment"&gt;Deploy your application&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://owncast.online/thirdparty/"&gt;Extend the app's functionality by building custom plugins and extensions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The plugins and extensions are particularly interesting to me because there may even be opportunities to experiment and insert AI capabilities at various layers such as moderation, translation, accessibility, show notes, chat summaries, etc.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I showed how you can combine the powerful live-steaming features of Owncast with the app orchestration and tooling in .NET Aspire to create rich self-hosted live-steaming applications.&lt;/p&gt;
&lt;p&gt;If you use this sample as a starting point for your own live-streaming or self-hosting explorations, &lt;a href="/contact"&gt;send me a message&lt;/a&gt;. I'd love to hear about it.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire</guid>
      <pubDate>2024-10-20 14:01</pubDate>
      <category>#owncast</category>
      <category>#dotnet</category>
      <category>#indieweb</category>
      <category>#fediverse</category>
      <category>#aspire</category>
      <category>#livestreaming</category>
      <category>#twitch</category>
      <category>#youtube</category>
      <category>#blazor</category>
    </item>
    <item>
      <title>Configure Ollama on Dev Containers and VS Code</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/install-ollama-vscode-devcontainer?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/install-ollama-vscode-devcontainer&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;The source for my website is &lt;a href="/github/luisquintanilla.me"&gt;hosted on GitHub&lt;/a&gt;. As a result, I use VS Code as the text editor to author my posts. Typically, I use &lt;a href="/colophon"&gt;Codespaces or github.dev&lt;/a&gt; to quickly draft and publish articles to simplify the process.&lt;/p&gt;
&lt;p&gt;As part of my authoring and publishing posts, there's a few things I do like:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Coming up with a title&lt;/li&gt;
&lt;li&gt;Creating relevant tags to help with discovery and connecting related information on my website.&lt;/li&gt;
&lt;li&gt;For long-form blog posts, I also include a description.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is generally relatively easy to do. However, sometimes I spend too much time coming up with something that truly summarizes and condenses the information in the post. It'd be nice if I had an assistant to help me brainstorm and workshop some of these items.&lt;/p&gt;
&lt;p&gt;In comes AI. Now, I could use something like &lt;a href="https://code.visualstudio.com/docs/copilot/overview"&gt;Copilot&lt;/a&gt; which would work wonderfully and easily plug into my workflow. However, my website is a labor of love and I don't make any money from it. In many instances, I've designed &lt;a href="/posts/receive-webmentions-fsharp-az-functions-fsadvent"&gt;various components to be as low-cost as possible&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Recently, I created a post which showed &lt;a href="/posts/getting-started-ollama-windows"&gt;how to get started with Ollama on Windows&lt;/a&gt;. In this post, I'll show how you can do the same for your Dev Container environments.&lt;/p&gt;
&lt;h2&gt;Install Ollama&lt;/h2&gt;
&lt;p&gt;Assuming you already have a &lt;a href="https://code.visualstudio.com/docs/devcontainers/create-dev-container"&gt;Dev Container configuration file&lt;/a&gt;, add the following line to it:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&amp;quot;postCreateCommand&amp;quot;: &amp;quot;curl -fsSL https://ollama.com/install.sh | sh&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command triggers when the container environment is created. This will install Ollama in your development environment. For more details, see the &lt;a href="https://ollama.com/download/linux"&gt;Ollama download instructions&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Start Ollama&lt;/h2&gt;
&lt;p&gt;Now that Ollama is installed, it's time to start the service so you can get models and use them. To start the Ollama service on your Dev Conatiner environment, add the following line to your Dev Container configuration file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&amp;quot;postStartCommand&amp;quot;: &amp;quot;ollama serve&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command will run &lt;code&gt;ollama serve&lt;/code&gt; when the Dev Container environment starts.&lt;/p&gt;
&lt;h2&gt;Pull a model&lt;/h2&gt;
&lt;p&gt;To use Ollama, you're going to need a model. In my case, I went with &lt;a href="https://ollama.com/library/phi"&gt;Phi-2&lt;/a&gt; because it's lightweight and space on Codespaces is limited.&lt;/p&gt;
&lt;p&gt;To get the model:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open the terminal&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Enter the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama pull phi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After a few minutes, the model is downloaded and ready to use.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Enter the following command to ensure that your model is now available&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result should look like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;NAME            ID              SIZE    MODIFIED      
phi:latest      e2fd6321a5fe    1.6 GB  6 seconds ago
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more details, see the &lt;a href="https://ollama.com/library"&gt;Ollama model library&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I showed how you can configure Ollama with Dev Containers to use AI models locally in your projects. In subsequent posts, I'll show how once the service is running, I use it as part of my authoring and publishing posts. You can see a preview of it in my &lt;a href="https://github.com/lqdev/luisquintanilla.me/blob/main/Scripts/ai.fsx"&gt;scripts directory&lt;/a&gt;. Happy coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/install-ollama-vscode-devcontainer?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/install-ollama-vscode-devcontainer</guid>
      <pubDate>2024-03-06 13:50</pubDate>
      <category>#ollama</category>
      <category>#vscode</category>
      <category>#devcontainer</category>
      <category>#ai</category>
      <category>#llm</category>
      <category>#opensource</category>
      <category>#development</category>
    </item>
    <item>
      <title>Getting started with Ollama on Windows</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/getting-started-ollama-windows?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/getting-started-ollama-windows&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;Recently &lt;a href="/feed/ollama-windows-preview"&gt;Ollama announced support for Windows&lt;/a&gt; in preview. In doing so, people who want to use AI models like Llama, Phi, and many others can do so locally on their PC. In this post, I'll go over how you can get started with Ollama on Windows.&lt;/p&gt;
&lt;h2&gt;Install Ollama&lt;/h2&gt;
&lt;p&gt;The first thing you'll want to do is install Ollama.&lt;/p&gt;
&lt;p&gt;You can do so by &lt;a href="https://ollama.com/download/windows"&gt;downloading the installer from the website&lt;/a&gt; and following the installation prompts.&lt;/p&gt;
&lt;h2&gt;Get a model&lt;/h2&gt;
&lt;p&gt;Once you've installed Ollama, it's time to get a model.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open PowerShell&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the following command&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;ollama pull llama2
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this case, I'm using llama2. However, you can choose another model. You could even download many models at once and switch between them. For a full list of supported models, see the &lt;a href="https://ollama.com/library"&gt;Ollama model documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Use the model&lt;/h2&gt;
&lt;p&gt;Now that you have the model, it's time to use it. The easiest way to use the model is using the REST API. When you install Ollama, it starts up a server to host your model. One other neat thing is, the REST API is &lt;a href="https://ollama.com/blog/openai-compatibility"&gt;OpenAI API compatible&lt;/a&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open PowerShell&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Send the following request:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;(Invoke-WebRequest -method POST -Body '{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;, &amp;quot;prompt&amp;quot;:&amp;quot;Why is the sky blue?&amp;quot;, &amp;quot;stream&amp;quot;: false}' -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command will issue an HTTP POST request to the server listening on port 11434.&lt;/p&gt;
&lt;p&gt;The main things to highlight in the body:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;model&lt;/em&gt;: The model you'll use. Make sure this is one of the models you pulled.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;prompt&lt;/em&gt;: The input to the model&lt;/li&gt;
&lt;li&gt;&lt;em&gt;stream&lt;/em&gt;: Whether to stream responses back to the client&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more details on the REST API, see the &lt;a href="https://github.com/ollama/ollama/blob/main/docs/api.md"&gt;Ollama REST API documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I went over how you can quickly install Ollama to start using generative AI models like Llama and Phi locally on your Windows PC. If you use Mac or Linux, you can perform similar steps as those outlined in this guide to get started on those operating systems. Happy coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/getting-started-ollama-windows?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/getting-started-ollama-windows</guid>
      <pubDate>2024-03-05 10:32</pubDate>
      <category>#ai</category>
      <category>#ollama</category>
      <category>#windows</category>
      <category>#llm</category>
      <category>#opensource</category>
      <category>#llama</category>
      <category>#openai</category>
      <category>#generativeai</category>
      <category>#genai</category>
    </item>
    <item>
      <title>Setting up your Rust development environment using Dev Containers</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/setting-up-rust-dev-env-devcontainers-vscode?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/setting-up-rust-dev-env-devcontainers-vscode&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;Setting up a new development environment that's configured with all the SDKs and tools you need to get started working can be a multi-step process that is error-prone. Especially when you don't know whether you want to commit to a specific technology or don't have too much space on your device, you might not want to install everything on your device. Additionally, cleaning up and removing everything you installed isn't always 100%. This is where development environments like Dev Containers can come in handy.&lt;/p&gt;
&lt;h2&gt;What are Dev Containers&lt;/h2&gt;
&lt;p&gt;A development container (Dev Container) lets you use a container as a full development environment.&lt;/p&gt;
&lt;p&gt;Because your development environment is in a container, the environment is isolated. The builds are reproducible and disposable. This means you can quickly stand up a new clean development environments and dispose of them just as easily.&lt;/p&gt;
&lt;p&gt;For more details, see the &lt;a href="https://containers.dev/"&gt;Dev Containers website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you have Docker installed, you can run them locally. Otherwise, you can run Dev Containers using GitHub Codespaces&lt;/p&gt;
&lt;h2&gt;What are GitHub Codespaces&lt;/h2&gt;
&lt;p&gt;A codespace is a development environment that's hosted in the cloud.&lt;/p&gt;
&lt;p&gt;For more details, see the &lt;a href="https://github.com/features/codespaces"&gt;GitHub Codespaces website&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Set up Dev Container for Rust Development&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a new directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add a Dev Container configuration file called &lt;em&gt;.devcontainer.json&lt;/em&gt; that contains the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;Rust Development Environment&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/rust:latest&amp;quot;,
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;rust-lang.rust-analyzer&amp;quot;
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;name&lt;/strong&gt;: A human friendly name to identify your development environment. It's not as important when you only have one environment but if you have different environments, it can be useful for differentiating between the various configurations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;image&lt;/strong&gt;: For the most customization, you can provide the path to a Docker file. However, to make things easier, there are a set of prebuilt container images. In this case, I'm using the &lt;a href="https://github.com/devcontainers/images/tree/main/src/rust"&gt;Rust image&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;customizations.vscode.extensions&lt;/strong&gt;: A list of Visual Studio Code extensions that can help you with your development. In this case, it's the &lt;a href="https://marketplace.visualstudio.com/items?itemName=rust-lang.rust-analyzer"&gt;&lt;em&gt;rust-analyzer&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At minimum, this is all you need to get started.&lt;/p&gt;
&lt;h2&gt;Start your environment&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://code.visualstudio.com/docs/devcontainers/tutorial"&gt;Visual Studio Code provides good support for Dev Containers&lt;/a&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install &lt;a href="https://code.visualstudio.com/download"&gt;VS Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open your directory where you configured your Dev Container in Visual Studio Code&lt;/li&gt;
&lt;li&gt;Install the &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers"&gt;Dev Containers VS Code extension&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Open the command palette in VS Code. In the menu bar, select &lt;strong&gt;View &amp;gt; Command Palette.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Enter the following command &lt;strong&gt;&amp;gt;Dev Containers: Open Dev Container&lt;/strong&gt;. This will start building your dev container. Wait for it to start.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Check your environment&lt;/h2&gt;
&lt;p&gt;Once your Dev Container starts, check that &lt;code&gt;rustc&lt;/code&gt; is installed.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Open the terminal.&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;rustc -h&lt;/code&gt;. A help message should output to the console for the Rust compiler.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Build your app&lt;/h2&gt;
&lt;p&gt;For simplicity, I've used the &lt;a href="https://doc.rust-lang.org/rust-by-example/hello.html"&gt;Hello World sample from the Rust By Example website&lt;/a&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a new file called &lt;em&gt;hello.rs&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn main ()
{
    println!(&amp;quot;Hello World!&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compile your program&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;rustc hello.rs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will generate an executable called &lt;em&gt;hello&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run your app&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;./hello
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The application should print out the string &amp;quot;Hello World&amp;quot; to the console.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That's it! You now have a Rust development environment to get started learning and building apps.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Dev Containers make it easy for you to configure full-featured development environments. When paired with Visual Studio Code and GitHub Codespaces, they can help you focus on learning and building applications rather than setting up your environment. You can find a &lt;a href="https://github.com/lqdev/rust-codespace-sandbox"&gt;final version of the configured environment and app on GitHub&lt;/a&gt;. Happy coding!&lt;/p&gt;
&lt;h2&gt;Additional Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://code.visualstudio.com/docs/languages/rust"&gt;Rust in Visual Studio Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://doc.rust-lang.org/rust-by-example/index.html"&gt;Rust by Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://code.visualstudio.com/docs/devcontainers/containers"&gt;Developing Inside a Container&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/setting-up-rust-dev-env-devcontainers-vscode?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/setting-up-rust-dev-env-devcontainers-vscode</guid>
      <pubDate>2024-01-31 21:05</pubDate>
      <category>#rust</category>
      <category>#vscode</category>
      <category>#devcontainers</category>
      <category>#codespaces</category>
      <category>#programming</category>
      <category>#technology</category>
      <category>#docker</category>
      <category>#containers</category>
      <category>#github</category>
    </item>
    <item>
      <title>POSSE to Mastodon using RSS and Azure Logic Apps</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/rss-to-mastodon-posse-azure-logic-apps?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/rss-to-mastodon-posse-azure-logic-apps&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="/feed/weblogging-rewind-2023"&gt;Over the past year, I've made this website the main place where I post content&lt;/a&gt;. The easiest way to &lt;a href="/subscribe"&gt;subscribe to content on my website&lt;/a&gt; is through the various RSS feeds. However, I have accounts on other platforms like X (formerly Twitter), Bluesky, and Mastodon where I'd still like to repost my content to. Since the &lt;a href="https://techcrunch.com/2023/03/29/twitter-announces-new-api-with-only-free-basic-and-enterprise-levels/"&gt;changes to the Twitter API&lt;/a&gt;, the only place I cross-post to is Mastodon. The main reason behind it is, as of now, it's one of the few platforms that allows me to automate post creation via its REST APIs without restrictions. A large part of that is I self-host my own Mastodon instance but I assume there aren't as many restrictions when using other instances like mastodon.social. The way I automate posting is by setting up workflows using Azure Logic Apps. These workflows subscribe to my various RSS feeds and whenever a new post is published, they make an HTTP request to my Mastodon instance to create a new post. I've been doing this for some time but never got around to documenting it. This blog post goes into more details about how to set up these workflows.&lt;/p&gt;
&lt;h2&gt;What is POSSE&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://indieweb.org/POSSE"&gt;POSSE&lt;/a&gt; is short for &amp;quot;Post on your Own Site Syndicate Elsewhere&amp;quot;. It's one of the patterns adopted within IndiWeb communities and projects. The main idea is, your website or a website you own or have administrative rights to becomes the main platform where you publish your content. Effectively, it's the cannonical version of your content. Once your content is on your website, you can optionally choose to distribute it on other platforms.&lt;/p&gt;
&lt;h2&gt;What is RSS&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/RSS"&gt;RSS&lt;/a&gt; is short for Really Simple Syndication. Accorting to Wikipedia, this protocol &amp;quot;allows users and applications to access updates to websites in a standardized, computer-readable format&amp;quot;.&lt;/p&gt;
&lt;h2&gt;What is Mastodon&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://joinmastodon.org/"&gt;Mastodon&lt;/a&gt; is an &lt;a href="https://github.com/mastodon/mastodon"&gt;open-source&lt;/a&gt; decentralized microblogging platform built on the &lt;a href="https://activitypub.rocks/"&gt;ActivityPub&lt;/a&gt; protocol and part of the larger collective of federated systems known as the &lt;a href="https://fediverse.info/"&gt;Fediverse&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;What are Azure Logic Apps&lt;/h2&gt;
&lt;p&gt;If you've used workflow automation systems like &lt;a href="https://ifttt.com/"&gt;IFTTT&lt;/a&gt;, then you generally know what Azure Logic Apps are.&lt;/p&gt;
&lt;p&gt;A more formal definition from the Azure documentation - &amp;quot;Azure Logic Apps is a cloud platform where you can create and run automated workflows with little to no code. By using the visual designer and selecting from prebuilt operations, you can quickly build a workflow that integrates and manages your apps, data, services, and systems.&amp;quot;&lt;/p&gt;
&lt;p&gt;For more details, see the &lt;a href="https://learn.microsoft.com/azure/logic-apps/logic-apps-overview"&gt;Azure Logic Apps documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Since you'll be creating Logic App Resources on Azure, you'll need an &lt;a href="https://aka.ms/free"&gt;Azure account&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Create Consumption Azure Logic App Resource&lt;/h2&gt;
&lt;p&gt;There's various ways to create an Azure Logic App Resource but the easiest one for this relatively simple workflow is using the Azure Portal. For more details, see the &lt;a href="https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#create-a-consumption-logic-app-resource"&gt;create a consumption logic app resource documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For the most part you can leave the defaults as is. Since the intended use for this workflow is personal and I don't need enterprise features, I chose to create my logic app using the consumption plan.&lt;/p&gt;
&lt;p&gt;When prompted to choose &lt;strong&gt;Plan Type&lt;/strong&gt;, select &lt;strong&gt;Consumption&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;The Workflow&lt;/h2&gt;
&lt;p&gt;The workflow is relatively simple. Whenever a new item is posted to an RSS feed, make an HTTP POST request to the Mastodon API&lt;/p&gt;
&lt;pre class="mermaid"&gt;flowchart TD
    A["RSS"] --&gt; B["HTTP (Mastodon API)"]
&lt;/pre&gt;
&lt;p&gt;Once your Logic App resource deploys to Azure, create a new logic app using the Blank Template. For more details, see the &lt;a href="https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#select-the-blank-template"&gt;select a blank template documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This will launch you into the Logic app designer UI where you can begin to configure your workflow.&lt;/p&gt;
&lt;h3&gt;RSS trigger&lt;/h3&gt;
&lt;p&gt;The first thing you'll want to do is set up the trigger that initiates the workflow. The trigger in this case will be new posts on an RSS feed.&lt;/p&gt;
&lt;p&gt;In the Logic app designer, search for &lt;em&gt;RSS&lt;/em&gt; and add it as a trigger.&lt;/p&gt;
&lt;p&gt;Once the RSS trigger is added to your workflow, configure it as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The RSS feed URL&lt;/strong&gt;: The feed you want want to get posts from. In this case, I'm using my blog posts &lt;a href="https://www.luisquintanilla.me/posts/index.xml"&gt;feed&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TIP: The RSS feed can be ANY feed. So if you wanted to subscribe and repost items from your favorite blog or website, you can as well. Just note though that if you don't own the content it might not be something the author wants you doing and for websites that update multiple times a day, it might produce a lot of noise in your feed. Other than that though, subscribe and post away!&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;How often do you want to check for items?&lt;/strong&gt;: The frequency at which you want to poll the RSS feed and check for new posts. In my case, I don't post very often but I do want my posts to be published on Mastodon within an hour of publishing. Therefore, I chose 1 hour as the update frequency.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;HTTP action&lt;/h3&gt;
&lt;p&gt;Now that your trigger is configured, it's time to do someting with the latest posts. In this case, since I want to create a new post on Mastodon, I can do so via their REST API.&lt;/p&gt;
&lt;h4&gt;Get Mastodon credentials&lt;/h4&gt;
&lt;p&gt;One thing that you'll need to publish posts to Mastodon is an application token. The easiest way to get one is using the Web UI.&lt;/p&gt;
&lt;p&gt;To get your app credentials through the web UI:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In the Mastodon Web UI, select &lt;strong&gt;Preferences&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;In the preferences page, select &lt;strong&gt;Development&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;In the development preferences tab, select &lt;strong&gt;New application&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;In the new application page:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Provide a name for your application&lt;/li&gt;
&lt;li&gt;Choose the &lt;strong&gt;write:statuses&lt;/strong&gt; scope checkbox. For details on required scopes to post statuses, see the &lt;a href="https://docs.joinmastodon.org/methods/statuses/#create"&gt;Post a new status REST API documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Select &lt;strong&gt;Submit&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If successful, this will create credentials and a token you can use to send authenticated requests to the Mastodon API.&lt;/p&gt;
&lt;h4&gt;Configure HTTP action&lt;/h4&gt;
&lt;p&gt;Now that you have your credentials, you can configure your HTTP action in the Logic Apps workflow.&lt;/p&gt;
&lt;p&gt;In the Logic App designer:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select &lt;strong&gt;+ New Step&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Search for &lt;em&gt;HTTP&lt;/em&gt; and add it as an action.&lt;/li&gt;
&lt;li&gt;Once the HTTP action is added, configure it as follows:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Method&lt;/strong&gt;: The HTTP Method. To create statuses, choose &lt;strong&gt;POST&lt;/strong&gt; from the dropdown.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;URI&lt;/strong&gt;: The endpoint to make the requests to. For mastodon, it's &lt;code&gt;https://&amp;lt;HOST&amp;gt;/api/v1/statuses&lt;/code&gt;. Make sure to replace &lt;code&gt;&amp;lt;HOST&amp;gt;&lt;/code&gt; with your Mastodon instance. In my case, it's &lt;code&gt;toot.lqdev.tech&lt;/code&gt; since that's where I host my mastodon instance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Headers&lt;/strong&gt;: HTTP Headers to use. In this case, set the following:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Key&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Content-Type&lt;/td&gt;
&lt;td&gt;application/x-www-form-urlencoded&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Queries&lt;/strong&gt;: URL Query Parameters to add to the request. This is where you'll set your access token.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Key&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;access_token&lt;/td&gt;
&lt;td&gt;&lt;code&gt;[YOUR-ACCESS-TOKEN]&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Make sure to replace &lt;code&gt;[YOUR-ACCESS-TOKEN]&lt;/code&gt; with the token credential generated in the Mastodon Web UI.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Body&lt;/strong&gt;: The content to be published in your Mastodon post. This is entirely up to you though at minumum, you'll add the following to the text feed. &lt;code&gt;status=&amp;lt;YOUR-CONTENT&amp;gt;&lt;/code&gt;. &lt;code&gt;&amp;lt;YOUR-CONTENT&amp;gt;&lt;/code&gt; is what will be displayed in the Mastodon post.&lt;/p&gt;
&lt;p&gt;One of the nice things about Azure Logic Apps is, properties from previous steps are available to you in subsequent steps. Since our trigger is an RSS feed, we can get access to the feed and item properties of our feed in the HTTP action. If all you wanted was to post the tile and URL, you can do that using the &lt;strong&gt;Feed title&lt;/strong&gt; and &lt;strong&gt;Primary Feed Link&lt;/strong&gt; properties. For more details, see the &lt;a href="https://learn.microsoft.com/connectors/rss/"&gt;RSS connector documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Save and run&lt;/h2&gt;
&lt;p&gt;That's it! Now you just need to select &lt;strong&gt;Save&lt;/strong&gt; in the Logic app designer page.&lt;/p&gt;
&lt;p&gt;Once it's saved, click &lt;strong&gt;Run trigger&lt;/strong&gt; which will kick off your trigger. If you have anything recent to publish and everything is configured correctly, it should show up in your Mastodon feed.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;By publishing content on your own website first, you're in full control of your content. Regardless of which platforms come and go, you won't have to adjust to those changes because your content is not locked in to those platforms. However, that doesn't mean you can't also publish your content there. Using protocols like RSS make it easy to subscribe to updates on your website. Using REST APIs provided by the respective platforms, you can automate publishing these updates. To further automate and simplify this process, you can use services like Azure Logic Apps to make publishing to all places easy.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/rss-to-mastodon-posse-azure-logic-apps?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/rss-to-mastodon-posse-azure-logic-apps</guid>
      <pubDate>2023-12-24 16:40</pubDate>
      <category>#azure</category>
      <category>#mastodon</category>
      <category>#indieweb</category>
      <category>#logicapps</category>
      <category>#fediverse</category>
      <category>#posse</category>
      <category>#internet</category>
      <category>#web</category>
      <category>#blogging</category>
      <category>#blog</category>
      <category>#automation</category>
      <category>#programming</category>
      <category>#rss</category>
    </item>
    <item>
      <title>Computing website metrics using GitHub Actions</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/website-metrics-github-actions?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/website-metrics-github-actions&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I recently posted my &lt;a href="/feed/weblogging-rewind-2023"&gt;(We)blogging Rewind&lt;/a&gt; and &lt;a href="/feed/weblogging-rewind-2023-continued"&gt;(We)blogging Rewind Continued&lt;/a&gt; for 2023. In these posts, I discuss some analysis I did of my posting behaviors over the past year on this website. I don't use platforms like Google Analytics to track visitors. I used to self-host a &lt;a href="https://www.goatcounter.com/"&gt;GoatCounter&lt;/a&gt; instance but personally I don't really care about traffic so I got rid of that. There are some standard reports I get from my &lt;a href="https://learn.microsoft.com/azure/cdn/cdn-advanced-http-reports"&gt;CDN provider on Azure&lt;/a&gt;, but again I don't really care about those metrics. What I do care about though is my output and understanding what I'm publishing and the topics that were important to me at any given time. In the case of those blog posts, it was for the year 2023. Given that I had already done the analysis and had written the script, I thought, why not automate it and run it on a more regular basis to have monthly summaries. Since my blog and scripts are already on GitHub, it makes sense to create a GitHub Action workflow. In this post, I discuss in more details what my post analytics script does and how I configured my workflow in GitHub Actions to run the script on the first of every month.&lt;/p&gt;
&lt;h2&gt;The script&lt;/h2&gt;
&lt;p&gt;This script loads the various posts on my website and computes aggregate metrics based on post types and their metadata. You can find the full script at &lt;a href="/snippets/lqdev-me-website-post-metrics"&gt;stats.fsx&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Loading files&lt;/h3&gt;
&lt;p&gt;The following are convenience functions which I use as part of my website build process.&lt;/p&gt;
&lt;p&gt;In general these functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Load the individual post files&lt;/li&gt;
&lt;li&gt;Parse the content and YAML metadata&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let posts = loadPosts()
let notes = loadFeed ()
let responses = loadReponses ()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of building new custom functions, I can repurpose them and apply additional transformations to compute aggregate statistics.&lt;/p&gt;
&lt;h3&gt;Computing aggregate statistics&lt;/h3&gt;
&lt;p&gt;Once the posts are loaded, I apply transformations on the collections to compute aggregate metrics:&lt;/p&gt;
&lt;h4&gt;Annual post counts&lt;/h4&gt;
&lt;p&gt;The following are annual aggreagates of blog posts, notes, and responses.&lt;/p&gt;
&lt;h5&gt;Blog posts&lt;/h5&gt;
&lt;p&gt;Takes the blog post collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let postCountsByYear = 
    posts
    |&amp;gt; Array.countBy (fun (x:Post) -&amp;gt; DateTime.Parse(x.Metadata.Date) |&amp;gt; _.Year)
    |&amp;gt; Array.sortByDescending fst 
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;Notes&lt;/h5&gt;
&lt;p&gt;Takes the note collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let noteCountsByYear = 
    notes
    |&amp;gt; Array.countBy (fun (x:Post) -&amp;gt; DateTime.Parse(x.Metadata.Date) |&amp;gt; _.Year)
    |&amp;gt; Array.sortByDescending fst
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;Responses&lt;/h5&gt;
&lt;p&gt;Takes the response collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let responseCountsByYear = 
    responses
    |&amp;gt; Array.countBy (fun (x:Response) -&amp;gt; DateTime.Parse(x.Metadata.DatePublished) |&amp;gt; _.Year)
    |&amp;gt; Array.sortByDescending fst
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Response counts by type&lt;/h4&gt;
&lt;p&gt;Takes the response collection, parses the published date, filters it for the current year, and computes counts by the post type (reply, bookmark, reshare, star).&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let responsesByType = 
    responses
    |&amp;gt; Array.filter(fun x -&amp;gt; (DateTime.Parse(x.Metadata.DatePublished) |&amp;gt; _.Year) = DateTime.UtcNow.Year)
    |&amp;gt; Array.countBy(fun x -&amp;gt; x.Metadata.ResponseType)
    |&amp;gt; Array.sortByDescending(snd)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Tag counts (responses)&lt;/h4&gt;
&lt;p&gt;Takes the response collection, parses the published date, filters it for the current year, and computes counts by the tag name, and sorts in descending order using the count.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let responsesByTag = 
    responses
    |&amp;gt; Array.filter(fun x -&amp;gt; (DateTime.Parse(x.Metadata.DatePublished) |&amp;gt; _.Year) = DateTime.UtcNow.Year)
    |&amp;gt; Array.collect(fun x -&amp;gt; 
            match x.Metadata.Tags with
            | null -&amp;gt; [|&amp;quot;untagged&amp;quot;|]
            | [||] -&amp;gt; [|&amp;quot;untagged&amp;quot;|]
            | _ -&amp;gt; x.Metadata.Tags
        )
    |&amp;gt; Array.countBy(fun x -&amp;gt; x)
    |&amp;gt; Array.sortByDescending(snd)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Domain counts (responses)&lt;/h4&gt;
&lt;p&gt;Takes the response collection, parses the published date, filters it for the current year, and computes counts by the target URL host name, and sorts it in descending order using the count.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let responsesByDomain = 
    responses
    |&amp;gt; Array.filter(fun x -&amp;gt; (DateTime.Parse(x.Metadata.DatePublished) |&amp;gt; _.Year) = DateTime.UtcNow.Year)
    |&amp;gt; Array.countBy(fun x -&amp;gt; Uri(x.Metadata.TargetUrl).Host)
    |&amp;gt; Array.sortByDescending(snd)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Displaying counts&lt;/h3&gt;
&lt;p&gt;Since the &lt;code&gt;countBy&lt;/code&gt; function is the one used to compute the counts, this produces a tuple. The tuple though could be &lt;code&gt;string&lt;/code&gt; or &lt;code&gt;int&lt;/code&gt;. Therefore, I set the collection of entry counts to use a generic &lt;code&gt;'a&lt;/code&gt; for the first item in the tuple. I'm also able to control using &lt;code&gt;n&lt;/code&gt; whether to display the entire collection by using &lt;code&gt;-1&lt;/code&gt; as input or a limit when &lt;code&gt;n &amp;gt;= 0&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let printEntryCounts&amp;lt;'a&amp;gt; (title:string) (entryCounts:('a * int) array) (n:int) = 
    printfn $&amp;quot;{title}&amp;quot;

    match n with 
    | n when n = -1 -&amp;gt; 
        entryCounts
        |&amp;gt; Array.iter(fun x -&amp;gt; printfn $&amp;quot;{fst x} {snd x}&amp;quot;)
        |&amp;gt; fun _ -&amp;gt; printfn $&amp;quot;&amp;quot;
    | n when n &amp;gt;= 0 -&amp;gt; 
        entryCounts
        |&amp;gt; Array.take n
        |&amp;gt; Array.iter(fun x -&amp;gt; printfn $&amp;quot;{fst x} {snd x}&amp;quot;)
        |&amp;gt; fun _ -&amp;gt; printfn $&amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result of running this script produces the following results:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Blogs
2023 5
2022 7

Notes
2023 34
2022 36

Responses
2023 216
2022 146

Response Types
bookmark 151
reshare 48
reply 10
star 7

Response Tags
ai 104
llm 42
untagged 41
opensource 31
internet 17

Domains
github.com 15
huggingface.co 11
arxiv.org 10
openai.com 6
www.theverge.com 4
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;The workflow file&lt;/h2&gt;
&lt;p&gt;The workflow file is a GitHub Actions workflow which you can find in my &lt;a href="https://github.com/lqdev/luisquintanilla.me/blob/main/.github/workflows/stats.yml"&gt;website repo&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Triggers&lt;/h3&gt;
&lt;p&gt;I don't really want the script to run every time I publish my website. Instead, I just want to have these aggregate values computed on a monthly basis. Optionally though, I'd like to be able to run ad-hoc reports and trigger this job manually.&lt;/p&gt;
&lt;p&gt;The triggers in my workflow file look like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;schedule: 
  - cron: '30 0 1 * *'
workflow_dispatch: 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using cron job syntax, I use the &lt;code&gt;schedule&lt;/code&gt; trigger to configure the script to run at 12:30 AM on the 1st day of every month.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;workflow_dispatch&lt;/code&gt; trigger is there so I can manually trigger this job.&lt;/p&gt;
&lt;h2&gt;Steps&lt;/h2&gt;
&lt;p&gt;The steps in the workflow file are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Check out the repo&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;- uses: actions/checkout@v2
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Install the .NET 8 SDK&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;- name: Setup .NET SDK 8.x
  uses: actions/setup-dotnet@v1.9.0
  with: 
    dotnet-version: '8.0.x'    
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Restore dependencies&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;- name: Install dependencies
  run: dotnet restore    
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Build the project&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;- name: Build project
  run: dotnet build --no-restore  
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the script and display metrics&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;- name: Display Post Metrics
  run: dotnet fsi Scripts/stats.fsx
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;If you have scripts that you run on a repo on a fairly regular basis, consider using GitHub Actions to automate the execution of these scripts. Happy coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/website-metrics-github-actions?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/website-metrics-github-actions</guid>
      <pubDate>2023-12-24 13:24</pubDate>
      <category>#github</category>
      <category>#cicd</category>
      <category>#website</category>
      <category>#blog</category>
      <category>#blogging</category>
      <category>#web</category>
      <category>#internet</category>
      <category>#fsharp</category>
      <category>#dotnet</category>
    </item>
    <item>
      <title>Using Generative AI to produce Spotify Clips</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/generative-ai-spotify-clips?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/generative-ai-spotify-clips&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;While listening to music on Spotify last night, this thought hit me. It's been cooking for some time so now you get to read about it. Spotify and YouTube, if you build this into your platforms for songs or podcasts, &lt;a href="/contact"&gt;contact me&lt;/a&gt; for info on where to send the check 😂. As the title of the post suggests, this is a thought experiment on how you might use Generative AI to produce Spotify Clips. This is not limited to songs but could also be extended to audio-only podcasts. However, given the long-form nature of podcasts it might be extremely resource intensive to produce clips per episode. This post is not exactly a how-to guide, though there are some practical examples of how some of the steps in the workflow might work using &lt;a href="https://copilot.microsoft.com/"&gt;Microsoft Copilot (formerly Bing Chat)&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Spotify Clips&lt;/h2&gt;
&lt;p&gt;Let's start off by talking about what Spotify Clips are. Back in March of 2023, &lt;a href="https://artists.spotify.com/blog/spotify-clips-get-started-short-form-video-stream-on"&gt;Spotify introduced Clips to their platform&lt;/a&gt;. Spotify describes &lt;a href="https://clips.byspotify.com/"&gt;Clips&lt;/a&gt; as &amp;quot;short, under-30-second vertical videos that are uploaded directly to Spotify for Artists. They’re designed to put you and your music front-and-center, so they can be attached to your artist profile, to a song, to an album, or to an upcoming release.&amp;quot;. When paired with a song, you can think of them as short music videos. Music videos being the videos made to accompany songs that played on TV channels like Music Television (MTV) before Jersey Shore and Catfish. Music videos are still in production today and they generally serve the same purpose. They provide a visual representation of the story being told by the song.&lt;/p&gt;
&lt;h2&gt;Generative AI for video&lt;/h2&gt;
&lt;p&gt;An overwhelming majority of Generative AI models only generate text. With models like DALL-E and Stable Diffusion, you can also generate images. Other models to generate audio and video have started coming on the scene. Most recently, Google announced &lt;a href="https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html"&gt;VideoPoet&lt;/a&gt;, a large language model for zero-shot video generation. There's many interesting techniques that VideoPoet introduces but I'll keep it to one of the more simple scenarios and the technique used in this post, text-to-image. Given a text prompt, VideoPoet can generate by default, a short 2-second clip. You can use VideoPoet as well to extend a clip so you could go well beyond the default 2 seconds. That however is outside the scope of this post. With that in mind, let's talk about how we might use a Generative AI model and apply it to the context of Spotify Clips.&lt;/p&gt;
&lt;h2&gt;Generative AI Spotify Clips workflow&lt;/h2&gt;
&lt;p&gt;The following is a rough idea of how you might use song information like lyrics to&lt;/p&gt;
&lt;pre class="mermaid"&gt;flowchart TD
  A["Get song lyrics"] --&gt; B["Generate scenes and script"]
  B --&gt; C["Generate prompts for each scene"]
  C --&gt; D["Generate video clips"]
  D --&gt; E["(Optional) Edit clips"]
  D --&gt; F["Stitch clips together"]
  E--&gt;F
&lt;/pre&gt;
&lt;p&gt;This is roughly what Google did in their VideoPoet samples. They &amp;quot;...produced a short movie composed of many short clips generated by the model. For the script, we asked Bard to write a series of prompts to detail a short story about a traveling raccoon. We then generated video clips for each prompt, and stitched together all resulting clips&amp;quot;. The resulting video looks like the following:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=70wZKfx6Ylk" title="An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities"&gt;&lt;img src="http://img.youtube.com/vi/70wZKfx6Ylk/0.jpg" class="img-fluid" alt="An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;One thing to note is that the videos generated are YouTube Shorts which are vertical as well. This makes it easier to produce videos in the format Spotify expects Clips to be in.&lt;/p&gt;
&lt;h2&gt;AI Generated Spotify Clips in practice&lt;/h2&gt;
&lt;p&gt;Now that we know the general process, what does it actually look like in practice? To help me with that, I used Microsoft Copilot.&lt;/p&gt;
&lt;h3&gt;Generate scenes and script&lt;/h3&gt;
&lt;p&gt;In the example shared by Google, they used a prompt to generate a script made up of a series of prompts. In the case of songs on Spotify, we can use that as context and the starting point for our script. From there, we can ask Copilot to compose scenes for the verses in the song. We can also make sure as these scenes are generated, that there is a smooth transition between them and the storytelling is consistent.&lt;/p&gt;
&lt;p&gt;Let's take a look at what this might look like using the Chicano Batman song &amp;quot;A Hundred Dead and Loving Souls&amp;quot;. Why that one? Just check out my &lt;a href="/feed/spotify-wrapped-2023"&gt;Spotify Wrapped&lt;/a&gt;. The general theme of that song is about the cycles of life experienced across millenia and generations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prompt&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given the following prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Using the song information below as context, generate a script for a story that visually represents the song lyrics. Ensure that the script is broken down into individual scenes and the scenes are linear and cohesively transition.

&amp;lt;song&amp;gt;
&amp;lt;title&amp;gt;A Hundred Dead and Loving Souls&amp;lt;/title&amp;gt;
&amp;lt;lyrics&amp;gt;
I am the sand stuck under miles of rock
watching the dry air sweep away my thoughts
as we travel protected by shards of glass
driving through the history of a million memories

In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
Climbing a thousand unclimbed trees
I don't know I want to know how it would feel
to be on top of one of these!

I am the root
quickly ascending
up through the surrounding dirt
quickly devoured by a hungry sun
quickly devoured by a hungry son
who is my father
and we are alike
we look alike we talk alike
we are alike
we are alike
we are alike we look alike
we talk alike

Thats why Im hungry
like the bellowing sky
this is no lie
as sure as light
entering eye
want to stay high
everything's cool too cool so cool
and I remember that everything falls

So I take a look around me now, as I speak
and I imagine a disinterested crowd
talking aloud
and I think of what I want most
and the crowd disappears
and a fixated gaze envelopes my wondering mind
and then she recruits and takes me to this place
I've been there before and that's where I saw her face

In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
Climbing a thousand unclimbed trees
I don't know I want to know how it would feel
to be on top of one of these!
&amp;lt;/lyrics&amp;gt;
&amp;lt;/song&amp;gt;

Story: 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Response&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The response looks like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;# Script for A Hundred Dead and Loving Souls

## Scene 1: The Sand
- A close-up shot of a grain of sand under a microscope, showing its intricate structure and texture.
- The camera zooms out to reveal that the sand is part of a vast desert landscape, with dunes and rocks stretching to the horizon.
- A voice-over of the protagonist (a young man) narrates the first verse of the song, expressing his feelings of being trapped and forgotten under the weight of the world.
- The camera pans to show a car driving on a dusty road, with the protagonist and his father inside. They are both wearing sunglasses and hats to protect themselves from the sun.
- The car is filled with boxes and bags, indicating that they are moving to a new place.
- The father is driving and listening to the radio, while the protagonist is looking out the window, bored and restless.
- The radio plays a news report about a nuclear fusion experiment in South Korea, followed by a song by a popular band.
- The father comments on the news, saying that it is a breakthrough for humanity and that he hopes to see it in his lifetime.
- The protagonist does not respond, but thinks to himself that he does not care about anything anymore.

## Scene 2: The Sea
- A montage of shots showing the protagonist and his father arriving at their new home, a small town by the sea.
- The father is excited and optimistic, while the protagonist is indifferent and detached.
- The father unpacks their belongings and sets up their new house, while the protagonist wanders around the town, exploring the beach, the pier, and the streets.
- The protagonist sees a group of teenagers hanging out at a skate park, laughing and having fun. He feels out of place and lonely.
- The protagonist walks to the edge of the pier, where he sees a sign that says &amp;quot;No diving&amp;quot;.
- He looks down at the water, where he sees hundreds of dead fish floating on the surface, victims of a recent oil spill.
- He hears the chorus of the song in his head, imagining himself diving into the sea of a hundred dead and loving souls, and climbing a thousand unclimbed trees.
- He wonders what it would feel like to be free and alive, instead of being stuck and dead inside.

## Scene 3: The Root
- A flashback scene, showing the protagonist as a child, playing in a garden with his father.
- The father is a botanist, and he teaches his son about different plants and flowers, showing him how to care for them and appreciate their beauty.
- The son is curious and eager, and he asks his father many questions about the natural world.
- The father tells his son that he is a root, quickly ascending up through the surrounding dirt, and that he will grow into a strong and beautiful tree one day.
- The son smiles and hugs his father, saying that he loves him and that he wants to be like him.
- The father smiles and hugs his son back, saying that he loves him too and that he is proud of him.

## Scene 4: The Sun
- A flash-forward scene, showing the protagonist as a young adult, working in a laboratory with his father.
- The father is a nuclear physicist, and he is working on a project to harness the power of nuclear fusion, the same process that fuels the sun.
- The son is his assistant, and he helps his father with the experiments and calculations, following his instructions and commands.
- The father tells his son that he is a hungry sun, quickly devoured by a hungry son, and that he will make a great contribution to science and humanity one day.
- The son nods and agrees, but he does not look happy or satisfied. He feels pressured and obligated to follow his father's footsteps, even though he does not share his passion or vision.
- The son thinks to himself that he is hungry, like the bellowing sky, and that he wants to stay high, away from the reality and expectations that weigh him down.

## Scene 5: The Crowd
- A present-day scene, showing the protagonist at a party, surrounded by a crowd of people.
- The party is hosted by one of the teenagers that he saw at the skate park, who invited him out of pity and curiosity.
- The party is loud and chaotic, with music, drinks, and drugs everywhere.
- The protagonist feels uncomfortable and anxious, and he does not fit in with the crowd. He does not know anyone, and no one knows him.
- He tries to make small talk with some of the guests, but they are either rude, bored, or high, and they do not pay attention to him.
- He feels invisible and insignificant, and he wishes he could disappear.
- He takes a look around him, as he speaks, and he imagines a disinterested crowd, talking aloud, and he thinks of what he wants most.

## Scene 6: The Gaze
- A close-up shot of a pair of eyes, staring intently at the protagonist.
- The eyes belong to a girl, who is also at the party, but who is different from the rest of the crowd. She is quiet and mysterious, and she has a captivating and mysterious aura.
- The girl notices the protagonist, and she is intrigued by him. She sees something in him that no one else sees, something that she relates to and understands.
- She walks up to him, and she introduces herself. She says her name is Luna, and that she likes his song. She says she heard him humming it to himself, and that she knows the lyrics by heart.
- The protagonist is surprised and confused, and he asks her how she knows his song. He says he wrote it himself, and that he never shared it with anyone.
- The girl smiles and says that she knows his song, because she wrote it too. She says she has a gift, or a curse, depending on how you look at it. She says she can hear the songs that people sing in their souls, and that she can sing them back to them.
- The protagonist is shocked and amazed, and he asks her to prove it. He asks her to sing his song, or any song that he knows.
- The girl nods and agrees, and she sings his song, word for word, note for note, perfectly. She sings it with emotion and expression, as if she wrote it herself.
- The protagonist is speechless and mesmerized, and he feels a connection with the girl that he never felt with anyone before. He feels like she knows him, and he wants to know her.
- The girl stops singing, and she looks at him with a fixated gaze, that envelopes his wondering mind. She says she likes him, and she asks him if he wants to go with her to a place that she knows.
- The protagonist nods and agrees, and he follows her out of the party, leaving the crowd behind.

## Scene 7: The Place
- A montage of shots showing the protagonist and the girl driving to a secluded spot in the woods, where they park their car and get out.
- The spot is a clearing, where there is a large tree, with a rope swing hanging from one of its branches.
- The tree is the only one of its kind in the area, and it stands out from the rest of the forest. It is tall and majestic, with green leaves and red flowers.
- The girl tells the protagonist that this is her place, and that she comes here often to escape from the world. She says she loves this tree, and that she calls it Luna, after herself.
- The protagonist tells the girl that this is his place too, and that he has been here before. He says he saw this tree in his dreams, and that he always wanted to climb it.
- The girl smiles and says that they have more in common than they thought, and that they are meant to be together. She says she wants to show him something, and she asks him to follow her.
- The girl climbs the tree, using the rope swing as a support. She is agile and graceful, and she reaches the top in no time.
- The protagonist follows her, using the rope swing as well. He is clumsy and nervous, and he struggles to keep up with her.
- The girl reaches the top of the tree, where she finds a small platform, made of wood and rope. She sits on the platform, and she looks at the view.
- The view is breathtaking, and it shows the sea, the town, and the sky, all in one panorama. The sun is setting, and it casts a golden glow on everything.
- The girl waves at the protagonist, and she invites him to join her. She says this is the best place to watch the sunset, and that she wants to share it with him.
- The protagonist reaches the top of the tree, where he joins the girl on the platform. He sits next to her, and he looks at the view. He is awestruck, and he agrees that this is the best place to watch the sunset.
- The girl leans in and kisses the protagonist, and he kisses her back. They embrace each other, and they watch the sunset together.
- The chorus of the song plays in the background, as the scene fades to black.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not too bad for a first pass. From here you might want to continue tweaking the different scenes but at least you have a starting point.&lt;/p&gt;
&lt;h3&gt;Generate prompts for each scene&lt;/h3&gt;
&lt;p&gt;Using the text-to-image technique, I first need to generate prompts to guide the model in the video generation process. In the case of this sample, you could take each of the bullet points in the scenes and generate a video for each of them.&lt;/p&gt;
&lt;p&gt;At the time of this writing, I didn't have access to a video generation model and I don't think VideoPoet is open to the public yet. Therefore, I had to slightly modify what I was generating. Instead of generating a video, I used Copilot to generate images using DALL-E. The concept is the same, except instead of generating videos, I'm generating still images.&lt;/p&gt;
&lt;p&gt;I could've applied the same concept of one image per bullet point. However, for brevity, I chose to summarize all of the bullet points in a scene into a single prompt.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prompt&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given the following prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Using storyboarding techniques, for each scene, generate a single prompt to use as input for DALL-E to generate an image. Since there are 7 scenes, there should be only 7 prompts. Distill as much of the concepts from each of the bullet points in each scene.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Response&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The results from that prompt are the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Scene 1: The Sand
    A grain of sand under a microscope and a car driving on a dusty road in a desert
Scene 2: The Sea
    A boy standing on a pier looking at a sea of dead fish and a sign that says “No diving”
Scene 3: The Root
    A father and a son playing in a garden with different plants and flowers
Scene 4: The Sun
    A father and a son working in a laboratory with a nuclear fusion reactor
Scene 5: The Crowd
    A boy at a party surrounded by a crowd of people with music, drinks, and drugs
Scene 6: The Gaze
    A girl with a pair of eyes that can hear the songs that people sing in their souls
Scene 7: The Place
    A boy and a girl sitting on a platform on top of a tree with a rope swing and a view of the sea, the town, and the sunset
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, not too bad for a first pass. The main points of the scene are captured. I can already foresee there are a few scenes like &amp;quot;The Sand&amp;quot; which might be difficult to illustrate in a single image.&lt;/p&gt;
&lt;h3&gt;Generate images (video clips)&lt;/h3&gt;
&lt;p&gt;Now that we have individual prompts, it's time to generate some images. Again, I'm doing this because I don't have access to a video generation model but the process would be generally the same.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prompt&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Use the prompts for each of the scenes to generate images with DALL-E. Render them as pencil art.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Response&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For brevity, I'll only include some of the images generated:&lt;/p&gt;
&lt;p&gt;&amp;quot;The Sand&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/generative-ai-spotify-clips/sand-car-dirt-road.jpg" class="img-fluid" alt="AI genreated image of a grain of sand under a microscope and a car driving on a dusty road in a desert" /&gt;&lt;/p&gt;
&lt;p&gt;&amp;quot;The Sea&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/generative-ai-spotify-clips/child-dock.jpg" class="img-fluid" alt="AI generated image of a boy standing on a pier looking at a sea of dead fish and a sign that says “No diving”" /&gt;&lt;/p&gt;
&lt;p&gt;&amp;quot;The Root&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/generative-ai-spotify-clips/father-son-playing.jpg" class="img-fluid" alt="AI generated image of a father and a son playing in a garden with different plants and flowers”" /&gt;&lt;/p&gt;
&lt;p&gt;The pencil art part was something I thought would look nice to illustrate this instead of digital art or photorealism. That was a personal choice though.&lt;/p&gt;
&lt;p&gt;As we can see from the images, in general they're not too bad. When not condensed into a single image though and you have an image or video per frame in a scene, the generated content can be richer.&lt;/p&gt;
&lt;h3&gt;Edit and stitch videos&lt;/h3&gt;
&lt;p&gt;For the image generation example, this seems like a natural stopping point. However, with videos, you can imagine using AI to edit these videos. The VideoPoet post mentions how you might be able to do that. Part of that editing might also include lengthening the video clips. Once you're happy with your videos, you would then stitch them together to compose a larger clip.&lt;/p&gt;
&lt;h2&gt;Improvements&lt;/h2&gt;
&lt;p&gt;Tons! I used the simplest tools available at hand to validate whether my idea is feasible.&lt;/p&gt;
&lt;h3&gt;Videos&lt;/h3&gt;
&lt;p&gt;The most obvious improvement here would be to actually generate the videos. Whether that's using VideoPoet or another video generation model, the whole point is to end up with a video clip to upload to Spotify.&lt;/p&gt;
&lt;h3&gt;Prompts&lt;/h3&gt;
&lt;p&gt;The prompts I used were not complex at all. I tried to keep them as simple as possible and in many cases, I wasn't the one providing the prompts, Copilot generated them for me. I would try to use more complex prompts to further guide the model in the formats I expect the outputs to be in and give more details for the model to generate better results.&lt;/p&gt;
&lt;h3&gt;Code&lt;/h3&gt;
&lt;p&gt;All of this was done using Copilot in the browser. As a starting point to validate my ideas this was fine. However, this approach is not composable, testable, reproducible, or scalable. I'd convert these steps to code in order to get all of the things Copilot doesn't give me.&lt;/p&gt;
&lt;h3&gt;Timestamp sync&lt;/h3&gt;
&lt;p&gt;One of the features Spotify offers is lyrics that are synced to the song. Similarly, being able to sync these generated clips to the song would be important so the visuals match the&lt;/p&gt;
&lt;h2&gt;Challenges&lt;/h2&gt;
&lt;h3&gt;Consistency&lt;/h3&gt;
&lt;p&gt;One challenge that someone brought up when I was explaining this idea was, how do you ensure that the visuals are consistent throughout all of the video clips.&lt;/p&gt;
&lt;p&gt;That is a great question I don't have a great answer for, though I have ideas.&lt;/p&gt;
&lt;p&gt;If there aren't many changes throughout scenes, you could just opt to keep lengthening the video clip.&lt;/p&gt;
&lt;p&gt;If there are flashbacks or different visuals between scenes, you might achieve consistency by  using the image-to-video modality. Using a snapshot from a frame in a previous video clip, you might try to edit that snapshot to use in between transitions and then lengthen the video to produce the next scene.&lt;/p&gt;
&lt;h3&gt;Power Consumption&lt;/h3&gt;
&lt;p&gt;As fascinating as this is, is it worth the energy and cooling demands it requires when using large models? Maybe not in its current form, but maybe it makes sense when using smaller models or more power efficient compute.&lt;/p&gt;
&lt;h3&gt;People&lt;/h3&gt;
&lt;p&gt;For music videos, there's a human element to it both from the performers and the people who produce them. Again, the Generative AI is impressive and for someone like me without any artistic skills, it can be an empowering tool. Although I'm tempted to see what an &lt;a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ"&gt;AI Rickroll&lt;/a&gt; might look like, I prefer the human version. 🙂&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post I discussed how you might use Generative AI models like VideoPoet to generate short videos for Spotify Clips. Since I don't have acess to VideoPoet or other video generation models at this time, I decided to show practical image generation examples instead using models like DALL-E to illustrate the general concepts. It's not wihtout its challenges and there's a ton of improvements I can think of but it was a fun experiment regardless. What else would you use these Generative AI models for?&lt;/p&gt;
&lt;p&gt;You can find an addendum to this post &lt;a href="/feed/genai-spotify-clips-instrumental"&gt;here&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/generative-ai-spotify-clips?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/generative-ai-spotify-clips</guid>
      <pubDate>2023-12-23 18:30</pubDate>
      <category>#ai</category>
      <category>#video</category>
      <category>#chicanobatman</category>
      <category>#generativeai</category>
      <category>#genai</category>
      <category>#spotify</category>
      <category>#google</category>
      <category>#microsoft</category>
      <category>#bing</category>
      <category>#copilot</category>
      <category>#dalle</category>
      <category>#image</category>
      <category>#prompt</category>
      <category>#projectidea</category>
      <category>#music</category>
      <category>#podcasts</category>
      <category>#llm</category>
    </item>
    <item>
      <title>AI like it's 1999 or 1899</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/ai-1999-1899?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/ai-1999-1899&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In an earlier note, I talked about the natural progression of technology which goes from &lt;a href="/feed/ai-abundance-scarcity-cycle-repeats-rss-enclosure-use-case"&gt;scarcity to abundance&lt;/a&gt; over time and AI is no different. That was in the context of media on the internet and paying per minute with early cell phones. One of the original challenges with AI was context length and token limits. It's becoming less relevant as context sizes continue to increase. However, it's not like you can fit the entire Harry Potter series either. So you still need to cleverly manage your context size especially when using documents and chat history as part of that context. Historically, people have used abbreviations to compress messages either because of cost or medium constraints. That's what this post explores in the context of AI.&lt;/p&gt;
&lt;h2&gt;Compressing text like T9&lt;/h2&gt;
&lt;p&gt;Below is a conversation Microsoft Copilot (formerly Bing Chat) using some information from the article &lt;a href="https://sundaymagazine.org/2010/08/10/from-1890-the-first-text-messages/"&gt;From 1890: The First Text Messages&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Me&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Summarize this text:&lt;/p&gt;
&lt;p&gt;I’m trying something new today. Sometimes in my research I find an interesting old article that I wouldn’t normally post because it’s not from the Sunday Magazine section, or it’s from further than 100 years ago so I’ll never get to it. Instead of letting these go unused, I figure I’ll occasionally post them midweek during what would otherwise be slow weeks. Since this weekend I only have three articles to post, it seems like a good week to try it.&lt;/p&gt;
&lt;p&gt;Telegraph operators on opposite sides of the country had some time to get to know each other when they weren’t busy sending other people’s messages. “Metaphorically they shake hands cordially twice a day — when they begin work and when they end it. And when business is dull they hold long conversations, with hundreds of miles — perhaps thousands — separating them, as two friends might do over a dinner table.”&lt;/p&gt;
&lt;p&gt;What really caught my eye, though, is that the abbreviations they used seem a lot like the abbreviations used in today’s text messages.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Copilot&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Here is a possible summary of the text:&lt;/p&gt;
&lt;p&gt;The author is trying a new feature of posting old articles from the Sunday Magazine section on midweek. The article is about how telegraph operators communicated with each other using abbreviations that are similar to today’s text messages. The author finds this interesting and wonders if there are other similarities between old and new forms of communication.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Me&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Now abbreviate it as if using a T9 keyboard for conciseness&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Copilot&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;That sounds like a fun challenge. Here is a possible abbreviation of the summary:&lt;/p&gt;
&lt;p&gt;try new feat post old artcls midwk. artcl abt telgrph oprtrs chat w/ abbrvtns lk txt msgs. find intrstng n wndr othr smilrts btwn old n new comms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this conversation, I first had Copilot summarize the original text. It's not a very long piece of text but it works for this example. You could imagine though how a longer piece of text would benefit even more from this summarization. Copilot does a good job of summarizing the text. However, what if I wanted to summarize it even more?&lt;/p&gt;
&lt;p&gt;In comes T9 (kind of). T9 is short for &amp;quot;Text on 9 keys&amp;quot; and it was predictive text technology that made it easier to compose SMS text messages using only the 9 keys on the phone keypad. Before T9, it was cumbersome and time consuming to compose full sentences so you might abbreviate them. For example &amp;quot;See you later&amp;quot; might be abbreviated as &amp;quot;c u l8r&amp;quot;. You can learn more in the &lt;a href="https://en.wikipedia.org/wiki/SMS_language"&gt;SMS language Wikipedia article&lt;/a&gt;. Not only would you abbreviate because it was tedious to compose messages one letter at a time, but also you were limited in the number of characters you could send in a single SMS. Somewhat related, in the early days of microblogging on Twitter, one of the ways you could compose and view tweets was through SMS. There was the cost aspects as well. You were charged per SMS so you wanted to send as much information in a single SMS as possible. Now SMS is only one example where this took place. Telegrams and telegraphs from yesteryear had similar constraints. Whether it was cost, size limits, or general UX challenges, people looked to abbreviations to draft their messages.&lt;/p&gt;
&lt;p&gt;Now that we're caught up on this history, that's what you're seeing in this response from Copilot.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;try new feat post old artcls midwk. artcl abt telgrph oprtrs chat w/ abbrvtns lk txt msgs. find intrstng n wndr othr smilrts btwn old n new comms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is an abbreviated form of the summarized message enabling you to use even less tokens as part of the prompts you submit to AI.&lt;/p&gt;
&lt;p&gt;When looking at &amp;quot;word&amp;quot; counts, they are roughly the following:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Text&lt;/th&gt;
&lt;th&gt;Word Count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Original&lt;/td&gt;
&lt;td&gt;58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Summarized&lt;/td&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;T9&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can see that the abbreviated version is ~3x smaller than the original message.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Given that models like GPT and others have been trained on the entire internet, it has some knowledge of these types of abbreviations to compose messages. Now, is it scalable and would you want to use this in your production applications, I can't say. Probably not because the abbreviations are lossy. While in general you could read and get the gist of the message using the abbreviations, there are things lost in translation. Now maybe the LLM might do a better job at interpreting the abbreviations. It'd be a fun experiment.&lt;/p&gt;
&lt;p&gt;&amp;quot;n d mntm, il kp usng AI lk its 1999. c u l8r!&amp;quot;&lt;/p&gt;
&lt;p&gt;Translation: &amp;quot;In the meantime, I'll keep using AI like it's 1999. See you later!&amp;quot; 🙂&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/ai-1999-1899?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/ai-1999-1899</guid>
      <pubDate>2023-12-15 08:56</pubDate>
      <category>#ai</category>
      <category>#communication</category>
      <category>#sms</category>
      <category>#retro</category>
      <category>#internet</category>
    </item>
    <item>
      <title>Quick thoughts about Snapdragon Summit 2023</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/quick-thoughts-snapdragon-summit-2023?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/quick-thoughts-snapdragon-summit-2023&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;This started as a note where I was planning on posting a bullet point or two with my thoughts but kept on writing so I decided to turn it into a blog post.&lt;/p&gt;
&lt;p&gt;Here's a few of my thoughts after the Snapdragon Summit 2023 keynote.&lt;/p&gt;
&lt;p&gt;Today was the start of Snapdragon Summit 2023 and all I can say is, &lt;strong&gt;WOW&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;CNET has a &lt;a href="https://www.youtube.com/watch?v=K7Q5iYHvgwo"&gt;14 minute clip&lt;/a&gt; that summarizes all the announcements.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=K7Q5iYHvgwo" title="Qualcomm’s Snapdragon AI Event: Everything Revealed in 14 Minutes"&gt;&lt;img src="http://img.youtube.com/vi/K7Q5iYHvgwo/0.jpg" class="img-fluid" alt="Qualcomm’s Snapdragon AI Event: Everything Revealed in 14 Minutes" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I was cautiously optimistic about what would be shown today but Qualcomm delivered.&lt;/p&gt;
&lt;p&gt;The main thing that I'm excited about are their Snapdragon X Elite Oryon-based CPUs.&lt;/p&gt;
&lt;p&gt;As someone who does all of their computing on a laptop, I'd love to have a PC that I can take with me on the go. Now you might think that's what your phone is for, and for many people that is definitely the case. However, I only use my phone for calls and messages. I find it much more enjoyable to have a larger screen and keyboard.&lt;/p&gt;
&lt;p&gt;Keeping my preferences in mind, being able to connect to your PC on the go is nothing new either. I can just use the hotspot on my phone or tether over USB in order to connect my PC to the internet. Also, SIM card slots in PCs have been a thing for a while and products like the &lt;a href="https://www.microsoft.com/d/surface-pro-9/93vkd8np4fvk?activetab=pivot:overviewtab"&gt;Surface Pro 9 with 5G&lt;/a&gt; provide an even more integrated experience.&lt;/p&gt;
&lt;p&gt;However, in order to truly make this experience mobile, I need to be untethered. Just like a mobile phone, I need my device battery to last all-day while on the go so I don't have to carry my charger. To do that, either you need a device with a large battery or a low-power chip. Larger battery immediately takes the mobile out of mobile computing. I recently carried around my 16&amp;quot; work laptop for a while walking in the city and that was no fun. So then low-power chip it is. Problem solved. Not quite, because in order for it to be low-power, performance needs to be throttled. So for the longest time, it's been a constant struggle between power-efficiency and battery life.&lt;/p&gt;
&lt;p&gt;Apple was the first to solve this problem with their M1 line of chips. However, that meant this innovation was confined to the Apple ecosystem. It's not until now with the Oryon CPUs that Windows (and hopefully soon Linux too) can tap into these types of computing platforms that promise to deliver true mobile computing. This now opens up a world of possibilities when we think about mobile computing. Our phones keep getting &lt;a href="https://www.theverge.com/23913658/best-small-phone-dead-iphone-mini-z-flip-pixel-8"&gt;larger in size&lt;/a&gt; that at this point it's harder to carry them in your pocket or use them with one hand. We've seen a surge in interest in handheld gaming devices like the &lt;a href="https://www.steamdeck.com/"&gt;SteamDeck&lt;/a&gt; which are full PCs. Their problem is they're using chips that are not power efficient, especially because of their requirements for gaming.&lt;/p&gt;
&lt;p&gt;What happens if you replace the current chips in those devices with Snapdragon X Elite? Now that it's high-performance and power efficient, does your handheld gaming device now become your handheld computing device? And if we're in handheld computing device territory, now all of a sudden it's bumping up against what smartphones do today. So at that point, do we need both a smartphone and a handheld computing device? If I was able to handle calls and messages from my PC then I wouldn't need a phone and I'd much rather just carry a single handheld computing device. Considering that smartphones are general computing devices that are serverely locked down and restricted, I'd much rather go for the general computing type of device like a PC because I have more freedom over what I can do with my PC. More importantly, because it's a full PC, it means when I need to dock down for serious work with a keyboard and multiple monitors, I can plug a USB-C Hub and connect all of my peripherals to the device. This was something projects like &lt;a href="https://learn.microsoft.com/windows-hardware/design/device-experiences/continuum-phone"&gt;Continuum&lt;/a&gt; tried to achieve but were unsuccessful doing for various reasons. Because now your PC is now also your mobile device, you take a big problem with these type of experiences which is reconciling mobile and desktop UX patterns.&lt;/p&gt;
&lt;p&gt;What about apps? Not exactly sure what this looks like in the near-term but considering devices running Oryon CPUs will take some time to get to market, by using the AI capabilities you can think of a future where AI is the interface and the apps become background services for interactive with external systems. So no need for specific apps because you can express intent and complete tasks using natural language.&lt;/p&gt;
&lt;p&gt;What about form factors? The steamdeck might not be what folks are looking for when it comes to a mobile device. Foldables are seeing an uptick in adoption. Despite it's lack of commercial success, the Surface Duo was an excellent foldable form factor that was compact but functional for productivity. I think the Surface Duo was misunderstood because it was put into the box of mobile phone. As a mobile phone it didn't work as well. But as a mobile computer, it struck the right form factor that enabled you to be productive on the go while still being portable. Needed more screen real estate? Surface Neo or Courier would've been  nice devices to have.&lt;/p&gt;
&lt;p&gt;Let's go 3 years into the future. What happens when virtual and augmented reality headsets all of a sudden become as small as the &lt;a href="https://www.ray-ban.com/usa/ray-ban-meta-smart-glasses"&gt;Ray-Ban Meta Smartglasses&lt;/a&gt;? Do we even need handheld PCs at that point if all we need is for the headset to display the screens we're looking at. Bring in IoT. If the items around us are also &amp;quot;smart&amp;quot; devices, all of a sudden your environment becomes your compute platform. Bring AI into the mix and your AI assistant has access to your environment.&lt;/p&gt;
&lt;p&gt;I'd say the biggest miss from Qualcomm and Windows partners today was not having a lineup of devices ready for purchase. That will come with time. Hopefully not too long after these announcements. I'm sure that Apple which has an announcement coming up on October 30th will have a response. The most important thing though is that now there's competition and as we enter this new era of AI and TRUE mobile computing, it'll lead to innovation. The next things that need to happen is operating systems like Windows need to ensure their existing applications work on these platforms and Linux needs to enable support for these chipsets. I'm so excited for the future of mobile computing and can't wait to be able to carry a device running Linux with always-on connectivity and all-day battery life in the form factor of a Surface Duo.&lt;/p&gt;
&lt;p&gt;An addendum to this post can be found &lt;a href="/feed/snapdragon-summit-2023-addendum"&gt;here&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/quick-thoughts-snapdragon-summit-2023?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/quick-thoughts-snapdragon-summit-2023</guid>
      <pubDate>2023-10-24 23:14</pubDate>
      <category>#arm</category>
      <category>#qualcomm</category>
      <category>#hardware</category>
      <category>#mobilecomputing</category>
      <category>#5g</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Generate QR Codes for MeCard Data in F#</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/generate-mecard-qr-code-fsharp?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/generate-mecard-qr-code-fsharp&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Today I learned about MeCard file formats for representing contact information while reading the post &lt;a href="https://andregarzia.com/2023/10/sharing-contacts-using-qr-code.html"&gt;Sharing contacts using QR Code without requiring a network connection&lt;/a&gt; by Andre Garzia.&lt;/p&gt;
&lt;p&gt;Currently I host my &lt;a href="../vcard.vcf"&gt;vCard&lt;/a&gt; on my site in a very similar way Andre describes in his post. I've also encoded it in &lt;a href="../images/contact/qr-vcard.svg"&gt;QR Code format&lt;/a&gt; to make it easy to import and share.&lt;/p&gt;
&lt;p&gt;When I saw the MeCard format, I immediately liked how simple it was so I decided to see how difficult it'd be to write some code to generate a QR code for my own contact data.&lt;/p&gt;
&lt;h2&gt;What is MeCard?&lt;/h2&gt;
&lt;p&gt;According to Wikipedia, MeCard is &amp;quot;...a data file similar to vCard...It is largely compatible with most QR-readers for smartphones. It is an easy way to share a contact with the most used fields. Usually, devices can recognize it and treat it like a contact ready to import.&amp;quot;&lt;/p&gt;
&lt;p&gt;An example might look like: &lt;code&gt;MECARD:N:Doe,John;TEL:13035551212;EMAIL:john.doe@example.com;;&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Encoding text as a QR code using F#&lt;/h2&gt;
&lt;p&gt;I'd written a &lt;a href="/snippets/qr-code-generator"&gt;snippet&lt;/a&gt; a few years ago to help me generate the QR Codes for all accounts in &lt;a href="/contact"&gt;my contact page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The snippet is a F# Interactive script which uses &lt;a href="https://www.nuget.org/packages/Net.Codecrete.QrCodeGenerator"&gt;Net.Codecrete.QrCodeGenerator NuGet package&lt;/a&gt; to encode text as a QR code and saves it to an SVG file.&lt;/p&gt;
&lt;p&gt;The core logic consists of the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let createQrCode (savePath:string) (target:string) = 
    let qr = QrCode.EncodeText(target,QrCode.Ecc.High)
    let svgString = qr.ToSvgString(4)
    File.WriteAllText(savePath,svgString, Encoding.UTF8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case &lt;code&gt;target&lt;/code&gt; is the text I want to encode and &lt;code&gt;savePath&lt;/code&gt; is the path of the SVG file I'm saving the generated QR code in.&lt;/p&gt;
&lt;p&gt;Given MeCard with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;MECARD:N:Doe,John;TEL:13035551212;EMAIL:john.doe@example.com;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The QR Code generated might look as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/generate-mecard-qr-code-fsharp/mecard.svg" class="img-fluid" alt="MECARD QR CODE" /&gt;&lt;/p&gt;
&lt;p&gt;Scanning the QR Code with a smartphone would then immediately recognize it as a contact and prompt you to save the contact information to your device.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Since I already had the code snippet to do this available, creating a MeCard was straightforward. I've since added my &lt;a href="/mecard.txt"&gt;MeCard&lt;/a&gt; alongside by vCard. You can check out the corresponding &lt;a href="/images/contact/qr-mecard.svg"&gt;QR code&lt;/a&gt; in my &lt;a href="/contact"&gt;contact&lt;/a&gt; page.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/generate-mecard-qr-code-fsharp?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/generate-mecard-qr-code-fsharp</guid>
      <pubDate>2023-10-23 19:24</pubDate>
      <category>#dotnet</category>
      <category>#fsharp</category>
      <category>#qrcode</category>
    </item>
    <item>
      <title>Down the Slow / Quiet Web Rabbit Hole</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/down-the-slow-web-rabbit-hole?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/down-the-slow-web-rabbit-hole&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;No real commentary on this post other than a web surf down the slow / quite web.&lt;/p&gt;
&lt;pre class="mermaid"&gt;flowchart TD
  A["My 20th anniversary of blogging!"] --&gt; B["The Quiet Web"]
  B --&gt; C["Asyncronous Conversations"]
  C --&gt; D["The Slow Web"]
  D --&gt; E["The Slow Web Movement - I DONE THIS"]
  E --&gt; F["The Slow Web (plus: the modern experience of film-watching)"]
  F --&gt; G["Minimal Gallery"]
  G --&gt; H["My website is a shifting house next to a river of knowledge. What could yours be?"]
  H --&gt; I["Quiet web"]
  I --&gt; J["Jeremy Sarber"]
  J --&gt; K["datagubbe"]
  K --&gt; L["Writers and talkers and leaders, oh my!"]
  L --&gt; M["Is Blogging Dead?"]
  click A href "https://tracydurnell.com/2023/09/23/my-20th-anniversary-of-blogging/" "My 20th anniversary of blogging!"
  click B href "https://briankoberlein.com/tech/quiet-web/" "The Quiet Web"
  click C href "https://manuelmoreale.com/asynchronous-conversations" "Asyncronous Conversations"
  click D href "https://www.jackcheng.com/the-slow-web/" "The Slow Web"
  click E href "https://blog.idonethis.com/the-slow-web-movement/?ref=jackcheng.com" "The Slow Web Movement - I DONE THIS"
  click F href "http://www.rebeccablood.net/archive/2010/06/a_slow_web.html" "The Slow Web (plus: the modern experience of film-watching)"
  click G href "https://minimal.gallery/" "Minimal Gallery"
  click H href "https://thecreativeindependent.com/essays/laurel-schwulst-my-website-is-a-shifting-house-next-to-a-river-of-knowledge-what-could-yours-be/" "My website is a shifting house next to a river of knowledge. What could yours be?"
  click I href "https://manuelmoreale.com/quiet-web" "Quiet web"
  click J href "https://jeremysarber.com/" "Jeremy Sarber"
  click K href "https://datagubbe.se/" "datagubbe"
  click L href "https://everythingchanges.us/blog/writers-and-talkers-and-leaders/" "Writers and talkers and leaders, oh my!"
  click M href "https://abeautifulmess.com/is-blogging-dead/" "Is Blogging Dead?"
&lt;/pre&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/down-the-slow-web-rabbit-hole?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/down-the-slow-web-rabbit-hole</guid>
      <pubDate>2023-09-23 23:36</pubDate>
      <category>#blogs</category>
      <category>#indieweb</category>
      <category>#internet</category>
      <category>#slowweb</category>
      <category>#quietweb</category>
    </item>
    <item>
      <title>First Steps with NixOS</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/first-steps-nixos?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/first-steps-nixos&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;For the longest time, when using Linux distributions, I chose those that were Debian-based. I still do for servers. However, on the dekstop I switched to &lt;a href="https://manjaro.org/"&gt;Manjaro&lt;/a&gt;, which is Arch-based, about two years ago. Two years is a long time without distro-hopping, so I was long overdue. I've been following the &lt;a href="https://nixos.org/"&gt;NixOS&lt;/a&gt; project for some time but since I was happy with my setup, I didn't consider making the switch. However, a month ago, I decided to &lt;a href="/feed/maybe-switching-nixos/"&gt;dip my toes&lt;/a&gt;. I installed NixOS on a flash drive and started tinkering with it. In this post, I'll provide some of my initial impressions and things I find appealing about the OS.&lt;/p&gt;
&lt;h2&gt;Declarative configuration&lt;/h2&gt;
&lt;p&gt;One of the first things I did when I booted into my new NixOS installation was get familiar with the &lt;a href="https://nixos.wiki/wiki/Overview_of_the_NixOS_Linux_distribution#Declarative_Configuration"&gt;configuration file&lt;/a&gt;. The configuration file is authored using the &lt;a href="https://nix.dev/tutorials/first-steps/nix-language"&gt;Nix programming language&lt;/a&gt; and it's the place where you configure your entire system, including services, packages, and desktop environments. The best way I can describe it is a Dockerfile for your desktop. If interested, you can find my &lt;a href="/snippets/nixos-configuration/"&gt;config file&lt;/a&gt; on the scripts section of this website.&lt;/p&gt;
&lt;p&gt;There's a few advantages to this approach:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Configurations are stored in a single place&lt;/strong&gt; - The advantage to this is, I don't have to figure out where the configuration files for each of the components of my system are. I can manage everything in one place. A perfect example where the configuration file goes beyond what I expected was being able to include my bash configuration. Typically, I'd have that configuration in my &lt;code&gt;bashrc&lt;/code&gt; file. With NixOS, I can just include that in the configuration file as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-nix"&gt;programs.bash = {
    shellAliases = {
        emacs=&amp;quot;emacs -nw&amp;quot;;
    };
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Enables composition&lt;/strong&gt; - Because the configuration file is effectively a script, I can modularize the difference pieces. For example, if I wanted to split out my service and package configurations into separate files, I can do so and reference those individual files from my configuration file, separating concerns while still keeping my system configuration simple.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Version control&lt;/strong&gt; - Because the configuration file is just like any other plain text file, it means I can check it into the version control system or cloud service of my choice and manage it that way. In doing so, not only can I roll back using the built-in NixOS mechanisms, but I have another layer of security in case I need to recover that file.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, because I can define everything about my system in the configuration file, this means I can seamlessly rebuild my entire system using this single file in a reproducible manner.&lt;/p&gt;
&lt;h2&gt;Reproducible builds&lt;/h2&gt;
&lt;p&gt;I got to experience first-hand how the configuration file can simplify out-of-box-experiences and system configurations. Initially, I did not install NixOS on my main hard drive. All of my configuration and learning took place on a flash drive. Once I got my configuration to a place I was satisfied, it was time to take the plunge and reimage my PC. In the past, when I've done something similar, I've had to document everything I did when configuring my system. That's where posts like &lt;a href="/posts/setting-up-new-ubuntu-pc/"&gt;Setting up a new Ubuntu PC&lt;/a&gt; come from. With NixOS, all I needed to do was replace my configuration file with the one I configured on the flash drive. Then, when I ran &lt;code&gt;nixos-rebuild switch&lt;/code&gt;, my system was configured exactly like the one on the flash drive.&lt;/p&gt;
&lt;h2&gt;Risk-free software evaluations&lt;/h2&gt;
&lt;p&gt;This is something I haven't tried yet, but I could easily see it coming in handy. Sometimes I might need a piece of software to do one thing or maybe I want to see whether it'll solve a problem I'm facing. In order to try it out and use it, I need to install it globally. This can cause changes to my system permanently that I don't want. With NixOS, you can create &lt;a href="https://nix.dev/tutorials/first-steps/ad-hoc-shell-environments#ad-hoc-shell-environments"&gt;ad-hoc shell environments&lt;/a&gt;. These environments make temporary changes to your system, so you can evaluate a piece of software or use it that one time. Then, when you exit the environment, the software is no longer installed in your system.&lt;/p&gt;
&lt;h2&gt;Large package selection&lt;/h2&gt;
&lt;p&gt;A &lt;a href="https://search.nixos.org/packages"&gt;large package selection&lt;/a&gt; to choose from is something I got used to with Manjaro. Being an Arch-based distribution, I had access to the &lt;a href="https://aur.archlinux.org/"&gt;Arch User Repository (AUR)&lt;/a&gt;. Moving to NixOS, it's nice to know that in this area, NixOS offers great support. Additionally, NixOS has built-in support for &lt;a href="https://nixos.wiki/wiki/Flatpak"&gt;Flatpak&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Still exploring&lt;/h2&gt;
&lt;p&gt;There's still a few things I don't really get. For example, when installing the .NET SDK, I wanted to have side-by-side installs of the latest STS (7.0) and LTS (6.0). However, including the package names in my configuration file didn't work as expected. Instead I had to use the following convention.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-nix"&gt;(with dotnetCorePackages; combinePackages [
    dotnet-sdk
    dotnet-sdk_7
])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As I learn more, this will probably make more sense but for now it's a mystery.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Overall, I'm happy with NixOS and I can see myself using it for the long-term. Hopefully longer than two years. I'm not a gamer so I can't say how well it does in that front. For web browsing and software development though, I really like what NixOS has to offer. If some of the things I mentioned above sound interesting, I'd encourage you to install it on a flash drive and start tinkering with it.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/first-steps-nixos?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/first-steps-nixos</guid>
      <pubDate>2023-08-22 18:03</pubDate>
      <category>#nixos</category>
      <category>#linux</category>
      <category>#os</category>
      <category>#nix</category>
      <category>#sysadmin</category>
      <category>#floss</category>
    </item>
    <item>
      <title>Accept Webmentions using F#, Azure Functions, and RSS</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/receive-webmentions-fsharp-az-functions-fsadvent?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/receive-webmentions-fsharp-az-functions-fsadvent&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This post is part of &lt;a href="https://sergeytihon.com/2022/10/28/f-advent-calendar-in-english-2022/"&gt;F# Advent 2022&lt;/a&gt;. Thanks to &lt;a href="https://sergeytihon.com"&gt;Sergey Tihon&lt;/a&gt; for organizing it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While not planned this way, this post is timely considering recent developments in the social media space. While Twitter is in the headlines, it's not the only time people have questioned their engagement and relationship with these social media platforms. In the context of Twitter though, users have been looking for alternative platforms to best serve their communities. While Mastodon and the Fediverse appear to be one of those alternatives, they're not the only one. Blogosphere platforms like Tumblr and WordPress, both owned by Automattic, are also positioning themselves as viable alternatives to the point they've &lt;a href="https://techcrunch.com/2022/11/21/tumblr-to-add-support-for-activitypub-the-social-protocol-powering-mastodon-and-other-apps/"&gt;considered the idea of supporting ActivityPub&lt;/a&gt;. ActivityPub is the protocol that powers many of the platforms on the Fediverse. However, it's not the only open protocol that enables communication and engagement on the open web. Webmentions is another protocol for enabling conversations on the web which I'll talk about more in this post.&lt;/p&gt;
&lt;p&gt;I'll also discuss how I used F#, Azure Functions, and RSS to implement a solution that helps me and my website engage in conversations on the open web thanks to Webmentions.&lt;/p&gt;
&lt;h2&gt;What are Webmentions?&lt;/h2&gt;
&lt;p&gt;First, let's start by talking a bit about Webmentions.&lt;/p&gt;
&lt;p&gt;According to the W3C specification, Webmentions are &amp;quot;...a simple way to notify any URL when you mention it on your site. From the receiver's perspective, it's a way to request notifications when other sites mention it.&amp;quot;.&lt;/p&gt;
&lt;p&gt;One of the goals according to the IndieWeb wiki is to enable cross-site conversations.&lt;/p&gt;
&lt;p&gt;Here's how a potential Webmention workflow might look like:&lt;/p&gt;
&lt;pre class="mermaid"&gt;sequenceDiagram
  autonumber
  actor A as Alice
  participant AS as Alice's "Website"
  participant AW as Alice's Webmention Service
  participant BS as Bob's "Website"
  actor B as Bob
  A -&gt;&gt; AS: Publishes post
  B -&gt;&gt; AS: Reads post
  B -&gt;&gt; BS: Publishes post with link to Alice's post
  rect rgb(230,230,230)
    alt Automated
      BS -&gt;&gt; AS: Fetch Webmention endpoint
      BS -&gt;&gt; AW: Sends Webmention
    else Manual
      B -&gt;&gt; AS: Fetch Webmention endpoint
      B -&gt;&gt; AW: Sends Webmention
    end 
  end
  rect rgb(230,230,230)
      note over AW:Validates Webmention request
      note over AW: Validates Webmention
      note over AW: Accepts / processes Webmention
      alt Automated
        AW -&gt;&gt; BS: Sends back response
      else Manual
        AW -&gt;&gt; B: Sends back response
      end
  end 
  opt Display Webmentions
    AS -&gt;&gt; AW: Fetch Webmentions
    note over AS: Display / publish Webmentions
  end
&lt;/pre&gt;
&lt;p&gt;I use the term website here very loosely because it's not only limited to your own personal website. You can technically use Webmentions through any public website including social media platforms like Twitter, Mastodon, or even GitHub.&lt;/p&gt;
&lt;p&gt;This post will focus on the middle blocks consisting of sending and receiving Webmentions. For more information, check out the &lt;a href="https://www.w3.org/TR/webmention/#abstract-p-1"&gt;specification&lt;/a&gt; and &lt;a href="https://indieweb.org/Webmention"&gt;Webmentions page&lt;/a&gt; on the IndieWeb wiki.&lt;/p&gt;
&lt;h2&gt;My webmention solution&lt;/h2&gt;
&lt;p&gt;In September of this year, I &lt;a href="/feed/webmentions-partially-implemented/"&gt;integrated a partial implementation of Webmentions&lt;/a&gt; into my website. This implementation only allowed me to send webmentions as part of my publishing process. If you're interested in the details, it's based on the blog post I wrote for last year's F# Advent, &lt;a href="/posts/sending-webmentions-fsharp-fsadvent/"&gt;Sending Webmentions with F#&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A couple of days ago, I &lt;a href="/feed/now-accepting-webmentions/"&gt;successfully deployed and integrated&lt;/a&gt; the other part of the implementation which allowed me to receive webmentions. While there are many great &lt;a href="https://indieweb.org/Webmention#Publisher_Services"&gt;services&lt;/a&gt; which will do this for you, I decided to roll my own for fun and to learn something new.&lt;/p&gt;
&lt;p&gt;To help with that, I released &lt;a href="https://www.nuget.org/packages/lqdev.WebmentionFs"&gt;WebmentionFs&lt;/a&gt;, an open-source .NET library written in F# with components for sending and receiving Webmentions.&lt;/p&gt;
&lt;p&gt;The backend service I use to receive webmentions is built on Azure Functions.&lt;/p&gt;
&lt;p&gt;If you're interested, here's the source code for &lt;a href="https://github.com/lqdev/WebmentionFs"&gt;WebmentionFs&lt;/a&gt; and my &lt;a href="https://github.com/lqdev/WebmentionService"&gt;Webmention service&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Before diving into the technical details of the solution, I'll first discuss some of the priorities driving my design decisions.&lt;/p&gt;
&lt;h3&gt;Requirements&lt;/h3&gt;
&lt;p&gt;While I don't want to trivialize it, at the end of the day, the Webmention service is just an endpoint that accepts HTTP POST requests. That means there's many ways to build it. However, here were the requirements important to me that guided my design choices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cost:&lt;/strong&gt; The solution should be low cost&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maintenance:&lt;/strong&gt; The solution should be easy to maintain&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Easy to consume:&lt;/strong&gt; I have multiple ways to view and access my webmentions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Notifications first:&lt;/strong&gt; Since I don't own or have responsibility over content that's not mine, prioritize Webmentions for notifications, not content.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With that in mind, I ended up choosing a solution built on Azure Functions to receive Webmentions and publish them as an RSS feed.&lt;/p&gt;
&lt;h3&gt;Why Azure Functions?&lt;/h3&gt;
&lt;p&gt;I don't make money from my website. It's a labor of love in which I document my experiences. Selfishly, it's for me to remind myself how to solve a problem or share something that was important to me at that time. If my posts end up helping others solve their own problems, that's a side effect and a reward unto itself. To that effect, I want my website to be as low-cost to run as possible. One way I achieve that is by using static hosting. This dramatically reduces the cost associated with my site. If you're interested in how this site is built, you can check out the &lt;a href="/colophon"&gt;colophon&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So where do Azure Functions come in?&lt;/p&gt;
&lt;p&gt;Azure Functions is a serverless solution which means there are no servers! (I kid 😏)&lt;/p&gt;
&lt;h4&gt;Cost ✔️&lt;/h4&gt;
&lt;p&gt;As a serverless solutions, it means web services are not constantly running. They only run whenever there is something to process so I only pay for what I need. That takes care of my low-cost requirement since processing Webmentions is not a long-running, resource-intensive operation that can easily run in a serverless environment.&lt;/p&gt;
&lt;h4&gt;Maintenance ✔️&lt;/h4&gt;
&lt;p&gt;Cost is not only monetary. There are also time costs associated with maintaining a solution. Because there is no infrastructure to maintain, that's not really a factor I need to consider. Instead I get to focus on writing the logic for accepting and processing Webmentions. Azure Functions provides a set of &lt;a href="https://learn.microsoft.com/azure/azure-functions/functions-triggers-bindings"&gt;prebuilt trigger and binding components&lt;/a&gt; to reduce the amount of code I have to write to handle things like accepting HTTP requests or storing data in a table or blob storage. Also, in the rare instance that this blog post breaks the internet, Azure Functions enables my service to automatically handle that scale. Don't believe me? Check out this post frorm &lt;a href="https://www.troyhunt.com/"&gt;Troy Hunt&lt;/a&gt; on &lt;a href="https://www.troyhunt.com/serverless-to-the-max-doing-big-things-for-small-dollars-with-cloudflare-workers-and-azure-functions/"&gt;Have I Been Pwned usage of Azure Functions&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Why RSS?&lt;/h3&gt;
&lt;p&gt;The choice of how to display and consume Webmentions is entirely up to you. At regular intervals, my Webmention service generates an RSS feed and publishes it to a private Azure Blob Storage. Here are some of the reasons why:&lt;/p&gt;
&lt;h4&gt;Easy to consume ✔️&lt;/h4&gt;
&lt;p&gt;The decision to use RSS comes from the fact that like Webmentions, it's an open protocol that gives choice to creators and consumers. Check out my post, &lt;a href="/posts/rediscovering-rss-user-freedom"&gt;Rediscovering the RSS protocol&lt;/a&gt; for more of my thoughts on that. Despite the appearance of decline in use, it's a protocol that's been around for over 20 years and it's one of the many reasons why podcasts continue to be one of the more open platforms despite some attempts to lock down the ecosystem.  With that it mind, by choosing RSS as the publishing format for my Webmentions feed, I can lean on the vast number of RSS readers (&lt;a href="http://newsblur.com/"&gt;NewsBlur&lt;/a&gt; and &lt;a href="https://github.com/skeeto/elfeed"&gt;Elfeed&lt;/a&gt; being my favorites) out there instead of having to write a custom UI. Not happy with my RSS reader? I can just add my feed to whichever other reader I move to and continue consuming my Webmentions without skipping a beat.&lt;/p&gt;
&lt;h3&gt;Notifications first ✔️&lt;/h3&gt;
&lt;p&gt;While not technical or specific to RSS, this is a topic I wrestled with as part of my implementation. However, choosing RSS made many parts of it simpler.&lt;/p&gt;
&lt;p&gt;Who owns your content? If you publish on your own website or a platform under your control, chances are you do. If I publish and redistribute your content on my website though, what parts do I own and which parts do you? I'm not sure. What about spam, harrassment, and lewd content? I'd have to put in place some sort of moderation instead of publishing directly on my site. I didn't want to have to deal with any of these questions so choosing to publish to a private RSS feed instead of directly on my site made it so Webmentions are for my eyes only. More importantly, I only capture the source and target links, not the content. Therefore, I don't store or redistribute your content. Now there's nothing stopping anyone from seeing your post since it's public but in cases where your site's content is not something I'm comfortable with, I'm not resharing or redistributing that content. If my RSS reader chooses to display the full contents of your post by using the &lt;code&gt;link&lt;/code&gt; element from the RSS feed, since your post is public, that's no different than me viewing the original post on your site. If at some point I feel differently about it, the changes to capture the content in the &lt;code&gt;description&lt;/code&gt; happen at the RSS file level and because the RSS is private, I'm still not redistributing your content.&lt;/p&gt;
&lt;p&gt;In short, with my implementation I chose to prioritize Webmentions for notification purposes rather than building a commenting and interaction (like, reshare) system for my website.&lt;/p&gt;
&lt;p&gt;Now that I've talked about the what and the why, in the next sections I'll get into the how.&lt;/p&gt;
&lt;h2&gt;Sending Webmentions&lt;/h2&gt;
&lt;p&gt;Once you've created a post linking to someone else's website, sending webmentions is a two-step process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;URL discovery of webmention endpoint&lt;/li&gt;
&lt;li&gt;Send webmention&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These steps can be automated or performed manually.&lt;/p&gt;
&lt;h3&gt;URL discovery of Webmention endpoint&lt;/h3&gt;
&lt;p&gt;The first thing you'll need is the endpoint where you'll be sending your Webmention to.&lt;/p&gt;
&lt;p&gt;According to the Webmention specification, a Webmention endpoint can be found in one of three places:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HTTP response headers&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;link&amp;gt;&lt;/code&gt; tag with &lt;code&gt;rel=webmention&lt;/code&gt; attribute&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; tag with &lt;code&gt;rel=me&lt;/code&gt; attribute&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When this process is automated, the discovery is performed in order and each subsequent option works as a fallback of the other. When done manually, either the website author will publish and advertise this on their website or you can inspect the website's source code to find it.&lt;/p&gt;
&lt;h3&gt;Send webmention&lt;/h3&gt;
&lt;p&gt;To send a Webmention, you need to send an HTTP POST request with an &lt;code&gt;x-www-form-urlencoded&lt;/code&gt; body containing two properties: &lt;code&gt;source&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt;. Source is the URL of your article and target is the URL of the original article. Again, this too can either be automated or done manually with tools like cURL, Insomnia, or any other HTTP client.&lt;/p&gt;
&lt;h3&gt;Sending Webmentions in practice&lt;/h3&gt;
&lt;p&gt;Let's use a redundant example to walk through the workflow. Pretend that you created or commented on a GitHub issue and added a link to one of my articles, &lt;a href="/posts/sending-webmentions-fsharp-fsadvent/"&gt;Sending Webmentions with F#&lt;/a&gt; for example. You then wanted to notify me either to get my attention on that issue or just as an FYI. Now I say this is a redundant example because GitHub already has notifications. However, what this does illustrate are the cross-site capabilities of Webmentions.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/receive-webmentions-fsharp-az-functions-fsadvent/send-gh-issue.png" class="img-fluid" alt="Image of GitHub Issue Post" /&gt;&lt;/p&gt;
&lt;p&gt;At this point, if GitHub implemented Webmentions, it would automatically:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Fetch my webpage and look for my Webmention endpoint.&lt;/li&gt;
&lt;li&gt;Compose and send the HTTP POST request.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Sadly, today that's not the case so we'll walk through the workflow manually.&lt;/p&gt;
&lt;p&gt;On my website, I include a &lt;code&gt;link&lt;/code&gt; tag with the &lt;code&gt;rel=webmention&lt;/code&gt; attribute that looks like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;link rel=&amp;quot;webmention&amp;quot; title=&amp;quot;Luis Quintanilla Webmention Endpoint&amp;quot; href=&amp;quot;https://lqdevwebmentions.azurewebsites.net/api/inbox&amp;quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My current Webmention endpoint is &lt;em&gt;https://lqdevwebmentions.azurewebsites.net/api/inbox&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Now that you have the Webmention endpoint, you could use any tool of your choice to create and send the HTTP POST request. I have made it simpler though and included a form on my website for each of my posts for this purpose.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/receive-webmentions-fsharp-az-functions-fsadvent/send-webmention-form.png" class="img-fluid" alt="luisquintanilla.me send Webmention form" /&gt;&lt;/p&gt;
&lt;p&gt;The source code looks like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;div&amp;gt;
    &amp;lt;script type=&amp;quot;application/javascript&amp;quot;&amp;gt;window.onload = function() { document.getElementById('webmention-target').value = window.location.href }&amp;lt;/script&amp;gt;
    &amp;lt;form action=&amp;quot;https://lqdevwebmentions.azurewebsites.net/api/inbox&amp;quot; method=&amp;quot;POST&amp;quot; enctype=&amp;quot;application/x-www-form-urlencoded&amp;quot;&amp;gt;
        &amp;lt;h5 class=&amp;quot;text-center&amp;quot;&amp;gt;Send me a &amp;lt;a href=&amp;quot;https://indieweb.org/webmentions&amp;quot;&amp;gt;webmention&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;
        &amp;lt;div class=&amp;quot;form-row justify-content-center&amp;quot;&amp;gt;
            &amp;lt;div class=&amp;quot;w-75&amp;quot;&amp;gt;
                &amp;lt;input type=&amp;quot;text&amp;quot; name=&amp;quot;source&amp;quot; class=&amp;quot;form-control&amp;quot; placeholder=&amp;quot;Your URL (source)&amp;quot;&amp;gt;
            &amp;lt;/div&amp;gt;
            &amp;lt;div class=&amp;quot;col-auto&amp;quot;&amp;gt;
                &amp;lt;input type=&amp;quot;submit&amp;quot; class=&amp;quot;btn btn-primary&amp;quot; value=&amp;quot;Send&amp;quot;&amp;gt;
            &amp;lt;/div&amp;gt;
            &amp;lt;input readonly=&amp;quot;&amp;quot; class=&amp;quot;form-control-plaintext&amp;quot; style=&amp;quot;visibility:hidden&amp;quot; type=&amp;quot;text&amp;quot; id=&amp;quot;webmention-target&amp;quot; name=&amp;quot;target&amp;quot;&amp;gt;
        &amp;lt;/div&amp;gt;
    &amp;lt;/form&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to send the webmention, navigate to the post you want to send a Webmention to. In this case, it's &lt;a href="https://www.luisquintanilla.me/posts/sending-webmentions-fsharp-fsadvent/"&gt;Sending Webmentions with F#&lt;/a&gt;. Then, paste the link to the GitHub issue that contains the link to my site into the post's text box and hit &amp;quot;Send&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/receive-webmentions-fsharp-az-functions-fsadvent/send-webmention-form-populated.png" class="img-fluid" alt="luisquintanilla.me send Webmention form filled-in" /&gt;&lt;/p&gt;
&lt;p&gt;At that point, the request is sent to my Webmention service which receives and processes the request and responds with a message whether the request was successful or not.&lt;/p&gt;
&lt;p&gt;That's all there is to it! Before moving the the receiving process though, I'll briefly show how to send Webmentions using WebmentionFs.&lt;/p&gt;
&lt;h3&gt;Sending Webmentions with WebmentionFs&lt;/h3&gt;
&lt;p&gt;In WebmentionFs, you can use the &lt;a href="https://github.com/lqdev/WebmentionFs/blob/main/UrlDiscoveryService.fs"&gt;&lt;code&gt;UrlDiscoveryService&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/lqdev/WebmentionFs/blob/main/WebmentionSenderService.fs"&gt;&lt;code&gt;WebmentionSenderService&lt;/code&gt;&lt;/a&gt; to send Webmentions. If you wanted to integrate sending Webmentions into your application or service, here's an example that shows how you'd use those components.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;open System
open WebmentionFs
open WebmentionFs.Services

let ds = new UrlDiscoveryService()

let ws = new WebmentionSenderService(ds)

let data = 
    {   
        Source = new Uri(&amp;quot;https://twitter.com/ljquintanilla/status/1603602055435894784&amp;quot;)  
        Target = new Uri(&amp;quot;https://www.luisquintanilla.me/feed/mastodon-hashtag-rss-boffosocko&amp;quot;)
    }

ws.SendAsync(data) |&amp;gt; Async.AwaitTask |&amp;gt; Async.RunSynchronously
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The main part to pay attention to is the &lt;code&gt;SendAsync&lt;/code&gt; function. This uses the &lt;code&gt;UrlDiscoveryService&lt;/code&gt; to perform the Webmention endpoint discovery and send the HTTP POST request with the provided data.&lt;/p&gt;
&lt;h2&gt;Receiving webmentions&lt;/h2&gt;
&lt;p&gt;Now that you know how to send webmentions, let's talk about what happens on the receiver end. At its core, it's about a three-step process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Validate request&lt;/li&gt;
&lt;li&gt;Validate Webmention&lt;/li&gt;
&lt;li&gt;Process Webmention&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Validate request&lt;/h3&gt;
&lt;p&gt;Before processing the Webmention, you want to make sure that the request is valid. To do so there are a few checks you perform:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is the protocol valid? Check that the protocol for the source and target URLs in the request body are HTTP or HTTPS.&lt;/li&gt;
&lt;li&gt;Are the URLs the same? Check that the source and target URLS in the request body are NOT the same.&lt;/li&gt;
&lt;li&gt;Is the target URL a valid resource? This is more loosely defined and up to you. In my case I defined valid as:
&lt;ul&gt;
&lt;li&gt;I own the domain&lt;/li&gt;
&lt;li&gt;The target URL doesn't return an HTTP 400 or 500 error.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the request is not valid, nothing is done with the Webmention and an error returns. Otherwise, it moves to the next step which is Webmention validation.&lt;/p&gt;
&lt;h3&gt;Validate Webmention&lt;/h3&gt;
&lt;p&gt;In the base case, Webmention validation just means you parse through the source document and make sure that the target URL is included in it. However, if you wanted to get more complex and take into account specific types of interactions such as likes, replies, reposts, bookmarks, etc. you can parse through the document looking for &lt;a href="http://microformats.org/wiki/h-entry#Core_Properties"&gt;microformat&lt;/a&gt; annotations. For more information on these annotations, see the &lt;a href="https://indieweb.org/posts#Types_of_Posts"&gt;Types of Posts&lt;/a&gt; or &lt;a href="https://indieweb.org/reply#Post_a_reply"&gt;reply&lt;/a&gt; pages in the IndieWeb Wiki.&lt;/p&gt;
&lt;p&gt;If the target link is NOT in the source document, you return an error. Otherwise, you move to the next step of processing the Webmention&lt;/p&gt;
&lt;h3&gt;Processing Webmentions&lt;/h3&gt;
&lt;p&gt;This step is optional and loosely defined. Once you've validated the request and Webmention, you can do with it as you please. One thing you can do is publish it (after moderation) on your website.&lt;/p&gt;
&lt;p&gt;In my case, I'm storing it in an Azure Tables database. Then, every morning, I generate an RSS feed of all my webmentions and save it to a private container on Azure Blob Storage.&lt;/p&gt;
&lt;h2&gt;Reciving Webmentions with WebmentionFs&lt;/h2&gt;
&lt;p&gt;To receive Webmentions, I use the &lt;a href="https://github.com/lqdev/WebmentionFs/blob/main/RequestValidationService.fs"&gt;&lt;code&gt;RequestValidationService&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://github.com/lqdev/WebmentionFs/blob/main/WebmentionValidationService.fs"&gt;&lt;code&gt;WebmentionValidationService&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://github.com/lqdev/WebmentionFs/blob/main/WebmentionReceiverService.fs"&gt;&lt;code&gt;WebmentionReceiverService&lt;/code&gt;&lt;/a&gt; inside my Azure Function.&lt;/p&gt;
&lt;p&gt;The source code for my Azure Function is available in the &lt;a href="https://github.com/lqdev/WebmentionService"&gt;WebmentionService GitHub repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The main components are in the &lt;code&gt;Startup.fs&lt;/code&gt; and &lt;code&gt;ReceiveWebmention.fs&lt;/code&gt; files.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/lqdev/WebmentionService/blob/main/Startup.fs"&gt;&lt;code&gt;Startup.fs&lt;/code&gt;&lt;/a&gt; files registers each of the services I'll use to to receive Webmentions. By doing so, I define the initialization logic for use later in my application using dependency injection. The main thing to note here is that the &lt;code&gt;WebmentionReceiverService&lt;/code&gt; is initialized with instances of a &lt;code&gt;RequestValidationService&lt;/code&gt; and &lt;code&gt;WebmentionValidationService&lt;/code&gt;. For the &lt;code&gt;RequestValidationService&lt;/code&gt;, it's initialized with a list of host names. In this case, I'm passing it in as a comma-delimited environment variable that looks something like &amp;quot;lqdev.me,luisquintanilla.me&amp;quot;. This is what the &lt;code&gt;RequestValidationService&lt;/code&gt; uses to check whether I own a domain.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;// Add request validation service
builder.Services.AddScoped&amp;lt;RequestValidationService&amp;gt;(fun _ -&amp;gt; 

    let hostNames = Environment.GetEnvironmentVariable(&amp;quot;PERSONAL_WEBSITE_HOSTNAMES&amp;quot;)

    let hostNameList = hostNames.Split(',')

    new RequestValidationService(hostNameList)) |&amp;gt; ignore


// Add webmention validation service
builder.Services.AddScoped&amp;lt;WebmentionValidationService&amp;gt;() |&amp;gt; ignore

// Add receiver service
builder.Services.AddScoped&amp;lt;IWebmentionReceiver&amp;lt;Webmention&amp;gt;,WebmentionReceiverService&amp;gt;(fun (s:IServiceProvider) -&amp;gt;
    let requestValidationService = s.GetRequiredService&amp;lt;RequestValidationService&amp;gt;()
    let webmentionValidationService = s.GetRequiredService&amp;lt;WebmentionValidationService&amp;gt;()
    new WebmentionReceiverService(requestValidationService,webmentionValidationService)) |&amp;gt; ignore
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, in the &lt;a href="https://github.com/lqdev/WebmentionService/blob/main/ReceiveWebmention.fs"&gt;&lt;code&gt;ReceiveWebmention.fs&lt;/code&gt;&lt;/a&gt; file, I inject my &lt;code&gt;WebmentionReceiverService&lt;/code&gt; into the constructor and add all the logic for running my serverless function in the &lt;code&gt;Run&lt;/code&gt; function. In this snippet, some code is ommitted for brevity.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;type ReceiveWebmention (webmentionReceiver: IWebmentionReceiver&amp;lt;Webmention&amp;gt;) = 

    ///...

    [&amp;lt;FunctionName(&amp;quot;ReceiveWebmention&amp;quot;)&amp;gt;]
    member x.Run 
        ([&amp;lt;HttpTrigger(AuthorizationLevel.Anonymous, &amp;quot;post&amp;quot;, Route = &amp;quot;inbox&amp;quot;)&amp;gt;] req: HttpRequest) 
        ([&amp;lt;Table(&amp;quot;webmentions&amp;quot;, Connection=&amp;quot;AzureWebJobsStorage&amp;quot;)&amp;gt;] t: TableClient)
        (log: ILogger) =
        task {
            
            log.LogInformation(&amp;quot;Processing webmention request&amp;quot;)

            let! validationResult = x.WebmentionReceiver.ReceiveAsync(req)

            let response = 
                match validationResult with
                | ValidationSuccess m -&amp;gt; 
                    let entity = mapMentionToTableEntity m
                    try
                        t.AddEntity(entity) |&amp;gt; ignore
                        OkObjectResult(&amp;quot;Webmention processed successfully&amp;quot;) :&amp;gt; IActionResult
                    with
                        | ex -&amp;gt; 
                            log.LogError($&amp;quot;{ex}&amp;quot;)
                            BadRequestObjectResult($&amp;quot;Error processing webmention. Webmention already exists&amp;quot;) :&amp;gt; IActionResult
                | ValidationError e -&amp;gt; 
                    log.LogError(e)
                    BadRequestObjectResult(e) :&amp;gt; IActionResult

            return response

        }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;Run&lt;/code&gt; function is triggered whenever an HTTP POST request is made to my Webmention endpoint. It then calls &lt;code&gt;ReceiveAsync&lt;/code&gt; on the &lt;code&gt;WebmentionReceiverService&lt;/code&gt; which under the hood uses the &lt;code&gt;RequestValidationService&lt;/code&gt; and &lt;code&gt;WebmentionValidationService&lt;/code&gt; for validation. For reference, here's the code that does that in WebmentionFS.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;member x.ReceiveAsync (data:UrlData) = 
    task {
        let! requestValidationResult = 
            x.RequestValidationService.ValidateAsync data

        match requestValidationResult with
        | RequestSuccess r -&amp;gt; 
            let! webmentionValidationResult = 
                x.WembentionValidationService.ValidateAsync r.Source r.Target

            let (result:ValidationResult&amp;lt;Webmention&amp;gt;) = 
                match webmentionValidationResult with
                | AnnotatedMention m -&amp;gt; 
                    ValidationSuccess {RequestBody = r; Mentions = m}
                | UnannotatedMention -&amp;gt; 
                    ValidationSuccess 
                        {
                            RequestBody = r
                            Mentions = 
                                {
                                    IsBookmark = false
                                    IsLike = false
                                    IsReply = false
                                    IsRepost = false        
                                }
                        }
                | MentionError e -&amp;gt; ValidationError e
            return result
        | RequestError e -&amp;gt; return ValidationError e
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A quick side-note. I love how elegant function composition in F# makes the request validation pipeline look. See for yourself 🙂&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let validateAsync = 
   isProtocolValid &amp;gt;&amp;gt; isSameUrl &amp;gt;&amp;gt; isTargetUrlValidAsync
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Inside my &lt;code&gt;Run&lt;/code&gt; function, if the request and Webmention validation are successful, I use the Azure Table Storage table client &lt;code&gt;AddEntity&lt;/code&gt; method to save the Webmention to a table. One thing to note is that &lt;code&gt;AddEntity&lt;/code&gt; ensures that an entry is only added once. If an entry already exists in the table, it returns an error.&lt;/p&gt;
&lt;p&gt;This is what the GitHub Webmention we sent looks like in the table.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/receive-webmentions-fsharp-az-functions-fsadvent/webmentions-azure-table.png" class="img-fluid" alt="Webmentions table on Azure Table Storage" /&gt;&lt;/p&gt;
&lt;p&gt;In this case, the &lt;code&gt;PartitionKey&lt;/code&gt; is the target URL and &lt;code&gt;RowKey&lt;/code&gt; is the source URL. The fact all other columns are set to false indicates this is an untagged mention with no special microformats annotations. Annotating your posts appropriately would set one or more of those columns to true.&lt;/p&gt;
&lt;p&gt;Now that we went over how I receive Webmentions, I'll show how I prepare my Webmentions for consumption.&lt;/p&gt;
&lt;h2&gt;Viewing Webmentions&lt;/h2&gt;
&lt;p&gt;I consume my Webmentions using RSS feeds and RSS readers. To do so, I have an Azure Function called &lt;a href="https://github.com/lqdev/WebmentionService/blob/main/WebmentionToRss.fs"&gt;&lt;code&gt;WebmentionToRss&lt;/code&gt;&lt;/a&gt; that triggers every day at 3 AM. This function queries the Azure Table Storage table containing my Webmentions and creates an RSS feed that's then stored to Azure Blob Storage. Like the other function to process my Webmentions, I:&lt;/p&gt;
&lt;p&gt;Register the &lt;code&gt;RssService&lt;/code&gt; responsible for generating the RSS feed in &lt;code&gt;Startup.fs&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;// Add RSS service
builder.Services.AddScoped&amp;lt;RssService&amp;gt;() |&amp;gt; ignore
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inject the service into my &lt;code&gt;WebmentionToRss&lt;/code&gt; constructor. Then in my &lt;code&gt;Run&lt;/code&gt; function, I call the &lt;code&gt;getMentions&lt;/code&gt; helper function to query the table, call &lt;code&gt;BuildRssFeed&lt;/code&gt; to generate the feed, and write it out to Azure Blob Storage using a &lt;code&gt;Stream&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;type WebmentionToRss (rssService:RssService) =

    let getMentions (t:TableClient) = 

        //...

        let webmentions = t.Query&amp;lt;WebmentionEntity&amp;gt;()

        webmentions


    //...

    [&amp;lt;FunctionName(&amp;quot;WebmentionToRss&amp;quot;)&amp;gt;]
    member x.Run
        ([&amp;lt;TimerTrigger(&amp;quot;0 0 3 * * *&amp;quot;)&amp;gt;] info: TimerInfo)
        ([&amp;lt;Table(&amp;quot;webmentions&amp;quot;,Connection=&amp;quot;AzureWebJobsStorage&amp;quot;)&amp;gt;] t: TableClient)
        ([&amp;lt;Blob(&amp;quot;feeds/webmentions/index.xml&amp;quot;, FileAccess.Write, Connection=&amp;quot;AzureWebJobsStorage&amp;quot;)&amp;gt;] rssBlob: Stream)
        (log: ILogger) =

        task {
            let mentions = getMentions t

            let rss = x.RssService.BuildRssFeed mentions &amp;quot;lqdev's Webmentions&amp;quot; &amp;quot;http://lqdev.me&amp;quot; &amp;quot;lqdev's Webmentions&amp;quot; &amp;quot;en&amp;quot;

            log.LogInformation(rss.ToString())                

            use xmlWriter = XmlWriter.Create(rssBlob)

            rss.WriteTo(xmlWriter) |&amp;gt; ignore
        }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to view my feed I add the URL to the RSS file in Azure Blob Storage to my RSS feed reader. The container and blob containing the file are private though, so how does my RSS reader get access to the file? I don't use the direct link. Instead, I use a link containing a &lt;a href="https://learn.microsoft.com/azure/storage/common/storage-sas-overview#sas-token"&gt;Shared Access Signatures (SAS) token&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The RSS feed reader then fetches my Webmentions feed at regular intervals.&lt;/p&gt;
&lt;p&gt;This is what the GitHub post looks like in my RSS reader.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/receive-webmentions-fsharp-az-functions-fsadvent/newsblur-webmentions-rss-feed.png" class="img-fluid" alt="Webmentions table on Azure Table Storage" /&gt;&lt;/p&gt;
&lt;p&gt;That's all there is to it!&lt;/p&gt;
&lt;h2&gt;What's next?&lt;/h2&gt;
&lt;p&gt;Overall I'm happy with my solution. I learned a few things along the way and got something working for myself. Publishing WebmentionFs as a library and making my Azure Function code open-source were attempts to help others looking to implement similar solutions get started. These solitions could be improved though. These are some updates I would like to make at some point:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Upgrade my website to use WebmentionFs for sending Webmentions. You can see the &lt;a href="https://github.com/lqdev/luisquintanilla.me/blob/main/Services/Webmention.fs"&gt;current implementation&lt;/a&gt; on my website repo which is basically what's in &lt;code&gt;UrlDiscoveryService&lt;/code&gt; and &lt;code&gt;WebmentionSenderService&lt;/code&gt; today.&lt;/li&gt;
&lt;li&gt;Add interfaces for all services in WebmentionFs. Currently most services have concrete implementations. By adding interfaces, I can make the library extensible and you can implement your own URL discovery and validation services.&lt;/li&gt;
&lt;li&gt;Implement &lt;code&gt;outbox&lt;/code&gt; enpoint in WebmentionService to handle sending Webmentions.&lt;/li&gt;
&lt;li&gt;Create Azure ARM template for deploying WebmentionService. Currently with some small edits, you could fork and get my WebmentionService project working for your website. However, that process isn't turnkey today. Maybe using something like &lt;a href="https://compositionalit.github.io/farmer/"&gt;Farmer&lt;/a&gt; would make onboarding easier for folks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Webmentions are a great way to enable cross-site conversations on the internet. In this post, I provided a brief introduction to the general workflow for sending and receiving Webmentions. I also detailed my own implementation which I use today on my website. That implementation relies on the WebmentionFs .NET library, Azure Functions, and RSS. By using the WebmentionFs .NET library, you can more quickly implement similar solutions into your own apps and services. If you need a reference, check out my WebmentionService repository on GitHub which contains the code I use for receiving Webmentions.&lt;/p&gt;
&lt;p&gt;I'll leave you with three things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;As you consider the next platform for your next online home, in addition to considering the health of the community, take some time to also think about the protocols it's built on top of and whether they give you more or less freedom.&lt;/li&gt;
&lt;li&gt;Try out the WebmentionFs library on NuGet and add Webmentions to your site.&lt;/li&gt;
&lt;li&gt;If you share this post anywhere publicly on the web, send me a Webmention.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Catch you on the world wide web!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/receive-webmentions-fsharp-az-functions-fsadvent?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/receive-webmentions-fsharp-az-functions-fsadvent</guid>
      <pubDate>2022-12-21 00:33</pubDate>
      <category>#fsharp</category>
      <category>#indieweb</category>
      <category>#protocol</category>
      <category>#internet</category>
      <category>#web</category>
      <category>#dotnet</category>
    </item>
    <item>
      <title>Get started with TiddlyWiki in VS Code</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/tiddlywiki-vscode?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/tiddlywiki-vscode&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A while back, I set up a knowledge base (KB) section on my website. As part of that knowledge base, I have a &lt;a href="/wiki"&gt;wiki&lt;/a&gt; and &lt;a href="/snippets"&gt;snippets&lt;/a&gt; section. As opposed to the &lt;a href="/posts"&gt;blog posts&lt;/a&gt; which tend to be more long-form and have structure or the microblog-like posts on my &lt;a href="/feed"&gt;main&lt;/a&gt; and &lt;a href="/feed/responses"&gt;response&lt;/a&gt; feeds, the wiki and snippets section are intended to have self-contained blocks of information that are linked somehow but not a linear or chronological order.&lt;/p&gt;
&lt;p&gt;My current wiki and snippets solution was fairly basic. It was a series of documents authored in Markdown which then get converted to static HTML pages. The index pages for wiki articles and snippets contain links to each of those documents. While that's worked okay, I felt I was missing functionality when it came to linking between documents, tagging, and more importantly searching.&lt;/p&gt;
&lt;p&gt;While I had seen wiki solutions like &lt;a href="https://www.mediawiki.org/wiki/MediaWiki"&gt;MediaWiki&lt;/a&gt;, they felt too bloated for my use case. Since my site is statically hosted, I didn't want to have to create, maintain, and pay for a server only to run my wiki. That's where TiddlyWiki comes in.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://tiddlywiki.com/"&gt;TiddlyWiki&lt;/a&gt; describes itself as &amp;quot;a unique non-linear notebook for capturing, organising and sharing complex information&amp;quot;. In short, TiddlyWiki is a self-contained single HTML file with WYSIWYG-like functionality for creating a wiki. Because everything is contained in a single HTML file, it's static-site friendly.&lt;/p&gt;
&lt;p&gt;Now I had heard of TiddlyWiki before but dismissed it because I didn't get how it worked. For one, I was confused how the WYSIWYG GUI supported editing, viewing, and exporting of these notes since there was just a single HTML file. More importantly, when it comes to &lt;a href="https://tiddlywiki.com/#Saving"&gt;saving changes&lt;/a&gt;, the recommendations on the website point users to browser extensions and apps. If it's just an HTML page, why can't I use the tools I'm already using to work with HTML?&lt;/p&gt;
&lt;p&gt;After giving it another look and tinkering a bit, I found I was right and could use tools like VS Code to work with TiddlyWiki. In this post, I'll show how you can get started with the empty edition of TiddlyWiki using VS Code and the Live Preview extension.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://code.visualstudio.com/#alt-downloads"&gt;Visual Studio Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode.live-server"&gt;Live Server Extension&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This guide was written on a Linux (Manjaro) PC but should work the same on Windows and macOS.&lt;/p&gt;
&lt;h2&gt;Download TiddlyWiki&lt;/h2&gt;
&lt;p&gt;The quickest way to get started with TiddlyWiki is to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Download the &lt;a href="https://tiddlywiki.com/#Empty%20Edition"&gt;empty edition&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A file called &lt;em&gt;empty.html&lt;/em&gt; should download to your computer. For simplicity, I would recommend creating a new directory on your computer that only contains that file.&lt;/p&gt;
&lt;p&gt;The empty edition contains the core functionality. For other editions, see &lt;a href="https://tiddlywiki.com/#Editions"&gt;TiddlyWiki editions&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Rename the file &lt;em&gt;index.html&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Edit and save your wiki&lt;/h2&gt;
&lt;h3&gt;Edit your wiki&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Open the directory containing your &lt;em&gt;index.html&lt;/em&gt; file in VS Code.&lt;/li&gt;
&lt;li&gt;Open the command palette (View &amp;gt; Command Palette).&lt;/li&gt;
&lt;li&gt;Enter into the text box &lt;em&gt;Live Preview: Start Server&lt;/em&gt; and press &lt;em&gt;Enter&lt;/em&gt;. The server should start.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src="/images/tiddlywiki-vscode/lqdev-tiddlywiki.png" class="img-fluid" alt="lqdev TiddlyWiki Main Page" /&gt;&lt;/p&gt;
&lt;p&gt;At this point, you can use the TiddlyWiki interface to create and edit your wiki.&lt;/p&gt;
&lt;h3&gt;Save your wiki&lt;/h3&gt;
&lt;p&gt;Saving can be a bit tricky but it's not too bad. Use the checkmark in the side pane to save your changes. When there are unsaved changes, the checkmark turns red.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/tiddlywiki-vscode/tiddlywiki-save-changes.png" class="img-fluid" alt="TiddlyWiki Side Panel Red Checkmark Save Changes" /&gt;&lt;/p&gt;
&lt;p&gt;To save your changes, overwrite the existing file or if you want, you can also save it to a file with a different name. I would recommend saving it to the same file because it's easier to manage. You don't want to end up with &lt;em&gt;index-version12341.html&lt;/em&gt;. That being said, I don't know if there may be risks in overwriting the file. In my limited testing I had no issues overwriting the file, but I'd still tread lightly.&lt;/p&gt;
&lt;h3&gt;Stop the server&lt;/h3&gt;
&lt;p&gt;Once you're done viewing and editing your wiki, stop the Live Preview server:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In VS Code, open the command palette &lt;strong&gt;(View &amp;gt; Command Palette)&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Enter into the text box &lt;em&gt;Live Preview: Stop Server&lt;/em&gt; and press &lt;em&gt;Enter&lt;/em&gt;. The server should stop.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Tips&lt;/h2&gt;
&lt;h3&gt;Plugins&lt;/h3&gt;
&lt;p&gt;I prefer working in Markdown and since some of my notes include code, I addded two plugins to my wiki:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://tiddlywiki.com/#Markdown%20Plugin"&gt;Markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tiddlywiki.com/#Highlight%20Plugin"&gt;Highlight&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information, see &lt;a href="https://tiddlywiki.com/#Installing%20a%20plugin%20from%20the%20plugin%20library"&gt;Installing a plugin from the plugin library&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Things to watch out for in VS Code&lt;/h3&gt;
&lt;p&gt;When using the Live Preview extension, some things might not work such as keyboard shortcuts and navigating to external links. Since a local server is running you should be able to use the browser as well. However, I would recommend only using the browser to &lt;em&gt;view&lt;/em&gt; your wiki. Trying to save changes made in the browser will download the HTML page containing your changes.&lt;/p&gt;
&lt;h3&gt;Use Git&lt;/h3&gt;
&lt;p&gt;Autosaving to Git is one of the ways you can save your TiddlyWiki. However, you have to enter and use your credentials. Since your wiki is a static HTML page, just save it like you would any other HTML file in a repo. Doing so also makes it easy to backup to a previous version in case data is corrupted or lost.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I this post, I shared how TiddlyWiki provides a relatively low overhead, static-website friendly way of creating wikis. If you use VS Code, you can use the Live Preview extension to view and edit your wiki. Although my intended use for TiddlyWiki is as a personal knowledge management solution, I can also think of ways you could use TiddlyWiki as a low-friction way of building your own website. The WYSIWYG functionality makes creating content extremely more accessible than static site generators and similar to WordPress. In contrast to WordPress though, since you only have a single HTML file, it's less overhead to manage and also low-cost since you can &lt;a href="https://nesslabs.com/tiddlywiki-static-website-generator"&gt;host it on GitHub Pages for free&lt;/a&gt; or any other file hosting service. In the near future, I plan to migrate my wiki and snippets to use TiddlyWiki.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/tiddlywiki-vscode?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/tiddlywiki-vscode</guid>
      <pubDate>2022-10-21 08:32</pubDate>
      <category>#web</category>
      <category>#vscode</category>
      <category>#wiki</category>
      <category>#personalknowledgemanagement</category>
      <category>#git</category>
      <category>#website</category>
    </item>
    <item>
      <title>Site redirects in static websites</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/static-website-redirects?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/static-website-redirects&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I recently came across the concept of &lt;a href="https://indieweb.org/own_your_links"&gt;owning your links&lt;/a&gt; on the IndieWeb wiki and right away it made lots of sense to me. The main idea behind owning your links is using your domain as the source of truth and using it to redirect to your content on various other platforms you own. For example, when sharing my Twitter profile, instead of saying, &amp;quot;Go to https://twitter.com/ljquintanilla&amp;quot;, I can instead point people to &lt;a href="http://lqdev.me/twitter"&gt;http://lqdev.me/twitter&lt;/a&gt;&amp;quot;. The link on my domain ultimately redirects to Twitter, but the main entrypoint is my website. There's various ways to handle redirects, but in most cases it's done on the server side. For static websites, if you're hosting them in GitHub Pages, Netlify, or Azure Blob Storage, you don't have a server. Therefore configuring redirects can be a challenge. In this post, I'll go through how you can configure redirects for your static website without a server or JavaScript.&lt;/p&gt;
&lt;h2&gt;Create redirect page&lt;/h2&gt;
&lt;p&gt;Depending on which generator you use to create your static website, how you create your  redirect pages will differ. However, the end result should look like the the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;html&amp;gt;
    &amp;lt;head&amp;gt;
        &amp;lt;meta http-equiv=&amp;quot;refresh&amp;quot; content=&amp;quot;0;REDIRECT-TARGET-URL&amp;quot;&amp;gt;
    &amp;lt;/head&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only thing you need to perform the redirect is add a &lt;code&gt;meta&lt;/code&gt; tag to the endpoint you want to handle the redirect. If I wanted to redirect to Twitter, I might create an HTML page at &lt;code&gt;http://lqdev.me/twitter&lt;/code&gt; on my website. Setting the &lt;code&gt;http-equiv&lt;/code&gt; attribute to &lt;code&gt;refresh&lt;/code&gt; instructs the browser to refresh the page. Then, you use the &lt;code&gt;content&lt;/code&gt; attribute to define your redirect target and how long you want to wait before redirecting the user. In the snippet above, the number &lt;code&gt;0&lt;/code&gt; indicates you want to wait 0 seconds before redirecting your users and the &lt;code&gt;REDIRECT-TARGET-URL&lt;/code&gt; is where you want to redirect to. This could be your Twitter profile, YouTube channel, or anywhere else. Wherever you want to redirect to, you place it in the &lt;code&gt;content&lt;/code&gt; attribute.&lt;/p&gt;
&lt;p&gt;It's important to note that the redirect happens automatically which takes control away from the user causing potential accesibility issues. Until now, I haven't found a better way of doing this that doesn't require JavaScript. For more information, see the &lt;a href="https://www.w3.org/WAI/standards-guidelines/wcag/"&gt;web content accessibility guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For more information on creating redirects in HTML, see the article &lt;a href="https://www.w3docs.com/snippets/html/how-to-redirect-a-web-page-in-html.html"&gt;how to redirect a web page in HTML&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;So far I've created redirect pages for most of my social media profiles. You can see samples in my &lt;a href="/contact.html"&gt;contact page&lt;/a&gt;. It's important to note that while this works for social profiles and any other properties you own on the internet, it could also be used in cases where you've changed the structure of your website and want to gracefully handle redirects for broken / updated links. Happy coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/static-website-redirects?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/static-website-redirects</guid>
      <pubDate>07/31/2022 14:52</pubDate>
      <category>#http</category>
      <category>#web</category>
      <category>#staticweb</category>
      <category>#indieweb</category>
      <category>#html</category>
    </item>
    <item>
      <title>Manage your reading list in Emacs using Org mode</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/manage-reading-list-org-mode-emacs?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/manage-reading-list-org-mode-emacs&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Emacs is a rich and customizable text editor. &lt;a href="https://orgmode.org/"&gt;Org mode&lt;/a&gt; for Emacs makes it easy to manage to-do lists, projects, and author technical documents. Many of my workflows like writing, planning, and organization make heavy use of plain text. One reason I enjoy working in plain text is, it's easy for my thoughts and ideas to naturally flow without having tools get in the way. Another reason I like plain text is, I'm not locked-in to any tool. Because plain text can be authored and rendered almost anywhere, it's easy for me to use whichever tool I find most productive. Some that I've found great success with recently have been &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt;, &lt;a href="https://joplinapp.org/"&gt;Joplin&lt;/a&gt;, and Emacs with Org mode. I've been trying to manage my reading list in plain text and while it's worked well, I'd like a way to track the state for each book by tagging them as read or in various stages of unread. One of the features of Org mode is the ability to track workflows. In this guide, I'll show how you can extend the TODO Org mode functionality to keep track of your reading list.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.gnu.org/software/emacs/"&gt;Emacs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since Emacs is cross-platform, the contents of this guide should work the same across Linux, Mac, and Windows.&lt;/p&gt;
&lt;h2&gt;Create org file&lt;/h2&gt;
&lt;p&gt;Org mode in Emacs provides to-do list management features. By default, the built-in states are TODO and DONE. However, you can define your own states to best match your workflow. For more information, see the &lt;a href="https://orgmode.org/manual/TODO-Items.html#TODO-Items"&gt;TODO item section in the Org mode guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Start by creating a new file in Emacs with the &lt;em&gt;.org&lt;/em&gt; file extension (i.e. reading-list.org).&lt;/p&gt;
&lt;p&gt;Then, add the following headers at the top of the file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;#+TODO: WISHLIST(w) QUEUED(q) INPROGRESS(p) | READ(r!)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What this heading does is, it defines the different states of completion of a TODO item. The scope of this definition is the file. Although you can define global states, I want these states to only exist in the context of this file. The complete and incomplete states are separated by the pipe character (&lt;code&gt;|&lt;/code&gt;). In this case, WISHLIST, QUEUED, and INPROGRESS are incomplete states and READ is the complete state. The letters in the parentheses are ways to quickly assign each of the states. One last thing you'll notice is the READ state in addition to having a letter also has an exclamation point. The exclamation point tells Emacs to add a timestamp whenever this book transitions to the READ state.&lt;/p&gt;
&lt;h2&gt;Add your book list&lt;/h2&gt;
&lt;p&gt;Once you've define your reading states, it's time to add your books. TODO items in Emacs Org-mode only work in the context of headings. Therefore you have to prefix your books with asterisks. For more information on headlines, see the &lt;a href="https://orgmode.org/manual/Headlines.html"&gt;Headlines section in the Org mode guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here are examples of some books in my reading list.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;* Hackers (Steven Levy)
* Ghost In The Wires (Kevin Mitnick)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since I don't intend to have sub headlines, I just used a top-level headline for each of the books.&lt;/p&gt;
&lt;h2&gt;Assign state&lt;/h2&gt;
&lt;p&gt;Now that you've populated your list of books, it's time to assign states to them.&lt;/p&gt;
&lt;p&gt;To assign a state:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Place your pointer / cursor on the same line as the book you want to assign the state to.&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;Ctrl-c&lt;/code&gt; + &lt;code&gt;Ctrl-t&lt;/code&gt; or in Emacs notation &lt;code&gt;C-c C-t&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A buffer opens prompting you to choose a state. Enter one of the letters to choose your state. For example, if you want to apply the QUEUED state to your book, press the letter &lt;code&gt;q&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The state you selected is prepended to your book. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;* QUEUED Hackers (Steven Levy)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Update state&lt;/h2&gt;
&lt;p&gt;To update the state, you can follow the steps in the assign state section or you can cycle through the various states by pressing &lt;code&gt;Alt-Left&lt;/code&gt; or &lt;code&gt;Alt-Right&lt;/code&gt; or in Emacs notation &lt;code&gt;M-LEFT&lt;/code&gt; or &lt;code&gt;M-RIGHT&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Marking a book as READ&lt;/h2&gt;
&lt;p&gt;To assign the READ state to a book, you need to update its state. The only difference between READ and the other states is, a timestamp is applied when the book transitions to the state.&lt;/p&gt;
&lt;p&gt;Here's an example of what a book in the READ state looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;* READ In The Beginning...Was The Command Line (Neal Stephenson) 
- State &amp;quot;READ&amp;quot;       from &amp;quot;INPROGRESS&amp;quot; [2022-07-26 Tue 00:18]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Querying your reading list&lt;/h2&gt;
&lt;p&gt;Org mode has agenda views that allow you to view TODO, timestamped, and tagged items in a single place. One of the built-in agendas that is useful for querying TODO items is the match view. The match view shows headlines based on the tags, properties, and TODO state associated with them. For more information, see the &lt;a href="https://orgmode.org/manual/Agenda-Views.html"&gt;Agenda views section in the Org-mode guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Before viewing your books in an agenda view, you need to add the file containing your reading list to the list of files tracked by Org. To do so, open the file in Emacs and press &lt;code&gt;Ctrl-c + [&lt;/code&gt; or in Emacs notation &lt;code&gt;C-c [&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To launch the match agenda view in org-mode:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Press &lt;code&gt;Alt-x&lt;/code&gt; or in Emacs notation &lt;code&gt;M-x&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;In the minibuffer, type &lt;code&gt;org-agenda&lt;/code&gt; and press Enter.&lt;/li&gt;
&lt;li&gt;A buffer appears prompting you to select a view. Press the letter &lt;code&gt;m&lt;/code&gt; to choose the match view.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Unread items&lt;/h3&gt;
&lt;p&gt;In this case, I want to see all the unfinished books which in this case are under the WISHLIST, QUEUED, and INPROGRESS states.&lt;/p&gt;
&lt;p&gt;In the match view, enter the following query into the minibuffer:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;TODO=&amp;quot;WISHLIST&amp;quot;|TODO=&amp;quot;QUEUED&amp;quot;|TODO=&amp;quot;INPROGRESS&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Enter&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The result of the query should look similar to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Headlines with TAGS match: TODO=&amp;quot;WISHLIST&amp;quot;|TODO=&amp;quot;QUEUED&amp;quot;|TODO=&amp;quot;INPROGRESS&amp;quot;
Press ‘C-u r’ to search again
  reading-list:QUEUED Hackers (Steven Levy)
  reading-list:WISHLIST Ghost In The Wires (Kevin Mitnick)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Read items&lt;/h3&gt;
&lt;p&gt;To query for read items, you go through the same process, except the query is different.&lt;/p&gt;
&lt;p&gt;In the match view, enter the following query into the minibuffer:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;TODO=&amp;quot;READ&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Enter&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The result of the query should look similar to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Headlines with TAGS match: TODO=&amp;quot;READ&amp;quot;
Press ‘C-u r’ to search again
  reading-list:READ In The Beginning...Was The Command Line (Neal Stephenson)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Reading lists are just one of the many things you can track using Emacs and Org mode. You can use this general process to track movies, TV shows, podcasts, etc. Thanks to the extensibility of Emacs, the options are endless!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/manage-reading-list-org-mode-emacs?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/manage-reading-list-org-mode-emacs</guid>
      <pubDate>07/26/2022 19:15</pubDate>
      <category>#linux</category>
      <category>#emacs</category>
      <category>#organization</category>
    </item>
    <item>
      <title>The lqvlc network protocol</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/lqvlc-network-protocol-firefox?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/lqvlc-network-protocol-firefox&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Whether it's videos, online radio, podcasts, or my personal music collection, VLC is my default player. I especially enjoy the ability of playing an online audio or video stream using the URL. What I don't like about that process is that I usually have to manually copy the link and use either the terminal or graphical user interface to play the stream. While not a large number of additional steps, it's still not ideal. The experience I would like is to automatically launch VLC whenever I click a link to an audio or video stream. That's possible using custom network protocols in Firefox. In this guide, I'll show how I defined and configured my own protocol (lqvlc) in Firefox to automatically launch VLC.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;VLC&lt;/li&gt;
&lt;li&gt;Firefox&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This guide was built to work on unix systems. However, setting up something similar with a PowerShell or CMD script on Windows should work similarly.&lt;/p&gt;
&lt;h2&gt;Network protocols&lt;/h2&gt;
&lt;p&gt;A network protocol defines how data is exchanged and transferred over a network. Some common protocols are HTTP, FTP, and SSH. In the context of browsers, the most common protocol is HTTP and the way the browser identifies the protocol is based on the scheme portion of the URL. So for the website &lt;a href="http://lqdev.me"&gt;http://lqdev.me&lt;/a&gt;, http is the scheme that tells the browser which protocol to use. For more information on network protocols and URLs, see the article &lt;a href="https://developer.mozilla.org/docs/Learn/Common_questions/What_is_a_URL"&gt;What is a URL?&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Firefox has a set of standard protocols like HTTP that are configured out of the box. However, you have the option of configuring a custom protocol.&lt;/p&gt;
&lt;h2&gt;What is the lqvlc protocol&lt;/h2&gt;
&lt;p&gt;Now that you have a general idea of what network protocols are and how browsers handle them, it's time to introduce the lqvlc protocol. The intent of the lqvlc protocol is to automatically take a URL from an online audio or video stream and launch it in VLC. lqvlc protocol URLs use lqvlc as the scheme instead of http.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;table class="table"&gt;
    &lt;tr&gt;
        &lt;th&gt;Original Audio Stream Link&lt;/th&gt;
        &lt;th&gt;lqvlc Link&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://somafm.com/u80s64.pls"&gt;https://somafm.com/u80s64.pls&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="lqvlc://somafm.com/u80s64.pls"&gt;lqvlc://somafm.com/u80s64.pls&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;    
&lt;/table&gt;
&lt;p&gt;You'll notice the only difference is the scheme. That's enough information for the browser to know, lqvlc is a protocol I need to handle in some way. In the next sections, I'll describe what you need to do to get Firefox to recognize and handle the lqvlc protocol.&lt;/p&gt;
&lt;h2&gt;Configure network protocols in Firefox&lt;/h2&gt;
&lt;p&gt;To configure a custom network protocol&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Launch Firefox&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the address bar, navigate to &lt;code&gt;about:config&lt;/code&gt; and select &lt;strong&gt;Accept the Risk and continue&lt;/strong&gt;. As the warning mentions, these are advanced configurations so proceed with caution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add the following configuration preferences:&lt;/p&gt;
 &lt;table class="table"&gt;
     &lt;tr&gt;
         &lt;th&gt;Configuration Preference&lt;/th&gt;
         &lt;th&gt;Type&lt;/th&gt;
         &lt;th&gt;Value&lt;/th&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
         &lt;td&gt;&lt;i&gt;network.protocol-handler.expose.lqvlc&lt;/i&gt;&lt;/td&gt;
         &lt;td&gt;Boolean&lt;/td&gt;
         &lt;td&gt;false&lt;/td&gt;
     &lt;/tr&gt;                            
     &lt;tr&gt;
         &lt;td&gt;&lt;i&gt;network.protocol-handler.external.lqvlc&lt;/i&gt;&lt;/td&gt;
         &lt;td&gt;Boolean&lt;/td&gt;
         &lt;td&gt;true&lt;/td&gt;
     &lt;/tr&gt;
     &lt;tr&gt;
         &lt;td&gt;&lt;i&gt;network.protocol-handler.warn-external.lqvlc&lt;/i&gt;&lt;/td&gt;
         &lt;td&gt;Boolean&lt;/td&gt;
         &lt;td&gt;true&lt;/td&gt;
     &lt;/tr&gt;
 &lt;/table&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/177898847-79cce773-9275-4e4d-8e7c-a6b5fbfe8f48.png" class="img-fluid" alt="Firefox lqvlc network protocol handler configuration preferences" /&gt;&lt;/p&gt;
&lt;p&gt;The last two tell Firefox to let an external application handle files with the protocol you've defined. In this case, that's &lt;code&gt;lqvlc&lt;/code&gt;. If you're configuring your own protocol, you'd replace the lqvlc in the configuration preference with your own.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Create a handler&lt;/h2&gt;
&lt;p&gt;Now that Firefox knows that it must use an external application to handle lqvlc URLs, it's time to configure the handler. The easiest way to create a handler is to use a bash script.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a new file called &lt;em&gt;lqvlc-handler.sh&lt;/em&gt; with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;#!/bin/bash

link=$1

vlc &amp;quot;${link/lqvlc/http}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The script takes the first argument it's passed, an lqvlc URL, replaces the scheme with &lt;code&gt;http&lt;/code&gt;, and launches it in VLC.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make the file executable. In the terminal, run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;chmod +x lqvlc-handler.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That's it! Now you just need to test it out.&lt;/p&gt;
&lt;h2&gt;Test it out&lt;/h2&gt;
&lt;p&gt;To use it for your own purposes, you'd have to create or update an HTML page and set the &lt;code&gt;href&lt;/code&gt; in your anchor tags to use &lt;code&gt;lqvlc&lt;/code&gt; instead of &lt;code&gt;http&lt;/code&gt;. However, I've already configured lqvlc links in my own website. If you go to the &lt;a href="/radio"&gt;radio page&lt;/a&gt; on my website, you can click on any of the lqvlc links. Alternatively, You can also click on the link to the &lt;a href="lqvlc://somafm.com/u80s64.pls"&gt;SOMA FM Underground 80's station lqvlc link&lt;/a&gt;. A prompt will pop up asking you to choose which app you want to use to handle the link. Select the lqvlc-handler.sh script you created. If everything is configured correctly, VLC should launch and the radio stations should start playing.&lt;/p&gt;
&lt;h2&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;Although I've gotten this working for my personal site, I'd like to be able to use the lqvlc protocol in any website I visit that has links to audio and video streams. Arguably, I can also launch lqvlc links from other websites. However, that would require website owners to configure them. The next step I think to make that happen would be to create an Add-On that automatically rewrites stream links to use the lqvlc protocol.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/lqvlc-network-protocol-firefox?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/lqvlc-network-protocol-firefox</guid>
      <pubDate>07/07/2022 18:47</pubDate>
      <category>#firefox</category>
      <category>#vlc</category>
      <category>#web</category>
      <category>#open-source</category>
      <category>#linux</category>
    </item>
    <item>
      <title>Install Manjaro RTL8821CE WiFi drivers</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/install-rtl8821ce-manjaro?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/install-rtl8821ce-manjaro&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;Until recently, I was usign an HP ProBook 430 G1 with Manjaro and a Surface Go (1st Gen) for my computing needs. To cut down on the devices I used, I purchased an &lt;a href="https://www.asus.com/us/laptops/for-home/everyday-use/asus-l210/"&gt;ASUS L210MA&lt;/a&gt; which is about the size of the Surface Go but I can run Manjaro on it. Although it doesn't have the same storage or compute power as the ProBook, I've set up a VM in Azure I can use for those instances where I need more compute power than the ASUS laptop can provide. As is sometimes the case, when you install Linux on a new PC, you have to install the right drivers to use your WiFi and other peripheral devices. In this guide I'll show how to install the RTL-8821-CE WiFi drivers.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;The instructions on this guide were run on a Manjaro XCFE 21.2.6.&lt;/p&gt;
&lt;h2&gt;Enable AUR and Install yay&lt;/h2&gt;
&lt;p&gt;The WiFi drivers you'll need are hosted in the &lt;a href="https://aur.archlinux.org/"&gt;Arch User Repository (AUR)&lt;/a&gt;. Before installing any packages, you'll have to enable it on your PC. To do so, install the &lt;code&gt;yay&lt;/code&gt; AUR helper package by running the following command in the terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo pacman -S git
git clone https://aur.archlinux.org/yay.git
cd yay
makepkg -si
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Install Kernel Headers&lt;/h2&gt;
&lt;p&gt;The first thing yuo'll need to install are your kernel headers. To check which version of the kernel you're using, run the following command in the terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;uname -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output should look similar to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;5.15.41-1-MANJARO
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first two values are your major kernel version. For example, in the output above the kernel version is 5.15.&lt;/p&gt;
&lt;p&gt;To install the kernel headers, run the following command in the terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo pacman -S linux-headers
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make sure that the kernel version that's installed is the same as the one output after running the &lt;code&gt;uname -r&lt;/code&gt; command.&lt;/p&gt;
&lt;h2&gt;Install RTL8821CE drivers&lt;/h2&gt;
&lt;p&gt;Once you've installed the headers, it's time to install the WiFi drivers. Use &lt;code&gt;yay&lt;/code&gt; to run the following command in the terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;yay -S rtl8821ce-dkms-git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After some time, the packages should be installed on your device. Reboot your PC and you should now be able to connect to the internet wirelessly&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/install-rtl8821ce-manjaro?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/install-rtl8821ce-manjaro</guid>
      <pubDate>06/11/2022 15:48</pubDate>
      <category>#networking</category>
      <category>#manjaro</category>
      <category>#linux</category>
    </item>
    <item>
      <title>Install OverDrive Media Console on Manjaro</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/overdrive-wine-manjaro?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/overdrive-wine-manjaro&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;For the last few months, Manjaro has been the operating system I use as my daily driver for personal tasks. I still have my Surface Go tablet which runs Windows 10 that I use for tasks line syncing my Garmin watch and GPS. On one of the &lt;a href="/feed/ipod-touch-discontinued.html"&gt;posts in my feed&lt;/a&gt;, I briefly mentioned my audio solution for podcasts and audiobooks which mainly consists of a standalone MP3 player (&lt;a href="https://fiio.com/m6"&gt;FiiO M6&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;For podcasts, I manage them through my &lt;a href="https://newsblur.com/"&gt;NewsBlur RSS feed reader&lt;/a&gt;. When I want to download them for offline listening, I've written a .NET console app (&lt;a href="https://github.com/lqdev/podnet"&gt;podnet&lt;/a&gt;) to manage that process for me. Audiobooks are a little trickier. Public domain works I get through &lt;a href="https://librivox.org/"&gt;Librivox&lt;/a&gt;. The limitation there though is that I'm only public domain works are available. Therefore if I want other audiobooks, I either have to purchase them (which has its own issues because of DRM) or borrow them from my local library. I prefer to borrow my books and audiobooks from the library. Not only am I supporting my local library but I also save money in the process.&lt;/p&gt;
&lt;p&gt;Many libraries including my own manage digital assets through &lt;a href="https://www.overdrive.com/"&gt;OverDrive&lt;/a&gt;. OverDrive offers a variety of ways for transferring books and audiobooks to your devices. Linux however is not one of those solutions. Until February 2022, OverDrive provided a Windows desktop application (OverDrive Media Console) for managing books and audiobooks. Although there is no Linux desktop application, Linux has Wine. &lt;a href="https://www.winehq.org/"&gt;Wine&lt;/a&gt; according to their website is &amp;quot;...a compatibility layer capable of running Windows applications on several POSIX-compliant operating systems, such as Linux, macOS, &amp;amp; BSD.&amp;quot;.&lt;/p&gt;
&lt;p&gt;In this post, I'll show how you can use Wine to install OverDrive Media Console on a PC running the Manjaro Linux distribution.&lt;/p&gt;
&lt;h2&gt;Install Wine&lt;/h2&gt;
&lt;p&gt;In order to install OverDrive Media Console, you'll need to install Wine. To do so:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open the terminal.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Install Wine and all its dependencies with the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo pacman -S wine winetricks wine-mono wine_gecko
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Install OverDrive Media Console&lt;/h2&gt;
&lt;p&gt;The next step is to install the OverDrive Media Console application for Windows desktop.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Download the latest version of &lt;a href="https://www.overdrive.com/apps/overdrive./"&gt;OverDrive Media Console for Windows Desktop application&lt;/a&gt;. This should be a Microsoft Installer file with the &lt;code&gt;.msi&lt;/code&gt; extension. Note that as of February 2022, the desktop application is &lt;a href="https://help.overdrive.com/en-us/0733.html?tocpath=Home%7CGet%20help%20with%20the%20OverDrive%20app%7CWindows%20(desktop)%7C_____7"&gt;no longer available for install&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open the terminal and use Wine to install OverDrive Media Console with the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;wine msiexec /i ODMediaConsoleSetup.msi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Update the name of the &lt;code&gt;.msi&lt;/code&gt; file as needed.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Run OverDrive Media Console&lt;/h2&gt;
&lt;p&gt;Once the installation is complete, OverDrive Media Console should be in your list of applications. Run it as you would any other app.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/167964715-bb8bfb21-5f72-4a86-9fb0-33c9d1fe9eae.png" class="img-fluid" alt="OverDrive Media Console running on Manjaro Linux" /&gt;&lt;/p&gt;
&lt;p&gt;Here's a guide for information on getting started with &lt;a href="https://help.overdrive.com/en-us/categories/getting-started.htm"&gt;OverDrive&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/overdrive-wine-manjaro?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/overdrive-wine-manjaro</guid>
      <pubDate>05/11/2022 19:22</pubDate>
      <category>#linux</category>
      <category>#books</category>
      <category>#audiobooks</category>
      <category>#manjaro</category>
    </item>
    <item>
      <title>How to listen to internet radio using VLC</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/how-to-listen-internet-radio-using-vlc?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/how-to-listen-internet-radio-using-vlc&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Music streaming services like Spotify generally do a good job of making recommendations based on your music tastes. By applying algorithms to your listening history, they're able to provide you with a listening experience tailored to you. When it comes to music discovery though, it can be difficult to find new music so you often end up listening to music that sounds very similar. Internet radio can help expose you to music you otherwise wouldn't normally listen to. Radio stations like those on &lt;a href="https://somafm.com/"&gt;SomaFM&lt;/a&gt; provide a wide variety of high quality music channels. There's other online radio stations out there, many which provide a built-in audio player. As I've written in previous posts though (&lt;a href="/posts/how-to-watch-twitch-using-vlc.html"&gt;How to watch Twitch streams using VLC&lt;/a&gt; &amp;amp; &lt;a href="/feed/rss-vlc-podcast-management.html"&gt;Podcast management with RSS &amp;amp; VLC&lt;/a&gt;), VLC is a versatile tool. As a result, it's no surprise you can also use it to listen to online radio streams. This post is mostly a note to myself, but I encourage you to try it as well.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.videolan.org/index.html"&gt;VLC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The commands below were run on Linux. Since VLC is cross-platform, the same commands should work on those platforms as well.&lt;/p&gt;
&lt;h2&gt;Listen to internet radio&lt;/h2&gt;
&lt;p&gt;The first thing you'll need is the URL of the stream you want to listen to. Usually it's provided on the station's website. You can also browse through &lt;a href="https://www.radio-browser.info/"&gt;RadioBrowser&lt;/a&gt; to find a station and its stream URL.&lt;/p&gt;
&lt;p&gt;Once you have the URL, in the terminal, enter the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;vlc &amp;lt;STREAM-URL&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Replace &lt;code&gt;&amp;lt;STREAM-URL&amp;gt;&lt;/code&gt; with the address of your stream.&lt;/p&gt;
&lt;p&gt;After a few seconds, VLC launches and your stream should start playing.&lt;/p&gt;
&lt;p&gt;If you prefer to stream audio without VLC's visual interface, use the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;cvlc &amp;lt;STREAM-URL&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that without the UI you won't have access to any playback controls.&lt;/p&gt;
&lt;p&gt;Happy listening!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/how-to-listen-internet-radio-using-vlc?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/how-to-listen-internet-radio-using-vlc</guid>
      <pubDate>12/28/2021 16:24</pubDate>
      <category>#vlc</category>
      <category>#open-source</category>
      <category>#radio</category>
    </item>
    <item>
      <title>Sending Webmentions with F#</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/sending-webmentions-fsharp-fsadvent?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/sending-webmentions-fsharp-fsadvent&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This post is part of the &lt;a href="https://sergeytihon.com/2021/10/18/f-advent-calendar-2021"&gt;F# Advent 2021&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Webmentions is a W3C specification that defines a standard way of implementing notifications between websites. It enables rich interactions between websites using standard protocols. According to the spec, &amp;quot;Webmention is a simple way to notify any URL when you mention it on your site. From the receiver's perspective, it's a way to request notifications when other sites mention it&amp;quot;.&lt;/p&gt;
&lt;p&gt;If you're interested in learning more about the spec and overall use cases, see the following links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://boffosocko.com/2018/07/19/webmentions-enabling-better-communication-on-the-internet-2/"&gt;Webmentions: Enabling better communication on the internet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aaronparecki.com/2018/06/30/11/your-first-webmention"&gt;Sending your first Webmention from scratch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.w3.org/TR/webmention/"&gt;Webmentions W3C specification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://webmention.rocks/"&gt;Webmentions implementation tests&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Webmentions protocol has send and receive components. In this post, I'll go over a lightweight implementation for sending Webmentions using F# and test it using the &lt;a href="https://webmention.rocks/"&gt;webmention.rocks&lt;/a&gt; website. Source code for this post can be found at the &lt;a href="https://github.com/lqdev/fsadvent-2021-webmentions"&gt;fsadvent-2021-webmentions GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Sending Webmention workflow&lt;/h2&gt;
&lt;p&gt;Let's say you want to use webmentions to reply to or mention content from another site. The general workflow works as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;You create a document (source) that mentions content from another website (target).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You perform discovery on the target website for the webmention endpoint URL. The URL might be in one of three places:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HTTP response headers.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;link&amp;gt;&lt;/code&gt; tag with &lt;code&gt;rel=webmention&lt;/code&gt; attribute.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; tag with &lt;code&gt;rel=webmention&lt;/code&gt; attribute.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The discovery is performed in order and each subsequent option works as a fallback of the other.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You make an HTTP POST request notifying the source that you've mentioned their article on your site. The request body is in &lt;code&gt;x-www-form-urlencoded&lt;/code&gt; form containing the &lt;code&gt;source&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; parameters where source is the URL of your article and the target is the URL of article being mentioned.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more details on this workflow, see the &lt;a href="https://www.w3.org/TR/webmention/#sending-webmentions"&gt;Sending Webmentions section&lt;/a&gt; of the specification.&lt;/p&gt;
&lt;h2&gt;Create a document&lt;/h2&gt;
&lt;p&gt;The document can be any publicly hosted HTML document as long as you have a source URL to provide during your notification request.&lt;/p&gt;
&lt;p&gt;For example, you can have something like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;!doctype html&amp;gt;
&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;
    &amp;lt;meta http-equiv=&amp;quot;X-UA-Compatible&amp;quot; content=&amp;quot;IE=edge&amp;quot;&amp;gt;
    &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;
    &amp;lt;title&amp;gt;F# Advent 2021 - Webmentions&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
    &amp;lt;body&amp;gt;
        &amp;lt;a href=&amp;quot;https://webmention.rocks/test/1&amp;quot;&amp;gt;F# Advent 2021 - This is a great post&amp;lt;/a&amp;gt;
    &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;a href="http://microformats.org/wiki/h-entry"&gt;microformats&lt;/a&gt; though, you can annotate your HTML to provide more context to the receiver. The same document with microformats providing the author, content, and source document being replied to might look like the following with microformats:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;
    &amp;lt;meta http-equiv=&amp;quot;X-UA-Compatible&amp;quot; content=&amp;quot;IE=edge&amp;quot;&amp;gt;
    &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;
    &amp;lt;title&amp;gt;F# Advent 2021 - Webmentions&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
    &amp;lt;div class=&amp;quot;h-entry&amp;quot;&amp;gt;
        &amp;lt;div class=&amp;quot;u-author h-card&amp;quot;&amp;gt;
            &amp;lt;a href=&amp;quot;http://lqdev.me&amp;quot; class=&amp;quot;u-url p-name&amp;quot;&amp;gt;lqdev&amp;lt;/a&amp;gt;
        &amp;lt;/div&amp;gt;
        &amp;lt;p&amp;gt;In reply to: &amp;lt;a href=&amp;quot;https://webmention.rocks/test/1&amp;quot; class=&amp;quot;u-in-reply-to&amp;quot;&amp;gt;@webmention.rocks&amp;lt;/a&amp;gt;&amp;lt;/p&amp;gt;
        &amp;lt;p class=&amp;quot;e-content&amp;quot;&amp;gt;F# Advent 2021 - This is a great post&amp;lt;/p&amp;gt;
    &amp;lt;/div&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Receiving Webmentions is beyond the scope of this post, so I'll skip over them. Once you have a document and make it publicly accessible to the web, it's time to find where to send notifications to.&lt;/p&gt;
&lt;h2&gt;Discover Webmention endpoint URL&lt;/h2&gt;
&lt;p&gt;To discover where to send the Webmention to, send an HTTP GET or HTTP HEAD request to the target URL. Once you get the response, the Webmention endpoint URL should be in one of three places:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;HTTP headers&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;GET /test/1 HTTP/1.1
Host: webmention.rocks
HTTP/1.1 200 OK
Link: &amp;lt;https://webmebtion.rocks/test/1/webmention&amp;gt;; rel=&amp;quot;webmention&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;&amp;lt;link&amp;gt;&lt;/code&gt; tag&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;html&amp;gt;
    &amp;lt;head&amp;gt;
    ...
        &amp;lt;link href=&amp;quot;http://webmention.rocks/test/1/webmention&amp;quot; rel=&amp;quot;webmention&amp;quot; /&amp;gt;
    ...
    &amp;lt;/head&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; tag&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;html&amp;gt;
    &amp;lt;body&amp;gt;
    ....
    &amp;lt;a href=&amp;quot;http://webmention.rocks/test/1/webmention&amp;quot; rel=&amp;quot;webmention&amp;quot;&amp;gt;webmention&amp;lt;/a&amp;gt;
    ...
    &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following examples show how you'd implement the discovery process for each of these scenarios in F#.&lt;/p&gt;
&lt;h3&gt;Discover Webmention URL in header&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;discoverUrlInHeaderAsync&lt;/code&gt; function uses the .NET HttpClient to make an HTTP HEAD request to get the headers from the target URL provided. Then, a search is performed for a &amp;quot;link&amp;quot; header containing the text &amp;quot;webmention&amp;quot;. Once the header is found, it's sanitized to extract the Webmention endpoint URL of the target site.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let discoverUrlInHeaderAsync (url:string) =
    async {
        // Initialize HttpClient
        use client = new HttpClient()

        // Prepare request message
        let reqMessage = new HttpRequestMessage(new HttpMethod(&amp;quot;HEAD&amp;quot;), url)
        
        // Send request
        let! response = client.SendAsync(reqMessage) |&amp;gt; Async.AwaitTask

        // Get request headers
        let responseHeaders = 
            [
                for header in response.Headers do
                    header.Key.ToLower(), header.Value
            ]

        // Look for webmention header
        try
            // Find &amp;quot;link&amp;quot; header that contains &amp;quot;webmention&amp;quot;
            let webmentionHeader =
                responseHeaders
                |&amp;gt; Seq.filter(fun (k,_) -&amp;gt; k = &amp;quot;link&amp;quot;)
                |&amp;gt; Seq.map(fun (_,v) -&amp;gt; v |&amp;gt; Seq.filter(fun header -&amp;gt; header.Contains(&amp;quot;webmention&amp;quot;)))
                |&amp;gt; Seq.head
                |&amp;gt; List.ofSeq
                |&amp;gt; List.head

            // Get first part of &amp;quot;link&amp;quot; header
            let webmentionUrl = 
                webmentionHeader.Split(';')
                |&amp;gt; Array.head

            // Remove angle brackets from URL
            let sanitizedWebmentionUrl = 
                webmentionUrl
                    .Replace(&amp;quot;&amp;lt;&amp;quot;,&amp;quot;&amp;quot;)
                    .Replace(&amp;quot;&amp;gt;&amp;quot;,&amp;quot;&amp;quot;)
                    .Trim()

            return Some sanitizedWebmentionUrl                    

        with
            | _ -&amp;gt; return None
        
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Discover Webmention URL endpoint in link tag&lt;/h3&gt;
&lt;p&gt;The next place to check for a Webmention endpoint URL is a &lt;code&gt;&amp;lt;link&amp;gt;&lt;/code&gt; tag in the document. In this example, the &lt;code&gt;discoverUrlInLinkTagAsync&lt;/code&gt; function uses &lt;code&gt;FSharp.Data&lt;/code&gt; library to get the contents of the target URL and parse the HTML contents to find a link tag with a &lt;code&gt;rel=webmention&lt;/code&gt; attribute value.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let discoverUrlInLinkTagAsync (url:string) = 
    async {
        // Load HTML document
        let! htmlDoc = HtmlDocument.AsyncLoad(url)

        // Get webmention URL from &amp;quot;&amp;lt;link&amp;gt;&amp;quot; tag
        try
            let webmentionUrl = 
                htmlDoc.CssSelect(&amp;quot;link[rel='webmention']&amp;quot;)
                |&amp;gt; List.map(fun link -&amp;gt; link.AttributeValue(&amp;quot;href&amp;quot;))
                |&amp;gt; List.head

            return Some webmentionUrl
        with
            | _ -&amp;gt; return None
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Discover Webmention endpoint URL in anchor tag&lt;/h3&gt;
&lt;p&gt;The last place to check for the Webmention endpoint URL is in an anchor tag in the contents of the target URL. In this example, the &lt;code&gt;discoverUrlInAnchorTagAsync&lt;/code&gt; uses the &lt;code&gt;FSharp.Data&lt;/code&gt; library to get the contents of the target URL and parse the HTML contents to find an anchor tag with a &lt;code&gt;rel=webmention&lt;/code&gt; attribute value.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let discoverUrlInAnchorTagAsync (url:string) = 
    async {
        // Load HTML document
        let!  htmlDoc = HtmlDocument.AsyncLoad(url)

        // Get webmention URL from &amp;quot;&amp;lt;a&amp;gt;&amp;quot; tag
        try
            let webmentionUrl = 
                htmlDoc.CssSelect(&amp;quot;a[rel='webmention'&amp;quot;)
                |&amp;gt; List.map(fun a -&amp;gt; a.AttributeValue(&amp;quot;href&amp;quot;))
                |&amp;gt; List.head

            return Some webmentionUrl
        with
            | _ -&amp;gt; return None
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the utility functions are in place to handle the different scenarios, create a new function to perform the discovery workflow.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;// Apply webmention URL discovery workflow
// 1. Check header
// 2. Check link tag
// 3. Check anchor tag
let discoverWebmentionUrlAsync (url:string) = 
    async {
        let! headerUrl = discoverUrlInHeaderAsync url
        let! linkUrl = discoverUrlInLinkTagAsync url
        let! anchorUrl = discoverUrlInAnchorTagAsync url

        // Aggregate results
        let discoveryResults = [headerUrl; linkUrl; anchorUrl]

        // Unwrap and take the first entry containing a value
        let webmentionUrl = 
            discoveryResults
            |&amp;gt; List.choose(fun url -&amp;gt; url)
            |&amp;gt; List.head

        return webmentionUrl
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;discoverWebmentionUrlAsync&lt;/code&gt; calls all of the discovery utility methods and chooses only the results that successfully extracted a Webmention endpoint URL. In the event of multiple endpoints, the first one is takes precedence.&lt;/p&gt;
&lt;h2&gt;Send Webmention&lt;/h2&gt;
&lt;p&gt;Now that you have a publicly accessible document and the Webmention URL endpoint to send your Webmention to, it's time to send it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let sendWebMentionAsync (url:string) (req:IDictionary&amp;lt;string,string&amp;gt;) = 
    async {
        use client = new HttpClient()
        let content = new FormUrlEncodedContent(req)
        let! response = client.PostAsync(url, content) |&amp;gt; Async.AwaitTask
        return response.IsSuccessStatusCode
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;sendWebMentionAsync&lt;/code&gt; function uses the .NET HttpClient to send an HTTP POST request to the Webmention endpoint URL you just extracted.&lt;/p&gt;
&lt;p&gt;Define your source and target URLs&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let sourceUrl = new Uri(&amp;quot;https://raw.githubusercontent.com/lqdev/fsadvent-2021-webmentions/main/reply.html&amp;quot;)
let targetUrl = new Uri(&amp;quot;https://webmention.rocks/test/1&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The source URL I used in this example is hosted on &lt;a href="https://raw.githubusercontent.com/lqdev/fsadvent-2021-webmentions/main/reply.html"&gt;GitHub&lt;/a&gt; and the target URL is the first implementation test in the &lt;a href="https://webmention.rocks/test/1"&gt;webmention.rocks&lt;/a&gt; website.&lt;/p&gt;
&lt;p&gt;Then, create a function to run the entire workflow end-to-end.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let runWebmentionWorkflow () = 
    async {
        // Discover webmention endpoint URL of target URL
        let! discoveredUrl = 
            targetUrl.OriginalString
            |&amp;gt; discoverWebmentionUrlAsync

        // Construct URL depending on whether it's absolute or relative
        let authority = targetUrl.GetLeftPart(UriPartial.Authority)

        let constructedUrl = 
            match (discoveredUrl.Contains(&amp;quot;http&amp;quot;)) with
            | true -&amp;gt; discoveredUrl
            | false -&amp;gt; 
                let noQueryUrl = 
                    discoveredUrl.Split('?')
                    |&amp;gt; Array.head
                    
                $&amp;quot;{authority}{noQueryUrl}&amp;quot;

        // Prepare webmention request data
        let reqData = 
            dict [
                (&amp;quot;source&amp;quot;, sourceUrl.OriginalString)
                (&amp;quot;target&amp;quot;, targetUrl.OriginalString)
            ]

        // Send web mentions
        return! sendWebMentionAsync constructedUrl reqData
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that you have everything set up, you're ready to send your webmention!&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;runWebmentionWorkflow ()
|&amp;gt; Async.RunSynchronously
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If your post is successful, you should see it on the target URL.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/145919398-2576bfef-3a5d-44f4-9813-9cf933539ede.png" class="img-fluid" alt="Display of Webmention post on webmention.rocks" /&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I showed how you can use F# to implement sending webmentions using F#. This is not a full implementation as there are still scenarios I need to account for like receiving webmentions. There are community maintained &lt;a href="https://indieweb.org/Webmention-developer#Libraries"&gt;libraries&lt;/a&gt;, &lt;a href="https://indieweb.org/Webmention#Publishing_Software"&gt;plugins&lt;/a&gt;, and &lt;a href="https://indieweb.org/Webmention#Services"&gt;services&lt;/a&gt; to simplify the process, but because the specification is built on open protocols and standards, it's possible to build your own implementation in F# and integrate it into your website. Happy coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/sending-webmentions-fsharp-fsadvent?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/sending-webmentions-fsharp-fsadvent</guid>
      <pubDate>12/13/2021 20:53</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Super simple captcha</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/super-simple-captcha?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/super-simple-captcha&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I've been thinking about implementing webmentions on this site. According to the spec, &amp;quot;Webmention is a simple way to notify any URL when you mention it on your site. From the receiver's perspective, it's a way to request notifications when other sites mention it&amp;quot;. I won't get into the technical details of webmentions in this post, but the simplest way to implement webmentions is to have a text input box on your site for each of your articles, kind of like a comment box. When someone wants to mention your post on their site, they can use the text input box on your site to add the URL from their site where they mention your post. When the user submits the webmention, there's some endpoint that listens for webmention submissions and processes them accordingly. If you're interested in learning more about webmentions, you can check out the following resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.w3.org/TR/webmention/"&gt;Webmentions spec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://webmention.rocks/"&gt;Webmention Rocks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://yewtu.be/watch?v=ZOlkS6xP2Zk"&gt;Sending Your First Webmention - IndieWebCamp Düsseldorf 2019&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While it's great that you can mention and comment on other people's content from your site, like comments, writing a script to submit spam is relatively simple. Therefore, I want to create some sort of &lt;a href="https://en.wikipedia.org/wiki/CAPTCHA"&gt;CAPTCHA&lt;/a&gt; challenge as an initial form of validation to prevent spam. At the same time, I don't want to have any external dependencies. In this post, I'll show how I went about implementing a captcha-like solution with zero dependencies to prevent spam submissions.&lt;/p&gt;
&lt;h2&gt;Simple captcha&lt;/h2&gt;
&lt;p&gt;The solution I came up with asks the users to add the day of the month (1-31) to a random number between 1-100. While not entirely foolproof, it's &amp;quot;complex&amp;quot; enough that it's not the same thing every time.&lt;/p&gt;
&lt;p&gt;Below is what the implementation looks like.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/138604591-dfe4c301-78fe-4338-a751-799b420a1791.png" class="img-fluid" alt="Captcha implementation asking for date" /&gt;&lt;/p&gt;
&lt;p&gt;The HTML markup for the webpage looks like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;
    &amp;lt;meta http-equiv=&amp;quot;X-UA-Compatible&amp;quot; content=&amp;quot;IE=edge&amp;quot;&amp;gt;
    &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;
    &amp;lt;title&amp;gt;Super Simple Captcha&amp;lt;/title&amp;gt;
    &amp;lt;script src=&amp;quot;main.js&amp;quot; type=&amp;quot;text/javascript&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body onload=&amp;quot;setQuery()&amp;quot;&amp;gt;
    &amp;lt;div&amp;gt;
        &amp;lt;span id=&amp;quot;query&amp;quot;&amp;gt;&amp;lt;/span&amp;gt;
        &amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;answerInput&amp;quot;&amp;gt;
        &amp;lt;button onclick=&amp;quot;displayResult()&amp;quot;&amp;gt;Submit&amp;lt;/button&amp;gt;
    &amp;lt;/div&amp;gt;
    &amp;lt;h3 id=&amp;quot;result&amp;quot;&amp;gt;&amp;lt;/h3&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When the &lt;code&gt;body&lt;/code&gt; element of the page loads, it invokes the &lt;code&gt;setQuery&lt;/code&gt; function, which displays the challenge the user is supposed to solve in the &lt;code&gt;query&lt;/code&gt; span element.&lt;/p&gt;
&lt;p&gt;The user then submits their answer via the &lt;code&gt;answerInput&lt;/code&gt; text input box.&lt;/p&gt;
&lt;p&gt;The answer is then checked by invoking the &lt;code&gt;displayResult&lt;/code&gt; function. The &lt;code&gt;displayResult&lt;/code&gt; function checks the user's answer against the expected answer. If the answer is correct, the text &amp;quot;OK&amp;quot; is displayed on the page's &lt;code&gt;result&lt;/code&gt; H3 element. Otherwise, the text &amp;quot;Try again&amp;quot; displays on the webpage.&lt;/p&gt;
&lt;p&gt;All of the code that handles this logic is in the &lt;code&gt;main.js&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-javascript"&gt;let date = new Date()
let day = date.getDate() // Day of the month
let randomNumber = Math.floor(Math.random() * 100) + 1
let answer = day + randomNumber // Expected answer

let setQuery = () =&amp;gt; {
    let element = document.getElementById('query');
    element.innerText = `Enter the sum of ${day} + ${randomNumber}`;    
}

let checkAnswer = () =&amp;gt; {
    let userAnswer = parseInt(document.getElementById(&amp;quot;answerInput&amp;quot;).value);
    return answer === userAnswer
}

let displayResult = () =&amp;gt; {
    let result = checkAnswer() ? &amp;quot;OK&amp;quot; : &amp;quot;Try again&amp;quot; // Ternary function to check if answer is correct
    document.getElementById(&amp;quot;result&amp;quot;).innerText = result;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That's all there is to it!&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I showed how to implement a dependency-free solution to present a challenge to users submitting comments / webmentions to your site. Although the solution isn't foolproof, it's just complex enough any spammers would have to work a little harder. Happy coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/super-simple-captcha?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/super-simple-captcha</guid>
      <pubDate>10/24/2021 13:00</pubDate>
      <category>#web-development</category>
      <category>#html</category>
      <category>#javascript</category>
      <category>#webmentions</category>
    </item>
    <item>
      <title>Automate YAML front-matter generation with custom Visual Studio Code snippets</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/automate-yaml-front-matter-vs-code-snippets?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/automate-yaml-front-matter-vs-code-snippets&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;When creating content with static site generators such as Jekyll, Hexo, and many others, page metadata such as publish date, title, tags, and other details is created using YAML front-matter. Typically these pages are authored in markdown. At the top of each page, there's a YAML formatted section containing all of these details. Sometimes these static site generators include tooling to make it easier to scaffold new pages. In the case of my website, I created my own static site generator. Therefore, scaffolding a new page, especially writing the boilerplate YAML front-matter is cumbersome and time consuming. Since I use Visual Studio Code for authoring content and developing the static site generator, I've decided to use &lt;a href="https://code.visualstudio.com/docs/editor/userdefinedsnippets"&gt;snippets&lt;/a&gt; to help me automate YAML front-matter generation. In this post, I'll show the general process.&lt;/p&gt;
&lt;h2&gt;Create snippets file&lt;/h2&gt;
&lt;p&gt;The first thing you'll want to do is create a snippets file. You can have one or as many as you want. For this website, I have a single snippets file called &lt;em&gt;metadata.code-snippets&lt;/em&gt; for all my YAML front-matter. To create a snippets file:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open the Visual Studio Code command palette.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Enter the command &lt;code&gt;&amp;gt;Preferences: Configure User Snippets&lt;/code&gt; into the text box.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select the &lt;strong&gt;New Snippets file for &amp;lt;REPO-OR-DIRECTORY-NAME&amp;gt;&lt;/strong&gt; from the list of options.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/135934846-a76dfa16-caed-4489-9e50-95183add673d.png" class="img-fluid" alt="Create snippet file from Visual Studio Code command palette" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Provide a name for your snippets file and press &lt;strong&gt;Enter&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A file with the &lt;em&gt;.code-snippets&lt;/em&gt; extension is created in the &lt;em&gt;.vscode&lt;/em&gt; directory inside your project.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One benefit of creating a snippets file and saving it in the &lt;em&gt;.vscode&lt;/em&gt; directory is you can check it into source control and use it anywhere, even github.dev.&lt;/p&gt;
&lt;h2&gt;Create snippets&lt;/h2&gt;
&lt;p&gt;Now that you've created the file, it's time to define your snippets.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Open your snippets file.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/135935306-1cbd48e1-6e48-423c-b87f-cb6f7a8eb085.png" class="img-fluid" alt="Snippet file for luisquintanilla.me" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a new JSON object and define your snippet.&lt;/p&gt;
&lt;p&gt;For example, the snippet for blog posts on this site looks like the following.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;&amp;quot;Article Post metadata&amp;quot;: {
    &amp;quot;scope&amp;quot;: &amp;quot;markdown&amp;quot;,
    &amp;quot;prefix&amp;quot;: &amp;quot;article&amp;quot;,
    &amp;quot;body&amp;quot;: [
        &amp;quot;---&amp;quot;,
        &amp;quot;post_type: \&amp;quot;article\&amp;quot; &amp;quot;,
        &amp;quot;title: \&amp;quot;\&amp;quot;&amp;quot;,
        &amp;quot;published_date: \&amp;quot;$CURRENT_MONTH/$CURRENT_DATE/$CURRENT_YEAR $CURRENT_HOUR:$CURRENT_MINUTE\&amp;quot;&amp;quot;,
        &amp;quot;tags: &amp;quot;,
        &amp;quot;---&amp;quot;
    ],
    &amp;quot;description&amp;quot;: &amp;quot;Blog post article metadata&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let's break down each of the properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;scope&lt;/code&gt;: By default, snippets apply to all languages and projects. In my case, since I intend only to use these snippets in markdown files, I set the scope to &lt;code&gt;markdown&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;prefix&lt;/code&gt;: The prefix is the word in the page that is associated with. When I type &lt;code&gt;article&lt;/code&gt; in a markdown page, a recommendation to use the snippet appears.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;body&lt;/code&gt;: The code or content of my snippet. In this case, the expected output is similar to the following.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;---
post_type: &amp;quot;article&amp;quot; 
title: &amp;quot;&amp;quot;
published_date: &amp;quot;10/04/2021 18:58&amp;quot;
tags: 
---
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;VS Code also has built-in variables that you can use to automatically set date, time, and other values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;description&lt;/code&gt;: A text description of your snippet.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more information on creating snippets, see &lt;a href="https://code.visualstudio.com/docs/editor/userdefinedsnippets#_create-your-own-snippets"&gt;Create your own snippets&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Use your snippets&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a new markdown file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Type your prefix into the file. In my case, for blog posts my prefix is &lt;code&gt;article&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Press &lt;strong&gt;Ctrl + Space&lt;/strong&gt;. For Macs you might have to use Cmd instead of Ctrl.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/135935808-46ca8314-c2be-47f1-9c9b-9b722e37d908.png" class="img-fluid" alt="Populate article front-matter using snippet" /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A tooltip with your snippet's description appears. Press &lt;strong&gt;Enter&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point, your snippet's prefix is replaced with the content defined in the snippet's body.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post I've shown how you can use Visual Studio Code snippets to automate YAML front-matter generation when working with common static site generators that don't provide tooling to scaffold new pages. Snippets can be used for a variety of scenarios and if there's any boilerplate code or content you constantly have to write, they can save you a lot of time.&lt;/p&gt;
&lt;p&gt;Happy writing!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/automate-yaml-front-matter-vs-code-snippets?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/automate-yaml-front-matter-vs-code-snippets</guid>
      <pubDate>10/04/2021 19:58</pubDate>
      <category>#blogging</category>
      <category>#tooling</category>
      <category>#visual-studio</category>
      <category>#visual-studio-code</category>
      <category>#markdown</category>
      <category>#yaml</category>
    </item>
    <item>
      <title>Proofread articles using Microsoft Edge's Immersive Reader and read aloud</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/proofread-read-aloud?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/proofread-read-aloud&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;This will be a relatively short post but it's something I just discovered and thought I'd share. Before publishing a post, I try to proofread and review the article to make sure spelling, grammar, and flow all make sense. Part of that process involves reading the article aloud. I'll miss things from time to time but generally it works well.&lt;/p&gt;
&lt;p&gt;I've known about read aloud in general, but have often found voices to be robotic and sound unnatural when they encounter words outside their vocabulary. However, I recently tried out the read aloud feature in Microsoft Edge and was blown away at how natural the voice sounds and even how well it enunciates technical terms like product and programming API names. Here's what it looks like when reading one of the posts on my site.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/134442715-e0af0c4a-a522-49a9-a0b1-ac658b284c68.png" class="img-fluid" alt="Microsoft Edge Immersive Reader read aloud reading blog post" /&gt;&lt;/p&gt;
&lt;p&gt;If you do any form of writing that can be displayed in Microsoft Edge, I highly encourage you to give read aloud a try. For more information, see &lt;a href="https://support.microsoft.com/topic/use-immersive-reader-in-microsoft-edge-78a7a17d-52e1-47ee-b0ac-eff8539015e1"&gt;Use Immersive Reader in Microsoft Edge&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy writing!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/proofread-read-aloud?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/proofread-read-aloud</guid>
      <pubDate>2021-09-22 18:00:00</pubDate>
      <category>#writing</category>
      <category>#tools</category>
    </item>
    <item>
      <title>Make predictions with ML.NET models without defining schema classes</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/mlnet-predictions-no-schema?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/mlnet-predictions-no-schema&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;To make predictions with ML.NET models you often have to define schema classes for your model inputs and outputs. In previous posts I wrote how you can &lt;a href="inspect-mlnet-models-netron.html"&gt;use Netron to inspect an ML.NET model&lt;/a&gt; to determine the name and types of your model inputs and outputs. If you have data samples of what your data input and output look like in JSON format, you can &lt;a href="vs-automate-mlnet-schema-generation.html"&gt;automate the generation of model input and output classes by using Visual Studio's &amp;quot;Paste JSON as Classes&amp;quot; feature&lt;/a&gt;. However, what if you want to make predictions without defining these classes? In this post I'll show how you can use the .NET DataFrame API to make predictions with ML.NET models without having to create model input and output classes. Code snippets are in F# but notebooks with complete &lt;a href="https://github.com/lqdev/mlnet-noschema-predictions/blob/main/CSharp-NB.ipynb"&gt;C#&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/lqdev/mlnet-noschema-predictions/blob/main/FSharp-NB.ipynb"&gt;F#&lt;/a&gt; code can be found in the &lt;a href="https://github.com/lqdev/mlnet-noschema-predictions"&gt;mlnet-noschema-predictions repo&lt;/a&gt; on GitHub.&lt;/p&gt;
&lt;h2&gt;Install and reference packages&lt;/h2&gt;
&lt;p&gt;In addition to the &lt;a href="https://www.nuget.org/packages/Microsoft.ML/"&gt;Microsoft.ML&lt;/a&gt; ML.NET NuGet package, you'll also need the &lt;a href="https://www.nuget.org/packages/Microsoft.Data.Analysis/"&gt;Microsoft.Data.Analysis&lt;/a&gt; NuGet package to use the .NET DataFrame API. For more information on the .NET DataFrame API, see &lt;a href="https://devblogs.microsoft.com/dotnet/an-introduction-to-dataframe/"&gt;an introduction to DataFrame&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once your packages are installed, reference them in your application.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;open Microsoft.ML
open Microsoft.Data.Analysis
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Initialize MLContext and load the model&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;MLContext&lt;/code&gt; is the entrypoint of ML.NET applications. Use it to load your model. The model used in this case categorizes sentiment as positive or negative. See the &lt;a href="inspect-mlnet-models-netron.html"&gt;use Netron to inspect an ML.NET model&lt;/a&gt; blog post to learn more about the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let ctx = MLContext()
let model,schema = ctx.Model.Load(&amp;quot;sentiment_model.zip&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both the model and the input schema are returned when you load the model. The input schema is a &lt;a href="https://docs.microsoft.com/dotnet/api/microsoft.ml.dataviewschema?view=ml-dotnet"&gt;DataViewSchema&lt;/a&gt; object containing a collection of &lt;a href="https://docs.microsoft.com/dotnet/api/microsoft.ml.dataviewschema.column?view=ml-dotnet"&gt;Columns&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Define input and output column names&lt;/h2&gt;
&lt;p&gt;The input and output column names are for the DataFrames containing your input data and predictions. They help the model map the input and output values.&lt;/p&gt;
&lt;p&gt;Use the &lt;code&gt;schema&lt;/code&gt; which was loaded with the model to get the name of your input columns.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let inputColumnNames = 
    schema 
    |&amp;gt; Seq.map(fun column -&amp;gt; column.Name) 
    |&amp;gt; Array.ofSeq
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this is a binary classification model by default only two columns are returned as part of the prediction:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Score&lt;/li&gt;
&lt;li&gt;PredictedLabel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can create an array containing the names of these columns. For more information on default output columns, see the &lt;a href="https://docs.microsoft.com/dotnet/machine-learning/resources/tasks#binary-classification-inputs-and-outputs"&gt;ML.NET Tasks documentation&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let outputColumnNames = [| &amp;quot;PredictedLabel&amp;quot; ; &amp;quot;Score&amp;quot; |]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Create input data for predictions&lt;/h2&gt;
&lt;p&gt;Use the &lt;a href="https://docs.microsoft.com/dotnet/api/microsoft.data.analysis.dataframe.loadcsvfromstring?view=ml-dotnet-preview#Microsoft_Data_Analysis_DataFrame_LoadCsvFromString_System_String_System_Char_System_Boolean_System_String___System_Type___System_Int64_System_Int32_System_Boolean_"&gt;&lt;code&gt;LoadCsvFromString&lt;/code&gt;&lt;/a&gt; method to load your input data into a DataFrame. In this case, there's only one column and data instance so I represent it as a string literal. Additionally, I provide the name of the input columns.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let sampleInput = &amp;quot;This was a very bad steak&amp;quot;

let inputDataFrame = 
    DataFrame.LoadCsvFromString(
        sampleInput, 
        header=false, 
        columnNames=inputColumnNames)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Make predictions&lt;/h2&gt;
&lt;p&gt;Now that you've loaded your input data, it's time to use the model to make predictions.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let predictionDV = 
    inputDataFrame 
    |&amp;gt; model.Transform 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Calling the &lt;a href="https://docs.microsoft.com/dotnet/api/microsoft.ml.itransformer.transform?view=ml-dotnet#Microsoft_ML_ITransformer_Transform_Microsoft_ML_IDataView_"&gt;&lt;code&gt;Transform&lt;/code&gt;&lt;/a&gt; method  returns an &lt;a href="https://docs.microsoft.com/dotnet/api/microsoft.ml.idataview?view=ml-dotnet"&gt;&lt;code&gt;IDataView&lt;/code&gt;&lt;/a&gt; with your predictions. You can then convert the &lt;code&gt;IDataView&lt;/code&gt; into a DataFrame for further processing with the &lt;a href="https://docs.microsoft.com/dotnet/api/microsoft.ml.idataviewextensions.todataframe?view=ml-dotnet-preview"&gt;&lt;code&gt;ToDataFrame&lt;/code&gt;&lt;/a&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let prediction = predictionDV.ToDataFrame(1L, outputColumnNames)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The resulting DataFrame should look something like the following:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;index&lt;/th&gt;
&lt;th&gt;PredictedLabel&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;False&lt;/td&gt;
&lt;td&gt;-2.1337974&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;If you want to load a model and make predictions without defining  classes for your input and output schema's you can load your data into a DataFrame using the .NET DataFrame API. While this solution works, because DataFrames and IDataViews process data differently, I haven't tested whether this solution would scale for larger data sets.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/mlnet-predictions-no-schema?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/mlnet-predictions-no-schema</guid>
      <pubDate>2021-09-16 18:00:00</pubDate>
      <category>#dotnet</category>
      <category>#machine-learning</category>
      <category>#mlnet</category>
      <category>#artificial-intelligence</category>
    </item>
    <item>
      <title>Automate ML.NET model schema generation from sample JSON data with Visual Studio</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/vs-automate-mlnet-schema-generation?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/vs-automate-mlnet-schema-generation&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;When using ML.NET models to make predictions, you often have to define classes for the model's input and output schema. In a previous post, I showed how you can &lt;a href="/posts/inspect-mlnet-models-netron.html"&gt;use Netron to inspect ML.NET models&lt;/a&gt; and manually define classes to represent the input and output schema for your model. That works for models that don't have a lot of features / columns. However, as the number of columns grows, it can become cumbersome to define these classes. Visual Studio has a feature that can help automate that process. Assuming you have a sample of your input and output data in JSON format, you can leverage a built-in feature &amp;quot;Paste JSON As Classes&amp;quot; to take the sample and convert it to a class. In this post, I'll show how to do that.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Visual Studio 2019. Though I haven't tested with VS2022, I assume &amp;quot;Paste JSON As Classes&amp;quot; is also available there.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Convert sample JSON data to C# classes&lt;/h2&gt;
&lt;p&gt;In this post, I'll work with the &lt;em&gt;sentiment_model.zip&lt;/em&gt; model to classify sentiment, which you can find in the &lt;a href="https://github.com/dotnet/samples/blob/main/machine-learning/models/sentimentanalysis/sentiment_model.zip"&gt;dotnet/samples&lt;/a&gt; repo.&lt;/p&gt;
&lt;p&gt;The model input has 3 columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SentimentText (string)&lt;/li&gt;
&lt;li&gt;Label (boolean)&lt;/li&gt;
&lt;li&gt;SamplingKeyColumn (float32)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model output has at least 2 columns since it uses a binary classification algorithm. For information on expected output columns based on the machine learning algorithm/task, see the &lt;a href="https://docs.microsoft.com/dotnet/machine-learning/resources/tasks#binary-classification-inputs-and-outputs"&gt;ML.NET tasks article&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Score (Single). Single is a single-precision floating-point number.&lt;/li&gt;
&lt;li&gt;PredictedLabel (boolean)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With that in mind, let's then assume that we have sample input data in JSON format that looks as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;SentimentText&amp;quot;: &amp;quot;This was a very bad steak&amp;quot;,
    &amp;quot;Label&amp;quot;: false,
    &amp;quot;SamplingKeyColumn&amp;quot;: 1.0
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside a C# project in Visual Studio:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a C# class&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Right-click your project.&lt;/li&gt;
&lt;li&gt;Select &lt;strong&gt;Add &amp;gt; Class&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Provide a name for your class in the New Item dialog.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Copy the input JSON data sample to your clipboard.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Place your cursor inside the namespace block of your newly created class.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the Visual Studio toolbar, select &lt;strong&gt;Edit &amp;gt; Paste Special &amp;gt; Paste JSON as Classes&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The result should look similar to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class Rootobject
{
    public string SentimentText { get; set; }
    public bool Label { get; set; }
    public float SamplingKeyColumn { get; set; }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Rename the class to something more descriptive like &lt;code&gt;ModelInput&lt;/code&gt;. The class should look similar to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class ModelInput
{
    public string SentimentText { get; set; }
    public bool Label { get; set; }
    public float SamplingKeyColumn { get; set; }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a JSON data sample for your output. In this case it'd look something like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;Score&amp;quot;: 1.0,
    &amp;quot;PredictedLabel&amp;quot;: false
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat steps 1-6 for your output JSON data sample. The resulting class should look similar to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class ModelOutput
{
    public float Score { get; set; }
    public bool PredictedLabel { get; set; }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It's important to note that the name of the class does not matter as long as the column names and types are the same the ones the model expects.&lt;/p&gt;
&lt;p&gt;At this point, you can go through the process of &lt;a href="https://docs.microsoft.com/dotnet/machine-learning/how-to-guides/machine-learning-model-predictions-ml-net"&gt;using the model to make predictions on new data&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Although the model used in this post does not have many columns, when you have many columns &amp;quot;Paste JSON As Classes&amp;quot; can significantly simplify the process of creating your input and output schema classes for ML.NET models. Happy coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/vs-automate-mlnet-schema-generation?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/vs-automate-mlnet-schema-generation</guid>
      <pubDate>2021-08-26 18:00:00</pubDate>
      <category>#dotnet</category>
      <category>#machine-learning</category>
      <category>#mlnet</category>
      <category>#artificial-intelligence</category>
      <category>#tooling</category>
      <category>#visual-studio</category>
    </item>
    <item>
      <title>Inspect ML.NET models with Netron</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/inspect-mlnet-models-netron?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/inspect-mlnet-models-netron&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Once you've trained a machine learning model, you typically serialize it and save it to a file. This serialized file contains information such as the model inputs and output schema (names, data types), the transformations and algorithms used by the model,  weights / coefficients, hyperparameters, and all other sorts of information about the model. The model file is then embedded in an end-user application such as a web API which deserializes the contents of the file and uses the model to make predictions on new data.&lt;/p&gt;
&lt;p&gt;How the model is serialized depends on the framework you use to train your model. To standardize model serialization and interoperability, you can use something like Open Neural Network Exchange (ONNX) to represent your models if supported by the framework you're using. That's beyond the scope of this post though.&lt;/p&gt;
&lt;p&gt;Regardless of which framework or serialization format is used, if you were to open up the model file, the contents of the model file are often not human readable or difficult to interpret. When the person who trains the model is also putting it into production, they know the format input data needs to be in to make predictions. That's not often the case though. As a result, the people putting models into production need the ability to inspect a model to get a sense of how they need to collect and preprocess input data before making predictions. That's where Netron comes in. &lt;a href="https://github.com/lutzroeder/Netron"&gt;Netron&lt;/a&gt; is a &amp;quot;visualizer for neural network, deep learning, and machine learning models&amp;quot;. In this post, I'll show how you can use Netron to inspect ML.NET models and use that information to define the model input and output schemas.&lt;/p&gt;
&lt;h2&gt;Inspecting an ML.NET model&lt;/h2&gt;
&lt;p&gt;ML.NET models are typically serialized and saved to files with the &lt;em&gt;.zip&lt;/em&gt; file extension. Using the &lt;em&gt;.zip&lt;/em&gt; file extension is standard convention. However, the extension can be whatever makes the most sense to you.&lt;/p&gt;
&lt;p&gt;A common question is, what's in the &lt;em&gt;zip&lt;/em&gt; file? The easy but vague answer to that question is, a serialized version of the model. Using Netron, you can go deeper and see exactly what is inside the &lt;em&gt;zip&lt;/em&gt; file.&lt;/p&gt;
&lt;p&gt;In this post, I'm using a pretrained ML.NET model that classifies sentiment. The model can be found in the &lt;a href="https://github.com/dotnet/samples/blob/main/machine-learning/models/sentimentanalysis/sentiment_model.zip"&gt;dotnet/samples&lt;/a&gt; repo. The same concept applies to any other ML.NET model.&lt;/p&gt;
&lt;p&gt;To inspect the ML.NET model using Netron:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href="https://github.com/dotnet/samples/raw/main/machine-learning/models/sentimentanalysis/sentiment_model.zip"&gt;Download the model&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Navigate to &lt;a href="https://netron.app/"&gt;https://netron.app&lt;/a&gt;. Alternatively, if you'd prefer to use Netron offline, you can also &lt;a href="https://github.com/lutzroeder/netron/releases"&gt;download the latest version of Netron&lt;/a&gt; for your operating system (Windows, Mac, or Linux). In this post, I use the web app.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select &lt;strong&gt;Open Model...&lt;/strong&gt; and use the file browser to select your ML.NET model. In this case, our model is &lt;em&gt;sentiment_model.zip&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After a few seconds, a graph describing you model appears. How long it takes for your model depends on its size. The larger your model, the longer it takes to load. The nodes in the graph represent the model inputs, transformations, algorithm, and outputs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Usually the top nodes represent the model inputs and the last node represents the algorithm or trainer. Click on any of the top nodes to display more information about the inputs.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/130704589-61ebb612-d65f-4364-b275-bd0d4991d3cf.png" class="img-fluid" alt="Sentiment Classification ML.NET model in netron" /&gt;&lt;/p&gt;
&lt;p&gt;For this model, we see that there are 3 input properties or columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SentimentText (string)&lt;/li&gt;
&lt;li&gt;Label (boolean)&lt;/li&gt;
&lt;li&gt;SamplingKeyColumn (float32)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using this information, we can represent the model inputs as a Plain-Old-CLR-Object (POCO) in our end-user application.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class ModelInput
{
    public string SentimentText {get;set;}
    public bool Label {get;set;}
    public float SamplingKeyColumn {get;set;} 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/130705880-0baea2f7-7b45-408a-b60c-16acceb54079.png" class="img-fluid" alt="ML.NET Netron Binary Predictor" /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the last node &lt;code&gt;BinaryPredXfer&lt;/code&gt;, we see that the algorithm used is for binary classification or predictions. Looking at the &lt;a href="https://docs.microsoft.com/dotnet/machine-learning/resources/tasks#binary-classification-inputs-and-outputs"&gt;ML.NET tasks documentation&lt;/a&gt;, we expect to get at least two columns in the prediction output:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Score (Single)&lt;/li&gt;
&lt;li&gt;PredictedLabel (boolean)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Like the input, we can also represent model outputs or predictions as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class ModelOutput
{
    public float Score {get;set;}
    public bool PredictedLabel {get;set;}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep in mind that the name of the class can be anything so long as the properties or column names and types match with those expected by the model.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once you have your model inputs and outputs defined in your end-user application, you can follow the standard process of &lt;a href="https://docs.microsoft.com/dotnet/machine-learning/how-to-guides/machine-learning-model-predictions-ml-net"&gt;loading your model and using it to make predictions&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Inspecting ML.NET models can be difficult since their serialized version is not human readable. When making predictions with ML.NET models but you're not familiar with what the input and output data should look like, use Netron to inspect the model. Then, use the information about the input data names and types, machine learning task, and algorithm to define model input and output schema classes in your end-user application. Once you've defined your model input and output, you can use the model to make predictions on new data.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/inspect-mlnet-models-netron?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/inspect-mlnet-models-netron</guid>
      <pubDate>2021-08-25 18:00:00</pubDate>
      <category>#dotnet</category>
      <category>#machine-learning</category>
      <category>#mlnet</category>
      <category>#netron</category>
      <category>#artificial-intelligence</category>
      <category>#tooling</category>
    </item>
    <item>
      <title>Self-hosting made easy with YunoHost</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/yunohost-self-hosting-simplified?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/yunohost-self-hosting-simplified&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Over the past few months I've started self-hosting a few of the services I use. Among those are a Matrix chat server and Mastodon instance. It may be of special interest to call out, many of these services are hosted on a Raspberry Pi 4, which not only speaks to the capabilities of the device but also shows opportunities for individuals to take control over the services they use and their data. Self-hosting however isn't the easiest thing to get started with. Mainly because each service has their own setup requirements. Technically you could use something like Docker to standardize the process, but that has its own complexities. In particular, there was a recent experience where I had trouble setting up a service which I got up and running in no time using YunoHost. YunoHost describes itself as &amp;quot;...a libre operating system aiming to simplify server administration and democratize self-hosting.&amp;quot;. In this post, I'll describe my experience with it and why you might consider it for your self-hosting project.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/128647305-c30ef44e-901d-4bf1-8e9d-af2449d5fd1d.png" class="img-fluid" alt="Image of server racks by Taylor Vick (Unsplash @tvick)" /&gt;&lt;/p&gt;
&lt;h2&gt;Why Self-Host?&lt;/h2&gt;
&lt;p&gt;First off, why even go through the trouble of self-hosting? Service providers nowadays have made it so easy to use their services, self-hosting seems like a lot of work. However, there are several reasons why you might consider self-hosting the applications and services you use. A few that I find interesting include:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Explore &amp;quot;new&amp;quot; technologies&lt;/li&gt;
&lt;li&gt;Own your data&lt;/li&gt;
&lt;li&gt;Learn something new&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Explore &amp;quot;new&amp;quot; technologies&lt;/h3&gt;
&lt;p&gt;The saying goes &amp;quot;there's nothing new under the sun&amp;quot;. However as with all things, people have found alternative uses and solutions to existing technologies. For example, in simple terms, the &lt;a href="https://en.wikipedia.org/wiki/Fediverse"&gt;Fediverse&lt;/a&gt; can be summarized as E-mail + RSS. It consists of a common decentralized protocol to handle communication between individuals who may not use the same server while providing &amp;quot;follow&amp;quot; capabilities for services like micro-blogging, photos, and video sharing. Though the Fediverse has been around for some time, it's still relatively new. Whether it's a new technology or new to you, self-hosting can help you learn more about technologies like the Fediverse.&lt;/p&gt;
&lt;h3&gt;Own your data&lt;/h3&gt;
&lt;p&gt;Part of accepting terms of service agreements is sometimes giving up some ownership of your personal data. Although you hope the company and service provider is responsible with your data, you can't always be 100% sure. With self-hosting, you're fully in control of your data and in most cases don't rely on third parties. As a result, your data is as private and secure as you want it to. From a security standpoint though, I'll note that in many cases, service providers have dedicated security teams whose main job is to keep your data secure. Therefore chances are that your data is usually secure when hosted by service providers. In any case, if you want to have complete control and ownership of your data, self-hosting might be for you.&lt;/p&gt;
&lt;h3&gt;Learn something new&lt;/h3&gt;
&lt;p&gt;Over the past few months, I've learned a few things that I don't usually deal with on my day-to-day like DNS records, Nginx, Systemd, certificates, and a few others. Self-hosting these services usually involves setting up several components some of which you may already be familiar with while others may be new to you. Self-hosting gives you the opportunity to learn new skills and as mentioned previously work with new technologies.&lt;/p&gt;
&lt;h2&gt;Self-hosting with YunoHost&lt;/h2&gt;
&lt;p&gt;Recently I was trying to set up an instance of &lt;a href="https://pixelfed.org/"&gt;Pixelfed&lt;/a&gt;, which describes itself as &amp;quot;A free and ethical photo sharing platform&amp;quot;. You can think of it as a self-hosted Instagram. The application is built with PHP and a few other components I'm not very familiar with. I spent about a week trying to configure my Raspberry Pi to successfully run these services. Unfortunately, I had no luck. I had heard about YunoHost previously but had never tried it. Essentially they offer a way to simplify self-hosting. I noticed YunoHost provided a way to self-host Pixelfed. All I had to do was:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://yunohost.org/en/install"&gt;Install YunoHost&lt;/a&gt;. In my case, I installed it on a Linode VM (the smallest one) running Debian 10, but you have options including a Raspberry Pi.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/YunoHost-Apps/pixelfed_ynh"&gt;Install Pixelfed&lt;/a&gt;. YunoHost has a variety of &lt;a href="https://yunohost.org/en/apps"&gt;applications&lt;/a&gt; with pre-configured scripts that make it easy to install applications like Pixelfed.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The entire process took about one hour. Some parts I skipped mentioning were getting a domain name and configuring DNS records, but aside from that, getting started with YunoHost was incredibly simple. If you're looking to self-host but don't want to spend too much time configuring your server and each of the services you host on it, give YunoHost a try.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/yunohost-self-hosting-simplified?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/yunohost-self-hosting-simplified</guid>
      <pubDate>2021-08-10 18:00:00</pubDate>
      <category>#fediverse</category>
      <category>#open-source</category>
      <category>#self-hosting</category>
      <category>#raspberry-pi</category>
      <category>#matrix</category>
      <category>#pixelfed</category>
      <category>#mastodon</category>
    </item>
    <item>
      <title>Surface Duo offline navigation hiking setup</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/surface-duo-offline-navigation-hiking-setup?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/surface-duo-offline-navigation-hiking-setup&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Over the past few months, I've been going on a hike almost every weekend. Not only does hiking provide an opportunity to discover the natural wonders all around you, it's also good exercise and free (if you don't count entry fees for some state and national parks). Throughout these trips, the &lt;a href="https://www.microsoft.com/surface/devices/surface-duo"&gt;Surface Duo&lt;/a&gt;, Microsoft's dual-screen Android phone has been a reliable companion. Although it's not the smallest or most rugged device, its large screen(s) paired with OSMAnd~ and AllTrails make it easy to navigate while on the trail. In this post, I'll detail my navigation setup.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/127246913-58472929-f067-41f0-9011-17193f863f71.JPG" class="img-fluid" alt="Picture of half-dome taken from Four Mile Trail" /&gt;&lt;/p&gt;
&lt;h2&gt;Getting trail routes from AllTrails&lt;/h2&gt;
&lt;p&gt;Although trails are often well marked, it's always a good idea to have a map of the trail and surrounding area. This is especially true if it's your first time there. I almost exclusively get my trail maps from &lt;a href="https://www.alltrails.com/"&gt;AllTrails&lt;/a&gt;. If you're not familiar with AllTrails, it's a platform and community where members share maps, reviews, and information about trails for all kinds of outdoor activities. The site is completely free and while you can pay for premium features, the free offering provides you with all the essentials you'd need to plan your trip.&lt;/p&gt;
&lt;p&gt;Trail information, maps, and reviews are entirely user supported. One of the (free) features is the ability to download trails to a variety of formats.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/127243740-5ef2e2e3-4c9c-4dbc-a8c5-be608fafeea5.png" class="img-fluid" alt="AllTrails Download Route" /&gt;&lt;/p&gt;
&lt;p&gt;Typically, the format I'll download routes to is GPX Route and GPX Track. &lt;a href="https://en.wikipedia.org/wiki/GPS_Exchange_Format"&gt;GPX&lt;/a&gt; is an XML schema for representing GPS data in software applications. Because GPX is a standard format, you can open and view these files in any app that supports them.&lt;/p&gt;
&lt;h2&gt;Offline navigation with OSMAnd~&lt;/h2&gt;
&lt;p&gt;AllTrails has a mobile app. However, for offline navigation, I use the &lt;a href="https://f-droid.org/en/packages/net.osmand.plus/"&gt;OSMAnd~&lt;/a&gt; Android app which I downloaded from &lt;a href="https://f-droid.org/"&gt;F-Droid&lt;/a&gt;. It's also available in the Google Play Store and Apple App Store.&lt;/p&gt;
&lt;p&gt;OSMAnd~ is a free map and navigation app supported by &lt;a href="https://www.openstreetmap.org/"&gt;OpenStreetMaps&lt;/a&gt;. OpenStreetMaps, like AllTrails is free and entirely community driven. OSMAnd~ lets you download &lt;strong&gt;detailed&lt;/strong&gt; maps for offline use. Detailed is the key here. Although you can save maps for offline use with applications like Here Maps and Google Maps, they often limit the size of the area you can save for offline use and don't provide much detail. With OSMAnd~ you can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download entire state and country maps for offline use.&lt;/li&gt;
&lt;li&gt;View details like contour-lines, hill shading, and other trail details.&lt;/li&gt;
&lt;li&gt;Turn-by-turn navigation.&lt;/li&gt;
&lt;li&gt;View and edit GPX files&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the GPX files I get from AllTrails, I load them onto my Surface Duo and view them using OSMAnd~. Using offline maps and the Duo's GPS I'm able to see my position on the trail and quickly find my way when I've gone off it even if I have no reception. This combination makes for a navigation device and experience like no other because I have the full screen real estate that the Duo provides to view areas beyond the trail.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/127246186-6b5f02ec-7fef-4f2e-909b-8a54615dc790.png" class="img-fluid" alt="OSMAnd Android App Displaying Track" /&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;By leveraging the power of crowd-sourcing and open-source, access to quality information has never been easier. When this information is displayed on a device like the Surface Duo it makes it easy to navigate when you're out on the trail especially in situations when you have no reception and you're not familiar with your surroundings.&lt;/p&gt;
&lt;p&gt;If you've been looking for an offline navigation solutions, hopefully this post helps provide some guidance. If you do any outdoor activities especially in remote areas, even if you don't have a Duo, I'd suggest giving AllTrails and OSMAnd~ a try. See you on the trail!&lt;/p&gt;
&lt;p&gt;P.S. Over the past month, I've transitioned to the Garmin Fenix 6X Pro for navigation. It's a smaller device that supports loading GPX files and provides offline maps with turn-by-turn navigation. I'm also not worried about dropping or scratching it because it's designed for the outdoors. This in no way takes away with how amazing the OSMAnd~ and Surface Duo pairing are for offline navigation. In fact, I still use the Duo for backup navigation when it's difficult to see the map on the Fenix.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/surface-duo-offline-navigation-hiking-setup?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/surface-duo-offline-navigation-hiking-setup</guid>
      <pubDate>2021-07-27 19:00:00</pubDate>
      <category>#hiking</category>
      <category>#openstreetmaps</category>
      <category>#open-source</category>
      <category>#surface</category>
      <category>#lifestyle</category>
      <category>#apps</category>
      <category>#fdroid</category>
    </item>
    <item>
      <title>Never change targeted ads</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/fenix-social-media-targeted-ads?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/fenix-social-media-targeted-ads&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;The origin of this post comes from a random observation I made earlier today on how fascinating and disturbing targeted ads can be. But first, some context.&lt;/p&gt;
&lt;p&gt;Over the past year, I've been doing a lot more hiking. Although most trails are well marked, it's always good to have some way to navigate in case you miss a turn. For remote locations with limited connectivity, one of the best ways to navigate, other than paper maps, is to download the trail GPX files and view them using a mapping application offline. This is just what I've been doing on my Surface Duo with the help of &lt;a href="https://www.alltrails.com/"&gt;AllTrails&lt;/a&gt; and &lt;a href="http://osmand.net/"&gt;OSMAnd+&lt;/a&gt;. That setup and workflow are a post on its own, but I'll leave that for another time. While that's worked extremely well, there are some downsides to it. For starters, the Surface Duo is not exactly the smallest or most rugged device so it can be cumbersome to handle while on the trail. That means I have to be extra careful to keep it from falling on rocks or off a cliff. Having navigation turned on also drains the phone battery, which is less than ideal in the event I were to need the phone for an emergency. Therefore, I started looking for an alternative navigation solution.&lt;/p&gt;
&lt;p&gt;After significant research, I landed on a wearable device with offline mapping capabilities, specifically the &lt;a href="https://discover.garmin.com/en-US/fenix-6/"&gt;Garmin Fenix 6&lt;/a&gt;. Although I've had my eye on this device for the past few months, I couldn't justify paying full price for it. Fast-forward to this past week. For work, I recently published some articles on Kubernetes. Around the same time, as part of Father's Day &amp;amp; Amazon Prime Day sales, the price of the device dropped significantly enough for me to buy it. This leads us to the main point of this post.&lt;/p&gt;
&lt;p&gt;While looking at my Twitter feed, I ran into this gem.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/123006831-47e1f080-d386-11eb-98ac-af80f6f5f6f0.png" class="img-fluid" alt="Garmin Kubernetes Twitter targeted ad" /&gt;&lt;/p&gt;
&lt;p&gt;Coincidence? I think not! It's fascinating to see how hyper-specific ads are based on things I'm interested in. In this case it's harmless. I'm always up for learning a thing or two about Kubernetes and if I get the chance to win this watch, count me in! The disturbing part is, what else does it know about me and how is that information being used? Depending on where on the spectrum of fascinated / disturbed you land on, one way to control the amount of information shared is through the privacy settings. Unfortunately privacy settings only go so far and visibility into who has what data is limited. Maybe it's wishful thinking but it'd be great to have a tool that provides more insights and control over what data is shared and with whom. Personally, I think whether targeted ads / recommendations are good or bad is not black or white. If it's something I'm already doing or looking into, it'd be nice to be presented with options to include as part of my research. On the other hand, if those recommendations / targeted ads are exploiting information from my interests to manipulate my behavior, that's not right. Where that line is drawn though is not always clear and often personal.&lt;/p&gt;
&lt;p&gt;In the meantime though, all I can say is, stay classy targeted ads!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/fenix-social-media-targeted-ads?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/fenix-social-media-targeted-ads</guid>
      <pubDate>2021-06-22 17:40:00</pubDate>
      <category>#random</category>
      <category>#privacy</category>
      <category>#social-media</category>
      <category>#ads</category>
    </item>
    <item>
      <title>Rediscovering the RSS protocol</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/rediscovering-rss-user-freedom?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/rediscovering-rss-user-freedom&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;RSS provides a standardized way of aggregating content giving content creators the freedom to choose how they publish content while giving consumers the freedom to choose how they consume that content. I've used RSS for a long time. One of my favorite RSS readers was Google Reader while it was still around and I believe &lt;a href="https://www.ripgooglereader.com/"&gt;I'm not the only fan&lt;/a&gt;. Since then, I've bounced around between different RSS feed readers not settling one until recently when I started using &lt;a href="https://newsblur.com"&gt;NewsBlur&lt;/a&gt;. As a result of finding an RSS feed reader I liked, I've also started to expand the content I follow via RSS feeds to social media, videos, podcasts, and software projects. In part, this has gotten me thinking about some of the benefits of protocols. The essay &lt;a href="https://knightcolumbia.org/content/protocols-not-platforms-a-technological-approach-to-free-speech"&gt;Protocols, Not Platforms: A Technological Approach To Free Speech&lt;/a&gt; makes good points on some of these benefits. Though the focus of that article is on free speech, I prefer to think about the benefits in terms of user freedom.&lt;/p&gt;
&lt;h2&gt;Creator freedom using RSS&lt;/h2&gt;
&lt;p&gt;RSS provides content creators with the freedom to choose how and where they publish content. As a content creator, depending on the type of content you produce you use different platforms to publish. For example, when I blog, I publish on this site, when I create videos, I publish to YouTube or livestream on Twitch and to further expand the reach of my content, I publish links on social media like Reddit or Twitter. In each case, I use different platforms to publish content. This might be okay because the audiences interested in the video content may not be interested in blog posts and vice versa. However, now the content is split across all these various platforms with their own accounts, interfaces, policies, and audiences. It also means, when I tell people where they can follow me, it usually involves an entire speech like: &amp;quot;Follow me on GitHub at &lt;a href="https://github.com/lqdev/"&gt;lqdev&lt;/a&gt;, Twitter at &lt;a href="https://twitter.com/ljquintanilla"&gt;ljquintanilla&lt;/a&gt;, my blog at &lt;a href="https://www.luisquintanilla.me/posts/1"&gt;luisquintanilla.me&lt;/a&gt;...&amp;quot;. Of course, I don't always list out all the platforms I'm active on and it's highly dependent on the setting, but the more platforms I'm active on the longer that speech gets.&lt;/p&gt;
&lt;p&gt;Fortunately, some of these platforms generate RSS feeds. On GitHub, RSS feeds are generated for releases, issues, and pull requests. For example, the ML.NET releases RSS feed can be accessed by appending &lt;em&gt;.atom&lt;/em&gt; to the releases URL &lt;a href="https://github.com/dotnet/machinelearning/releases.atom"&gt;https://github.com/dotnet/machinelearning/releases.atom&lt;/a&gt;. Note that while Atom and RSS are not the same, they both offer a way to aggregate content. Similarly, YouTube provides the option to generate feeds for channels. For example, you can find the RSS feed to my YouTube channel at the following URL &lt;a href="https://www.youtube.com/feeds/videos.xml?channel_id=UCkA5fHdQ4cf3D1J19UNgV7A"&gt;https://www.youtube.com/feeds/videos.xml?channel_id=UCkA5fHdQ4cf3D1J19UNgV7A&lt;/a&gt;. On this blog, the RSS feed can be found at &lt;a href="https://www.luisquintanilla.me/posts/index.xml"&gt;https://www.luisquintanilla.me/posts/index.xml&lt;/a&gt;. For social media, the experience is a bit more limited since platforms like Twitter, Instagram, or Facebook don't generate RSS feeds which is unfortunate since they all have feeds of their own making them the perfect use case for RSS feeds. Platforms like Reddit provide RSS feeds to public subreddits and users by appending &lt;em&gt;.rss&lt;/em&gt; at the end of the URL. For example, the RSS feed for the Zune subreddit can be found at &lt;a href="https://www.reddit.com/r/Zune.rss"&gt;https://www.reddit.com/r/Zune.rss&lt;/a&gt;. In each of these cases, though the content and platforms are different, the underlying protocol and format used is the same, RSS.&lt;/p&gt;
&lt;p&gt;Therefore, a strategy to improve how I publish my content using RSS is, instead of sending users to the various platforms, I can provide links to the different RSS feeds for each of the platforms I'm active on in my personal site. Instead of having a long speech on the different platforms I'm active on, I can just say, &amp;quot;Go to my personal website at ...&amp;quot;, and that's it! That becomes the gateway to the rest of my content. I can continue publishing on the different platforms because of the features they may provide me with for content creation and discovery, but only send users to a single place. For even more control over publishing, I can self-host videos or other content and create RSS feeds for it just like I do for my blog. In either case, users don't need to have an account in order to access and follow my content lowering the barrier to access content. This brings me to my next point of user freedom.&lt;/p&gt;
&lt;h2&gt;User freedom using RSS&lt;/h2&gt;
&lt;p&gt;RSS provides users with the freedom to choose how they consume content. One way it does so is by being a standard protocol for aggregating content. Because there's an agreed upon set of standards, clients or RSS feed readers that consume this protocol at minimum only need to be able to read an RSS feed. Once a client can read the RSS feed, it's free to build on top of that leading to diversity and competition among the various clients. Diversity and competition is good for users because it provides choice. If I want a minimal feed reader, I can subscribe to RSS feeds using an e-mail client like &lt;a href="https://support.microsoft.com/en-us/office/subscribe-to-an-rss-feed-73c6e717-7815-4594-98e5-81fa369e951c"&gt;Outlook&lt;/a&gt; or &lt;a href="https://support.mozilla.org/en-US/kb/how-subscribe-news-feeds-and-blogs"&gt;Thunderbird&lt;/a&gt;. If I want more features, I can use something like NewsBlur. As an aside, one of the other things I like about NewsBlur is being provided with an e-mail address that I can use to subscribe to newsletters helping me both clear up my inbox and letting me access information in a single place. This shows how RSS readers don't have to stop at only reading RSS feeds and can build on top of the protocol while still performing their core functionality.&lt;/p&gt;
&lt;p&gt;Another reason to use RSS is having a single place to consume almost all your content. As mentioned before, an RSS feed can aggregate content from a YouTube channel, subreddit, blog, newspaper, podcast, etc. RSS is agnostic in the sense it doesn't care what's in the feed so long as you're able to link to the content. Now, because RSS readers can read any RSS feed, it means you can follow almost all of your content from a single place so long as you're able to subscribe to the RSS feed. In fact, aside from some social media, the only other content I don't consume via RSS is podcasts. Not because I'm not able to since podcasts are usually aggregated using RSS feeds, but because I'm not able to play and sync them with other devices. Being able to consume content in a single place provides users with an incredible amount of freedom by not requiring the use of multiple apps, websites, or accounts.&lt;/p&gt;
&lt;p&gt;One last way in which RSS gives user freedom to choose how they consume content is by giving them control of their feed. Various platforms like social media sites and YouTube generate feeds and recommend content using algorithms. These algorithms are sometimes useful by exposing you to new and diverse content. Other times, algorithms can create a bubble by reinforcing preferences and interests. In any case, there's little visibility or control into how the algorithms make choices and recommendations. RSS by default is ordered in chronological order. Therefore you know you're getting the newest content. Depending on the client, you might be able to sort content into folders in whichever way makes the most sense to you. You might also be able to choose how often the feeds you're subscribed to are checked for new content. If you don't want to get the latest updates every second, set it to update daily or weekly. If you want to get rid of the clutter and only see content from the previous day, you can automatically delete or mark older content as read. Note that all these capabilities are dependent on the client, but if this is important to you, you can choose a client that gives you these features making the possibilities endless. Finally, because you are actively choosing which feeds you subscribe to, you're not presented with content other people liked, retweeted, shared, or recommended. You only have access to the content you choose to subscribe to. If at any point you're no longer interested in the content, unsubscribe and you no longer see it on your feed.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As I continue my rediscovery of RSS, I want to find ways to subscribe to content on platforms I'm already using via RSS. It's important to note that this is not an either or approach. You can subscribe to content from the various platforms using RSS to get updates in a single place and since RSS links to the original content, you can consume content on the platform itself helping content creators with more page views, likes, subscriptions, and other forms of engagement. Ultimately though, the choice is yours. A choice of how you publish and consume content.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/rediscovering-rss-user-freedom?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/rediscovering-rss-user-freedom</guid>
      <pubDate>2021-04-05 19:30:00</pubDate>
      <category>#protocols</category>
      <category>#web</category>
      <category>#rss</category>
    </item>
    <item>
      <title>Alternatives to WhatsApp</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/alternatives-to-whatsapp?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/alternatives-to-whatsapp&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently I've heard a lot of buzz around people moving away from the WhatsApp messaging app. I'm not tuned into all the details as to why there's been a mass exodus, but from what I understand it has to do with its privacy policies. I've seen many people including Elon Musk recommend Signal. Signal is a great alternative but there are a few others that are worth considering. This recommendation was originally planned as a tweet, but there's so many details that just can't be captured in a tweet or thread so instead I turned it into a blog post. If you haven't made the switch or if you're still not completely committed to the platform you've switched to, this post is for you.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/104107289-08689180-5289-11eb-890d-54527902df97.png" class="img-fluid" alt="Phone and notebook - Oscar Mucyo Unsplash" /&gt;&lt;/p&gt;
&lt;h2&gt;Matrix&lt;/h2&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/104107320-3e0d7a80-5289-11eb-9449-71e34315b96e.png" class="img-fluid" alt="Element web client screenshot" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://matrix.org/"&gt;Matrix&lt;/a&gt; is my go-to messaging platform. Matrix is an open-source project that provides an open standard for real-time communication. Built on top of this standard are several implementations with &lt;a href="https://github.com/matrix-org/synapse"&gt;Synapse&lt;/a&gt; being the most popular server and &lt;a href="https://element.io/"&gt;Element&lt;/a&gt; being the most popular client.&lt;/p&gt;
&lt;p&gt;Matrix is relatively new. However, it has a lot of potential and there are a lot of exciting things happening around it. Some of the most notable include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://matrix.org/blog/2019/12/19/welcoming-mozilla-to-matrix"&gt;Mozilla selected Matrix&lt;/a&gt; to be the successor to its IRC communications.&lt;/li&gt;
&lt;li&gt;Matrix recently &lt;a href="https://matrix.org/blog/2020/09/30/welcoming-gitter-to-matrix"&gt;acquired Gitter&lt;/a&gt;, a popular chat platform for software projects, and has quickly worked to &lt;a href="https://matrix.org/blog/2020/12/07/gitter-now-speaks-matrix"&gt;integrate Gitter with Matrix&lt;/a&gt; allowing for it to take advantage of many of the features provided by the protocol.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://matrix.org/blog/2021/01/04/taking-fosdem-online-via-matrix"&gt;Matrix will power FOSDEM 2021&lt;/a&gt; communications.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://matrix.org/blog/2020/06/02/introducing-p-2-p-matrix"&gt;Matrix is experimenting with P2P architectures&lt;/a&gt; to help users have full control over their communications.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Although Matrix provides several features commonly associated with messaging platforms such as 1:1 messaging, video/audio calls, and group chat, there are a few features worth highlighting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;End-to-end encryption&lt;/li&gt;
&lt;li&gt;Interoperability with other communication platforms&lt;/li&gt;
&lt;li&gt;Decentralization&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;End-to-end encryption&lt;/h3&gt;
&lt;p&gt;End-to-end encryption makes sure your communications stay private. Only you and the intended recipients of the message are able to see the messages.&lt;/p&gt;
&lt;h3&gt;Interoperability&lt;/h3&gt;
&lt;p&gt;One of the biggest challenges of moving over to a new platform is that your friends and family don't or won't join you, making it a lonely place. Matrix provides interoperability with other communication platforms and protocols through &lt;a href="https://matrix.org/bridges/"&gt;bridges&lt;/a&gt;. Bridges facilitate the exchange of messages with other platforms without requiring all participants to be on Matrix. The core team maintains bridges for Slack, IRC, XMPP, and Gitter. There are also community maintained bridges for WhatsApp, Discord, Facebook Messenger, Signal, and many others.&lt;/p&gt;
&lt;h3&gt;Decentralization&lt;/h3&gt;
&lt;p&gt;Decentralization means that there's no central authority that holds control over the platform or data. Matrix is decentralized through federation. One of the most common examples of federation is e-mail. I may have an AOL e-mail address and you may have a Hotmail e-mail address, yet we're still able to exchange messages. With Matrix, in a similar way, users and groups don't have to have their accounts on the same server in order to communicate. It also means that users have a choice over &lt;a href="https://www.hello-matrix.net/public_servers.php"&gt;which server they join&lt;/a&gt;. If I don't agree with the conversations or moderation of a particular server, I have the choice to open an account on another server without leaving Matrix altogether. Even better, because Matrix is built using open technologies, I have the option of hosting my own server.&lt;/p&gt;
&lt;p&gt;To get started with Matrix, visit the &lt;a href="https://element.io/get-started"&gt;Element&lt;/a&gt; website and create an account.&lt;/p&gt;
&lt;h2&gt;Jami&lt;/h2&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/104107348-64331a80-5289-11eb-8606-944269e17d10.png" class="img-fluid" alt="Jami Screenshot" /&gt;&lt;/p&gt;
&lt;p&gt;Another project that I like but haven't used it to the same extent as Matrix is &lt;a href="https://jami.net/"&gt;Jami&lt;/a&gt;. Jami is an open-source project for real-time communication. Similar to Matrix, Jami is cross-platform, free (as in freedom and beer), end-to-end encrypted, and decentralized. Jami achieves decentralization by using P2P protocols. This means that the data is stored on your device so you're in full control of your data. Jami also doesn't require the internet to work. As long as you're in the same local network as your peers, you're able to communicate.&lt;/p&gt;
&lt;p&gt;To get started with Jami, &lt;a href="https://jami.net/download/"&gt;download the app&lt;/a&gt; and create an account.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I've mentioned some communication platforms that I consider great alternatives to WhatsApp. Regardless of the platform you choose, some things that I believe should be top of mind are security and privacy. It's important that you own your data and you have control over who has access to it and what they do with it. Feel free say hi on Matrix &lt;a href="https://matrix.to/#/@lqdev:lqdev.tech"&gt;@lqdev:lqdev.tech&lt;/a&gt; or Jami where you can find me as lqdev1.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/alternatives-to-whatsapp?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/alternatives-to-whatsapp</guid>
      <pubDate>2021-01-09 13:44:27</pubDate>
      <category>#privacy</category>
      <category>#self-hosted</category>
      <category>#security</category>
      <category>#open-source</category>
      <category>#apps</category>
    </item>
    <item>
      <title>How to watch Twitch streams using VLC</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/how-to-watch-twitch-using-vlc?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/how-to-watch-twitch-using-vlc&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Twitch has become a tool for various creators to connect with audiences. Originally, streams were centered around gaming but now there is a wide variety of creators that include musicians, software developers, and craftspeople. One of my favorites is &lt;a href="https://www.twitch.tv/brainfeeder"&gt;Brainfeeder&lt;/a&gt;. I myself try (and often fail) to build software projects on stream. If you're interested in hanging out and talking machine learning, .NET and all things tech, feel free to join me at &lt;a href="https://twitch.tv/lqdev1"&gt;https://twitch.tv/lqdev1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Watching Twitch streams on the desktop via the browser can take a toll on compute resources. A more efficient solution to this problem is to use VLC. VLC is a free, open-source, cross-platform media player that supports various protocols. While it was possible to watch Twitch streams using VLC before, it often required additional software such as &lt;a href="https://github.com/streamlink/streamlink"&gt;Streamlink&lt;/a&gt;. More recent versions make this requirement redundant. In this post, I'll show how to use VLC to watch Twitch streams.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/103722126-4883f780-4f9d-11eb-8cc8-304d1a249ef9.png" class="img-fluid" alt="Twitch iOS App on iPhoneX - Caspar Camille Rubin" /&gt;&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;VLC (version &lt;strong&gt;3.11.0 or higher&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Watch Twitch Stream&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Update 11/12/2021&lt;/strong&gt;&lt;br /&gt;
It's been a while since I've gotten the original steps to work, so feel free to skip them. You can still watch Twitch streams on VLC using Streamlink. To use Streamlink:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Download and install &lt;a href="https://streamlink.github.io/install.html"&gt;Streamlink&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open the terminal and run:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;streamlink https://twitch.tv/&amp;lt;username&amp;gt; best
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For more information, see the &lt;a href="https://streamlink.github.io/cli.html"&gt;Streamlink Command-Line Interface documentation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;Open VLC&lt;/li&gt;
&lt;li&gt;From the menu, Select &lt;strong&gt;Media &amp;gt; Open Network Stream&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Paste the URL to the stream you want to watch in the text box (i.e. https://twitch.tv/lqdev1)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point, the stream should start playing.&lt;/p&gt;
&lt;h3&gt;CLI&lt;/h3&gt;
&lt;p&gt;Alternatively on Unix (Mac / Linux) systems, you can use the CLI. To watch a stream, open a terminal and enter the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;vlc https://twitch.tv/&amp;lt;username&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make sure to replace the username with the streamer you want to watch.&lt;/p&gt;
&lt;p&gt;After a few seconds, VLC loads and starts playing the stream.&lt;/p&gt;
&lt;h2&gt;Performance comparison&lt;/h2&gt;
&lt;p&gt;Below is a comparison of the same stream playing on the browser vs VLC. These test were performed on a Surface Laptop 1 running Windows 10. By no means should you take this as a definitive performance comparison as there are many variables that may affect performance. However, it's a nice way to visualize each of the methods.&lt;/p&gt;
&lt;h3&gt;Browser&lt;/h3&gt;
&lt;p&gt;In the browser scenario, I only had a single tab playing the Twitch stream on Microsoft Edge (chromium).&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/103723087-43c04300-4f9f-11eb-9969-df40541693bd.png" class="img-fluid" alt="Browser task manager" /&gt;&lt;/p&gt;
&lt;h3&gt;VLC&lt;/h3&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/103721933-e1fed980-4f9c-11eb-92f3-0147f2c09134.png" class="img-fluid" alt="VLC task manager" /&gt;&lt;/p&gt;
&lt;p&gt;As it can be seen from this image, there's about a 4x decrease in CPU resource consumption when using VLC.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post I showed how you can use VLC to watch Twitch streams. One of the drawbacks is, you can't see the channels you follow and who's online. You also can't browse streams like you would on the Twitch web client. You also don't have access to chat, which is half the fun on many streams. However, if you're only interested in the content, VLC is an excellent and less resource-hungry way of watching Twitch streams. See you on Twitch!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/how-to-watch-twitch-using-vlc?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/how-to-watch-twitch-using-vlc</guid>
      <pubDate>2021-01-05 21:14:39</pubDate>
      <category>#streaming</category>
      <category>#hacks</category>
      <category>#video</category>
      <category>#it</category>
    </item>
    <item>
      <title>Train an image classifier using F# and ML .NET</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/image-classification-mlnet-fsadvent2020?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/image-classification-mlnet-fsadvent2020&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This post is part of F# Advent 2020. Thank you to &lt;a href="https://twitter.com/sergey_tihon"&gt;Sergey Tihon&lt;/a&gt; for organizing this and the rest of the contributors for producing high-quality content. Make sure to check out the rest of the &lt;a href="https://sergeytihon.com/2020/10/22/f-advent-calendar-in-english-2020/"&gt;F# Advent 2020 content&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When picking who's on the naughty or nice list, I often wonder how Santa decides. I took a shot at answering this question by training an image classifier using the ML.NET image classification API and images of Disney heroes and villains to tell whether they're naughty or nice. You shouldn't judge someone by the way they look (even if they are the Evil Queen), so it's safe to say, don't try this at home or with your neighbors 😉. This sample is just for demo purposes. You can find the full code on &lt;a href="https://gist.github.com/lqdev/0c4dc9eea93b7b8541f31ddd429afb53"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;This sample was built on a Windows 10 PC, but should work on Mac / Linux PCs&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://dotnet.microsoft.com/download"&gt;.NET 5 SDK&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The data&lt;/h2&gt;
&lt;p&gt;The dataset contains images of Disney characters, both real and animated. They were obtained from the &lt;a href="https://disney.fandom.com/wiki/The_Disney_Wiki"&gt;Disney Fandom Wiki&lt;/a&gt;. The characters are split into two categories, &lt;a href="https://disney.fandom.com/wiki/Category:Villains"&gt;villains&lt;/a&gt; and &lt;a href="https://disney.fandom.com/wiki/Category:Protagonists"&gt;heroes&lt;/a&gt;. For the purpose of this sample, we'll label heroes as nice and villains as naughty. The dataset used to train this model contains 2400 villain (naughty) and 675 hero (nice) images stored in top-level directories with the naughty/nice names. This means that the dataset is unbalanced and may skew predictions as it can be seen when making predictions.&lt;/p&gt;
&lt;h2&gt;Install  NuGet packages&lt;/h2&gt;
&lt;p&gt;Use the &lt;code&gt;#r&lt;/code&gt; convention to install the necessary NuGet packages used in this sample.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;#r &amp;quot;nuget:Microsoft.ML&amp;quot;
#r &amp;quot;nuget:Microsoft.ML.Vision&amp;quot;
#r &amp;quot;nuget:Microsoft.ML.ImageAnalytics&amp;quot;
#r &amp;quot;nuget:SciSharp.TensorFlow.Redist&amp;quot; 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, import the packages.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;open System
open System.IO
open Microsoft.ML
open Microsoft.ML.Data
open Microsoft.ML.Vision
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Define data types&lt;/h2&gt;
&lt;p&gt;Start off by defining the data types containing your input and output schema. You can do this by creating two records, &lt;code&gt;ImageData&lt;/code&gt; and &lt;code&gt;ImagePrediction&lt;/code&gt;. &lt;code&gt;ImageData&lt;/code&gt; is the input which contains the path to image file and the category it belongs to and the &lt;code&gt;ImagePrediction&lt;/code&gt; contains the prediction generated by the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;[&amp;lt;CLIMutable&amp;gt;]
type ImageData = {
    ImagePath: string
    Label: string
}

[&amp;lt;CLIMutable&amp;gt;]
type ImagePrediction = {
    PredictedLabel: string
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Training&lt;/h2&gt;
&lt;p&gt;The training process loads a set of training images, preprocesses them, and uses the ML.NET image classification API to train an image classification model.&lt;/p&gt;
&lt;h3&gt;Initialize MLContext&lt;/h3&gt;
&lt;p&gt;Once you've defined the data type, initialize the &lt;code&gt;MLContext&lt;/code&gt;. &lt;code&gt;MLContext&lt;/code&gt; is the entrypoint for ML.NET applications.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let ctx = new MLContext()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Load training data&lt;/h3&gt;
&lt;p&gt;Then, load the data using the helper function &lt;code&gt;loadImagesFromDirectory&lt;/code&gt; and point it to the top-level directory containing the subdirectories of images in the nice and naughty categories.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let imageData = loadImagesFromDirectory &amp;quot;C:/Datasets/fsadvent2020/Train&amp;quot; true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;loadImagesFromDirectory&lt;/code&gt; function looks like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let loadImagesFromDirectory (path:string) (useDirectoryAsLabel:bool) = 

    let files = Directory.GetFiles(path, &amp;quot;*&amp;quot;,searchOption=SearchOption.AllDirectories)

    files
    |&amp;gt; Array.filter(fun file -&amp;gt; 
        (Path.GetExtension(file) = &amp;quot;.jpg&amp;quot;) ||
        (Path.GetExtension(file) = &amp;quot;.png&amp;quot;))
    |&amp;gt; Array.map(fun file -&amp;gt; 
        let mutable label = Path.GetFileName(file)
        if useDirectoryAsLabel then
            label &amp;lt;-  Directory.GetParent(file).Name
        else
            let mutable brk = false
            for index in 0..label.Length do
                while not brk do
                    if not (label.[index] |&amp;gt; Char.IsLetter) then
                        label &amp;lt;- label.Substring(0,index)
                        brk &amp;lt;- true

        {ImagePath=file; Label=label}
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, create an &lt;code&gt;IDataView&lt;/code&gt; for the training images. An &lt;code&gt;IDataView&lt;/code&gt; is the way data is represented in ML.NET.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let imageIdv = ctx.Data.LoadFromEnumerable&amp;lt;ImageData&amp;gt;(imageData)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Define training pipeline&lt;/h3&gt;
&lt;p&gt;Once your data is is loaded into an &lt;code&gt;IDataView&lt;/code&gt;, set the classifier options by using &lt;code&gt;ImageClassificationTrainer.Options&lt;/code&gt;. Use it to define the name of the network architecture, input and output columns, and some additional parameters. The network architecture used in this case is &lt;a href="https://www.paperswithcode.com/method/inception-resnet-v2"&gt;&lt;code&gt;ResNet V2&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let classifierOptions = ImageClassificationTrainer.Options()
classifierOptions.FeatureColumnName &amp;lt;- &amp;quot;Image&amp;quot; 
classifierOptions.LabelColumnName &amp;lt;- &amp;quot;LabelKey&amp;quot; 
classifierOptions.TestOnTrainSet &amp;lt;- true  
classifierOptions.Arch &amp;lt;- ImageClassificationTrainer.Architecture.ResnetV2101
classifierOptions.MetricsCallback &amp;lt;- Action&amp;lt;ImageClassificationTrainer.ImageClassificationMetrics&amp;gt;(fun x -&amp;gt; printfn &amp;quot;%s&amp;quot; (x.ToString()))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define the preprocessing steps, image classification trainer (along with the previously defined options) and postprocessing steps.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let pipeline = 
    EstimatorChain()
        .Append(ctx.Transforms.LoadRawImageBytes(&amp;quot;Image&amp;quot;,null,&amp;quot;ImagePath&amp;quot;))
        .Append(ctx.Transforms.Conversion.MapValueToKey(&amp;quot;LabelKey&amp;quot;,&amp;quot;Label&amp;quot;))
        .Append(ctx.MulticlassClassification.Trainers.ImageClassification(classifierOptions))
        .Append(ctx.Transforms.Conversion.MapKeyToValue(&amp;quot;PredictedLabel&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ML.NET image classification API leverages a technique known as transfer learning. Transfer learning uses pretrained models (usually neural networks) and retrains the last few layers using new data. This significantly cuts down the amount of time, resources, and data you need to train deep learning models. ML .NET is able to do this with the help of &lt;a href="https://github.com/SciSharp/TensorFlow.NET"&gt;TensorFlow .NET&lt;/a&gt;, a set of .NET bindings for the TensorFlow deep learning framework. Although transfer learning usually makes the process of training a deep learning models less resource intensive, the TensorFlow API is usually low level and still requires a significant amount of code. See this &lt;a href="https://github.com/SciSharp/SciSharp-Stack-Examples/blob/master/src/TensorFlowNET.Examples/ImageProcessing/TransferLearningWithInceptionV3.cs"&gt;transfer learning example&lt;/a&gt; from TensorFlow.NET to see how you'd do it in TensorFlow.NET. Although the low-level nature of the TensorFlow API gives you control over what you're building, many times you don't need that level of control. ML.NET through the image classification trainer greatly simplifies this process by providing a high-level API for achieving the same task.&lt;/p&gt;
&lt;p&gt;To train the model, use the &lt;code&gt;Fit&lt;/code&gt; method on the training image &lt;code&gt;IDataView&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let model = pipeline.Fit(imageIdv)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Throughout the training process, you should see output similar to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-console"&gt;Phase: Bottleneck Computation, Dataset used:      Train, Image Index: 279
Phase: Bottleneck Computation, Dataset used:      Train, Image Index: 280
Phase: Bottleneck Computation, Dataset used: Validation, Image Index:   1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the model trained, it's time to use it to make predictions. Optionally, you can save it for use in other applications.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;ctx.Model.Save(model,imageIdv.Schema,&amp;quot;fsadvent2020-model.zip&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Make predictions&lt;/h2&gt;
&lt;p&gt;Load the test images and create an &lt;code&gt;IDataView&lt;/code&gt; for them. The test images used are of Jack Skellington and The Grinch.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let testImages = 
    Directory.GetFiles(&amp;quot;C:/Datasets/fsadvent2020/Test&amp;quot;)
    |&amp;gt; Array.map(fun file -&amp;gt; {ImagePath=file; Label=&amp;quot;&amp;quot;})

let testImageIdv = ctx.Data.LoadFromEnumerable&amp;lt;ImageData&amp;gt;(testImages)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="https://www.khwiki.com/images/thumb/f/ff/Jack_Skellington_KHII.png/180px-Jack_Skellington_KHII.png" class="img-fluid" alt="The grinch" /&gt;&lt;/p&gt;
&lt;p&gt;Then, use the model to make predictions.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let predictionIdv = model.Transform(testImageIdv)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of the easiest ways to access the predictions is to create an &lt;code&gt;IEnumerable&lt;/code&gt;. To do so, use the &lt;code&gt;CreateEnumerable&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let predictions = ctx.Data.CreateEnumerable&amp;lt;ImagePrediction&amp;gt;(predictionIdv,false)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, use the built-in F# sequence operations to display the predictions&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;predictions |&amp;gt; Seq.iter(fun pred -&amp;gt; 
    printfn &amp;quot;%s is %s&amp;quot; (Path.GetFileNameWithoutExtension(pred.ImagePath)) pred.PredictedLabel)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output should look like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-console"&gt;grinch is Naughty
jack is Naughty
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I showed how you can use the ML.NET and TensorFlow.NET to train an image classification model to classify Disney characters as naughty or nice. Depending on the level of control you need, you might choose to use TensorFlow.NET or if you want a high-level API for training an image classifier, you can use the ML.NET. Most importantly, we figured out that Jack Skellington and The Grinch are naughty, so I guess no gifts for them this year? Happy coding!&lt;/p&gt;
&lt;h3&gt;Call to Action&lt;/h3&gt;
&lt;p&gt;Originally, I had planned on writing this article using &lt;a href="https://github.com/SciSharp/TensorFlow.NET"&gt;TensorFlow.Keras&lt;/a&gt;, which is part of the SciSharp TensorFlow.NET project. TensorFlow.Keras provides .NET bindings for Keras. Keras provides a high-level API for TensorFlow which makes the process of building custom neural networks much simpler than working with the TensorFlow API. Unfortunately, while trying to adapt my scenario to an &lt;a href="https://github.com/SciSharp/SciSharp-Stack-Examples/blob/master/src/TensorFlowNET.Examples/ImageProcessing/ImageClassificationKeras.cs"&gt;existing sample&lt;/a&gt;, I ran into an &lt;a href="https://github.com/SciSharp/SciSharp-Stack-Examples/issues/23"&gt;issue&lt;/a&gt;.  This is not something I would have been able to resolve in time to publish this post, so I defaulted to using ML.NET.&lt;/p&gt;
&lt;p&gt;I'm a big fan of the work being done by the SciSharp community and the machine learning and data science capabilities it brings to the .NET ecosystem. The work and efforts are all community driven, and as such, there's plenty of opportunities to contribute. Here are just some examples of ways to contribute, especially from an F# perspective. From my end, I plan on eventually converting this sample to use TensorFlow.Keras. See you in the SciSharp repos! 🙂&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/102030239-f4ac3700-3d7f-11eb-9898-f18990a56326.png" class="img-fluid" alt="FsLab SciSharp contribute" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/image-classification-mlnet-fsadvent2020?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/image-classification-mlnet-fsadvent2020</guid>
      <pubDate>2020-12-14 20:03:18</pubDate>
      <category>#image-classification</category>
      <category>#mlnet</category>
      <category>#fsharp</category>
      <category>#dotnet</category>
      <category>#deep-learning</category>
      <category>#fsadvent2020</category>
      <category>#tensorflow</category>
    </item>
    <item>
      <title>Use .NET Interactive to run .NET code in Jupyter Notebooks on an Azure Machine Learning compute instance</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/dotnet-jupyter-notebooks-azure-machine-learning-compute-instance?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/dotnet-jupyter-notebooks-azure-machine-learning-compute-instance&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this post, I'll go over how you can run .NET code in Jupyter Notebooks using .NET Interactive and Azure Machine Learning compute instances.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://jupyter.org/"&gt;Jupyter Notebooks&lt;/a&gt; are an graphical interactive computing environment used in disciplines like data science and education. When it comes to prototyping or learning scenarios, they can help users see near real-time what their code is doing. The interactive computing protocol used by Jupyter Notebooks is extensible and as a result, you're able to run a variety of languages inside of this environment via kernels. Kernels are processes that take the input from the user, execute the code, and return the output for display and further processing. Using &lt;a href="https://github.com/dotnet/interactive"&gt;.NET Interactive&lt;/a&gt;, a .NET Core global tool, that among its many features provides a kernel for Jupyter Notebooks to run .NET (C#/F#) and PowerShell code.&lt;/p&gt;
&lt;p&gt;Although you can install and run Jupyter Notebooks on your local computer, services like &lt;a href="https://docs.microsoft.com/azure/machine-learning/"&gt;Azure Machine Learning&lt;/a&gt; provide single VMs known as compute instances which have Jupyter Notebooks as well as many popular data science libraries and development tools like Docker preinstalled. Therefore, installing .NET Interactive on one of these VMs is a seamless experience. Additionally, Azure Machine Learning gives you the ability to share computing resources with teams of various sizes.&lt;/p&gt;
&lt;p&gt;Let's get started!&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://azure.microsoft.com/free/"&gt;Azure account&lt;/a&gt;. If you don't have one, create one.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Create Azure Machine Learning workspace&lt;/h2&gt;
&lt;p&gt;A workspace organizes all of you Azure Machine Learning resources and assets such as compute instances in a single place. In order to use Azure Machine Learning, you have to create a workspace.&lt;/p&gt;
&lt;p&gt;There's multiple ways to create a workspace. For this writeup, I use the Azure Portal.&lt;/p&gt;
&lt;p&gt;Navigate to &lt;a href="https://portal.azure.com"&gt;portal.azure.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the portal, select &lt;strong&gt;Create a resource&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/91647989-d79d6380-ea2f-11ea-9d97-cc265c79801f.png" class="img-fluid" alt="Create new resource Azure Portal" /&gt;&lt;/p&gt;
&lt;p&gt;From the resource list, select &lt;strong&gt;AI + Machine Learning &amp;gt; Machine Learning&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/91647999-01ef2100-ea30-11ea-954a-3c3280a9cbca.png" class="img-fluid" alt="Create Azure Machine Learning Resource" /&gt;&lt;/p&gt;
&lt;p&gt;Fill in the form and select &lt;strong&gt;Review + create&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/91648065-07993680-ea31-11ea-9273-c28b4ec4bf7c.png" class="img-fluid" alt="Create Azure Machine Learning workspace" /&gt;&lt;/p&gt;
&lt;p&gt;Review your information before creating the workspace and select &lt;strong&gt;Create&lt;/strong&gt;. Deployment takes a few minutes. As part of your workspace deployment, additional Azure resources are created such as Azure Container Registry, Azure Storage account, Azure Application Insights, and Azure KeyVault.&lt;/p&gt;
&lt;p&gt;For more information on workspaces, see the &lt;a href="https://docs.microsoft.com/azure/machine-learning/concept-workspace"&gt;Azure Machine Learning workspace documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Create an Azure Machine Learning compute instance&lt;/h2&gt;
&lt;p&gt;As mentioned earlier, an Azure Machine Learning compute instance is a single VM that comes with a variety of development tools and libraries commonly used for data science and machine learning workflows preinstalled.&lt;/p&gt;
&lt;p&gt;Navigate to &lt;a href="https://ml.azure.com"&gt;Azure Machine Learning studio&lt;/a&gt;. The studio is a web interface for managing your Azure Machine Learning resources such as data, experiments, models, and compute.&lt;/p&gt;
&lt;p&gt;In studio, select &lt;strong&gt;Create new &amp;gt; Compute instance&lt;/strong&gt; to create a new compute instance.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/91648186-73c86a00-ea32-11ea-9986-205306fe76b0.png" class="img-fluid" alt="Create new Azure Machine Learning compute instance" /&gt;&lt;/p&gt;
&lt;p&gt;Provide a name for your instance and select &lt;strong&gt;Create&lt;/strong&gt;. You can also customize the size of your VM and whether you want it to be GPU enabled. In this case, I just selected the preselected option.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/91648251-1c76c980-ea33-11ea-91de-818e1f63767c.png" class="img-fluid" alt="Provision Azure Machine Learning compute instance" /&gt;&lt;/p&gt;
&lt;p&gt;Once your compute instance is provisioned, select the &lt;strong&gt;Jupyter&lt;/strong&gt; link to launch Jupyter Notebooks.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/91648307-f271d700-ea33-11ea-9b80-57e44f0bcdf5.png" class="img-fluid" alt="Launch Jupyter Notebooks" /&gt;&lt;/p&gt;
&lt;p&gt;For more information on compute instances, see the &lt;a href="https://docs.microsoft.com/azure/machine-learning/concept-compute-instance"&gt;Azure Machine Learning compute instance documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Install the .NET Core 3.1 SDK&lt;/h2&gt;
&lt;p&gt;At the time of this writing, the image used by compute instances is Ubuntu 16.04. Therefore, we'll be installing the Linux version of the .NET Core SDK.&lt;/p&gt;
&lt;p&gt;Inside the Jupyter Notebooks, select &lt;strong&gt;New &amp;gt; Terminal&lt;/strong&gt; to create a new terminal.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/91648326-1c2afe00-ea34-11ea-8f84-48b98e1ec148.png" class="img-fluid" alt="image" /&gt;&lt;/p&gt;
&lt;p&gt;In the terminal, add the Microsoft package signing key to your list of trusted keys.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://packages.microsoft.com/config/ubuntu/16.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb
sudo dpkg -i packages-microsoft-prod.deb
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install the .NET Core 3.1 SDK with the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update; \
  sudo apt-get install -y apt-transport-https &amp;amp;&amp;amp; \
  sudo apt-get update &amp;amp;&amp;amp; \
  sudo apt-get install -y dotnet-sdk-3.1
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Note that as of the time of this writing, .NET Interactive requires .NET Core 3.1.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can also find these installation instructions in the &lt;a href="https://docs.microsoft.com/dotnet/core/install/linux-ubuntu#1604-"&gt;.NET Core installation documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Install .NET Interactive&lt;/h2&gt;
&lt;p&gt;Start off by checking which kernels are installed. In the terminal, enter the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;jupyter kernelspec list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output should look something like the output below&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-console"&gt;Available kernels:
  python3            /anaconda/envs/azureml_py36/share/jupyter/kernels/python3
  ir                 /usr/local/share/jupyter/kernels/ir
  python3-azureml    /usr/local/share/jupyter/kernels/python3-azureml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, install the .NET Interactive global tool&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet tool install -g --add-source &amp;quot;https://dotnet.myget.org/F/dotnet-try/api/v3/index.json&amp;quot; Microsoft.dotnet-interactive
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the .NET Interactive tool to install the .NET and powershell kernels.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet interactive jupyter install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, run the following command to confirm that the kernels have been installed.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;jupyter kernelspec list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the output you should see the C#, F#, and PowerShell kernels listed&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;Available kernels:
  .net-csharp        /home/azureuser/.local/share/jupyter/kernels/.net-csharp
  .net-fsharp        /home/azureuser/.local/share/jupyter/kernels/.net-fsharp
  .net-powershell    /home/azureuser/.local/share/jupyter/kernels/.net-powershell
  python3            /anaconda/envs/azureml_py36/share/jupyter/kernels/python3
  ir                 /usr/local/share/jupyter/kernels/ir
  python3-azureml    /usr/local/share/jupyter/kernels/python3-azureml
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Add .NET global tools directory to PATH&lt;/h2&gt;
&lt;p&gt;The Jupyter Notebook server is managed by a &lt;code&gt;systemd&lt;/code&gt; service. Although during installation .NET Interactive is added to your &lt;code&gt;PATH&lt;/code&gt; environment variable, it's not added to the environment file &lt;code&gt;/etc/environment&lt;/code&gt; which is used by &lt;code&gt;systemd&lt;/code&gt;. Therefore, in order to run .NET Interactive, you have to add the .NET global tools directory to that file.&lt;/p&gt;
&lt;p&gt;The contents of the file should look similar to the one below&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;PATH=&amp;quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games&amp;quot;
AML_CloudName=AzureCloud
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Open the &lt;code&gt;/etc/environment&lt;/code&gt; using your preferred text editor (vi/nano) and replace the &lt;code&gt;PATH&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;PATH=&amp;quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/azureuser/.dotnet/tools&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Save your changes and restart the Jupyter Server. In the terminal, run the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo service jupyter restart
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will close your terminal session since the notebook server has been restarted.&lt;/p&gt;
&lt;p&gt;Close all Jupyter Notebook windows.&lt;/p&gt;
&lt;h2&gt;Create new .NET Jupyter Notebook&lt;/h2&gt;
&lt;p&gt;From Azure Machine Learning studio, launch Jupyter Notebooks again by selecting the &lt;strong&gt;Jupyter&lt;/strong&gt; link.&lt;/p&gt;
&lt;p&gt;Then, in Jupyter Notebooks select &lt;strong&gt;New&lt;/strong&gt; and create a new notebook (C#/F#/PowerShell). In this case, I created an F# notebook.&lt;/p&gt;
&lt;p&gt;Once the kernel is ready, enter code into the first cell and run it.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/11130940/91648958-20f3b000-ea3c-11ea-95a1-3e4ba71c31d8.png" class="img-fluid" alt="FSharp Jupyter Notebook running in AML compute instance" /&gt;&lt;/p&gt;
&lt;p&gt;Congratulations! You should now be able to run .NET and PowerShell code inside of your Azure Machine Learning compute instance.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I showed how you can run .NET code in Jupyter Notebooks on an Azure Machine Learning compute instance with the help of .NET Interactive. This enables you to interactively prototype solutions remotely while still having control over your environment and dependencies. Now that you have .NET interactive setup, compute instances also give you the option of using JupyterLab. Feel free to tinker with the different environments and see which one works best for you. Happy coding!&lt;/p&gt;
&lt;h2&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/azure/machine-learning/"&gt;Azure Machine Learning documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dotnet/interactive"&gt;.NET Interactive GitHub repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://jupyter.org/"&gt;Jupyter Notebooks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/dotnet-jupyter-notebooks-azure-machine-learning-compute-instance?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/dotnet-jupyter-notebooks-azure-machine-learning-compute-instance</guid>
      <pubDate>2020-08-29 20:23:09</pubDate>
      <category>#dotnet</category>
      <category>#jupyter-notebooks</category>
      <category>#dotnet-interactive</category>
      <category>#azure-machine-learning</category>
      <category>#azure</category>
      <category>#programming</category>
      <category>#development-tools</category>
    </item>
    <item>
      <title>Serverless Racket Applications Using Azure Functions Custom Handlers</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/serverless-racket-azure-functions-custom-handlers?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/serverless-racket-azure-functions-custom-handlers&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://racket-lang.org/"&gt;Racket&lt;/a&gt; is a fun and powerful general purpose programming language based on the Scheme dialect of Lisp that provides tools and packages that allow individuals to quickly be productive. Although you can build traditional web applications with it, it would be nice to use it with cloud-native technologies like serverless. Azure Functions is designed for these types of event-driven workflows but unfortunately does not officially support Racket. Recently, a new feature called custom handlers was announced which allows individuals to run web applications written in any language that supports HTTP primitives as an Azure Function. When I learned of this feature, my immediate thought was, challenge accepted!&lt;/p&gt;
&lt;p&gt;Custom handlers require the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Write a web server to process requests&lt;/li&gt;
&lt;li&gt;Define the bindings for the request and response function payloads&lt;/li&gt;
&lt;li&gt;Configure the Azure Functions host to send request to the web server&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this writeup, I'll show how to set up a Racket web server that processes &lt;code&gt;GET&lt;/code&gt; requests running as an Azure Function. The source code for this project can be found in the &lt;a href="https://github.com/lqdev/RacketAzureFunctionsCustomHandlerSample"&gt;RacketAzureFunctionsCustomHandlerSample GitHub repository&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;This project was built on a Windows 10 PC, but it should work cross-platform on Mac and Linux.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://nodejs.org/en/"&gt;Node.js&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://download.racket-lang.org/"&gt;Racket&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/azure/azure-functions/functions-run-local"&gt;Azure Functions Core Tools&lt;/a&gt;. This sample uses v2.x of the tool.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Create Azure Functions project&lt;/h2&gt;
&lt;p&gt;Create a new directory called &lt;code&gt;RacketAzureFunctionsCustomHandlerSample&lt;/code&gt; and navigate to it.&lt;/p&gt;
&lt;h2&gt;Create Racket server&lt;/h2&gt;
&lt;p&gt;The way Azure Functions custom handlers work is by having the Azure Functions host proxy requests to a web server written in the language of choice which processes the request and sends the response back to the Azure Functions host.&lt;/p&gt;
&lt;p&gt;Start by setting the environment variable where Azure Functions and the server listen on. Create and environment variable called &lt;code&gt;FUNCTIONS_HTTPWORKER_PORT&lt;/code&gt;. In this example, I set the variable to &lt;code&gt;7071&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Inside of your application directory, create a file called &lt;em&gt;server.rkt&lt;/em&gt; which will contain the server logic.&lt;/p&gt;
&lt;p&gt;Open the &lt;em&gt;server.rkt&lt;/em&gt; file. Define the language and import the required packages.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-racket"&gt;#lang racket
(require json)
(require web-server/servlet)
(require web-server/servlet-env)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, get the port where the server listens on from the &lt;code&gt;FUNCTIONS_HTTPWORKER_PORT&lt;/code&gt; environment variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-racket"&gt;(define PORT (string-&amp;gt;number (getenv &amp;quot;FUNCTIONS_HTTPWORKER_PORT&amp;quot;)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, create a function called &lt;code&gt;get-values&lt;/code&gt; to process your request. In this case, the function receives a &lt;code&gt;GET&lt;/code&gt; request that returns a JSON object containing a list of integers.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-racket"&gt;(define (get-values req)
    (response/full
        200
        #&amp;quot;OK&amp;quot;
        (current-seconds)
        #&amp;quot;application/json;charset=utf-8&amp;quot;
        empty
        (list (jsexpr-&amp;gt;bytes #hasheq((value . (1 2 3)))))))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After that, define the routes so your server dispatches requests to the appropriate endpoint. In this case, &lt;code&gt;GET&lt;/code&gt; requests to the &lt;code&gt;/values&lt;/code&gt; endpoint are sent to and processed by the &lt;code&gt;get-values&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-racket"&gt;(define-values (dispatch req)
    (dispatch-rules
        [(&amp;quot;values&amp;quot;) #:method &amp;quot;get&amp;quot; get-values]
        [else (error &amp;quot;Route does not exist&amp;quot;)]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final &lt;em&gt;server.rkt&lt;/em&gt; file should contains content similar to the one below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-racket"&gt;;; Define language and import packages
#lang racket
(require json)
(require web-server/servlet)
(require web-server/servlet-env)

;; Get port where server listens on
(define PORT (string-&amp;gt;number (getenv &amp;quot;FUNCTIONS_HTTPWORKER_PORT&amp;quot;)))

;; Create function to handle GET /values request
(define (get-values req)
    (response/full
        200
        #&amp;quot;OK&amp;quot;
        (current-seconds)
        #&amp;quot;application/json;charset=utf-8&amp;quot;
        empty
        (list (jsexpr-&amp;gt;bytes #hasheq((value . (1 2 3)))))))

;; Define routes
(define-values (dispatch req)
    (dispatch-rules
        [(&amp;quot;values&amp;quot;) #:method &amp;quot;get&amp;quot; get-values]
        [else (error &amp;quot;Route does not exist&amp;quot;)]))

;; Define and start server
(serve/servlet
    (lambda (req) (dispatch req))
    #:launch-browser? #f
    #:quit? #f
    #:port PORT
    #:servlet-path &amp;quot;/&amp;quot;
    #:servlet-regexp #rx&amp;quot;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Test the Racket server&lt;/h2&gt;
&lt;p&gt;Start the server by running the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;racket --require server.rkt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, using an application like Postman or Insomina, make a &lt;code&gt;GET&lt;/code&gt; request to &lt;code&gt;http://localhost:7071/values&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The response should look like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;value&amp;quot;: [
        1,
        2,
        3
    ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Define function bindings&lt;/h2&gt;
&lt;p&gt;The way Azure Functions discovers functions is through subdirectories containing a binding definition called &lt;em&gt;function.json&lt;/em&gt;. The name of the subdirectories must match the name of your function's route path. For example if the route path is &lt;code&gt;/values&lt;/code&gt;, then the name of the subdirectory is  &lt;code&gt;values&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Create a subdirectory inside the main application directory called &lt;em&gt;values&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Inside the &lt;em&gt;values&lt;/em&gt; subdirectory, create a file called &lt;em&gt;function.json&lt;/em&gt; and add the following content to it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;bindings&amp;quot;: [
      {
        &amp;quot;type&amp;quot;: &amp;quot;httpTrigger&amp;quot;,
        &amp;quot;direction&amp;quot;: &amp;quot;in&amp;quot;,
        &amp;quot;name&amp;quot;: &amp;quot;req&amp;quot;,
        &amp;quot;methods&amp;quot;: [&amp;quot;get&amp;quot;]
      },
      {
        &amp;quot;type&amp;quot;: &amp;quot;http&amp;quot;,
        &amp;quot;direction&amp;quot;: &amp;quot;out&amp;quot;,
        &amp;quot;name&amp;quot;: &amp;quot;res&amp;quot;
      }
    ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;function.json&lt;/em&gt; file defines the request and response payloads. In this case, the incoming request is an &lt;code&gt;HttpTrigger&lt;/code&gt; that only handles &lt;code&gt;GET&lt;/code&gt; requests and returns an HTTP response.&lt;/p&gt;
&lt;h2&gt;Create server executable&lt;/h2&gt;
&lt;p&gt;Package your server application into a single executable by entering the following command into the command prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;raco exe server.rkt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once your application is packaged, an executable with the name &lt;em&gt;server.exe&lt;/em&gt; should be created in your application directory.&lt;/p&gt;
&lt;h2&gt;Configure Azure Functions host&lt;/h2&gt;
&lt;p&gt;In your application directory, create a file called &lt;em&gt;host.json&lt;/em&gt; and add the following contents:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;version&amp;quot;: &amp;quot;2.0&amp;quot;,
  &amp;quot;httpWorker&amp;quot;: {
      &amp;quot;description&amp;quot;: {
          &amp;quot;defaultExecutablePath&amp;quot;: &amp;quot;server.exe&amp;quot;
      }
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This &lt;em&gt;host.json&lt;/em&gt; configuration file tells the Azure Functions host where to find the web server executable.&lt;/p&gt;
&lt;h2&gt;Run the Azure Functions application&lt;/h2&gt;
&lt;p&gt;Inside the root application directory, enter the following command into the command prompt.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;func start
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using an application like Postman or Insomnia, make a &lt;code&gt;GET&lt;/code&gt; request to &lt;code&gt;localhost:7071/api/values&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The response should look like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;value&amp;quot;: [
        1,
        2,
        3
    ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this writeup, I showed how to create a Racket serverless application that runs on Azure Functions by using custom handlers. Doing so requires you to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Write a web server to process requests&lt;/li&gt;
&lt;li&gt;Define the bindings for the request and response function payloads&lt;/li&gt;
&lt;li&gt;Configure the Azure Functions host to send request to the web server&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Although in this example, the server was written in Racket, the same process is applicable to other languages. Keep in mind that at the time of this writing, custom handlers are preview and may change. Happy coding!&lt;/p&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/azure/azure-functions/functions-custom-handlers"&gt;Azure Functions Custom Handlers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/serverless-racket-azure-functions-custom-handlers?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/serverless-racket-azure-functions-custom-handlers</guid>
      <pubDate>2020-03-21 13:45:43</pubDate>
      <category>#serverless</category>
      <category>#racket</category>
      <category>#lisp</category>
      <category>#azure-functions</category>
      <category>#azure</category>
      <category>#programming</category>
      <category>#web</category>
    </item>
    <item>
      <title>Deploy ML.NET Machine Learning Model in Blazor WebAssembly Static Website</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/deploy-machine-learning-mlnet-models-blazor-webassembly?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/deploy-machine-learning-mlnet-models-blazor-webassembly&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;There are many ways of deploying machine learning models. The most common way to do so is by exposing models as a web service through APIs or serverless functions. One of the considerations when deploying a model as a web service is latency and performance. The process of making predictions over HTTP using a model involves accepting the user input, loading the serialized version of the model from a file, using the model to make a prediction, and returning the prediction back to the user. Since models are typically just static files, another way of deploying a model is as a static asset over the web, just like any other HTML, CSS, or JavaScript file. This deployment method is similar to that of TensorFlow.js. Deploying in this way has several advantages. One advantage is that there is no longer a web service just to serve the model, making it more cost-efficient. Another advantage is that once the model has been downloaded onto the user's PC, the resources used at that point are those of the user's PC rather than the server the model would otherwise be hosted in. Finally, because the model is a static file, it can be distributed via CDNs.&lt;/p&gt;
&lt;p&gt;One of the challenges with this is that machine learning models are usually built using languages other than JavaScript. This makes using the same code / library the model was built difficult or nearly impossible. WebAssembly is changing that by allowing Rust, C++, C# and other languages to run natively inside the browser. Having that ability, the code / logic to load the model and make predictions is much easier and almost comparable to that of a native platform. Blazor WebAssembly provides users the ability to create modern component-based web applications completely in C#. Additionally, Blazor WebAssembly allows users to publish and deploy their applications as static websites in an easy and cost-efficient way. ML.NET is an open-source, cross-platform framework that allows developers to create machine learning models using .NET. In this post, I'll show how to train a multiclass classification machine learning model that predicts iris flower species. Then, I'll take that model and deploy it alongside a Blazor WebAssembly static website to Azure Storage. The full code for this application may be found at the &lt;a href="https://github.com/lqdev/MLNETBlazorWASMSample"&gt;MLNETBlazorWASMSample repository on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;This project was built on a Windows PC but should work cross platform on Mac and Linux.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://dotnet.microsoft.com/download/dotnet-core/3.1"&gt;.NET Core SDK 3.1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nuget.org/packages/Microsoft.AspNetCore.Blazor.Templates/3.2.0-preview1.20073.1"&gt;Blazor WebAssembly Template&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aka.ms/amlFree"&gt;Azure Subscription&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://azure.microsoft.com/en-us/features/storage-explorer/"&gt;Azure Storage Explorer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Set up the solution&lt;/h2&gt;
&lt;p&gt;The solution built in this post contains three projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SchemaLibrary: C# .NET Standard 2.0 class library that contains the schema definition classes of the data used to train the model as well as the prediction output generated by the model.&lt;/li&gt;
&lt;li&gt;TrainingConsole: C# .NET Core 3.1 console application used to train the machine learning model.&lt;/li&gt;
&lt;li&gt;BlazorWebApp: Blazor WebAssembly web application to make predictions using machine learning model trained by TrainingConsole application.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Install Blazor WebAssembly Template&lt;/h3&gt;
&lt;p&gt;Use the .NET CLI to run the following command in the command prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet new -i Microsoft.AspNetCore.Blazor.Templates::3.2.0-preview1.20073.1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create the solution&lt;/h3&gt;
&lt;p&gt;Create a new directory for the solution called &lt;em&gt;MLNETBlazorWASMSample&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;mkdir MLNETBlazorWASMSample
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Navigate to the newly created solution directory and create a solution:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;cd MLNETBlazorWASMSample
dotnet new sln
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create schema class library&lt;/h3&gt;
&lt;p&gt;The data schema for the model input and output are shared during training as well as when making predictions. To share resources, create a class library that's shared by the &lt;code&gt;ConsoleTraining&lt;/code&gt; and &lt;code&gt;BlazorWebApp&lt;/code&gt; projects. In the solution directory, enter the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet new classlib -o SchemaLibrary
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Install the &lt;em&gt;Microsoft.ML&lt;/em&gt; NuGet package (This solution is built with version 1.4.0). The &lt;em&gt;Microsoft.ML&lt;/em&gt; package is used throughout the entire solution.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet add SchemaLibrary package Microsoft.ML
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the library project to the solution.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet sln add SchemaLibrary
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create the training console application&lt;/h3&gt;
&lt;p&gt;The console application contains the series of data transformations and algorithms used to train the model. In the solution directory, create a new console application.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet new console -o TrainingConsole
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the console application to the solution.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet sln add TrainingConsole
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Reference the &lt;em&gt;SchemaLibrary&lt;/em&gt; project.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet add TrainingConsole reference SchemaLibrary
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create the Blazor WebAssembly web application&lt;/h3&gt;
&lt;p&gt;The web application contains a few input elements so users can provide new data that the model then uses to make predictions. In the solution directory, create a new Blazor WebAssembly application.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet new blazorwasm -o BlazorWebApp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the web application project to the solution.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet sln add BlazorWebApp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Reference the &lt;em&gt;SchemaLibrary&lt;/em&gt; project.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet add BlazorWebApp reference SchemaLibrary
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Define the schema&lt;/h2&gt;
&lt;h3&gt;Understand the data&lt;/h3&gt;
&lt;p&gt;The data used to train the model comes from the &lt;a href="https://archive.ics.uci.edu/ml/datasets/Iris"&gt;iris dataset&lt;/a&gt;. It contains four numerical columns which are sepal and petal measurements and one numerical column for the species of iris flower. This is a sample of the data.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Sepal length (cm)&lt;/th&gt;
&lt;th&gt;Sepal width (cm)&lt;/th&gt;
&lt;th&gt;Petal length (cm)&lt;/th&gt;
&lt;th&gt;Petal width (cm)&lt;/th&gt;
&lt;th&gt;Class (iris species)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;5.1&lt;/td&gt;
&lt;td&gt;3.5&lt;/td&gt;
&lt;td&gt;1.4&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;Iris-setosa&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7.0&lt;/td&gt;
&lt;td&gt;3.2&lt;/td&gt;
&lt;td&gt;4.7&lt;/td&gt;
&lt;td&gt;1.4&lt;/td&gt;
&lt;td&gt;Iris-versicolor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6.3&lt;/td&gt;
&lt;td&gt;3.3&lt;/td&gt;
&lt;td&gt;6.0&lt;/td&gt;
&lt;td&gt;2.5&lt;/td&gt;
&lt;td&gt;Iris-virginica&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Define model input schema&lt;/h3&gt;
&lt;p&gt;In the &lt;em&gt;SchemaLibrary&lt;/em&gt; project, create a class called &lt;code&gt;ModelInput&lt;/code&gt; to model the data used for training and as model input.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;ni ModelInput.cs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;ModelInput&lt;/code&gt; class should look like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Microsoft.ML.Data;

namespace SchemaLibrary
{
    public class ModelInput
    {
        [LoadColumn(0)]
        public float SepalLength { get; set; }

        [LoadColumn(1)]
        public float SepalWidth { get; set; }

        [LoadColumn(2)]
        public float PetalLength { get; set; }

        [LoadColumn(3)]
        public float PetalWidth { get; set; }

        [LoadColumn(4)]
        public string Label { get; set; }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the &lt;code&gt;Class&lt;/code&gt; column is now a property called &lt;code&gt;Label&lt;/code&gt;. This is for two reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Avoid using the &lt;code&gt;class&lt;/code&gt; keyword.&lt;/li&gt;
&lt;li&gt;In ML.NET, the default column name of the column to predict expected by algorithms is &lt;code&gt;Label&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Also notice the &lt;code&gt;LoadColumn&lt;/code&gt; attributes at the top of each property. This is used to tell the loader the index of the column where the data for the respective property is.&lt;/p&gt;
&lt;h3&gt;Define model output schema&lt;/h3&gt;
&lt;p&gt;Similar to the input schema, there's a schema for the output of the model. The type of model used in this solution is a multiclass classification model since there are more than two categories to choose from for iris flower species. Multiclass classification models output a column called &lt;code&gt;PredictedLabel&lt;/code&gt; which contains the name of the predicted category. In the &lt;em&gt;SchemaLibrary&lt;/em&gt; project, create a class called &lt;code&gt;ModelOutput&lt;/code&gt; to model the predictions made by the model.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;ni ModelOutput.cs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;ModelOutput&lt;/code&gt; class should look like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;namespace SchemaLibrary
{
    public class ModelOutput
    {
        public string PredictedLabel { get; set; }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Train the model&lt;/h2&gt;
&lt;p&gt;Now it's time to create the application that trains the model.&lt;/p&gt;
&lt;h3&gt;Get the data&lt;/h3&gt;
&lt;p&gt;Download the data and save it inside the &lt;em&gt;TrainingConsole&lt;/em&gt; project directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;curl https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data -o iris.data
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Define the data preparation and training steps&lt;/h3&gt;
&lt;p&gt;In the &lt;em&gt;TrainingConsole&lt;/em&gt; project, open the &lt;em&gt;Program.cs&lt;/em&gt; file and add the following using statements at the top:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;
using System.Linq;
using Microsoft.ML;
using SchemaLibrary;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, delete the contents inside the &lt;code&gt;Main&lt;/code&gt; method and replace it with the following.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;// 1. Initialize MLContext
MLContext mlContext = new MLContext();

// 2. Load the data
IDataView data = mlContext.Data.LoadFromTextFile&amp;lt;ModelInput&amp;gt;(&amp;quot;iris.data&amp;quot;, separatorChar:',');

// 3. Shuffle the data
IDataView shuffledData = mlContext.Data.ShuffleRows(data);

// 3. Define the data preparation and training pipeline.
IEstimator&amp;lt;ITransformer&amp;gt; pipeline = 
    mlContext.Transforms.Concatenate(&amp;quot;Features&amp;quot;,&amp;quot;SepalLength&amp;quot;,&amp;quot;SepalWidth&amp;quot;,&amp;quot;PetalLength&amp;quot;,&amp;quot;PetalWidth&amp;quot;)
        .Append(mlContext.Transforms.NormalizeMinMax(&amp;quot;Features&amp;quot;))
        .Append(mlContext.Transforms.Conversion.MapValueToKey(&amp;quot;Label&amp;quot;))
        .Append(mlContext.MulticlassClassification.Trainers.NaiveBayes())
        .Append(mlContext.Transforms.Conversion.MapKeyToValue(&amp;quot;PredictedLabel&amp;quot;));

// 4. Train with cross-validation
var cvResults = mlContext.MulticlassClassification.CrossValidate(shuffledData, pipeline);

// 5. Get the highest performing model and its accuracy
(ITransformer, double) model = 
    cvResults
        .OrderByDescending(fold =&amp;gt; fold.Metrics.MacroAccuracy)
        .Select(fold =&amp;gt; (fold.Model, fold.Metrics.MacroAccuracy))
        .First();

Console.WriteLine($&amp;quot;Top performing model's macro-accuracy: {model.Item2}&amp;quot;);

// 6. Save the model
mlContext.Model.Save(model.Item1, data.Schema, &amp;quot;model.zip&amp;quot;);

Console.WriteLine(&amp;quot;Model trained&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The training application loads the data from the &lt;code&gt;iris.data&lt;/code&gt; file and applies a series of transformations. First, all of the individual numerical columns are combined into a single vector and stored in a new column called &lt;code&gt;Features&lt;/code&gt;. The &lt;code&gt;Features&lt;/code&gt; column is then normalized and the &lt;code&gt;MapValueToKey&lt;/code&gt; transform is used to convert the text in the &lt;code&gt;Label&lt;/code&gt; column to a number. Then, the transformed data is used to train a model using the &lt;code&gt;NaiveBayes&lt;/code&gt; algorithm. &lt;strong&gt;Note that at the time of this writing, for multiclass classification problems, only Naive Bayes has been confirmed to work with Blazor WebAssembly&lt;/strong&gt;. Finally, the &lt;code&gt;PredictedLabel&lt;/code&gt; is stored as a number so it has to be converted back into text.&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;Fit&lt;/code&gt; method, the data is applied to the pipeline. Because the dataset is small, a technique called cross-validation is used to build a more robust model. Once the model is trained, the model with the top performance is then serialized and saved to a file called &lt;em&gt;model.zip&lt;/em&gt; for later use in the web application.&lt;/p&gt;
&lt;p&gt;The final &lt;em&gt;Program.cs&lt;/em&gt; file should look like the content below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;
using System.Linq;
using Microsoft.ML;
using SchemaLibrary;

namespace TrainingConsole
{
    class Program
    {
        static void Main(string[] args)
        {
            // 1. Initialize MLContext
            MLContext mlContext = new MLContext();

            // 2. Load the data
            IDataView data = mlContext.Data.LoadFromTextFile&amp;lt;ModelInput&amp;gt;(&amp;quot;iris.data&amp;quot;, separatorChar:',');

            // 3. Shuffle the data
            IDataView shuffledData = mlContext.Data.ShuffleRows(data);

            // 3. Define the data preparation and training pipeline.
            IEstimator&amp;lt;ITransformer&amp;gt; pipeline = 
                mlContext.Transforms.Concatenate(&amp;quot;Features&amp;quot;,&amp;quot;SepalLength&amp;quot;,&amp;quot;SepalWidth&amp;quot;,&amp;quot;PetalLength&amp;quot;,&amp;quot;PetalWidth&amp;quot;)
                    .Append(mlContext.Transforms.NormalizeMinMax(&amp;quot;Features&amp;quot;))
                    .Append(mlContext.Transforms.Conversion.MapValueToKey(&amp;quot;Label&amp;quot;))
                    .Append(mlContext.MulticlassClassification.Trainers.NaiveBayes())
                    .Append(mlContext.Transforms.Conversion.MapKeyToValue(&amp;quot;PredictedLabel&amp;quot;));

            // 4. Train with cross-validation
            var cvResults = mlContext.MulticlassClassification.CrossValidate(shuffledData, pipeline);

            // 5. Get the highest performing model and its accuracy
            (ITransformer, double) model = 
                cvResults
                    .OrderByDescending(fold =&amp;gt; fold.Metrics.MacroAccuracy)
                    .Select(fold =&amp;gt; (fold.Model, fold.Metrics.MacroAccuracy))
                    .First();

            Console.WriteLine($&amp;quot;Top performing model's macro-accuracy: {model.Item2}&amp;quot;);

            // 6. Save the model
            mlContext.Model.Save(model.Item1, data.Schema, &amp;quot;model.zip&amp;quot;);

            Console.WriteLine(&amp;quot;Model trained&amp;quot;);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Run the application&lt;/h3&gt;
&lt;p&gt;In the &lt;em&gt;TrainConsole&lt;/em&gt; project directory, use the following command to run the application and train the model:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet run
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Host the model&lt;/h2&gt;
&lt;p&gt;Once you have the model saved, use the Azure Portal to create an Azure Storage account.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly1.png" class="img-fluid" alt="Create Azure Storage Account" /&gt;&lt;/p&gt;
&lt;p&gt;Then, navigate to your newly created storage account resource and create a blob container called &lt;code&gt;models&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly2.png" class="img-fluid" alt="Create Blob Container" /&gt;&lt;/p&gt;
&lt;p&gt;Once your container is created, navigate to it and upload the &lt;em&gt;model.zip&lt;/em&gt; file.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly3.png" class="img-fluid" alt="Upload Model" /&gt;&lt;/p&gt;
&lt;h2&gt;Create prediction web page&lt;/h2&gt;
&lt;p&gt;To make predictions, create a web page to take in user input. Then, provide the user input to the model and display the prediction to the user.&lt;/p&gt;
&lt;h3&gt;Set up imports&lt;/h3&gt;
&lt;p&gt;In the &lt;em&gt;BlazorWebApp&lt;/em&gt; project directory, open the &lt;em&gt;_Imports.razor&lt;/em&gt; file. This contains the using statements for the pages and components in your application. Add the following using statements:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-razor"&gt;@using System.IO
@using Microsoft.ML
@using SchemaLibrary
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create user input page&lt;/h3&gt;
&lt;p&gt;In the &lt;em&gt;BlazorWebApp&lt;/em&gt; project, create a new razor page called &lt;em&gt;Prediction.razor&lt;/em&gt; inside the &lt;em&gt;Pages&lt;/em&gt; directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;ni Prediction.razor
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the following content to it:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-razor"&gt;@page &amp;quot;/prediction&amp;quot;
@inject HttpClient _client

&amp;lt;label&amp;gt;Sepal Length: &amp;lt;/label&amp;gt;
&amp;lt;input type=&amp;quot;text&amp;quot; @bind=&amp;quot;_sepalLength&amp;quot;&amp;gt;&amp;lt;br&amp;gt;
&amp;lt;label&amp;gt;Sepal Width: &amp;lt;/label&amp;gt;
&amp;lt;input type=&amp;quot;text&amp;quot; @bind=&amp;quot;_sepalWidth&amp;quot;&amp;gt;&amp;lt;br&amp;gt;
&amp;lt;label&amp;gt;Petal Length: &amp;lt;/label&amp;gt;
&amp;lt;input type=&amp;quot;text&amp;quot; @bind=&amp;quot;_petalLength&amp;quot;&amp;gt;&amp;lt;br&amp;gt;
&amp;lt;label&amp;gt;Petal Width: &amp;lt;/label&amp;gt;
&amp;lt;input type=&amp;quot;text&amp;quot; @bind=&amp;quot;_petalWidth&amp;quot;&amp;gt;&amp;lt;br&amp;gt;
&amp;lt;button @onclick=&amp;quot;GetPrediction&amp;quot;&amp;gt;Make prediction&amp;lt;/button&amp;gt;
@if(@ModelPrediction == null)
{
    &amp;lt;p&amp;gt;Enter data to get a prediction&amp;lt;/p&amp;gt;
} else
{
    &amp;lt;p&amp;gt;@ModelPrediction&amp;lt;/p&amp;gt;
}


@code {
    private PredictionEngine&amp;lt;ModelInput,ModelOutput&amp;gt; _predictionEngine;
    private string _sepalLength, _sepalWidth, _petalLength, _petalWidth, ModelPrediction;

    protected override async Task OnInitializedAsync()
    {
        Stream savedModel = await _client.GetStreamAsync(&amp;quot;&amp;lt;YOUR-MODEL-ENDPOINT&amp;gt;&amp;quot;);
        MLContext mlContext = new MLContext();
        ITransformer _model = mlContext.Model.Load(savedModel,out DataViewSchema schema);
        _predictionEngine = mlContext.Model.CreatePredictionEngine&amp;lt;ModelInput,ModelOutput&amp;gt;(_model);
    }

    private void GetPrediction()
    {
        ModelInput input = new ModelInput
        {
            SepalLength=float.Parse(_sepalLength),
            SepalWidth=float.Parse(_sepalWidth),
            PetalLength=float.Parse(_petalLength),
            PetalWidth=float.Parse(_petalWidth)
        };

        ModelOutput prediction = _predictionEngine.Predict(input);

        ModelPrediction = prediction.PredictedLabel;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;Predict.razor&lt;/em&gt; page contains a text input element for each of the columns the model was original trained on. When the page is initialized, the model is loaded from Azure Storage and a &lt;code&gt;PredictionEngine&lt;/code&gt; is created. &lt;strong&gt;Make sure to replace &lt;code&gt;&amp;lt;YOUR-MODEL-ENDPOINT&amp;gt;&lt;/code&gt; with the URL of the blob that contains your &lt;code&gt;model.zip&lt;/code&gt;&lt;/strong&gt;. A &lt;code&gt;PredictionEngine&lt;/code&gt; is a convenience API to make a single prediction. Traditionally when models are served as a web service, it is recommended to use the &lt;code&gt;PredictionEnginePool&lt;/code&gt; service because it's thread-safe and more performant in multi-threaded application. However, in this case since the model is downloaded onto the individual user's browser, it's okay to use &lt;code&gt;PredictionEngine&lt;/code&gt;. After a user enters input values and clicks the &amp;quot;Make Prediction&amp;quot; button, the &lt;code&gt;GetPrediction&lt;/code&gt; method executes by taking the user input and making a prediction using the &lt;code&gt;PredictionEngine&lt;/code&gt;. The prediction is then displayed in the browser.&lt;/p&gt;
&lt;h3&gt;Add to navigation menu&lt;/h3&gt;
&lt;p&gt;In the &lt;em&gt;BlazorWebApp&lt;/em&gt; project, open the &lt;em&gt;NavMenu.razor&lt;/em&gt; file in the &lt;em&gt;Shared&lt;/em&gt; directory.&lt;/p&gt;
&lt;p&gt;Add the following list item to the &lt;code&gt;&amp;lt;ul&amp;gt;&lt;/code&gt; element.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;li class=&amp;quot;nav-item px-3&amp;quot;&amp;gt;
    &amp;lt;NavLink class=&amp;quot;nav-link&amp;quot; href=&amp;quot;prediction&amp;quot;&amp;gt;
        &amp;lt;span class=&amp;quot;oi oi-list-rich&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/span&amp;gt; Prediction
    &amp;lt;/NavLink&amp;gt;
&amp;lt;/li&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final &lt;em&gt;NavMenu.razor&lt;/em&gt; page should look like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-razor"&gt;&amp;lt;div class=&amp;quot;top-row pl-4 navbar navbar-dark&amp;quot;&amp;gt;
    &amp;lt;a class=&amp;quot;navbar-brand&amp;quot; href=&amp;quot;&amp;quot;&amp;gt;BlazorWebApp&amp;lt;/a&amp;gt;
    &amp;lt;button class=&amp;quot;navbar-toggler&amp;quot; @onclick=&amp;quot;ToggleNavMenu&amp;quot;&amp;gt;
        &amp;lt;span class=&amp;quot;navbar-toggler-icon&amp;quot;&amp;gt;&amp;lt;/span&amp;gt;
    &amp;lt;/button&amp;gt;
&amp;lt;/div&amp;gt;

&amp;lt;div class=&amp;quot;@NavMenuCssClass&amp;quot; @onclick=&amp;quot;ToggleNavMenu&amp;quot;&amp;gt;
    &amp;lt;ul class=&amp;quot;nav flex-column&amp;quot;&amp;gt;
        &amp;lt;li class=&amp;quot;nav-item px-3&amp;quot;&amp;gt;
            &amp;lt;NavLink class=&amp;quot;nav-link&amp;quot; href=&amp;quot;&amp;quot; Match=&amp;quot;NavLinkMatch.All&amp;quot;&amp;gt;
                &amp;lt;span class=&amp;quot;oi oi-home&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/span&amp;gt; Home
            &amp;lt;/NavLink&amp;gt;
        &amp;lt;/li&amp;gt;
        &amp;lt;li class=&amp;quot;nav-item px-3&amp;quot;&amp;gt;
            &amp;lt;NavLink class=&amp;quot;nav-link&amp;quot; href=&amp;quot;counter&amp;quot;&amp;gt;
                &amp;lt;span class=&amp;quot;oi oi-plus&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/span&amp;gt; Counter
            &amp;lt;/NavLink&amp;gt;
        &amp;lt;/li&amp;gt;
        &amp;lt;li class=&amp;quot;nav-item px-3&amp;quot;&amp;gt;
            &amp;lt;NavLink class=&amp;quot;nav-link&amp;quot; href=&amp;quot;fetchdata&amp;quot;&amp;gt;
                &amp;lt;span class=&amp;quot;oi oi-list-rich&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/span&amp;gt; Fetch data
            &amp;lt;/NavLink&amp;gt;
        &amp;lt;/li&amp;gt;
        &amp;lt;li class=&amp;quot;nav-item px-3&amp;quot;&amp;gt;
            &amp;lt;NavLink class=&amp;quot;nav-link&amp;quot; href=&amp;quot;prediction&amp;quot;&amp;gt;
                &amp;lt;span class=&amp;quot;oi oi-list-rich&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/span&amp;gt; Prediction
            &amp;lt;/NavLink&amp;gt;
        &amp;lt;/li&amp;gt;
    &amp;lt;/ul&amp;gt;
&amp;lt;/div&amp;gt;

@code {
    private bool collapseNavMenu = true;

    private string NavMenuCssClass =&amp;gt; collapseNavMenu ? &amp;quot;collapse&amp;quot; : null;

    private void ToggleNavMenu()
    {
        collapseNavMenu = !collapseNavMenu;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Configure the web application&lt;/h2&gt;
&lt;p&gt;The web application will he hosted as a static site on Azure Storage.&lt;/p&gt;
&lt;p&gt;In the Azure Portal, navigate to the storage account resource where you are hosting your model.&lt;/p&gt;
&lt;h3&gt;Enable static website&lt;/h3&gt;
&lt;p&gt;Enable a static website for the storage account and set the index document name and error document path to &lt;em&gt;index.html&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly4.png" class="img-fluid" alt="Enable Static Website" /&gt;&lt;/p&gt;
&lt;p&gt;At this point, a new container called &lt;em&gt;$web&lt;/em&gt; is created in your storage account. This is where all your site's static files will reside. Additionally, a primary endpoint is created. This is the URL you will use to access your application&lt;/p&gt;
&lt;h3&gt;Configure CORS&lt;/h3&gt;
&lt;p&gt;The storage account has some default CORS settings. In order to download and use your model from your application, you'll have to configure them.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly5.png" class="img-fluid" alt="Configure CORS" /&gt;&lt;/p&gt;
&lt;p&gt;For &amp;quot;Allowed origins&amp;quot;, enter your primary endpoint.&lt;/p&gt;
&lt;h2&gt;Publish and deploy the web application&lt;/h2&gt;
&lt;p&gt;To publish your application, run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet publish -c Release
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This generates all the files you'll need to host your web application as a static site in the &lt;em&gt;bin/Release/netstandard2.1/publish/BlazorWebApp/dist&lt;/em&gt; directory of your &lt;em&gt;BlazorWebApp&lt;/em&gt; project.&lt;/p&gt;
&lt;p&gt;To deploy your application, use Azure Storage Explorer to copy all of the files in the &lt;em&gt;dist&lt;/em&gt; directory into the &lt;em&gt;$web&lt;/em&gt; container of your Azure Storage Account.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly6.png" class="img-fluid" alt="Copy files into web container" /&gt;&lt;/p&gt;
&lt;h2&gt;Test the application&lt;/h2&gt;
&lt;p&gt;In your browser, navigate to your static website's primary endpoint and select the Prediction page. Input data and click &amp;quot;Make prediction&amp;quot;. The page should look as follows.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly7.png" class="img-fluid" alt="Test the application" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You may note that performance of Naive Bayes on this dataset is not the best so some of the predictions may not be as accurate. I am okay with this at the moment because this is a proof-of-concept to show how these technologies might work together. Perhaps using a better dataset may yield better results.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I went over how to deploy an ML.NET multiclass classification model alongside a Blazor WebAssembly static website to Azure Storage. While more limited than other methods of deployment due to the early stages of WebAssembly and Blazor WebAssembly, this shows the possibilities for the technologies. Deploying in this manner reduces the amount of resources required to deploy these models and shifts processing from a server or web service to the client's browser making deployment and distribution of machine learning models more efficient, scalable and cost-efficient.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/deploy-machine-learning-mlnet-models-blazor-webassembly?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/deploy-machine-learning-mlnet-models-blazor-webassembly</guid>
      <pubDate>2020-03-01 19:21:04</pubDate>
      <category>#azure</category>
      <category>#staticwebsites</category>
      <category>#machinelearning</category>
      <category>#ai</category>
      <category>#artifificalintelligence</category>
      <category>#ml</category>
      <category>#dotnet</category>
      <category>#dotnetcore</category>
      <category>#webassembly</category>
      <category>#webdevelopment</category>
      <category>#mldotnet</category>
      <category>#mlnet</category>
      <category>#blazor</category>
    </item>
    <item>
      <title>Use machine learning to categorize web links with F# and ML.NET</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/categorize-web-links-ml-net-fsharp-fsadvent2019?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/categorize-web-links-ml-net-fsharp-fsadvent2019&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This post is part of &lt;a href="https://sergeytihon.com/2019/11/05/f-advent-calendar-in-english-2019/"&gt;F# Advent 2019&lt;/a&gt;. Thank you &lt;a href="https://twitter.com/sergey_tihon"&gt;Sergey Tihon&lt;/a&gt; for organizing this and the rest of the contributors for producing interesting, high-quality content.&lt;/p&gt;
&lt;p&gt;I scan and read articles on a constant basis, such as those published as part of F# Advent. Those that I find interesting, or I want to save for later, I bookmark using &lt;a href="https://getpocket.com/"&gt;Pocket&lt;/a&gt;. One of the neat features it provides is tagging. You can add as many tags as you want to organize the bookmarked links. When I first started using the service, I was fairly good at adding tags. However, I've gotten lazy and don't do it as much. It would be nice if bookmarked links could automatically be categorized for me without having to provide the tags manually. Using machine learning, this task can be automated. In this writeup, I will show how to build a machine learning model using &lt;a href="https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet"&gt;ML.NET&lt;/a&gt;, a .NET, open-source, cross-platform machine learning framework to automatically categorize web links / articles.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;This application was built on a Windows 10 PC, but should work cross-platform.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://dotnet.microsoft.com/download"&gt;.NET Core SDK 2.1+&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://code.visualstudio.com/Download"&gt;Visual Studio Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ionide.io/"&gt;Ionide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nuget.org/packages/Microsoft.ML/"&gt;Microsoft.ML NuGet package&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Create the solution&lt;/h2&gt;
&lt;p&gt;Make a new directory and create a solution by using the .NET CLI.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;mkdir FsAdvent2019
cd FsAdvent2019
dotnet new sln
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, create an F# Console application.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;dotnet new console -o FsAdvent2019 -lang f#
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Navigate to the console application directory and install the Microsoft.ML NuGet package.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;cd FsAdvent2019
dotnet add package Microsoft.ML -v 1.4.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Get the data&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip"&gt;Click on this link to download and unzip the data anywhere on your PC&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The data contains information about several articles that are separated into four categories: business (b), science and technology (t), entertainment (e) and health (h). Visit the &lt;a href="https://archive.ics.uci.edu/ml/datasets/News+Aggregator"&gt;UCI Machine Learning repository website&lt;/a&gt; to learn more about the dataset.&lt;/p&gt;
&lt;p&gt;Below is a sample of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;ID    Title    Url    Publisher    Category    Story    Hostname    Timestamp
2	Fed's Charles Plosser sees high bar for change in pace of tapering	http://www.livemint.com/Politics/H2EvwJSK2VE6OF7iK1g3PP/Feds-Charles-Plosser-sees-high-bar-for-change-in-pace-of-ta.html	Livemint	b	ddUyU0VZz0BRneMioxUPQVP6sIxvM	www.livemint.com	1394470371207
3	US open: Stocks fall after Fed official hints at accelerated tapering	http://www.ifamagazine.com/news/us-open-stocks-fall-after-fed-official-hints-at-accelerated-tapering-294436	IFA Magazine	b	ddUyU0VZz0BRneMioxUPQVP6sIxvM	www.ifamagazine.com	1394470371550
4	Fed risks falling 'behind the curve', Charles Plosser says	http://www.ifamagazine.com/news/fed-risks-falling-behind-the-curve-charles-plosser-says-294430	IFA Magazine	b	ddUyU0VZz0BRneMioxUPQVP6sIxvM	www.ifamagazine.com	1394470371793
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the console application directory, create a new directory called &lt;em&gt;data&lt;/em&gt; and copy the &lt;em&gt;newsCorpora.csv&lt;/em&gt; file to it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;mkdir data
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Define the schema&lt;/h2&gt;
&lt;p&gt;Open the &lt;em&gt;Program.fs&lt;/em&gt; file and add the following &lt;code&gt;open&lt;/code&gt; statements at the top.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;open Microsoft.ML
open Microsoft.ML.Data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Directly below the &lt;code&gt;open&lt;/code&gt; statements, define the data schema of the input and output of the machine learning model as records called &lt;code&gt;ModelInput&lt;/code&gt; and &lt;code&gt;ModelOutput&lt;/code&gt; respectively.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;[&amp;lt;CLIMutable&amp;gt;]
type ModelInput = {
    [&amp;lt;LoadColumn(1)&amp;gt;]
    Title:string
    [&amp;lt;LoadColumn(2)&amp;gt;]
    Url:string
    [&amp;lt;LoadColumn(3)&amp;gt;]
    Publisher:string
    [&amp;lt;LoadColumn(4)&amp;gt;]
    Category:string
    [&amp;lt;LoadColumn(6)&amp;gt;]
    Hostname:string
}

[&amp;lt;CLIMutable&amp;gt;]
type ModelOutput = {
    PredictedLabel: string
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As input, only the &lt;strong&gt;Title&lt;/strong&gt;, &lt;strong&gt;Url&lt;/strong&gt;, &lt;strong&gt;Publisher&lt;/strong&gt; and &lt;strong&gt;Hostname&lt;/strong&gt; columns are used to train the machine learning model and make predictions. The label or value to predict in this case is the &lt;strong&gt;Category&lt;/strong&gt;. When a prediction is output by the model, its value is stored in a column called &lt;strong&gt;PredictedLabel&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Create the application entry point&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;MLContext&lt;/code&gt; is the entry point of all ML.NET applications which binds all tasks like data loading, data transformations, model training, model evaluation, and model saving/loading.&lt;/p&gt;
&lt;p&gt;Inside of the &lt;code&gt;main&lt;/code&gt; function, create an instance of &lt;code&gt;MLContext&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let mlContext = MLContext()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Load the data&lt;/h2&gt;
&lt;p&gt;Once the &lt;code&gt;MLContext&lt;/code&gt; is initialized, use the &lt;code&gt;LoadFromTextFile&lt;/code&gt; function and provide the path to the file containing the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let data = mlContext.Data.LoadFromTextFile&amp;lt;ModelInput&amp;gt;(&amp;quot;data/newsCorpora.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Create training and test datasets&lt;/h2&gt;
&lt;p&gt;It's often good practice to split the data into train and test sets. The goal of a machine learning model is to accurately make predictions on data it has not seen before. Therefore, making predictions using inputs that are the same as those it was trained on may provide misleading accuracy metrics.&lt;/p&gt;
&lt;p&gt;Use the &lt;code&gt;TrainTestSplit&lt;/code&gt; to split the data into train / test sets with 90% of the data used for training and 10% used for testing.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let datasets = mlContext.Data.TrainTestSplit(data,testFraction=0.1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Define the transformation and algorithm pipelines&lt;/h2&gt;
&lt;p&gt;Now that the data is split, define the set of transformations to be applied to the data. The purpose of transforming the data is to convert it into numbers which are more easily processed by machine learning algorithms.&lt;/p&gt;
&lt;h3&gt;Preprocessing pipeline&lt;/h3&gt;
&lt;p&gt;The preprocessing pipeline contains the series of transformations that take place before training the model. To create a pipeline, initialize an &lt;code&gt;EstimatorChain&lt;/code&gt; and append the desired transformations to it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let preProcessingPipeline = 
    EstimatorChain()
        .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedTitle&amp;quot;,&amp;quot;Title&amp;quot;))
        .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedUrl&amp;quot;,&amp;quot;Url&amp;quot;))
        .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedPublisher&amp;quot;,&amp;quot;Publisher&amp;quot;))
        .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedHost&amp;quot;,&amp;quot;Hostname&amp;quot;))
        .Append(mlContext.Transforms.Concatenate(&amp;quot;Features&amp;quot;,[|&amp;quot;FeaturizedTitle&amp;quot;; &amp;quot;FeaturizedUrl&amp;quot; ;&amp;quot;FeaturizedPublisher&amp;quot;; &amp;quot;FeaturizedHost&amp;quot;|]))
        .Append(mlContext.Transforms.Conversion.MapValueToKey(&amp;quot;Label&amp;quot;,&amp;quot;Category&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this preprocessing pipeline, the following transformations are taking place:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Convert the &lt;em&gt;Title&lt;/em&gt;, &lt;em&gt;Url&lt;/em&gt;, &lt;em&gt;Publisher&lt;/em&gt; and &lt;em&gt;Hostname&lt;/em&gt; columns into numbers and store the transformed value into the &lt;em&gt;FeaturizedTitle&lt;/em&gt;, &lt;em&gt;FeaturizedUrl&lt;/em&gt;, &lt;em&gt;FeaturizedPublisher&lt;/em&gt; and &lt;em&gt;FeaturizedHost&lt;/em&gt; columns respectively.&lt;/li&gt;
&lt;li&gt;Combine the &lt;em&gt;FeaturizedTitle&lt;/em&gt;, &lt;em&gt;FeaturizedUrl&lt;/em&gt;, &lt;em&gt;FeaturizedPublisher&lt;/em&gt; and &lt;em&gt;FeaturizedHost&lt;/em&gt; into one column called &lt;em&gt;Features&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Create a mapping of the text value contained in the &lt;em&gt;Category&lt;/em&gt; column to a numerical key and store the result into a new column called &lt;em&gt;Label&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Algorithm pipeline&lt;/h3&gt;
&lt;p&gt;The algorithm pipeline contains the algorithm used to train the machine learning model. In this application, the multiclass classification algorithm used is &lt;code&gt;LbfgsMaximumEntropy&lt;/code&gt;. To learn more about the algorithm, see the &lt;a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.trainers.lbfgsmaximumentropymulticlasstrainer?view=ml-dotnet#training-algorithm-details"&gt;ML.NET LbfgsMaximumEntropy multiclass trainer API documentation&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let algorithm = 
    mlContext.MulticlassClassification.Trainers.LbfgsMaximumEntropy()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Postprocessing pipeline&lt;/h3&gt;
&lt;p&gt;The postprocessing pipeline contains the series of transformations to get the output of training into a more readable format. The only transformation performed in this pipeline is mapping back the numerical value mapping of the predicted value into text form.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let postProcessingPipeline = 
    mlContext.Transforms.Conversion.MapKeyToValue(&amp;quot;PredictedLabel&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create training pipeline&lt;/h3&gt;
&lt;p&gt;Once the pipelines are defined, combine them into a single pipeline which applies all of the transformations to the data with a single function call.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let trainingPipeline = 
    preProcessingPipeline
        .Append(algorithm)
        .Append(postProcessingPipeline)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Train the model&lt;/h2&gt;
&lt;p&gt;Use the &lt;code&gt;Fit&lt;/code&gt; function to train the model by applying the set of transformations defined by &lt;code&gt;trainingPipeline&lt;/code&gt; to the training dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let model =
    datasets.TrainSet |&amp;gt; trainingPipeline.Fit
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Evaluate the model&lt;/h2&gt;
&lt;p&gt;Once the model is trained, evaluate how well it performs against the test dataset. First, use the trained model to get the predicted category by using the &lt;code&gt;Transform&lt;/code&gt; function. Then, provide the test dataset containing predictions to the &lt;code&gt;Evaluate&lt;/code&gt; function which calculates the model's performance metrics by comparing the predicted category to the actual category and print some of them out.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let metrics = 
    (datasets.TestSet |&amp;gt; model.Transform)
    |&amp;gt; mlContext.MulticlassClassification.Evaluate

printfn &amp;quot;Log Loss: %f | MacroAccuracy: %f&amp;quot; metrics.LogLoss metrics.MacroAccuracy
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Using the model on real data&lt;/h2&gt;
&lt;p&gt;Create a list of &lt;code&gt;ModelInput&lt;/code&gt; items and use the &lt;code&gt;Transform&lt;/code&gt; method to get the predicted category.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let predictions = 
    [
        { 
            Title=&amp;quot;A FIRST LOOK AT SURFACE DUO, MICROSOFT’S FOLDABLE ANDROID PHONE&amp;quot;
            Url=&amp;quot;https://www.theverge.com/2019/10/3/20895268/microsoft-surface-duo-foldable-phone-dual-screen-android-hands-on-features-price-photos-video&amp;quot;
            Publisher=&amp;quot;The Verge&amp;quot;
            Hostname=&amp;quot;www.theverge.com&amp;quot; 
            Category = &amp;quot;&amp;quot; 
        }
        { 
            Title=&amp;quot;This Shrinking Economy With Low Inflation Is Stuck on Rates&amp;quot;
            Url=&amp;quot;https://www.bloomberg.com/news/articles/2019-12-12/when-a-shrinking-economy-and-low-inflation-don-t-mean-rate-cuts?srnd=economics-vp&amp;quot;
            Publisher=&amp;quot;Bloomberg&amp;quot;
            Hostname=&amp;quot;www.bloomberg.com&amp;quot; 
            Category = &amp;quot;&amp;quot; 
        }
    ] 
    |&amp;gt; mlContext.Data.LoadFromEnumerable
    |&amp;gt; model.Transform
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, create a &lt;code&gt;Sequence&lt;/code&gt; of &lt;code&gt;ModelOutput&lt;/code&gt; values and print out the &lt;em&gt;PredictedLabel&lt;/em&gt; values.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;mlContext.Data.CreateEnumerable&amp;lt;ModelOutput&amp;gt;(predictions,false)
|&amp;gt; Seq.iter(fun prediction -&amp;gt; printfn &amp;quot;Predicted Value: %s&amp;quot; prediction.PredictedLabel)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final &lt;em&gt;Program.fs&lt;/em&gt; file should look as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;open System
open Microsoft.ML
open Microsoft.ML.Data

[&amp;lt;CLIMutable&amp;gt;]
type ModelInput = {
    [&amp;lt;LoadColumn(1)&amp;gt;]
    Title:string
    [&amp;lt;LoadColumn(2)&amp;gt;]
    Url:string
    [&amp;lt;LoadColumn(3)&amp;gt;]
    Publisher:string
    [&amp;lt;LoadColumn(4)&amp;gt;]
    Category:string
    [&amp;lt;LoadColumn(6)&amp;gt;]
    Hostname:string
}

[&amp;lt;CLIMutable&amp;gt;]
type ModelOutput = {
    PredictedLabel: string
}

[&amp;lt;EntryPoint&amp;gt;]
let main argv =

    let mlContext = MLContext()

    let data = mlContext.Data.LoadFromTextFile&amp;lt;ModelInput&amp;gt;(&amp;quot;data/newsCorpora.csv&amp;quot;)

    let datasets = mlContext.Data.TrainTestSplit(data,testFraction=0.1)

    let preProcessingPipeline = 
        EstimatorChain()
            .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedTitle&amp;quot;,&amp;quot;Title&amp;quot;))
            .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedUrl&amp;quot;,&amp;quot;Url&amp;quot;))
            .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedPublisher&amp;quot;,&amp;quot;Publisher&amp;quot;))
            .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedHost&amp;quot;,&amp;quot;Hostname&amp;quot;))
            .Append(mlContext.Transforms.Concatenate(&amp;quot;Features&amp;quot;,[|&amp;quot;FeaturizedTitle&amp;quot;; &amp;quot;FeaturizedUrl&amp;quot; ;&amp;quot;FeaturizedPublisher&amp;quot;; &amp;quot;FeaturizedHost&amp;quot;|]))
            .Append(mlContext.Transforms.Conversion.MapValueToKey(&amp;quot;Label&amp;quot;,&amp;quot;Category&amp;quot;))

    let algorithm = 
        mlContext.MulticlassClassification.Trainers.LbfgsMaximumEntropy()

    let postProcessingPipeline = 
        mlContext.Transforms.Conversion.MapKeyToValue(&amp;quot;PredictedLabel&amp;quot;)

    let trainingPipeline = 
        preProcessingPipeline
            .Append(algorithm)
            .Append(postProcessingPipeline)

    let model =
        datasets.TrainSet |&amp;gt; trainingPipeline.Fit

    let metrics = 
        (datasets.TestSet |&amp;gt; model.Transform)
        |&amp;gt; mlContext.MulticlassClassification.Evaluate

    printfn &amp;quot;Log Loss: %f | MacroAccuracy: %f&amp;quot; metrics.LogLoss metrics.MacroAccuracy

    let predictions = 
        [
            { 
                Title=&amp;quot;A FIRST LOOK AT SURFACE DUO, MICROSOFT’S FOLDABLE ANDROID PHONE&amp;quot;
                Url=&amp;quot;https://www.theverge.com/2019/10/3/20895268/microsoft-surface-duo-foldable-phone-dual-screen-android-hands-on-features-price-photos-video&amp;quot;
                Publisher=&amp;quot;The Verge&amp;quot;
                Hostname=&amp;quot;www.theverge.com&amp;quot; 
                Category = &amp;quot;&amp;quot; 
            }
            { 
                Title=&amp;quot;This Shrinking Economy With Low Inflation Is Stuck on Rates&amp;quot;
                Url=&amp;quot;https://www.bloomberg.com/news/articles/2019-12-12/when-a-shrinking-economy-and-low-inflation-don-t-mean-rate-cuts?srnd=economics-vp&amp;quot;
                Publisher=&amp;quot;Bloomberg&amp;quot;
                Hostname=&amp;quot;www.bloomberg.com&amp;quot; 
                Category = &amp;quot;&amp;quot; 
            }
        ] 
        |&amp;gt; mlContext.Data.LoadFromEnumerable
        |&amp;gt; model.Transform
 
    mlContext.Data.CreateEnumerable&amp;lt;ModelOutput&amp;gt;(predictions,false)
    |&amp;gt; Seq.iter(fun prediction -&amp;gt; printfn &amp;quot;Predicted Value: %s&amp;quot; prediction.PredictedLabel)

    0 // return an integer exit code
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Run the application&lt;/h2&gt;
&lt;p&gt;This particular model achieved a macro-accuracy of 0.92, where closer to 1 is preferred and log loss of 0.20 where closer to 0 is preferred.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;Log Loss: 0.200502 | MacroAccuracy: 0.927742
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The predicted values are the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;Predicted Value: t
Predicted Value: b
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Upon inspection, they appear to be correct, science and technology for the first link and business for the second link.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this writeup, I showed how to build a machine learning multiclass classification model that categorizes web links using ML.NET. Now that you have a model trained, you can save it and deploy it in another application (desktop, web) that bookmarks links. This model can be further improved and personalized by using data from Pocket which has already been tagged. Happy coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/categorize-web-links-ml-net-fsharp-fsadvent2019?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/categorize-web-links-ml-net-fsharp-fsadvent2019</guid>
      <pubDate>2019-12-17 19:59:06</pubDate>
      <category>#dotnet</category>
      <category>#dotnet-core</category>
      <category>#machine-learning</category>
      <category>#ai</category>
      <category>#artificial-intelligence</category>
      <category>#ml</category>
      <category>#fsharp</category>
      <category>#functional-programming</category>
    </item>
    <item>
      <title>Create an HTTP Trigger Azure Function using FSharp</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/http-trigger-azure-functions-fsharp?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/http-trigger-azure-functions-fsharp&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Since the upgrade to the 2.0 version of the Azure Functions runtime, .NET Core has been natively supported by the platform. As a result some changes took effect. Most notably, in version 1.0, a template for an F# HttpTrigger function was available. The template was removed in 2.0. However, that does not mean Azure Functions does not support F#. Azure Functions can be built in F# using a .NET Standard Class Library. This writeup provides a detailed walk-through of how to build an Azure Function that processes HTTP requests using F#. The complete code sample can be found on &lt;a href="https://github.com/lqdev/FsHttpTriggerSample"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;This solution was built using a Windows PC but should work on Mac and Linux.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://dotnet.microsoft.com/download/dotnet-core"&gt;.NET SDK (2.x or 3.x)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nodejs.org/en/download/"&gt;Node.js&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local#windows-npm"&gt;Azure Functions Core Tools&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Create Solution&lt;/h2&gt;
&lt;p&gt;Open the command prompt and create a new directory for your solution called &amp;quot;FsHttpTriggerSample&amp;quot;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir FsHttpTriggerSample
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Navigate into the new directory and create a solution using the .NET CLI.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;cd FsHttpTriggerSample
dotnet new sln
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Create Azure Functions Project&lt;/h2&gt;
&lt;p&gt;Inside the &lt;em&gt;FsHttpTriggerSample&lt;/em&gt; solution directory, use the .NET CLI to create a new F# .NET Standard Class Library project.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new classlib -o FsHttpTriggerSample -lang f#
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the project to the solution&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet sln add FsHttpTriggerSample
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Install NuGet Packages&lt;/h2&gt;
&lt;p&gt;To use Azure Functions, install the &lt;a href="https://www.nuget.org/packages/Microsoft.NET.Sdk.Functions"&gt;&lt;strong&gt;Microsoft.Net.Sdk.Functions&lt;/strong&gt; NuGet package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Inside the &lt;em&gt;FsHttpTriggerSample&lt;/em&gt; project directory, enter the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet add package Microsoft.Net.Sdk.Functions
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Create the Azure Function&lt;/h2&gt;
&lt;h3&gt;Prepare Files&lt;/h3&gt;
&lt;p&gt;Delete the default &lt;em&gt;Library.fs&lt;/em&gt; file inside the &lt;em&gt;FsHttpTriggerSample&lt;/em&gt; project directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;del Library.fs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a new file called &lt;em&gt;GreetFunction.fs&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;type nul &amp;gt; GreetFunction.fs
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Configure Files&lt;/h3&gt;
&lt;p&gt;Open the &lt;em&gt;FsHttpTriggerSample.fsproj&lt;/em&gt; file and find the following snippet.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-xml"&gt;&amp;lt;Compile Include=&amp;quot;Library.fs&amp;quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Replace the snippet with the content below.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-xml"&gt;&amp;lt;Compile Include=&amp;quot;GreetFunction.fs&amp;quot; /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Configure Host&lt;/h3&gt;
&lt;p&gt;At a minimum, Azure Functions requires the runtime version to run. This information is provided by a file called &lt;em&gt;host.json&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Create a new file called &lt;em&gt;host.json&lt;/em&gt; inside the &lt;em&gt;FsHttpTriggerSample&lt;/em&gt; project directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;type nul &amp;gt; host.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Open the &lt;em&gt;host.json&lt;/em&gt; file and add the following content&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;version&amp;quot;: &amp;quot;2.0&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Implement Azure Function&lt;/h3&gt;
&lt;p&gt;Open the &lt;em&gt;GreetFunction.fs&lt;/em&gt; file and add the namespace and module for it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;namespace FsHttpTriggerSample

module GreetFunction = 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below the module definition, add the following &lt;code&gt;open&lt;/code&gt; statements:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;open Microsoft.AspNetCore.Mvc
open Microsoft.Azure.WebJobs
open Microsoft.AspNetCore.Http
open Newtonsoft.Json
open System.IO
open Microsoft.Extensions.Logging
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define a &lt;code&gt;User&lt;/code&gt; type containing a single property called &lt;code&gt;Name&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;type User = {
    Name: string
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The entrypoint of an Azure Function is the &lt;code&gt;Run&lt;/code&gt; function. Create a function called &lt;code&gt;Run&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;[&amp;lt;FunctionName(&amp;quot;Greet&amp;quot;)&amp;gt;]
let Run ([&amp;lt;HttpTrigger(Methods=[|&amp;quot;POST&amp;quot;|])&amp;gt;] req:HttpRequest) (log:ILogger) = 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To register an Azure Function, use the &lt;code&gt;FunctionName&lt;/code&gt; attribute. In this case, the name of the function is &lt;code&gt;Greet&lt;/code&gt;. The &lt;code&gt;Run&lt;/code&gt; function takes two parameters, an &lt;code&gt;HttpRequest&lt;/code&gt; and an &lt;code&gt;ILogger&lt;/code&gt;. Since the binding used by HTTP Trigger functions is &lt;code&gt;HttpTrigger&lt;/code&gt;, the request object is annotated with the &lt;code&gt;HttpTrigger&lt;/code&gt; attribute. Options such as the accepted methods can be provided through the &lt;code&gt;HttpTrigger&lt;/code&gt; attribute. In this case, only &lt;code&gt;POST&lt;/code&gt; requests are accepted.&lt;/p&gt;
&lt;p&gt;Create an &lt;code&gt;async&lt;/code&gt; computation expression inside the &lt;code&gt;Run&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;async {

}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the &lt;code&gt;async&lt;/code&gt; expression, add logging to indicate that the function has initialized.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;&amp;quot;Running function&amp;quot;
|&amp;gt; log.LogInformation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below that, get the body of the request.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let! body = 
    new StreamReader(req.Body) 
    |&amp;gt; (fun stream -&amp;gt; stream.ReadToEndAsync()) 
    |&amp;gt; Async.AwaitTask
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, deserialized the body into an instance of &lt;code&gt;User&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let user = JsonConvert.DeserializeObject&amp;lt;User&amp;gt;(body)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Return a personalized greeting with the user's name.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;return OkObjectResult(sprintf &amp;quot;Hello %s&amp;quot; user.Name)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, use the &lt;code&gt;StartAsTask&lt;/code&gt; function to start the &lt;code&gt;async&lt;/code&gt; expression as a &lt;code&gt;Task&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;|&amp;gt; Async.StartAsTask
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once finished, the contents of the &lt;em&gt;GreetFunction.fs&lt;/em&gt; should look similar to the following.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;namespace FsHttpTriggerSample

module GreetFunction = 

    open Microsoft.AspNetCore.Mvc
    open Microsoft.Azure.WebJobs
    open Microsoft.AspNetCore.Http
    open Newtonsoft.Json
    open System.IO
    open Microsoft.Extensions.Logging

    type User = {
        Name: string
    }

    [&amp;lt;FunctionName(&amp;quot;Greet&amp;quot;)&amp;gt;]
    let Run ([&amp;lt;HttpTrigger(Methods=[|&amp;quot;POST&amp;quot;|])&amp;gt;] req:HttpRequest) (log:ILogger) = 
        async {
            &amp;quot;Runnning Function&amp;quot;
            |&amp;gt; log.LogInformation

            let! body = 
                new StreamReader(req.Body) 
                |&amp;gt; (fun stream -&amp;gt; stream.ReadToEndAsync()) 
                |&amp;gt; Async.AwaitTask

            let user = JsonConvert.DeserializeObject&amp;lt;User&amp;gt;(body)

            return OkObjectResult(sprintf &amp;quot;Hello %s&amp;quot; user.Name)
        } |&amp;gt; Async.StartAsTask
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Run the Function Locally&lt;/h2&gt;
&lt;p&gt;Build the project by using the &lt;code&gt;build&lt;/code&gt; command inside the &lt;em&gt;FsHttpTriggerSample&lt;/em&gt; project directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet build
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, navigate to the output directory&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;cd bin\Debug\netstandard2.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the Azure Functions Core Tools to start the Azure Functions host locally.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;func host start
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the host is initialized, the function is available at the following endpoint &lt;code&gt;http://localhost:7071/api/Greet&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Test the function&lt;/h2&gt;
&lt;p&gt;Using a REST client like Postman or Insomnia, make a POST request to &lt;code&gt;http://localhost:7071/api/Greet&lt;/code&gt; with the following body. Feel free to replace the name with your own.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;Name&amp;quot;: &amp;quot;Luis&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If successful, the response should look similar to the following output.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Hello Luis
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This writeup showed how to create an HTTP Trigger Azure Function using F#. Creating additional functions inside the same project is relatively trivial since the structure of the &lt;em&gt;GreetFunction.fs&lt;/em&gt; file can be copied and the logic inside the &lt;code&gt;Run&lt;/code&gt; function can be adapted to meet your requirements.&lt;/p&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-reference-fsharp"&gt;Azure Functions F# Developer Reference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-host-json"&gt;Azure Functions Host 2.x Reference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/azure-functions/deployment-zip-push"&gt;Azure Functions Zip Deployment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/http-trigger-azure-functions-fsharp?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/http-trigger-azure-functions-fsharp</guid>
      <pubDate>2019-11-16 17:33:59</pubDate>
      <category>#fsharp</category>
      <category>#serverless</category>
      <category>#azure-functions</category>
      <category>#azure</category>
      <category>#dotnet</category>
      <category>#dotnet-core</category>
      <category>#functional-programming</category>
    </item>
    <item>
      <title>Restaurant Inspections ETL &amp; Data Enrichment with Spark.NET and ML.NET Automated (Auto) ML</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/restaurant-inspections-etl-data-enrichment-spark-auto-ml-net?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/restaurant-inspections-etl-data-enrichment-spark-auto-ml-net&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Apache Spark is an open-source, distributed, general-purpose analytics engine. For years, it has been a staple in the Big Data ecosystem for batch and real-time processing on large datasets. Although native support for the platform is limited to the JVM set of languages, other languages typically used for data processing and analytics like Python and R have plugged into Spark's Interop layer to make use of its functionality. Around the Build 2019 conference, Microsoft announced Spark.NET. Spark.NET provides bindings written for the Spark Interop layer that allow you to work with components like Spark SQL and Spark Streaming inside your .NET applications. Because Spark.NET is .NET Standard 2.0 compliant, it can run operating systems like Windows, Mac and Linux. Spark.NET is an evolution of the Mobius project which provided .NET bindings for Spark.&lt;/p&gt;
&lt;p&gt;This sample takes a restaurant violation dataset from the NYC Open Data portal and processes it using Spark.NET. Then, the processed data is used to train a machine learning model that attempts to predict the grade an establishment will receive after an inspection. The model will be trained using ML.NET, an open-source, cross-platform machine learning framework. Finally, data for which no grade currently exists will be enriched using the trained model to assign an expected grade.&lt;/p&gt;
&lt;p&gt;The source code for this sample can be found in the &lt;a href="https://github.com/lqdev/RestaurantInspectionsSparkMLNET"&gt;lqdev/RestaurantInspectionsSparkMLNET
GitHub repo&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Pre-requisites&lt;/h2&gt;
&lt;p&gt;This project was built using Ubuntu 18.04 but should work on Windows and Mac devices.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://dotnet.microsoft.com/download/dotnet-core/2.1"&gt;.NET Core 2.1 SDK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.java.com/en/download/"&gt;Java 8&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://archive.apache.org/dist/spark/spark-2.4.1/"&gt;Apache Spark 2.4.1 with Hadoop 2.7&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/dotnet/spark/releases"&gt;.NET Spark Worker 0.4.0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Install Java&lt;/h3&gt;
&lt;p&gt;Since Spark runs on the JVM, you'll need Java on your PC. The minimum version required is version 8. To install and Java, enter the following command into the terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install openjdk-8-jdk openjdk-8-jre
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, make sure that the recently installed version is the default&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo update-alternatives --config java
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Download and configure Spark&lt;/h3&gt;
&lt;p&gt;Download Spark 2.4.1 with Hadoop 2.7 onto your computer. In this case, I'm placing it into my &lt;em&gt;Downloads&lt;/em&gt; folder.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://archive.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz -O ~/Downloads/spark-2.4.1-bin-hadoop2.7.tgz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Extract the contents of the recently downloaded file into the &lt;em&gt;/usr/bin/local&lt;/em&gt; directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo tar -xvf ~/Downloads/spark-2.4.1-bin-hadoop2.7.tgz --directory /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Download and configure .NET Spark Worker&lt;/h3&gt;
&lt;p&gt;Download the .NET Spark worker onto your computer. In this case, I'm placing it into the &lt;em&gt;Downloads&lt;/em&gt; folder.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://github.com/dotnet/spark/releases/download/v0.4.0/Microsoft.Spark.Worker.netcoreapp2.1.linux-x64-0.4.0.tar.gz -O ~/Downloads/Microsoft.Spark.Worker.netcoreapp2.1.linux-x64-0.4.0.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Extract the contents of the recently downloaded file into the &lt;em&gt;/usr/bin/local&lt;/em&gt; directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo tar -xvf ~/Downloads/Microsoft.Spark.Worker.netcoreapp2.1.linux-x64-0.4.0.tar.gz --directory /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, raise the permissions on the &lt;code&gt;Microsoft.Spark.Worker&lt;/code&gt; program. This is required to execute User-Defined Functions (UDF).&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo chmod +x /usr/local/bin/Microsoft.Spark.Worker-0.4.0/Microsoft.Spark.Worker
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Configure environment variables&lt;/h3&gt;
&lt;p&gt;Once you download and configure the pre-requisites, configure their locations in the system as environment variables. Open the &lt;em&gt;~/.bashrc&lt;/em&gt; file and add the following content at the end of the file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;export SPARK_PATH=/usr/local/bin/spark-2.4.1-bin-hadoop2.7
export PATH=$SPARK_PATH/bin:$PATH
export HADOOP_HOME=$SPARK_PATH
export SPARK_HOME=$SPARK_PATH
export DOTNET_WORKER_DIR=/usr/local/bin/Microsoft.Spark.Worker-0.4.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Solution description&lt;/h2&gt;
&lt;h3&gt;Understand the data&lt;/h3&gt;
&lt;p&gt;The dataset used in this solution is the &lt;a href="https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j"&gt;&lt;em&gt;DOHMH New York City Restaurant Inspection Results&lt;/em&gt;&lt;/a&gt; and comes from the NYC Open Data portal. It is updated daily and contains assigned and pending inspection results and violation citations for restaurants and college cafeterias. The dataset excludes establishments that have gone out of business. Although the dataset contains several columns, only a subset of them are used in this solution. For a detailed description of the dataset, visit the dataset website.&lt;/p&gt;
&lt;h3&gt;Understand the solution&lt;/h3&gt;
&lt;p&gt;This solution is made up of four different .NET Core applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;RestaurantInspectionsETL&lt;/em&gt;: .NET Core Console application that takes raw data and uses Spark.NET to clean and transform the data into a format that is easier to use as input for training and making predictions with a machine learning model built with ML.NET.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;RestaurantInspectionsML&lt;/em&gt;: .NET Core Class Library that defines the input and output schema of the ML.NET machine learning model. Additionally, this is where the trained model is saved to.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;RestaurantInspectionsTraining&lt;/em&gt;: .NET Core Console application that uses the graded data generated by the &lt;em&gt;RestaurantInspectionsETL&lt;/em&gt; application to train a multiclass classification machine learning model using ML.NET's Auto ML.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;RestaurantInspectionsEnrichment&lt;/em&gt;: .NET Core Console application that uses the ungraded data generated by the &lt;em&gt;RestaurantInspectionsETL&lt;/em&gt; application as input for the trained ML.NET machine learning model which predicts what grade an establishment is most likely to receive based on the violations found during inspection.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Set up the solution&lt;/h2&gt;
&lt;h3&gt;Create solution directory&lt;/h3&gt;
&lt;p&gt;Create a new directory for your projects called &lt;em&gt;RestaurantInspectionsSparkMLNET&lt;/em&gt; and navigate to it with the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir RestaurantInspectionsSparkMLNET &amp;amp;&amp;amp; cd RestaurantInspectionsSparkMLNET
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, create a solution using the &lt;code&gt;dotnet cli&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new sln
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To ensure that the 2.1 version of the .NET Core SDK is used as the target framework, especially if you have multiple versions of the .NET SDK installed, create a file called &lt;em&gt;globals.json&lt;/em&gt; in the &lt;em&gt;RestaurantInspectionsSparkMLNET&lt;/em&gt; solution directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;touch global.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;em&gt;global.json&lt;/em&gt; file, add the following content. Make sure to use the specific version of the SDK installed on your computer. In this case, I have version &lt;code&gt;2.1.801&lt;/code&gt; installed on my computer. You can use the &lt;code&gt;dotnet --list-sdks&lt;/code&gt; command to list the installed SDK versions.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;sdk&amp;quot;: {
    &amp;quot;version&amp;quot;: &amp;quot;2.1.801&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create and configure the ETL project&lt;/h3&gt;
&lt;p&gt;The ETL project is responsible for taking the raw source data and using Spark to apply a series of transformations to prepare the data to train the machine learning model as well as to enrich data with missing grades.&lt;/p&gt;
&lt;p&gt;Inside the &lt;em&gt;RestaurantInspectionsSparkMLNET&lt;/em&gt; solution directory, create a new console application called &lt;em&gt;RestaurantInspectionsETL&lt;/em&gt; using the &lt;code&gt;dotnet cli&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new console -o RestaurantInspectionsETL
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the newly created project to the solution with the &lt;code&gt;dotnet cli&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet sln add ./RestaurantInspectionsETL/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this project uses the &lt;code&gt;Microsoft.Spark&lt;/code&gt; NuGet package, use the &lt;code&gt;dotnet cli&lt;/code&gt; to install it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet add ./RestaurantInspectionsETL/ package Microsoft.Spark --version 0.4.0
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create and configure the ML model project&lt;/h3&gt;
&lt;p&gt;The ML model class library will contain the domain model that defines the schema of model inputs and outputs as well as the trained model itself.&lt;/p&gt;
&lt;p&gt;Inside the &lt;em&gt;RestaurantInspectionsSparkMLNET&lt;/em&gt; solution directory, create a new class library called &lt;em&gt;RestaurantInspectionsML&lt;/em&gt; using the &lt;code&gt;dotnet cli&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new classlib -o RestaurantInspectionsML
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the newly created project to the solution with the &lt;code&gt;dotnet cli&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet sln add ./RestaurantInspectionsML/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this project uses the &lt;code&gt;Microsoft.ML&lt;/code&gt; NuGet package, use the &lt;code&gt;dotnet cli&lt;/code&gt; to install it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet add ./RestaurantInspectionsML/ package Microsoft.ML --version 1.3.1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create and configure the ML training project&lt;/h3&gt;
&lt;p&gt;The purpose of the training project is to use the pre-processed graded data output by the &lt;em&gt;RestaurantInspectionsETL&lt;/em&gt; project as input to train a multiclass classification model with ML.NET's Auto ML API. The trained model will then be saved in the &lt;em&gt;RestaurantInspectionsML&lt;/em&gt; directory.&lt;/p&gt;
&lt;p&gt;Inside the &lt;em&gt;RestaurantInspectionsSparkMLNET&lt;/em&gt; solution directory, create a new console application called &lt;em&gt;RestaurantInspectionsTraining&lt;/em&gt; using the &lt;code&gt;dotnet cli&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new console -o RestaurantInspectionsTraining
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the newly created project to the solution with the &lt;code&gt;dotnet cli&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet sln add ./RestaurantInspectionsTraining/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This project depends on the domain model created in the &lt;em&gt;RestaurantInspectionsML&lt;/em&gt; project, so you need to add a reference to it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet add ./RestaurantInspectionsTraining/ reference ./RestaurantInspectionsML/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this project uses the &lt;code&gt;Microsoft.Auto.ML&lt;/code&gt; NuGet package, use the &lt;code&gt;dotnet cli&lt;/code&gt; to install it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet add ./RestaurantInspectionsTraining/ package Microsoft.ML.AutoML --version 0.15.1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create and configure the data enrichment project&lt;/h3&gt;
&lt;p&gt;The data enrichment application uses the trained machine learning model created by the &lt;em&gt;RestaurantInspectionsTraining&lt;/em&gt; application and use it on the pre-processed ungraded data created by the &lt;em&gt;RestaurantInspectionsETL&lt;/em&gt; application to predict what grade that establishment is most likely to receive based on the violations found during inspection.&lt;/p&gt;
&lt;p&gt;Inside the &lt;em&gt;RestaurantInspectionsSparkMLNET&lt;/em&gt; solution directory, create a new console application called &lt;em&gt;RestaurantInspectionsEnrichment&lt;/em&gt; using the &lt;code&gt;dotnet cli&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new console -o RestaurantInspectionsEnrichment
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Add the newly created project to the solution with the &lt;code&gt;dotnet cli&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet sln add ./RestaurantInspectionsEnrichment/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This project depends on the domain model created in the &lt;em&gt;RestaurantInspectionsML&lt;/em&gt; project, so you need to add a reference to it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet add ./RestaurantInspectionsEnrichment/ reference ./RestaurantInspectionsML/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This uses the following NuGet packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Microsoft.Spark&lt;/li&gt;
&lt;li&gt;Microsoft.ML.LightGBM (This is not required but predictions may fail if the final model is a LightGBM model).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Install the packages with the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet add ./RestaurantInspectionsEnrichment/ package Microsoft.Spark --version 0.4.0
dotnet add ./RestaurantInspectionsEnrichment/ package Microsoft.ML.LightGBM --version 1.3.1
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Build ETL application&lt;/h2&gt;
&lt;p&gt;The first step is to prepare the data. To do so, apply a set of transformations using Spark.NET.&lt;/p&gt;
&lt;h3&gt;Download the data&lt;/h3&gt;
&lt;p&gt;Navigate to the &lt;em&gt;RestaurantInspectionsETL&lt;/em&gt; project and create a &lt;em&gt;Data&lt;/em&gt; directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir Data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, download the data into the newly created &lt;em&gt;Data&lt;/em&gt; directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv?accessType=DOWNLOAD -O Data/NYC-Restaurant-Inspections.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Build the ETL pipeline&lt;/h3&gt;
&lt;p&gt;Add the following usings to the &lt;em&gt;Program.cs&lt;/em&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;
using System.IO;
using Microsoft.Spark.Sql;
using static Microsoft.Spark.Sql.Functions;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not all of the columns are relevant. Inside the &lt;code&gt;Main&lt;/code&gt; method of the &lt;em&gt;Program.cs&lt;/em&gt; file, define the columns to be removed.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;string[] dropCols = new string[]
{
    &amp;quot;CAMIS&amp;quot;,
    &amp;quot;CUISINE DESCRIPTION&amp;quot;,
    &amp;quot;VIOLATION DESCRIPTION&amp;quot;,
    &amp;quot;BORO&amp;quot;,
    &amp;quot;BUILDING&amp;quot;,
    &amp;quot;STREET&amp;quot;,
    &amp;quot;ZIPCODE&amp;quot;,
    &amp;quot;PHONE&amp;quot;,
    &amp;quot;ACTION&amp;quot;,
    &amp;quot;GRADE DATE&amp;quot;,
    &amp;quot;RECORD DATE&amp;quot;,
    &amp;quot;Latitude&amp;quot;,
    &amp;quot;Longitude&amp;quot;,
    &amp;quot;Community Board&amp;quot;,
    &amp;quot;Council District&amp;quot;,
    &amp;quot;Census Tract&amp;quot;,
    &amp;quot;BIN&amp;quot;,
    &amp;quot;BBL&amp;quot;,
    &amp;quot;NTA&amp;quot;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The entrypoint of Spark applications is the &lt;code&gt;SparkSession&lt;/code&gt;. Create &lt;code&gt;SparkSession&lt;/code&gt; inside the &lt;code&gt;Main&lt;/code&gt; method of the &lt;em&gt;Program.cs&lt;/em&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var sc =
    SparkSession
        .Builder()
        .AppName(&amp;quot;Restaurant_Inspections_ETL&amp;quot;)
        .GetOrCreate();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, load the data stored in the &lt;em&gt;NYC-Restaurant-Inspections.csv&lt;/em&gt; file into a &lt;code&gt;DataFrame&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;DataFrame df =
    sc
    .Read()
    .Option(&amp;quot;header&amp;quot;, &amp;quot;true&amp;quot;)
    .Option(&amp;quot;inferSchema&amp;quot;, &amp;quot;true&amp;quot;)
    .Csv(&amp;quot;Data/NYC-Restaurant-Inspections.csv&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;DataFrames&lt;/code&gt; can be thought of as tables in a database or sheets in Excel. Spark has various ways of representing data but &lt;code&gt;DataFrames&lt;/code&gt; are the format supported by Spark.NET. Additionally, the &lt;code&gt;DataFrame&lt;/code&gt; API is higher-level and easier to work with.&lt;/p&gt;
&lt;p&gt;Once the data is loaded, get rid of the data that are not needed by creating a new &lt;code&gt;DataFrame&lt;/code&gt; that excludes the &lt;code&gt;dropCols&lt;/code&gt; as well as missing values.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;DataFrame cleanDf =
    df
        .Drop(dropCols)
        .WithColumnRenamed(&amp;quot;INSPECTION DATE&amp;quot;,&amp;quot;INSPECTIONDATE&amp;quot;)
        .WithColumnRenamed(&amp;quot;INSPECTION TYPE&amp;quot;,&amp;quot;INSPECTIONTYPE&amp;quot;)
        .WithColumnRenamed(&amp;quot;CRITICAL FLAG&amp;quot;,&amp;quot;CRITICALFLAG&amp;quot;)
        .WithColumnRenamed(&amp;quot;VIOLATION CODE&amp;quot;,&amp;quot;VIOLATIONCODE&amp;quot;)
        .Na()
        .Drop();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Typically, machine learning models expect values to be numerical, so in the ETL step try to convert as many values as possible into numerical values. The &lt;code&gt;CRITICALFLAG&lt;/code&gt; column contains &amp;quot;Y&amp;quot;/&amp;quot;N&amp;quot; values that can be encoded as 0 and 1.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;DataFrame labeledFlagDf =
    cleanDf
        .WithColumn(&amp;quot;CRITICALFLAG&amp;quot;,
            When(Functions.Col(&amp;quot;CRITICALFLAG&amp;quot;) == &amp;quot;Y&amp;quot;,1)
            .Otherwise(0));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This dataset contains one violation per row which correspond to different inspections. Therefore, all of the violations need to be aggregated by business and inspection.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;DataFrame groupedDf =
    labeledFlagDf
        .GroupBy(&amp;quot;DBA&amp;quot;, &amp;quot;INSPECTIONDATE&amp;quot;, &amp;quot;INSPECTIONTYPE&amp;quot;, &amp;quot;CRITICALFLAG&amp;quot;, &amp;quot;SCORE&amp;quot;, &amp;quot;GRADE&amp;quot;)
        .Agg(Functions.CollectSet(Functions.Col(&amp;quot;VIOLATIONCODE&amp;quot;)).Alias(&amp;quot;CODES&amp;quot;))
        .Drop(&amp;quot;DBA&amp;quot;, &amp;quot;INSPECTIONDATE&amp;quot;)
        .WithColumn(&amp;quot;CODES&amp;quot;, Functions.ArrayJoin(Functions.Col(&amp;quot;CODES&amp;quot;), &amp;quot;,&amp;quot;))
        .Select(&amp;quot;INSPECTIONTYPE&amp;quot;, &amp;quot;CODES&amp;quot;, &amp;quot;CRITICALFLAG&amp;quot;, &amp;quot;SCORE&amp;quot;, &amp;quot;GRADE&amp;quot;);  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that the data is in the format used to train and make predictions, split the cleaned &lt;code&gt;DataFrame&lt;/code&gt; into two new &lt;code&gt;DataFrames&lt;/code&gt;, graded and ungraded. The graded dataset is the data used for training the machine learning model. The ungraded data will be used for enrichment.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;DataFrame gradedDf =
    groupedDf
    .Filter(
        Col(&amp;quot;GRADE&amp;quot;) == &amp;quot;A&amp;quot; |
        Col(&amp;quot;GRADE&amp;quot;) == &amp;quot;B&amp;quot; |
        Col(&amp;quot;GRADE&amp;quot;) == &amp;quot;C&amp;quot; );

DataFrame ungradedDf =
    groupedDf
    .Filter(
        Col(&amp;quot;GRADE&amp;quot;) != &amp;quot;A&amp;quot; &amp;amp;
        Col(&amp;quot;GRADE&amp;quot;) != &amp;quot;B&amp;quot; &amp;amp;
        Col(&amp;quot;GRADE&amp;quot;) != &amp;quot;C&amp;quot; );  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Take the &lt;code&gt;DataFrames&lt;/code&gt; and save them as csv files for later use.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var timestamp = ((DateTimeOffset) DateTime.UtcNow).ToUnixTimeSeconds().ToString();

var saveDirectory = Path.Join(&amp;quot;Output&amp;quot;,timestamp);

if(!Directory.Exists(saveDirectory))
{
    Directory.CreateDirectory(saveDirectory);
}

gradedDf.Write().Csv(Path.Join(saveDirectory,&amp;quot;Graded&amp;quot;));

ungradedDf.Write().Csv(Path.Join(saveDirectory,&amp;quot;Ungraded&amp;quot;));
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Publish and run the ETL application&lt;/h3&gt;
&lt;p&gt;The final &lt;em&gt;Program.cs&lt;/em&gt; file should look as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;
using System.IO;
using Microsoft.Spark.Sql;
using static Microsoft.Spark.Sql.Functions;

namespace RestaurantInspectionsETL
{
    class Program
    {
        static void Main(string[] args)
        {
            // Define columns to remove
            string[] dropCols = new string[]
            {
                &amp;quot;CAMIS&amp;quot;,
                &amp;quot;CUISINE DESCRIPTION&amp;quot;,
                &amp;quot;VIOLATION DESCRIPTION&amp;quot;,
                &amp;quot;BORO&amp;quot;,
                &amp;quot;BUILDING&amp;quot;,
                &amp;quot;STREET&amp;quot;,
                &amp;quot;ZIPCODE&amp;quot;,
                &amp;quot;PHONE&amp;quot;,
                &amp;quot;ACTION&amp;quot;,
                &amp;quot;GRADE DATE&amp;quot;,
                &amp;quot;RECORD DATE&amp;quot;,
                &amp;quot;Latitude&amp;quot;,
                &amp;quot;Longitude&amp;quot;,
                &amp;quot;Community Board&amp;quot;,
                &amp;quot;Council District&amp;quot;,
                &amp;quot;Census Tract&amp;quot;,
                &amp;quot;BIN&amp;quot;,
                &amp;quot;BBL&amp;quot;,
                &amp;quot;NTA&amp;quot;
            };

            // Create SparkSession
            var sc =
                SparkSession
                    .Builder()
                    .AppName(&amp;quot;Restaurant_Inspections_ETL&amp;quot;)
                    .GetOrCreate();

            // Load data
            DataFrame df =
                sc
                .Read()
                .Option(&amp;quot;header&amp;quot;, &amp;quot;true&amp;quot;)
                .Option(&amp;quot;inferSchema&amp;quot;, &amp;quot;true&amp;quot;)
                .Csv(&amp;quot;Data/NYC-Restaurant-Inspections.csv&amp;quot;);

            //Remove columns and missing values
            DataFrame cleanDf =
                df
                    .Drop(dropCols)
                    .WithColumnRenamed(&amp;quot;INSPECTION DATE&amp;quot;,&amp;quot;INSPECTIONDATE&amp;quot;)
                    .WithColumnRenamed(&amp;quot;INSPECTION TYPE&amp;quot;,&amp;quot;INSPECTIONTYPE&amp;quot;)
                    .WithColumnRenamed(&amp;quot;CRITICAL FLAG&amp;quot;,&amp;quot;CRITICALFLAG&amp;quot;)
                    .WithColumnRenamed(&amp;quot;VIOLATION CODE&amp;quot;,&amp;quot;VIOLATIONCODE&amp;quot;)
                    .Na()
                    .Drop();

            // Encode CRITICAL FLAG column
            DataFrame labeledFlagDf =
                cleanDf
                    .WithColumn(&amp;quot;CRITICALFLAG&amp;quot;,
                        When(Functions.Col(&amp;quot;CRITICALFLAG&amp;quot;) == &amp;quot;Y&amp;quot;,1)
                        .Otherwise(0));

             // Aggregate violations by business and inspection
            DataFrame groupedDf =
                labeledFlagDf
                    .GroupBy(&amp;quot;DBA&amp;quot;, &amp;quot;INSPECTIONDATE&amp;quot;, &amp;quot;INSPECTIONTYPE&amp;quot;, &amp;quot;CRITICALFLAG&amp;quot;, &amp;quot;SCORE&amp;quot;, &amp;quot;GRADE&amp;quot;)
                    .Agg(Functions.CollectSet(Functions.Col(&amp;quot;VIOLATIONCODE&amp;quot;)).Alias(&amp;quot;CODES&amp;quot;))
                    .Drop(&amp;quot;DBA&amp;quot;, &amp;quot;INSPECTIONDATE&amp;quot;)
                    .WithColumn(&amp;quot;CODES&amp;quot;, Functions.ArrayJoin(Functions.Col(&amp;quot;CODES&amp;quot;), &amp;quot;,&amp;quot;))
                    .Select(&amp;quot;INSPECTIONTYPE&amp;quot;, &amp;quot;CODES&amp;quot;, &amp;quot;CRITICALFLAG&amp;quot;, &amp;quot;SCORE&amp;quot;, &amp;quot;GRADE&amp;quot;);

            // Split into graded and ungraded DataFrames
            DataFrame gradedDf =
                groupedDf
                .Filter(
                    Col(&amp;quot;GRADE&amp;quot;) == &amp;quot;A&amp;quot; |
                    Col(&amp;quot;GRADE&amp;quot;) == &amp;quot;B&amp;quot; |
                    Col(&amp;quot;GRADE&amp;quot;) == &amp;quot;C&amp;quot; );

            DataFrame ungradedDf =
                groupedDf
                    .Filter(
                        Col(&amp;quot;GRADE&amp;quot;) != &amp;quot;A&amp;quot; &amp;amp;
                        Col(&amp;quot;GRADE&amp;quot;) != &amp;quot;B&amp;quot; &amp;amp;
                        Col(&amp;quot;GRADE&amp;quot;) != &amp;quot;C&amp;quot; );

            // Save DataFrames
            var timestamp = ((DateTimeOffset) DateTime.UtcNow).ToUnixTimeSeconds().ToString();

            var saveDirectory = Path.Join(&amp;quot;Output&amp;quot;,timestamp);

            if(!Directory.Exists(saveDirectory))
            {
                Directory.CreateDirectory(saveDirectory);
            }

            gradedDf.Write().Csv(Path.Join(saveDirectory,&amp;quot;Graded&amp;quot;));

            ungradedDf.Write().Csv(Path.Join(saveDirectory,&amp;quot;Ungraded&amp;quot;));
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Publish the application with the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet publish -f netcoreapp2.1 -r ubuntu.18.04-x64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Run the application with &lt;code&gt;spark-submit&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master local bin/Debug/netcoreapp2.1/ubuntu.18.04-x64/publish/microsoft-spark-2.4.x-0.4.0.jar dotnet bin/Debug/netcoreapp2.1/ubuntu.18.04-x64/publish/RestaurantInspectionsETL.dll
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Build ML Domain&lt;/h2&gt;
&lt;h3&gt;Define the model input schema&lt;/h3&gt;
&lt;p&gt;Navigate to the &lt;em&gt;RestaurantInspectionsTraining&lt;/em&gt; project directory and create a new file called &lt;em&gt;ModelInput.cs&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;touch ModelInput.cs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Open the &lt;em&gt;ModelInput.cs&lt;/em&gt; file and add the following code.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Microsoft.ML.Data;

namespace RestaurantInspectionsML
{
    public class ModelInput
    {
        [LoadColumn(0)]
        public string InspectionType { get; set; }

        [LoadColumn(1)]
        public string Codes { get; set; }

        [LoadColumn(2)]
        public float CriticalFlag { get; set; }

        [LoadColumn(3)]
        public float InspectionScore { get; set; }

        [LoadColumn(4)]
        [ColumnName(&amp;quot;Label&amp;quot;)]
        public string Grade { get; set; }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using attributes in the schema, five properties are defined:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;InspectionType: The type of inspection performed.&lt;/li&gt;
&lt;li&gt;Codes: Violation codes found during inspection.&lt;/li&gt;
&lt;li&gt;CriticalFlag: Indicates if any of the violations during the inspection were critical (contribute to food-borne illness).&lt;/li&gt;
&lt;li&gt;InspectionScore: Score assigned after inspection.&lt;/li&gt;
&lt;li&gt;Grade: Letter grade assigned after inspection&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;LoadColumn&lt;/code&gt; attribute defines the position of the column in the file. Data in the last column is assigned to the &lt;code&gt;Grade&lt;/code&gt; property but is then referenced as &lt;code&gt;Label&lt;/code&gt; in the &lt;code&gt;IDataView&lt;/code&gt;. The reason for using the &lt;code&gt;ColumnName&lt;/code&gt; attribute is ML.NET algorithms have default column names and renaming properties at the schema class level removes the need to define the feature and label columns as parameters in the training pipeline.&lt;/p&gt;
&lt;h3&gt;Define the model output schema&lt;/h3&gt;
&lt;p&gt;In the &lt;em&gt;RestaurantInspectionsTraining&lt;/em&gt; project directory and create a new file called &lt;em&gt;ModelOutput.cs&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;touch ModelOutput.cs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Open the &lt;em&gt;ModelOutput.cs&lt;/em&gt; file and add the following code.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;namespace RestaurantInspectionsML
{
    public class ModelOutput
    {
        public float[] Scores { get; set; }
        public string PredictedLabel { get; set; }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the output schema, the &lt;code&gt;ModelOutput&lt;/code&gt; class uses properties with the default column names of the outputs generated by the model training process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scores: A float vector containing the probabilties for all the predicted classes.&lt;/li&gt;
&lt;li&gt;PredictedLabel: The value of the prediction. In this case, the &lt;code&gt;PredictedLabel&lt;/code&gt; is the predicted grade expected to be assigned after inspection given the set of features for that inspection.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Build the model training application&lt;/h2&gt;
&lt;p&gt;The application trains a multiclass classification algorithm. Finding the &amp;quot;best&amp;quot; algorithm with the right parameters requires experimentation. Fortunately, ML.NET's Auto ML does this for you given you provide it with the type of algorithm you want to train.&lt;/p&gt;
&lt;h3&gt;Load the graded data&lt;/h3&gt;
&lt;p&gt;Navigate to the &lt;em&gt;RestaurantInspectionsTraining&lt;/em&gt; project directory and add the following using statements to the &lt;em&gt;Program.cs&lt;/em&gt; class.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;
using System.IO;
using System.Linq;
using Microsoft.ML;
using static Microsoft.ML.DataOperationsCatalog;
using Microsoft.ML.AutoML;
using RestaurantInspectionsML;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the &lt;code&gt;Main&lt;/code&gt; method of the &lt;em&gt;Program.cs&lt;/em&gt; file, define the path where the data files are stored.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;string solutionDirectory = &amp;quot;/home/lqdev/Development/RestaurantInspectionsSparkMLNET&amp;quot;;
string dataLocation = Path.Combine(solutionDirectory,&amp;quot;RestaurantInspectionsETL&amp;quot;,&amp;quot;Output&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The entrypoint of an ML.NET application is the &lt;code&gt;MLContext&lt;/code&gt;. Initialize an &lt;code&gt;MLContext&lt;/code&gt; instance.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;MLContext mlContext = new MLContext();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, get the paths of the data files. The output generated by the &lt;em&gt;RestaurantInspectionsETL&lt;/em&gt; application contains both the csv files as well as files containing information about the partitions that created them. For training, only the csv files are needed.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var latestOutput =
    Directory
        .GetDirectories(dataLocation)
        .Select(directory =&amp;gt; new DirectoryInfo(directory))
        .OrderBy(directoryInfo =&amp;gt; directoryInfo.Name)
        .Select(directory =&amp;gt; Path.Join(directory.FullName,&amp;quot;Graded&amp;quot;))
        .First();

var dataFilePaths =
    Directory
        .GetFiles(latestOutput)
        .Where(file =&amp;gt; file.EndsWith(&amp;quot;csv&amp;quot;))
        .ToArray();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, load the data into an &lt;code&gt;IDataView&lt;/code&gt;. An &lt;code&gt;IDataView&lt;/code&gt; is similar to a &lt;code&gt;DataFrame&lt;/code&gt; in that it is a way to represent data as rows, columns and their schema.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var dataLoader = mlContext.Data.CreateTextLoader&amp;lt;ModelInput&amp;gt;(separatorChar:',', hasHeader:false, allowQuoting:true, trimWhitespace:true);

IDataView data = dataLoader.Load(dataFilePaths);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It's good practice to split the data into training and test sets for evaluation. Split the data into 80% training and 20% test sets.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;TrainTestData dataSplit = mlContext.Data.TrainTestSplit(data,testFraction:0.2);
IDataView trainData = dataSplit.TrainSet;
IDataView testData = dataSplit.TestSet;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create the experiment&lt;/h3&gt;
&lt;p&gt;Auto ML takes the data and runs experiments using different models and hyper-parameters in search of the &amp;quot;best&amp;quot; model. Define the settings for your experiment. In this case, the model will run for 600 seconds or 10 minutes and will try to find the model with the lowest log loss metric.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var experimentSettings = new MulticlassExperimentSettings();
experimentSettings.MaxExperimentTimeInSeconds = 600;
experimentSettings.OptimizingMetric = MulticlassClassificationMetric.LogLoss;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, create the experiment.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var experiment = mlContext.Auto().CreateMulticlassClassificationExperiment(experimentSettings);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After creating the experiment, run it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var experimentResults = experiment.Execute(data, progressHandler: new ProgressHandler());
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, running the application won’t display progress information. However, a &lt;code&gt;ProgressHandler&lt;/code&gt; object can be passed into the &lt;code&gt;Execute&lt;/code&gt; method of an experiment which calls the implemented &lt;code&gt;Report&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;Inside the &lt;em&gt;RestaurantInspectionsTraining&lt;/em&gt; project directory, create a new file called &lt;em&gt;ProgressHandler.cs&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;touch ProgressHandler.cs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, add the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;
using Microsoft.ML.Data;
using Microsoft.ML.AutoML;

namespace RestaurantInspectionsTraining
{
    public class ProgressHandler : IProgress&amp;lt;RunDetail&amp;lt;MulticlassClassificationMetrics&amp;gt;&amp;gt;
    {
        public void Report(RunDetail&amp;lt;MulticlassClassificationMetrics&amp;gt; run)
        {
            Console.WriteLine($&amp;quot;Trained {run.TrainerName} with Log Loss {run.ValidationMetrics.LogLoss:0.####} in {run.RuntimeInSeconds:0.##} seconds&amp;quot;);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ProgressHandler class derives from the &lt;code&gt;IProgress&amp;lt;T&amp;gt;&lt;/code&gt; interface which requires the implementation of the &lt;code&gt;Report&lt;/code&gt; method. The object being passed into the Report method after each run is an &lt;code&gt;RunDetail&amp;lt;MulticlassClassificationMetrics&amp;gt;&lt;/code&gt; object. Each time a run is complete, the &lt;code&gt;Report&lt;/code&gt; method is called and the code inside it executes.&lt;/p&gt;
&lt;h3&gt;Evaluate the results&lt;/h3&gt;
&lt;p&gt;Once the experiment has finished running, get the model from the best run. Add the following code to the &lt;code&gt;Main&lt;/code&gt; method of the &lt;em&gt;Program.cs&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var bestModel = experimentResults.BestRun.Model;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evaluate the performance of the model using the test dataset and measure it's Micro-Accuracy metric.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;IDataView scoredTestData = bestModel.Transform(testData);  
var metrics = mlContext.MulticlassClassification.Evaluate(scoredTestData);
Console.WriteLine($&amp;quot;MicroAccuracy: {metrics.MicroAccuracy}&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Save the trained model&lt;/h3&gt;
&lt;p&gt;Finally, save the trained model to the &lt;em&gt;RestaurantInspectionsML&lt;/em&gt; project.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;string modelSavePath = Path.Join(solutionDirectory,&amp;quot;RestaurantInspectionsML&amp;quot;,&amp;quot;model.zip&amp;quot;);
mlContext.Model.Save(bestModel, data.Schema, modelSavePath);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A file called &lt;em&gt;model.zip&lt;/em&gt; should be created inside the &lt;em&gt;RestaurantInspectionsML&lt;/em&gt; project.&lt;/p&gt;
&lt;p&gt;Make sure that the trained model file is copied to the output directory by adding the following contents to the &lt;em&gt;RestaurantInspectionsML.csproj&lt;/em&gt; file in the &lt;em&gt;RestaurantInspectionsML&lt;/em&gt; directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-xml"&gt;&amp;lt;ItemGroup&amp;gt;
  &amp;lt;None Include=&amp;quot;model.zip&amp;quot;&amp;gt;
    &amp;lt;CopyToOutputDirectory&amp;gt;PreserveNewest&amp;lt;/CopyToOutputDirectory&amp;gt;
  &amp;lt;/None&amp;gt;
&amp;lt;/ItemGroup&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Copying it to the output directory of the &lt;em&gt;RestaurantInspectionsML&lt;/em&gt; makes it easier to reference from the &lt;em&gt;RestaurantInspectionsEnrichment&lt;/em&gt; project since that project already contains a reference to the &lt;em&gt;RestaurantInspectionsML&lt;/em&gt; class library.&lt;/p&gt;
&lt;h2&gt;Train the model&lt;/h2&gt;
&lt;p&gt;The final &lt;em&gt;Program.cs&lt;/em&gt; file should look as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;
using System.IO;
using System.Linq;
using Microsoft.ML;
using static Microsoft.ML.DataOperationsCatalog;
using Microsoft.ML.AutoML;
using RestaurantInspectionsML;

namespace RestaurantInspectionsTraining
{
    class Program
    {
        static void Main(string[] args)
        {
            // Define source data directory paths
            string solutionDirectory = &amp;quot;/home/lqdev/Development/RestaurantInspectionsSparkMLNET&amp;quot;;
            string dataLocation = Path.Combine(solutionDirectory,&amp;quot;RestaurantInspectionsETL&amp;quot;,&amp;quot;Output&amp;quot;);

            // Initialize MLContext
            MLContext mlContext = new MLContext();

            // Get directory name of most recent ETL output
            var latestOutput =
                Directory
                    .GetDirectories(dataLocation)
                    .Select(directory =&amp;gt; new DirectoryInfo(directory))
                    .OrderBy(directoryInfo =&amp;gt; directoryInfo.Name)
                    .Select(directory =&amp;gt; Path.Join(directory.FullName,&amp;quot;Graded&amp;quot;))
                    .First();

            var dataFilePaths =
                Directory
                    .GetFiles(latestOutput)
                    .Where(file =&amp;gt; file.EndsWith(&amp;quot;csv&amp;quot;))
                    .ToArray();

            // Load the data
            var dataLoader = mlContext.Data.CreateTextLoader&amp;lt;ModelInput&amp;gt;(separatorChar:',', hasHeader:false, allowQuoting:true, trimWhitespace:true);
            IDataView data = dataLoader.Load(dataFilePaths);

            // Split the data
            TrainTestData dataSplit = mlContext.Data.TrainTestSplit(data,testFraction:0.2);
            IDataView trainData = dataSplit.TrainSet;
            IDataView testData = dataSplit.TestSet;

            // Define experiment settings
            var experimentSettings = new MulticlassExperimentSettings();
            experimentSettings.MaxExperimentTimeInSeconds = 600;
            experimentSettings.OptimizingMetric = MulticlassClassificationMetric.LogLoss;

            // Create experiment
            var experiment = mlContext.Auto().CreateMulticlassClassificationExperiment(experimentSettings);

            // Run experiment
            var experimentResults = experiment.Execute(data, progressHandler: new ProgressHandler());

            // Best Run Results
            var bestModel = experimentResults.BestRun.Model;

            // Evaluate Model
            IDataView scoredTestData = bestModel.Transform(testData);  
            var metrics = mlContext.MulticlassClassification.Evaluate(scoredTestData);
            Console.WriteLine($&amp;quot;MicroAccuracy: {metrics.MicroAccuracy}&amp;quot;);

            // Save Model
            string modelSavePath = Path.Join(solutionDirectory,&amp;quot;RestaurantInspectionsML&amp;quot;,&amp;quot;model.zip&amp;quot;);
            mlContext.Model.Save(bestModel, data.Schema, modelSavePath);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once all the code and configurations are complete, from the &lt;em&gt;RestaurantInspectionsTraining&lt;/em&gt; directory, run the application using the &lt;code&gt;dotnet cli&lt;/code&gt;. Remember this will take 10 minutes to run.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet run
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The console output should look similar to the output below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Trained LightGbmMulti with Log Loss 0.1547 in 1.55 seconds
Trained FastTreeOva with Log Loss 0.0405 in 65.58 seconds
Trained FastForestOva with Log Loss 0.0012 in 53.37 seconds
Trained LightGbmMulti with Log Loss 0.0021 in 4.55 seconds
Trained FastTreeOva with Log Loss 0.8315 in 5.22 seconds
MicroAccuracy: 0.999389615839469
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Build the data enrichment application&lt;/h2&gt;
&lt;p&gt;Now that the model is trained, it can be used to enrich the ungraded data.&lt;/p&gt;
&lt;h3&gt;Initialize the PredictionEngine&lt;/h3&gt;
&lt;p&gt;Navigate to the &lt;em&gt;RestaurantInspectionsEnrichment&lt;/em&gt; project directory and add the following using statements to the &lt;em&gt;Program.cs&lt;/em&gt; class.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System.IO;
using System.Linq;
using Microsoft.ML;
using Microsoft.Spark.Sql;
using static Microsoft.Spark.Sql.Functions;
using RestaurantInspectionsML;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make predictions, the model has to be loaded into the applicaton and because predictions are made one row at a time, a &lt;code&gt;PredictionEngine&lt;/code&gt; has be created as well.&lt;/p&gt;
&lt;p&gt;Inside the &lt;code&gt;Program&lt;/code&gt; class, define the &lt;code&gt;PredictionEngine&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;private static readonly PredictionEngine&amp;lt;ModelInput,ModelOutput&amp;gt; _predictionEngine;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, create a constructor to load the model and initialize it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;static Program()
{
    MLContext mlContext = new MLContext();
    ITransformer model = mlContext.Model.Load(&amp;quot;model.zip&amp;quot;,out DataViewSchema schema);
    _predictionEngine = mlContext.Model.CreatePredictionEngine&amp;lt;ModelInput,ModelOutput&amp;gt;(model);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Load the ungraded data&lt;/h3&gt;
&lt;p&gt;In the &lt;code&gt;Main&lt;/code&gt; method of the &lt;code&gt;Program&lt;/code&gt; class, define the location of the data files.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;string solutionDirectory = &amp;quot;/home/lqdev/Development/RestaurantInspectionsSparkMLNET&amp;quot;;
string dataLocation = Path.Combine(solutionDirectory,&amp;quot;RestaurantInspectionsETL&amp;quot;,&amp;quot;Output&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, get the path of the most recent ungraded data generated by the &lt;em&gt;RestaurantInspectionsETL&lt;/em&gt; application.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var latestOutput =
    Directory
        .GetDirectories(dataLocation)
        .Select(directory =&amp;gt; new DirectoryInfo(directory))
        .OrderBy(directoryInfo =&amp;gt; directoryInfo.Name)
        .Select(directory =&amp;gt; directory.FullName)
        .First();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initialize a &lt;code&gt;SparkSession&lt;/code&gt; for your enrichment application.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var sc =
    SparkSession
        .Builder()
        .AppName(&amp;quot;Restaurant_Inspections_Enrichment&amp;quot;)
        .GetOrCreate();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data generated by the &lt;em&gt;RestaurantInspectionsETL&lt;/em&gt; does not have headers. However, the schema can be defined and set when the data is loaded.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var schema = @&amp;quot;
    INSPECTIONTYPE string,
    CODES string,
    CRITICALFLAG int,
    INSPECTIONSCORE int,
    GRADE string&amp;quot;;

DataFrame df = 
    sc
    .Read()
    .Schema(schema)
    .Csv(Path.Join(latestOutput,&amp;quot;Ungraded&amp;quot;));
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Define the UDF&lt;/h3&gt;
&lt;p&gt;There is no built-in function in Spark that allows you to use a &lt;code&gt;PredictionEngine&lt;/code&gt;. However, Spark can be extended through UDFs. Keep in mind that UDFs are not optimized like the built-in functions. Therefore, whenever possible, try to use the built-in functions as much as possible.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;Program&lt;/code&gt; class, create a new method called &lt;code&gt;PredictGrade&lt;/code&gt; which takes in the set of features that make up the &lt;code&gt;ModelInput&lt;/code&gt; expected by the trained model.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public static string PredictGrade(
    string inspectionType,
    string violationCodes,
    int criticalFlag,
    int inspectionScore)
{
    ModelInput input = new ModelInput
    {
        InspectionType=inspectionType,
        Codes=violationCodes,
        CriticalFlag=(float)criticalFlag,
        InspectionScore=(float)inspectionScore
    };

    ModelOutput prediction = _predictionEngine.Predict(input);

    return prediction.PredictedLabel;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, inside the &lt;code&gt;Main&lt;/code&gt; method, register the &lt;code&gt;PredictGrade&lt;/code&gt; method as a UDF in your &lt;code&gt;SparkSession&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;sc.Udf().Register&amp;lt;string,string,int,int,string&amp;gt;(&amp;quot;PredictGrade&amp;quot;,PredictGrade);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Enrich the data&lt;/h3&gt;
&lt;p&gt;Once the UDF is registered, use it inside of a &lt;code&gt;Select&lt;/code&gt; statement which creates a new &lt;code&gt;DataFrame&lt;/code&gt; that includes the input features as well as the predicted grade output by the trained mdoel.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var enrichedDf =
    df
    .Select(
        Col(&amp;quot;INSPECTIONTYPE&amp;quot;),
        Col(&amp;quot;CODES&amp;quot;),
        Col(&amp;quot;CRITICALFLAG&amp;quot;),
        Col(&amp;quot;INSPECTIONSCORE&amp;quot;),
        CallUDF(&amp;quot;PredictGrade&amp;quot;,
            Col(&amp;quot;INSPECTIONTYPE&amp;quot;),
            Col(&amp;quot;CODES&amp;quot;),
            Col(&amp;quot;CRITICALFLAG&amp;quot;),
            Col(&amp;quot;INSPECTIONSCORE&amp;quot;)
        ).Alias(&amp;quot;PREDICTEDGRADE&amp;quot;)
    );
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, save the enriched &lt;code&gt;DataFrame&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;string outputId = new DirectoryInfo(latestOutput).Name;
string enrichedOutputPath = Path.Join(solutionDirectory,&amp;quot;RestaurantInspectionsEnrichment&amp;quot;,&amp;quot;Output&amp;quot;);
string savePath = Path.Join(enrichedOutputPath,outputId);

if(!Directory.Exists(savePath))
{
    Directory.CreateDirectory(enrichedOutputPath);
}

enrichedDf.Write().Csv(savePath);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Publish and run the enrichment application&lt;/h3&gt;
&lt;p&gt;The final &lt;em&gt;Program.cs&lt;/em&gt; file should look as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System.IO;
using System.Linq;
using Microsoft.ML;
using Microsoft.Spark.Sql;
using static Microsoft.Spark.Sql.Functions;
using RestaurantInspectionsML;

namespace RestaurantInspectionsEnrichment
{
    class Program
    {
        private static readonly PredictionEngine&amp;lt;ModelInput,ModelOutput&amp;gt; _predictionEngine;

        static Program()
        {
            MLContext mlContext = new MLContext();
            ITransformer model = mlContext.Model.Load(&amp;quot;model.zip&amp;quot;,out DataViewSchema schema);
            _predictionEngine = mlContext.Model.CreatePredictionEngine&amp;lt;ModelInput,ModelOutput&amp;gt;(model);
        }

        static void Main(string[] args)
        {
            // Define source data directory paths
            string solutionDirectory = &amp;quot;/home/lqdev/Development/RestaurantInspectionsSparkMLNET&amp;quot;;
            string dataLocation = Path.Combine(solutionDirectory,&amp;quot;RestaurantInspectionsETL&amp;quot;,&amp;quot;Output&amp;quot;);

            var latestOutput = 
                Directory
                    .GetDirectories(dataLocation)
                    .Select(directory =&amp;gt; new DirectoryInfo(directory))
                    .OrderBy(directoryInfo =&amp;gt; directoryInfo.Name)
                    .Select(directory =&amp;gt; directory.FullName)
                    .First();

            var sc = 
                SparkSession
                    .Builder()
                    .AppName(&amp;quot;Restaurant_Inspections_Enrichment&amp;quot;)
                    .GetOrCreate();

            var schema = @&amp;quot;
                INSPECTIONTYPE string,
                CODES string,
                CRITICALFLAG int,
                INSPECTIONSCORE int,
                GRADE string&amp;quot;;

            DataFrame df = 
                sc
                .Read()
                .Schema(schema)
                .Csv(Path.Join(latestOutput,&amp;quot;Ungraded&amp;quot;));

            sc.Udf().Register&amp;lt;string,string,int,int,string&amp;gt;(&amp;quot;PredictGrade&amp;quot;,PredictGrade);

            var enrichedDf = 
                df
                .Select(
                    Col(&amp;quot;INSPECTIONTYPE&amp;quot;),
                    Col(&amp;quot;CODES&amp;quot;),
                    Col(&amp;quot;CRITICALFLAG&amp;quot;),
                    Col(&amp;quot;INSPECTIONSCORE&amp;quot;),
                    CallUDF(&amp;quot;PredictGrade&amp;quot;,
                        Col(&amp;quot;INSPECTIONTYPE&amp;quot;),
                        Col(&amp;quot;CODES&amp;quot;),
                        Col(&amp;quot;CRITICALFLAG&amp;quot;),
                        Col(&amp;quot;INSPECTIONSCORE&amp;quot;)
                    ).Alias(&amp;quot;PREDICTEDGRADE&amp;quot;)
                );

            string outputId = new DirectoryInfo(latestOutput).Name;
            string enrichedOutputPath = Path.Join(solutionDirectory,&amp;quot;RestaurantInspectionsEnrichment&amp;quot;,&amp;quot;Output&amp;quot;);
            string savePath = Path.Join(enrichedOutputPath,outputId);

            if(!Directory.Exists(savePath))
            {
                Directory.CreateDirectory(enrichedOutputPath);
            }

            enrichedDf.Write().Csv(savePath);

        }

        public static string PredictGrade(
            string inspectionType,
            string violationCodes,
            int criticalFlag,
            int inspectionScore)
        {
            ModelInput input = new ModelInput
            {
                InspectionType=inspectionType,
                Codes=violationCodes,
                CriticalFlag=(float)criticalFlag,
                InspectionScore=(float)inspectionScore
            };

            ModelOutput prediction = _predictionEngine.Predict(input);

            return prediction.PredictedLabel;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the &lt;em&gt;RestaurantInspectionsEnrichment&lt;/em&gt; project publish the application with the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet publish -f netcoreapp2.1 -r ubuntu.18.04-x64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Navigate to the &lt;em&gt;publish&lt;/em&gt; directory. In this case, it's &lt;em&gt;bin/Debug/netcoreapp2.1/ubuntu.18.04-x64/publish&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;From the &lt;em&gt;publish&lt;/em&gt; directory, run the application with &lt;code&gt;spark-submit&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master local microsoft-spark-2.4.x-0.4.0.jar dotnet RestaurantInspectionsEnrichment.dll
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The file output should look similar to the contents below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Cycle Inspection / Initial Inspection,04N,1,13,A
Cycle Inspection / Re-inspection,08A,0,9,A
Cycle Inspection / Initial Inspection,&amp;quot;10B,10H&amp;quot;,0,10,A
Cycle Inspection / Initial Inspection,10F,0,10,A
Cycle Inspection / Reopening Inspection,10F,0,3,C
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This solution showcased how Spark can be used within .NET applications with Spark.NET. Because it's part of the .NET ecosystem, other components and frameworks such as ML.NET can be leveraged to extend the system's capabilities. Although this sample was developed and run on a local, single-node cluster, Spark was made to run at scale. As such, this application can be further improved by setting up a cluster and running the ETL and enrichment workloads on there.&lt;/p&gt;
&lt;h6&gt;Resources&lt;/h6&gt;
&lt;p&gt;&lt;a href="https://spark.apache.org/"&gt;Apache Spark&lt;/a&gt;
&lt;a href="https://dotnet.microsoft.com/apps/data/spark"&gt;Spark.NET&lt;/a&gt;
&lt;a href="https://github.com/dotnet/spark"&gt;Spark.NET GitHub&lt;/a&gt;
&lt;a href="https://github.com/Microsoft/Mobius"&gt;Mobius&lt;/a&gt;
&lt;a href="https://opendata.cityofnewyork.us/"&gt;NYC OpenData&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/restaurant-inspections-etl-data-enrichment-spark-auto-ml-net?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/restaurant-inspections-etl-data-enrichment-spark-auto-ml-net</guid>
      <pubDate>2019-09-15 15:12:01</pubDate>
      <category>#mlnet</category>
      <category>#ml</category>
      <category>#machine-learning</category>
      <category>#ai</category>
      <category>#artificial-intelligence</category>
      <category>#big-data</category>
      <category>#spark</category>
      <category>#dotnet</category>
      <category>#dotnetcore</category>
    </item>
    <item>
      <title>Machine Learning Lifecycle Management with ML.NET, Automated ML and MLFlow</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/ml-lifecycle-management-mlflow-automated-ml-net?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/ml-lifecycle-management-mlflow-automated-ml-net&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As machine learning matures, best practices are starting to be adopted. Application lifecycle management has been a common practice within software development for some time. Now, some of those practices are starting to become adopted in the machine learning space. One of the challenges application lifecycle management addresses in machine learning is reproducibility.&lt;/p&gt;
&lt;p&gt;Machine learning is extremely experimental in nature. Therefore, in order to find the &amp;quot;best&amp;quot; model, various algorithms and hyper-parameters need to be tested. This can sometimes be a manual process. At Build 2019, &lt;a href="https://devblogs.microsoft.com/dotnet/announcing-ml-net-1-0/"&gt;Automated ML was announced&lt;/a&gt; for ML.NET. In addition to automating the training process, this framework will try to find the best model by iterating over various algorithms and hyper-parameters until it finds the &amp;quot;best&amp;quot; model based on the selected optimization metric. The output will consist of results for the best run along with results for all other runs. These runs contain performance metrics, learned model parameters, the training pipeline used and the trained model for the respective run. This information can then be used for auditing purposes as well as to reproduce results.&lt;/p&gt;
&lt;p&gt;The results from running Automated ML can be persisted locally or in a database. However in 2018 a product called MLFlow was launched. &lt;a href="https://mlflow.org/"&gt;MLFlow&lt;/a&gt; is an open source machine learning lifecycle management platform. Since its announcement, MLFlow has seen adoption throughout the industry and most recently &lt;a href="https://databricks.com/company/newsroom/press-releases/databricks-collaborates-with-microsoft-on-mlflow-open-source-project"&gt;Microsoft announced native support for it inside of Azure ML&lt;/a&gt;. Although MLFlow does not natively support .NET, it has a REST API that allows extensibility to non-natively supported languages. This means that if throughout your enterprise or projects, MLFlow has been adopted in Python or R applications, using the REST API you can integrate MLFlow into your ML.NET applications.&lt;/p&gt;
&lt;p&gt;In this writeup, I will go over how to automatically build an ML.NET classification model that predicts iris flower types using Automated ML and then integrate MLFlow to log the results generated by the best run. The code for this sample can be found on &lt;a href="https://github.com/lqdev/MLNETMLFlowSample"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;This project was built on an Ubuntu 18.04 PC but should work on Windows and Mac. Note that MLFlow does not natively run on Windows at the time of this writing. To run it on Windows use &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10"&gt;Windows Subsystem for Linux (WSL)&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python 3.x&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.mlflow.org/docs/latest/quickstart.html"&gt;MLFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://dotnet.microsoft.com/download"&gt;.NET SDK 2.x&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;h3&gt;Create Solution&lt;/h3&gt;
&lt;p&gt;First we'll start off by creating a solution for our project. In the terminal enter the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir MLNETMLFlowSample &amp;amp;&amp;amp; cd MLNETMLFlowSample
dotnet new sln
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create Console Application&lt;/h3&gt;
&lt;p&gt;Once the solution is created, from the root solution directory, enter the following commands into the terminal to create a console application.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new console -o MLNETMLFlowSampleConsole
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, navigate into the newly created console application folder.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;cd MLNETMLFlowSampleConsole
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Install NuGet Packages&lt;/h3&gt;
&lt;p&gt;For this solution, you'll need the following NuGet packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.nuget.org/packages/Microsoft.ML/"&gt;Microsoft.ML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nuget.org/packages/Microsoft.ML.AutoML/"&gt;Microsoft.ML.Auto&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nuget.org/packages/MLFlow.NET/1.0.0-CI-20181206-054144"&gt;MLFlow.NET&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;From the console application directory, enter the following commands into the terminal to install the packages.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet add package Microsof.ML
dotnet add package Microsoft.ML.AutoML
dotnet add package MLFlow.NET --version 1.0.0-CI-20181206-054144
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Get The Data&lt;/h3&gt;
&lt;p&gt;The data used in this dataset comes from the &lt;a href="http://archive.ics.uci.edu/ml/index.php"&gt;UCI Machine Learning Repository&lt;/a&gt; and looks like the data below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, create a directory for the data inside the console application directory:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir Data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, download and save the file into the &lt;code&gt;Data&lt;/code&gt; directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -o Data/iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, make sure that the data file is copied into your output directory by adding the following section to your console application &lt;code&gt;csproj&lt;/code&gt; file inside the &lt;code&gt;Project&lt;/code&gt; tags:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-xml"&gt;&amp;lt;ItemGroup&amp;gt;
    &amp;lt;Content Include=&amp;quot;Data/iris.data&amp;quot;&amp;gt;
        &amp;lt;CopyToOutputDirectory&amp;gt;PreserveNewest&amp;lt;/CopyToOutputDirectory&amp;gt;
    &amp;lt;/Content&amp;gt;
&amp;lt;/ItemGroup&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Configure MLFlow Service&lt;/h2&gt;
&lt;h3&gt;MLFlow.NET Settings&lt;/h3&gt;
&lt;p&gt;MLFlow.NET requires two settings, the base URL where the MLFlow server listens on and the API endpoint. In this case since it will be running locally, the base URL is &lt;code&gt;http://localhost:5000&lt;/code&gt;. In the console application directory, create a file called &lt;code&gt;appsettings.json&lt;/code&gt; and add the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;MLFlowConfiguration&amp;quot;: {
        &amp;quot;MLFlowServerBaseUrl&amp;quot;: &amp;quot;http://localhost:5000&amp;quot;,
        &amp;quot;APIBase&amp;quot;: &amp;quot;api/2.0/preview/mlflow/&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make sure that your &lt;code&gt;appsettings.json&lt;/code&gt; is copied into your output directory, add the following content to your &lt;code&gt;csproj&lt;/code&gt; file under the content tags that include the &lt;code&gt;iris.data&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-xml"&gt;&amp;lt;Content Include=&amp;quot;appsettings.json&amp;quot;&amp;gt;
    &amp;lt;CopyToOutputDirectory&amp;gt;PreserveNewest&amp;lt;/CopyToOutputDirectory&amp;gt;
&amp;lt;/Content&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Register MLFLow.NET Service&lt;/h3&gt;
&lt;p&gt;In this sample, the MLFlow.NET service is registered and used via dependency injection. However, in order to use dependency injection in our console application it first needs to be configured. In the console application directory, create a new file called &lt;code&gt;Startup.cs&lt;/code&gt; and add the following code to it:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;
using Microsoft.Extensions.Configuration;
using Microsoft.Extensions.DependencyInjection;
using MLFlow.NET.Lib;
using MLFlow.NET.Lib.Contract;
using MLFlow.NET.Lib.Model;

namespace MLNETMLFlowSampleConsole
{
    public class Startup
    {
        private readonly IConfiguration Configuration;
        public IServiceProvider Services;

        public Startup()
        {
            Configuration =
                new ConfigurationBuilder()
                    .AddJsonFile(&amp;quot;appsettings.json&amp;quot;, false)
                    .Build();
        }

        public void ConfigureServices()
        {
            var services = new ServiceCollection();

            // Add and configure MLFlow Service
            services.AddMFlowNet();
            services.Configure&amp;lt;MLFlowConfiguration&amp;gt;(
                Configuration.GetSection(nameof(MLFlowConfiguration))
            );

            Services = services.BuildServiceProvider();
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;Startup&lt;/code&gt; class loads configuration settings from the &lt;code&gt;appsettings.json&lt;/code&gt; file in the constructor. Then, the &lt;code&gt;ConfigureServices&lt;/code&gt; method registers the MLFlow.NET service and configures it using the settings defined in the &lt;code&gt;appsettings.json&lt;/code&gt; file. Once that this is set up, the service can be used throughout the application.&lt;/p&gt;
&lt;h2&gt;Create Data Schema&lt;/h2&gt;
&lt;p&gt;When working with ML.NET, it often helps to create data models or classes that define the data's schema.&lt;/p&gt;
&lt;p&gt;For this sample, there are four columns with float values which will be the input data or features. The last column is the type of iris flower which will be used as the label or the value to predict.&lt;/p&gt;
&lt;p&gt;First, create a new directory called &lt;code&gt;Domain&lt;/code&gt; inside the console application directory to store the data model classes.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir Domain
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the &lt;code&gt;Domain&lt;/code&gt; directory, create a new file called &lt;code&gt;IrisData.cs&lt;/code&gt; and add the following contents to the file:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Microsoft.ML.Data;

namespace MLNETMLFlowSample.Domain
{
    public class IrisData
    {
        [LoadColumn(0, 3),
        VectorType(4),
        ColumnName(&amp;quot;Features&amp;quot;)]
        public float Features { get; set; }

        [LoadColumn(4),
        ColumnName(&amp;quot;Label&amp;quot;)]
        public string IrisType { get; set; }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using attributes in the schema, we define two properties: &lt;code&gt;Features&lt;/code&gt; and &lt;code&gt;IrisType&lt;/code&gt;. Data from columns in positions 0-3 will be loaded as a float vector of size four into the &lt;code&gt;Features&lt;/code&gt; property. ML.NET will then reference that column by the name &lt;code&gt;Features&lt;/code&gt;. Data in the last column will be loaded into the &lt;code&gt;IrisType&lt;/code&gt; property and be referenced by the name &lt;code&gt;Label&lt;/code&gt;. The reason for setting column names is ML.NET algorithms have a default column names and renaming properties at the schema level removes the need to define the feature and label columns in the pipeline.&lt;/p&gt;
&lt;h2&gt;Create Progress Handler&lt;/h2&gt;
&lt;p&gt;By default, running the application won't display progress information. However, a &lt;code&gt;ProgressHandler&lt;/code&gt; object can be passed into the &lt;code&gt;Execute&lt;/code&gt; method of an experiment which calls the implemented &lt;code&gt;Report&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;Inside the console application directory, create a new file called &lt;code&gt;ProgressHandler.cs&lt;/code&gt; and add the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;
using Microsoft.ML.AutoML;
using Microsoft.ML.Data;
using Microsoft.ML.Trainers;

namespace MLNETMLFlowSampleConsole
{
    public class ProgressHandler : IProgress&amp;lt;RunDetail&amp;lt;MulticlassClassificationMetrics&amp;gt;&amp;gt;
    {
        public void Report(RunDetail&amp;lt;MulticlassClassificationMetrics&amp;gt; runDetails)
        {
            Console.WriteLine($&amp;quot;Ran {runDetails.TrainerName} in {runDetails.RuntimeInSeconds:0.##} with Log Loss {runDetails.ValidationMetrics.LogLoss:0.####}&amp;quot;);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;ProgressHandler&lt;/code&gt; class derives from the &lt;code&gt;IProgress&amp;lt;T&amp;gt;&lt;/code&gt; interface which requires the implementation of the &lt;code&gt;Report&lt;/code&gt; method. The object being passed into the &lt;code&gt;Report&lt;/code&gt; method after each run is an &lt;code&gt;RunDetail&amp;lt;MulticlassCLassificationMetrics&amp;gt;&lt;/code&gt; object. Each time a run is complete, the &lt;code&gt;Report&lt;/code&gt; method is called and the code inside it executes.&lt;/p&gt;
&lt;h2&gt;Create Experiment&lt;/h2&gt;
&lt;p&gt;Open the &lt;code&gt;Program.cs&lt;/code&gt; file and add the following &lt;code&gt;using&lt;/code&gt; statements.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;
using System.IO;
using System.Threading.Tasks;
using Microsoft.ML;
using Microsoft.ML.Data;
using Microsoft.ML.AutoML;
using Microsoft.Extensions.DependencyInjection;
using MLNETMLFlowSampleConsole.Domain;
using MLFlow.NET.Lib;
using MLFlow.NET.Lib.Contract;
using MLFlow.NET.Lib.Model;
using MLFlow.NET.Lib.Model.Responses.Experiment;
using MLFlow.NET.Lib.Model.Responses.Run;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Initialize Services&lt;/h3&gt;
&lt;p&gt;Then, inside of the &lt;code&gt;Program&lt;/code&gt; class, define the &lt;code&gt;IMLFlowService&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;private readonly static IMLFlowService _mlFlowService;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Directly after that, create a constructor which is where &lt;code&gt;_mlFLowService&lt;/code&gt; will be instantiated.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;static Program()
{
    // Initialize app configuration
    var appConfig = new Startup();
    appConfig.ConfigureServices();

    // Initialize MLFlow service
    _mlFlowService = appConfig.Services.GetService&amp;lt;IMLFlowService&amp;gt;();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Define Experiment Steps&lt;/h3&gt;
&lt;p&gt;Then, add a method called &lt;code&gt;RunExperiment&lt;/code&gt; inside the &lt;code&gt;Program&lt;/code&gt; class that contains the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public static async Task RunExperiment()
{
    // 1. Create MLContext
    MLContext ctx = new MLContext();

    // 2. Load data
    IDataView data = ctx.Data.LoadFromTextFile&amp;lt;IrisData&amp;gt;(&amp;quot;Data/iris.data&amp;quot;, separatorChar: ',');

    // 3. Define Automated ML.NET experiment settings
    var experimentSettings = new MulticlassExperimentSettings();
    experimentSettings.MaxExperimentTimeInSeconds = 30;
    experimentSettings.OptimizingMetric = MulticlassClassificationMetric.LogLoss;

    // 4. Create Automated ML.NET
    var experiment = ctx.Auto().CreateMulticlassClassificationExperiment(experimentSettings);

    // 5. Create experiment in MLFlow
    var experimentName = Guid.NewGuid().ToString();
    var experimentRequest = await _mlFlowService.GetOrCreateExperiment(experimentName);

    // 6. Run Automated ML.NET experiment
    var experimentResults = experiment.Execute(data, progressHandler: new ProgressHandler());

    // 7. Log Best Run
    LogRun(experimentRequest.ExperimentId,experimentResults);

    string savePath = Path.Join(&amp;quot;MLModels&amp;quot;, $&amp;quot;{experimentName}&amp;quot;);
    string modelPath = Path.Join(savePath, &amp;quot;model.zip&amp;quot;);

    if (!Directory.Exists(savePath))
    {
        Directory.CreateDirectory(savePath);
    }

    // 8. Save Best Trained Model
    ctx.Model.Save(experimentResults.BestRun.Model, data.Schema, modelPath);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;RunExperiment&lt;/code&gt; method does the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Creates an &lt;code&gt;MLContext&lt;/code&gt; object.&lt;/li&gt;
&lt;li&gt;Loads data from the &lt;code&gt;iris.data&lt;/code&gt; file into an &lt;code&gt;IDataView&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Configures experiment to run for 30 seconds and optimize the Log Loss metric.&lt;/li&gt;
&lt;li&gt;Creates a new Automated ML.NET experiment.&lt;/li&gt;
&lt;li&gt;Creates a new experiment in MLFlow to log information to.&lt;/li&gt;
&lt;li&gt;Runs the Automated ML.NET experiment and provide an instance of &lt;code&gt;ProgressHandler&lt;/code&gt; to output progress to the console.&lt;/li&gt;
&lt;li&gt;Uses the &lt;code&gt;LogRun&lt;/code&gt; method to log the results of the best run to MLFlow.&lt;/li&gt;
&lt;li&gt;Creates a directory inside the &lt;code&gt;MLModels&lt;/code&gt; directory using the name of the experiment and saves the trained model inside it under the &lt;code&gt;model.zip&lt;/code&gt; file name.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Log Progress&lt;/h3&gt;
&lt;p&gt;After the &lt;code&gt;RunExperiment&lt;/code&gt; method, create the &lt;code&gt;LogRun&lt;/code&gt; method and add the following code to it:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;static async void LogRun(int experimentId, ExperimentResult&amp;lt;MulticlassClassificationMetrics&amp;gt; experimentResults)
{
    // Define run
    var runObject = new CreateRunRequest();
    runObject.ExperimentId = experimentId;
    runObject.StartTime = ((DateTimeOffset)DateTime.UtcNow).ToUnixTimeMilliseconds();
    runObject.UserId = Environment.UserName;
    runObject.SourceType = SourceType.LOCAL;

    // Create new run in MLFlow
    var runRequest = await _mlFlowService.CreateRun(runObject);

    // Get information for best run
    var runDetails = experimentResults.BestRun;

    // Log trainer name
    await _mlFlowService.LogParameter(runRequest.Run.Info.RunUuid, nameof(runDetails.TrainerName), runDetails.TrainerName);

    // Log metrics
    await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.RuntimeInSeconds), (float)runDetails.RuntimeInSeconds);
    await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.ValidationMetrics.LogLoss), (float)runDetails.ValidationMetrics.LogLoss);
    await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.ValidationMetrics.MacroAccuracy), (float)runDetails.ValidationMetrics.MacroAccuracy);
    await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.ValidationMetrics.MicroAccuracy), (float)runDetails.ValidationMetrics.MicroAccuracy);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;LogRun&lt;/code&gt; method takes in the experiment ID and results. Then, it does the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Configures local &lt;code&gt;RunRequest&lt;/code&gt; object to log in MLFlow.&lt;/li&gt;
&lt;li&gt;Creates run in MLFLow using predefined configuration.&lt;/li&gt;
&lt;li&gt;Logs the name of the machine learning algorithm used to train the best model in MLFlow.&lt;/li&gt;
&lt;li&gt;Logs the amount of seconds it took to train the model in MLFLow.&lt;/li&gt;
&lt;li&gt;Logs various performance metrics in MLFlow&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Implement Main Method&lt;/h3&gt;
&lt;p&gt;Finally, in the &lt;code&gt;Main&lt;/code&gt; method of the &lt;code&gt;Program&lt;/code&gt; class, call the &lt;code&gt;RunExperiment&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;static async Task Main(string[] args)
{
    // Run experiment
    await RunExperiment();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the &lt;code&gt;Main&lt;/code&gt; method in the &lt;code&gt;Program&lt;/code&gt; class will be async, you need to use the latest version of C#. To do so, add the following configuration inside the &lt;code&gt;PropertyGroup&lt;/code&gt; section of your &lt;code&gt;csproj&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-xml"&gt;&amp;lt;LangVersion&amp;gt;latest&amp;lt;/LangVersion&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Run The Application&lt;/h2&gt;
&lt;h3&gt;Start MLFlow Server&lt;/h3&gt;
&lt;p&gt;In the terminal, from the console application directory, enter the following command to start the MLFlow Server:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mlflow server
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Navigate to &lt;code&gt;http://localhost:5000&lt;/code&gt; in your browser. This will load the MLFLow UI.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/ml-lifecycle-management-mlflow-automated-ml-net/ml-lifecycle-management-mlflow-automated-ml-net-1.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h3&gt;Train Model&lt;/h3&gt;
&lt;p&gt;Then, in another terminal, from the console application directory, enter the following command to run the experiment:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet build
dotnet run
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As the experiment is running, the progress handler should be posting output to the console after each run.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Ran AveragedPerceptronOva in 1.24 with Log Loss 0.253
Ran SdcaMaximumEntropyMulti in 3.21 with Log Loss 0.0608
Ran LightGbmMulti in 0.64 with Log Loss 0.2224
Ran FastTreeOva in 1.54 with Log Loss 0.3978
Ran LinearSvmOva in 0.25 with Log Loss 0.2874
Ran LbfgsLogisticRegressionOva in 0.36 with Log Loss 0.566
Ran SgdCalibratedOva in 0.78 with Log Loss 0.7224
Ran FastForestOva in 1.28 with Log Loss 0.125
Ran LbfgsMaximumEntropyMulti in 0.25 with Log Loss 0.4537
Ran SdcaMaximumEntropyMulti in 0.19 with Log Loss 0.4576
Ran LightGbmMulti in 0.25 with Log Loss 0.2592
Ran FastForestOva in 1.98 with Log Loss 0.0961
Ran SdcaMaximumEntropyMulti in 0.39 with Log Loss 0.108
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Navigate to &lt;code&gt;http://localhost:5000&lt;/code&gt; in your browser. You should then see the results of your experiment and runs there.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/ml-lifecycle-management-mlflow-automated-ml-net/ml-lifecycle-management-mlflow-automated-ml-net-2.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Inspecting the detailed view of the best run for that experiment would look like the image below:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/ml-lifecycle-management-mlflow-automated-ml-net/ml-lifecycle-management-mlflow-automated-ml-net-3.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;You'll also notice that two directories were created inside the console application directory. On is an &lt;code&gt;MLModels&lt;/code&gt; directory, inside of which a nested directory with the name of the experiment contains the trained model. Another called &lt;code&gt;mlruns&lt;/code&gt;. In the &lt;code&gt;mlruns&lt;/code&gt; directory are the results of the experiments logged in MLFlow.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this writeup, we built an ML.NET classification model using Automated ML. Then, the results of the best run generated by Automated ML were logged to an MLFlow server for later analysis. Note that although some of these tools are still in their nascent stage, as open source software, the opportunities for extensibility and best practice implementations are there. Happy coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/ml-lifecycle-management-mlflow-automated-ml-net?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/ml-lifecycle-management-mlflow-automated-ml-net</guid>
      <pubDate>2019-05-09 19:19:20</pubDate>
      <category>#machine-learning</category>
      <category>#artificial-intelligence</category>
      <category>#mlnet</category>
      <category>#dotnet</category>
      <category>#dotnetcore</category>
      <category>#automatedml</category>
    </item>
    <item>
      <title>Automating Resource Provisioning for Machine Learning in Azure with Cognitive Services and Terraform</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/automate-machine-learning-service-provisioning-azure-terraform?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/automate-machine-learning-service-provisioning-azure-terraform&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Resources in the cloud, particularly Azure can be created in several ways some of which include Powershell, Azure CLI, Azure Portal and programatically through code. Another tool that can be leveraged to create resources in Azure is &lt;a href="https://www.terraform.io/"&gt;Terraform&lt;/a&gt;. Terraform via configuration files can build and manage infrastructure across various providers such as AWS, Azure and Google. One of the benefits of Terraform is that it allows infastructure and resources to be defined as code. The configuration files serve as a blueprint as well as an execution plan of what needs to be provisioned in the cloud. Not only does this allow for automation and removal of human error in configurations but it also serves as a way to document the different changes infrastructure has gone through thoroughout the application lifecycle. The purpose of this writeup is to show how to create an environment using Terraform for an application that utilizes Azure Cognitive Services. Our application will create a Computer Vision service that we can then use via HTTP requests. Source code for this writeup can be found at the following &lt;a href="https://github.com/lqdev/azcognitiveserviceterraformsample"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Prerequisites&lt;/h1&gt;
&lt;p&gt;This writeup was built and tested using a PC running Ubuntu 18.04 but should work on both Windows and Mac. It also assumes that you have an Azure account as well as Azure CLI and Terraform CLI installed. Below are links to get all of the resources needed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://azure.microsoft.com/en-us/free/"&gt;Azure Account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest"&gt;Azure CLI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.terraform.io/downloads.html"&gt;Terraform CLI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Log into Azure CLI&lt;/h2&gt;
&lt;p&gt;The first thing we need to do is authenticate with Azure CLI so that Terraform can use your account information to create services. In the terminal, enter the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az login
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Create Terraform Resource Configuration Script&lt;/h2&gt;
&lt;p&gt;Once we're logged in, it's time to create the configuration file that Terraform will use to provision our services. First we'll need to create a directory for our application:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir azcognitiveserviceterraformsample
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we'll enter that directory and create our configuration file:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;cd azcognitiveserviceterraformsample
touch azcomputervision.tf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using your preferred text editor, open the newly created &lt;code&gt;azcomputervision.tf&lt;/code&gt; file and begin editing.&lt;/p&gt;
&lt;h3&gt;Defining the Provider&lt;/h3&gt;
&lt;p&gt;At the top of our file we'll want to configure our provider. This tells Terraform where our resources will be deployed to. In the &lt;code&gt;azcomputervision.tf&lt;/code&gt; file enter the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# Configure Azure Provider
provider &amp;quot;azurerm&amp;quot; {
  version=&amp;quot;=1.20.0&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we've already authenticated with the Azure CLI, there's no need to provide credentials. The only thing we need to do is specify the provider name and the version of the provider that will be used by the configuration file. For more details on how to configure the Azure provider, visit this &lt;a href="https://www.terraform.io/docs/providers/azurerm/index.html"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Create Resource Group&lt;/h3&gt;
&lt;p&gt;After defining the provider, it's time to create a Resource Group that will contain the resources we create. In the configuration file, under the provider definition, enter the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create Resource Group
resource &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;terraform_cognitive_sample&amp;quot; {
  name=&amp;quot;terraform_cognitive_sample&amp;quot;
  location=&amp;quot;East US&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This defines the name of our resource groups as well as where it is hosted. The syntax for resources looks like the snippet below where the &lt;code&gt;type&lt;/code&gt; is the type of resources as required by Terraform and the &lt;code&gt;name&lt;/code&gt; is any value of your choosing.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;resource &amp;quot;type&amp;quot; &amp;quot;name&amp;quot; {
# Properties
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more details on Resource Group configuration visit this &lt;a href="https://www.terraform.io/docs/providers/azurerm/d/resource_group.html"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Create Azure Cognitive Service&lt;/h3&gt;
&lt;p&gt;With our resource group defined, it's time to define our Cognitive Service resource. Below our Resource Group definition, enter the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;resource &amp;quot;azurerm_cognitive_account&amp;quot; &amp;quot;computer_vision_service&amp;quot; {
  name=&amp;quot;computer_vision_service&amp;quot;
  resource_group_name=&amp;quot;${azurerm_resource_group.terraform_cognitive_sample.name}&amp;quot;
  location=&amp;quot;${azurerm_resource_group.terraform_cognitive_sample.location}&amp;quot;
  kind=&amp;quot;ComputerVision&amp;quot;
  
  sku {
      name=&amp;quot;F0&amp;quot;
      tier=&amp;quot;Free&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like our Resource Group, we provide a name for our service. Additional properties we need to provide are the name of the Resource Group and the location of where to deploy our service to. Because this has been previously defined in our Resource Group, Terraform allows us to access the configuration values as variables. Additionally, we need to specify the &lt;code&gt;kind&lt;/code&gt; of cognitive service to deploy. In our case it will be the Computer Vision service so we use &lt;code&gt;ComputerVision&lt;/code&gt;. The last thing we need to do is specify the pricing tier to use. We'll be using the free tier for this project. There are several services and tiers to choose from. To get more details on acceptable values for these properties visit the following &lt;a href="https://www.terraform.io/docs/providers/azurerm/r/cognitive_account.html"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Store Output Variables&lt;/h3&gt;
&lt;p&gt;When our application is deployed, in order to use it we'll need the endpoint to where we will be making HTTP requests to. This can be persisted by Terraform by defining an output variable inside of the configuration file. To get the enpoint of our cognitive service, enter the following in the &lt;code&gt;azcomputervision.tf&lt;/code&gt; file:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;output &amp;quot;computer_vision_endpoint&amp;quot; {
    value=&amp;quot;${azurerm_cognitive_account.computer_vision_service.endpoint}&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once finished, the &lt;code&gt;azcomputervision.tf&lt;/code&gt; file should look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# Configure Azure Provider
provider &amp;quot;azurerm&amp;quot; {
  version=&amp;quot;=1.20.0&amp;quot;
}

# Create Resource Group
resource &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;terraform_cognitive_sample&amp;quot; {
  name=&amp;quot;terraform_cognitive_sample&amp;quot;
  location=&amp;quot;East US&amp;quot;
}

# Create Cognitive Service
resource &amp;quot;azurerm_cognitive_account&amp;quot; &amp;quot;computer_vision_service&amp;quot; {
  name=&amp;quot;computer_vision_service&amp;quot;
  resource_group_name=&amp;quot;${azurerm_resource_group.terraform_cognitive_sample.name}&amp;quot;
  location=&amp;quot;${azurerm_resource_group.terraform_cognitive_sample.location}&amp;quot;
  kind=&amp;quot;ComputerVision&amp;quot;
  
  sku {
      name=&amp;quot;F0&amp;quot;
      tier=&amp;quot;Free&amp;quot;
  }
}

# Output Cognitive Services Endpoint
output &amp;quot;computer_vision_endpoint&amp;quot; {
    value=&amp;quot;${azurerm_cognitive_account.computer_vision_service.endpoint}&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Provision Resources&lt;/h2&gt;
&lt;h3&gt;Initialize Terraform Resources&lt;/h3&gt;
&lt;p&gt;When provisioning resources we first want to get the necessary plugins needed by Terraform to create such resources. Using Terraform CLI, enter the following in the terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;terraform init
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Check Execution Plan&lt;/h3&gt;
&lt;p&gt;Although this is a single resource provision, it's always a good idea to see the execution plan of the resources that will be created. To do so, enter the following command in the terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;terraform plan
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output should look similar to the content below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.


------------------------------------------------------------------------

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  + azurerm_cognitive_account.computer_vision_service
      id:                  &amp;lt;computed&amp;gt;
      endpoint:            &amp;lt;computed&amp;gt;
      kind:                &amp;quot;ComputerVision&amp;quot;
      location:            &amp;quot;eastus&amp;quot;
      name:                &amp;quot;computer_vision_service&amp;quot;
      resource_group_name: &amp;quot;terraform_cognitive_sample&amp;quot;
      sku.#:               &amp;quot;1&amp;quot;
      sku.0.name:          &amp;quot;F0&amp;quot;
      sku.0.tier:          &amp;quot;Free&amp;quot;
      tags.%:              &amp;lt;computed&amp;gt;

  + azurerm_resource_group.terraform_cognitive_sample
      id:                  &amp;lt;computed&amp;gt;
      location:            &amp;quot;eastus&amp;quot;
      name:                &amp;quot;terraform_cognitive_sample&amp;quot;
      tags.%:              &amp;lt;computed&amp;gt;


Plan: 2 to add, 0 to change, 0 to destroy.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Create Resources&lt;/h2&gt;
&lt;p&gt;It's now time to create our Computer Vision resource. Enter the following command in the terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;terraform apply
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will be asked to review the execution plan once again. If everything looks good, type &lt;code&gt;yes&lt;/code&gt; in the terminal to continue with the creation of resources. If everything deployed successfully, you should be able to see it in Azure.&lt;/p&gt;
&lt;h2&gt;Test Provisioned Resources&lt;/h2&gt;
&lt;p&gt;Using Azure CLI, enter the following command in the terminal to see whether your resource was deployed:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az resource list --resource-group terraform_cognitive_sample --output table
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That command will output something similar to the content below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;Name                     ResourceGroup               Location    Type                                  Status
-----------------------  --------------------------  ----------  ------------------------------------  --------
computer_vision_service  terraform_cognitive_sample  eastus      Microsoft.CognitiveServices/accounts
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Get Resource Endpoint and Keys&lt;/h3&gt;
&lt;p&gt;In order to make a request to our service, we need both the endpoint and a key to authenticate our request.&lt;/p&gt;
&lt;p&gt;To get the endpoint, we can use Terraform CLI to extract the output variable defined in our configuration file. We can do that by using the following command in the terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;terraform output computer_vision_endpoint
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Save the output value somewhere because that's what will be used to make a request to the Computer Vision Cognitive Service.&lt;/p&gt;
&lt;p&gt;To get the keys, we'll use Azure CLI. In the terminal, enter:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az cognitiveservices account keys list --resource-group terraform_cognitive_sample --name computer_vision_service
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will output the keys of your deployed service.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;key1&amp;quot;: &amp;quot;&amp;lt;YOUR-KEY-1&amp;gt;&amp;quot;,
  &amp;quot;key2&amp;quot;: &amp;quot;&amp;lt;YOUR-KEY-2&amp;gt;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Make HTTP Request&lt;/h3&gt;
&lt;p&gt;Either key can be used to make requests. To test the service, I'll be using an image from the web. All you need is to get the URL for that image. The image I will be using can be found at this URL &lt;code&gt;https://upload.wikimedia.org/wikipedia/commons/1/10/Empire_State_Building_%28aerial_view%29.jpg&lt;/code&gt;. Using cURL, I will make a POST request to the endpoint with the key provided.&lt;/p&gt;
&lt;p&gt;One thing to note is that the endpoint is generic to many Cognitive Services. Therefore, we need to append to the path which service we will be using and the action we want to perform. In our case, we'll append the following path the the endpoint &lt;code&gt;/vision/v1.0/describe&lt;/code&gt;. For more details, visit the Computer Vision API documentation at this &lt;a href="https://eastus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fe"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To make the request, enter the following into the terminal where &lt;code&gt;&amp;lt;YOUR-KEY&amp;gt;&lt;/code&gt; is one of the keys of your Computer Vision service.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -H 'Ocp-Apim-Subscription-Key: &amp;lt;YOUR-KEY&amp;gt;' -H &amp;quot;Content-type: application/json&amp;quot; -d '{&amp;quot;url&amp;quot;:&amp;quot;https://upload.wikimedia.org/wikipedia/commons/1/10/Empire_State_Building_%28aerial_view%29.jpg&amp;quot;}' 'https://eastus.api.cognitive.microsoft.com/vision/v1.0/describe'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If on Windows, you might want to try using a REST client such as POSTMAN or Insomnia.&lt;/p&gt;
&lt;p&gt;If the request is successful, the output should be similar to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;description&amp;quot;:{
        &amp;quot;tags&amp;quot;:[
            &amp;quot;mountain&amp;quot;,&amp;quot;building&amp;quot;,&amp;quot;city&amp;quot;,&amp;quot;sitting&amp;quot;,&amp;quot;table&amp;quot;,&amp;quot;view&amp;quot;,&amp;quot;full&amp;quot;,&amp;quot;filled&amp;quot;,&amp;quot;large&amp;quot;,&amp;quot;old&amp;quot;,&amp;quot;skyscraper&amp;quot;,&amp;quot;many&amp;quot;,&amp;quot;stacked&amp;quot;,&amp;quot;water&amp;quot;,&amp;quot;room&amp;quot;,&amp;quot;white&amp;quot;
        ],
        &amp;quot;captions&amp;quot;:[
            {
                &amp;quot;text&amp;quot;:&amp;quot;a view of a city&amp;quot;,&amp;quot;confidence&amp;quot;:0.927147938733121
            }
        ]
    },
    &amp;quot;requestId&amp;quot;:&amp;quot;a9aaf779-2aa1-4012-b1e0-2d2d9c20c6a5&amp;quot;,
    &amp;quot;metadata&amp;quot;:{
        &amp;quot;width&amp;quot;:846,
        &amp;quot;height&amp;quot;:1270,
        &amp;quot;format&amp;quot;:&amp;quot;Jpeg&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this writeup I went over how to create a Terraform configuration file for automating the creation of a Computer Vision Cognitive Service resource in Azure. Although this is a simple example, using the same concepts, more complex environments and infrastructure can be provisioned just as seamlessly in a safe and efficient manner. An example of how to extend this application would be to provision an Azure Storage Account as well as an Azure Function application that utilizes the Computer Vision Cognitive Service to automatically classify photos when they are uploaded to an Azure Storage container. In such scenario, the benefits of automation and documentation that Terraform provides can leveraged to a greater extent.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/automate-machine-learning-service-provisioning-azure-terraform?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/automate-machine-learning-service-provisioning-azure-terraform</guid>
      <pubDate>2019-01-05 22:58:39</pubDate>
      <category>#azure</category>
      <category>#cognitive-services</category>
      <category>#terraform</category>
      <category>#machine-learning</category>
      <category>#cloud</category>
    </item>
    <item>
      <title>The Case for Doing Machine Learning with F#</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/case-fsharp-machine-learning?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/case-fsharp-machine-learning&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This post is part of the 2018 &lt;a href="https://sergeytihon.com/tag/fsadvent/"&gt;FsAdvent&lt;/a&gt; series organized by &lt;a href="https://twitter.com/sergey_tihon"&gt;Sergey Tihon&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When searching for tools and languages to implement machine learning applications, there are numerous options to choose from each with their own set of advantages and disadvantages. Out of all, however, Python seems to be the most salient. Not only is it a popular language but also many of the tools available for machine learning are either implemented in Python or support it in some capacity whether it's native or through community libraries. Very rarely though is F# mentioned in these discussions despite having many of the features that make languages like Python so loved and extending them to empower users. In this writeup, I will do a short review of many of the advantages of Python such as succinctness, platform support, library availability as well as many others and compare it to F#'s capabilities.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/case-fsharp-machine-learning/case-fsharp-ml-0.PNG" class="img-fluid" alt="2017 Top Languages" /&gt;&lt;/p&gt;
&lt;h2&gt;Python's Advantages&lt;/h2&gt;
&lt;h3&gt;Learning Curve&lt;/h3&gt;
&lt;p&gt;One of the reasons why Python is so widely adopted is its learning curve. Whether an individual knows how to program or not, at times, Python can look like pseudocode making it accessible to not only readers but also writers. As with anything the more complex the task, the steeper the learning curve. However, at a simpler level, Python makes it as easy as possible to get started.&lt;/p&gt;
&lt;p&gt;Although at first it may not appear to be the case with F#, the learning curve is not much steeper than that of Python. The syntax can sometimes look intimidating to individuals, but the steepest part of the learning curve doesn't necessarily come from the language itself but rather from the way of reasoning about the logic of the programs. As a functional language, there is somewhat of a paradigm shift from that of a procedural execution model. Below is an example that defines a function that doubles an integer in both languages.&lt;/p&gt;
&lt;h5&gt;Python&lt;/h5&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def double(x):
    return x * 2
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;FSharp&lt;/h5&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let double x = 
    x * 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As it can be seen, despite some minor syntax and character differences the functions are essentially the same.&lt;/p&gt;
&lt;h3&gt;Intended Purpose&lt;/h3&gt;
&lt;p&gt;Depending on the task at hand, some languages are more adept for handling respective tasks. For example, R and Julia are excellent languages when performing statistical tasks. However, outside of those types of tasks their abilities are more limited. Python, being a general-purpose language means that not only can you use it for machine learning tasks but also to build n-tier applications entirely in Python without having to worry about integrations, plugins or having to learn an entirely different language to perform such actions.&lt;/p&gt;
&lt;p&gt;Similarly, F# is a general-purpose language which allows you to build web and console applications as well as machine learning applications all from the comfort of the same ecosystem.&lt;/p&gt;
&lt;h3&gt;Strong Library Support&lt;/h3&gt;
&lt;p&gt;When performing data analysis and machine learning, practicioners use a variety of libraries for their development such as &lt;a href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt; and &lt;a href="https://pandas.pydata.org/"&gt;Pandas&lt;/a&gt; for data wrangling, &lt;a href="https://scikit-learn.org/stable/index.html"&gt;scikit-learn&lt;/a&gt; for machine learning algorithms and &lt;a href="https://matplotlib.org/"&gt;matplotlib&lt;/a&gt; for creating visualizations. Although all of these tasks could most certainly be implemented from scratch, libraries speed up the development process allowing practitioners to focus more on the domain and experiment with the models that best solve the respective problem they are facing.&lt;/p&gt;
&lt;p&gt;F#, like Python has exceptional library support, specifically as it regards data science and machine learning. &lt;a href="https://fslab.org/"&gt;FsLab&lt;/a&gt; is a collection of open source F# packages for machine learning that contain libraries such as &lt;a href="https://fsharp.github.io/FSharp.Data/"&gt;FSharp.Data&lt;/a&gt; and &lt;a href="https://fslab.org/Deedle/"&gt;Deedle&lt;/a&gt; for data wrangling, &lt;a href="https://numerics.mathdotnet.com/"&gt;Math.NET Numerics&lt;/a&gt; for machine learning algorithms and &lt;a href="https://fslab.org/XPlot/"&gt;XPlot&lt;/a&gt; to help with data visualization. Furthermore, at Build 2018, Microsoft released &lt;a href="https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet"&gt;ML.NET&lt;/a&gt;, an open-source, cross-platform machine learning framework for .NET which allows .NET users to not only perform machine learning tasks within .NET but also allows extensibility via Tensorflow, ONNX and Python extensions. For a more detailed writup on using ML.NET with F#, check out the post &lt;a href="https://towardsdatascience.com/f-advent-calendar-a-christmas-classifier-72fb9e9d8f23"&gt;A Christmas Classifier&lt;/a&gt; by Willie Tetlow or my post &lt;a href="http://luisquintanilla.me/2018/06/13/mlnet-classification-fsharp/"&gt;Classification with F# ML.NET Models&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Libraries for F# are also not confined only to those written in F#. In many instances, because F# is a .NET language, there is interoperability with C# libraries further extending the capabilities of F#.&lt;/p&gt;
&lt;h3&gt;Platform Support&lt;/h3&gt;
&lt;p&gt;One of the things that makes Python so attractive is that it runs cross-platform. It does not matter whether you're on Windows, Mac or Linux; Python code runs the same. That being said though, not all platforms are created equal and although it is possible to run Python code on all platforms, essential libraries such as NumPy, Pandas and scikit-learn run best on Unix environments. It is possible to run them on Windows but the set up is not as straightforward.&lt;/p&gt;
&lt;p&gt;As a .NET Language, F# runs on Windows. With the help of the Mono runtime and most recently .NET Core, it also runs on Mac and Linux. Like Python, depending on the runtime and dependencies used by the respective software packages there may be limitations as to which platform code can be run on. However, from my experience most of the FsLab set of packages work cross-platform.&lt;/p&gt;
&lt;h3&gt;Succintness&lt;/h3&gt;
&lt;p&gt;Productivity is an important measure of a language. One way to achieve it is to write logic using the least number of characters. This is where Python shines. As a dynamically typed, whitespace ruled language, Python does not require developers to declare types associated with the objects and variables defined nor does it require the use of brackets and any other special characters. As expected, this allows for developers to write the same logic with less characters much faster.&lt;/p&gt;
&lt;p&gt;Unlike Python, F# is a statically typed language. However, thanks to type inference and the help of the compiler, writing programs often does not require developers to explicitly define what types objects and variables are. Additionally, it is a whitespace ruled language therefore removing the need for brackets and additional characters further speeding up the development process in a safe manner.&lt;/p&gt;
&lt;h3&gt;Immediate Feedback&lt;/h3&gt;
&lt;p&gt;Writing safe and effective code takes time and experience. However, even the most experienced developers often makes mistakes. Therefore, getting immediate feedback before adding certain logic to programs goes a long way to making code that is safe, efficient and tested.&lt;/p&gt;
&lt;h4&gt;Read-Evaluate-Print Loop (REPL)&lt;/h4&gt;
&lt;p&gt;One way in which both Python and F# provide immediate feedback is via the command line using the Read-Evaluate-Print Loop (REPL). The REPL is a programming environment that reads the user input, evaluates it and prints out the results, hence the name. Below are screenshots of what that environment looks like using both Python and F#.&lt;/p&gt;
&lt;h5&gt;Python&lt;/h5&gt;
&lt;p&gt;&lt;img src="/images/case-fsharp-machine-learning/case-fsharp-ml-1.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h5&gt;FSharp&lt;/h5&gt;
&lt;p&gt;&lt;img src="/images/case-fsharp-machine-learning/case-fsharp-ml-2.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;As it can be seen, getting that immediate feedback makes it easier to see whether the code is behaving the way it should. Although useful, this environment is not ideal when experimenting and making tweaks to the code. Additionally, because of the lack of a graphical user interface, the navigation can be less than ideal. Fortunately there is a solution out there that provides the same level of interactivity along with a graphical user interface that allows for ad-hoc experimentation and re-running of code at different points in time which is essential when developing machine learning applications.&lt;/p&gt;
&lt;h4&gt;Jupyter Notebooks&lt;/h4&gt;
&lt;p&gt;Much of machine learning deals with experimentation. Experimentation involves having a way to tweak parameters and evaluate the results. In addition, experiments should have a way of being documented and reproduced. As alluded to previously, such development environment and capabilities can be found in &lt;a href="https://jupyter.org/"&gt;Jupyter Notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As mentioned on the project's website:&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Jupyter Notebooks work with various languages which include but are not limited to Python and F# and can be run both on local machines as well as via a hosted service. One such service is Microsoft's &lt;a href="https://notebooks.azure.com/"&gt;Azure Notebooks&lt;/a&gt; which allows you to use Jupyter Notebooks in the cloud for free. All you need is to have a Microsoft account. Below are screenshots of what that environment looks like in both Python and F#.&lt;/p&gt;
&lt;h5&gt;Python&lt;/h5&gt;
&lt;p&gt;&lt;img src="/images/case-fsharp-machine-learning/case-fsharp-ml-3.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h5&gt;FSharp&lt;/h5&gt;
&lt;p&gt;&lt;img src="/images/case-fsharp-machine-learning/case-fsharp-ml-4.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h2&gt;Beyond Python&lt;/h2&gt;
&lt;p&gt;F#, is comparable to Python on many of the features that make Python a great language for both development and machine learning. However, there are some areas where F# provides additional safety and functionality that greatly improve the way in which machine learning applications are built.&lt;/p&gt;
&lt;h3&gt;Typing&lt;/h3&gt;
&lt;p&gt;As previously mentioned, F# is a statically typed language. However, it also makes use of type inference which means that declaring types is not always required. With the help of the compiler, based on the structure of functions and variables it is possible to determine which type an object or variable is. This has several advantages. One advantage of being strongly typed is that when the types are declared the code becomes self-documented because it is easier to deduce what functions are doing based on the types being passed in and returned. Another advantage is that it makes it harder to write bad code. Having the compiler help you when you write your code allows you to find errors prior to compilation or running the code. This along with the REPL gives you additional reassurance that your code is executing the intended logic.&lt;/p&gt;
&lt;p&gt;Python is making strides in acquiring some of that functionality with the introduction of type hints in version 3.6. However, this has not always been a core feature of the language and is only in its nascent stages.&lt;/p&gt;
&lt;h3&gt;Immutability&lt;/h3&gt;
&lt;p&gt;As a functional language, immutability is something that is a native part of the language. While in some cases it can change the way in which code is written, immutability has one advantage, especially when it comes to machine learning. With immutability, parallelization can be fully exploited for those algorithms which take advantage of it.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this writeup, I went over how F# is comparable to Python on many of the features that make it such a popular language such as library support, succinctness, general purpose and interactivity. However, F# has additional capabilities such as static typing and immutability that further enhance its capabilities as a language for machine learning. That is not to say that one is better than the other as they both are more adept for performing certain tasks. When it comes to machine learning it does become a matter of choice as they are both robust, powerful and strongly supported languages. Therefore next time you're looking to build a machine learning application, hopefully you give F# a try. Happy coding!&lt;/p&gt;
&lt;h6&gt;Resources&lt;/h6&gt;
&lt;p&gt;&lt;a href="https://spectrum.ieee.org/at-work/innovation/the-2018-top-programming-languages"&gt;2018 Top Programming Languages&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://www.manning.com/books/get-programming-with-f-sharp"&gt;Get Programming with F#&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://github.com/dotnet/machinelearning-samples"&gt;ML.NET Machine Learning Samples&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/case-fsharp-machine-learning?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/case-fsharp-machine-learning</guid>
      <pubDate>2018-12-14 23:34:50</pubDate>
      <category>#mlnet</category>
      <category>#machine-learning</category>
      <category>#fsharp</category>
      <category>#dotnet</category>
      <category>#programming</category>
      <category>#python</category>
      <category>#ai</category>
      <category>#dotnetcore</category>
      <category>#artificial-intelligence</category>
      <category>#fsadvent</category>
      <category>#artificialintelligence</category>
      <category>#machinelearning</category>
      <category>#data-science</category>
      <category>#functional-programming</category>
    </item>
    <item>
      <title>Operationalizing Machine Learning with ML.NET, Azure DevOps and Azure Container Instances</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/azdevops-mlnet-aci?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/azdevops-mlnet-aci&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Azure DevOps, formerly known as Visual Studio Team Services (VSTS), helps individuals and organizations plan, collaborate and ship products faster. One if its noteworthy services, Azure Pipelines, helps developers build Continuous Integration (CI) and Continuous Delivery (CD) pipelines that automate and standardize the build, test and deploy phases of the software development process. In addition, Azure Pipelines provides native container support and works with any language, platform and cloud. Machine learning like software development is also a process that includes a build, test and deploy phase which makes it a good candidate for automation and standardization. At Build 2018, Microsoft announced &lt;a href="https://github.com/dotnet/machinelearning"&gt;ML.NET&lt;/a&gt;, an open-source, cross-plaform machine learning framework for .NET. If we were to put all of these tools and services together, it means that we can automate and standardize the training of a machine learning model built with ML.NET, package it into a Docker container and deploy it to Azure Container Instances (ACI). In this writeup, I will go through the process of building a CI/CD pipeline in Azure Devops that trains, packages and deploys an ML.NET machine learning model to predict which class an Iris flower belongs to using a variety of measurements. Source code for this project can be found at this &lt;a href="https://github.com/lqdev/mlnetazdevopssample"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://git-scm.com/"&gt;Git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/"&gt;GitHub Account&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.microsoft.com/net/download"&gt;.NET Core SDK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://azure.microsoft.com/en-us/free/"&gt;Azure Account&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Application&lt;/h2&gt;
&lt;p&gt;Because the purpose of this post is to demonstrate the functionality of Azure Devops and not that of ML.NET, I'll start with a pre-built application. For some more information and detail into the functionality of ML.NET, check out the official documentation &lt;a href="https://docs.microsoft.com/en-us/dotnet/machine-learning/"&gt;page&lt;/a&gt; as well as some of my previous posts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://luisquintanilla.me/2018/08/21/serverless-machine-learning-mlnet-azure-functions/"&gt;Serverless Machine Learning with ML.NET and Azure Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/"&gt;Deploy .NET Machine Learning Models with ML.NET, ASP.NET Core, Docker and Azure Container Instances&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The application used in this writeup contains three .NET Core projects within it. One is a class library which is what we'll use to wrap ML.NET functionality for training models as well as loading pre-trained models that will then be used to make predictions. Another is a .NET Core console application which references the class library to train and persist an ML.NET model. Finally, there's the ASP.NET Core Web API which also references the class library application to load the pre-trained model created by the console application and then makes predictions via HTTP. This application can be utilized and deployed standalone but in this writeup it will be packaged into a Docker image that will then be deployed to Azure Container Instances.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-1.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h3&gt;Class Library&lt;/h3&gt;
&lt;p&gt;The class library can be found in the &lt;code&gt;MLModel&lt;/code&gt; directory. The class library defines the observation and prediction data classes which can be found in the &lt;code&gt;IrisData.cs&lt;/code&gt; and &lt;code&gt;IrisPrediction.cs&lt;/code&gt; files respectively. Additionally, the &lt;code&gt;Model&lt;/code&gt; class contains helper methods that train and save machine learning models, load pre-trained models and use these models to make predictions.&lt;/p&gt;
&lt;h3&gt;Console Application&lt;/h3&gt;
&lt;p&gt;In the solution directory we also have a console application in the &lt;code&gt;ModelTrainer&lt;/code&gt; directory. This application references the class library in the &lt;code&gt;MLModel&lt;/code&gt; directory to train and persist the machine learning model.&lt;/p&gt;
&lt;h3&gt;API&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;ModelApi&lt;/code&gt; directory contains an ASP.NET Core Web API application that references the &lt;code&gt;MLModel&lt;/code&gt; class library project to load the pre-trained model that is trained by the &lt;code&gt;ModelTrainer&lt;/code&gt; console application and makes predictions via HTTP. The logic for making predictions can be found in the &lt;code&gt;PredictController.cs&lt;/code&gt; class in the &lt;code&gt;Controllers&lt;/code&gt; directory of the &lt;code&gt;ModelApi&lt;/code&gt; application.&lt;/p&gt;
&lt;h2&gt;CI/CD Pipeline Flow&lt;/h2&gt;
&lt;p&gt;Conceptually, when the application is built and deployed manually, the machine learning model is defined and developed inside the &lt;code&gt;MLModel&lt;/code&gt; class library. Once satisfied with the model, the class library is built. The console application which references the &lt;code&gt;MLModel&lt;/code&gt; class library is built as well as run to train and persist a classification model in a file called &lt;code&gt;model.zip&lt;/code&gt;. The &lt;code&gt;MLModel&lt;/code&gt; class library is also referenced in the &lt;code&gt;ModelApi&lt;/code&gt; ASP.NET Core project. Because &lt;code&gt;ModelApi&lt;/code&gt; is the application we're looking to deploy in order to expose our pre-trained machine learning model, we need to find a way to package it for deployment. We'll be deploying &lt;code&gt;ModelApi&lt;/code&gt; using Azure Container Instances which means we need to create a Docker image of the project that will then be pushed to a Docker registry where it will be made available for public consumption. The building of multiple projects as well as the building, publishing and deployment of the Docker image to Azure Container Instances can be standardized and automated using Azure DevOps. The rest of this write-up will focus on demonstrating step-by-step how to operationalize this machine learning application via CI/CD pipelines in Azure DevOps using Azure Pipelines.&lt;/p&gt;
&lt;h3&gt;Getting The Code&lt;/h3&gt;
&lt;p&gt;Before getting started, the first thing you'll want to do is fork the &lt;a href="https://github.com/lqdev/mlnetazdevopssample"&gt;mlnetazdevopssample&lt;/a&gt; GitHub repository into your own GitHub account.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-2.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h3&gt;Creating the Project&lt;/h3&gt;
&lt;p&gt;Navigate to &lt;a href="https://devops.azure.com"&gt;https://devops.azure.com&lt;/a&gt;, click &lt;code&gt;Start Free&lt;/code&gt; and follow the prompts to either create a new account or sign into your existing account.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-3.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Once logged in, click &lt;code&gt;Create Project&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-4.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Enter the name of your project as well as a short description. Then, click &lt;code&gt;Create&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-5.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h2&gt;The Continuous Integration (CI) Pipeline&lt;/h2&gt;
&lt;p&gt;Using Azure Pipelines, we'll configure a CI pipeline for the build and packaging steps of our application. Below is an illustration of all the steps involved in our CI pipeline:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-6.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Build the class library application&lt;/li&gt;
&lt;li&gt;Build the console application&lt;/li&gt;
&lt;li&gt;Train and persist the ML.NET Model by running the console application.&lt;/li&gt;
&lt;li&gt;Copy ML.NET model file created by console application into ASP.NET Core Web API application directory&lt;/li&gt;
&lt;li&gt;Build ASP.NET Core Web API application&lt;/li&gt;
&lt;li&gt;Build Docker image&lt;/li&gt;
&lt;li&gt;Push Docker image to Docker Hub&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;CI Pipeline Setup&lt;/h3&gt;
&lt;p&gt;Once the project is created, in the main project page, hover over &lt;code&gt;Pipelines&lt;/code&gt; and click on &lt;code&gt;Builds&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-7.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;Builds&lt;/code&gt; pipeline page, click &lt;code&gt;New pipeline&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-8.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Select GitHub as the source and connect your GitHub account with Azure DevOps.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-9.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Once you have authorized Azure DevOps to use your GitHub account, select the repository and branch that will be used for this build pipeline. In our case, we'll be using the master branch of the &lt;code&gt;mlnetazdevopssample&lt;/code&gt; repository. When finished configuring, click &lt;code&gt;Continue&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-10.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;The next step will be to select the jobs to execute in our pipeline. Because there are multiple steps in this build pipeline, let's start with an &lt;code&gt;Empty Job&lt;/code&gt; and customize it to our needs.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-11.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;From inside the build pipeline page, before we start adding jobs, lets select the agent that will execute the jobs. For this pipeline, select the &lt;code&gt;Hosted Ubuntu 1604&lt;/code&gt; option from the dropdown.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-12.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h3&gt;1. Build the Class Library Application&lt;/h3&gt;
&lt;p&gt;The first step in our CI Pipeline will be to build our class library which contains methods that wrap the training, loading and prediction functionality of the ML.NET framework and persisted models.&lt;/p&gt;
&lt;p&gt;To achieve that, we'll add a .NET Core task to our &lt;code&gt;Agent Job 1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-13.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Once added to the pipeline, let's configure this task. To make it more descriptive, we can give it a name such as &lt;code&gt;Build Class Library&lt;/code&gt;. Because this task will be responsible for building the .NET Core class library, we'll leave the default &lt;code&gt;build&lt;/code&gt; Command setting as is.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-14.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;The other setting we'll want to configure is the &lt;code&gt;Working Directory&lt;/code&gt;. We can do so by clicking the &lt;code&gt;Advanced&lt;/code&gt; tab.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-15.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;For this task we'll use the &lt;code&gt;MLModel&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-16.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;When finished with the configuration, click &lt;code&gt;Save &amp;amp; Queue&lt;/code&gt; → &lt;code&gt;Save&lt;/code&gt; on the top toolbar.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-17.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Enter a detailed comment describing the change to the pipeline and click &lt;code&gt;Save&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-18.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h3&gt;2. Building The Console Application&lt;/h3&gt;
&lt;p&gt;Once we've built the class library application which we'll reference from the .NET Core console and ASP.NET Core Web API applications, it's time to build the console application which will serve the purpose of training and persisting the ML.NET model.&lt;/p&gt;
&lt;p&gt;Similar to the previous step, add a new .NET Core &lt;em&gt;build&lt;/em&gt; task to the pipeline. The only setting that will change for this task is the &lt;code&gt;Working Directory&lt;/code&gt; which will have the value of &lt;code&gt;ModelTrainer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-19.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Although not required, when finished configuring the task, click &lt;code&gt;Save &amp;amp; Queue&lt;/code&gt; → &lt;code&gt;Save&lt;/code&gt; to save and comment the changes to the pipeline.&lt;/p&gt;
&lt;h3&gt;3. Train and persist the ML.NET Model&lt;/h3&gt;
&lt;p&gt;Now that our console application is built, it's time to run it in order to train and persist the ML.NET model. To do so, we'll add another .NET Core task. The difference is that the &lt;code&gt;Command&lt;/code&gt; setting will now be configured with the &lt;code&gt;run&lt;/code&gt; value.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-20.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Working Directory&lt;/code&gt; will be set to &lt;code&gt;ModelTrainer&lt;/code&gt; like in the previous task.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-21.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Remember to save and comment the new changes to the pipeline.&lt;/p&gt;
&lt;h3&gt;4. Copy ML.NET Model to Web API Directory&lt;/h3&gt;
&lt;p&gt;After the console application is run and the ML.NET model is trained, it is persisted in a file called &lt;code&gt;model.zip&lt;/code&gt; inside the &lt;code&gt;ModelTrainer&lt;/code&gt; directory. We can use this persisted version of the model to make predictions from both the console application or any other application of our choice. In this case, we'll be making predictions via an ASP.NET Core Web API. In order for our API to reference this file, we need to copy it into the root directory of our &lt;code&gt;ModelApi&lt;/code&gt; directory. A way to perform that task is via bash script. To add a bash script to our pipeline, all we need to do is add a Bash task to it.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-22.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Once added to our pipeline, it's time to configure the task. We'll set the &lt;code&gt;Type&lt;/code&gt; setting to &lt;code&gt;Inline&lt;/code&gt; which will bring up a text box for us to type in the script.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-23.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Inside of the text box, enter the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# Write your commands here

cp ../ModelTrainer/model.zip .

# Use the environment variables input below to pass secret variables to this script
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command will copy the &lt;code&gt;model.zip&lt;/code&gt; file from the &lt;code&gt;ModelTrainer&lt;/code&gt; directory to the &lt;code&gt;ModelApi&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;We can set the &lt;code&gt;Working Directory&lt;/code&gt; of this step to &lt;code&gt;ModelApi&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-24.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Once finished, save and comment the new changes to the pipeline.&lt;/p&gt;
&lt;h3&gt;5. Build ASP.NET Core Web API application&lt;/h3&gt;
&lt;p&gt;Now that we have the necessary files inside our &lt;code&gt;ModelApi&lt;/code&gt; application, it's time to build it. We'll add a .NET Core task to our pipeline and set the &lt;code&gt;Command&lt;/code&gt; to &lt;code&gt;build&lt;/code&gt;. The &lt;code&gt;Working Directory&lt;/code&gt; will be &lt;code&gt;ModelApi&lt;/code&gt; like the previous task.&lt;/p&gt;
&lt;p&gt;Save and comment the new changes to the pipeline when finished.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-25.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h3&gt;6. Build ASP.NET Core Web API Docker Image&lt;/h3&gt;
&lt;p&gt;The method of deployment for the ASP.NET Core Web API application is via containers. Therefore, after building the application, we have to build a Docker image for it that can then be pushed to a Docker registry of your choice. To build a Docker image, we'll add a Docker task to our pipeline.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-26.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;When we configure the task, we'll start off by setting the &lt;code&gt;Container Registry Type&lt;/code&gt; to &lt;code&gt;Container Registry&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-27.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;This will prompt the setup of a service connection to a Docker registry if one does not already exist.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-28.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;The Docker registry type we'll be using is Docker Hub. Give the connection a name, enter the credentials to your Docker Hub account and click &lt;code&gt;Verify this connection&lt;/code&gt; to make sure that your credentials are valid and a connection can be established with Docker Hub. When finished click &lt;code&gt;OK&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-29.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Command&lt;/code&gt; setting will be &lt;code&gt;build&lt;/code&gt; so we can leave the default as is as well as the &lt;code&gt;Dockerfile&lt;/code&gt; setting which will use the Dockerfile in the root &lt;code&gt;mlnetazdevopssample&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-30.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Finally, we'll configure the &lt;code&gt;Image name&lt;/code&gt; setting. The convention we'll use is &lt;code&gt;&amp;lt;docker-hub-username&amp;gt;/&amp;lt;image-name&amp;gt;&lt;/code&gt;. In my case, &lt;code&gt;lqdev&lt;/code&gt; is my Docker Hub username and I'll name my image &lt;code&gt;mlnetazdevopssample&lt;/code&gt; resulting in &lt;code&gt;lqdev/mlnetazdevopssample&lt;/code&gt;. Additionally, check the &lt;code&gt;Include latest tag&lt;/code&gt; checkbox to have every build be the latest as opposed to tagging it with versions numbers.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-31.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Remember to save and comment the recent changes to the pipeline.&lt;/p&gt;
&lt;h3&gt;7. Push Docker Image to Docker Hub&lt;/h3&gt;
&lt;p&gt;The last step in our CI pipeline is to push our newly built image to Docker Hub. To do so we'll use anoter Docker task.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-32.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Like in the previous task, we'll set the &lt;code&gt;Container registry type&lt;/code&gt; to &lt;code&gt;Container Registry&lt;/code&gt;. Set the &lt;code&gt;Docker registry service connection&lt;/code&gt; to the most recently created connection by selecting it from the dropdown. We'll be changing our &lt;code&gt;Command&lt;/code&gt; to &lt;code&gt;push&lt;/code&gt; and set the &lt;code&gt;Image name&lt;/code&gt; to the name of the image built in the previous step. The naming convention is &lt;code&gt;&amp;lt;docker-hub-username&amp;gt;/&amp;lt;image-name&amp;gt;:latest&lt;/code&gt;. The latest tag was added by our previous Docker build task so make sure that you include it in this task.&lt;/p&gt;
&lt;p&gt;Once finished, click &lt;code&gt;Save &amp;amp; Queue&lt;/code&gt; → &lt;code&gt;Save &amp;amp; Queue&lt;/code&gt;. As opposed to only clicking &lt;code&gt;Save&lt;/code&gt;, this action will manually trigger the CI pipeline.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-33.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Don't forget to comment your changes and click &lt;code&gt;Save &amp;amp; queue&lt;/code&gt; to kick off the CI pipeline.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-34.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h3&gt;Monitoring the Build&lt;/h3&gt;
&lt;p&gt;When the build starts, you can click on &lt;code&gt;Builds&lt;/code&gt; under the &lt;code&gt;Pipelines&lt;/code&gt; section on the left pane.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-35.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Select the first build from the list to get more details on the build.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-36.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;This will take you to the logs which show the status of the pipeline near real-time.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-37.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h3&gt;Confirming CI Pipeline Success&lt;/h3&gt;
&lt;p&gt;If the build is successful, navigate to &lt;a href="https://hub.docker.com/"&gt;https://hub.docker.com/&lt;/a&gt; to check whether the Docker image was pushed to the registry.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-38.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h2&gt;The Continuous Delivery (CD) Pipeline&lt;/h2&gt;
&lt;p&gt;Now that we have our CI pipeline set up which will build and package our application, it's time to deploy it. We could do this ourselves or automate it using a CD pipeline. Our application wil be deployed to Azure Container Instances which is an Azure service that offers a quick way to run containers without having to worry about the management of virtual machines or orchestration services. The steps involved in our CD pipeline are the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create Azure Resource Group for deployment&lt;/li&gt;
&lt;li&gt;Deploy application to Azure Container Instances.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;CD Pipeline Setup&lt;/h3&gt;
&lt;p&gt;To get started setting up a CD pipeline, from the Azure DevOps project main page, hover over &lt;code&gt;Pipelines&lt;/code&gt; and click on &lt;code&gt;Releases&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-39.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Once in that page, click on &lt;code&gt;New pipeline&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-40.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;As with our CI pipeline, we'll start off with an &lt;code&gt;Empty Job&lt;/code&gt; which we'll configure at a later time.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-41.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h3&gt;Triggering Deployments&lt;/h3&gt;
&lt;p&gt;Once our pipeline is created, it's time to configure it. The first thing we'll want to do is add an artifact. An artifact can be a variety of things including the output of our build pipeline. In our case, the end our CI pipeline will be the trigger for our CD pipeline. To add an artifact, click &lt;code&gt;Add an artifact&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-42.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;In the configuration form, set the &lt;code&gt;Source type&lt;/code&gt; to &lt;code&gt;Build&lt;/code&gt; and the &lt;code&gt;Source&lt;/code&gt; to the name of the CI pipeline created in the previous steps. When finished, click &lt;code&gt;Add&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-43.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;After configuring our artifact, it's time to configure the steps in the CD pipeline. To do so, click on the &lt;code&gt;Stage 1&lt;/code&gt; option in the &lt;code&gt;Stages&lt;/code&gt; section of the release pipeline page and change the name to something more descriptive.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-44.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;When finished, close out the form and click on the hyperlink below the stage title.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-45.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;You should now be on a page similar to the CI pipeline job configuration page. On this page, we'll want to click on the &lt;code&gt;Agent Job&lt;/code&gt; panel to set the &lt;code&gt;Agent pool&lt;/code&gt; setting to &lt;code&gt;Hosted Ubuntu 1604&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-46.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Once that is complete, it's time to configure the tasks in the CD pipeline.&lt;/p&gt;
&lt;h3&gt;1. Create Azure Resource Group&lt;/h3&gt;
&lt;p&gt;Start off adding an &lt;code&gt;Azure CLI&lt;/code&gt; task to the pipeline. In this task we'll create a resource group in Azure to which we'll deploy our application to.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-47.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Before doing anything else, link DevOps to an Azure Subscription by selecting one from the dropdown and clicking &lt;code&gt;Authorize&lt;/code&gt; which will prompt you to authenticate your subscription.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-48.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Once an Azure subscription has been linked, let's change the &lt;code&gt;Script Location&lt;/code&gt; setting to &lt;code&gt;Inline Script&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-49.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;Inline Script&lt;/code&gt; text box enter the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az group create --name mlnetazdevopssampleresourcegroup --location eastus
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This script will create a resource group in Azure called &lt;code&gt;mlnetazdevopssampleresourcegroup&lt;/code&gt; that is located in &lt;code&gt;eastus&lt;/code&gt;. Both of these are configurable to your preference.&lt;/p&gt;
&lt;h3&gt;2. Deploy Docker Image to Azure Container Instances&lt;/h3&gt;
&lt;p&gt;The next and final step in our CD pipeline is the deployment to Azure Container Instances. To deploy our application, we'll add another &lt;code&gt;Azure CLI&lt;/code&gt; task. This time, since we already configured our &lt;code&gt;Azure subscription&lt;/code&gt; in the previous task, we can select the service connection as opposed to a subscription from the dropdown.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-50.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Like in the previous task, our script will be inline.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-51.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;Inline Script&lt;/code&gt; text box enter the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az container create --resource-group mlnetazdevopssampleresourcegroup --name mlnetcontainer --image lqdev/mlnetazdevopssample:latest --ports 80 --ip-address public
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This script creates a container in the resource group created by the previous task of the pipeline with the name &lt;code&gt;mlnetcontainer&lt;/code&gt; from the Docker image that was pushed to Docker Hub by the CI pipeline. Additionally, it opens up port 80 and assigns a publicly accessible IP address for the container to be accessed externally.&lt;/p&gt;
&lt;p&gt;Once this step has been configured, make sure to save and comment all your changes by clicking &lt;code&gt;Save&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-52.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-53.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Then, to make it easily recognizable, edit the name of the pipeline by hovering near &lt;code&gt;New release pipeline&lt;/code&gt; and clicking on the pencil icon.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-54.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Make sure to save and comment your changes.&lt;/p&gt;
&lt;h2&gt;Automating CI/CD Pipelines&lt;/h2&gt;
&lt;p&gt;In the previous steps, we configured CI and CD pipelines. However, we have still not fully automated the triggers that kick off both of these.&lt;/p&gt;
&lt;h3&gt;CI Pipeline Trigger&lt;/h3&gt;
&lt;p&gt;First, lets start off by automating the CI pipeline. To do so, go the project's main page, hover over &lt;code&gt;Pipelines&lt;/code&gt; and click on &lt;code&gt;Builds&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-55.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;This will take you to the CI pipeline page. While on this page, click &lt;code&gt;Edit&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-56.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Then, click on &lt;code&gt;Triggers&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-57.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Once on this page, check the &lt;code&gt;Enable continous integration&lt;/code&gt; checkbox and save and comment your changes by clicking &lt;code&gt;Save &amp;amp; Queue&lt;/code&gt; → &lt;code&gt;Save&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-58.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h3&gt;CD Pipeline Trigger&lt;/h3&gt;
&lt;p&gt;To automate the CD pipeline trigger, click on &lt;code&gt;Releases&lt;/code&gt; under the &lt;code&gt;Pipelines&lt;/code&gt; page to automate the CD pipeline.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-59.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Once on the CD pipeline's page, click &lt;code&gt;Edit&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-60.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Then, click on the lightning icon in the Artifacts section which will show a configuration form. In this form, toggle the &lt;code&gt;Continuous deployment trigger&lt;/code&gt; setting to &lt;code&gt;Enabled&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-61.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;When finished, save and comment your changes.&lt;/p&gt;
&lt;h2&gt;Running CI/CD Pipelines&lt;/h2&gt;
&lt;p&gt;Although going forward builds and deployments will be started when new changes are checked into the master branch of the &lt;code&gt;mlnetazdevopssample&lt;/code&gt; repository, for demonstration purposes we will manually kick off the CI/CD pipelines we have just configured. To do so, click on &lt;code&gt;Builds&lt;/code&gt; under the &lt;code&gt;Pipelines&lt;/code&gt; section on the left pane.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-62.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;From the CI pipeline page click &lt;code&gt;Queue&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-63.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;This will prompt a modal to show up in which you can just click &lt;code&gt;Queue&lt;/code&gt; to start the build.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-64.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;This will kick off a new CI build which subsequently will also kick off the CD pipeline of your application.&lt;/p&gt;
&lt;h2&gt;Testing The Deployment&lt;/h2&gt;
&lt;p&gt;If all is successful, a Docker image of an ASP.NET Core Web API application will be deployed to Azure Container Instances which can be accessed via a public IP address.&lt;/p&gt;
&lt;p&gt;To see whether the deployment worked, navigate to &lt;a href="https://portal.azure.com/"&gt;https://portal.azure.com/&lt;/a&gt; and click on &lt;code&gt;Resource groups&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-65.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;At this point, you should see the resource group that was created by the CD pipeline. If that's the case, click on it.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-66.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;This will then show a page that displays the container that was deployed to this resource group. Click on that.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-67.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;The container page will display diagnostic and configuration information about the container. The information we're interested in is the &lt;code&gt;IP address&lt;/code&gt;. Hover to the right of it and click on the icon that says &lt;code&gt;Click to copy&lt;/code&gt;. This will copy the address to the clipboard.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-68.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;In an application like Postman or Insomnia, make an HTTP POST request to &lt;code&gt;http://&amp;lt;ip-address&amp;gt;/api/predict&lt;/code&gt; where &lt;code&gt;ip-address&lt;/code&gt; is the public IP address of the container in Azure with the following body.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;SepalLength&amp;quot;:3.3,
    &amp;quot;SepalWidth&amp;quot;:1.6,
    &amp;quot;PetalLength&amp;quot;:0.2,
    &amp;quot;PetalWidth&amp;quot;:5.1
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If successful, the response will be &lt;code&gt;Iris-virginica&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/azdevops-mlnet-aci/azdevops-mlnet-69.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this writeup, we operationalized the building, packaging and deployment of an ML.NET application that predicts the class of an Iris flower using a variety of mesurements with Azure DevOps. We created both a Continous Integration as well as a Continous Delivery pipeline which deploys the Docker image of an ASP.NET Core Web API to Azure Container Instances. Keep in mind this is just one way of doing it and Azure DevOps is flexible in how all of these tasks and workflows are configured to meet your requirements. Happy coding!&lt;/p&gt;
&lt;h6&gt;Resources&lt;/h6&gt;
&lt;p&gt;&lt;a href="https://github.com/dotnet/machinelearning-samples"&gt;ML.NET Samples&lt;/a&gt;
&lt;a href="https://www.youtube.com/watch?v=bUTBBS1TECc"&gt;DevOps for Data Science&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/azdevops-mlnet-aci?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/azdevops-mlnet-aci</guid>
      <pubDate>2018-11-26 23:50:23</pubDate>
      <category>#mlnet</category>
      <category>#machinelearning</category>
      <category>#ai</category>
      <category>#dotnet</category>
      <category>#dotnetcore</category>
      <category>#devops</category>
      <category>#azure</category>
      <category>#docker</category>
      <category>#containers</category>
      <category>#microsoft</category>
      <category>#artificialintelligence</category>
      <category>#programming</category>
      <category>#webapi</category>
      <category>#aci</category>
      <category>#development</category>
    </item>
    <item>
      <title>Blogging Tools</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/blog-tools?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/blog-tools&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;TLDR&lt;/h2&gt;
&lt;p&gt;These are the tools I use to write this blog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Editor&lt;/strong&gt;: &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Markup&lt;/strong&gt;: &lt;a href="https://daringfireball.net/projects/markdown/"&gt;Markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Static Site Generator&lt;/strong&gt;: &lt;a href="https://hexo.io/"&gt;Hexo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Blog Theme&lt;/strong&gt;: &lt;a href="https://github.com/probberechts/hexo-theme-cactus"&gt;Cactus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hosting&lt;/strong&gt;: &lt;a href="https://www.namecheap.com/"&gt;Namecheap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I have gotten a few questions regarding what I use to write this blog so I thought it would be a good idea to write a post about it. In this writeup I'll discuss what I use for writing posts as well as building and hosting the site.&lt;/p&gt;
&lt;h2&gt;Writing&lt;/h2&gt;
&lt;h3&gt;Editor&lt;/h3&gt;
&lt;p&gt;For writing, I use Visual Studio Code. Visual Studio Code is an open source, cross-platform text editor. One of the nice features it includes is Markdown Preview. Markdown Preview allows you to see how the content would look when rendered as a web page. Additionally, Visual Studio Code allows you to split panes. What that provides is the ability to write and preview the content side-by-side.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/blog-tools/blog-tools-1.PNG" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h3&gt;Markup&lt;/h3&gt;
&lt;p&gt;I write my content in Markdown. Markdown is a plain text formatting syntax that allows authors to write easy-to-read and easy-to-write plain text formats that are then converted to HTML. It can take a while to get used to the syntax especially when including images, links and code snippets but once you get comfortable with it, it becomes natural and no different than writing sentences in plain text. While this is not a requirement for everyone, since a fair amount of my content includes code snippets, Markdown makes it easy to include such content in the language of choice.&lt;/p&gt;
&lt;h2&gt;Website&lt;/h2&gt;
&lt;h3&gt;Static Site Generation&lt;/h3&gt;
&lt;p&gt;While Markdown helps me format my plain text, at some point it needs to be converted into HTML or some format that can be rendered on the web. To help with that, I use Hexo. Hexo is an open source blog framework that allows you to write content using Markdown or other languages and generates static files which have a theme applied to them. Hexo is built on JavaScript, therefore you'll need NodeJS to configure it.&lt;/p&gt;
&lt;h3&gt;Theme&lt;/h3&gt;
&lt;p&gt;Hexo has numerous themes that can be used to style your site. The one I use is the &lt;code&gt;Cactus&lt;/code&gt; theme.&lt;/p&gt;
&lt;h2&gt;Hosting&lt;/h2&gt;
&lt;p&gt;For hosting, I use Namecheap. There is no special reason for this other than I had a slot available in an existing hosting plan. My method of deployment is via FTP. Since my site is all static assets, I upload them into a folder that is publicly accessible. With that in mind, by having my site made up of static assets I have the flexibility of hosting it on an S3 bucket or Azure Storage. This is the method I'm looking to use for deployments and hosting in the near future. Here is a &lt;a href="https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-static-website"&gt;link&lt;/a&gt; that details how to host a static site on Azure Storage.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/blog-tools?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/blog-tools</guid>
      <pubDate>2018-11-18 00:07:36</pubDate>
      <category>#markdown</category>
      <category>#vscode</category>
      <category>#tools</category>
      <category>#development</category>
      <category>#blogging</category>
    </item>
    <item>
      <title>Serverless Machine Learning with ML.NET and Azure Functions</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/serverless-machine-learning-mlnet-azure-functions?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/serverless-machine-learning-mlnet-azure-functions&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In a previous blog &lt;a href="http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/"&gt;post&lt;/a&gt;, I explored how to build and deploy machine learning models built with the &lt;code&gt;ML.NET&lt;/code&gt; framework using an ASP.NET Core Web API, Docker and Azure Container Instances. While this is certainly a good way to deploy such models especially those that are critical and require high availability and/or consist of long-running processes, it's not the case when those requirements are not needed. In such cases serverless computing makes more sense from a cost and resource utilization standpoint. Therefore, in this blog post I will go over how to train a classification model with &lt;code&gt;ML.NET&lt;/code&gt; and deploy it using Azure Functions. Source code for this post can be found at the following &lt;a href="https://github.com/lqdev/azfnmlnetdemo"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Prior to starting, make sure you have all of the necessary software to build this project. Although this project was built on a system running Ubuntu 16.04 it should work cross-platform.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest"&gt;Azure CLI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local"&gt;Azure Functions Core Tools Version 2.x&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.microsoft.com/net/download"&gt;.NET Core SDK 2.0+&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Set Up Azure Environment&lt;/h2&gt;
&lt;p&gt;Before writing any code we want to configure our Azure environment. To do so we'll be using the Azure CLI. Although in these examples I am providing the resource group name, storage account name and function application name feel free to use one of your choosing. Naming is not as important for resource group or storage account but definitely is the case for the application.&lt;/p&gt;
&lt;p&gt;Fist we want to log into our account using the following command&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az login
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will guide you through a series of prompts that will eventually result in you being logged in. To make sure you are logged in you can use the &lt;code&gt;account&lt;/code&gt; command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az account list
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following output should appear if successfull.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;[
  {
    &amp;quot;cloudName&amp;quot;: &amp;quot;AzureCloud&amp;quot;,
    &amp;quot;id&amp;quot;: &amp;quot;&amp;lt;YOUR-ID&amp;gt;&amp;quot;,
    &amp;quot;isDefault&amp;quot;: true,
    &amp;quot;name&amp;quot;: &amp;quot;Pay-As-You-Go&amp;quot;,
    &amp;quot;state&amp;quot;: &amp;quot;Enabled&amp;quot;,
    &amp;quot;tenantId&amp;quot;: &amp;quot;&amp;lt;YOUR-TENANT-ID&amp;gt;&amp;quot;,
    &amp;quot;user&amp;quot;: {
      &amp;quot;name&amp;quot;: &amp;quot;&amp;lt;YOUR-USERNAME&amp;gt;&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;user&amp;quot;
    }
  }
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we want to create a resource group to contain all of our Azure resources for this application.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az group create --name azfnmlnetdemo --location eastus
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once our resource group is created, it's time to start adding resources for it. First we'll add a storage account which will contain our trained model.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az storage account create --name azfnmlnetdemostorage --location eastus --resource-group azfnmlnetdemo --sku Standard_LRS
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we'll create an Serverless Function Application and link it to our storage account. We'll want to create a unique name for it. An easy way to do so is to add the date to the end of the name of our application (i.e. myappname20180816).&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az functionapp create --name azfnmlnetdemo20180821 --storage-account azfnmlnetdemostorage --consumption-plan-location eastus --resource-group azfnmlnetdemo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final step in the environment setup is to set the runtime of our Serverless Function Application in the Application Settings to &lt;code&gt;beta&lt;/code&gt; which supports &lt;code&gt;.NET Core&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az functionapp config appsettings set --name azfnmlnetdemo20180821 --resource-group azfnmlnetdemo --settings FUNCTIONS_EXTENSION_VERSION=beta
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we're ready to build our machine learning model and upload it to our storage account&lt;/p&gt;
&lt;h2&gt;Building The Model&lt;/h2&gt;
&lt;p&gt;Once our environment is set up we can start building our solution. The first step is to create a directory and initialize our solution inside of it.&lt;/p&gt;
&lt;h3&gt;Set Up The Solution&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir azfnmlnetdemo
cd azfnmlnetdemo
dotnet new sln
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create The Model Project&lt;/h3&gt;
&lt;p&gt;Then we want to create a console project for our model and add it to our solution.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new console -o model
dotnet sln add model/model.csproj
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Add Dependencies&lt;/h3&gt;
&lt;p&gt;Since we’ll be using the &lt;code&gt;ML.NET&lt;/code&gt; framework, we need to add it to our model project.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd model
dotnet add package Microsoft.ML
dotnet restore
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Download The Data&lt;/h3&gt;
&lt;p&gt;Before we start training the model, we need to download the data we’ll be using to train. We do so by creating a directory called &lt;code&gt;data&lt;/code&gt; and downloading the data file onto there.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir data
curl -o data/iris.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we take a look at the data file, it should look something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa
5.0,3.4,1.5,0.2,Iris-setosa
4.4,2.9,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Train The Model&lt;/h3&gt;
&lt;p&gt;Now that we have all our dependencies set up, it’s time to train our model. I leveraged the demo that is used on the &lt;a href="https://www.microsoft.com/net/learn/machine-learning-and-ai/get-started-with-ml-dotnet-tutorial"&gt;ML.NET Getting-Started website&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Define Data Structures&lt;/h4&gt;
&lt;p&gt;In the root directory of our &lt;code&gt;model&lt;/code&gt; project, let’s create two classes called &lt;code&gt;IrisData&lt;/code&gt; and &lt;code&gt;IrisPrediction&lt;/code&gt; which will define our features and predicted attribute respectively. Both of them will use &lt;code&gt;Microsoft.ML.Runtime.Api&lt;/code&gt; to add the property attributes.&lt;/p&gt;
&lt;p&gt;Here is what our &lt;code&gt;IrisData&lt;/code&gt; class looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Microsoft.ML.Runtime.Api;

namespace model
{
    public class IrisData
    {
        [Column(&amp;quot;0&amp;quot;)]
        public float SepalLength;

        [Column(&amp;quot;1&amp;quot;)]
        public float SepalWidth;

        [Column(&amp;quot;2&amp;quot;)]
        public float PetalLength;

        [Column(&amp;quot;3&amp;quot;)]
        public float PetalWidth;

        [Column(&amp;quot;4&amp;quot;)]
        [ColumnName(&amp;quot;Label&amp;quot;)]
        public string Label;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, here is the &lt;code&gt;IrisPrediction&lt;/code&gt; class:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Microsoft.ML.Runtime.Api;

namespace model
{
    public class IrisPrediction
    {
        [ColumnName(&amp;quot;PredictedLabel&amp;quot;)]
        public string PredictedLabels;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Build the Training Pipeline&lt;/h4&gt;
&lt;p&gt;The way the &lt;code&gt;ML.NET&lt;/code&gt; computations process data is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. Therefore, we can create a class called &lt;code&gt;Model&lt;/code&gt; to perform all of these tasks for us.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Microsoft.ML.Data;
using Microsoft.ML;
using Microsoft.ML.Runtime.Api;
using Microsoft.ML.Trainers;
using Microsoft.ML.Transforms;
using Microsoft.ML.Models;
using System;
using System.Threading.Tasks;

namespace model
{
    class Model
    {

        public static async Task&amp;lt;PredictionModel&amp;lt;IrisData,IrisPrediction&amp;gt;&amp;gt; Train(LearningPipeline pipeline, string dataPath, string modelPath)
        {
            // Load Data
            pipeline.Add(new TextLoader(dataPath).CreateFrom&amp;lt;IrisData&amp;gt;(separator:','));

            // Transform Data
            // Assign numeric values to text in the &amp;quot;Label&amp;quot; column, because
            // only numbers can be processed during model training
            pipeline.Add(new Dictionarizer(&amp;quot;Label&amp;quot;));

            // Vectorize Features
            pipeline.Add(new ColumnConcatenator(&amp;quot;Features&amp;quot;, &amp;quot;SepalLength&amp;quot;, &amp;quot;SepalWidth&amp;quot;, &amp;quot;PetalLength&amp;quot;, &amp;quot;PetalWidth&amp;quot;));

            // Add Learner
            pipeline.Add(new StochasticDualCoordinateAscentClassifier());

            // Convert Label back to text
            pipeline.Add(new PredictedLabelColumnOriginalValueConverter() {PredictedLabelColumn = &amp;quot;PredictedLabel&amp;quot;});

            // Train Model
            var model = pipeline.Train&amp;lt;IrisData,IrisPrediction&amp;gt;();

            // Persist Model
            await model.WriteAsync(modelPath);

            return model;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition to building our pipeline and training our machine learning model, the &lt;code&gt;Model&lt;/code&gt; class also serialized and persisted the model for future use in a file called &lt;code&gt;model.zip&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Test The Model&lt;/h3&gt;
&lt;p&gt;Now that we have our data structures and model training pipeline set up, it’s time to test everything to make sure it’s working. We’ll put our logic inside of our &lt;code&gt;Program.cs&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;
using Microsoft.ML;

namespace model
{
    class Program
    {
        static void Main(string[] args)
        {

            string dataPath = &amp;quot;model/data/iris.txt&amp;quot;;

            string modelPath = &amp;quot;model/model.zip&amp;quot;;

            var model = Model.Train(new LearningPipeline(),dataPath,modelPath).Result;

            // Test data for prediction
            var prediction = model.Predict(new IrisData()
            {
                SepalLength = 3.3f,
                SepalWidth = 1.6f,
                PetalLength = 0.2f,
                PetalWidth = 5.1f
            });

            Console.WriteLine($&amp;quot;Predicted flower type is: {prediction.PredictedLabels}&amp;quot;);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All set to run. We can do so by entering the following command from our solution directory:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet run -p model/model.csproj
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the application has been run, the following output should display on the console.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;Automatically adding a MinMax normalization transform, use 'norm=Warn' or
'norm=No' to turn this behavior off.Using 2 threads to train.
Automatically choosing a check frequency of 2.Auto-tuning parameters: maxIterations = 9998.
Auto-tuning parameters: L2 = 2.667734E-05.
Auto-tuning parameters: L1Threshold (L1/L2) = 0.Using best model from iteration 882.
Not training a calibrator because it is not needed.
Predicted flower type is: Iris-virginica
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, you’ll notice that a file called &lt;code&gt;model.zip&lt;/code&gt; was created in the root directory of our model project. This persisted model can now be used outside of our application to make predictions, but first we need to upload it to our Azure Storage account.&lt;/p&gt;
&lt;h3&gt;Upload The Model&lt;/h3&gt;
&lt;p&gt;Now that we have a trained model and it has been persisted to the &lt;code&gt;model.zip&lt;/code&gt; file, it's time to upload it to Azure Storage so that it is available to our Azure Functions application.&lt;/p&gt;
&lt;p&gt;To get started with that, first we need the access keys for our storage account. You can get those by using the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az storage account keys list --account-name azfnmlnetdemostorage --resource-group azfnmlnetdemo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result of that command should return your primary and secondary keys. You can use either one for the following steps.&lt;/p&gt;
&lt;p&gt;Although we can upload directly to the account, it's best to create a container to upload our model to. To keep it simple, I'll call the container &lt;code&gt;models&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az storage container create --name models --account-key &amp;lt;YOUR-ACCOUNT-KEY&amp;gt; --account-name azfnmlnetdemostorage --fail-on-exist
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once our container's created, we can upload our &lt;code&gt;model.zip&lt;/code&gt; file to it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az storage blob upload --container-name models --account-name azfnmlnetdemostorage --file model/model.zip --name model.zip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To verify that the file has been uploaded, you can list the files inside the &lt;code&gt;models&lt;/code&gt; storage container.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az storage blob list --container-name models --account-name azfnmlnetdemostorage --output table
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That command should produce output similar to that below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;Name       Blob Type    Blob Tier    Length    Content Type     Last Modified              Snapshot
---------  -----------  -----------  --------  ---------------  -------------------------  ----------
model.zip  BlockBlob                 4373      application/zip  2018-08-21T19:26:09+00:00
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That's all there is to the upload process. It's now time to build our Azure Functions Application&lt;/p&gt;
&lt;h2&gt;Build The Azure Functions Application&lt;/h2&gt;
&lt;h3&gt;Initialize Azure Function Project&lt;/h3&gt;
&lt;p&gt;In our solution directory, we want to create a new directory for our Azure Function project&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir serverlessfunctionapp
dotnet sln add serverlessfunctionapp/serverlessfunctionapp.csproj
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we can scaffold an Azure Functions project inside our newly created &lt;code&gt;serverlessfunctionapp&lt;/code&gt; project directory using Azure Functions Core Tools&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;cd serverlessfunctionapp
func init
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point you will be prompted to select the runtime for your application. For this application select &lt;code&gt;dotnet&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This will generate a few files in the &lt;code&gt;serverlessfunctionapp&lt;/code&gt; directory. Keep in mind though that this does not create the function.&lt;/p&gt;
&lt;h3&gt;Add Dependencies&lt;/h3&gt;
&lt;p&gt;Before we create any functions, we need to add the dependencies for our Azure Functions application. Since we'll be using &lt;code&gt;Microsoft.ML&lt;/code&gt; in our Azure Function application, we'll need to add it as a dependency. From the &lt;code&gt;serverlessfunctionapp&lt;/code&gt; enter the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet add package Microsoft.ML
dotnet restore
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create Serverless Function&lt;/h3&gt;
&lt;p&gt;Once we've added the dependencies it's time to create a new function. To do so we'll use the Azure Functions Core Tools &lt;code&gt;new&lt;/code&gt; command. Although not required, it's good practice to separate functions and related files into their own directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir Predict
cd Predict
func new
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this time you will be prompted to select a template. For our classification model, we'll be using an HttpTrigger which is exactly what it sounds like. An HTTP request is what calls or invokes our function. With that being said, select the &lt;code&gt;HttpTrigger&lt;/code&gt; option.&lt;/p&gt;
&lt;p&gt;You will then be prompted to enter a name for your function. You can use any name but to make things easy, name it the same as the directory the function is in. Once that process is complete, there should be a file called &lt;code&gt;Predict.cs&lt;/code&gt; inside our &lt;code&gt;serverlessfunctionapp/Predict&lt;/code&gt; directory. This is where we'll write the logic for our application.&lt;/p&gt;
&lt;h3&gt;Define Data Structures&lt;/h3&gt;
&lt;p&gt;We'll also be making use of the IrisData and IrisPrediction classes inside our &lt;code&gt;Predict&lt;/code&gt; function. Therefore, we need to create classes for them inside our &lt;code&gt;Predict&lt;/code&gt; directory. The content will be the same as when we trained our model with the exception of the namespace which will now be &lt;code&gt;serverlessfunctionapp.Predict&lt;/code&gt;. The content of those files should look like the code below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;//IrisData.cs
using Microsoft.ML.Runtime.Api;

namespace serverlessfunctionapp.Predict
{
    public class IrisData
    {
        [Column(&amp;quot;0&amp;quot;)]
        public float SepalLength;

        [Column(&amp;quot;1&amp;quot;)]
        public float SepalWidth;

        [Column(&amp;quot;2&amp;quot;)]
        public float PetalLength;

        [Column(&amp;quot;3&amp;quot;)]
        public float PetalWidth;

        [Column(&amp;quot;4&amp;quot;)]
        [ColumnName(&amp;quot;Label&amp;quot;)]
        public string Label;
    }
}

//IrisPrediction.cs
using Microsoft.ML.Runtime.Api;

namespace serverlessfunctionapp.Predict
{
    public class IrisPrediction
    {
        [ColumnName(&amp;quot;PredictedLabel&amp;quot;)]
        public string PredictedLabels;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Write Function Logic&lt;/h3&gt;
&lt;p&gt;With our dependencies and data structures set up, it's time to write our function logic to make predictions. The first thing we want to do is replace the &lt;code&gt;Run&lt;/code&gt; method inside the &lt;code&gt;Predict&lt;/code&gt; class with the following code.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public static IActionResult Run(
    [HttpTrigger(AuthorizationLevel.Function, &amp;quot;get&amp;quot;, &amp;quot;post&amp;quot;, Route = null)]HttpRequest req,
    [Blob(&amp;quot;models/model.zip&amp;quot;, FileAccess.Read, Connection = &amp;quot;AzureWebJobsStorage&amp;quot;)] Stream serializedModel,
    TraceWriter log)
{
    // Workaround for Azure Functions Host
    if (typeof(Microsoft.ML.Runtime.Data.LoadTransform) == null ||
        typeof(Microsoft.ML.Runtime.Learners.LinearClassificationTrainer) == null ||
        typeof(Microsoft.ML.Runtime.Internal.CpuMath.SseUtils) == null ||
        typeof(Microsoft.ML.Runtime.FastTree.FastTree) == null)
    {
        log.Error(&amp;quot;Error loading ML.NET&amp;quot;);
        return new StatusCodeResult(500);
    }

    //Read incoming request body
    string requestBody = new StreamReader(req.Body).ReadToEnd();

    log.Info(requestBody);

    //Bind request body to IrisData object
    IrisData data = JsonConvert.DeserializeObject&amp;lt;IrisData&amp;gt;(requestBody);

    //Load prediction model
    var model = PredictionModel.ReadAsync&amp;lt;IrisData, IrisPrediction&amp;gt;(serializedModel).Result;

    //Make prediction
    IrisPrediction prediction = model.Predict(data);

    //Return prediction
    return (IActionResult)new OkObjectResult(prediction.PredictedLabels);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are a few notable change worth looking at. One of them is the workaround at the beginning of the function.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;if (typeof(Microsoft.ML.Runtime.Data.LoadTransform) == null ||
    typeof(Microsoft.ML.Runtime.Learners.LinearClassificationTrainer) == null ||
    typeof(Microsoft.ML.Runtime.Internal.CpuMath.SseUtils) == null ||
    typeof(Microsoft.ML.Runtime.FastTree.FastTree) == null)
{
    log.Error(&amp;quot;Error loading ML.NET&amp;quot;);
    return new StatusCodeResult(500);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are some issues with Azure Functions and ML.NET Assemblies which are being worked on by both teams at Microsoft (see &lt;a href="https://github.com/Azure/azure-functions-host/issues/3190"&gt;Github Issue&lt;/a&gt;). In the meantime, it's safe to just include that code in there.&lt;/p&gt;
&lt;p&gt;The other addition to note is the method signature. As you can see, I have added an additional parameter called &lt;code&gt;serializedModel&lt;/code&gt; which is decorated by the &lt;code&gt;Blob&lt;/code&gt; attribute.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;[Blob(&amp;quot;models/model.zip&amp;quot;, FileAccess.Read, Connection = &amp;quot;AzureWebJobsStorage&amp;quot;)] Stream serializedModel
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What this code is doing is telling the function to import the blob &lt;code&gt;model.zip&lt;/code&gt; as a &lt;code&gt;Stream&lt;/code&gt; and bind it to &lt;code&gt;serializedModel&lt;/code&gt;. Using additional arguments, I tell my function to only have &lt;code&gt;Read&lt;/code&gt; access to the &lt;code&gt;model.zip&lt;/code&gt; blob inside the &lt;code&gt;models&lt;/code&gt; container which can be accessed with the &lt;code&gt;AzureWebJobsStorage&lt;/code&gt; connection string. Right now that last part might seem confusing, but this is something we configured when we set up the Azure environment and linked &lt;code&gt;azfnmlnetdemostorage&lt;/code&gt; account with our &lt;code&gt;azfnmlnetdemo20180821&lt;/code&gt; serverless function app using the &lt;code&gt;--storage-account&lt;/code&gt; option. Although the production environment is configured, if we try to test our application locally we won't be able to access our storage account because we have not configured the connection string locally. We can do so by looking in the &lt;code&gt;local.settings.json&lt;/code&gt; file inside our &lt;code&gt;serverlessfunctionapp&lt;/code&gt; directory. The contents should look like the following.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;IsEncrypted&amp;quot;: false,
  &amp;quot;Values&amp;quot;: {
    &amp;quot;AzureWebJobsStorage&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;AzureWebJobsDashboard&amp;quot;: &amp;quot;&amp;quot;,
    &amp;quot;FUNCTIONS_WORKER_RUNTIME&amp;quot;: &amp;quot;dotnet&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our function running locally will look in this file, try to find &lt;code&gt;AzureWebJobsStorage&lt;/code&gt; and use the connection string value in the &lt;code&gt;Predict&lt;/code&gt; function. To get the connection string for our &lt;code&gt;azfnmlnetdemostorage&lt;/code&gt; account, enter the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az storage account show-connection-string --name azfnmlnetdemostorage
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output of that command should look like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;connectionString&amp;quot;: &amp;quot;&amp;lt;YOUR-CONNECTION-STRING&amp;gt;&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, you just need to copy the value of &lt;code&gt;connectionString&lt;/code&gt; to your &lt;code&gt;local.settings.json&lt;/code&gt; file and replace the current empty string for &lt;code&gt;AzureWebJobsStorage&lt;/code&gt;. It's important to note that it's okay to paste the connection string in here since the &lt;code&gt;local.settings.json&lt;/code&gt; file is not committed to version control. (See &lt;code&gt;.gitignore&lt;/code&gt; inside &lt;code&gt;serverlessfunctionapp&lt;/code&gt; directory). Now the application is ready to be tested locally.&lt;/p&gt;
&lt;h3&gt;Testing The Function Locally&lt;/h3&gt;
&lt;p&gt;To test the application, first build your project by entering the following command from the &lt;code&gt;serverlessfunctionapp&lt;/code&gt; directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet build
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, navigate to the build directory &lt;code&gt;./bin/Debug/netstandard2.0&lt;/code&gt; and enter the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;func host start
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, using a tool like Postman or Insomnia make an HTTP POST request to the &lt;code&gt;http://localhost:7071/api/Predict&lt;/code&gt; endpoint with the following body:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;SepalLength&amp;quot;: 3.3,
  &amp;quot;SepalWidth&amp;quot;: 1.6,
  &amp;quot;PetalLength&amp;quot;: 0.2,
  &amp;quot;PetalWidth&amp;quot;: 5.1
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If everything is set up correctly, you should receive the following output&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;Iris-virginica
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once satisfied with testing, press &lt;code&gt;Ctrl + C&lt;/code&gt; to stop the application.&lt;/p&gt;
&lt;h2&gt;Deploy To Azure&lt;/h2&gt;
&lt;h3&gt;Push Build&lt;/h3&gt;
&lt;p&gt;Great! Now on to the final step, deploying our application to production. Since we already configured everything it should only require a few commands to do so.&lt;/p&gt;
&lt;p&gt;First, make sure you are logged in. Using Azure Functions Core Tools log in with the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;func azure login
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like with the Azure CLI, you will follow a series of prompts to log into your account.&lt;/p&gt;
&lt;p&gt;Once you have successfully logged in, it's time to publish our application to Azure. From the &lt;code&gt;serverlessfunctionapp&lt;/code&gt; directory enter the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;func azure functionapp publish azfnmlnetdemo20180821
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When our deployment is complete, we can check whether our function was published successfully by using the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;func azure functionapp list-functions azfnmlnetdemo20180821
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output should look similar to that below.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;Functions in azfnmlnetdemo20180821:
    Predict - [httpTrigger]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Configure Platform&lt;/h3&gt;
&lt;p&gt;For the last part of the deployment step, we'll need to head over to the Azure Portal. To do so, visit &lt;a href="https://portal.azure.com"&gt;https://portal.azure.com&lt;/a&gt; and log in.&lt;/p&gt;
&lt;p&gt;Once logged in, type the name of your application into the search bar at the top of the page and select your Azure Function application of type &lt;code&gt;App Service&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/serverless-machine-learning-mlnet-azure-functions/azfnmlnetdemo1.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Then, from the accordion element on the left, select the top-most item with your appplication name on it. Then, select the &lt;code&gt;Platform features&lt;/code&gt; tab and open the &lt;code&gt;Application settings&lt;/code&gt; option.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/serverless-machine-learning-mlnet-azure-functions/azfnmlnetdemo2.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;When the &lt;code&gt;Application settings&lt;/code&gt; page loads, change the &lt;code&gt;Platform&lt;/code&gt; setting to &lt;code&gt;64-bit&lt;/code&gt;. The reason for this is &lt;code&gt;ML.NET&lt;/code&gt; has to be built and run on a 64-bit environment due to some of its native dependencies.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/serverless-machine-learning-mlnet-azure-functions/azfnmlnetdemo3.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;That's all there is to it.&lt;/p&gt;
&lt;h3&gt;Test The Deployed Function&lt;/h3&gt;
&lt;p&gt;Now it's time to test our deployed function. We can do so from the portal by going back to the accordion and selecting the function name below the &lt;code&gt;Functions&lt;/code&gt; parent element and clicking on the &lt;code&gt;Test&lt;/code&gt; button on the far right. Doing so will show a form that will allow us to test our application. Make sure the &lt;code&gt;HTTP method&lt;/code&gt; option is set to POST. In the text area for the &lt;code&gt;Request body&lt;/code&gt; paste the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;SepalLength&amp;quot;: 3.3,
  &amp;quot;SepalWidth&amp;quot;: 1.6,
  &amp;quot;PetalLength&amp;quot;: 0.2,
  &amp;quot;PetalWidth&amp;quot;: 5.1
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the form is filled in, click &lt;code&gt;Run&lt;/code&gt; at the top of the page and if successful &lt;code&gt;Iris-virginica&lt;/code&gt; should show up in the &lt;code&gt;Output&lt;/code&gt; area.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/serverless-machine-learning-mlnet-azure-functions/azfnmlnetdemo4.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;To test the function outside the portal, you can click on the &lt;code&gt;Get function URL&lt;/code&gt; link next to the &lt;code&gt;Run&lt;/code&gt; button and make an HTTP POST request using that link.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this writeup, we trained a classification model that predicts a class of flower using Microsoft's &lt;code&gt;ML.NET&lt;/code&gt; framework. Then, we exposed this model for inference via an Azure Functions serverless application. In doing so, we can more efficiently manage our cost as well as our resource utilization. Happy coding!&lt;/p&gt;
&lt;h6&gt;Resources&lt;/h6&gt;
&lt;p&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/azure-functions/scripts/functions-cli-create-serverless"&gt;Create a function app for serverless code execution&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://docs.microsoft.com/en-us/azure/storage/common/storage-azure-cli"&gt;Using the Azure CLI 2.0 with Azure Storage&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local"&gt;Work with Azure Functions Core Tools&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/serverless-machine-learning-mlnet-azure-functions?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/serverless-machine-learning-mlnet-azure-functions</guid>
      <pubDate>2018-08-21 19:13:47</pubDate>
      <category>#serverless</category>
      <category>#azurefunctions</category>
      <category>#mlnet</category>
      <category>#machinelearning</category>
      <category>#dotnet</category>
      <category>#dotnetcore</category>
      <category>#csharp</category>
      <category>#microsoft</category>
      <category>#devops</category>
      <category>#artificialintelligence</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Hacker News Vue Top Stories Client</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/hn-vue-topstories-client?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/hn-vue-topstories-client&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Over the past few years, I've used AngularJS as my front-end framework for most web projects. With the release of Angular and React, I tried both of them and while both are great tools the learning took significantly longer because I also had to learn the bundler systems and in the case of Angular, TypeScript. Although the CLI tools go a long way towards automating a lot of the setup process, building an application from scratch in these frameworks is not intuitive especially for a newcomer. In the early days of Vue, I was able to attend a Meetup presentation by &lt;a href="https://twitter.com/youyuxi"&gt;Evan You&lt;/a&gt; which went over the philosophy of Vue as well as its capabilities. I was instantly interested because of how easy it was to get started. Since that presentation, I have not had time to try it but with AngularJS being phased out, I felt it was about time to start exploring another framework. To get a better understanding of the framework's features prior to using CLI tools, I created a standalone application that relies only on NPM packages. In this writeup, I will create a Hacker News client with Vue that calls the Hacker News API and displays the top 50 stories. Source code for this project can be found in the following &lt;a href="https://github.com/lqdev/hnvuedemo"&gt;repository&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://nodejs.org/en/"&gt;Node&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Initialize Project&lt;/h2&gt;
&lt;p&gt;The first thing we want to do is create a directory for our project and initialize it with NPM&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir hnvuedemo
cd hnvuedemo
npm init -y
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Install Dependencies&lt;/h2&gt;
&lt;p&gt;Next, we want to install our dependencies. (Express is optional)&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install --save vue axios express
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Create Server (Optional)&lt;/h2&gt;
&lt;p&gt;We can set up a server to host and serve our content. This is optional though because the application should still work when the &lt;code&gt;index.html&lt;/code&gt; file is opened in the browser.&lt;/p&gt;
&lt;p&gt;In our root project directory, we can create the &lt;code&gt;server.js&lt;/code&gt; file and add the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-javascript"&gt;var express = require(&amp;quot;express&amp;quot;);

const PORT = 3000 || process.env.PORT;

var app = express();

app.use(express.static(&amp;quot;.&amp;quot;));

app.get(&amp;quot;/&amp;quot;, (req, res) =&amp;gt; {
  res.sendFile(&amp;quot;index.html&amp;quot;);
});

app.listen(PORT, () =&amp;gt; {
  console.log(`Listening on port ${PORT}`);
});
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Create Application&lt;/h2&gt;
&lt;h3&gt;Create The View&lt;/h3&gt;
&lt;p&gt;Now it's time to create our application. Let's start by scaffolding the &lt;code&gt;index.html&lt;/code&gt; which is where our application will be displayed.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;script src=&amp;quot;./node_modules/vue/dist/vue.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src=&amp;quot;./node_modules/axios/dist/axios.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
    &amp;lt;!--App Content Here--&amp;gt;

    &amp;lt;!--App Scripts Here--&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want to add the scripts to the &lt;code&gt;Vue&lt;/code&gt; and &lt;code&gt;axios&lt;/code&gt; packages in our &lt;code&gt;head&lt;/code&gt; element. We'll hold off on the content for now.&lt;/p&gt;
&lt;h3&gt;Get Data&lt;/h3&gt;
&lt;p&gt;Our data source for this project will be the Hacker News &lt;a href="https://github.com/HackerNews/API"&gt;API&lt;/a&gt;. To interact with it, we'll be using the &lt;code&gt;axios&lt;/code&gt; NPM package. The data collection happens over several HTTP requests, therefore to help with it we'll be creating a file called &lt;code&gt;apihelpers.js&lt;/code&gt; in our root project directory that will contain functions to get and manipulate the data.&lt;/p&gt;
&lt;h4&gt;Get Top Stories&lt;/h4&gt;
&lt;p&gt;The first thing we want to do is get a list of top stories. We can do so via the &lt;code&gt;/topstories&lt;/code&gt; endpoint. The response of this request returns a list of ids which can then be used in conjunction with the &lt;code&gt;/item/{id}&lt;/code&gt; endpoint to get the individual story data. We can then validate the story object to make sure it is a story as opposed to a user or job posting and return a list of all the story objects. The functions that will help us with that are &lt;code&gt;getIds&lt;/code&gt;, &lt;code&gt;isStory&lt;/code&gt; and &lt;code&gt;extractStories&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-javascript"&gt;/**
 * Checks whether the item is a story
 * @param {Object} story - Story object
 */
function isStory(story) {
  return story.type == &amp;quot;story&amp;quot;;
}

/**
 * Gets ids of stories
 * @param {string} url - Url to fetch story ids
 */
function getIds(url) {
  return axios.get(url);
}

/**
 * 
 * @param {Array&amp;lt;Object&amp;gt;} - List of resolved promises 
 */
function extractStories(...responses) {
    var stories = responses.map(story =&amp;gt; story = story.data);
    return stories.filter(isStory);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Create Vue App&lt;/h3&gt;
&lt;p&gt;Now that we have our helper methods to make calls to the API, we can create our application.&lt;/p&gt;
&lt;p&gt;We can start by creating our Vue instance which is where our application will live. We can create a file called &lt;code&gt;app.js&lt;/code&gt; in our root project directory and add the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-javascript"&gt;//Components Go Here

new Vue({
  el: &amp;quot;#app&amp;quot;,
  data: {
    title: &amp;quot;Hacker News Vue Simple Reader&amp;quot;,
    loading: true,
    topStories: []
  },
  methods: {
    getTopStories: function() {
      getIds(
        &amp;quot;https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty&amp;quot;
      )
        .then(ids =&amp;gt; buildRequest(ids.data.splice(0, 50))) //Only get first 50
        .then(getStories)
        .then(axios.spread(extractStories))
        .then(stories =&amp;gt; {
          this.topStories = stories;
          this.loading = false;
        })
        .catch(e =&amp;gt; console.log(e));
    }
  },
  created: function() {
    this.getTopStories();
  }
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There's a lot happening here, so let's break it down based on the properties of the object passed to the Vue constructor.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;el&lt;/code&gt; property is like a selector which tells the application which element it should operate on. The value &lt;code&gt;#app&lt;/code&gt; tells it to look for an element where the &lt;code&gt;id&lt;/code&gt; attribute is &lt;code&gt;app&lt;/code&gt;. This of course can be anything of your choosing.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;data&lt;/code&gt; property specifies the properties that contain the data of our application. These properties are reactive which means that whenever they change, that change is reflected in our view. As such, even if they have no value when the application starts, in order for them to automatically change when data is passed to them, they need to be declared in the &lt;code&gt;data&lt;/code&gt; property. In our application, we have a &lt;code&gt;title&lt;/code&gt; property which will be the title of our web page, &lt;code&gt;topStories&lt;/code&gt; which is where we'll store the list of Hacker News stories and &lt;code&gt;loading&lt;/code&gt; which we'll use to let us know when data is being loaded into our application.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;methods&lt;/code&gt; property, we define functions that we want our application to use. In this case, I created the &lt;code&gt;getTopStories&lt;/code&gt; method which chains together all the functions defined in our &lt;code&gt;apihelpers.js&lt;/code&gt; file to return the top 50 stories on Hacker News.&lt;/p&gt;
&lt;p&gt;Finally, Vue has instance lifecycle hooks. The &lt;code&gt;created&lt;/code&gt; property defines what should happen when our instance is created. In our case, we want to call the &lt;code&gt;getTopProperties&lt;/code&gt; method which is defined in our &lt;code&gt;methods&lt;/code&gt; property to load the data, update our &lt;code&gt;topStories&lt;/code&gt; data property and set &lt;code&gt;loading&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; because our data has loaded successfully.&lt;/p&gt;
&lt;h3&gt;Add App to View&lt;/h3&gt;
&lt;p&gt;Now that we have created the logic of our application, it's time to add it to our view. We can do so by adding the &lt;code&gt;apihelpers.js&lt;/code&gt; and &lt;code&gt;app.js&lt;/code&gt; files to our &lt;code&gt;index.html&lt;/code&gt; file via &lt;code&gt;script&lt;/code&gt; elements.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;script src=&amp;quot;./node_modules/vue/dist/vue.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src=&amp;quot;./node_modules/axios/dist/axios.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
    &amp;lt;!--App Content Here--&amp;gt;

    &amp;lt;!--App Scripts Here--&amp;gt;
    &amp;lt;script src=&amp;quot;apihelpers.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src=&amp;quot;app.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Display Data&lt;/h3&gt;
&lt;p&gt;Although we can now use our application logic inside of our view, we still can't see any of it because we have not added elements to display it. To view our application data, we can add the following code to our &lt;code&gt;index.html&lt;/code&gt; file below the &lt;code&gt;&amp;lt;!--App Content Here--&amp;gt;&lt;/code&gt; section.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;div id=&amp;quot;app&amp;quot;&amp;gt;
    &amp;lt;h2&amp;gt;{{title}}&amp;lt;/h2&amp;gt;
    &amp;lt;h5 v-if=&amp;quot;loading&amp;quot;&amp;gt;Loading...&amp;lt;/h5&amp;gt;
    &amp;lt;story v-for=&amp;quot;story in topStories&amp;quot; :key=&amp;quot;story.id&amp;quot; :story=&amp;quot;story&amp;quot;&amp;gt;&amp;lt;/story&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final &lt;code&gt;index.html&lt;/code&gt; contents should look like the content below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;script src=&amp;quot;./node_modules/vue/dist/vue.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src=&amp;quot;./node_modules/axios/dist/axios.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
    &amp;lt;div id=&amp;quot;app&amp;quot;&amp;gt;
        &amp;lt;h2&amp;gt;{{title}}&amp;lt;/h2&amp;gt;
        &amp;lt;h5 v-if=&amp;quot;loading&amp;quot;&amp;gt;Loading...&amp;lt;/h5&amp;gt;
        &amp;lt;story v-for=&amp;quot;story in topStories&amp;quot; :key=&amp;quot;story.id&amp;quot; :story=&amp;quot;story&amp;quot;&amp;gt;&amp;lt;/story&amp;gt;
    &amp;lt;/div&amp;gt;
    &amp;lt;script src=&amp;quot;apihelpers.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;script src=&amp;quot;app.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As mentioned earlier, we created an element, in this case a &lt;code&gt;div&lt;/code&gt; that has the &lt;code&gt;id&lt;/code&gt; attribute with a value of &lt;code&gt;app&lt;/code&gt;. This is how our application knows where to display our content. Inside of an &lt;code&gt;h2&lt;/code&gt; element, we display our &lt;code&gt;title&lt;/code&gt; data property. Below it, we have an &lt;code&gt;h5&lt;/code&gt; element that displays the text &amp;quot;Loading...&amp;quot;. However, this is to only be displayed when our &lt;code&gt;loading&lt;/code&gt; data property is &lt;code&gt;true&lt;/code&gt;. If not, it should not be visible. We can achieve this conditional rendering via the &lt;code&gt;v-if&lt;/code&gt; directive. This directive evaluates the expression inside of it and renders content based on its truthiness.&lt;/p&gt;
&lt;p&gt;Finally, there's one last piece that looks like an element, but not one of the built-in HTML elements. So then, what is it? It's a component. The Vue website defines a component as &amp;quot;...a reusable Vue instance with a name: in this case, &lt;code&gt;&amp;lt;story&amp;gt;&lt;/code&gt;. We can use this component as a custom element inside a root Vue instance...&amp;quot;. In our component, the &lt;code&gt;v-for&lt;/code&gt; directive is what it sounds like. It creates a sequence of &lt;code&gt;story&lt;/code&gt; components based on a list of objects defined in our &lt;code&gt;Vue&lt;/code&gt; instance's &lt;code&gt;data&lt;/code&gt; property. Our &lt;code&gt;story&lt;/code&gt; component iterates over the &lt;code&gt;topStories&lt;/code&gt; data property and assigns the value of the individual object in the list to the variable &lt;code&gt;story&lt;/code&gt;. We bind the &lt;code&gt;id&lt;/code&gt; property of the &lt;code&gt;story&lt;/code&gt; object to the &lt;code&gt;key&lt;/code&gt; attribute of the component and pass in the entire object to the component via the &lt;code&gt;story&lt;/code&gt; prop. A prop is a custom attribute that you can register for the component. We can use props to pass data into the component. In all cases, the &lt;code&gt;:&lt;/code&gt; prefix on the attributes and props of the component are shorthand for the &lt;code&gt;v-bind&lt;/code&gt; directive which dynamically binds an expression to an attribute or component prop.&lt;/p&gt;
&lt;p&gt;With all that being said your next question might be, how does the view know about this component? The answer is it doesn't at least not until you define it which is what we'll do next in our &lt;code&gt;app.js&lt;/code&gt; file. In order for our &lt;code&gt;story&lt;/code&gt; component to be usable, we need to define it above the instantiation of our &lt;code&gt;Vue&lt;/code&gt; instance. The definition looks like the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-javascript"&gt;Vue.component('story',{
    props: ['story'],
    template: `
        &amp;lt;div&amp;gt;
            &amp;lt;h3&amp;gt;&amp;lt;a :href=&amp;quot;story.url&amp;quot; target=&amp;quot;_blank&amp;quot;&amp;gt;{{story.title}}&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;
        &amp;lt;/div&amp;gt;
    `
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like our &lt;code&gt;Vue&lt;/code&gt; instance, let's unpack what's happening here. The first parameter is a string with the name of our component. The &lt;code&gt;props&lt;/code&gt; property is a list of the props or custom attributes that are accepted by our component. The &lt;code&gt;template&lt;/code&gt; property is where we set up the template of what will be rendered in place of our component in the view. In our case, we'll have an &lt;code&gt;h3&lt;/code&gt; element with a nested &lt;code&gt;a&lt;/code&gt; element whose &lt;code&gt;href&lt;/code&gt; attribute is the &lt;code&gt;url&lt;/code&gt; property of our &lt;code&gt;story&lt;/code&gt; object and the display text is the &lt;code&gt;title&lt;/code&gt; property of our &lt;code&gt;story&lt;/code&gt; object.&lt;/p&gt;
&lt;h2&gt;Run Application&lt;/h2&gt;
&lt;p&gt;At this point, our application should be ready to run. You can either start the server with the following command &lt;code&gt;npm start&lt;/code&gt;  and navigate to &lt;code&gt;http://localhost:3000&lt;/code&gt; or open the &lt;code&gt;index.html&lt;/code&gt; page in the browser of your choice. The result should look like the screenshot below:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/hn-vue-topstories-client/hnvueclient.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this writeup, I built a standalone Hacker News client that displays the top 50 stories using Vue while also highlighting some of its main features. Overall, I really enjoyed building this application. The setup process was extremely simple and after a few hours looking through the excellent documentation and working through some bugs, it took less than two hours to get this application up and running from start to finish. For prototyping and learning purposes, Vue is great because you're able to take advantage of the core features of the framework without having too much overhead. Although this may not be the most appropriate way to build production-ready applications, it's nice to know you can have a modern web application with minimal setup required. My next steps will be to continue learning some of the other features the framework provides and eventually build up to learning how to use the CLI tools.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/hn-vue-topstories-client?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/hn-vue-topstories-client</guid>
      <pubDate>2018-06-21 18:30:38</pubDate>
      <category>#vue</category>
      <category>#web-development</category>
      <category>#programming</category>
      <category>#api</category>
      <category>#development</category>
      <category>#frontend</category>
      <category>#js</category>
    </item>
    <item>
      <title>Classification with F# ML.NET Models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/mlnet-classification-fsharp?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/mlnet-classification-fsharp&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In a previous &lt;a href="http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/"&gt;post&lt;/a&gt;, I detailed how to build and deploy C# &lt;code&gt;ML.NET&lt;/code&gt; models with &lt;code&gt;Docker&lt;/code&gt; and &lt;code&gt;ASP.NET Core&lt;/code&gt;. With inspiration from &lt;a href="https://twitter.com/csharpfritz"&gt;Jeff Fritz&lt;/a&gt;, I have been learning F# for the past week and a half or so. When trying to think of projects to start practicing my F#, porting over the code I had built in C# naturally came to mind. After overcoming many obstacles and with much guidance from &lt;a href="https://github.com/voronoipotato"&gt;Alan Ball&lt;/a&gt; and &lt;a href="https://twitter.com/isaac_abraham"&gt;Isaac Abraham&lt;/a&gt; whose F# &lt;a href="https://www.amazon.com/Get-Programming-guide-NET-developers/dp/1617293997/ref=sr_1_1?ie=UTF8&amp;amp;qid=1528929802&amp;amp;sr=8-1&amp;amp;keywords=get+programming+with+F%23"&gt;book&lt;/a&gt; I highly recommend, I was able to successfully port over the main parts of my code which highlight &lt;code&gt;ML.NET&lt;/code&gt; functionality. In this writeup, I will port a C# &lt;code&gt;ML.NET&lt;/code&gt; classification model to F# which predicts the type of flower based on four numerical measurement inputs. I tried to keep the organization of this post nearly identical to that of the C# article where possible. Sample code for this project can be found at the following &lt;a href="https://github.com/lqdev/fsmlnetdemo"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;This project was built on a Linux PC but should work cross-platform on Mac and Windows.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.microsoft.com/net/download/linux"&gt;.NET Core SDK 2.0+&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fsharp.org/use/linux/"&gt;Ionide Extension - Option 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.nuget.org/packages/Microsoft.ML/"&gt;ML.NET v 0.2.0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Setting Up The Project&lt;/h2&gt;
&lt;p&gt;The first thing we want to do is create a folder for our solution.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir fsharpmlnetdemo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we want to create a solution inside our newly created folder.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;cd fsharpmlnetdemo
dotnet new sln
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Building The Model&lt;/h2&gt;
&lt;h3&gt;Setting Up The Model Project&lt;/h3&gt;
&lt;p&gt;First, we want to create the project. From the solution folder enter:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new console -o model -lang f#
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we want to add this new project to our solution.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet sln add model/model.fsproj
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Adding Dependencies&lt;/h3&gt;
&lt;p&gt;Since we’ll be using the &lt;code&gt;ML.NET&lt;/code&gt; framework, we need to add it to our &lt;code&gt;model&lt;/code&gt; project.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet add model/model.fsproj package Microsoft.ML
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Download The Data&lt;/h3&gt;
&lt;p&gt;Before we start training the model, we need to download the data we’ll be using to train. We do so by downloading the data file into our root solution directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -o iris-data.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we take a look at the data file, it should look something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa
5.0,3.4,1.5,0.2,Iris-setosa
4.4,2.9,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Train Model&lt;/h3&gt;
&lt;p&gt;Now that we have all our dependencies set up, it’s time to build our model. I leveraged the demo that is used on the &lt;code&gt;ML.NET&lt;/code&gt; Getting-Started &lt;a href="https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet/get-started/linux/ubuntu16-04"&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Defining Data Structures&lt;/h4&gt;
&lt;p&gt;At the time of this writing, &lt;code&gt;ML.NET&lt;/code&gt; version &lt;code&gt;0.2.0&lt;/code&gt; does not fully support F# Records. A workaround for this are mutable classes. Not really inline with F# paradigms, but it should be good enough.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;Program.fs&lt;/code&gt; file of our &lt;code&gt;model&lt;/code&gt; project directory, let’s create two mutable classes called &lt;code&gt;IrisData&lt;/code&gt; and &lt;code&gt;IrisPrediction&lt;/code&gt; which will define our features and predicted attribute respectively. Both of them will use &lt;code&gt;Microsoft.ML.Runtime.Api&lt;/code&gt; to add the property attributes.&lt;/p&gt;
&lt;p&gt;Here is what our &lt;code&gt;IrisData&lt;/code&gt; class looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;type IrisData() =
    [&amp;lt;Column(ordinal = &amp;quot;0&amp;quot;);DefaultValue&amp;gt;]
    val mutable public SepalLength: float32

    [&amp;lt;Column(ordinal = &amp;quot;1&amp;quot;);DefaultValue&amp;gt;]
    val mutable public SepalWidth: float32

    [&amp;lt;Column(ordinal = &amp;quot;2&amp;quot;);DefaultValue&amp;gt;]
    val mutable public PetalLength:float32

    [&amp;lt;Column(ordinal = &amp;quot;3&amp;quot;);DefaultValue&amp;gt;]
    val mutable public PetalWidth:float32

    [&amp;lt;Column(ordinal = &amp;quot;4&amp;quot;,name=&amp;quot;Label&amp;quot;);DefaultValue&amp;gt;]
    val mutable public Label: string
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, here is the &lt;code&gt;IrisPrediction&lt;/code&gt; class:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;type IrisPrediction() =
    [&amp;lt;ColumnName &amp;quot;PredictedLabel&amp;quot;;DefaultValue&amp;gt;] val mutable public PredictedLabel : string
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Building Training Pipeline&lt;/h4&gt;
&lt;p&gt;The way the &lt;code&gt;ML.NET&lt;/code&gt; computations process is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. We can add that logic inside the &lt;code&gt;main&lt;/code&gt; function of our &lt;code&gt;Program.fs&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;let dataPath = &amp;quot;./iris-data.txt&amp;quot;

// Initialize Compute Graph
let pipeline = new LearningPipeline()

// Load Data
pipeline.Add((new TextLoader(dataPath).CreateFrom&amp;lt;IrisData&amp;gt;separator=','))

// Transform Data
// Assign numeric values to text in the &amp;quot;Label&amp;quot; column, because
// only numbers can be processed during model training
pipeline.Add(new Transforms.Dictionarizer(&amp;quot;Label&amp;quot;))

// Vectorize Features
pipeline.Add(new ColumnConcatenator(&amp;quot;Features&amp;quot;,&amp;quot;SepalLength&amp;quot;, &amp;quot;SepalWidth&amp;quot;, &amp;quot;PetalLength&amp;quot;, &amp;quot;PetalWidth&amp;quot;))

// Add Learner
pipeline.Add(new StochasticDualCoordinateAscentClassifier())

// Convert Label back to text
pipeline.Add(new ransforms.PredictedLabelColumnOriginalValueConverterPredictedLabelColumn = &amp;quot;PredictedLabel&amp;quot;))

//Train the model
let model = pipeline.Train&amp;lt;IrisData, IrisPrediction&amp;gt;()
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Testing Our Model&lt;/h4&gt;
&lt;p&gt;Now that we have our data structures and model trained, it’s time to test it to make sure it's working. Following our training operation, we can add the following code.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;// Test data for prediction
let testInstance = IrisData()
testInstance.SepalLength &amp;lt;- 3.3f
testInstance.SepalWidth &amp;lt;- 1.6f
testInstance.PetalLength &amp;lt;- 0.2f
testInstance.PetalWidth &amp;lt;- 5.1f

//Get Prediction
let prediction = model.Predict(testInstance)

//Output Prediction
printfn &amp;quot;Predicted flower type is: %s&amp;quot; prediction.PredictedLabel
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our final &lt;code&gt;Program.fs&lt;/code&gt; file should contain content similar to that below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;open System
open Microsoft.ML
open Microsoft.ML.Runtime
open Microsoft.ML.Runtime.Api
open Microsoft.ML.Data
open Microsoft.ML.Transforms
open Microsoft.ML.Trainers

type IrisData() =

    [&amp;lt;Column(ordinal = &amp;quot;0&amp;quot;);DefaultValue&amp;gt;] val mutable public SepalLength: float32
    [&amp;lt;Column(ordinal = &amp;quot;1&amp;quot;);DefaultValue&amp;gt;] val mutable public SepalWidth: float32
    [&amp;lt;Column(ordinal = &amp;quot;2&amp;quot;);DefaultValue&amp;gt;] val mutable public PetalLength:float32
    [&amp;lt;Column(ordinal = &amp;quot;3&amp;quot;);DefaultValue&amp;gt;] val mutable public PetalWidth:float32
    [&amp;lt;Column(ordinal = &amp;quot;4&amp;quot;,name=&amp;quot;Label&amp;quot;);DefaultValue&amp;gt;] val mutable public Label: string


type IrisPrediction() =

    [&amp;lt;ColumnName &amp;quot;PredictedLabel&amp;quot;;DefaultValue&amp;gt;] val mutable public PredictedLabel : string

[&amp;lt;EntryPoint&amp;gt;]
let main argv =

    let dataPath = &amp;quot;./iris-data.txt&amp;quot;

    // Initialize Compute Graph
    let pipeline = new LearningPipeline()

    // Load Data
    pipeline.Add((new TextLoader(dataPath)).CreateFrom&amp;lt;IrisData&amp;gt;(separator=','))

    // Transform Data
    // Assign numeric values to text in the &amp;quot;Label&amp;quot; column, because
    // only numbers can be processed during model training
    pipeline.Add(new Transforms.Dictionarizer(&amp;quot;Label&amp;quot;))

    // Vectorize Features
    pipeline.Add(new ColumnConcatenator(&amp;quot;Features&amp;quot;,&amp;quot;SepalLength&amp;quot;, &amp;quot;SepalWidth&amp;quot;, &amp;quot;PetalLength&amp;quot;, &amp;quot;PetalWidth&amp;quot;))

    // Add Learner
    pipeline.Add(new StochasticDualCoordinateAscentClassifier())

    // Convert Label back to text
    pipeline.Add(new Transforms.PredictedLabelColumnOriginalValueConverter(PredictedLabelColumn = &amp;quot;PredictedLabel&amp;quot;))

    //Train the model
    let model = pipeline.Train&amp;lt;IrisData, IrisPrediction&amp;gt;()

    // Test data for prediction
    let testInstance = IrisData()
    testInstance.SepalLength &amp;lt;- 3.3f
    testInstance.SepalWidth &amp;lt;- 1.6f
    testInstance.PetalLength &amp;lt;- 0.2f
    testInstance.PetalWidth &amp;lt;- 5.1f

    //Get Prediction
    let prediction = model.Predict(testInstance)

    //Output Prediction
    printfn &amp;quot;Predicted flower type is: %s&amp;quot; prediction.PredictedLabel
    0 // return an integer exit code
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All set to run. We can do so by entering the following command from our solution directory:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;dotnet run -p model/model.fsproj
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the application has been run, the following output should display on the console.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Using 2 threads to train.
Automatically choosing a check frequency of 2.
Auto-tuning parameters: maxIterations = 9998.
Auto-tuning parameters: L2 = 2.667734E-05.Auto-tuning parameters: L1Threshold (L1/L2) = 0.
Using best model from iteration 1066.Not training a calibrator because it is not needed.
Predicted flower type is: Iris-virginica
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we ported over a C# &lt;code&gt;ML.NET&lt;/code&gt; classification model to F# which predicts the class of flower based on numerical measurement inputs. While several workarounds needed to be made, &lt;code&gt;ML.NET&lt;/code&gt; is still in its infancy. As more people become involved and provide feedback hopefully in the near future, F# support and functionality will become more stable. Happy coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/mlnet-classification-fsharp?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/mlnet-classification-fsharp</guid>
      <pubDate>2018-06-13 18:19:05</pubDate>
      <category>#fsharp</category>
      <category>#dotnet</category>
      <category>#dotnetcore</category>
      <category>#programming</category>
      <category>#development</category>
      <category>#mlnet</category>
      <category>#machinelearning</category>
      <category>#artificialintelligence</category>
      <category>#functionalprogramming</category>
    </item>
    <item>
      <title>Organizing F# Modules Into Separate Files</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/fsharpmodulesample?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/fsharpmodulesample&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;After watching &lt;a href="https://twitter.com/csharpfritz"&gt;Jeff Fritz's&lt;/a&gt; F# Friday &lt;a href="https://www.twitch.tv/videos/268107540"&gt;Stream&lt;/a&gt;, I was inspired to do some tinkering of my own. I have looked at F# in the past, but often times have not done much with it. After the stream, I decided to keep plugging away at it and one of the first things I thought about doing was organizing my code into separate files. As simple as it may seem, it was no easy task and while there are many different ways of doing it, the method described in this writeup is one of the easiest I found. Keep in mind though that this may not be the appropriate way of doing it although it seems to work nicely for our purposes. In this solution I will create a set of mathematical operations that will be organized into a single module that will then be imported and used inside of my console application. Source code for this sample solution can be found at this &lt;a href="https://github.com/lqdev/fsharpmoduledemo"&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;This solution was built on a Mac, but should work on both Windows and Linux environments.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.microsoft.com/net/download/macos"&gt;.NET Core SDK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://fsharp.org/use/mac/"&gt;Ionide Extension - Option 1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Create Project&lt;/h2&gt;
&lt;p&gt;Once you have everything installed, you can create a new F# project by entering the following command into the console.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new console -lang f#
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Create Module&lt;/h2&gt;
&lt;p&gt;The goal of this solution will be to create functions that add, subtract, multiply and divide two numbers. Since all of these functions are mathematical operations, we can organize them inside a single module. Therefore, we can create a new file inside of our project called &lt;code&gt;Math.fs&lt;/code&gt; which will contain all of our functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;module Math
    let add x y = x + y
    let subtract x y = x - y
    let multiply x y = x * y
    let divide x y = x / y
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Compile File&lt;/h2&gt;
&lt;p&gt;Now that our file has been created, we need to add it to our list of files to be compiled by our project. To do so, we can add an entry for our &lt;code&gt;Math.fs&lt;/code&gt; file to the &lt;code&gt;ItemGroup&lt;/code&gt; property inside our &lt;code&gt;fsproj&lt;/code&gt; file. The contents of the file should look as follows after the addition:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-xml"&gt;&amp;lt;Project Sdk=&amp;quot;Microsoft.NET.Sdk&amp;quot;&amp;gt;

  &amp;lt;PropertyGroup&amp;gt;
    &amp;lt;OutputType&amp;gt;Exe&amp;lt;/OutputType&amp;gt;
    &amp;lt;TargetFramework&amp;gt;netcoreapp2.0&amp;lt;/TargetFramework&amp;gt;
  &amp;lt;/PropertyGroup&amp;gt;

  &amp;lt;ItemGroup&amp;gt;
    &amp;lt;Compile Include=&amp;quot;Math.fs&amp;quot; /&amp;gt;
    &amp;lt;Compile Include=&amp;quot;Program.fs&amp;quot; /&amp;gt;
  &amp;lt;/ItemGroup&amp;gt;

&amp;lt;/Project&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Import Module Into Console Application&lt;/h2&gt;
&lt;p&gt;Once we have our module created and our &lt;code&gt;fsproj&lt;/code&gt; file configured, we can call the module from our console application. To do so, we need to first import it into our &lt;code&gt;Program.fs&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;open Math
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once imported, we can call the &lt;code&gt;Math&lt;/code&gt; module functions from our &lt;code&gt;main&lt;/code&gt; function in the &lt;code&gt;Program.fs&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-fsharp"&gt;    printfn &amp;quot;Add: %i&amp;quot; (add 1 2)
    printfn &amp;quot;Subtract: %i&amp;quot; (subtract 1 2)
    printfn &amp;quot;Multiply: %i&amp;quot; (multiply 1 2)
    printfn &amp;quot;Divide: %i&amp;quot; (divide 4 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Run Application&lt;/h2&gt;
&lt;p&gt;Now that everything is set up, we can build and run our console application with the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet build
dotnet run
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output should be the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;Add: 3
Subtract: -1
Multiply: 2
Divide: 2
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this writeup, I went over how to create an F# console application that organizes a set of mathematical functions into a module whose contents are stored in a separate file. This is good for organization and to reduce clutter. Happy coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/fsharpmodulesample?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/fsharpmodulesample</guid>
      <pubDate>2018-06-05 20:56:52</pubDate>
      <category>#dotnet</category>
      <category>#fsharp</category>
      <category>#programming</category>
      <category>#development</category>
      <category>#functionalprogramming</category>
      <category>#dotnetcore</category>
    </item>
    <item>
      <title>Deploy .NET Machine Learning Models with ML.NET, ASP.NET Core, Docker and Azure Container Instances</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/deploy-netml-docker-aci?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/deploy-netml-docker-aci&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Leading up to and during MS Build 2018 Microsoft has released a wide range of products that reduce the complexity that comes with building and deploying software. The focus this year was on Machine Learning and Artificial Intelligence. Some of the products I found particularly interesting are &lt;a href="https://azure.microsoft.com/en-us/services/container-instances/"&gt;Azure Container Instances&lt;/a&gt; which makes it easier to run containerized applications without provisioning or managing servers and &lt;a href="https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet"&gt;ML.NET&lt;/a&gt; which is a .NET cross-platform machine learning framework. In this writeup, I will make use of both these products by creating a machine learning classification model with &lt;code&gt;ML.NET&lt;/code&gt;, exposing it via an ASP.NET Core Web API, packaging it into a Docker container and deploying it to the cloud via Azure Container Instances. Source code for this project can be found &lt;a href="https://github.com/lqdev/mlnetacidemo"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;This writeup assumes that you have some familiarity with Docker. The following software/dependencies are also required to build and deploy the sample application. It's important to note the application was built on a Ubuntu 16.04 PC, but all the software is cross-platform and should work on any environment.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/"&gt;Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest"&gt;Azure CLI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.microsoft.com/net/download/linux"&gt;.NET Core 2.0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://hub.docker.com/"&gt;Docker Hub Account&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Setting Up The Project&lt;/h2&gt;
&lt;p&gt;The first thing we want to do is create a folder for our solution.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir mlnetacidemo
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we want to create a solution inside our newly created folder.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;cd mlnetacidemo
dotnet new sln
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Building The Model&lt;/h2&gt;
&lt;p&gt;Inside our solution folder, we want to create a new console application which is where we'll build and test our machine learning model.&lt;/p&gt;
&lt;h3&gt;Setting Up the Model Project&lt;/h3&gt;
&lt;p&gt;First, we want to create the project. From the solution folder enter:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new console -o model
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we want to add this new project to our solution.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet sln mlnetacidemo.sln add model/model.csproj
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Adding Dependencies&lt;/h3&gt;
&lt;p&gt;Since we'll be using the &lt;code&gt;ML.NET&lt;/code&gt; framework, we need to add it to our &lt;code&gt;model&lt;/code&gt; project.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;cd model
dotnet add package Microsoft.ML
dotnet restore
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Download The Data&lt;/h3&gt;
&lt;p&gt;Before we start training the model, we need to download the data we'll be using to train. We do so by creating a directory called &lt;code&gt;data&lt;/code&gt; and downloading the data file onto there.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir data
curl -o data/iris.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we take a look at the data file, it should look something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa
5.0,3.4,1.5,0.2,Iris-setosa
4.4,2.9,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Train Model&lt;/h3&gt;
&lt;p&gt;Now that we have all our dependencies set up, it's time to build our model. I leveraged the demo that is used on the &lt;a href="https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet/get-started/linux/ubuntu16-04"&gt;ML.NET Getting-Started website&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Defining Data Structures&lt;/h4&gt;
&lt;p&gt;In the root directory of our &lt;code&gt;model&lt;/code&gt; project, let's create two classes called &lt;code&gt;IrisData&lt;/code&gt; and &lt;code&gt;IrisPrediction&lt;/code&gt; which will define our features and predicted attribute respectively. Both of them will use &lt;code&gt;Microsoft.ML.Runtime.Api&lt;/code&gt; to add the property attributes.&lt;/p&gt;
&lt;p&gt;Here is what our &lt;code&gt;IrisData&lt;/code&gt; class looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Microsoft.ML.Runtime.Api;

namespace model
{
public class IrisData
    {
        [Column(&amp;quot;0&amp;quot;)]
        public float SepalLength;

        [Column(&amp;quot;1&amp;quot;)]
        public float SepalWidth;

        [Column(&amp;quot;2&amp;quot;)]
        public float PetalLength;
        
        [Column(&amp;quot;3&amp;quot;)]
        public float PetalWidth;

        [Column(&amp;quot;4&amp;quot;)]
        [ColumnName(&amp;quot;Label&amp;quot;)]
        public string Label;
    }       
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, here is the &lt;code&gt;IrisPrediction&lt;/code&gt; class:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Microsoft.ML.Runtime.Api;

namespace model
{
    public class IrisPrediction
    {
        [ColumnName(&amp;quot;PredictedLabel&amp;quot;)]
        public string PredictedLabels;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Building Training Pipeline&lt;/h4&gt;
&lt;p&gt;The way the &lt;code&gt;ML.NET&lt;/code&gt; computations process is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. Therefore, we can create a class called &lt;code&gt;Model&lt;/code&gt; to perform all of these tasks for us.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Microsoft.ML.Data;
using Microsoft.ML;
using Microsoft.ML.Runtime.Api;
using Microsoft.ML.Trainers;
using Microsoft.ML.Transforms;
using Microsoft.ML.Models;
using System;
using System.Threading.Tasks;

namespace model
{
    class Model
    {
        
        public static async Task&amp;lt;PredictionModel&amp;lt;IrisData,IrisPrediction&amp;gt;&amp;gt; Train(LearningPipeline pipeline, string dataPath, string modelPath)
        {
            // Load Data
            pipeline.Add(new TextLoader(dataPath).CreateFrom&amp;lt;IrisData&amp;gt;(separator:',')); 

            // Transform Data
            // Assign numeric values to text in the &amp;quot;Label&amp;quot; column, because 
            // only numbers can be processed during model training   
            pipeline.Add(new Dictionarizer(&amp;quot;Label&amp;quot;));

            // Vectorize Features
            pipeline.Add(new ColumnConcatenator(&amp;quot;Features&amp;quot;, &amp;quot;SepalLength&amp;quot;, &amp;quot;SepalWidth&amp;quot;, &amp;quot;PetalLength&amp;quot;, &amp;quot;PetalWidth&amp;quot;));

            // Add Learner
            pipeline.Add(new StochasticDualCoordinateAscentClassifier());

            // Convert Label back to text 
            pipeline.Add(new PredictedLabelColumnOriginalValueConverter() {PredictedLabelColumn = &amp;quot;PredictedLabel&amp;quot;});

            // Train Model
            var model = pipeline.Train&amp;lt;IrisData,IrisPrediction&amp;gt;();

            // Persist Model
            await model.WriteAsync(modelPath);

            return model;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition to building our pipeline and training our machine learning model, the &lt;code&gt;Model&lt;/code&gt; class also serialized and persisted the model for future use in a file called &lt;code&gt;model.zip&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;Testing Our Model&lt;/h4&gt;
&lt;p&gt;Now that we have our data structures and model training pipeline set up, it's time to test everything to make sure it's working. We'll put our logic inside of our &lt;code&gt;Program.cs&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;
using Microsoft.ML;

namespace model
{
    class Program
    {
        static void Main(string[] args)
        {

            string dataPath = &amp;quot;model/data/iris.txt&amp;quot;;

            string modelPath = &amp;quot;model/model.zip&amp;quot;;

            var model = Model.Train(new LearningPipeline(),dataPath,modelPath).Result;

            // Test data for prediction
            var prediction = model.Predict(new IrisData() 
            {
                SepalLength = 3.3f,
                SepalWidth = 1.6f,
                PetalLength = 0.2f,
                PetalWidth = 5.1f
            });

            Console.WriteLine($&amp;quot;Predicted flower type is: {prediction.PredictedLabels}&amp;quot;);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All set to run. We can do so by entering the following command from our solution directory:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet run -p model/model.csproj
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the application has been run, the following output should display on the console.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;Automatically adding a MinMax normalization transform, use 'norm=Warn' or
'norm=No' to turn this behavior off.Using 2 threads to train.
Automatically choosing a check frequency of 2.Auto-tuning parameters: maxIterations = 9998.
Auto-tuning parameters: L2 = 2.667734E-05.
Auto-tuning parameters: L1Threshold (L1/L2) = 0.Using best model from iteration 882.
Not training a calibrator because it is not needed.
Predicted flower type is: Iris-virginica
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, you'll notice that a file called &lt;code&gt;model.zip&lt;/code&gt; was created in the root directory of our &lt;code&gt;model&lt;/code&gt; project. This persisted model can now be used outside of our application to make predictions, which is what we'll do next via an API.&lt;/p&gt;
&lt;h2&gt;Exposing The Model&lt;/h2&gt;
&lt;p&gt;Once a machine learning model is built, you want to deploy it so it can start making predictions. One way to do that is via a REST API. At it's core, all our API needs to do is accept data input from the client and respond back with a prediction. To help us do that, we'll be using an ASP.NET Core API.&lt;/p&gt;
&lt;h3&gt;Setting Up The API Project&lt;/h3&gt;
&lt;p&gt;The first thing we want to do is create the project.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new webapi -o api
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we want to add this new project to our solution&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet sln mlnetacidemo.sln add api/api.csproj
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Adding Dependencies&lt;/h3&gt;
&lt;p&gt;Because we'll be loading our model and making predictions via our API, we need to add the &lt;code&gt;ML.NET&lt;/code&gt; package to our &lt;code&gt;api&lt;/code&gt; project.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;cd api
dotnet add package Microsoft.ML
dotnet restore
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Referencing Our Model&lt;/h3&gt;
&lt;p&gt;In the previous step when we built our machine learning model, it was saved to a file called &lt;code&gt;model.zip&lt;/code&gt;. This is the file we'll be referencing in our API to help us make predictions. To reference it in our API, simply copy it from the model project directory into our &lt;code&gt;api&lt;/code&gt; project directory.&lt;/p&gt;
&lt;h3&gt;Creating Data Models&lt;/h3&gt;
&lt;p&gt;Our model was built using data structures &lt;code&gt;IrisData&lt;/code&gt; and &lt;code&gt;IrisPrediction&lt;/code&gt; to define the features as well as the predicted attribute. Therefore, when our model makes predictions via our API, it needs to reference these data types as well. As a result, we need to define &lt;code&gt;IrisData&lt;/code&gt; and &lt;code&gt;IrisPrediction&lt;/code&gt; classes inside of our &lt;code&gt;api&lt;/code&gt; project. The contents of the classes will be nearly identical to those in the &lt;code&gt;model&lt;/code&gt; project with the only exception of our namespace changing from &lt;code&gt;model&lt;/code&gt; to &lt;code&gt;api&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Microsoft.ML.Runtime.Api;

namespace api
{
    public class IrisData
    {
        [Column(&amp;quot;0&amp;quot;)]
        public float SepalLength;

        [Column(&amp;quot;1&amp;quot;)]
        public float SepalWidth;

        [Column(&amp;quot;2&amp;quot;)]
        public float PetalLength;
        
        [Column(&amp;quot;3&amp;quot;)]
        public float PetalWidth;

        [Column(&amp;quot;4&amp;quot;)]
        [ColumnName(&amp;quot;Label&amp;quot;)]
        public string Label;
    }    
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using Microsoft.ML.Runtime.Api;

namespace api
{
    public class IrisPrediction
    {
        [ColumnName(&amp;quot;PredictedLabel&amp;quot;)]
        public string PredictedLabels;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Building Endpoints&lt;/h3&gt;
&lt;p&gt;Now that our project is set up, it's time to add a controller that will handle prediction requests from the client. In the &lt;code&gt;Controllers&lt;/code&gt; directory of our &lt;code&gt;api&lt;/code&gt; project we can create a new class called &lt;code&gt;PredictController&lt;/code&gt; with a single &lt;code&gt;POST&lt;/code&gt; endpoint. The contents of the file should look like the code below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;
using System.Collections.Generic;
using System.Linq;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Mvc;
using Microsoft.ML;

namespace api.Controllers
{
    [Route(&amp;quot;api/[controller]&amp;quot;)]
    public class PredictController : Controller
    {
        // POST api/predict
        [HttpPost]
        public string Post([FromBody] IrisData instance)
        {
            var model = PredictionModel.ReadAsync&amp;lt;IrisData,IrisPrediction&amp;gt;(&amp;quot;model.zip&amp;quot;).Result;
            var prediction = model.Predict(instance);
            return prediction.PredictedLabels;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Testing The API&lt;/h3&gt;
&lt;p&gt;Once our &lt;code&gt;predict&lt;/code&gt; endpoint is set up, it's time to test it. From the root directory of our &lt;code&gt;mlnetacidemo&lt;/code&gt; solution, enter the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet run -p api/api.csproj
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a client like POSTMAN or Insomnia, send an HHTP POST request to the endpoint &lt;code&gt;http://localhost:5000/api/predict&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The body our request should look similar to the snippet below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
	&amp;quot;SepalLength&amp;quot;: 3.3,
	&amp;quot;SepalWidth&amp;quot;: 1.6,
	&amp;quot;PetalLength&amp;quot;: 0.2,
	&amp;quot;PetalWidth&amp;quot;: 5.1,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If successful, the output returned should equal &lt;code&gt;Iris-virginica&lt;/code&gt; just like our console application.&lt;/p&gt;
&lt;h2&gt;Packaging The Application&lt;/h2&gt;
&lt;p&gt;Great! Now that our application is successfully running locally, it's time to package it up into a Docker container and push it to Docker Hub.&lt;/p&gt;
&lt;h3&gt;Creating The Dockerfile&lt;/h3&gt;
&lt;p&gt;In our &lt;code&gt;mlnetacidemo&lt;/code&gt; solution directory, create a &lt;code&gt;Dockerfile&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-Dockerfile"&gt;FROM microsoft/dotnet:2.0-sdk AS build
WORKDIR /app

# copy csproj and restore as distinct layers
COPY *.sln .
COPY api/*.csproj ./api/
RUN dotnet restore

# copy everything else and build app
COPY api/. ./api/
WORKDIR /app/api
RUN dotnet publish -c release -o out


FROM microsoft/aspnetcore:2.0 AS runtime
WORKDIR /app
COPY api/model.zip .
COPY --from=build /app/api/out ./
ENTRYPOINT [&amp;quot;dotnet&amp;quot;, &amp;quot;api.dll&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Building Our Image&lt;/h3&gt;
&lt;p&gt;To build the image, we need to enter the following command into the command prompt. This make take a while because it needs to download the .NET Core SDK and ASP.NET Core runtime Docker images.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t &amp;lt;DOCKERUSERNAME&amp;gt;/&amp;lt;IMAGENAME&amp;gt;:latest .
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Test Image Locally&lt;/h3&gt;
&lt;p&gt;We need to test our image locally to make sure it can run on the cloud. To do so, we can use the &lt;code&gt;docker run&lt;/code&gt; command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p 5000:80 &amp;lt;DOCKERUSERNAME&amp;gt;/&amp;lt;IMAGENAME&amp;gt;:latest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although the API is exposing port 80, we bind it to the local port 5000 just to keep our prior API request intact. When sending a POST request to &lt;code&gt;http://localhost:5000/api/predict&lt;/code&gt; with the appropriate body, the response should again equal &lt;code&gt;Iris-virginica&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To stop the container, use &lt;code&gt;Ctrl + C&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Push to Docker Hub&lt;/h3&gt;
&lt;p&gt;Now that the Docker image is successfully running locally, it's time to push to Docker Hub. Again, we use the Docker CLI to do this.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;docker login
docker push &amp;lt;DOCKERUSERNAME&amp;gt;/&amp;lt;IMAGENAME&amp;gt;:latest
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Deploying To The Cloud&lt;/h2&gt;
&lt;p&gt;Now comes the final step which is to deploy and expose our machine learning model and API to the world. Our deployment will occur via Azure Container Instances because it requires almost no provisioning or management of servers.&lt;/p&gt;
&lt;h3&gt;Prepare Deployment Manifest&lt;/h3&gt;
&lt;p&gt;Although deployments can be performed inline in the command line, it's usually best to place all the configurations in a file for documentation and to save time not having to type in the parameters every time. With Azure, we can do that via a JSON file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
  &amp;quot;$schema&amp;quot;:
    &amp;quot;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#&amp;quot;,
  &amp;quot;contentVersion&amp;quot;: &amp;quot;1.0.0.0&amp;quot;,
  &amp;quot;parameters&amp;quot;: {
    &amp;quot;containerGroupName&amp;quot;: {
      &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
      &amp;quot;defaultValue&amp;quot;: &amp;quot;mlnetacicontainergroup&amp;quot;,
      &amp;quot;metadata&amp;quot;: {
        &amp;quot;description&amp;quot;: &amp;quot;Container Group name.&amp;quot;
      }
    }
  },
  &amp;quot;variables&amp;quot;: {
    &amp;quot;containername&amp;quot;: &amp;quot;mlnetacidemo&amp;quot;,
    &amp;quot;containerimage&amp;quot;: &amp;quot;&amp;lt;DOCKERUSERNAME&amp;gt;/&amp;lt;IMAGENAME&amp;gt;:latest&amp;quot;
  },
  &amp;quot;resources&amp;quot;: [
    {
      &amp;quot;name&amp;quot;: &amp;quot;[parameters('containerGroupName')]&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;Microsoft.ContainerInstance/containerGroups&amp;quot;,
      &amp;quot;apiVersion&amp;quot;: &amp;quot;2018-04-01&amp;quot;,
      &amp;quot;location&amp;quot;: &amp;quot;[resourceGroup().location]&amp;quot;,
      &amp;quot;properties&amp;quot;: {
        &amp;quot;containers&amp;quot;: [
          {
            &amp;quot;name&amp;quot;: &amp;quot;[variables('containername')]&amp;quot;,
            &amp;quot;properties&amp;quot;: {
              &amp;quot;image&amp;quot;: &amp;quot;[variables('containerimage')]&amp;quot;,
              &amp;quot;resources&amp;quot;: {
                &amp;quot;requests&amp;quot;: {
                  &amp;quot;cpu&amp;quot;: 1,
                  &amp;quot;memoryInGb&amp;quot;: 1.5
                }
              },
              &amp;quot;ports&amp;quot;: [
                {
                  &amp;quot;port&amp;quot;: 80
                }
              ]
            }
          }
        ],
        &amp;quot;osType&amp;quot;: &amp;quot;Linux&amp;quot;,
        &amp;quot;ipAddress&amp;quot;: {
          &amp;quot;type&amp;quot;: &amp;quot;Public&amp;quot;,
          &amp;quot;ports&amp;quot;: [
            {
              &amp;quot;protocol&amp;quot;: &amp;quot;tcp&amp;quot;,
              &amp;quot;port&amp;quot;: &amp;quot;80&amp;quot;
            }
          ]
        }
      }
    }
  ],
  &amp;quot;outputs&amp;quot;: {
    &amp;quot;containerIPv4Address&amp;quot;: {
      &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
      &amp;quot;value&amp;quot;:
        &amp;quot;[reference(resourceId('Microsoft.ContainerInstance/containerGroups/', parameters('containerGroupName'))).ipAddress.ip]&amp;quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It's a lot to look at but for now we can use this template and save it to the file &lt;code&gt;azuredeploy.json&lt;/code&gt; in the root directory of our &lt;code&gt;mlnetacidemo&lt;/code&gt; solution. The only thing that needs to be changed is the value of the &lt;code&gt;containerimage&lt;/code&gt; property. Replace it with your Docker Hub username and the name of the image you just pushed to Docker Hub.&lt;/p&gt;
&lt;h3&gt;Deploy&lt;/h3&gt;
&lt;p&gt;In order to deploy our application we need to make sure to log into our Azure account. To do so via the Azure CLI, type into the command prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az login
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Follow the prompts to log in. Once logged in, it's time to create a resource group for our container.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az group create --name mlnetacidemogroup --location eastus
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After the group has been successfully created it's time to deploy our application.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az group deployment create --resource-group mlnetacidemogroup --template-file azuredeploy.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Give it a few minutes for your deployment to initialize. If the deployment was successful, you should see some output on the command line. Look for the &lt;code&gt;ContainerIPv4Address&lt;/code&gt; property. This is the IP Address where your container is accessible. In POSTMAN or Insomnia, replace the URL to which you previously made a POST request to with &lt;code&gt;http://&amp;lt;ContainerIPv4Address&amp;gt;/api/predict&lt;/code&gt; where &lt;code&gt;ContainerIPv4Address&lt;/code&gt; is the value that was returned to the command line after the deployment. If successful, the response should be just like previous requests &lt;code&gt;Iris-virginica&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once you're finished, you can clean up resources with the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;az group delete --name mlnetacidemogroup
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this writeup, we built a classification machine learning model using &lt;code&gt;ML.NET&lt;/code&gt; that predicts the class of an iris plant given four measurement features, exposed it via an ASP.NET Core REST API, packaged it into a container and deployed it to the cloud using Azure Container Instances. As the model changes and becomes more complex, the process is standardized enough that extending this example would require minimal changes to our existing application. Happy Coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/deploy-netml-docker-aci?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/deploy-netml-docker-aci</guid>
      <pubDate>2018-05-11 21:17:00</pubDate>
      <category>#azure</category>
      <category>#devops</category>
      <category>#dotnet</category>
      <category>#ml</category>
      <category>#ai</category>
      <category>#microsoft</category>
      <category>#programming</category>
      <category>#development</category>
      <category>#csharp</category>
      <category>#aci</category>
      <category>#docker</category>
      <category>#mlnet</category>
      <category>#webapi</category>
      <category>#aspnetcore</category>
      <category>#aspnet</category>
      <category>#machinelearning</category>
      <category>#artificialintelligence</category>
    </item>
    <item>
      <title>E2E Dockerizing a MEAN Stack Application</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/e2e-mean-docker-build?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/e2e-mean-docker-build&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Lately I've been getting familiar with &lt;a href="https://docker.com"&gt;Docker&lt;/a&gt;. I have built single container applications using a &lt;code&gt;Dockerfile&lt;/code&gt; and run them locally. This works fine, especially for deployment purposes to a production VM but it's no different than setting up the VM with all the required dependencies and pushing updates via FTP or source control. However, one of the features that I have found extremely useful is multi-container building and deployment via &lt;code&gt;docker-compose&lt;/code&gt;. With &lt;code&gt;docker-compose&lt;/code&gt;, not only can I build and run my application, but also dependent services like databases, caches, proxies, etc. Best of all, the builds are standardized and initialized at once without having to individually install the dependencies and components. This writeup explores how to containerize a MEAN stack application and set up a &lt;code&gt;docker-compose.yml&lt;/code&gt; file for it to build and start the server and database services defined within it.&lt;/p&gt;
&lt;h2&gt;Requirements&lt;/h2&gt;
&lt;p&gt;This writeup assumes that &lt;code&gt;Docker&lt;/code&gt;, &lt;code&gt;Docker-Compose&lt;/code&gt; and &lt;code&gt;Node&lt;/code&gt; are installed on your PC.&lt;/p&gt;
&lt;h2&gt;The Application&lt;/h2&gt;
&lt;p&gt;The application is a CRUD todo MEAN stack application. The repo for this application can be found &lt;a href="https://github.com/lqdev/todomeandockerdemo"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Project Structure&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;|_models (Mongoose models)
| |_todo.model.js
|_public
| |_scripts
| | |_controllers
| | | |_main.controller.js
| | |_services
| |   |_todo.service.js
| |_views
| | |_main.html
| |_app.js (front-end application)
| |_index.html
|_Dockerfile (server service)
|_docker-compose.yml
|_api.js (todo api routes)
|_config.js
|_server.js (back-end application)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The front-end is built with &lt;code&gt;AngularJS&lt;/code&gt; and the back-end is built with &lt;code&gt;NodeJS&lt;/code&gt; using the &lt;code&gt;Express&lt;/code&gt; web framework and &lt;code&gt;MongoDB&lt;/code&gt; database. &lt;code&gt;MongoDB&lt;/code&gt; models are defined with the &lt;code&gt;Mongoose&lt;/code&gt; package. In the application, users can create, view, update and delete todo tasks. The &lt;code&gt;Dockerfile&lt;/code&gt; is used to define the container for the web application and the &lt;code&gt;docker-compose.yml&lt;/code&gt; defines both the &lt;code&gt;MongoDB&lt;/code&gt; database container as well as the web application container defined in the &lt;code&gt;Dockerfile&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;The Docker File&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-docker"&gt;#Define base image
FROM node:8

#Set Working Directory
WORKDIR /app

#Copy pakage.json file from current directory to working directory
ADD package.json /app

#Install npm packages
RUN npm install

#Copy all application files from local directory to working directory
ADD . /app

#Open port where app will be listening
EXPOSE 3000

#Start application
CMD ['npm','start']
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Define Docker Image and Application Directory&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;Dockerfile&lt;/code&gt; has no extension and the syntax is like a standard text file. &lt;code&gt;#&lt;/code&gt; characters denote comments. &lt;code&gt;Docker&lt;/code&gt; works based off images which are basically pre-built packages that are stored in one of many registries such as &lt;code&gt;DockerHub&lt;/code&gt;. &lt;code&gt;DockerHub&lt;/code&gt; can be thought of as a package repository/manager like &lt;code&gt;npm&lt;/code&gt; or &lt;code&gt;dpkg&lt;/code&gt;. In the first two lines of the file we define which base image we want to create our container with. Since this is a MEAN stack application built entirely in &lt;code&gt;JavaScript&lt;/code&gt;, we'll be using the &lt;code&gt;node&lt;/code&gt; version 8 image. Then we want set the directory in which our application will reside. We do this by using the &lt;code&gt;WORKDIR&lt;/code&gt; command and setting &lt;code&gt;/app&lt;/code&gt; as our application directory, but any directory of your choosing is valid.&lt;/p&gt;
&lt;h3&gt;Install Dependecies&lt;/h3&gt;
&lt;p&gt;All of our dependencies should be defined in our &lt;code&gt;package.json&lt;/code&gt; file. In order to install these dependencies in our container, we need to copy our local &lt;code&gt;package.json&lt;/code&gt; file into our container application directory. This can be done with the &lt;code&gt;ADD&lt;/code&gt; command by passing the &lt;code&gt;package.json&lt;/code&gt; and application directory &lt;code&gt;/app&lt;/code&gt; as arguments. Once that file has been copied, it's time to install the dependencies. To run commands through the build process of the application we use the &lt;code&gt;RUN&lt;/code&gt; command. The command is no different than the one you'd use on your local machine. Therefore, to install the dependencies defined in the &lt;code&gt;package.json&lt;/code&gt; file we use the command &lt;code&gt;RUN npm install&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Copy Application Files&lt;/h3&gt;
&lt;p&gt;Once our dependencies are installed, we need to copy the rest of the files in our local project directory to our container application directory. Like with the &lt;code&gt;package.json&lt;/code&gt; file we use the &lt;code&gt;ADD&lt;/code&gt; command and pass &lt;code&gt;.&lt;/code&gt; and &lt;code&gt;/app&lt;/code&gt; as our source and destination arguments respectively.&lt;/p&gt;
&lt;h4&gt;.dockerignore&lt;/h4&gt;
&lt;p&gt;Something to keep in mind is that locally we have a &lt;code&gt;node_modules&lt;/code&gt; directory containing our installed dependencies. In the previous step, we ran the &lt;code&gt;npm install&lt;/code&gt; command which will create the &lt;code&gt;node_modules&lt;/code&gt; directory inside our container. Therefore, there is no need to copy all these files over. Like &lt;code&gt;git&lt;/code&gt;, we can set up a &lt;code&gt;.dockerignore&lt;/code&gt; file which will contain the files and directory to be ignored by &lt;code&gt;Docker&lt;/code&gt; when packaging and building the container. The &lt;code&gt;.dockerignore&lt;/code&gt; file looks like the following.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;node_modules/*
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Opening Ports&lt;/h3&gt;
&lt;p&gt;Our application will be listening for connections on a port. This particular application will use port 3000. We need to define the port to listen on in the &lt;code&gt;Dockerfile&lt;/code&gt; as well. To do so, we'll use the &lt;code&gt;EXPOSE&lt;/code&gt; command and pass the port(s) that the application will listen on. (MongoDB listens on 27017, but since the &lt;code&gt;Dockerfile&lt;/code&gt; only deals with the web application and not the database we only need to specify the web application's port).&lt;/p&gt;
&lt;h3&gt;Starting the Application&lt;/h3&gt;
&lt;p&gt;After our container is set up, dependencies are installed and port is defined, it's time to start our application. Unlike the process of running commands while building the container using the &lt;code&gt;RUN&lt;/code&gt; command, we'll use the &lt;code&gt;CMD&lt;/code&gt; command to start our application. The arguments accepted by this are an array of strings. In this case, we start our application like we would locally by typing in &lt;code&gt;npm start&lt;/code&gt;. The &lt;code&gt;Dockerfile&lt;/code&gt; command to start our application is the following &lt;code&gt;CMD ['npm','start']&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;The docker-compose.yml File&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;version: '2'
services:
  db:
    image: mongo
    ports: 
      - 27017:27017
  web:
    build: .
    ports:
      - 3000:3000
    links:
      - &amp;quot;db&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;docker-compose.yml&lt;/code&gt; file is a way of defining, building and starting multi-container applications using &lt;code&gt;docker-compose&lt;/code&gt;. In our case we have a two container application, one of the containers is the web application we defined and built in the &lt;code&gt;Dockerfile&lt;/code&gt; and the other is a &lt;code&gt;MongoDB&lt;/code&gt; database. The &lt;code&gt;docker-compose.yml&lt;/code&gt; file can take many options, but the only ones we'll be using are the &lt;code&gt;version&lt;/code&gt; and &lt;code&gt;services&lt;/code&gt; option. The &lt;code&gt;version&lt;/code&gt; option defines which syntax version of the &lt;code&gt;docker-compose.yml&lt;/code&gt; file we'll be using. In our case we'll be using version 2. The &lt;code&gt;services&lt;/code&gt; option defines the individual containers to be packaged and initialized.&lt;/p&gt;
&lt;h3&gt;Services&lt;/h3&gt;
&lt;p&gt;As mentioned, we have two containers. The names of our containers are &lt;code&gt;web&lt;/code&gt; and &lt;code&gt;db&lt;/code&gt;. These names can be anything you want, as long as they're descriptive and make sense to you. The &lt;code&gt;web&lt;/code&gt; container will be our web application and the &lt;code&gt;db&lt;/code&gt; container will be our &lt;code&gt;MongoDB&lt;/code&gt; database. Notice that we have listed our &lt;code&gt;db&lt;/code&gt; service first and then our &lt;code&gt;web&lt;/code&gt; service. The reason for this is we want to build and initialize our database prior to our application so that by the time that the web application is initialized, it's able to successfully connect to the database. If done the other way around, an error will be thrown because the database will not be listening for connections and the web application won't be able to connect. Another way to ensure that our database is initialized prior to our web application is to use the &lt;code&gt;links&lt;/code&gt; option in our &lt;code&gt;web&lt;/code&gt; service and add the name of the database service &lt;code&gt;db&lt;/code&gt; to the list of dependent services. The &lt;code&gt;ports&lt;/code&gt; option like in our &lt;code&gt;Dockerfile&lt;/code&gt; defines which ports that container will need to operate. In this case, our &lt;code&gt;web&lt;/code&gt; app listens on port 3000 and the &lt;code&gt;db&lt;/code&gt; service will listen on port 27017.&lt;/p&gt;
&lt;h4&gt;Container Images&lt;/h4&gt;
&lt;p&gt;The &lt;code&gt;docker-compose.yml&lt;/code&gt; can build containers based on images hosted in a registry as well as those defined by a &lt;code&gt;Dockerfile&lt;/code&gt;. To use images from a registry, we use the &lt;code&gt;image&lt;/code&gt; option inside of our service. Take the &lt;code&gt;db&lt;/code&gt; service for example. &lt;code&gt;MongoDB&lt;/code&gt; already has an image in the &lt;code&gt;DockerHub&lt;/code&gt; registry which we will use to build the container. Our &lt;code&gt;web&lt;/code&gt; container does not have an image that is listed in a registry. However, we can still build an image based off a &lt;code&gt;Dockerfile&lt;/code&gt;. To do this, we use the &lt;code&gt;build&lt;/code&gt; option inside our service and pass the directory of the respective &lt;code&gt;Dockerfile&lt;/code&gt; containing the build instructions for the container.&lt;/p&gt;
&lt;h2&gt;Building and Running Containers&lt;/h2&gt;
&lt;p&gt;Now that our container definitions and files are set up, we're ready to build and run our application. This can be done by typing &lt;code&gt;docker-compose up -d&lt;/code&gt; in the terminal from inside our local project directory. The &lt;code&gt;-d&lt;/code&gt; option runs the command detached allowing us to continue using the terminal. This command will both build and start our containers simultaneously. Once the &lt;code&gt;web&lt;/code&gt; and &lt;code&gt;db&lt;/code&gt; containers are up and running, we can visit &lt;code&gt;http://localhost:3000&lt;/code&gt; from our browser to view and test our application. To stop the application, inside the local project directory, we can type &lt;code&gt;docker-compose stop&lt;/code&gt; in our terminal to stop both containers.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This writeup uses a pre-configured MEAN stack CRUD todo application and explores how to define a single container &lt;code&gt;Dockerfile&lt;/code&gt; as well as a multi-container application using &lt;code&gt;docker-compose&lt;/code&gt;. Docker streamlines how applications are built and deployed while &lt;code&gt;docker-compose&lt;/code&gt; allows more complex multi-container applications to be orchestrated, linked, deployed and managed simultaneously allowing developers to spend more time  developing solutions and less time managing infrastructure and dependencies.&lt;/p&gt;
&lt;h6&gt;Links/Resources&lt;/h6&gt;
&lt;p&gt;&lt;a href="https://www.docker.com/community-edition#/download"&gt;Docker Community Edition&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://docs.docker.com/get-started/"&gt;Docker: Getting Started&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://nodejs.org/en/"&gt;NodeJS&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/e2e-mean-docker-build?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/e2e-mean-docker-build</guid>
      <pubDate>2018-04-29 20:10:09</pubDate>
      <category>#devops</category>
      <category>#mean</category>
      <category>#mongodb</category>
      <category>#docker</category>
      <category>#angular</category>
      <category>#angularjs</category>
      <category>#nodejs</category>
      <category>#javascript</category>
      <category>#programming</category>
      <category>#development</category>
      <category>#docker-compose</category>
    </item>
    <item>
      <title>Type Driven Development - Scaling Safely with Python</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/type-driven-development-scaling-safely-with-python?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/type-driven-development-scaling-safely-with-python&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Python is a great language. The syntax is readable and allows pseudocode to be converted into code nearly verbatim. While this is great for prototyping and moving fast, scaling can become an issue. One of the issues is with regards to documentation. In statically typed languages, even if there's no documentation, types help provide some sort of documentation that allow new contributors as well current developers to remember where they left off and what their code does. There are ways around this using docstrings as well as unit tests. However, this often involves performing tasks outside of writing code which can be time consuming. In Python 3.5, type hints or optional static typing are allowed and tools like &lt;code&gt;mypy&lt;/code&gt; help write safer, more scalable code. The best part is, if the code already has docstrings and unit tests, optional static typing adds an additional layer of safety and documentation to existing projects. This writeup explores practices for documenting and developing scalable Python code as well as illustrating how to use optional static types and type checkers.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;This writeup assumes that Python 3.5 or greater is being used and both &lt;code&gt;mypy&lt;/code&gt; and &lt;code&gt;pytest&lt;/code&gt; packages are installed. To install them using &lt;code&gt;pip&lt;/code&gt; we can type the following command in the shell:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pytest mypy
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Docstrings&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.python.org/dev/peps/pep-0257/"&gt;PEP 257&lt;/a&gt; provides a great overview of what docstrings are and how to use them. The summarized version of it is a string literal in classes and functions that allows developers to document logic, inputs and outputs of those particular sections of code. Below are examples of code with and without docstrings:&lt;/p&gt;
&lt;h3&gt;No Docstring&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def combine(a,b):
    return a + b
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;With Docstring&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def combine(a,b):
    &amp;quot;&amp;quot;&amp;quot;
    Returns the sum of two numbers

    Keyword arguments:
    a -- the first number
    b -- the second number

    Returns:
    Sum of a and b
    &amp;quot;&amp;quot;&amp;quot;
    return a + b
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, the string literal or docstring allows individuals who are looking at the code for the first time as well as someone who worked on it and has forgotten the logic of a program to easily decipher what the code does.&lt;/p&gt;
&lt;p&gt;Something else to notice is the function name. In this case, the logic is relatively simple and the name may make some sense at the time of writing the code. However, this simple logic is dangerous. Without knowing what is expected as input and output, there's not a clear way of knowing what this code should do. For example, someone might try to run the undocumented version of this function with parameter &lt;code&gt;a&lt;/code&gt; having the value 'Hello' and &lt;code&gt;b&lt;/code&gt; with the value 'World'. The function would not throw any errors and return 'HelloWorld' as a result. However, if we look at the intented logic as well as expected input and output provided by the docstring, we'd know that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are both supposed to be numerical values, not strings or any other type.&lt;/p&gt;
&lt;p&gt;It's clear that writing a docstring can become tedious and take up a substantial amount of time as a project grows. However, the benefits are reaped when extending the code and using it in production because more time is spent being productive rather than figuring out what the code should do and whether it's being done correctly. Docstrings however are not a panacea since there is no way to enforce what is documented in the code and serves as more of an FYI for developers using and extending the code.&lt;/p&gt;
&lt;h2&gt;Unit Testing&lt;/h2&gt;
&lt;p&gt;One way to prevent code from being misused is by writing tests. By writing unit tests and making sure that they pass, developers can test edge cases such as passing a string and immediately getting feedback through failing tests. Here's an example of what a unit test would look like for the &lt;code&gt;combine&lt;/code&gt; function written above.&lt;/p&gt;
&lt;p&gt;In the file &lt;code&gt;main.py&lt;/code&gt;, we can write the logic for our &lt;code&gt;combine&lt;/code&gt; function. However, keeping in mind the docstring, we might want to add some exception handling.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&amp;quot;&amp;quot;&amp;quot;
Module Containing Logic
&amp;quot;&amp;quot;&amp;quot;

def combine(a,b):
    &amp;quot;&amp;quot;&amp;quot;                    
    Returns the sum of two numbers                                          
    Keyword arguments:                 
    a -- the first number  
    b -- the second number                                                      
    Returns:             
    Sum of a and b       
    &amp;quot;&amp;quot;&amp;quot;
    if(type(a) == str or type(b) == str)
        return a + b
    else:
        raise TypeError
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In another file called &lt;code&gt;test_main.py&lt;/code&gt;, we can write our tests. Our test file will look like the code below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pytest
from main import combine

testparams = [(1,2,3),(2,4,6)]
@pytest.mark.parametrize(&amp;quot;a,b,expected&amp;quot;,testparams)
def test_combine(a,b,expected): 
    assert combine(a,b) == expected

testparams = [('a','b'),('a','b')]
@pytest.mark.parametrize(&amp;quot;a,b&amp;quot;,testparams)
def test_combine_exception(a,b):
    with pytest.raises(TypeError):
        combine(a,b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our shell we can enter the &lt;code&gt;pytest&lt;/code&gt; command inside of our project directory and get the following output.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;pytest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="/images/type-driven-development-scaling-safely-with-python/typedrivendevelopment1.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;The results from &lt;code&gt;pytest&lt;/code&gt; ensure that passing the expected parameters returns the expected output which is the sum of two numbers and passing in the wrong parameters such as those of type string return a &lt;code&gt;TypeError&lt;/code&gt;. This gets us closer to where we want to be where we're able to test whether the functionality of our application does what it's supposed to. Like docstrings, there is additional work and time that needs to be accounted for when writing tests. However, this is a practice that should be taking place already and in the case of Python which does not provide the type checking or compilation is a way to if not ensure that our logic is sound, at least it provides us with an additional form of documentation and peace of mind that the code is being used accurately.&lt;/p&gt;
&lt;h2&gt;Type Hints (Optional Static Types)&lt;/h2&gt;
&lt;p&gt;Good practice would have us write docstrings to document our code and unit tests to ensure the soundness of our logic and code. However, what if that seems like too much work or there's not much time to perform those tasks. Is there a shorthand way that we can both document our code for posterity as well as ensure that we can only use the code as intended. That's where type hints comes in and starting with Python 3.5 have been accepted by the Python community per &lt;a href="https://www.python.org/dev/peps/pep-0484/"&gt;PEP 484&lt;/a&gt;. With type hints our code would not change much and with a few extra characters, we can write safer code. Our &lt;code&gt;combine&lt;/code&gt; function from previous examples would look as follows with type hints:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def combine(a:float,b:float) -&amp;gt; float:
    return a + b    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we run this, it should run as expected given the appropriate parameters. That being said, as with the undocumented example, if we pass in parameters 'Hello' and 'World', it should work as well and we get the result 'HelloWorld'. If we still don't get the result we want and our code is still unsafe, then what's the point? One of the benefits is the documentation piece. In the event that we had no docstring, we could still tell that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are both of type &lt;code&gt;float&lt;/code&gt; and return a &lt;code&gt;float&lt;/code&gt;. The second benefit comes from the use of &lt;code&gt;mypy&lt;/code&gt;, a type checker for Python. To see it in action, we can create a script called &lt;code&gt;mainmypy.py&lt;/code&gt; and add the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def combine(a:float,b:float) -&amp;gt; float:
    return a + b

combine(1,2)
combine('Hello','World')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the shell, we can then use the &lt;code&gt;mypy&lt;/code&gt; command on our script to check types.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mypy mainmypy.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output is the following:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/type-driven-development-scaling-safely-with-python/typedrivendevelopment2.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;As we can see, without having to run our code, &lt;code&gt;mypy&lt;/code&gt; checks the types and throws errors that we would not have found unless we ran our code. Therefore, we get both documentation by defining the types of parameters and outputs we expect which make it easier for individuals using or writing code to safely do so without having to write long, descriptive docstrings. With &lt;code&gt;mypy&lt;/code&gt;, we enforce the good use of code by checking that the correct parameters are being passed in and the correct results are being returned prior to runtime making it safe to scale and write correct code most of the time.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Python is a very expressive language that allows applications to be prototyped in no time. However, the tradeoff is that writing new code or returning to it at a later time without documenting it, particularly the types needed by functions or classes to produce an accurate result can be unsafe. Some existing practices such as docstrings and unit tests can help with documenting and writing safe code. However, tools like &lt;code&gt;mypy&lt;/code&gt; and the recently introduced type hints achieve what both docstrings and unit tests do in less time and code. This is not to say that these tools are perfect and ideally, unit tests, docstrings and type hints are all integrated to make developers more productive and create safe, scalable code. Happy coding!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/type-driven-development-scaling-safely-with-python?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/type-driven-development-scaling-safely-with-python</guid>
      <pubDate>2018-03-24 18:16:31</pubDate>
      <category>#python</category>
      <category>#static-typing</category>
      <category>#productivity</category>
      <category>#programming</category>
      <category>#development</category>
    </item>
    <item>
      <title>Scaffolding A Web Page In One Line</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/scaffold-a-webpage-in-one-line?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/scaffold-a-webpage-in-one-line&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A few months ago, I saw VS Code released an update that had &lt;a href="https://www.emmet.io/"&gt;Emmet&lt;/a&gt; abbreviations built in. At the time, I saw a demo and it looked like an interesting tool but I knew very little about it. This past week, while practicing &lt;code&gt;css&lt;/code&gt; and scaffolding a blog-like web page for practice I decided to give Emmet a try. After reading some of the documentation and trying it out, within five minutes I was able to replicate what I had done manually with one line of &amp;quot;code&amp;quot;. The entire process is described below.&lt;/p&gt;
&lt;h2&gt;What We'll Build&lt;/h2&gt;
&lt;p&gt;Displayed below is the scaffold of the webpage I built. As it can be seen, in the &lt;code&gt;body&lt;/code&gt; element there is a &lt;code&gt;nav&lt;/code&gt; element and four &lt;code&gt;div&lt;/code&gt; elements that will contain our articles and their metadata.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
    &amp;lt;title&amp;gt;&amp;lt;/title&amp;gt;
    &amp;lt;link rel=&amp;quot;stylesheet&amp;quot; href=&amp;quot;&amp;quot;&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
    &amp;lt;nav&amp;gt;
        &amp;lt;ul&amp;gt;
            &amp;lt;li class=&amp;quot;nav-item&amp;quot;&amp;gt;&amp;lt;/li&amp;gt;
            &amp;lt;li class=&amp;quot;nav-item&amp;quot;&amp;gt;&amp;lt;/li&amp;gt;
            &amp;lt;li class=&amp;quot;nav-item&amp;quot;&amp;gt;&amp;lt;/li&amp;gt;
        &amp;lt;/ul&amp;gt;
    &amp;lt;/nav&amp;gt;
    &amp;lt;div&amp;gt;
        &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;
            &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;&amp;lt;/h2&amp;gt;
            &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;&amp;lt;/h6&amp;gt;
            &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;&amp;lt;/p&amp;gt;
        &amp;lt;/div&amp;gt;
    &amp;lt;/div&amp;gt;
    &amp;lt;div&amp;gt;
        &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;
            &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;&amp;lt;/h2&amp;gt;
            &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;&amp;lt;/h6&amp;gt;
            &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;&amp;lt;/p&amp;gt;
        &amp;lt;/div&amp;gt;
    &amp;lt;/div&amp;gt;
    &amp;lt;div&amp;gt;
        &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;
            &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;&amp;lt;/h2&amp;gt;
            &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;&amp;lt;/h6&amp;gt;
            &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;&amp;lt;/p&amp;gt;
        &amp;lt;/div&amp;gt;
    &amp;lt;/div&amp;gt;
    &amp;lt;div&amp;gt;
        &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;
            &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;&amp;lt;/h2&amp;gt;
            &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;&amp;lt;/h6&amp;gt;
            &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;&amp;lt;/p&amp;gt;
        &amp;lt;/div&amp;gt;
    &amp;lt;/div&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;How We'll Build It&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;hmtl&lt;/code&gt; above can be programatically built using the abbreviation below.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;html&amp;gt;(head&amp;gt;title+link)+(body&amp;gt;(nav&amp;gt;ul&amp;gt;li.nav-item*3)+(div&amp;gt;div.article&amp;gt;h2.article-title+h6.article-date+p.article-description)*4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To break down some of the things that are going on here, we can start with the syntax. The &lt;code&gt;&amp;gt;&lt;/code&gt; character denotes that the item on the left is the parent of the item(s) on the right. Similarly, the &lt;code&gt;+&lt;/code&gt; character is used to describe siblings. Similar to math operations, parentheses evaluate what's inside them. Elements themselves can contain additional data such as class names and ids. This data can be appended to the element name just like &lt;code&gt;css&lt;/code&gt; with &lt;code&gt;.&lt;/code&gt; for classes and &lt;code&gt;#&lt;/code&gt; for ids.&lt;/p&gt;
&lt;p&gt;With that knowledge, we can see that we have an &lt;code&gt;html&lt;/code&gt; root element with &lt;code&gt;head&lt;/code&gt; and &lt;code&gt;body&lt;/code&gt; children. The &lt;code&gt;head&lt;/code&gt; element has a &lt;code&gt;title&lt;/code&gt; and &lt;code&gt;link&lt;/code&gt; child elements. The &lt;code&gt;body&lt;/code&gt; element has a &lt;code&gt;nav&lt;/code&gt; element with a &lt;code&gt;ul&lt;/code&gt; child which contains three &lt;code&gt;li&lt;/code&gt; elements. The &lt;code&gt;nav&lt;/code&gt; element has a &lt;code&gt;div&lt;/code&gt; sibling which contains four &lt;code&gt;div&lt;/code&gt; children that serve as the containers for our articles and their metadata.&lt;/p&gt;
&lt;h2&gt;Adding some style&lt;/h2&gt;
&lt;p&gt;To help visualize what the page looks like we can add some &lt;code&gt;css&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-css"&gt;* {
    margin: 0;
}

nav {
    padding: 0;
    background-color: black;
    color: white;
}

.nav-item {
    margin: 20px;
    display: inline-block;
    font-weight: bold;
    font-size: 20px;
}

.nav-item:hover {
    opacity: 0.7;
}

.article {
    display: block;
    margin: 20px;
    padding-left: 5px;
    background-color: lightgray;
    border: black solid 2px;
}

.article-title {
    display: block;
    font-family: serif;
    font-size: 30px;
    text-decoration: underline;
    display: inline;
}

.article-date {
    font-size: 20px;
    font-family: fantasy;
    float: right;
    margin-right: 5px;
}

.article-description {
    display: block;
    font-size: 18px;
    font-family: sans-serif;
    margin-top: 5px;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Result&lt;/h2&gt;
&lt;p&gt;After linking our &lt;code&gt;css&lt;/code&gt; with the webpage as well as adding some content to the page, the result will look as follows.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/scaffold-a-webpage-in-one-line/scaffoldawebpageoneline1.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;html&amp;gt;
    &amp;lt;head&amp;gt;
        &amp;lt;link rel=&amp;quot;stylesheet&amp;quot; type=&amp;quot;text/css&amp;quot; href=&amp;quot;main.css&amp;quot;/&amp;gt;
        &amp;lt;title&amp;gt;HTML CSS Practice&amp;lt;/title&amp;gt;
    &amp;lt;/head&amp;gt;
    &amp;lt;body&amp;gt;
        &amp;lt;nav&amp;gt;
            &amp;lt;ul&amp;gt;
                &amp;lt;li class=&amp;quot;nav-item&amp;quot;&amp;gt;Home&amp;lt;/li&amp;gt;
                &amp;lt;li class=&amp;quot;nav-item&amp;quot;&amp;gt;About&amp;lt;/li&amp;gt;
                &amp;lt;li class=&amp;quot;nav-item&amp;quot;&amp;gt;Contact&amp;lt;/li&amp;gt;
            &amp;lt;/ul&amp;gt;
        &amp;lt;/nav&amp;gt;
        &amp;lt;div&amp;gt;
            &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;
                &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;Hello World&amp;lt;/h2&amp;gt;
                &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;January 8, 2017&amp;lt;/h6&amp;gt;
                &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;This is a short description of my program. There are more things than this.&amp;lt;/p&amp;gt;
            &amp;lt;/div&amp;gt;
            &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;
                &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;Second Post&amp;lt;/h2&amp;gt;
                &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;February 8, 2017&amp;lt;/h6&amp;gt;
                &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;I think that I got my feet wet a little bit. Maybe there are other things that I can write about&amp;lt;/p&amp;gt;
            &amp;lt;/div&amp;gt;
            &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;
                &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;Third Post&amp;lt;/h2&amp;gt;
                &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;March 8, 2017&amp;lt;/h6&amp;gt;
                &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;This is the third go around. I think I got pretty good at this design stuff.&amp;lt;/p&amp;gt;
            &amp;lt;/div&amp;gt;
            &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;
                &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;Fourth Post&amp;lt;/h2&amp;gt;
                &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;June 8, 2017&amp;lt;/h6&amp;gt;
                &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;I really like my description of the things that are happening here and there is another post on the way very soon.&amp;lt;/p&amp;gt;
            &amp;lt;/div&amp;gt;                        
        &amp;lt;/div&amp;gt;
    &amp;lt;/body&amp;gt;    
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h6&gt;Sources&lt;/h6&gt;
&lt;p&gt;&lt;a href="https://www.emmet.io/"&gt;Emmet&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/scaffold-a-webpage-in-one-line?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/scaffold-a-webpage-in-one-line</guid>
      <pubDate>2018-03-10 16:07:00</pubDate>
      <category>#html</category>
      <category>#web-development</category>
      <category>#development</category>
      <category>#tools</category>
      <category>#emmet</category>
    </item>
    <item>
      <title>Testing and Deploying Python Projects with Travis CI</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/testing-deploying-python-projects-travisci?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/testing-deploying-python-projects-travisci&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;When working on projects, especially those that others or yourself may depend upon, it is important to test to make sure that everything is working as expected. Furthermore, being able to deploy your code/packages to a central repository with a package manager makes distribution easier. Although this can all be done manually, it can also be automated. Both testing and deployment can be automated using Travis CI which makes the entire process as easy as pushing your most recent changes to GitHub. In this writeup, I will go over how to create a Python project with unit tests and deploy to PyPI. A sample project can be found at this &lt;a href="https://github.com/lqdev/TravisTest"&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/"&gt;GitHub Login&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pypi.python.org/pypi"&gt;PyPI Login&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://virtualenv.pypa.io/en/stable/"&gt;virtualenv&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Install virtualenv&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo pip install virtualenv
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Create The Project&lt;/h1&gt;
&lt;p&gt;For the test project, I will create a module that performs adding, subtracting, increment and decrement operations.&lt;/p&gt;
&lt;h2&gt;Define Folder Structure&lt;/h2&gt;
&lt;p&gt;We start out by creating a directory for our project.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir travistest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside that directory, we want to have have a directory for our module as well as for our tests. Therefore, we need to create a directory for both of them.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir travistest
mkdir test
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we want to initialize the virtual environment for our project. To do so, we can use the &lt;code&gt;virtualenv&lt;/code&gt; package. For the project &lt;code&gt;python 3.5&lt;/code&gt; is the version that will be used.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;virtualenv -p python3.5 ENV
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After installation, a folder with the name &lt;code&gt;ENV&lt;/code&gt; should appear in the root directory of the project. The final directory structure should look like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;travistest
|_ENV
|_travistest
|_test
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Install Modules&lt;/h2&gt;
&lt;p&gt;For this project, I'll be using &lt;code&gt;pytest&lt;/code&gt; for unit testing. Before installing anything however, I'll need to activate the virtual environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;source ENV/bin/activate
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once our virtual environment is activated, we can install &lt;code&gt;pytest&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pytest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After installation, we can persist installed packages inside a &lt;code&gt;requirements.txt&lt;/code&gt; file with the &lt;code&gt;freeze&lt;/code&gt; command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;pip freeze &amp;gt; requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Create The Module&lt;/h2&gt;
&lt;p&gt;Inside the &lt;code&gt;travistest&lt;/code&gt; module directory, the easiest way to create a module is to include an &lt;code&gt;__init__.py&lt;/code&gt; file inside the directory. It's okay if it's empty.&lt;/p&gt;
&lt;p&gt;Therefore, we can start by creating the &lt;code&gt;__init__&lt;/code&gt; file in that directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;touch __init__.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once that's created, we can begin writing the main functionality of our module. Inside a file called &lt;code&gt;Operations.py&lt;/code&gt;, we can put the following code in.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Operations:
    def __init__(self):
        pass
    
    def add(self,x,y):
        return x + y

    def subtract(self,x,y):
        return x - y

    def increment(self,x):
        return x + 1

    def decrement(self,x):
    	return x - 1
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Unit Test&lt;/h2&gt;
&lt;p&gt;Once we have our code, we need to write tests for it. Navigating to the &lt;code&gt;test&lt;/code&gt; directory, we can add the following code to the &lt;code&gt;test_operations.py&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from pytest import fixture

@fixture
def op():
    from travistest.Operations import Operations
    return Operations()

def test_add(op):
    assert op.add(1,2) == 3

def test_subtract(op):
    assert op.subtract(2,1) == 1

def test_increment(op):
    assert op.increment(1) == 2

def test_decrement(op):
assert op.decrement(2) == 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make sure everything is working correctly, from the project's root directory, we can run the &lt;code&gt;pytest&lt;/code&gt; command. If all goes well, an output similar to the one below should appear.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;============================= test session starts ==============================
platform linux -- Python 3.5.2, pytest-3.4.0, py-1.5.2, pluggy-0.6.0

collected 4 items                                                              

test/test_operations.py ....                                             [100%]

=========================== 4 passed in 0.04 seconds ===========================
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Prepare For Deployment&lt;/h1&gt;
&lt;p&gt;To prepare for deployment and uploading to PyPI, we need to add a &lt;code&gt;setup.py&lt;/code&gt; file to the root directory of our project. The contents of this file for our purposes are mostly metadata that will populate information in PyPI.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from distutils.core import setup

setup(
    name='travistest',
    packages=['travistest'],
    version='0.0.7',
    description='Test project to get acquainted with TravisCI',
    url='https://github.com/lqdev/TravisTest',    
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Setup Travis&lt;/h2&gt;
&lt;h3&gt;Enable Repository&lt;/h3&gt;
&lt;p&gt;Assuming that you have a &lt;code&gt;GitHub&lt;/code&gt; login and a repository has been created for your project:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Log into &lt;a href="https://travis-ci.org/"&gt;travis-ci.org&lt;/a&gt; with your GitHub credentials.&lt;/li&gt;
&lt;li&gt;Once all of your repositores are synced, toggle the switch next to the repository containing your project.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Configure .travis.yml&lt;/h3&gt;
&lt;p&gt;Once the project has been enabled, we need fo configure Travis. This is all done using the &lt;code&gt;.travis.yml&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;In this file we'll tell Travis that the language of our project is Python version 3.5 and that we'll be using a virtual environment. Additionally we'll require sudo priviliges and target the Ubuntu Xenial 16.04 distribution. All of these configurations can be done as follows.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;sudo: required
dist: xenial
language: python
virtualenv:
  system_site_packages: true
python:
- '3.5'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once that is set up, we can tell it to install all of the dependencies stored in our &lt;code&gt;requirements.txt&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;install:
- pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After this, we need to tell Travis to run our tests just like we would on our local machine.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;script: pytest
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once our tests have run, we need to make sure we are back in the root directory of our project for deployment.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;after_script: cd ~
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We're done with the automated testing script portion of our project. Now we need to setup deployment options. This section will mainly contain the credentials of your PyPI account.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;deploy:
  provider: pypi
  user: &amp;quot;YOURUSERNAME&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After setting the provider and user, we need to set the password. Because this will be a public repository DO NOT enter your password on this file. Instead, we can set an encrypted version that only Travis can decrypt. To do so, while in the root directory of our project, we can enter the following command into the terminal.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;travis encrypt --add deployment.password
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Type your password into the terminal and press &lt;code&gt;Ctrl + D&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you take a look at your &lt;code&gt;.travis.yml&lt;/code&gt; file you should see something like the following in your &lt;code&gt;deploy&lt;/code&gt; options&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;deploy:
  provider: pypi
  user: &amp;quot;YOURUSERNAME&amp;quot;
  password:
    secure: &amp;quot;YOURPASSWORD&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final &lt;code&gt;.travis.yml&lt;/code&gt; file should like like so&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-yaml"&gt;sudo: required
dist: xenial
language: python
virtualenv:
  system_site_packages: true
python:
- '3.5'
install:
- pip install -r requirements.txt
script: pytest
after_script: cd ~
deploy:
  provider: pypi
  user: &amp;quot;YOURUSERNAME&amp;quot;
  password:
    secure: &amp;quot;YOURPASSWORD&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Deploy&lt;/h1&gt;
&lt;p&gt;Now that we have everything set up, deployment should be relatively easy. A build is triggered when changes are pushed to the repository on GitHub. Therefore, pushing your local changes to the remote GitHub repository should initialize the build. To track progress, visit the project's page on Travis CI.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE: WHEN PUSHING NEW CHANGES, INCREMENT VERSION NUMBER IN THE &lt;code&gt;SETUP.PY&lt;/code&gt; FILE SO THAT EXISTING FILE ERRORS DO NOT CRASH BUILDS&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In this writeup, we created a Python package that performs basic operations and set up automated testing and deployment with Travis CI. Configurations can be further customized and refined to adapt more complex build processes.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/testing-deploying-python-projects-travisci?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/testing-deploying-python-projects-travisci</guid>
      <pubDate>2018-02-18 20:51:32</pubDate>
      <category>#devops</category>
      <category>#travisci</category>
      <category>#python</category>
      <category>#ci/cd</category>
      <category>#programming</category>
      <category>#development</category>
    </item>
    <item>
      <title>Transcribing Podcasts with Microsoft Speech API</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/transcribing-podcasts-microsoft-speech-api?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/transcribing-podcasts-microsoft-speech-api&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I enjoy listening to podcasts in a wide range of topics that include but are not limited to politics, software development, history, comedy and true crime. While some of the more entertainment related podcasts are best enjoyed through audio, those that are related to software development or other type of hands-on topics would benefit greatly from having a transcript. Having a transcript allows me to go back after having listened to an interesting discussion and look directly at the content I am interested in without having to listen to the podcast again. This however is not always feasible given that it costs both time and money to produce a transcript. Fortunately, there are tools out there such as Google Cloud Speech and Microsoft Speech API which allow users to convert speech to text. For this writeup, I will be focusing on the Microsoft Speech API. Because podcasts tend to be long-form, I will be using the C# client library because it allows for long audio (greater than 15 seconds) to be transcribed. The purpose of this exercise is to create a console application that takes audio segments, converts them to text and stores the results in a text file with the goal of evaluating how well the Microsoft Speech API works. The source code for the console application can be found on &lt;a href="https://github.com/lqdev/PodcastsBingSpeechAPIDemo"&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.visualstudio.com/vs/community/"&gt;Visual Studio Community&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.microsoft.com/store/productId/9NBLGGH4MSV6"&gt;Windows Subsystem for Linux (Ubuntu)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ffmpeg.org/"&gt;ffmpeg&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Install/Enable Windows Subsystem for Linux&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Open Powershell as Administrator and input&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-powershell"&gt;Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;Restart your computer&lt;/li&gt;
&lt;li&gt;Open the Microsoft Store and install &lt;a href="https://www.microsoft.com/store/productId/9NBLGGH4MSV6"&gt;Ubuntu&lt;/a&gt; Linux distribution&lt;/li&gt;
&lt;li&gt;Once installed, click &lt;code&gt;Launch&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create your LINUX user account (Keep in mind that this is not the same as your Windows account therefore it can be different).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Install ffmpeg&lt;/h2&gt;
&lt;p&gt;Once Ubuntu is installed on your computer and you have created your LINUX user account, it's time to install &lt;code&gt;ffmpeg&lt;/code&gt;. This will allow us to convert our file from &lt;code&gt;mp3&lt;/code&gt; which is usually the format podcasts are in to &lt;code&gt;wav&lt;/code&gt; which is the format accepted by the Microsoft Speech API&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install -y ffmpeg
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Get Bing Speech API Key&lt;/h2&gt;
&lt;p&gt;In order to use the Microsoft Speech API, an API key is required. This can be obtained using the following steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Navigate to the &lt;a href="https://azure.microsoft.com/en-us/try/cognitive-services/"&gt;Azure Cognitive Services&lt;/a&gt; page.&lt;/li&gt;
&lt;li&gt;Select the &lt;code&gt;Speech&lt;/code&gt; tab&lt;/li&gt;
&lt;li&gt;Click &lt;code&gt;Get API Key&lt;/code&gt; and follow the instructions&lt;/li&gt;
&lt;li&gt;Once you have an API key, make sure to store it somewhere like a text file in your computer for future use.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;File Prep&lt;/h1&gt;
&lt;p&gt;Before transcribing the files, they need to be converted to &lt;code&gt;wav&lt;/code&gt; format if they are not already in it. In order to ease processing, they should be split into segments as opposed to having the API process an entire multi megabyte file. To do all this, inside the Ubuntu shell, first navigate to your &lt;code&gt;Documents&lt;/code&gt; folder and create a new directory for the project.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;cd /mnt/c/Users/&amp;lt;USERNAME&amp;gt;/Documents
mkdir testspeechapi
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;USERNAME&lt;/code&gt; is your Windows user name. This can be found by typing the following command into the Windows &lt;code&gt;CMD&lt;/code&gt; prompt&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;echo %USERNAME%
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Download Audio Files&lt;/h2&gt;
&lt;p&gt;One of the podcasts I listen to is &lt;a href="https://talkpython.fm/"&gt;Talk Python To Me&lt;/a&gt; by &lt;a href="https://twitter.com/mkennedy"&gt;Michael Kennedy&lt;/a&gt;. For the purpose of this exercise, aside from having great content, all of the episodes are transcribed and a link is provided to the &lt;code&gt;mp3&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;The file I will be using is from episode &lt;a href="https://talkpython.fm/episodes/show/149/4-python-web-frameworks-compared"&gt;#149&lt;/a&gt;, but it can easily be any of the episodes.&lt;/p&gt;
&lt;p&gt;In the Ubuntu shell, download the file into the &lt;code&gt;testspeechapi&lt;/code&gt; directory that was recently created using &lt;code&gt;wget&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://talkpython.fm/episodes/download/149/4-python-web-frameworks-compared.mp3 -O originalinput.mp3
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Download Transcripts&lt;/h2&gt;
&lt;p&gt;The transcripts can be found in GitHub at this &lt;a href="https://github.com/mikeckennedy/talk-python-transcripts/tree/master/transcripts"&gt;link&lt;/a&gt;. The original transcript will allow me to compare it to the output of the Microsoft Speech API and evaluate the accuracy.&lt;/p&gt;
&lt;p&gt;We can download this file into the &lt;code&gt;testspeechapi&lt;/code&gt; directory just like the audio file&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://raw.githubusercontent.com/mikeckennedy/talk-python-transcripts/master/transcripts/149.txt -O originaltranscript.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Convert MP3 to WAV&lt;/h2&gt;
&lt;p&gt;Now that we have both the original audio file and transcript, it's time to convert the format from &lt;code&gt;mp3&lt;/code&gt; to &lt;code&gt;wav&lt;/code&gt;. To do this, we can use &lt;code&gt;ffmpeg&lt;/code&gt;. In the Ubuntu shell, enter the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;ffmpeg -i originalinput.mp3 -f wav originalinput.wav
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Create Audio Segments&lt;/h2&gt;
&lt;p&gt;Once we have the proper format, it's time to make the files more manageable for processing. This can be done by splitting them up into equal segments using &lt;code&gt;ffmpeg&lt;/code&gt;. In this case I'll be splitting it up into sixty second segments and storing them into a directory called &lt;code&gt;input&lt;/code&gt; with the name &lt;code&gt;filexxxx.wav&lt;/code&gt;. The &lt;code&gt;%04d&lt;/code&gt; indicates that there will be 4 digits in the file name. Inside the Ubuntu shell and &lt;code&gt;testspeechapi&lt;/code&gt; directory enter the following commands.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir input
ffmpeg -i originalinput.wav -f segment -segment_time 60 -c copy input/file%04d.wav
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Building The Console Application&lt;/h1&gt;
&lt;p&gt;To get started building the console application, we can leverage the &lt;a href="https://github.com/Azure-Samples/Cognitive-Speech-STT-ServiceLibrary"&gt;Cognitive-Speech-STT-ServiceLibrary&lt;/a&gt; sample program.&lt;/p&gt;
&lt;h2&gt;Download The Sample Program&lt;/h2&gt;
&lt;p&gt;The first step will be to clone the sample program from GitHub onto your computer. In the Windows &lt;code&gt;CMD&lt;/code&gt; prompt navigate to the &lt;code&gt;testspeechapi&lt;/code&gt; folder and enter the following command.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Azure-Samples/Cognitive-Speech-STT-ServiceLibrary
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the project is on your computer, open the directory and launch the &lt;code&gt;SpeechClient.sln&lt;/code&gt; solution in the &lt;code&gt;sample&lt;/code&gt; directory&lt;/p&gt;
&lt;p&gt;When the solution launches, open the &lt;code&gt;Program.cs&lt;/code&gt; file and begin making modifications.&lt;/p&gt;
&lt;h2&gt;Reading Files&lt;/h2&gt;
&lt;p&gt;To read files, we can create a function that will return the list of files in the specified directory.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;private static string[] GetFiles(string directory)
{
    string[] files = Directory.GetFiles(directory);
    return files;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Process Files&lt;/h2&gt;
&lt;p&gt;Once we have the list of files, we can then process each file individually using the &lt;code&gt;Run&lt;/code&gt; method. To do so, we need to make a slight modification to our &lt;code&gt;Main&lt;/code&gt; method so that it iterates over each file and calls the &lt;code&gt;Run&lt;/code&gt; method on it. To store the responses from the API, we'll also need a &lt;code&gt;StringBuilder&lt;/code&gt; object which is declared at the top of our &lt;code&gt;Program.cs&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;finalResponse = new StringBuilder();
string[] files = GetFiles(args[0]);
foreach(var file in files)
{
    p.Run(file,&amp;quot;en-us&amp;quot;,LongDictationUrl,args[1]).Wait();
    Console.WriteLine(&amp;quot;File {0} processed&amp;quot;,file);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Transcribe Audio&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;Run&lt;/code&gt; method can be left intact. However, the &lt;code&gt;Run&lt;/code&gt; method uses the &lt;code&gt;OnRecognitionResult&lt;/code&gt; method to handle the result of API responses. In the &lt;code&gt;OnRecognitionResult&lt;/code&gt; method, we can remove almost everything that is originally there and replace it. The response from the API returns various results of potential phrases as well as a confidence value. Generally, most of the phrases are alike and the first value is good enough for our purposes. The code for this part will take the response from the API, append it to a &lt;code&gt;StringBuilder&lt;/code&gt; object and return when completed.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public Task OnRecognitionResult(RecognitionResult args)
{

    var response = args;

    if(response.Phrases != null)
    {
	finalResponse.Append(response.Phrases[0].DisplayText);
	finalResponse.Append(&amp;quot;\n&amp;quot;);
    }

    return CompletedTask;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Output Transcribed Speech&lt;/h2&gt;
&lt;p&gt;When all the audio files have been processed, we can save the final output to a text file in the &lt;code&gt;testspeechapi&lt;/code&gt; directory. This can be done with the &lt;code&gt;SaveOutput&lt;/code&gt; function to which we pass in a file name and the &lt;code&gt;StringBuilder&lt;/code&gt; object that captured responses from the API.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;private static void SaveOutput(string filename,StringBuilder content)
{
    StreamWriter writer = new StreamWriter(filename);
    writer.Write(content.ToString());
    writer.Close();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;SaveOutput&lt;/code&gt; can then be called from our &lt;code&gt;Main&lt;/code&gt; method like so.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;string username = Environment.GetEnvironmentVariable(&amp;quot;USERNAME&amp;quot;, EnvironmentVariableTarget.Process);
SaveOutput(String.Format(@&amp;quot;C:\\Users\\{0}\\Documents\\testspeechapi\\apitranscript.txt&amp;quot;,username), finalResponse);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final &lt;code&gt;Main&lt;/code&gt; method should look similar to the code below&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public static void Main(string[] args)
{
    // Send a speech recognition request for the audio.
    finalResponse = new StringBuilder();

    string[] files = GetFiles(args[0]);

    var p = new Program();

    foreach (var file in files)
    {
	p.Run(file, &amp;quot;en-us&amp;quot;, LongDictationUrl, args[1]).Wait();
	Console.WriteLine(&amp;quot;File {0} processed&amp;quot;, file);
    }

    string username = Environment.GetEnvironmentVariable(&amp;quot;USERNAME&amp;quot;, EnvironmentVariableTarget.Process);

    SaveOutput(String.Format(@&amp;quot;C:\\Users\\{0}\\Documents\\testspeechapi\\apitranscript.txt&amp;quot;,username), finalResponse);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Output&lt;/h1&gt;
&lt;p&gt;The program will save the output of the &lt;code&gt;StringBuilder&lt;/code&gt; object &lt;code&gt;finalResponse&lt;/code&gt; to the file &lt;code&gt;apitranscript.txt&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Prior to running the program it needs to be built. In Visual Studio change the Solutions Configurations option from &lt;code&gt;Debug&lt;/code&gt; to &lt;code&gt;Release&lt;/code&gt; and build the application.&lt;/p&gt;
&lt;p&gt;To run the program, navigate to the &lt;code&gt;C:\Users\%USERNAME%\Documents\testspeechapi\Cognitive-Speech-STT-ServiceLibrary\sample\SpeechClientSample\bin\Release&lt;/code&gt; directory in the Windows &lt;code&gt;CMD&lt;/code&gt; prompt and enter the following command and pass in the &lt;code&gt;input&lt;/code&gt; directory where all the audio segments are stored and your API Key.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;SpeechClientSample.exe C:\\Users\\%USERNAME%\\Documents\\testspeechapi\\input &amp;lt;YOUR-API-KEY&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This process may take a while due to the number of files being processed, so be patient.&lt;/p&gt;
&lt;h2&gt;Sample Output&lt;/h2&gt;
&lt;p&gt;Original&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Are you considering getting into web programming? Choosing a web framework like Pyramid, Flask, or Django can be daunting. It would be great to see them all build out the same application and compare the results side-by-side. That's why when I heard what Nicholas Hunt-Walker was up to, I had to have him on the podcast. He and I chat about four web frameworks compared. He built a data-driven web app with Flask, Tornado, Pyramid, and Django and then put it all together in a presentation. We're going to dive into that right now. This is Talk Python To Me, Episode 149, recorded January 30th, 2018. Welcome to Talk Python to Me, a weekly podcast on Python, the language, the libraries, the ecosystem, and the personalities. This is your host, Michael Kennedy, follow me on Twitter where I'm @mkennedy. Keep up with the show and listen to past episodes at talkpython.fm, and follow the show on Twitter via @talkpython. Nick, welcome to Talk Python.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;API Response&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Are you considering getting into web programming cheese in the web framework like plaster. Django can be daunting. It would be great to see them. Although that the same application and compare the results side by side. That's Why? When I heard. But Nicholas Hunt. Walker was up to, I had him on the podcast. He night chat about 4 web frameworks, compared he built a data driven web app with last tornado. Peermade Angangueo and then put it all together in a presentation. We have dive into that right now. This is talk by the to me, I was 149 recorded January 30th 2018. Welcome to talk python to me a weekly podcast on Python Language the library ecosystem in the personalities. Did you host Michael Kennedy? Follow me on Twitter where I'm at in Kennedy keep up with the show in the past episodes at talk python. Damn info, the show on Twitter via at Popeyes on. Nick Welcome to talk by phone.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In this writeup, I converted an &lt;code&gt;mp3&lt;/code&gt; podcast file to &lt;code&gt;wav&lt;/code&gt; format and split it into sixty second segments using &lt;code&gt;ffmpeg&lt;/code&gt;. The segments were then processed by a console application which uses the Microsoft Speech API to convert speech to text and the results were saved to a text file. When compared to the original transcript, the result produced by the API is inconsistent and at times incomprehensible. This does not mean that overall the API is unusable or inaccurate as many things could have contributed to the inaccuracy. However, the results were not as good as I would have hoped for.&lt;/p&gt;
&lt;h6&gt;Sources&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10"&gt;Windows Subsystem for Linux Install Instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.microsoft.com/en-us/azure/cognitive-services/speech/home"&gt;Microsoft Speech API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/transcribing-podcasts-microsoft-speech-api?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/transcribing-podcasts-microsoft-speech-api</guid>
      <pubDate>2018-02-11 20:34:37</pubDate>
      <category>#azure</category>
      <category>#csharp</category>
      <category>#apis</category>
      <category>#nlp</category>
      <category>#machine-learning</category>
      <category>#artificial-intelligence</category>
      <category>#microsoft</category>
    </item>
    <item>
      <title>Real-Time Sentiment Analysis with C#</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/real-time-sentiment-analysis-csharp?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/real-time-sentiment-analysis-csharp&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h6&gt;This is strictly for use with the .NET Framework. With Mono it might be able to work on other platforms. &lt;code&gt;SimpleNetNlp&lt;/code&gt; does not currently work with .NET Core/Standard&lt;/h6&gt;
&lt;p&gt;In this project, I will demonstate how to perform sentiment analysis on tweets using various C# libraries.&lt;/p&gt;
&lt;h2&gt;Dependencies&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;TweetInviAPI&lt;/li&gt;
&lt;li&gt;SimpleNetNlp&lt;/li&gt;
&lt;li&gt;SimpleNetNlp.Models.LexParser&lt;/li&gt;
&lt;li&gt;SimpleNetNlP.Models.Sentiment&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Create Console Application&lt;/h2&gt;
&lt;p&gt;In Visual Studio, Click File &amp;gt; New &amp;gt; New Project &amp;gt; Console Application&lt;/p&gt;
&lt;p&gt;All of the code below will be placed in the &lt;code&gt;Program&lt;/code&gt; class.&lt;/p&gt;
&lt;h2&gt;Creating A Stream&lt;/h2&gt;
&lt;h3&gt;Authenticate&lt;/h3&gt;
&lt;p&gt;Thanks to the &lt;code&gt;Tweetinvi&lt;/code&gt; library, the authentication with the Twitter API is a breeze. Assuming that an application has been registered at &lt;a href="http://apps.twitter.com"&gt;http://apps.twitter.com&lt;/a&gt;, the &lt;code&gt;SetUserCredentials&lt;/code&gt; method can be used and the &lt;code&gt;Consumer Key&lt;/code&gt;, &lt;code&gt;Consumer Secret&lt;/code&gt;,&lt;code&gt;Access Token&lt;/code&gt; and &lt;code&gt;Access Token Secret&lt;/code&gt; can be passed into it. This type of global authentication makes it easy to perform authenticated calls throughout the entire application.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;Auth.SetUserCredentials(&amp;quot;consumer-key&amp;quot;,&amp;quot;consumer-secret&amp;quot;,&amp;quot;access-token&amp;quot;,&amp;quot;access-token-secret&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Build Stream&lt;/h3&gt;
&lt;p&gt;Like the authentication, creating a stream is seamless.&lt;/p&gt;
&lt;p&gt;We can create a stream by calling the &lt;code&gt;CreateFilteredStream&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var stream = Stream.CreateFilteredStream();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then add conditions to filter on using the &lt;code&gt;AddTrack&lt;/code&gt; method. In this case, I will be filtering for cryptocurrencies, ether, bitcoin, and litecoin.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;stream.AddTrack(&amp;quot;cryptocurrencies&amp;quot;);
stream.AddTrack(&amp;quot;bitcoin&amp;quot;);
stream.AddTrack(&amp;quot;ether&amp;quot;);
stream.AddTrack(&amp;quot;Litecoin&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, we can filter by language. In my case, I will only be filtering on English. This can be done by using the &lt;code&gt;AddTweetLanguageFilter&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;stream.AddTweetLanguageFilter(&amp;quot;en&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have all the filters set up, we need to handle what will happen when a matching tweet is detected. This will be handled by an &lt;code&gt;EventHandler&lt;/code&gt; called &lt;code&gt;MatchingTweetReceived&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;stream.MatchingTweetReceived += OnMatchedTweet;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;MatchingTweetReceived&lt;/code&gt; will be bound to the &lt;code&gt;OnMatchedTweet&lt;/code&gt; method which I created.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;private static void OnMatchedTweet(object sender, MatchedTweetReceivedEventArgs args)
{
  //Do Stuff
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The logic inside of this method will perform sentiment analysis and output the sentiment as well as the full text of the tweet.&lt;/p&gt;
&lt;h2&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;Tweets can contain many non-ascii characters. Therefore, we need to sanitize it as best as possible so that it can be processed by the sentiment analyzer. To help with that, I used regular expresions to replace non-ascii characters inside of the &lt;code&gt;sanitize&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;private static string sanitize(string raw)
{
  return Regex.Replace(raw, @&amp;quot;(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)&amp;quot;, &amp;quot; &amp;quot;).ToString();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Sentiment Analysis&lt;/h2&gt;
&lt;p&gt;In order to perform sentiment analysis, we will be using the &lt;code&gt;SimpleNetNlp&lt;/code&gt; library. This library is built on top of the Stanford CoreNLP library. In order to get the sentiment of a piece of text, we need to create a &lt;code&gt;Sentence&lt;/code&gt; object which takes a string as a parameter and then get the &lt;code&gt;Sentiment&lt;/code&gt; property. In our case, the parameter that will be used to instantiate a new &lt;code&gt;Sentence&lt;/code&gt; object will be the sanitized text of a tweet.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var sanitized = sanitize(args.Tweet.FullText);
string sentence = new Sentence(sanitized);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above will be placed inside of the &lt;code&gt;OnMatchedTweet&lt;/code&gt; method.&lt;/p&gt;
&lt;h2&gt;Produce Output&lt;/h2&gt;
&lt;p&gt;Now that we have everything set up, we can just output to the console the sentiment and raw text of the tweet. To do that, we can place the code below inside the &lt;code&gt;OnMatchedTweet&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;Console.WriteLine(sentence.Sentiment + &amp;quot;|&amp;quot; + args.Tweet);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final &lt;code&gt;OnMatchedTweet&lt;/code&gt; method looks as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;private static void OnMatchedTweet(object sender, MatchedTweetReceivedEventArgs args)
{
    var sanitized = sanitize(args.Tweet.FullText); //Sanitize Tweet
    var sentence = new Sentence(sanitized); //Get Sentiment

    //Output Tweet and Sentiment
    Console.WriteLine(sentence.Sentiment + &amp;quot;|&amp;quot; + args.Tweet);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Run&lt;/h2&gt;
&lt;p&gt;Once we run the application, our console application should look something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/real-time-sentiment-analysis-csharp/sentiment-analysis-1.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;C# is not always the first language that comes to mind when doing analytics and machine learning. However, tasks such as sentiment analysis can be trivially performed thanks to libraries such as &lt;code&gt;Tweetinvi&lt;/code&gt; and &lt;code&gt;SimpleNetNlp&lt;/code&gt;. In its current state, this application is not very useful because it just outputs to the console sentiments and the respective tweets. In order to make it more useful, we can collect and aggregate the data someplace for more robust analysis.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/real-time-sentiment-analysis-csharp?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/real-time-sentiment-analysis-csharp</guid>
      <pubDate>2018-01-18 03:41:47</pubDate>
      <category>#data-analysis</category>
      <category>#sentiment-analysis</category>
      <category>#nlp</category>
      <category>#machine-learning</category>
      <category>#csharp</category>
      <category>#csharp</category>
      <category>#twitter</category>
      <category>#api</category>
      <category>#dotnet</category>
      <category>#dotnet</category>
    </item>
    <item>
      <title>Authorization Code Authentication Flow in Python</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/authorization-code-flow-python?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/authorization-code-flow-python&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;For the longest time, authentication had been my worst enemy. I took small steps towards becoming comfortable with it. At first I learned Implicit Authorization with AngularJS, then I learned Client Credentials Authorization with Python and C# and finally, I learned Authorization Code authentication with C#. The one that always gave me trouble was Authorization Code authentication because it requires user credentials.&lt;/p&gt;
&lt;p&gt;To help with that, I started with the &lt;code&gt;WebAuthenticationBroker&lt;/code&gt; in C#. Although, most of it was done for me in terms of creating a &lt;code&gt;WebView&lt;/code&gt; for users to authenticate with their credentials, I wanted to know whether I could do it myself without having to rely on a service to do it for me.&lt;/p&gt;
&lt;p&gt;First, I had to find an API to test this out with and in an attempt to reduce complexity, I decided to prototype it in Python. As an avid user of &lt;a href="http://getpocket.com"&gt;Pocket&lt;/a&gt;, I chose its API to learn Authorization Flow authentication.&lt;/p&gt;
&lt;h1&gt;Getting Started&lt;/h1&gt;
&lt;h2&gt;Register An Application&lt;/h2&gt;
&lt;p&gt;In order to get started, we need to register an application with Pocket. This can be done by visiting the following &lt;a href="https://getpocket.com/developer/apps/new"&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Install Requests&lt;/h2&gt;
&lt;p&gt;In order to create HTTP requests, we need to install the &lt;code&gt;requests&lt;/code&gt; module&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pip install requests
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Get Request Token&lt;/h1&gt;
&lt;h2&gt;Import Modules&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import requests
import json
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Prepare Request&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;consumer_key = &amp;quot;YOUR_CONSUMER_KEY&amp;quot;
request_url = &amp;quot;https://getpocket.com/v3/oauth/request&amp;quot;
headers = {&amp;quot;Content-Type&amp;quot;:&amp;quot;application/json&amp;quot;,&amp;quot;X-Accept&amp;quot;:&amp;quot;application/json&amp;quot;}
redirect_uri = &amp;quot;http://www.google.com&amp;quot;
payload = {&amp;quot;consumer_key&amp;quot;:consumer_key,&amp;quot;redirect_uri&amp;quot;:redirect_uri}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how the &lt;code&gt;redirect_uri&lt;/code&gt; is set to &lt;code&gt;http://www.google.com&lt;/code&gt;. This, for the most part is irrelevant, especially for console/desktop applications. Since this is a console application and we are not hosting a server for it, the value assigned to this field is arbitrary.&lt;/p&gt;
&lt;h2&gt;Make Request&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;request = requests.post(request_url,headers=headers,data=json.dumps(payload))
code = request.json()['code']
code
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Redirect User to Pocket to Continue Authorization&lt;/h1&gt;
&lt;h2&gt;Import Modules&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import webbrowser
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;webbrowser.open('https://getpocket.com/auth/authorize?request_token='+ code + '&amp;amp;redirect_uri=' + redirect_uri)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will open your default browser and redirect you to an authorization page. After logging in, should you accept giving the application access to your account, it should redirect you to &lt;code&gt;http://www.google.com&lt;/code&gt;. Once on the Google page, you can close out of it.&lt;/p&gt;
&lt;h1&gt;Convert A Request Token Into A Pocket Access Token&lt;/h1&gt;
&lt;p&gt;After authorizing the application to access your account, you need to exchange the code you received for an access token.&lt;/p&gt;
&lt;h2&gt;Prepare the Request&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;access_token_url = &amp;quot;https://getpocket.com/v3/oauth/authorize&amp;quot;
payload = {&amp;quot;consumer_key&amp;quot;:consumer_key,&amp;quot;code&amp;quot;:code}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Get Access Token&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;access_token_request = requests.post(access_token_url,headers=headers,data=json.dumps(payload))
access_token = access_token_request.json()['access_token']
access_token
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Make Authenticated Request&lt;/h1&gt;
&lt;p&gt;In order to test whether the authentication was successful, try making an authenticated request.&lt;/p&gt;
&lt;h2&gt;Prepare Request&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;get_url = &amp;quot;https://getpocket.com/v3/get&amp;quot;
get_payload = {&amp;quot;consumer_key&amp;quot;:consumer_key,&amp;quot;access_token&amp;quot;:access_token,&amp;quot;contentType&amp;quot;:&amp;quot;article&amp;quot;,&amp;quot;count&amp;quot;:10}
get_request = requests.get(get_url,data=json.dumps(get_payload),headers=headers)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Get Response&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;response = get_request.json()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Sample Response&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{'complete': 1,
 'error': None,
 'list': {'1519049132': {'excerpt': 'Heads up! As part of our efforts to improve security and standards-based interoperability, we have implemented several new features in our authentication flows and made changes to existing ones.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://auth0.com/docs/api-auth/grant/authorization-code-pkce',
   'has_image': '0',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '1519049132',
   'resolved_id': '1519049132',
   'resolved_title': 'Calling APIs from Mobile Apps',
   'resolved_url': 'https://auth0.com/docs/api-auth/grant/authorization-code-pkce',
   'sort_id': 5,
   'status': '0',
   'time_added': '1514493110',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514493110',
   'word_count': '445'},
  '1607678551': {'excerpt': 'What is the smallest number of Democrats that could have changed the outcome of the 2016 United States presidential election by relocating to another state? And where should they have moved?  It turns out this question is a variant of the knapsack problem, an NP-hard computer science classic.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://mike.place/2017/ecknapsack/',
   'has_image': '0',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '1607678551',
   'resolved_id': '1607678551',
   'resolved_title': 'The Electoral College and the knapsack problem',
   'resolved_url': 'http://mike.place/2017/ecknapsack/',
   'sort_id': 7,
   'status': '0',
   'time_added': '1514478701',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514478701',
   'word_count': '1564'},
  '1616406703': {'excerpt': 'OAuth 2.0 is a protocol that allows a user to grant limited access to their resources on one site, to another site. This is done without the users having to expose their credentials. According to OAuth‘s website the protocol is not unlike a valet key.  Many luxury cars today come with a valet key.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://auth0.com/docs/protocols/oauth2',
   'has_image': '1',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '1616406703',
   'resolved_id': '1616406703',
   'resolved_title': 'OAuth 2.0',
   'resolved_url': 'https://auth0.com/docs/protocols/oauth2',
   'sort_id': 4,
   'status': '0',
   'time_added': '1514493120',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514493120',
   'word_count': '823'},
  '1859670338': {'excerpt': 'Last Tuesday saw the official launch of Will Robots Take My Job? and 5 days later we have passed 500K visitors and 4M page views.  To say this surpassed any expectations we had would be a major understatement. This is a recap of how the project got started and the first 5 days after launch.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://hackernoon.com/from-idea-to-4m-page-views-in-4-weeks-622aa194787d?ref=quuu',
   'has_image': '1',
   'has_video': '1',
   'is_article': '1',
   'is_index': '0',
   'item_id': '1859670338',
   'resolved_id': '1773505341',
   'resolved_title': 'From Idea to 4M Page Views in 4\xa0Weeks',
   'resolved_url': 'https://hackernoon.com/from-idea-to-4m-page-views-in-4-weeks-622aa194787d',
   'sort_id': 0,
   'status': '0',
   'time_added': '1514494606',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514494606',
   'word_count': '1837'},
  '2001727306': {'amp_url': 'https://venturebeat.com/2017/12/22/how-ibm-builds-an-effective-data-science-team/amp/',
   'excerpt': 'Data science is a team sport. This sentiment rings true not only with our experiences within IBM, but with our enterprise customers, who often ask us for advice on how to structure data science teams within their own organizations.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://venturebeat.com/2017/12/22/how-ibm-builds-an-effective-data-science-team/',
   'has_image': '1',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '2001727306',
   'resolved_id': '2001727306',
   'resolved_title': 'How IBM builds an effective data science team',
   'resolved_url': 'https://venturebeat.com/2017/12/22/how-ibm-builds-an-effective-data-science-team/',
   'sort_id': 1,
   'status': '0',
   'time_added': '1514493979',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514493979',
   'word_count': '773'},
  '2006596547': {'excerpt': 'I ran, a lot, this year— 2017 miles over 280 hours, or just short of 12 full days. For many of you, that seems like a ridiculous amount of distance and time, but to put it in perspective, elite marathoners and ultrarunners easily run two or three times that distance in a year.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://medium.com/run-ruminate/17-thoughts-on-running-life-and-startups-5d305223669b',
   'has_image': '1',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '2006596547',
   'resolved_id': '2006596547',
   'resolved_title': '17 thoughts on running, startups, and life while running 2017 miles in\xa02017',
   'resolved_url': 'https://medium.com/run-ruminate/17-thoughts-on-running-life-and-startups-5d305223669b',
   'sort_id': 6,
   'status': '0',
   'time_added': '1514479812',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514479812',
   'word_count': '1561'},
  '2006920392': {'excerpt': 'What the heck is “work” anyway?  In the Information Age, the dictionary definition of the word just doesn’t cut it anymore. Skill sets, jobs, and entire companies are forming daily based on new technology, market demands, and trends that didn’t exist even just a few years ago.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://www.hellosign.com/blog/the-future-of-work',
   'has_image': '1',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '2006920392',
   'resolved_id': '2006920392',
   'resolved_title': '4 Pillars Of The Future of Work',
   'resolved_url': 'https://www.hellosign.com/blog/the-future-of-work',
   'sort_id': 2,
   'status': '0',
   'time_added': '1514493449',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514493449',
   'word_count': '1805'},
  '2007117419': {'excerpt': 'Finding a job is not an easy task. There are multiple ways to look for jobs. Multiple steps has to be completed before you are called for an interview. After completing all these the most annoying part is when your interview doesn’t even last for 10 minutes.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://www.jobsstudios.com/blog/interview-tips',
   'has_image': '0',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '2007117419',
   'resolved_id': '2007117419',
   'resolved_title': '7 Tips to prepare for an Interview',
   'resolved_url': 'https://www.jobsstudios.com/blog/interview-tips',
   'sort_id': 8,
   'status': '0',
   'time_added': '1514477292',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514477292',
   'word_count': '468'},
  '2007245672': {'excerpt': 'RESTful API Versioning, though a simple and elegant concept, is a LOT harder to enforce than it sounds. It’s hard to not break backward compatibility on a continually evolving API, and though API versioning is a great concept, it’s rarely followed without flaw.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://shift.infinite.red/snapshot-testing-api-calls-the-right-way-58ef59b7f71b',
   'has_image': '1',
   'has_video': '1',
   'is_article': '1',
   'is_index': '0',
   'item_id': '2007245672',
   'resolved_id': '2007245672',
   'resolved_title': 'Snapshot Testing API Calls The Right\xa0Way',
   'resolved_url': 'https://shift.infinite.red/snapshot-testing-api-calls-the-right-way-58ef59b7f71b',
   'sort_id': 3,
   'status': '0',
   'time_added': '1514493189',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514493189',
   'word_count': '1230'},
  '234097661': {'excerpt': 'Sanchez examines the question, concluding that intellectuals support government intervention because it makes their work have greater importance.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://www.libertarianism.org/blog/why-do-intellecuals-support-government-solutions',
   'has_image': '0',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '234097661',
   'resolved_id': '234097661',
   'resolved_title': 'Why Do Intellectuals Support Government Solutions?',
   'resolved_url': 'https://www.libertarianism.org/blog/why-do-intellecuals-support-government-solutions',
   'sort_id': 9,
   'status': '0',
   'time_added': '1514477287',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514477287',
   'word_count': '1423'}},
 'search_meta': {'search_type': 'normal'},
 'since': 1514514495,
 'status': 1}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hope this helps!&lt;/p&gt;
&lt;h6&gt;Sources&lt;/h6&gt;
&lt;p&gt;&lt;a href="https://getpocket.com/developer/docs/authentication"&gt;Pocket API&lt;/a&gt;&lt;br /&gt;
&lt;a href="http://docs.python-requests.org/en/master/"&gt;Requests&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/authorization-code-flow-python?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/authorization-code-flow-python</guid>
      <pubDate>2017-12-29 18:45:19</pubDate>
      <category>#python</category>
      <category>#api</category>
      <category>#programming</category>
      <category>#security</category>
      <category>#authentication</category>
    </item>
    <item>
      <title>Client Credentials Authorization in C#</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/client-credentials-authentication-csharp?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/client-credentials-authentication-csharp&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Getting Started&lt;/h1&gt;
&lt;p&gt;For this writeup, I'm going to use the Spotify API. Spotify API supports different authorization flows. In this writeup, I will be using the client credentials authorization flow. Generally this works for server-to-server authentication. Because this does not allow users the ability to provide their own credentials, there is no access to endpoints that contain user data.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/client-credentials-authentication-csharp/clientcredentialsflowdiagram.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h1&gt;Create a new .NET Core Console Application&lt;/h1&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new console -o authtest
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Add Dependencies&lt;/h1&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet add package Newtonsoft.Json --version 10.0.3
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Access Token Model&lt;/h1&gt;
&lt;p&gt;When a request is made, it needs to be parsed. To better capture the data into a Plain Old CLR Object (POCO), a model can be created.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System;

namespace authtest
{
    class AccessToken
    {
        public string access_token { get; set; }
        public string token_type { get; set; }
        public long expires_in { get; set; }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Request Token&lt;/h1&gt;
&lt;p&gt;In order to extract our token, an HTTP Request needs to be made to the Spotify API in order to get an access token. To do so, we can leverage the &lt;code&gt;HTTPClient&lt;/code&gt; functionalities.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;private static async Task&amp;lt;string&amp;gt; GetToken()
{
    string clientId = &amp;quot;YOUR CLIENT ID&amp;quot;;
    string clientSecret = &amp;quot;YOUR CLIENT SECRET&amp;quot;;
    string credentials = String.Format(&amp;quot;{0}:{1}&amp;quot;,clientId,clientSecret);

    using(var client = new HttpClient())
    {
        //Define Headers
        client.DefaultRequestHeaders.Accept.Clear();
        client.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(&amp;quot;application/json&amp;quot;));

        client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(&amp;quot;Basic&amp;quot;,Convert.ToBase64String(Encoding.UTF8.GetBytes(credentials)));

        //Prepare Request Body
        List&amp;lt;KeyValuePair&amp;lt;string,string&amp;gt;&amp;gt; requestData = new List&amp;lt;KeyValuePair&amp;lt;string,string&amp;gt;&amp;gt;();
        requestData.Add(new KeyValuePair&amp;lt;string,string&amp;gt;(&amp;quot;grant_type&amp;quot;,&amp;quot;client_credentials&amp;quot;));

        FormUrlEncodedContent requestBody = new FormUrlEncodedContent(requestData);

        //Request Token
        var request = await client.PostAsync(&amp;quot;https://accounts.spotify.com/api/token&amp;quot;,requestBody);
        var response = await request.Content.ReadAsStringAsync();
        return JsonConvert.DeserializeObject&amp;lt;AccessToken&amp;gt;(response);   
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then use this function in our main method to request the token.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;static void Main(string[] args)
{
    Console.WriteLine(&amp;quot;Spotify API&amp;quot;);
    AccessToken token = GetToken().Result;
    Console.WriteLine(String.Format(&amp;quot;Access Token: {0}&amp;quot;,token.access_token));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This should produce the following output&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;Spotify API
Getting Token
Access Token: &amp;quot;YOUR ACCESS TOKEN&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The token can now be used to make requests to the Spotify API.&lt;/p&gt;
&lt;p&gt;Source Code can be found here &lt;a href="https://gist.github.com/lqdev/5e82a5c856fcf0818e0b5e002deb0c28"&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h6&gt;Sources&lt;/h6&gt;
&lt;p&gt;&lt;a href="https://developer.spotify.com/web-api/authorization-guide/#client_credentials_flow"&gt;Spotify Authorization Guide&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/client-credentials-authentication-csharp?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/client-credentials-authentication-csharp</guid>
      <pubDate>2017-12-25 19:28:48</pubDate>
      <category>#csharp</category>
      <category>#authentication</category>
      <category>#web-applications</category>
      <category>#dotnet</category>
      <category>#api</category>
      <category>#dotnet-core</category>
    </item>
    <item>
      <title>Read A Text File From a URL in .NET Core</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/read-file-from-url-dotnet-core?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/read-file-from-url-dotnet-core&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Create New .NET Core Project&lt;/h1&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet new console -o urlreader
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Navigate to Project Folder&lt;/h1&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;cd urlreader
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Import Dependencies&lt;/h1&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using System.IO;
using System.Net.Http;
using System.Threading.Tasks;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Create Reader&lt;/h1&gt;
&lt;p&gt;Create a function in the &lt;code&gt;Program.cs&lt;/code&gt; file that will return a &lt;code&gt;StreamReader&lt;/code&gt; from the specified URL. Because in this case it will be used inside the &lt;code&gt;Main&lt;/code&gt; method which is &lt;code&gt;static&lt;/code&gt;, the new function will also have to be &lt;code&gt;static&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;static StreamReader URLStream(String fileurl){
    return new StreamReader(new HttpClient().GetStreamAsync(fileurl).Result);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the function, we return a &lt;code&gt;StreamReader&lt;/code&gt; which is instantiated by creating an &lt;code&gt;HTTPClient&lt;/code&gt; and asynchronously executing a &lt;code&gt;GET&lt;/code&gt; request which returns a &lt;code&gt;Task&amp;lt;StreamReader&amp;gt;&lt;/code&gt;. In order to resolve the object, the &lt;code&gt;Result&lt;/code&gt; property needs to be requested.&lt;/p&gt;
&lt;h1&gt;Reading File Contents&lt;/h1&gt;
&lt;p&gt;Replace the contents of the &lt;code&gt;Main&lt;/code&gt; method with the following&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;static void Main(string[] args) {
    string line;
    StreamReader s = URLStream(@&amp;quot;https://algs4.cs.princeton.edu/15uf/tinyUF.txt&amp;quot;);
    String myline = s.ReadLine(); //First Line
    while((line = s.ReadLine()) != null) //Subsequent Lines
    {
        Console.WriteLine(line);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, the format of the first line is different from the others. However, reading the contents of that line is no different than the other lines.&lt;/p&gt;
&lt;h1&gt;Run The Program&lt;/h1&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;dotnet run

#Expected Output
#4 3
#3 8
#6 5
#9 4
#2 1
#8 9
#5 0
#7 2
#6 1
#1 0
#6 7
&lt;/code&gt;&lt;/pre&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/read-file-from-url-dotnet-core?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/read-file-from-url-dotnet-core</guid>
      <pubDate>2017-12-18 21:09:15</pubDate>
      <category>#dotnet</category>
      <category>#dotnet-core</category>
      <category>#microsoft</category>
      <category>#programming</category>
    </item>
    <item>
      <title>Design and Build Your Next Home in Minecraft</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/build-your-house-minecraft?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/build-your-house-minecraft&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h4&gt;TLDR&lt;/h4&gt;
&lt;p&gt;Using Minecraft and Mods, virtual representations of the real world can be built to train autonomous agents to navigate and perform tasks in this virtual environment. These virtual models can then be used by physical agents such as Boston Dynamics bots to implement the tasks and instructions learned using the virtual models. Ultimately this makes it reasonable to think of a time where we can design and have our house built in a short time span by autonomous artificial agents.&lt;/p&gt;
&lt;h1&gt;Minecraft&lt;/h1&gt;
&lt;p&gt;Minecraft is so hard to describe. Is it a game or is it an educational tool? At first I didn’t get it. I watched younger siblings play this game endlessly always finding ways to improve their worlds. Eventually it was bought by Microsoft and since then it has left the core experience relatively untouched. Instead of shelving it away, Microsoft has expanded across more platforms and loaded it with tons of new features such as an education edition and cross-platform play. In Minecraft I have seen individuals build houses, cities, and even rudimentary models of circuits using Redstone. Furthermore, because the project is programmable, individuals can build modifications also known as Mods to extend the default capabilities.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/build-your-house-minecraft/buildyourhouseminecraft1.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h1&gt;Dynamic Mapping&lt;/h1&gt;
&lt;p&gt;A virtual world is great, but what about the real world? Can we model the real world? A few months ago I came across a project by the New York Public Library which generated topographically accurate Minecraft maps using historical map scans and Python scripts. Although the accuracy of historical maps may be questionable, we can only hope that mapping practices have advanced significantly since then and can provide more details and data. Regardless, this project demonstrates that it is possible to model the real world in a virtual environment given the appropriate data.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/build-your-house-minecraft/buildyourhouseminecraft2.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h1&gt;Project Malmo&lt;/h1&gt;
&lt;p&gt;Aside from having the obvious benefit of having a relatively accurate virtual model of the real world, there is the additional benefit of having a starting point for training autonomous agents to navigate this environment. To aid in this endeavor, Project Malmo is an adequate tool. Released to the public in 2015 by Microsoft Research, Project Malmo is a platform for artificial intelligence experimentation and research built on top of Minecraft. The project is cross-platform and agents can be built in Python, Lua, C#, C++ or Java.&lt;/p&gt;
&lt;p&gt;While the benefits of Project Malmo aside from the pedagogical exercise may not be clearly evident, this project is extremely valuable. The cost not only in terms of price but also time to build physical autonomous bots can be high. If for some reason they were to break or malfunction during field tests it can be costly. Therefore a more effective solution might be to simulate the navigation and mobility of these physical autonomous agents in a real world scenario using virtual environments.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.youtube.com/watch?v=KkVj_ddseO8"&gt;&lt;img src="/images/build-your-house-minecraft/buildyourhouseminecraft3.png" class="img-fluid" alt="Project Malmo" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Manual Labor&lt;/h1&gt;
&lt;p&gt;Once trained, we just need to find or build the physical agents that will carry out the work and instructions based on training performed in the virtual environments. There have been recent demonstrations of agents that may be able of handling such tasks. One of those is Boston Dynamics. For some time, Boston Dynamics has been creating anthropomorphic and zoomorphic robots that perform amazing feats. It can only be assumed that given a model that has been trained in a real-world environment, these physical agents can perform a task like building a home.&lt;/p&gt;
&lt;p&gt;The thought of a bot building a house is not too abstract. On March of 2017 several reports suggested that a home in Russia was built in under 24 hours using 3-D printing technologies. While the house had a predefined layout and the robotic arm that built it was limited in its mobility, such capabilities could be expanded with the aid of a trained model being executed by sophisticated physical autonomous agents such as those built by Boston Dynamics.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/build-your-house-minecraft/buildyourhouseminecraft4.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;h6&gt;Sources&lt;/h6&gt;
&lt;p&gt;&lt;a href="https://www.engadget.com/2017/06/14/minecraft-4k-slider-xbox-one-x-e3-microsoft-mojang/"&gt;4K Minecraft&lt;/a&gt;
&lt;a href="https://github.com/NYPL/historical-minecraft/"&gt;Historical Minecraft&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://www.microsoft.com/en-us/research/project/project-malmo/"&gt;Project Malmo&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://www.bostondynamics.com"&gt;Boston Dynamics&lt;/a&gt;&lt;br /&gt;
&lt;a href="http://mentalfloss.com/article/92757/3d-printed-house-built-24-hours-russia"&gt;House Built in 24 Hours&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/build-your-house-minecraft?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/build-your-house-minecraft</guid>
      <pubDate>2017-12-15 22:34:12</pubDate>
      <category>#microsoft</category>
      <category>#artificial-intelligence</category>
      <category>#thoughts</category>
      <category>#robots</category>
    </item>
    <item>
      <title>The Case for .NET in the Classroom</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/dotnet-in-the-classroom?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/dotnet-in-the-classroom&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I recently finished by M.S. Computer Science degree and throughout the course, I used a variety of programming languages to complete assignments such as Matlab (Octave), Python, R, Java, Scala, Scheme (Lisp), and C. While all powerful in their own way, in my personal time (if and when I had any), I tried to learn about .NET tools and the ecosystem as a whole mainly because I have been a long time user of Microsoft products that include but are not limited to Zune, Kin, Surface and Windows Phone. Coincidentally, around the time I started my program, Microsoft was in the process of making an aggressive push to be more open and return to its roots of making tools and services that developers love. That being said, this is not exactly something that many people are aware of and the company's image in the eyes of consumers and developers using open source technologies has only recently started to improve. With recent acquisitions of companies like Skype and Xamarin as well as products like HoloLens, the opportunities for developers to build transformative solutions for consumers and businesses across a wide range of devices is limitless. Equally exciting and advantageous is the fact that the underlying ecosystem many of these technologies rely on is .NET. Keeping all that in mind, I strongly believe that there is no better time to introduce .NET into the classroom, especially at the university level in order to equip the next generation of developers with the necessary tools to achieve more in a wide range of environments.&lt;/p&gt;
&lt;h2&gt;Microsoft Past&lt;/h2&gt;
&lt;h3&gt;Enterprise and Licensing&lt;/h3&gt;
&lt;p&gt;For the longest time, Microsoft has been synonymous with enterprise and rightfully so as most businesses run Windows and Office. What this also meant was licensing. Many of the products and services that Microsoft provided for businesses and developers were licensed products. Even though there were in some instances free versions of the products, they had their limitations. As a result, this naturally created a barrier to entry to try and experiment with their products.&lt;/p&gt;
&lt;h3&gt;Overhead&lt;/h3&gt;
&lt;p&gt;Even when trying to do the simplest thing with .NET Framework (particularly C#, ASP.NET or F#), you needed to run Visual Studio. Like the enterprise and licensing, this provided a barrier to entry for someone trying to learn the language. Conversely, in the most closely related language Java, you could've gotten started with a text editor and command line (among other things, this might be one reason Java has been widely used in the classroom).&lt;/p&gt;
&lt;h3&gt;Platform Specific&lt;/h3&gt;
&lt;p&gt;Although most of the world ran Windows, developer tools and products like the .NET Framework targeted Windows only. While this was acceptable a few years ago, the world is much different today than it was then. The current state of computing, while not entirely a statement on Microsoft and their products is one that Microft has astutely recognized. People want their services and experiences to travel with them regardless of the platform or products they use.&lt;/p&gt;
&lt;p&gt;Along these lines is the type of devices individuals performed their computing on. A few years ago, the Windows PC was the main device for productivity. Therefore, there was little to no need to focus on mobile, let alone cross-platform capabilities. Now, individuals can perform elaborate and deeply complex workflows from the palm of their hand. Therefore, the focus has shifted from a single device category to multi-device and multi-platform one.&lt;/p&gt;
&lt;h2&gt;Microsoft Today&lt;/h2&gt;
&lt;p&gt;Microsoft has significantly transformed their business practices and refocused on what at one time made them one of the top companies in the world by making experiences and products consumers and more importantly developers love.&lt;/p&gt;
&lt;h3&gt;Mobile-First, Cloud-First&lt;/h3&gt;
&lt;p&gt;This was the strategy Satya Nadella tried to get the entire team to rally around when he became CEO of the company. At the time, Microsoft had recently purchased Nokia, yet its mobile phone efforts were non-existent aside from the three percent. Therefore, this strategy seemed ambiguous and unattainable at least from a consumer perspective.&lt;/p&gt;
&lt;p&gt;Upon further inspection, this strategy is not limited to mobile phones. Instead it refers to the ability of individuals' computing experiences, with the aid of the cloud, to go wherever they can be most productive. The hardware and to some extent, the software in terms of the platform no longer bind individuals as the services are available nearly everywhere.&lt;/p&gt;
&lt;p&gt;A shining example of this is Xamarin. Xamarin is a mobile app development platform built on the .NET ecosystem. With this purchase, Microsoft allowed developers who were native .NET developers to work in the same environment they were used to, while targeting a new family of devices running Android or iOS. Aside from learning the some of the inner workings the Xamarin framework, at the core, individuals can leverage the .NET tools they're used to using.&lt;/p&gt;
&lt;h3&gt;Emerging Technologies&lt;/h3&gt;
&lt;h4&gt;Artificial Intelligence&lt;/h4&gt;
&lt;p&gt;Artificial intelligence, data science and machine learning have been at the forefront of discussions involving technology both in the enterprise as well as academia. Microsoft has made strides on many fronts to position themselves and provide the means for individuals to use the Microsoft set of tools to create solutions.
One of the ways Microsoft has worked on providing developers and individuals with tools for deep learning is the Microsoft Cognitive Toolkit (CNTK). This framework is very similar to Tensorflow. Although models can be built using BrainScript or Python, the models themselves can be easily plugged into .NET solutions seamlessly.
Another way in which Microsoft is providing tools for artificial intelligence is through its Bot Framework. The Bot Framework has different parts. One of them is the creation of chat bots using either NodeJS or C#. These bots can target different chat applications that include but are not limited to Skype, Facebook Messenger, Slack and many others. These bots can be made even more intelligent with Microsoft's Language Understanding Service (LUIS) which takes care of the natural language processing the bot might need to process requests adequately.&lt;/p&gt;
&lt;p&gt;The second part of the Bot Framework is Cortana Skills. Using LUIS and the Bot Framework, individuals can create speech and text-enabled solutions for individuals that interact with Cortana enabled devices which include but are not limited to smart speakers, PCs and mobile devices. This new kind of experiences again is entirely backed by the .NET ecosystem many developers already use today.&lt;/p&gt;
&lt;h4&gt;Virtual and Mixed Reality&lt;/h4&gt;
&lt;p&gt;While still in its infancy, virtual and mixed reality have received tons of support from technology companies like Facebook with Oculus, Google with Google VR, Apple with AR Kit. Likewise, Microsoft's solution is HoloLens. Applications for HoloLens can be developed with .NET tools and Unity. Yet again, developers can leverage their .NET skills to build applications for yet another family of devices to build new immersive experiences that not too long ago were closer to science-fiction than reality.&lt;/p&gt;
&lt;h3&gt;.NET Core and .NET Standard&lt;/h3&gt;
&lt;p&gt;A lot of computing functions overall are fairly standard. For example, reading from the console is a relatively universal procedure regardless of the platform. Microsoft has found a way to take advantage of the power of .NET for these scenarios and released .NET Standard and .NET Core which are cross-platform environments to develop C#, ASP.NET and F# solutions. These applications can run on the web as well as any OS platform further expanding the capabilities of the ecosystem as well as extending the number of solutions developers can create.&lt;/p&gt;
&lt;p&gt;Furthermore, Visual Studio, while helpful is not a requirement to start creating applications. Most things if not all can be done via the command line and a text editor. Therefore, the barrier to entry both in terms of cost and start time has been greatly reduced and individuals get started building applications in little to no time for free.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;While this is not the entire gamut of tools and services Microsoft provides, it is a good demonstration of how learning a single set of skills like C# and F# in conjunction with the standard curriculum can greatly empower the next generation of computer scientists and developers by expanding the number of devices and platforms they can target across a wide variety of industries and disciplines making the number of applications nearly limitless.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/dotnet-in-the-classroom?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/dotnet-in-the-classroom</guid>
      <pubDate>2017-12-14 16:57:39</pubDate>
      <category>#dotnet</category>
      <category>#microsoft</category>
      <category>#thoughts</category>
      <category>#programming</category>
      <category>#development</category>
    </item>
    <item>
      <title>Working with URLs in Racket</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/working-with-urls-racket?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/working-with-urls-racket&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;Often times when working with web applications, parameters and values are passed in the path as well as the query string of a URL. In order to do something useful, parsing of the url is required. Noted below are examples of how this can be done in an elegant manner using Racket.&lt;/p&gt;
&lt;p&gt;The first step is to import the appropriate packages&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-lisp"&gt;(require net/url)
(require net/url-structs)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Assuming we are working with the url string &amp;quot;https://hacker-news.firebaseio.com/v0/item/192327.json?print=pretty&amp;quot;, we can convert the string to a URL struct using the &lt;code&gt;string-&amp;gt;url&lt;/code&gt; procedure.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-lisp"&gt;(define myurl (string-&amp;gt;url &amp;quot;https://hacker-news.firebaseio.com/v0/item/192327.json?print=pretty&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is the following output&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-lisp"&gt;myurl
;;(url
;; &amp;quot;https&amp;quot;
;; #f
;; &amp;quot;hacker-news.firebaseio.com&amp;quot;
;; #f
;; #t
;; (list
;;  (path/param &amp;quot;v0&amp;quot; '())
;;  (path/param &amp;quot;item&amp;quot; '())
;;  (path/param &amp;quot;192327.json&amp;quot; '()))
;; '((print . &amp;quot;pretty&amp;quot;))
;; #f)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The structure of the URL struct is as follows and directly maps to the values displayed above.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-lisp"&gt;(struct url ( scheme
 	      user
 	      host
 	      port
 	      path-absolute?
	      path
 	      query
 	      fragment)
    #:extra-constructor-name make-url)
  scheme : (or/c false/c string?)
  user : (or/c false/c string?)
  host : (or/c false/c string?)
  port : (or/c false/c exact-nonnegative-integer?)
  path-absolute? : boolean?
  path : (listof path/param?)
  query : (listof (cons/c symbol? (or/c false/c string?)))
  fragment : (or/c false/c string?)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To access the individual data members of the struct, the syntax &lt;code&gt;&amp;lt;type&amp;gt;-&amp;lt;member&amp;gt; &amp;lt;instance&amp;gt;&lt;/code&gt;. What is returned can be further manipulated based on its type (string, list, boolean)&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-lisp"&gt;(url-host myurl)
;;&amp;quot;hacker-news.firebaseio.com&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h6&gt;Source&lt;/h6&gt;
&lt;p&gt;&lt;a href="https://docs.racket-lang.org/net/url.html"&gt;URLS and HTTP&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/working-with-urls-racket?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/working-with-urls-racket</guid>
      <pubDate>2017-12-13 16:01:57</pubDate>
      <category>#lisp</category>
      <category>#racket</category>
    </item>
    <item>
      <title>Reading Local JSON Files in R</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/reading-json-in-r?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/reading-json-in-r&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;The following example is a way of reading local JSON files into R scripts. This can be useful for config files, although it's still also acceptable for data files as well.&lt;/p&gt;
&lt;h1&gt;Import package to work with JSON and lists&lt;/h1&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;require(rlist)
require(rjson)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Load file into script&lt;/h1&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;configPath &amp;lt;- '.'
fileName &amp;lt;- 'config.json'
jsonFile &amp;lt;- fromJSON(file=paste(configPath,fileName,sep=&amp;quot;/&amp;quot;),method=&amp;quot;C&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data gets loaded in as a list. It may be tedious to access nested contents via indices. To aid with this, a helper function can be created to extract the individual nested data elements of the file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;#Extract values form nested list
#top_level = list.select(obj$topkey,obj$subkey)
extractData &amp;lt;- function(top_level) {
    output &amp;lt;- unlist(lapply(top_level,'[[',1));
    return(output);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Extract data&lt;/h1&gt;
&lt;p&gt;Below is a default sample json file that includes both nested and non-nested data.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;port&amp;quot;: 2100,
    &amp;quot;connectionStrings&amp;quot;: {
        &amp;quot;default&amp;quot;: &amp;quot;123.567.890&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data in that file can be accessed as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-r"&gt;#Not Nested
port &amp;lt;- config$port # Should store 2100

#Nested
connectionString &amp;lt;- extractData(list.select(config$connectionStrings,default)) #Should store 123.567.890
&lt;/code&gt;&lt;/pre&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/reading-json-in-r?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/reading-json-in-r</guid>
      <pubDate>2017-12-11 14:21:21</pubDate>
      <category>#r</category>
      <category>#programming</category>
    </item>
    <item>
      <title>Windows 10 Intel Display Driver Blank Screen Fix</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/windows-10-intel-display-driver-blank-screen?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/windows-10-intel-display-driver-blank-screen&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;Recently my Dell Inspiron P57G was experiencing issues when entering sleep mode. When trying to wake the PC, a blank screen was coming on. After some research, I concluded it was due to bugs with the most recent version of the display driver. These were the steps I took to remediate the issue.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go to &lt;a href="http://www.dell.com/support/home/us/en/04?app=drivers&amp;amp;c=us&amp;amp;l=en&amp;amp;%7Eck=mn"&gt;Dell Support&lt;/a&gt; page and find the Intel HD Graphics 510 515 520 530 540 Driver.&lt;/li&gt;
&lt;li&gt;Click 'Other Versions'&lt;/li&gt;
&lt;li&gt;Download the most recent driver prior to 08/13/2017.&lt;/li&gt;
&lt;li&gt;Follow the installation instructions and restart the PC.
&lt;ul&gt;
&lt;li&gt;If prompted to overwrite newer drivers, select 'Yes'&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After restart, either force the PC to sleep or let it sleep on its own. The problem should've been fixed.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/windows-10-intel-display-driver-blank-screen?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/windows-10-intel-display-driver-blank-screen</guid>
      <pubDate>2017-12-09 14:27:31</pubDate>
      <category>#sysadmin</category>
      <category>#drivers</category>
      <category>#pc-issues</category>
    </item>
    <item>
      <title>Creating A Login Screen With React Native</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/create-login-screen-react-native?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/create-login-screen-react-native&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;This assumes that you have Node and NPM installed on your PC. If not, you can go to the official website and install them. Then, come back to this page.&lt;/p&gt;
&lt;h1&gt;Part I – Install React-Native&lt;/h1&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g create-react-native-app
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Part II – Create and Run Project&lt;/h1&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;create-react-native-app myfirstreactnativeapp
cd myfirstreactnativeapp
npm start
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Part III – Create Login Screen&lt;/h1&gt;
&lt;p&gt;In the &lt;code&gt;App.js&lt;/code&gt; file that is created, we’re going to want to edit a few details.&lt;/p&gt;
&lt;p&gt;At the top of the file, add the following line of code to import the visual components we will utilize to create the page.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-javascript"&gt;Import { TextInput, StyleSheet, Button } from 'react-native';
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside of your &lt;code&gt;App&lt;/code&gt; class, you want to add a few things. The first would be to add a constructor for the component. This will allow us to define and keep track of the text input being provided by the user.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-javascript"&gt;constructor(props) {
  super(props);
  this.state = {
    name: &amp;quot;&amp;quot;,
    email: &amp;quot;&amp;quot;,
    password: &amp;quot;&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we want to update our render method to show the text inputs and submit button.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-javascript"&gt;render(){
  return (
    &amp;lt;View&amp;gt;
      &amp;lt;TextInput
        style={styles.input}
        onChangeText={(text) =&amp;gt; this.setState({name: text})}
        placeholder=&amp;quot;Name&amp;quot;
      /&amp;gt;
      &amp;lt;TextInput
        style={styles.input}
        onChangeText={(text) =&amp;gt; this.setState({email: text})}
        placeholder=&amp;quot;E-Mail&amp;quot;
      /&amp;gt;
      &amp;lt;TextInput
        style={styles.input}
        onChangeText={(text) =&amp;gt; this.setState({password:text})}
        secureTextEntry={true}
        placeholder=&amp;quot;Password&amp;quot;
      /&amp;gt;
      &amp;lt;Button
        onPress={this.submit}
        title=&amp;quot;Submit&amp;quot;
        color=&amp;quot;#841584&amp;quot;
      /&amp;gt;
    &amp;lt;/View&amp;gt;
  )
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;StyleSheet&lt;/code&gt; component, we’ll also want to add some styling to our inputs. We can do so by adding the following lines of code outside of our App class definition. These styles are applied as properties of the &lt;code&gt;TextInput&lt;/code&gt; components inside the App class.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-javascript"&gt;const styles = StyleSheet.create({
  input: {
    width: 250,
    margin: 5
  }
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we need to create a submit method to handle the press event on the submit button. I have left it blank, since how you pass the e-mail and password is entirely up to you (although hopefully it is in the body of an HTTP Request as opposed to url parameters).&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-javascript"&gt;submit() {
//Do Something
}
&lt;/code&gt;&lt;/pre&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/create-login-screen-react-native?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/create-login-screen-react-native</guid>
      <pubDate>2017-12-09 13:15:57</pubDate>
      <category>#react</category>
      <category>#react-native</category>
      <category>#javascript</category>
    </item>
    <item>
      <title>Back To School: Emacs Edition</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/back-to-school-emacs-edition?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/back-to-school-emacs-edition&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;As the summer comes to a close and I prepare to enter the fall semester, I am getting back into school mode. When programming in the terminal, I usually used vim. When compared to Emacs, the learning curve was relatively flat and it handled file editing just fine. I’ve read many articles that rave about Emacs and how powerful it can be. Therefore, a couple of months ago I took the plunge and decided to give it a try. Using dired mode to browse through the various directories in my system or project was a neat feature but for my purposes of basic file editing it did not offer anything that would warrant a switch. Furthermore, the default key bindings seemed a bit too much for me which caused me to  revert back to vim.&lt;/p&gt;
&lt;p&gt;As part of the articles that I have read in praise of Emacs, one particular feature seemed to be a recurring theme – Org-Mode. Although I had heard about it, I never really understood what it was. Out of curiosity, I decided to try it out for myself and see what it had to offer. While taking notes in it seemed normal enough, it was the export functionality that truly rocked my world and made me realize how amazing this could be for note-taking. Traditionally a pen-and-paper note taker, one of the main reasons I dislike taking notes on my PC is the lack of continuity and organization. Although tools like OneNote of Evernote are extremely powerful, they were not exactly the best when it came to a semester’s worth of notes and although they could be made to have all of the notes for a semester on one page, this defeated the purpose of having different pages and notebooks. Enter Org-Mode. Again, while the process of taking notes is nothing special, the export capabilities are amazing. Furthermore, by default a table of contents is created with links so you can always find what you want when you want. Here are a few examples of what I mean.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/back-to-school-emacs-edition/backtoschoolemacsedition1.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;As it can be seen, other than * or + characters in front of titles and lists, there is nothing special in the terminal. This is how it looks like in HTML format&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/back-to-school-emacs-edition/backtoschoolemacsedition2.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Amazing! Not only do you get an organized set of notes, but you can see how this can easily contain a semester’s worth of notes in a single HTML file with main ideas being easily accessible via links. Now, maybe you don’t have an internet connection or prefer to put these notes on your tablet or phone. No problem. Just export it to a PDF.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/back-to-school-emacs-edition/backtoschoolemacsedition3.png" class="img-fluid" alt="" /&gt;&lt;/p&gt;
&lt;p&gt;Magic!&lt;/p&gt;
&lt;p&gt;This is definitely going to take my note-taking to the next level by allowing me to have these documents synced across all my devices. Furthermore, it will keep notes organized and in one place. If you’re interested in getting started there may be just a few things that you need to do.&lt;/p&gt;
&lt;h1&gt;Install Org-Mode&lt;/h1&gt;
&lt;p&gt;Org Mode is built in to the latest version of Emacs. However, in the event that it is not installed you can simply try the following commands in the Emacs mini buffer.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;M-x package-install RET org RET&lt;/code&gt;&lt;/p&gt;
&lt;h1&gt;Export to HTML&lt;/h1&gt;
&lt;p&gt;To access the export menu, simply type the following in the mini buffer:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;C-c C-e&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;HTML export is included by default. To do that, we can add on to the command used to access the export menu as follows:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;C-c C-e h h&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;PDF export is a bit mode complicated. The file is first exported to LaTeX format and then converted to PDF. However, in order to do this you need to install a package on your system. Assuming you are on a Unix machine, the following command should download everything necessary to convert LaTeX files to PDF. Keep in mind, this is relatively large and should take up roughly 1-2 GB in your system. In return, it limits the number of headaches due to installing necessary packages piecemeal that have multiple dependencies. To download the package, type in the following into your terminal:&lt;/p&gt;
&lt;h1&gt;Install texlive-full&lt;/h1&gt;
&lt;p&gt;In the terminal, enter:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get install texlive-full
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once installed, you can use a similar command to that of HTML export.&lt;/p&gt;
&lt;h1&gt;Export to PDF&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;C-c C-e l p&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This should be all you need to get started. For more information on how to use Org Mode, here’s a guide with live examples. Happy note-taking!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://home.fnal.gov/%7Eneilsen/notebook/orgExamples/org-examples.html"&gt;Emacs Examples and Cookbook&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/back-to-school-emacs-edition?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/back-to-school-emacs-edition</guid>
      <pubDate>2017-12-09 13:01:19</pubDate>
      <category>#emacs</category>
      <category>#org-mode</category>
    </item>
    <item>
      <title>HP ProBook 430 G1 Ubuntu BIOS Upgrade</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/hp-probook-430g1-ubuntu-bios-upgrade?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/hp-probook-430g1-ubuntu-bios-upgrade&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;I recently got an HP ProBook 430 G1 which came preinstalled with Windows 10. I have several Windows 10 devices, but needed a Linux system while on the go. I had given my HP Chromebook a try using Crouton. This worked okay. However, I was limited by the specs of the Chromebook and wanted something with more power for development purposes. Therefore, I erased Windows 10 on the ProBook and installed Ubuntu. The PC worked well. My only problem with it was that I found my battery meter was inaccurate and would reach a percentage in the 60’s before suddenly dropping to zero and shutting down. This was not something I could have, especially during the middle of a work session where I might lose some/all of my work.&lt;/p&gt;
&lt;p&gt;Immediately, I assumed it had something to do with drivers/BIOS being out of date. To my surprise, BIOS updates were only available for Windows. Fortunately, through some research I was able to find a solution that allowed me to update my BIOS and fix my battery problems.&lt;/p&gt;
&lt;h2&gt;Prerequisites:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Make sure PC is connected to the charger&lt;/li&gt;
&lt;li&gt;4GB USB Drive&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Procedure:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Format USB drive. (FAT32 required)&lt;/li&gt;
&lt;li&gt;Create folder inside USB drive called Hewlett-Packard\BIOS\New&lt;/li&gt;
&lt;li&gt;Download latest BIOS .exe file from HP Support website&lt;/li&gt;
&lt;li&gt;Unzip .exe file. While 7Zip is not required, I found this to work well.&lt;/li&gt;
&lt;li&gt;Copy .bin file to Hewlett-Packard\BIOS\New folder&lt;/li&gt;
&lt;li&gt;Copy entire BIOSUpdate folder to Hewlett-Packard folder.&lt;/li&gt;
&lt;li&gt;Restart PC&lt;/li&gt;
&lt;li&gt;Continuously press F10 to enter BIOS Setup&lt;/li&gt;
&lt;li&gt;Select Update System BIOS menu option&lt;/li&gt;
&lt;li&gt;Select Update BIOS Using Local Media option&lt;/li&gt;
&lt;li&gt;Select Accept&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point, the installation process will take effect and the PC will restart itself at which point the your BIOS should be up to date.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/hp-probook-430g1-ubuntu-bios-upgrade?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/hp-probook-430g1-ubuntu-bios-upgrade</guid>
      <pubDate>2017-12-09 12:57:12</pubDate>
      <category>#sysadmin</category>
      <category>#ubuntu</category>
      <category>#bios</category>
    </item>
    <item>
      <title>Setting Up A New Ubuntu PC</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/setting-up-new-ubuntu-pc?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/setting-up-new-ubuntu-pc&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;h1&gt;Users&lt;/h1&gt;
&lt;p&gt;This assumes that you are currently the root user.&lt;/p&gt;
&lt;p&gt;Enter the adduser to command to create a new user.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;adduser &amp;lt;username&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Immediately, you will be prompted for a password. Enter the password and continue the process.&lt;/p&gt;
&lt;h2&gt;Adding Root Privileges&lt;/h2&gt;
&lt;p&gt;This is only applicable when granting root priviliges to the newly created user.&lt;/p&gt;
&lt;p&gt;Enter the usermod command to add the new user to the sudo (root) group&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;usermod -aG sudo &amp;lt;username&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The user should now have root priviliges.&lt;/p&gt;
&lt;p&gt;To test that it worked log in as that user and try to run a command with sudo&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;su - &amp;lt;username&amp;gt;
sudo ls -la /root
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Emacs&lt;/h1&gt;
&lt;p&gt;By default, Ubuntu usually comes preloaded or has the default 24.x version of Emacs. I often like to use Emacs along with org-mode and to take full advantage, especially as it pertains to working with source code I have found version 25.x is much better suited for the task. While there are many ways to go about it, installing it via package manager appears to be the easiest.&lt;/p&gt;
&lt;h2&gt;Add repository to PPA&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo add-apt-repository ppa:kelleyk/emacs
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Update the packages&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt update
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Install Emacs.&lt;/h2&gt;
&lt;h3&gt;Text-Only Interface&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get install emacs25-nox
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;GUI Interface&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get install emacs25
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Backup files&lt;/h2&gt;
&lt;p&gt;Sometimes Emacs keeps a backup file by default. This can cause a lot of clutter in the current directory where work is being performed. There is a way to disable this.&lt;/p&gt;
&lt;p&gt;Navigate to &lt;code&gt;~/.emacs.d/&lt;/code&gt; and edit the &lt;code&gt;init.el&lt;/code&gt; file and add the following line.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-lisp"&gt;(setq make-backup-files nil)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Java&lt;/h1&gt;
&lt;p&gt;Make sure that your packages and repositories are up to date&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;OpenJDK&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get install default-jdk
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Oracle JDK&lt;/h2&gt;
&lt;h3&gt;Add PPA Repository&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo add-apt-repository ppa:webupd8team/java
sudo apt-get update
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Install the installer&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get install oracle-java8-installer
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Sources&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://www.digitalocean.com/community/tutorials/how-to-create-a-sudo-user-on-ubuntu-quickstart"&gt;Users&lt;/a&gt;&lt;br /&gt;
&lt;a href="http://ubuntuhandbook.org/index.php/2017/04/install-emacs-25-ppa-ubuntu-16-04-14-04/"&gt;Emacs&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://www.digitalocean.com/community/tutorials/how-to-install-java-with-apt-get-on-ubuntu-16-04"&gt;Java&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/setting-up-new-ubuntu-pc?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/setting-up-new-ubuntu-pc</guid>
      <pubDate>2017-12-09 12:49:19</pubDate>
      <category>#linux</category>
      <category>#ubuntu</category>
      <category>#java</category>
      <category>#sysadmin</category>
      <category>#emacs</category>
    </item>
    <item>
      <title>Octave Development in the Terminal</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/octave-development-in-the-terminal?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/octave-development-in-the-terminal&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;MatLab is a powerful tool that is used througout various disciplines for data processing and analysis. However, in order to get access to it and all its features, there is a fee associated with it. Depending on the needs of the user, this price can at times be somewhat large. Luckily, an open source alternative known as Octave exists which works just as good in many cases at zero monetary cost. Octave's syntax is for the most part identical to MatLab making the transition a relatively simple one.&lt;/p&gt;
&lt;h1&gt;Tools&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Octave&lt;/li&gt;
&lt;li&gt;Tmux&lt;/li&gt;
&lt;li&gt;Text Editor (Emacs)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Installation&lt;/h1&gt;
&lt;h2&gt;Tmux&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get install tmux
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Octave&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-add-repository ppa:octave/stable
sudo apt-get update
sudo apt-get install octave
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Setup&lt;/h1&gt;
&lt;h2&gt;Create a new tmux session&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;tmux new -s octave-ide
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Configure Layout&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-ascii"&gt;=========================
|           |           |
| Console   |           |
|           |           |
|-----------|  Script   |
|           |           |
| Terminal  |           |
|           |           |
=========================
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to get the layout shown above, the following steps need to be taken&lt;/p&gt;
&lt;h3&gt;Split panes horizontally&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Ctrl+b %&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;Split panes vertically&lt;/h3&gt;
&lt;h4&gt;Switch to pane 0&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Ctrl+b q 0&lt;/code&gt;&lt;/p&gt;
&lt;h4&gt;Do vertical split&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Ctrl+b &amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;h1&gt;Workflow&lt;/h1&gt;
&lt;p&gt;Now that the layout is setup, work can finally be done. To start off, initialize Octave in pane 0.&lt;/p&gt;
&lt;h2&gt;Test code snippets in the console&lt;/h2&gt;
&lt;p&gt;Enter the following snippet in the console window&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-octave"&gt;function res = square(x)
    res = x * x
endfunction
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then call it to make sure it does what is expected&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-octave"&gt;square(2) #Should equal 4
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Write code on script file&lt;/h2&gt;
&lt;p&gt;Once satisfied that the code works:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Switch to pane 2 using &lt;code&gt;Ctrl+b q 2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create a script file &lt;code&gt;myscript.m&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Edit &lt;code&gt;myscript.m&lt;/code&gt; with desired code&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-octave"&gt;1; # Prevents octave from processing file as function file.

function res = square(x)
    res = x * x
endfunction

square(2);
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Save the script&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Run script file&lt;/h2&gt;
&lt;p&gt;Switch to pane 1 &lt;code&gt;Ctrl+b q 1&lt;/code&gt; and try to run the newly created script&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;octave -W myscript.m
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output should look the same as that in the console window.&lt;/p&gt;
&lt;h6&gt;Sources&lt;/h6&gt;
&lt;p&gt;&lt;a href="http://wiki.octave.org/Octave_for_Debian_systems"&gt;Octave&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/octave-development-in-the-terminal?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/octave-development-in-the-terminal</guid>
      <pubDate>2017-12-09 12:46:54</pubDate>
      <category>#octave</category>
      <category>#tmux</category>
      <category>#developer-tools</category>
    </item>
    <item>
      <title>Hello World</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/posts/hello-world?utm_medium=feed"&gt;https://www.luisquintanilla.me/posts/hello-world&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;Welcome to my personal blog. This will be a blog post for documenting my learning process as well as sharing projects and ideas I come across. I often find myself doing things, researching how to do them and completing a task. A few weeks/months later I might have to do something similar and have to go through the process again. Using this blog will help me keep me reference that knowledge.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/hello-world?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/posts/hello-world</guid>
      <pubDate>2017-12-09 12:00:00</pubDate>
      <category>#untagged</category>
    </item>
  </channel>
</rss>