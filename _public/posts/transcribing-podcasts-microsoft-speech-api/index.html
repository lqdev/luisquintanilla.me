<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="stylesheet" href="/assets/css/custom/main.css"><link rel="stylesheet" href="/assets/css/custom/timeline.css"><link rel="stylesheet" href="/assets/css/bootstrap-icons-1.5.0/bootstrap-icons.css"><link rel="stylesheet" href="/assets/css/highlight.github-dark-dimmed.min.css"><link rel="stylesheet" href="/assets/css/main.css"><link rel="stylesheet" href="/lib/revealjs/dist/reveal.css"><link rel="stylesheet" href="/lib/revealjs/dist/theme/black.css"><link rel="stylesheet" href="/lib/revealjs/plugin/highlight/monokai.css"><meta property="og:title" content="Transcribing Podcasts with Microsoft Speech API - Luis Quintanilla"><meta property="og:type" content="website"><meta property="og:image" content="https://www.lqdev.me/avatar.png"><meta property="og:image:secure_url" content="https://www.lqdev.me/avatar.png"><meta property="og:image:type" content="image/png"><meta property="og:image:width" content="200"><meta property="og:image:height" content="200"><meta property="og:site_name" content="Luis Quintanilla Personal Website"><meta property="og:locale" content="en_US"><meta property="twitter:image" content="https://www.lqdev.me/avatar.png"><meta property="fediverse:creator" content="@lqdev@toot.lqdev.tech"><link rel="alternate" type="application/rss+xml" title="Luis Quintanilla Blog RSS Feed" href="/blog.rss"><link rel="alternate" type="application/rss+xml" title="Luis Quintanilla Microblog RSS Feed" href="/microblog.rss"><link rel="alternate" type="application/rss+xml" title="Luis Quintanilla Response RSS Feed" href="/responses.rss"><link rel="webmention" title="Luis Quintanilla Webmention Endpoint" href="https://webmentions.lqdev.tech/api/inbox"><link rel="feeds" type="text/xml" title="Luis Quintanilla&#39;s Feeds" href="/feed/index.opml"><link rel="blogroll" type="text/xml" title="Luis Quintanilla&#39;s Blogroll" href="/collections/blogroll/index.opml"><link rel="podroll" type="text/xml" title="Luis Quintanilla&#39;s Podroll" href="/collections/podroll/index.opml"><link rel="youtuberoll" type="text/xml" title="Luis Quintanilla&#39;s YouTube Roll" href="/collections/youtube/index.opml"><meta name="robots" content="nosnippet"><title>Transcribing Podcasts with Microsoft Speech API - Luis Quintanilla</title></head><body><button class="mobile-toggle" id="mobile-nav-toggle" aria-label="Toggle navigation menu" aria-expanded="false"><div class="hamburger"><span></span><span></span><span></span></div></button><div class="nav-overlay" id="nav-overlay"></div><nav class="desert-nav" id="sidebar-menu" role="navigation" aria-label="Main navigation"><div class="nav-brand"><a href="/" class="brand-text"><img src="/avatar.png" alt="Luis Quintanilla avatar" loading="lazy">Luis Quintanilla</a></div><div class="nav-section"><a class="nav-link" href="/"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M8.354 1.146a.5.5 0 0 0-.708 0l-6 6A.5.5 0 0 0 1.5 7.5v7a.5.5 0 0 0 .5.5h4.5a.5.5 0 0 0 .5-.5v-4h2v4a.5.5 0 0 0 .5.5H14a.5.5 0 0 0 .5-.5v-7a.5.5 0 0 0-.146-.354L8.354 1.146zM2.5 14V7.707l5.5-5.5 5.5 5.5V14H10v-4a.5.5 0 0 0-.5-.5h-3a.5.5 0 0 0-.5.5v4H2.5z"></path></svg>Home</a><a class="nav-link" href="/about"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M8 15A7 7 0 1 1 8 1a7 7 0 0 1 0 14zm0 1A8 8 0 1 0 8 0a8 8 0 0 0 0 16z"></path><path d="m8.93 6.588-2.29.287-.082.38.45.083c.294.07.352.176.288.469l-.738 3.468c-.194.897.105 1.319.808 1.319.545 0 1.178-.252 1.465-.598l.088-.416c-.2.176-.492.246-.686.246-.275 0-.375-.193-.304-.533L8.93 6.588zM9 4.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0z"></path></svg>About</a><a class="nav-link" href="/contact"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V4Zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1H2Zm13 2.383-4.708 2.825L15 11.105V5.383Zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741ZM1 11.105l4.708-2.897L1 5.383v5.722Z"></path></svg>Contact</a><a class="nav-link" href="/search"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path></svg>Search</a><a class="nav-link" href="/feed"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M5.5 12a1.5 1.5 0 1 1-3 0 1.5 1.5 0 0 1 3 0zm-3-8.5a1 1 0 0 1 1-1c5.523 0 10 4.477 10 10a1 1 0 1 1-2 0 8 8 0 0 0-8-8 1 1 0 0 1-1-1zm0 4a1 1 0 0 1 1-1 6 6 0 0 1 6 6 1 1 0 1 1-2 0 4 4 0 0 0-4-4 1 1 0 0 1-1-1z"></path></svg>Subscribe</a></div><div class="nav-section dropdown"><button class="nav-link dropdown-toggle" data-target="collections-dropdown"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M2.5 3A1.5 1.5 0 0 0 1 4.5v.793c.026.009.051.02.076.032L7.674 8.51c.206.1.446.1.652 0l6.598-3.185A.755.755 0 0 1 15 5.293V4.5A1.5 1.5 0 0 0 13.5 3h-11Z"></path><path d="M15 6.954 8.978 9.86a2.25 2.25 0 0 1-1.956 0L1 6.954V11.5A1.5 1.5 0 0 0 2.5 13h11a1.5 1.5 0 0 0 1.5-1.5V6.954Z"></path></svg>Collections<svg class="dropdown-arrow" viewBox="0 0 16 16" fill="currentColor"><path d="M7.247 11.14 2.451 5.658C1.885 5.013 2.345 4 3.204 4h9.592a1 1 0 0 1 .753 1.659l-4.796 5.48a1 1 0 0 1-1.506 0z"></path></svg></button><div class="dropdown-menu" id="collections-dropdown"><a class="dropdown-item" href="/collections/blogroll">Blogroll</a><a class="dropdown-item" href="/collections/podroll">Podroll</a><a class="dropdown-item" href="/collections/youtube">YouTube</a><a class="dropdown-item" href="/collections/forums">Forums</a><div class="dropdown-divider"></div><a class="dropdown-item" href="/collections/starter-packs">Starter Packs</a><div class="dropdown-divider"></div><a class="dropdown-item" href="/radio">Radio</a><a class="dropdown-item" href="/tags">Tags</a></div></div><div class="nav-section dropdown"><button class="nav-link dropdown-toggle" data-target="resources-dropdown"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M1 2.828c.885-.37 2.154-.769 3.388-.893 1.33-.134 2.458.063 3.112.752v9.746c-.935-.53-2.12-.603-3.213-.493-1.18.12-2.37.461-3.287.811V2.828zm7.5-.141c.654-.689 1.782-.886 3.112-.752 1.234.124 2.503.523 3.388.893v9.923c-.918-.35-2.107-.692-3.287-.81-1.094-.111-2.278-.039-3.213.492V2.687zM8 1.783C7.015.936 5.587.81 4.287.94c-1.514.153-3.042.672-3.994 1.105A.5.5 0 0 0 0 2.5v11a.5.5 0 0 0 .707.455c.882-.4 2.303-.881 3.68-1.02 1.409-.142 2.59.087 3.223.877a.5.5 0 0 0 .78 0c.633-.79 1.814-1.019 3.222-.877 1.378.139 2.8.62 3.681 1.02A.5.5 0 0 0 16 13.5v-11a.5.5 0 0 0-.293-.455c-.952-.433-2.48-.952-3.994-1.105C10.413.809 8.985.936 8 1.783z"></path></svg>Resources<svg class="dropdown-arrow" viewBox="0 0 16 16" fill="currentColor"><path d="M7.247 11.14 2.451 5.658C1.885 5.013 2.345 4 3.204 4h9.592a1 1 0 0 1 .753 1.659l-4.796 5.48a1 1 0 0 1-1.506 0z"></path></svg></button><div class="dropdown-menu" id="resources-dropdown"><a class="dropdown-item" href="/resources/snippets">Snippets</a><a class="dropdown-item" href="/resources/wiki">Wiki</a><a class="dropdown-item" href="/resources/presentations">Presentations</a></div></div><div class="theme-toggle"><button class="theme-toggle-btn" id="theme-toggle" aria-label="Toggle dark mode"><span id="theme-toggle-icon">☀️</span><span>Theme</span></button></div></nav><main role="main" class="main-content" id="main-content"><div class="content-wrapper"><div class="mr-auto"><article class="h-entry individual-post"><header class="post-header"><h1 class="p-name post-title">Transcribing Podcasts with Microsoft Speech API</h1><div class="post-meta"><time class="dt-published" datetime="2018-02-11 20:34:37 -05:00">February 11, 2018</time></div><div class="u-author h-card microformat-hidden"><img src="/avatar.png" class="u-photo" alt="Luis Quintanilla" /><a href="/about" class="u-url p-name">Luis Quintanilla</a></div></header><div class="e-content post-content"><h1>Introduction</h1>
<p>I enjoy listening to podcasts in a wide range of topics that include but are not limited to politics, software development, history, comedy and true crime. While some of the more entertainment related podcasts are best enjoyed through audio, those that are related to software development or other type of hands-on topics would benefit greatly from having a transcript. Having a transcript allows me to go back after having listened to an interesting discussion and look directly at the content I am interested in without having to listen to the podcast again. This however is not always feasible given that it costs both time and money to produce a transcript. Fortunately, there are tools out there such as Google Cloud Speech and Microsoft Speech API which allow users to convert speech to text. For this writeup, I will be focusing on the Microsoft Speech API. Because podcasts tend to be long-form, I will be using the C# client library because it allows for long audio (greater than 15 seconds) to be transcribed. The purpose of this exercise is to create a console application that takes audio segments, converts them to text and stores the results in a text file with the goal of evaluating how well the Microsoft Speech API works. The source code for the console application can be found on <a href="https://github.com/lqdev/PodcastsBingSpeechAPIDemo">GitHub</a></p>
<h2>Prerequisites</h2>
<ul>
<li><a href="https://www.visualstudio.com/vs/community/">Visual Studio Community</a></li>
<li><a href="https://www.microsoft.com/store/productId/9NBLGGH4MSV6">Windows Subsystem for Linux (Ubuntu)</a></li>
<li><a href="https://ffmpeg.org/">ffmpeg</a></li>
</ul>
<h2>Install/Enable Windows Subsystem for Linux</h2>
<ol>
<li>Open Powershell as Administrator and input</li>
</ol>
<pre><code class="language-powershell">Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux
</code></pre>
<ol start="2">
<li>Restart your computer</li>
<li>Open the Microsoft Store and install <a href="https://www.microsoft.com/store/productId/9NBLGGH4MSV6">Ubuntu</a> Linux distribution</li>
<li>Once installed, click <code>Launch</code></li>
<li>Create your LINUX user account (Keep in mind that this is not the same as your Windows account therefore it can be different).</li>
</ol>
<h2>Install ffmpeg</h2>
<p>Once Ubuntu is installed on your computer and you have created your LINUX user account, it's time to install <code>ffmpeg</code>. This will allow us to convert our file from <code>mp3</code> which is usually the format podcasts are in to <code>wav</code> which is the format accepted by the Microsoft Speech API</p>
<pre><code class="language-bash">sudo apt install -y ffmpeg
</code></pre>
<h2>Get Bing Speech API Key</h2>
<p>In order to use the Microsoft Speech API, an API key is required. This can be obtained using the following steps.</p>
<ol>
<li>Navigate to the <a href="https://azure.microsoft.com/en-us/try/cognitive-services/">Azure Cognitive Services</a> page.</li>
<li>Select the <code>Speech</code> tab</li>
<li>Click <code>Get API Key</code> and follow the instructions</li>
<li>Once you have an API key, make sure to store it somewhere like a text file in your computer for future use.</li>
</ol>
<h1>File Prep</h1>
<p>Before transcribing the files, they need to be converted to <code>wav</code> format if they are not already in it. In order to ease processing, they should be split into segments as opposed to having the API process an entire multi megabyte file. To do all this, inside the Ubuntu shell, first navigate to your <code>Documents</code> folder and create a new directory for the project.</p>
<pre><code class="language-bash">cd /mnt/c/Users/&lt;USERNAME&gt;/Documents
mkdir testspeechapi
</code></pre>
<p><code>USERNAME</code> is your Windows user name. This can be found by typing the following command into the Windows <code>CMD</code> prompt</p>
<pre><code class="language-bash">echo %USERNAME%
</code></pre>
<h2>Download Audio Files</h2>
<p>One of the podcasts I listen to is <a href="https://talkpython.fm/">Talk Python To Me</a> by <a href="https://twitter.com/mkennedy">Michael Kennedy</a>. For the purpose of this exercise, aside from having great content, all of the episodes are transcribed and a link is provided to the <code>mp3</code> file.</p>
<p>The file I will be using is from episode <a href="https://talkpython.fm/episodes/show/149/4-python-web-frameworks-compared">#149</a>, but it can easily be any of the episodes.</p>
<p>In the Ubuntu shell, download the file into the <code>testspeechapi</code> directory that was recently created using <code>wget</code>.</p>
<pre><code class="language-bash">wget https://talkpython.fm/episodes/download/149/4-python-web-frameworks-compared.mp3 -O originalinput.mp3
</code></pre>
<h2>Download Transcripts</h2>
<p>The transcripts can be found in GitHub at this <a href="https://github.com/mikeckennedy/talk-python-transcripts/tree/master/transcripts">link</a>. The original transcript will allow me to compare it to the output of the Microsoft Speech API and evaluate the accuracy.</p>
<p>We can download this file into the <code>testspeechapi</code> directory just like the audio file</p>
<pre><code class="language-bash">wget https://raw.githubusercontent.com/mikeckennedy/talk-python-transcripts/master/transcripts/149.txt -O originaltranscript.txt
</code></pre>
<h2>Convert MP3 to WAV</h2>
<p>Now that we have both the original audio file and transcript, it's time to convert the format from <code>mp3</code> to <code>wav</code>. To do this, we can use <code>ffmpeg</code>. In the Ubuntu shell, enter the following command.</p>
<pre><code class="language-bash">ffmpeg -i originalinput.mp3 -f wav originalinput.wav
</code></pre>
<h2>Create Audio Segments</h2>
<p>Once we have the proper format, it's time to make the files more manageable for processing. This can be done by splitting them up into equal segments using <code>ffmpeg</code>. In this case I'll be splitting it up into sixty second segments and storing them into a directory called <code>input</code> with the name <code>filexxxx.wav</code>. The <code>%04d</code> indicates that there will be 4 digits in the file name. Inside the Ubuntu shell and <code>testspeechapi</code> directory enter the following commands.</p>
<pre><code class="language-bash">mkdir input
ffmpeg -i originalinput.wav -f segment -segment_time 60 -c copy input/file%04d.wav
</code></pre>
<h1>Building The Console Application</h1>
<p>To get started building the console application, we can leverage the <a href="https://github.com/Azure-Samples/Cognitive-Speech-STT-ServiceLibrary">Cognitive-Speech-STT-ServiceLibrary</a> sample program.</p>
<h2>Download The Sample Program</h2>
<p>The first step will be to clone the sample program from GitHub onto your computer. In the Windows <code>CMD</code> prompt navigate to the <code>testspeechapi</code> folder and enter the following command.</p>
<pre><code class="language-bash">git clone https://github.com/Azure-Samples/Cognitive-Speech-STT-ServiceLibrary
</code></pre>
<p>Once the project is on your computer, open the directory and launch the <code>SpeechClient.sln</code> solution in the <code>sample</code> directory</p>
<p>When the solution launches, open the <code>Program.cs</code> file and begin making modifications.</p>
<h2>Reading Files</h2>
<p>To read files, we can create a function that will return the list of files in the specified directory.</p>
<pre><code class="language-csharp">private static string[] GetFiles(string directory)
{
    string[] files = Directory.GetFiles(directory);
    return files;
}
</code></pre>
<h2>Process Files</h2>
<p>Once we have the list of files, we can then process each file individually using the <code>Run</code> method. To do so, we need to make a slight modification to our <code>Main</code> method so that it iterates over each file and calls the <code>Run</code> method on it. To store the responses from the API, we'll also need a <code>StringBuilder</code> object which is declared at the top of our <code>Program.cs</code> file.</p>
<pre><code class="language-csharp">finalResponse = new StringBuilder();
string[] files = GetFiles(args[0]);
foreach(var file in files)
{
    p.Run(file,&quot;en-us&quot;,LongDictationUrl,args[1]).Wait();
    Console.WriteLine(&quot;File {0} processed&quot;,file);
}
</code></pre>
<h2>Transcribe Audio</h2>
<p>The <code>Run</code> method can be left intact. However, the <code>Run</code> method uses the <code>OnRecognitionResult</code> method to handle the result of API responses. In the <code>OnRecognitionResult</code> method, we can remove almost everything that is originally there and replace it. The response from the API returns various results of potential phrases as well as a confidence value. Generally, most of the phrases are alike and the first value is good enough for our purposes. The code for this part will take the response from the API, append it to a <code>StringBuilder</code> object and return when completed.</p>
<pre><code class="language-csharp">public Task OnRecognitionResult(RecognitionResult args)
{

    var response = args;

    if(response.Phrases != null)
    {
	finalResponse.Append(response.Phrases[0].DisplayText);
	finalResponse.Append(&quot;\n&quot;);
    }

    return CompletedTask;
}
</code></pre>
<h2>Output Transcribed Speech</h2>
<p>When all the audio files have been processed, we can save the final output to a text file in the <code>testspeechapi</code> directory. This can be done with the <code>SaveOutput</code> function to which we pass in a file name and the <code>StringBuilder</code> object that captured responses from the API.</p>
<pre><code class="language-csharp">private static void SaveOutput(string filename,StringBuilder content)
{
    StreamWriter writer = new StreamWriter(filename);
    writer.Write(content.ToString());
    writer.Close();
}
</code></pre>
<p><code>SaveOutput</code> can then be called from our <code>Main</code> method like so.</p>
<pre><code class="language-csharp">string username = Environment.GetEnvironmentVariable(&quot;USERNAME&quot;, EnvironmentVariableTarget.Process);
SaveOutput(String.Format(@&quot;C:\\Users\\{0}\\Documents\\testspeechapi\\apitranscript.txt&quot;,username), finalResponse);
</code></pre>
<p>The final <code>Main</code> method should look similar to the code below</p>
<pre><code class="language-csharp">public static void Main(string[] args)
{
    // Send a speech recognition request for the audio.
    finalResponse = new StringBuilder();

    string[] files = GetFiles(args[0]);

    var p = new Program();

    foreach (var file in files)
    {
	p.Run(file, &quot;en-us&quot;, LongDictationUrl, args[1]).Wait();
	Console.WriteLine(&quot;File {0} processed&quot;, file);
    }

    string username = Environment.GetEnvironmentVariable(&quot;USERNAME&quot;, EnvironmentVariableTarget.Process);

    SaveOutput(String.Format(@&quot;C:\\Users\\{0}\\Documents\\testspeechapi\\apitranscript.txt&quot;,username), finalResponse);
}
</code></pre>
<h1>Output</h1>
<p>The program will save the output of the <code>StringBuilder</code> object <code>finalResponse</code> to the file <code>apitranscript.txt</code>.</p>
<p>Prior to running the program it needs to be built. In Visual Studio change the Solutions Configurations option from <code>Debug</code> to <code>Release</code> and build the application.</p>
<p>To run the program, navigate to the <code>C:\Users\%USERNAME%\Documents\testspeechapi\Cognitive-Speech-STT-ServiceLibrary\sample\SpeechClientSample\bin\Release</code> directory in the Windows <code>CMD</code> prompt and enter the following command and pass in the <code>input</code> directory where all the audio segments are stored and your API Key.</p>
<pre><code class="language-bash">SpeechClientSample.exe C:\\Users\\%USERNAME%\\Documents\\testspeechapi\\input &lt;YOUR-API-KEY&gt;
</code></pre>
<p>This process may take a while due to the number of files being processed, so be patient.</p>
<h2>Sample Output</h2>
<p>Original</p>
<blockquote class="blockquote">
<p>Are you considering getting into web programming? Choosing a web framework like Pyramid, Flask, or Django can be daunting. It would be great to see them all build out the same application and compare the results side-by-side. That's why when I heard what Nicholas Hunt-Walker was up to, I had to have him on the podcast. He and I chat about four web frameworks compared. He built a data-driven web app with Flask, Tornado, Pyramid, and Django and then put it all together in a presentation. We're going to dive into that right now. This is Talk Python To Me, Episode 149, recorded January 30th, 2018. Welcome to Talk Python to Me, a weekly podcast on Python, the language, the libraries, the ecosystem, and the personalities. This is your host, Michael Kennedy, follow me on Twitter where I'm @mkennedy. Keep up with the show and listen to past episodes at talkpython.fm, and follow the show on Twitter via @talkpython. Nick, welcome to Talk Python.</p>
</blockquote>
<p>API Response</p>
<blockquote class="blockquote">
<p>Are you considering getting into web programming cheese in the web framework like plaster. Django can be daunting. It would be great to see them. Although that the same application and compare the results side by side. That's Why? When I heard. But Nicholas Hunt. Walker was up to, I had him on the podcast. He night chat about 4 web frameworks, compared he built a data driven web app with last tornado. Peermade Angangueo and then put it all together in a presentation. We have dive into that right now. This is talk by the to me, I was 149 recorded January 30th 2018. Welcome to talk python to me a weekly podcast on Python Language the library ecosystem in the personalities. Did you host Michael Kennedy? Follow me on Twitter where I'm at in Kennedy keep up with the show in the past episodes at talk python. Damn info, the show on Twitter via at Popeyes on. Nick Welcome to talk by phone.</p>
</blockquote>
<h1>Conclusion</h1>
<p>In this writeup, I converted an <code>mp3</code> podcast file to <code>wav</code> format and split it into sixty second segments using <code>ffmpeg</code>. The segments were then processed by a console application which uses the Microsoft Speech API to convert speech to text and the results were saved to a text file. When compared to the original transcript, the result produced by the API is inconsistent and at times incomprehensible. This does not mean that overall the API is unusable or inaccurate as many things could have contributed to the inaccuracy. However, the results were not as good as I would have hoped for.</p>
<h6>Sources</h6>
<ul>
<li><a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">Windows Subsystem for Linux Install Instructions</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/cognitive-services/speech/home">Microsoft Speech API</a></li>
</ul>
</div><footer class="post-footer"><div class="permalink-info">Permalink: <a class="u-url permalink-link" href="/posts/transcribing-podcasts-microsoft-speech-api/">/posts/transcribing-podcasts-microsoft-speech-api/</a></div><div><script type="application/javascript">window.onload = function() { document.getElementById('webmention-target').value = window.location.href }</script><form action="https://webmentions.lqdev.tech/api/inbox" method="POST" enctype="application/x-www-form-urlencoded"><h5 class="text-center">Send me a <a href="/contact">message</a> or <a href="https://indieweb.org/webmentions">webmention</a></h5><div class="form-row justify-content-center"><div class="w-75"><input type="url" class="form-control" name="source" placeholder="Your URL (webmention source)" required /></div><div class="col-auto"><input type="submit" class="btn btn-primary" value="Send" /></div><input type="hidden" readonly name="target" id="webmention-target" /></div></form></div></footer></article></div></div></main><script src="/assets/lib/jquery/jquery.slim.min.js"></script><script src="/assets/lib/boostrap/bootstrap.min.js"></script><script src="/assets/lib/highlight/highlight.min.js"></script><script src="/assets/lib/highlight/highlight.fsharp.min.js"></script><script src="/assets/lib/highlight/highlight.nix.min.js"></script><script src="/assets/js/timeline.js"></script><script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script><script type="application/javascript">mermaid.initialize({startOnLoad:true});</script><script type="application/javascript">hljs.initHighlightingOnLoad();</script><script src="/lib/revealjs/dist/reveal.js"></script><script src="/lib/revealjs/plugin/markdown/markdown.js"></script><script src="/lib/revealjs/plugin/highlight/highlight.js"></script><script type="application/javascript">
                    // Initialize Reveal.js when a presentation container is found on the page
                    document.addEventListener('DOMContentLoaded', function() {
                        const presentationContainer = document.querySelector('.presentation-container');
                        if (presentationContainer && typeof Reveal !== 'undefined') {
                            Reveal.initialize({
                                plugins: [RevealMarkdown, RevealHighlight],
                                embedded: true
                            });
                        }
                    });
                    </script></body><footer><a rel="me" href="https://toot.lqdev.tech/@lqdev"></a><a rel="me" href="https://github.com/lqdev"></a><a rel="me" href="https://twitter.com/ljquintanilla"></a><a rel="me" href="https://www.linkedin.com/in/lquintanilla01/"></a><a rel="me" href="mailto:lqdev@outlook.com"></a></footer></html>