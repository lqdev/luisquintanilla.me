<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>Orca: Progressive Learning from Complex Explanation Traces of GPT-4 - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>Orca: Progressive Learning from Complex Explanation Traces of GPT-4</h1><div class="content-meta"><div class="content-type">bookmarks</div><time datetime="2023-06-07">June 7, 2023</time><p>Tags: ai, finetuning, gpt</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/bookmarks/">← All bookmarks</a> | <a href="https://www.lqdev.me/bookmarks/orca-lm-progressive-learning-gpt-4">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/bookmarks/orca-lm-progressive-learning-gpt-4/" >Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a></h2><div class="response-target" >→ <a href="https://arxiv.org/pdf/2306.02707.pdf" >https://arxiv.org/pdf/2306.02707.pdf</a></div><div class="response-content" ><blockquote class="blockquote">
<p>Recent research has focused on enhancing the capability of smaller models
through imitation learning, drawing on the outputs generated by large
foundation models (LFMs). A number of issues impact the quality of these
models, ranging from limited imitation signals from shallow LFM outputs;
small scale homogeneous training data; and most notably a lack of rigorous
evaluation resulting in overestimating the small model’s capability as they
tend to learn to imitate the style, but not the reasoning process of LFMs. To
address these challenges, we develop Orca, a 13-billion parameter model
that learns to imitate the reasoning process of LFMs. Orca learns from
rich signals from GPT-4 including explanation traces; step-by-step thought
processes; and other complex instructions, guided by teacher assistance from
ChatGPT. To promote this progressive learning, we tap into large-scale and
diverse imitation data with judicious sampling and selection. Orca surpasses
conventional state-of-the-art instruction-tuned models such as Vicuna-13B
by more than 100% in complex zero-shot reasoning benchmarks like Big-
Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity
with ChatGPT on the BBH benchmark and shows competitive performance
(4 pts gap with optimized system message) in professional and academic
examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot
settings without CoT; while trailing behind GPT-4. Our research indicates
that learning from step-by-step explanations, whether these are generated
by humans or more advanced AI models, is a promising direction to improve
model capabilities and skills.</p>
</blockquote>
</div></article></div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>