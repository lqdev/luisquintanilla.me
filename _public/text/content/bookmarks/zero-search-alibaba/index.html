<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>ZeroSearch - Incentivize the Search Capability of LLMs without Searching - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>ZeroSearch - Incentivize the Search Capability of LLMs without Searching</h1><div class="content-meta"><div class="content-type">bookmarks</div><time datetime="2025-05-10">May 10, 2025</time><p>Tags: ai, search, llm, research</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/bookmarks/">← All bookmarks</a> | <a href="https://www.lqdev.me/bookmarks/zero-search-alibaba">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/bookmarks/zero-search-alibaba/" >ZeroSearch - Incentivize the Search Capability of LLMs without Searching</a></h2><div class="response-target" >→ <a href="https://alibaba-nlp.github.io/ZeroSearch/" >https://alibaba-nlp.github.io/ZeroSearch/</a></div><div class="response-content" ><blockquote class="blockquote">
<p>Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) <strong>Uncontrolled Document Quality</strong>: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) <strong>Prohibitively High API Costs</strong>: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce <strong>ZeroSearch</strong>, a reinforcement learning framework that <strong>enhances the search capabilities of LLMs without interacting with real search engines</strong>. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model’s reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that <strong>ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module</strong>. Remarkably, <strong>a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it</strong>. Furthermore, it generalizes well across both base and instruction-tuned models of varying sizes and is compatible with a wide range of RL algorithms.</p>
</blockquote>
</div></article></div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>