<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>LLM360: Towards Fully Transparent Open-Source LLMs - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>LLM360: Towards Fully Transparent Open-Source LLMs</h1><div class="content-meta"><div class="content-type">bookmarks</div><time datetime="2023-12-13">December 13, 2023</time><p>Tags: ai, research</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/bookmarks/">← All bookmarks</a> | <a href="https://www.lqdev.me/bookmarks/llm360-transparent-open-source-llm">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/bookmarks/llm360-transparent-open-source-llm/" >LLM360: Towards Fully Transparent Open-Source LLMs</a></h2><div class="response-target" >→ <a href="https://arxiv.org/abs/2312.06550" >https://arxiv.org/abs/2312.06550</a></div><div class="response-content" ><blockquote class="blockquote">
<p>The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at this <a href="https://www.llm360.ai/">https URL</a>). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.</p>
</blockquote>
<h2>Additional Resources</h2>
<p><a href="https://www.llm360.ai/">https://www.llm360.ai/</a></p>
</div></article></div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>