<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>VideoPoet: A large language model for zero-shot video generation - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>VideoPoet: A large language model for zero-shot video generation</h1><div class="content-meta"><div class="content-type">bookmarks</div><time datetime="2023-12-24">December 24, 2023</time><p>Tags: ai, genai, videopoet, generativeai, google</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/bookmarks/">← All bookmarks</a> | <a href="https://www.lqdev.me/bookmarks/videopoet-llm-zero-shot-video-generation">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/bookmarks/videopoet-llm-zero-shot-video-generation/" >VideoPoet: A large language model for zero-shot video generation</a></h2><div class="response-target" >→ <a href="https://sites.research.google/videopoet/" >https://sites.research.google/videopoet/</a></div><div class="response-content" ><blockquote class="blockquote">
<p>VideoPoet is a simple modeling method that can convert any autoregressive language model or large language model (LLM) into a high-quality video generator. It contains a few simple components:</p>
<ul>
<li>A pre-trained MAGVIT V2 video tokenizer and a SoundStream audio tokenizer transform images, video, and audio clips with variable lengths into a sequence of discrete codes in a unified vocabulary. These codes are compatible with text-based language models, facilitating an integration with other modalities, such as text.</li>
<li>An autoregressive language model learns across video, image, audio, and text modalities to autoregressively predict the next video or audio token in the sequence.</li>
<li>A mixture of multimodal generative learning objectives are introduced into the LLM training framework, including text-to-video, text-to-image, image-to-video, video frame continuation, video inpainting and outpainting, video stylization, and video-to-audio. Furthermore, such tasks can be composed together for additional zero-shot capabilities (e.g., text-to-audio).</li>
</ul>
<p>This simple recipe shows that language models can synthesize and edit videos with a high degree of temporal consistency. VideoPoet demonstrates state-of-the-art video generation, in particular in producing a wide range of large, interesting, and high-fidelity motions. The VideoPoet model supports generating videos in square orientation, or portrait to tailor generations towards short-form content, as well as supporting audio generation from a video input.</p>
</blockquote>
<p><a href="https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html">Blog Post</a></p>
</div></article></div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>