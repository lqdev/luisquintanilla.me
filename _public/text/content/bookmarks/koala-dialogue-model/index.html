<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>Koala: A Dialogue Model for Academic Research - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>Koala: A Dialogue Model for Academic Research</h1><div class="content-meta"><div class="content-type">bookmarks</div><time datetime="2023-04-06">April 6, 2023</time><p>Tags: ai, llm</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/bookmarks/">← All bookmarks</a> | <a href="https://www.lqdev.me/bookmarks/koala-dialogue-model">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/bookmarks/koala-dialogue-model/" >Koala: A Dialogue Model for Academic Research</a></h2><div class="response-target" >→ <a href="https://bair.berkeley.edu/blog/2023/04/03/koala/" >https://bair.berkeley.edu/blog/2023/04/03/koala/</a></div><div class="response-content" ><blockquote class="blockquote">
<p>...Koala, a chatbot trained by fine-tuning Meta’s LLaMA on dialogue data gathered from the web.</p>
</blockquote>
<blockquote class="blockquote">
<p>Many of the most capable LLMs require huge computational resources to train, and oftentimes use large and proprietary datasets. This suggests that in the future, highly capable LLMs will be largely controlled by a small number of organizations, and both users and researchers will pay to interact with these models without direct access to modify and improve them on their own. On the other hand, recent months have also seen the release of increasingly capable freely available or (partially) open-source models, such as LLaMA. These systems typically fall short of the most capable closed models, but their capabilities have been rapidly improving. This presents the community with an important question: will the future see increasingly more consolidation around a handful of closed-source models, or the growth of open models with smaller architectures that approach the performance of their larger but closed-source cousins?</p>
</blockquote>
<blockquote class="blockquote">
<p>Our results suggest that learning from high-quality datasets can mitigate some of the shortcomings of smaller models, maybe even matching the capabilities of large closed-source models in the future.</p>
</blockquote>
</div></article></div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>