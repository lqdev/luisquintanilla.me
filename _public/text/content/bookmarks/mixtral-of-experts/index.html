<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>Mixtral of experts - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>Mixtral of experts</h1><div class="content-meta"><div class="content-type">bookmarks</div><time datetime="2023-12-11">December 11, 2023</time><p>Tags: ai, llm, opensource</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/bookmarks/">← All bookmarks</a> | <a href="https://www.lqdev.me/bookmarks/mixtral-of-experts">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/bookmarks/mixtral-of-experts/" >Mixtral of experts</a></h2><div class="response-target" >→ <a href="https://mistral.ai/news/mixtral-of-experts/" >https://mistral.ai/news/mixtral-of-experts/</a></div><div class="response-content" ><blockquote class="blockquote">
<p>Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.</p>
<p>Mixtral has the following capabilities.</p>
<ul>
<li>It gracefully handles a context of 32k tokens.</li>
<li>It handles English, French, Italian, German and Spanish.</li>
<li>It shows strong performance in code generation.</li>
<li>It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench.</li>
</ul>
</blockquote>
<blockquote class="blockquote">
<p>Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the “experts”) to process the token and combine their output additively.</p>
<p>This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.</p>
</blockquote>
</div></article></div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>