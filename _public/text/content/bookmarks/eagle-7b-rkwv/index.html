<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages (RWKV-v5) - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages (RWKV-v5)</h1><div class="content-meta"><div class="content-type">bookmarks</div><time datetime="2024-01-30">January 30, 2024</time><p>Tags: ai, llm, rwkv, deeplearning, neuralnetwork</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/bookmarks/">← All bookmarks</a> | <a href="https://www.lqdev.me/bookmarks/eagle-7b-rkwv">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/bookmarks/eagle-7b-rkwv/" >Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages (RWKV-v5)</a></h2><div class="response-target" >→ <a href="https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers" >https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers</a></div><div class="response-content" ><blockquote class="blockquote">
<p>Eagle 7B is a 7.52B parameter model that:<br />
<br>
Built on the <a href="https://wiki.rwkv.com/">RWKV-v5 architecture</a> (a linear transformer with 10-100x+ lower inference cost)<br />
<br>
Ranks as the <a href="https://blog.rwkv.com/p/the-worlds-greenest-ai-model-rwkvs">world’s greenest 7B model (per token)</a><br />
<br>
Trained on 1.1 Trillion Tokens across 100+ languages<br />
<br>
Outperforms all 7B class models in multi-lingual benchmarks<br />
<br>
Approaches Falcon (1.5T), LLaMA2 (2T), Mistral (&gt;2T?) level of performance in English evals<br />
<br>
Trade blows with MPT-7B (1T) in English evals<br />
<br>
<a href="https://www.isattentionallyouneed.com/">All while being an “Attention-Free Transformer”</a><br />
<br>
Is a foundation model, with a very small instruct tune - further fine-tuning is required for various use cases!</p>
</blockquote>
<blockquote class="blockquote">
<p>We are releasing RWKV-v5 Eagle 7B, <a href="https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as">licensed as Apache 2.0 license, under the Linux Foundation</a>, and can be used personally or commercially without restrictions</p>
</blockquote>
<p><a href="https://huggingface.co/RWKV/v5-Eagle-7B/">Download from HuggingFace</a></p>
</div></article></div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>