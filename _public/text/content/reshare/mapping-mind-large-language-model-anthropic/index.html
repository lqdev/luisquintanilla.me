<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>Mapping the Mind of a Large Language Model - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>Mapping the Mind of a Large Language Model</h1><div class="content-meta"><div class="content-type">reshare</div><time datetime="2024-06-12">June 12, 2024</time><p>Tags: ai, interpretability, anthropic, llm</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/reshare/">← All reshare</a> | <a href="https://www.lqdev.me/responses/mapping-mind-large-language-model-anthropic">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/responses/mapping-mind-large-language-model-anthropic/" >Mapping the Mind of a Large Language Model</a></h2><div class="response-target" >→ <a href="https://www.anthropic.com/research/mapping-mind-language-model" >https://www.anthropic.com/research/mapping-mind-language-model</a></div><div class="response-content" ><blockquote class="blockquote">
<p>Today we report a significant advance in understanding the inner workings of AI models. We have identified how millions of concepts are represented inside Claude Sonnet, one of our deployed large language models. This is the first ever detailed look inside a modern, production-grade large language model. This interpretability discovery could, in future, help us make AI models safer.</p>
</blockquote>
<blockquote class="blockquote">
<p>Previously, we made some progress matching patterns of neuron activations, called features, to human-interpretable concepts. We used a technique called &quot;dictionary learning&quot;, borrowed from classical machine learning, which isolates patterns of neuron activations that recur across many different contexts. In turn, any internal state of the model can be represented in terms of a few active features instead of many active neurons. Just as every English word in a dictionary is made by combining letters, and every sentence is made by combining words, every feature in an AI model is made by combining neurons, and every internal state is made by combining features.</p>
</blockquote>
<blockquote class="blockquote">
<p>In October 2023, we reported success applying dictionary learning to a very small &quot;toy&quot; language model and found coherent features corresponding to concepts like uppercase text, DNA sequences, surnames in citations, nouns in mathematics, or function arguments in Python code.</p>
</blockquote>
<blockquote class="blockquote">
<p>We used the same scaling law philosophy that predicts the performance of larger models from smaller ones to tune our methods at an affordable scale before launching on Sonnet.</p>
</blockquote>
<blockquote class="blockquote">
<p>We successfully extracted millions of features from the middle layer of Claude 3.0 Sonnet, (a member of our current, state-of-the-art model family, currently available on claude.ai), providing a rough conceptual map of its internal states halfway through its computation. This is the first ever detailed look inside a modern, production-grade large language model.</p>
</blockquote>
</div></article>
</div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>