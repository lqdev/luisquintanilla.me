<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation</h1><div class="content-meta"><div class="content-type">reshare</div><time datetime="2025-04-08">April 8, 2025</time><p>Tags: llama, meta, ai, llm</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/reshare/">← All reshare</a> | <a href="https://www.lqdev.me/responses/llama-4-herd">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/responses/llama-4-herd/" >The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation</a></h2><div class="response-target" >→ <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/" >https://ai.meta.com/blog/llama-4-multimodal-intelligence/</a></div><div class="response-content" ><blockquote class="blockquote">
<p>We’re introducing Llama 4 Scout and Llama 4 Maverick, the first open-weight natively multimodal models with unprecedented context length support and our first built using a mixture-of-experts (MoE) architecture. We’re also previewing Llama 4 Behemoth, one of the smartest LLMs in the world and our most powerful yet to serve as a teacher for our new models.</p>
</blockquote>
<blockquote class="blockquote">
<p>Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3</p>
</blockquote>
<blockquote class="blockquote">
<p>Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks</p>
</blockquote>
<p>After tinkering with Mistral Small 3.1, I'm excited to try Llama 4 Scout once it lands in GitHub Models and Ollama.</p>
<p>I don't think that Mistral Small 3.1 uses MoE architecture which should help a ton considering the 10 million token context size.</p>
</div></article>
</div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>