<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>Meta AI's Segment Anything Model (SAM) 2 - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>Meta AI's Segment Anything Model (SAM) 2</h1><div class="content-meta"><div class="content-type">reshare</div><time datetime="2024-07-30">July 30, 2024</time><p>Tags: meta, ai, computervision, sam2, segmentanythingmodel, aimodel, cv</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/reshare/">← All reshare</a> | <a href="https://www.lqdev.me/responses/meta-ai-segment-anything-model-2">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/responses/meta-ai-segment-anything-model-2/" >Meta AI&#39;s Segment Anything Model (SAM) 2</a></h2><div class="response-target" >→ <a href="https://ai.meta.com/blog/segment-anything-2/" >https://ai.meta.com/blog/segment-anything-2/</a></div><div class="response-content" ><blockquote class="blockquote">
<h2>Takeaways</h2>
<ul>
<li>Following up on the success of the Meta Segment Anything Model (SAM) for images, we're releasing SAM 2, a unified model for real-time promptable object segmentation in images and videos that achieves state-of-the-art performance.</li>
<li>In keeping with our approach to open science, we're sharing the code and model weights with a permissive Apache 2.0 license.</li>
<li>We're also sharing the SA-V dataset, which includes approximately 51,000 real-world videos and more than 600,000 masklets (spatio-temporal masks).</li>
<li>SAM 2 can segment any object in any video or image - even for objects and visual domains it has not seen previously, enabling a diverse range of use cases without custom adaptation.</li>
<li>SAM 2 has many potential real-world applications. For example, the outputs of SAM 2 can be used with a generative video model to create new video effects and unlock new creative applications. SAM 2 could also aid in faster annotation tools for visual data to build better computer vision systems.</li>
</ul>
</blockquote>
</div></article>
</div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>