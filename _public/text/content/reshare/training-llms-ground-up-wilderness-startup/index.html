<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>Training great LLMs entirely from ground up in the wilderness as a startup - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>Training great LLMs entirely from ground up in the wilderness as a startup</h1><div class="content-meta"><div class="content-type">reshare</div><time datetime="2024-03-08">March 8, 2024</time><p>Tags: llm, ai, startup, compute, gpu, training</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/reshare/">← All reshare</a> | <a href="https://www.lqdev.me/responses/training-llms-ground-up-wilderness-startup">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/responses/training-llms-ground-up-wilderness-startup/" >Training great LLMs entirely from ground up in the wilderness as a startup</a></h2><div class="response-target" >→ <a href="https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness" >https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness</a></div><div class="response-content" ><blockquote class="blockquote">
<p>Given that we’ve successfully trained pretty strong multimodal language models at Reka, many people have been particularly curious about the experiences of building infrastructure and training large language &amp; multimodal models from scratch from a completely clean slate.<br />
<br>
I complain a lot about external (outside Google) infrastructure and code on my social media, leading people to really be curious about what are the things I miss and what I hate/love in the wilderness. So here’s a post (finally). This blogpost sheds light on the challenges and lessons learned.</p>
</blockquote>
<blockquote class="blockquote">
<p>Figuring out things in the wilderness was an interesting experience. It was unfortunately not painless. Compute scarcity and also unreliable compute providers made things significantly harder than expected but we’re glad we pulled through with brute technical strength.<br />
<br>
All in all, this is only a small part of the story of how we started a company, raised some money, bought some chips and matched Gemini pro/GPT 3.5 and outperformed many others in less than a year having to build everything from scratch.</p>
</blockquote>
</div></article>
</div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>