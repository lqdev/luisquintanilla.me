<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>The Ultra-Scale Playbook: Training LLMs on GPU Clusters - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>The Ultra-Scale Playbook: Training LLMs on GPU Clusters</h1><div class="content-meta"><div class="content-type">reshare</div><time datetime="2025-02-20">February 20, 2025</time><p>Tags: ai, llm, training</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/reshare/">← All reshare</a> | <a href="https://www.lqdev.me/responses/ultrascale-playbook-training-llms-gpu-clusters">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/responses/ultrascale-playbook-training-llms-gpu-clusters/" >The Ultra-Scale Playbook: Training LLMs on GPU Clusters</a></h2><div class="response-target" >→ <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook" >https://huggingface.co/spaces/nanotron/ultrascale-playbook</a></div><div class="response-content" ><p>Gold.</p>
<blockquote class="blockquote">
<p>All the techniques we'll cover in this book tackle one or several of the following three key challenges, which we'll keep bumping into throughout the book:</p>
<hr>
  1. **Memory Usage**: it's a hard limitation - if a training step doesn't fit in memory, training cannot proceed  
  2. **Compute Efficiency**: we want our hardware to spend most time computing, so we need to reduce time spent on data transfers or waiting for other GPUs to perform work.  
  3. **Communication overhead**: we want to minimize communication overhead as it keeps GPUs idle. To archieve this we will try to make best use of intra-node (fast) and inter-node (slower) bandwidths as well as overlap communication with compute as much as possible.  
<hr>
In many places we'll see that we can trade one of these (computation, communication, memory) for another (e.g. recomputation or Tensor Parallelism). Finding the right balance is key to scaling training.
</blockquote>
<p><a href="https://nanotron-ultrascale-playbook.static.hf.space/dist/files/images/ultra-cheatsheet.svg" target="_blank">[Image: Ultra-Scale Playbook Cheatsheet]</a></p>
</div></article>
</div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>