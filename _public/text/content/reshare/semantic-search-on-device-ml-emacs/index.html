<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>Semantic Search and On-Device ML in Emacs - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>Semantic Search and On-Device ML in Emacs</h1><div class="content-meta"><div class="content-type">reshare</div><time datetime="2025-01-21">January 21, 2025</time><p>Tags: onnx, emacs, ai, ml</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/reshare/">← All reshare</a> | <a href="https://www.lqdev.me/responses/semantic-search-on-device-ml-emacs">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/responses/semantic-search-on-device-ml-emacs/" >Semantic Search and On-Device ML in Emacs</a></h2><div class="response-target" >→ <a href="https://lepisma.xyz/2025/01/17/emacs-on-device-ml/index.html" >https://lepisma.xyz/2025/01/17/emacs-on-device-ml/index.html</a></div><div class="response-content" ><p>This is a cool walk-through of how to add semantic search to Emacs using local ML models.</p>
<blockquote class="blockquote">
<p>With ONNX.el you can run any ML model inside Emacs, including ones for images, audios, etc. In case you are running embedding models, combine this with sem.el to perform semantic searches. Depending on your input modality, you might have to figure out preprocessing. For text, tokenizers.el should cover all of what you need in modern NLP for preprocessing, though you will have to do something on your own for anything non-text or multimodal.</p>
</blockquote>
<blockquote class="blockquote">
<p>By default sem runs a local all-MiniLM-L6-v2 model for text embeddings. This model is small and runs fast on CPU. Additionally, we load an O2 optimized variant which helps further. The vectors are stored in a lancedb database on file system which runs without a separate process. I was originally writing the vector db core myself to learn more of Rust, but then stopped doing that since lancedb gives all that I needed, out of the box.</p>
</blockquote>
</div></article>
</div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>