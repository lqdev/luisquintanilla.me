<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>Quanto: a PyTorch quantization toolkit  - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>Quanto: a PyTorch quantization toolkit </h1><div class="content-meta"><div class="content-type">reshare</div><time datetime="2024-03-20">March 20, 2024</time><p>Tags: huggingface, quantization, pytorch, tools, ai</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/reshare/">← All reshare</a> | <a href="https://www.lqdev.me/responses/quanto-pytorch-quantization-toolkit">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/responses/quanto-pytorch-quantization-toolkit/" >Quanto: a PyTorch quantization toolkit </a></h2><div class="response-target" >→ <a href="https://huggingface.co/blog/quanto-introduction" >https://huggingface.co/blog/quanto-introduction</a></div><div class="response-content" ><blockquote class="blockquote">
<p>Quantization is a technique to reduce the computational and memory costs of evaluating Deep Learning Models by representing their weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).</p>
</blockquote>
<blockquote class="blockquote">
<p>Today, we are excited to introduce quanto, a versatile pytorch quantization toolkit, that provides several unique features:<br />
<br></p>
<ul>
<li>available in eager mode (works with non-traceable models)</li>
<li>quantized models can be placed on any device (including CUDA and MPS),</li>
<li>automatically inserts quantization and dequantization stubs,</li>
<li>automatically inserts quantized functional operations,</li>
<li>automatically inserts quantized modules (see below the list of supported modules),</li>
<li>provides a seamless workflow for a float model, going from a dynamic to a static quantized model,</li>
<li>supports quantized model serialization as a state_dict,</li>
<li>supports not only int8 weights, but also int2 and int4,</li>
<li>supports not only int8 activations, but also float8.</li>
</ul>
</blockquote>
</div></article>
</div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>