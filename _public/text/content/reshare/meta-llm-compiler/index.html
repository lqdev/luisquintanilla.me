<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>Meta Large Language Model Compiler: Foundation Models of Compiler Optimization - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>Meta Large Language Model Compiler: Foundation Models of Compiler Optimization</h1><div class="content-meta"><div class="content-type">reshare</div><time datetime="2024-06-28">June 28, 2024</time><p>Tags: ai, compiler, meta, llm</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/reshare/">← All reshare</a> | <a href="https://www.lqdev.me/responses/meta-llm-compiler">View Full Version</a></p><div class="content"><article class="response-card h-entry" ><h2><a href="/responses/meta-llm-compiler/" >Meta Large Language Model Compiler: Foundation Models of Compiler Optimization</a></h2><div class="response-target" >→ <a href="https://huggingface.co/collections/facebook/llm-compiler-667c5b05557fe99a9edd25cb" >https://huggingface.co/collections/facebook/llm-compiler-667c5b05557fe99a9edd25cb</a></div><div class="response-content" ><blockquote class="blockquote">
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.</p>
</blockquote>
<ul>
<li><a href="https://huggingface.co/collections/facebook/llm-compiler-667c5b05557fe99a9edd25cb">HuggingFace Collection</a></li>
<li><a href="https://scontent-lga3-1.xx.fbcdn.net/v/t39.2365-6/448997590_1496256481254967_2304975057370160015_n.pdf?_nc_cat=106&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=4Yn8V9DFdbsQ7kNvgHtvfLI&amp;_nc_ht=scontent-lga3-1.xx&amp;oh=00_AYDYp657HzWrcs2CZ6ZBjStwB03bo760w9voXwJorfXA_w&amp;oe=6683F28D">Paper</a></li>
</ul>
</div></article>
</div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>