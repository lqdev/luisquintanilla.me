<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="/text/assets/text-only.css"><meta name="description" content="Text-only accessible version of Luis Quintanilla&#39;s website"><meta name="robots" content="noindex, nofollow"><title>Digitize Analog Bookmarks using AI, .NET, and GitHub Models - Text-Only Site</title></head><body><a href="#main-content" class="skip-link">Skip to main content</a><header role="banner"><h1><a href="/text/">Luis Quintanilla</a></h1><p>Text-Only Accessible Website</p></header><nav role="navigation" aria-label="Main navigation"><ul><li><a href="/text/">Home</a></li><li><a href="/text/about/">About</a></li><li><a href="/text/contact/">Contact</a></li><li><a href="/text/content/">All Content</a></li><li><a href="/text/feeds/">RSS Feeds</a></li><li><a href="/text/help/">Help</a></li></ul></nav><main role="main" id="main-content"><div><h1>Digitize Analog Bookmarks using AI, .NET, and GitHub Models</h1><div class="content-meta"><div class="content-type">posts</div><time datetime="2024-11-01">November 1, 2024</time><p>Tags: ai, dotnet, github, reading, bookmarks, analog, digital, reading, notes</p></div><p><a href="/text/">← Home</a> | <a href="/text/content/posts/">← All posts</a> | <a href="https://www.lqdev.me/posts/digitize-analog-bookmarks-with-ai-dotnet">View Full Version</a></p><div class="content"><h2>Introduction</h2>
<p>This past year I've made more of an effort to read. I track books in my <a href="/library">/library</a> page.</p>
<p>Although I have a <a href="https://shop.boox.com/products/novaair2">Nova Air 2 E-Note device</a>, given the choice, I prefer physical books. Despite the conveniene of an electronic note-taking device, theres something about analog that I find hard to quit.</p>
<p>As I read books, especially non-fiction, I annotate them in various ways. Eventually, those annotations make their way into my library page.</p>
<p>Here's an example of those annotations for <a href="/reviews/building-a-second-brain">Building A Second Brain by Thiago Forte</a>.</p>
<p>The process of transferring notes is manual and tedious. I don't always have the discipline to transfer them at periodic intervals and what ends up happening is, I get to the end of the book without transferring any notes. To make space for new books, I donate it or resell the ones I've read. If I didn't take the time to transfer those notes, they're gone. <a href="/reviews/slow-productivity-newport">Slow Productivity</a> is an example of that.</p>
<p>I want to find a better system that's low-maintenance for keeping more of these notes and retaining knowledge I've found valuable.</p>
<p>Then it hit me, why not use AI? I know I could use OCR or even some of the AI models of yesteryear. The challenge is, those systems are error prone and given I don't always have the motivation to transfer notes manually, I have even less motivation to build and maintain such a system.</p>
<p>However, vision models have advanced significantly and when paired with language models, the barrier to entry for reasoning over image data has drastically decreased.</p>
<p>That's what led to this post. In this post, I'll show how you can use AI models like GPT-4o Mini to extract the passages I've annotated in physical books from an image. I then format those passages in markdown to make them easy to directly copy and paste them onto the website.</p>
<p>I know there's probably a ton of services that do this for you, but it's a lot more fun to build one from scratch. With that in mind, let's get started.</p>
<p>You can find the source for the application in the <a href="/github/AIBookmarks">AIBookmarks GitHub repository</a>.</p>
<p>Alternatively, I've configured the repo to use GitHub CodeSpaces, so you can launch the application there as well.</p>
<p><a href="https://codespaces.new/lqdev/AIBookmarks"><a href="https://github.com/codespaces/badge.svg" target="_blank">[Image: Open in GitHub Codespaces]</a></a></p>
<h2>Configure access to GitHub Models</h2>
<p>I'll be using <a href="https://github.blog/news-insights/product-news/introducing-github-models/">GitHub Models</a> as my AI model provider. GitHub Models provides developers with access to a catalog of AI models such as GPT-4o, Llama 3.2, Phi 3.5, and many others. Best of all, it's completely free, <a href="https://docs.github.com/en/github-models/prototyping-with-ai-models#rate-limits">though restrictions apply</a>.</p>
<p>I chose GitHub Models for the following reasons:</p>
<ul>
<li>Zero installation. Although I really like Ollama and some of the other local AI model providers, I didn't want to fill precious hard-drive space with AI models.</li>
<li>It's free! Since I'm just prototyping, even with their limited capacity, it should be enough to prove out whether my scenario is feasible.</li>
<li>They provide access to multi-modal models such as GPT-4o, Llama 3.2 Vision, and Phi 3.5 Vision, which can reason over text and images, which is what I need.</li>
<li>I'm one of the GPU poor. My <a href="https://www.lenovo.com/us/en/p/laptops/thinkpad/thinkpadx1/x1-titanium-g1/22tp2x1x1t1?orgRef=https%253A%252F%252Fduckduckgo.com%252F">Lenovo Thinkpad X1</a> couldn't handle running one of the vision models.</li>
</ul>
<p>Getting set up with GitHub models is fairly easy. At minimum, it requires:</p>
<ol>
<li>A GitHub Account.</li>
<li>A Personal Access Token. For more details, see the <a href="https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens">GitHub documentation</a>.</li>
</ol>
<p>Once you have both of those, you can get started building your application.</p>
<h2>Create a client</h2>
<p>The sample application is a C# console application which targets .NET 9. However, the code shown here should work on the latest LTS version as well.</p>
<p>For this solution, I use the Azure AI Inference implementation of <a href="https://www.nuget.org/packages/Microsoft.Extensions.AI.AzureAIInference/">Microsoft.Extensions.AI</a>. <a href="https://devblogs.microsoft.com/dotnet/introducing-microsoft-extensions-ai-preview/">Microsoft.Extensions.AI</a>, M.E.AI for short, is a set of abstractions that provides a common set of interfaces for interacting with AI models.</p>
<p>Although we're using the Azure AI Inference client SDK implementation of M.E.AI, we can still use it to work with models from the GitHub Model catalog.</p>
<pre><code class="language-csharp">IChatClient client =
    new ChatCompletionsClient(
        endpoint: new Uri(&quot;https://models.inference.ai.azure.com&quot;), 
        new AzureKeyCredential(Environment.GetEnvironmentVariable(&quot;GITHUB_TOKEN&quot;)))
        .AsChatClient(&quot;gpt-4o-mini&quot;);
</code></pre>
<h2>Load your images</h2>
<p>In the repo, I have a set of sample images containing annotations. These are the images I'll send to the AI model for processing.</p>
<p>The following is an sample from <a href="/reviews/creative-act-way-of-being-rubin">The Creative Act by Rick Rubin</a>.</p>
<p><a href="https://github.com/lqdev/AIBookmarks/raw/main/data/creative-act-1.jpg" target="_blank">[Image: An image of a book with pencil markings]</a></p>
<ol>
<li><p>Load the files</p>
<pre><code class="language-csharp">var filePaths = Directory.GetFiles(&quot;data&quot;);
</code></pre>
</li>
<li><p>Create a collection to store extracted passages</p>
<pre><code class="language-csharp">var passages = new List&lt;AIBookmark&gt;();
</code></pre>
</li>
</ol>
<h2>Process the images</h2>
<p>Once you have the images loaded, it's time to proces them.</p>
<ol>
<li><p>Start by setting the system prompt. This will provide the initial guidance for the extraction task.</p>
<pre><code class="language-csharp">var systemPrompt = 
    &quot;&quot;&quot;
    You are an AI assistant that extracts underlined, highlighted, and marked passages from book page images.

    When passages have a natural continuation between pages, merge them and assign the page number where the first passage starts.
    &quot;&quot;&quot;;
</code></pre>
</li>
<li><p>Then, iterate over each of the images and process them.</p>
<pre><code class="language-csharp">foreach(var path in filePaths)
{
    var file = await File.ReadAllBytesAsync(path);
    var messages = new List&lt;ChatMessage&gt;
    {
        new ChatMessage(ChatRole.System, systemPrompt),
        new ChatMessage(ChatRole.User, new AIContent[] {
            new ImageContent(file, &quot;image/jpeg&quot;),
            new TextContent(&quot;Extract the marked passages from the image&quot;),
        })
    };

    var response = await client.CompleteAsync&lt;List&lt;AIBookmark&gt;&gt;(messages, options: new ChatOptions {Temperature = 0.1f});

    passages.AddRange(response.Result);
}
</code></pre>
<p>This code:</p>
<ol>
<li><p>Loads the image as a <code>byte[]</code>.</p>
</li>
<li><p>Composes a set of messages that include:</p>
<ul>
<li>The system prompt</li>
<li>The image</li>
<li>The user prompt instructing the model to extract the marked passages from the image</li>
</ul>
</li>
<li><p>Sends the messages to the model for processing and returns a list of <code>AIBookmark</code>. An <code>AIBookmark</code> is a custom class I've defined as follows:</p>
<pre><code class="language-csharp">class AIBookmark
{
    public string Text {get;set;}

    public int PageNumber {get;set;}
}
</code></pre>
<p>Some of the more recent AI models support structured output, which enforce a schema on AI model outputs. You can take a look at the <a href="https://openai.com/index/introducing-structured-outputs-in-the-api/">OpenAI documentation</a> for more details. It's important to note though that the functionality is not exclusive to OpenAI models.</p>
</li>
<li><p>Adds the extracted passages to the <code>passages</code> collection.</p>
</li>
</ol>
</li>
</ol>
<h2>Format the results</h2>
<p>Once all of the files are processed by the AI model, additional processing is done to ensure that they're in the correct page order as well as formatted as markdown blockquotes.</p>
<pre><code class="language-csharp">var sortedPassages = 
    passages
        .OrderBy(p =&gt; p.PageNumber)
        .Select(p =&gt; $&quot;&gt; {p.Text} (pg. {p.PageNumber})&quot;);
</code></pre>
<h2>Display the results</h2>
<pre><code class="language-csharp">foreach(var passage in sortedPassages)
{
    Console.WriteLine(passage);
    Console.WriteLine(&quot;&quot;);
}
</code></pre>
<p>For the images included in the repo's <em>data</em> directory, output might look like the following.</p>
<pre><code class="language-markdown">&gt; This isn’t a matter of blind belief in yourself. It’s a matter of experimental faith. (pg. 278)

&gt; When we don’t yet know where we’re going, we don’t wait. We move forward in the dark. If nothing we attempt yields progress, we rely on belief and will. We may take several steps backward in the sequence to move ahead. (pg. 278)

&gt; If we try ten experiments and none of them work; we have a choice. We can take it personally, and think of ourselves as a failure and question our ability to solve the problem. (pg. 278)

&gt; Staying in it means a commitment to remain open to what’s around you. Paying attention and listening. Looking for connections and relationships in the outside world. Searching for beauty. Seeking stories. Noticing what you find interesting, what makes you lean forward. And knowing all of this is available to use next time you sit down to work, where the raw data gets put into form. (pg. 296)

&gt; Just as a surfer can’t control the waves, artists are at the mercy of the creative rhythms of nature. This is why it’s of such great importance to remain aware and present at all times. Watching and waiting. (pg. 296)

&gt; Maybe the best idea is the one you’re going to come up with this evening. (pg. 297)
</code></pre>
<h2>Improvements and next steps</h2>
<p>Putting the application together took me less than an hour so this is far from done. However, it does provide me with a starting point and offers validation that this could be a way to more easily capture the knowledge I'm curating from physical books.</p>
<p>Some improvements I can make here:</p>
<ol>
<li>Update the system prompt with some samples to help guide the extraction. For example, in the output I shared, that last passage is not annotated. It just happens to be a mostly blank page with that quote in the center, therefore giving the illusion that the passage is important in some way.</li>
<li>Add additional information to the AIBookmark class like <code>Index</code> so I can ensure order within a page is preserved. Right now page number is good enough, but I can't guarantee the correct order. An index property might help here.</li>
<li>Use a service with higher rate limits. The current rate limits wouldn't allow me to process a large number of images at once. Therefore, I'd need to use a service with higher limits. Alternatively, I could make this a job that runs in the background which abides by the rate limits but I also don't have to spend money on. Given I'm not using this for anything mission-critical, that'd be an acceptable solution as well.</li>
<li>Refactor the solution so I can more easily swap between tasks. For example, sometimes I might want to use it with images from my bullet journal. Other times, I might want to use it with handwritten notes. Whatever the case may be, it'd be good to not have to rewrite the prompts every time.</li>
</ol>
<p>Some ways I see myself using this project:</p>
<ol>
<li>Periodically collect images of annotated pages and save them to cloud storage.</li>
<li>When I'm done with the book, drop all the images in the <em>data</em> directory.</li>
<li>Further enrich data by condensing repetitive passages and extracting key concepts.</li>
<li>Storing this knowledge into some sort of knowledge store to make it actionable.</li>
</ol>
<h2>Conclusion</h2>
<p>Just for fun, Tyler released all of the lyrics to his <a href="/responses/chromakopia-tyler-the-creator-released">latest album</a> as <a href="https://twitter.com/tylerthecreator/status/1852105825650708651/photo/1">images on X</a>. With a few tweaks, I was able to repurpose this solution to extract the text from them and that worked relatively well.</p>
<p>Just with this simple solution, there's a ton of other applications I can think of in my daily life to help bridge my analog and digital lives.</p>
<p>What other use cases do you see yourself using something like this for? <a href="/contact">Let me know</a>.</p>
<p>Happy coding!</p>
</div></div></main><footer role="contentinfo"><hr><p><a href="/">Full Site</a> | <a href="/text/accessibility/">Accessibility</a></p></footer></body></html>