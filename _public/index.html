<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="stylesheet" href="/assets/css/custom/main.css"><link rel="stylesheet" href="/assets/css/custom/timeline.css"><link rel="stylesheet" href="/assets/css/bootstrap-icons-1.5.0/bootstrap-icons.css"><link rel="stylesheet" href="/assets/css/highlight.github-dark-dimmed.min.css"><link rel="stylesheet" href="/assets/css/main.css"><meta property="og:title" content="Luis Quintanilla - Personal Website"><meta property="og:type" content="website"><meta property="og:image" content="https://www.luisquintanilla.me/avatar.png"><meta property="og:image:secure_url" content="https://www.luisquintanilla.me/avatar.png"><meta property="og:image:type" content="image/png"><meta property="og:image:width" content="200"><meta property="og:image:height" content="200"><meta property="og:site_name" content="Luis Quintanilla Personal Website"><meta property="og:locale" content="en_US"><meta property="twitter:image" content="https://www.luisquintanilla.me/avatar.png"><meta property="fediverse:creator" content="@lqdev@toot.lqdev.tech"><link rel="alternate" type="application/rss+xml" title="Luis Quintanilla Blog RSS Feed" href="/blog.rss"><link rel="alternate" type="application/rss+xml" title="Luis Quintanilla Microblog RSS Feed" href="/microblog.rss"><link rel="alternate" type="application/rss+xml" title="Luis Quintanilla Response RSS Feed" href="/responses.rss"><link rel="webmention" title="Luis Quintanilla Webmention Endpoint" href="https://webmentions.lqdev.tech/api/inbox"><link rel="feeds" type="text/xml" title="Luis Quintanilla&#39;s Feeds" href="/feed/index.opml"><link rel="blogroll" type="text/xml" title="Luis Quintanilla&#39;s Blogroll" href="/collections/blogroll/index.opml"><link rel="podroll" type="text/xml" title="Luis Quintanilla&#39;s Podroll" href="/collections/podroll/index.opml"><link rel="youtuberoll" type="text/xml" title="Luis Quintanilla&#39;s YouTube Roll" href="/collections/youtube/index.opml"><meta name="robots" content="nosnippet"><title>Luis Quintanilla - Personal Website</title></head><body><button class="mobile-toggle" id="mobile-nav-toggle" aria-label="Toggle navigation menu" aria-expanded="false"><div class="hamburger"><span></span><span></span><span></span></div></button><div class="nav-overlay" id="nav-overlay"></div><nav class="desert-nav" id="sidebar-menu" role="navigation" aria-label="Main navigation"><div class="nav-brand"><a href="/" class="brand-text"><img src="/avatar.png" alt="Luis Quintanilla avatar" loading="lazy">Luis Quintanilla</a></div><div class="nav-section"><a class="nav-link" href="/"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M8.354 1.146a.5.5 0 0 0-.708 0l-6 6A.5.5 0 0 0 1.5 7.5v7a.5.5 0 0 0 .5.5h4.5a.5.5 0 0 0 .5-.5v-4h2v4a.5.5 0 0 0 .5.5H14a.5.5 0 0 0 .5-.5v-7a.5.5 0 0 0-.146-.354L8.354 1.146zM2.5 14V7.707l5.5-5.5 5.5 5.5V14H10v-4a.5.5 0 0 0-.5-.5h-3a.5.5 0 0 0-.5.5v4H2.5z"></path></svg>Home</a><a class="nav-link" href="/about"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M8 15A7 7 0 1 1 8 1a7 7 0 0 1 0 14zm0 1A8 8 0 1 0 8 0a8 8 0 0 0 0 16z"></path><path d="m8.93 6.588-2.29.287-.082.38.45.083c.294.07.352.176.288.469l-.738 3.468c-.194.897.105 1.319.808 1.319.545 0 1.178-.252 1.465-.598l.088-.416c-.2.176-.492.246-.686.246-.275 0-.375-.193-.304-.533L8.93 6.588zM9 4.5a1 1 0 1 1-2 0 1 1 0 0 1 2 0z"></path></svg>About</a><a class="nav-link" href="/contact"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V4Zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1H2Zm13 2.383-4.708 2.825L15 11.105V5.383Zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741ZM1 11.105l4.708-2.897L1 5.383v5.722Z"></path></svg>Contact</a><a class="nav-link" href="/search"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z"></path></svg>Search</a><a class="nav-link" href="/feed"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M5.5 12a1.5 1.5 0 1 1-3 0 1.5 1.5 0 0 1 3 0zm-3-8.5a1 1 0 0 1 1-1c5.523 0 10 4.477 10 10a1 1 0 1 1-2 0 8 8 0 0 0-8-8 1 1 0 0 1-1-1zm0 4a1 1 0 0 1 1-1 6 6 0 0 1 6 6 1 1 0 1 1-2 0 4 4 0 0 0-4-4 1 1 0 0 1-1-1z"></path></svg>Subscribe</a></div><div class="nav-section dropdown"><button class="nav-link dropdown-toggle" data-target="collections-dropdown"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M2.5 3A1.5 1.5 0 0 0 1 4.5v.793c.026.009.051.02.076.032L7.674 8.51c.206.1.446.1.652 0l6.598-3.185A.755.755 0 0 1 15 5.293V4.5A1.5 1.5 0 0 0 13.5 3h-11Z"></path><path d="M15 6.954 8.978 9.86a2.25 2.25 0 0 1-1.956 0L1 6.954V11.5A1.5 1.5 0 0 0 2.5 13h11a1.5 1.5 0 0 0 1.5-1.5V6.954Z"></path></svg>Collections<svg class="dropdown-arrow" viewBox="0 0 16 16" fill="currentColor"><path d="M7.247 11.14 2.451 5.658C1.885 5.013 2.345 4 3.204 4h9.592a1 1 0 0 1 .753 1.659l-4.796 5.48a1 1 0 0 1-1.506 0z"></path></svg></button><div class="dropdown-menu" id="collections-dropdown"><a class="dropdown-item" href="/collections/blogroll">Blogroll</a><a class="dropdown-item" href="/collections/podroll">Podroll</a><a class="dropdown-item" href="/collections/youtube">YouTube</a><a class="dropdown-item" href="/collections/forums">Forums</a><div class="dropdown-divider"></div><a class="dropdown-item" href="/collections/starter-packs">Starter Packs</a><div class="dropdown-divider"></div><a class="dropdown-item" href="/radio">Radio</a><a class="dropdown-item" href="/tags">Tags</a></div></div><div class="nav-section dropdown"><button class="nav-link dropdown-toggle" data-target="resources-dropdown"><svg class="nav-icon" viewBox="0 0 16 16" fill="currentColor"><path d="M1 2.828c.885-.37 2.154-.769 3.388-.893 1.33-.134 2.458.063 3.112.752v9.746c-.935-.53-2.12-.603-3.213-.493-1.18.12-2.37.461-3.287.811V2.828zm7.5-.141c.654-.689 1.782-.886 3.112-.752 1.234.124 2.503.523 3.388.893v9.923c-.918-.35-2.107-.692-3.287-.81-1.094-.111-2.278-.039-3.213.492V2.687zM8 1.783C7.015.936 5.587.81 4.287.94c-1.514.153-3.042.672-3.994 1.105A.5.5 0 0 0 0 2.5v11a.5.5 0 0 0 .707.455c.882-.4 2.303-.881 3.68-1.02 1.409-.142 2.59.087 3.223.877a.5.5 0 0 0 .78 0c.633-.79 1.814-1.019 3.222-.877 1.378.139 2.8.62 3.681 1.02A.5.5 0 0 0 16 13.5v-11a.5.5 0 0 0-.293-.455c-.952-.433-2.48-.952-3.994-1.105C10.413.809 8.985.936 8 1.783z"></path></svg>Resources<svg class="dropdown-arrow" viewBox="0 0 16 16" fill="currentColor"><path d="M7.247 11.14 2.451 5.658C1.885 5.013 2.345 4 3.204 4h9.592a1 1 0 0 1 .753 1.659l-4.796 5.48a1 1 0 0 1-1.506 0z"></path></svg></button><div class="dropdown-menu" id="resources-dropdown"><a class="dropdown-item" href="/resources/snippets">Snippets</a><a class="dropdown-item" href="/resources/wiki">Wiki</a><a class="dropdown-item" href="/resources/presentations">Presentations</a></div></div><div class="theme-toggle"><button class="theme-toggle-btn" id="theme-toggle" aria-label="Toggle dark mode"><span id="theme-toggle-icon">☀️</span><span>Theme</span></button></div></nav><main role="main" class="main-content" id="main-content"><div class="content-wrapper"><div class="h-feed unified-timeline"><header class="timeline-header text-center p-4"><div class="avatar-section mb-3"><img src="/avatar.png" alt="Luis Quintanilla Avatar Image" class="rounded-circle" height="150" width="150" /><div class="mt-2"><h1 class="p-name">Hi, I&#39;m <a href="/about" class="author-link">Luis</a><span> &#x1F44B;</span></h1><p class="tagline">Latest updates from across the site</p></div></div><div class="content-filters mt-3" id="contentFilters"><button class="filter-btn active" data-filter="all" type="button">All</button><button class="filter-btn" data-filter="posts" type="button">Blog Posts</button><button class="filter-btn" data-filter="notes" type="button">Notes</button><button class="filter-btn" data-filter="responses" type="button">Responses</button><button class="filter-btn" data-filter="reviews" type="button">Reviews</button><button class="filter-btn" data-filter="bookmarks" type="button">Bookmarks</button><button class="filter-btn" data-filter="media" type="button">Media</button></div></header><main class="timeline-content"><div class="initial-content" id="initialContent"><article class="h-entry content-card" data-type="reshare" data-date="2025-08-18 19:31 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-18 19:31 -05:00">Aug 18, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="reshare">Reshare</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/responses/reflections-on-social-web/">Reflections on the social web</a></h2><div class="e-content card-content"><div class="response-target" >→ <a href="https://john.onolan.org/reflections-on-the-social-web/" >https://john.onolan.org/reflections-on-the-social-web/</a></div><div class="response-content" ><blockquote class="blockquote">
<p>The work of a product team, when working with new technology, is to abstract away as much of this complexity as possible, so that it feels friendly and approachable to new people.</p>
</blockquote>
<blockquote class="blockquote">
<p>My strongest belief about the social web is that if we want it to succeed, we have to keep lowering the barrier to entry.</p>
</blockquote>
</div>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/ghost/">#ghost</a><a class="tag-link" href="/tags/socialweb/">#socialweb</a><a class="tag-link" href="/tags/indieweb/">#indieweb</a></div></div></footer></article><article class="h-entry content-card" data-type="reshare" data-date="2025-08-18 19:24 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-18 19:24 -05:00">Aug 18, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="reshare">Reshare</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/responses/ghost-6-changelog/">Ghost 6.0</a></h2><div class="e-content card-content"><div class="response-target" >→ <a href="https://ghost.org/changelog/6/" >https://ghost.org/changelog/6/</a></div><div class="response-content" ><blockquote class="blockquote">
<p>In Ghost 6.0 we're introducing another new distribution channel: The social web. Now, millions of people can discover, follow, like and reply to your posts from any supported social web client - including Bluesky, Mastodon, Threads, Flipboard, Ghost, WordPress, Surf, WriteFreely, and many more.</p>
</blockquote>
<blockquote class="blockquote">
<p>Just as people can visit your Ghost website in any browser, subscribe with any feed reader, or receive newsletters in any email client, they can now follow your Ghost publication from any social web client.</p>
</blockquote>
<blockquote class="blockquote">
<p>Unlike closed platforms with restrictive algorithms, open web protocols create a direct connection between you and your subscribers which you have full control over.</p>
</blockquote>
<blockquote class="blockquote">
<p>If you've been around on the web for while, and you can remember back that far... you might even call it the return of the blogosphere.</p>
</blockquote>
<p>Love the introduction of social components to the Ghost platform. I'm so ready for the return of the blogosphere and the return to open protocols.</p>
</div>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/ghost/">#ghost</a><a class="tag-link" href="/tags/fediverse/">#fediverse</a><a class="tag-link" href="/tags/socialweb/">#socialweb</a><a class="tag-link" href="/tags/publishing/">#publishing</a></div></div></footer></article><article class="h-entry content-card" data-type="star" data-date="2025-08-18 19:21 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-18 19:21 -05:00">Aug 18, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="star">Star</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/responses/newly-redesigned-blogroll/">The newly redesigned Blogroll Club</a></h2><div class="e-content card-content"><div class="response-target" >→ <a href="https://blogroll.club/the-newly-redesigned-blogroll-club/" >https://blogroll.club/the-newly-redesigned-blogroll-club/</a></div><div class="response-content" ></div>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/blogrollclub/">#blogrollclub</a><a class="tag-link" href="/tags/blogroll/">#blogroll</a><a class="tag-link" href="/tags/indieweb/">#indieweb</a><a class="tag-link" href="/tags/socialweb/">#socialweb</a></div></div></footer></article><article class="h-entry content-card" data-type="bookmarks" data-date="2025-08-18 18:53 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-18 18:53 -05:00">Aug 18, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="bookmarks">Bookmark</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/responses/fedicon-2025-videos/">FediCon 2025 Videos</a></h2><div class="e-content card-content"><div class="response-target" >→ <a href="https://spectra.video/w/p/52g7HzMoUCxEm9nU339mPf?playlistPosition=1" >https://spectra.video/w/p/52g7HzMoUCxEm9nU339mPf?playlistPosition=1</a></div><div class="response-content" ></div>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/fediverse/">#fediverse</a><a class="tag-link" href="/tags/fedicon2025/">#fedicon2025</a><a class="tag-link" href="/tags/conference/">#conference</a></div></div></footer></article><article class="h-entry content-card" data-type="bookmarks" data-date="2025-08-18 18:53 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-18 18:53 -05:00">Aug 18, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="bookmarks">Bookmark</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/responses/hope-16-videos/">HOPE 16 Conference Videos</a></h2><div class="e-content card-content"><div class="response-target" >→ <a href="https://www.youtube.com/watch?v=2xuoa3PLMvU&amp;amp;amp;list=PLlg8We3ePxcPG0FRTcEEdsViRevFjikIU" >https://www.youtube.com/watch?v=2xuoa3PLMvU&amp;amp;list=PLlg8We3ePxcPG0FRTcEEdsViRevFjikIU</a></div><div class="response-content" ></div>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/hope/">#hope</a><a class="tag-link" href="/tags/hacker/">#hacker</a><a class="tag-link" href="/tags/conference/">#conference</a></div></div></footer></article><article class="h-entry content-card" data-type="reshare" data-date="2025-08-18 18:36 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-18 18:36 -05:00">Aug 18, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="reshare">Reshare</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/responses/fediurbanism-fedicon-2025/">FediUrbanism — Johanna Botari — FediCon 2025</a></h2><div class="e-content card-content"><div class="response-target" >→ <a href="https://spectra.video/w/f7GBsBf3nAhF8WfTMWUEXm" >https://spectra.video/w/f7GBsBf3nAhF8WfTMWUEXm</a></div><div class="response-content" ><p>I really enjoyed this talk by <a href="https://cosocial.ca/@johannab">@johannab</a> on the concept of FediUrbanism.</p>
<p><a href="https://spectra.video/w/f7GBsBf3nAhF8WfTMWUEXm" title="Thumbnail of FediUrbanism Talk Recording"><img src="https://spectra.video/lazy-static/previews/40a5e4bd-f8d7-438a-b35c-0c81e753ddbd.jpg" class="img-fluid" alt="Thumbnail of FediUrbanism Talk Recording" /></a></p>
</div>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/fediurbanism/">#fediurbanism</a><a class="tag-link" href="/tags/fedicon2025/">#fedicon2025</a><a class="tag-link" href="/tags/fediverse/">#fediverse</a><a class="tag-link" href="/tags/thirdspace/">#thirdspace</a><a class="tag-link" href="/tags/community/">#community</a></div></div></footer></article><article class="h-entry content-card" data-type="snippets" data-date="2025-08-14 13:35 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-14 13:35 -05:00">Aug 14, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="snippets">snippets</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/resources/snippets/owncast-optimized-azure-deployment-guide/">Deploy Owncast to Azure Container Apps with Persistent Storage</a></h2><div class="e-content card-content">
<p>This guide shows how to deploy Owncast to Azure Container Apps with persistent storage and scale-to-zero capability to minimize costs.</p>

<ul>
<li>Azure CLI installed and logged in</li>
<li>An Azure subscription</li>
<li>A resource group created</li>
</ul>

<h3>Create Storage Account for Persistent Data</h3>
<pre><code class="language-bash"># Set variables (choose cheapest regions)
RESOURCE_GROUP=&amp;quot;your-resource-group&amp;quot;
LOCATION=&amp;quot;centralus&amp;quot;  # Often cheaper than eastus
STORAGE_ACCOUNT=&amp;quot;owncaststorage$(date +%s)&amp;quot;  # Must be globally unique
CONTAINER_APP_ENV=&amp;quot;owncast-env&amp;quot;
CONTAINER_APP_NAME=&amp;quot;owncast-app&amp;quot;

<p>az storage account create <br />
--name $STORAGE_ACCOUNT <br />
--resource-group $RESOURCE_GROUP <br />
--location $LOCATION <br />
--sku Standard_LRS <br />
--kind StorageV2 <br />
--access-tier Cool <br />
--allow-blob-public-access false <br />
--https-only true <br />
--min-tls-version TLS1_2</p>

<p>az storage share create <br />
--name &amp;quot;owncast-data&amp;quot; <br />
--account-name $STORAGE_ACCOUNT <br />
--quota 1  # Start with 1GB, scales automatically
</code></pre></p>
<h3>Create Container Apps Environment</h3>
<pre><code class="language-bash"># Create the Container Apps environment
az containerapp env create \
  --name $CONTAINER_APP_ENV \
  --resource-group $RESOURCE_GROUP \
  --location $LOCATION
</code></pre>

<p>Get the storage account key and create the storage mount:</p>
<pre><code class="language-bash"># Get storage account key
STORAGE_KEY=$(az storage account keys list \
  --account-name $STORAGE_ACCOUNT \
  --resource-group $RESOURCE_GROUP \
  --query &amp;quot;[0].value&amp;quot; -o tsv)

<p>az containerapp env storage set <br />
--name $CONTAINER_APP_ENV <br />
--resource-group $RESOURCE_GROUP <br />
--storage-name &amp;quot;owncast-storage&amp;quot; <br />
--azure-file-account-name $STORAGE_ACCOUNT <br />
--azure-file-account-key $STORAGE_KEY <br />
--azure-file-share-name &amp;quot;owncast-data&amp;quot; <br />
--access-mode ReadWrite
</code></pre></p>

<p>Create the container app with persistent storage:</p>
<pre><code class="language-bash">az containerapp create \
  --name $CONTAINER_APP_NAME \
  --resource-group $RESOURCE_GROUP \
  --environment $CONTAINER_APP_ENV \
  --image &amp;quot;owncast/owncast:latest&amp;quot; \
  --target-port 8080 \
  --ingress external \
  --min-replicas 0 \
  --max-replicas 1 \
  --cpu 0.5 \
  --memory 1Gi \
  --volume-mount &amp;quot;data:/app/data&amp;quot; \
  --volume-name &amp;quot;data&amp;quot; \
  --volume-storage-name &amp;quot;owncast-storage&amp;quot; \
  --volume-storage-type AzureFile
</code></pre>

<p>For Owncast to work properly, you need both HTTP (8080) and RTMP (1935) ports. This requires a <strong>Virtual Network (VNet)</strong> integration:</p>
<h3>Create VNet and Subnet</h3>
<pre><code class="language-bash"># Create MINIMAL virtual network (smallest possible address space)
az network vnet create \
  --name &amp;quot;owncast-vnet&amp;quot; \
  --resource-group $RESOURCE_GROUP \
  --location $LOCATION \
  --address-prefix &amp;quot;10.0.0.0/24&amp;quot;  # Smaller than default /16

<p>az network vnet subnet create <br />
--name &amp;quot;container-apps-subnet&amp;quot; <br />
--resource-group $RESOURCE_GROUP <br />
--vnet-name &amp;quot;owncast-vnet&amp;quot; <br />
--address-prefix &amp;quot;10.0.0.0/27&amp;quot;  # Only 32 IPs instead of /23 (512 IPs)
</code></pre></p>
<h3>Recreate Container Apps Environment with VNet</h3>
<pre><code class="language-bash"># Get subnet ID
SUBNET_ID=$(az network vnet subnet show \
  --name &amp;quot;container-apps-subnet&amp;quot; \
  --vnet-name &amp;quot;owncast-vnet&amp;quot; \
  --resource-group $RESOURCE_GROUP \
  --query id -o tsv)

<p>az containerapp env delete <br />
--name $CONTAINER_APP_ENV <br />
--resource-group $RESOURCE_GROUP <br />
--yes</p>

<p>az containerapp env create <br />
--name $CONTAINER_APP_ENV <br />
--resource-group $RESOURCE_GROUP <br />
--location $LOCATION <br />
--infrastructure-subnet-resource-id $SUBNET_ID <br />
--enable-workload-profiles false  # Forces consumption-only pricing
</code></pre></p>
<h3>Deploy Container App with MINIMAL Resources</h3>
<pre><code class="language-bash">az containerapp create \
  --name $CONTAINER_APP_NAME \
  --resource-group $RESOURCE_GROUP \
  --environment $CONTAINER_APP_ENV \
  --image &amp;quot;owncast/owncast:latest&amp;quot; \
  --target-port 8080 \
  --exposed-port 1935 \
  --ingress external \
  --transport auto \
  --min-replicas 0 \
  --max-replicas 1 \
  --cpu 0.25 \
  --memory 0.5Gi \
  --volume-mount &amp;quot;data:/app/data&amp;quot; \
  --volume-name &amp;quot;data&amp;quot; \
  --volume-storage-name &amp;quot;owncast-storage&amp;quot; \
  --volume-storage-type AzureFile
</code></pre>
<p><strong>Cost-Optimized Resource Allocation:</strong></p>
<ul>
<li><strong>CPU</strong>: 0.25 cores (minimum allowed, sufficient for small streams)</li>
<li><strong>Memory</strong>: 0.5Gi (minimum allowed, will work for basic streaming)</li>
<li><strong>Scaling</strong>: Aggressive scale-to-zero with max 1 replica</li>
</ul>

<p>For maximum cost optimization, use this YAML approach with the smallest possible resource allocation:</p>
<pre><code class="language-yaml"># owncast-minimal-cost.yaml
properties:
  managedEnvironmentId: /subscriptions/{subscription-id}/resourceGroups/{resource-group}/providers/Microsoft.App/managedEnvironments/{environment-name}
  configuration:
    ingress:
      external: true
      targetPort: 8080
      additionalPortMappings:
      - external: true
        targetPort: 1935
        exposedPort: 1935
    secrets: []
  template:
    containers:
    - image: owncast/owncast:latest
      name: owncast
      resources:
        cpu: 0.25
        memory: 0.5Gi
      volumeMounts:
      - mountPath: /app/data
        volumeName: data
      env:
      - name: OWNCAST_RTMP_PORT
        value: &amp;quot;1935&amp;quot;
      - name: OWNCAST_WEBSERVER_PORT  
        value: &amp;quot;8080&amp;quot;
    scale:
      minReplicas: 0
      maxReplicas: 1
      rules:
      - name: &amp;quot;http-rule&amp;quot;
        http:
          metadata:
            concurrentRequests: &amp;quot;10&amp;quot;  # Scale up quickly but keep minimal
    volumes:
    - name: data
      storageType: AzureFile
      storageName: owncast-storage
</code></pre>
<p>Deploy with:</p>
<pre><code class="language-bash">az containerapp create \
  --name $CONTAINER_APP_NAME \
  --resource-group $RESOURCE_GROUP \
  --yaml owncast-minimal-cost.yaml
</code></pre>

<h3>Scale-to-Zero Configuration</h3>
<ul>
<li><strong>Min Replicas</strong>: Set to 0 to completely scale down when not in use</li>
<li><strong>Max Replicas</strong>: Set to 1 (Owncast doesn't need horizontal scaling)</li>
<li><strong>Scale Rules</strong>: Container Apps will automatically scale up when requests arrive</li>
</ul>
<h3>Resource Limits (Ultra Cost-Optimized)</h3>
<ul>
<li><strong>CPU</strong>: 0.25 cores (absolute minimum, sufficient for 1-2 viewer streams)</li>
<li><strong>Memory</strong>: 0.5Gi (minimum allowed by Azure Container Apps)</li>
<li><strong>Storage</strong>: Cool tier with 1GB initial quota (auto-scales as needed)</li>
<li><strong>Network</strong>: Minimal VNet addressing to reduce overhead</li>
</ul>

<p>After deployment, configure OBS for streaming:</p>
<ol>
<li><p><strong>Server Settings</strong>: Use <code>rtmp://your-app-url:1935/live</code> (note: <code>rtmp://</code> not <code>https://</code>)</p>
</li>
<li><p><strong>Stream Key</strong>: Use the key from Owncast admin panel (Configuration &amp;gt; Server Setup &amp;gt; Stream Keys)</p>
</li>
<li><p><strong>Owncast Web Interface</strong>: Access at <code>https://your-app-url</code> (port 8080 is handled automatically by ingress)</p>
</li>
<li><p><strong>Persistent Data</strong>: All Owncast configuration, database, and uploaded files are stored in Azure Files and persist across container restarts and scale-to-zero events.</p>
</li>
<li><p><strong>Cold Start</strong>: When scaling from zero, there will be a brief cold start delay as the container initializes.</p>
</li>
<li><p><strong>VNet Requirement</strong>: For dual-port access (HTTP + RTMP), you <strong>must</strong> use a Virtual Network integration. This is a requirement for exposing additional TCP ports in Azure Container Apps.</p>
</li>
<li><p><strong>Security Configuration</strong>: After deployment, immediately change the default admin credentials:</p>
<ul>
<li>Navigate to <code>https://your-app-url/admin</code></li>
<li>Default login: <code>admin</code> / <code>abc123</code></li>
<li>Go to Configuration &amp;gt; Server Setup and change the admin password</li>
<li>Create/copy stream keys from Configuration &amp;gt; Server Setup &amp;gt; Stream Keys tab</li>
</ul>
</li>
<li><p><strong>Custom Domain</strong>: You can configure a custom domain using:</p>
<pre><code class="language-bash">az containerapp hostname add \
  --name $CONTAINER_APP_NAME \
  --resource-group $RESOURCE_GROUP \
  --hostname &amp;quot;your-domain.com&amp;quot;
</code></pre>
</li>
<li><p><strong>SSL Certificate</strong>: Azure Container Apps provides automatic SSL certificates for custom domains.</p>
</li>
</ol>

<p>Check your deployment:</p>
<pre><code class="language-bash"># Get the URL
az containerapp show \
  --name $CONTAINER_APP_NAME \
  --resource-group $RESOURCE_GROUP \
  --query properties.configuration.ingress.fqdn

<p>az containerapp logs show <br />
--name $CONTAINER_APP_NAME <br />
--resource-group $RESOURCE_GROUP
</code></pre></p>

<p>With these optimizations, your monthly costs should be:</p>
<p><strong>When Streaming (4 hours/month example):</strong></p>
<ul>
<li><strong>Compute</strong>: ~$0.50/month (0.25 CPU + 0.5Gi RAM × 4 hours)</li>
<li><strong>Container Apps Environment</strong>: ~$0.00 (consumption plan, no dedicated resources)</li>
<li><strong>Networking</strong>: ~$0.05/month (minimal VNet overhead)</li>
</ul>
<p><strong>When Idle (Scale-to-Zero):</strong></p>
<ul>
<li><strong>Compute</strong>: $0.00 (scaled to zero)</li>
<li><strong>Environment</strong>: $0.00 (consumption plan)</li>
</ul>
<p><strong>Always-On Costs:</strong></p>
<ul>
<li><strong>Storage</strong>: ~$0.05-0.10/month (1-2GB in Cool tier)</li>
<li><strong>VNet</strong>: ~$0.00 (no gateways or dedicated resources)</li>
</ul>
<p><strong>Total Monthly Cost: ~$0.60-0.65/month</strong> (assuming 4 hours of streaming)</p>
<h3>Performance Expectations at Minimal Resources:</h3>
<ul>
<li><strong>0.25 CPU + 0.5Gi RAM</strong>: Suitable for 480p-720p streams with 1-5 concurrent viewers</li>
<li><strong>Scale-up Path</strong>: Monitor performance and increase to 0.5 CPU + 1Gi if needed</li>
<li><strong>Cold Start</strong>: ~10-15 seconds when scaling from zero (acceptable for personal streaming)</li>
</ul>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/owncast/">#owncast</a><a class="tag-link" href="/tags/fediverse/">#fediverse</a><a class="tag-link" href="/tags/azure/">#azure</a><a class="tag-link" href="/tags/livestream/">#livestream</a></div></div></footer></article><article class="h-entry content-card" data-type="reshare" data-date="2025-08-14 12:40"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-14 12:40">Aug 14, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="reshare">Reshare</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/responses/introducing-gemma-3-270m/">Introducing Gemma 3 270M</a></h2><div class="e-content card-content"><div class="response-target" >→ <a href="https://developers.googleblog.com/en/introducing-gemma-3-270m/" >https://developers.googleblog.com/en/introducing-gemma-3-270m/</a></div><div class="response-content" ><blockquote class="blockquote">
<p>Today, we're adding a new, highly specialized tool to the Gemma 3 toolkit: Gemma 3 270M, a compact, 270-million parameter model designed from the ground up for task-specific fine-tuning with strong instruction-following and text structuring capabilities already trained in.</p>
</blockquote>
</div>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/ai/">#ai</a><a class="tag-link" href="/tags/google/">#google</a><a class="tag-link" href="/tags/gemma/">#gemma</a><a class="tag-link" href="/tags/slm/">#slm</a><a class="tag-link" href="/tags/genai/">#genai</a></div></div></footer></article><article class="h-entry content-card" data-type="posts" data-date="2025-08-14 01:45 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-14 01:45 -05:00">Aug 14, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="posts">Blog Post</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/posts/website-mobile-publishing-discord-client/">Mobile-First Static Site Publishing: Discord Bot Pipeline via Azure and GitHub</a></h2><div class="e-content card-content">
<p>Ever since I published my <a href="/notes/hello-world">first note</a>(<a href="https://indieweb.org/note">microblog post</a>) on my website, I've always wanted a way to quickly publish while on the go. Unfortunately, I never found a good solution.</p>
<p>Because my website is statically generated and the source is hosted on GitHub(check out the <a href="/colophon">colophon</a> for more details), there is no backend for me to talk to. At the same time, I didn't want to build an entire backend to support my website because I want to keep things as lean and cost-efficient as possible.</p>
<p>Since my posts are just frontmatter and Markdown, I <a href="/uses">use VS Code as my editor</a>. For some time, back when I used to have a Surface Duo, I <a href="https://www.lqdev.me/notes/surface-duo-blogging-github-dev">authored posts from mobile using the github.dev experience</a>. On two screens, while not ideal, it was manageable. After switching devices (because sadly there were no more security updates on the Surface Duo) and upgrading to a dumbphone and later a single screen smartphone, that workflow wasn't feasible.</p>
<p>At that point, what I resorted to was sending messages to myself via <a href="https://element.io/">Element</a>. The message would contain a link I wanted to check out later. Once I was on my laptop, I would check out the link and if I wanted to post about it on my website, I'd do so then.</p>
<p>That process, while it worked, wasn't necessarily scalable. In part that's a feature because I could spend more time digesting the content and writing a thoughtful article. However, it stopped me from sharing more in the moment and there were posts that were never authored or bookmarks that weren't captured because eventually that link got lost in the river of other links.</p>
<p>Basically what I wanted to replicate was the more instant posting that social media gives you, but do so on my own site.</p>
<p>That led me to doing some thinking and requirement gathering around the type of experience I wanted to have.</p>

<p>When it came to requirements for my solution, I was focused more on the workflow and experience rather on technical details.</p>
<p>Here is a list of those solution requirements:</p>
<ul>
<li>Mobile is the primary publishing interface. Desktop publishing is a nice to have.</li>
<li>Be as low-friction as sharing a link via Element or posting on social media</li>
<li>Doesn't require implementing my own client or frontend</li>
<li>Doesn't require me to use existing micropub clients</li>
<li>Handles short-form and media posts supported by my website
<ul>
<li>Notes</li>
<li>Responses
<ul>
<li>Repost</li>
<li>Reply</li>
<li>Like</li>
</ul>
</li>
<li>Bookmark</li>
<li>Media
<ul>
<li>Image</li>
<li>Audio (not used as often but technically supported)</li>
<li>Video (not used as often but technically supported)</li>
</ul>
</li>
</ul>
</li>
<li>Low-cost</li>
</ul>

<p>For years, I had been struggling with actually implementing this system. The main part that gave me pause was not implementing my own client or relying on existing micropub clients.</p>
<p>Eventually, I just accepted that it might never happen.</p>
<p>One day, it eventually hit me. If the notes to self Element workflow worked so well, why not use a chat client as the frontend for publishing. At least have it serve as the capture system that would then format the content into a post that gets queued for publishing on GitHub. I'd seen <a href="https://github.com/benjifs/serverless-micropub">Benji do something similar with his micropub endpoint</a>.</p>
<p>While I could've used Element since that's my preferred platform, I've been contemplating no longer hosting my own Matrix server. So if I went through with this, I'd want something that I didn't feel bad about investing the time on this solution if that chat client went away.</p>
<p>That then left me with Discord as the next best option. Primarily because of its support for bots as well as its cross-platform support across mobile and desktop.</p>
<p>In the end, the solution then ended up being fairly straightforward.</p>
<p>More importantly, with the help of AI, I wrote none of the code.</p>
<p>Using Copilot and Claude Sonnet 4, I was able to go from idea to deployment in 1-2 days. At that time the solution supported all of the posts except for media which I hadn't figured out what the best way of uploading media through Discord was. Figuring that out, implementing it, and deploying it took another day or two.</p>
<p>Since I wanted for my solution to be as low-cost as possible, serverless seemed like a good option. I only pay for compute when it's actually being used which can be infrequent in my case. I don't need the server running 24/7 or even to be powerful. However, I didn't want to write my system as an Azure Function. I wanted the flexibility of deploying on a shared VM or container. A VM though wasn't an option since it's running 24/7. Keeping all of that in mind, my choice was narrowed down to Azure Container Apps which gave me the characteristics I was looking for. Serverless containers.</p>
<p>Once that decision was made, I used Copilot again to figure out how to optimize my container image so that it's space and resource efficient. And while at it, I used Copilot again to figure out the right incantations to get the container deployed to Azure Container Apps.</p>
<p>All-in-all, the solution had been staring at me in the face since I already had a workflow that for the most part worked for me, it just needed some optimizations and with the help of AI, I was able to quickly build and deploy something I'd been ruminating over for years.</p>

<p>The workflow for publishing is as follows:</p>
<ol>
<li><p>Invoke the bot in Discord to capture my input using slash command <code>/post</code> and the respective post type.</p>
<p><img src="/files/images/invoke-discord-publishing-bot.png" class="img-fluid" alt="Using slash commands to invoke discord publishing bot" /></p>
</li>
<li><p>Provide post details. For media posts, I can provide an attachment which gets uploaded to Azure Blob Storage.</p>
<p><img src="/files/images/discord-publishing-client-modal.png" class="img-fluid" alt="A modal in discord with note post fields filled in" /></p>
</li>
<li><p><a href="https://github.com/lqdev/luisquintanilla.me/pull/152">Bot creates a branch and PR in my repo</a> with the post content</p>
</li>
<li><p>While logged into GitHub from my phone, if everything looks good, I merge the PR which kicks off my GitHub Actions workflow to build and publish the site including the new post.</p>
</li>
<li><p>Post <a href="/media/test-media-post-from-mobile/">displays on my website</a>.</p>
</li>
</ol>

<p>The solution is not perfect.</p>
<p>One of the problems I've run into is cold-start. Since I scale my solution down to zero when it's not being used to save on costs, I suffer from the cold start problem. Therefore, when I first invoke the bot, it fails. I have to give it a few seconds and retry the invocation. It's usually about 5 seconds so it's not a huge issue but it does add some friction.</p>

<p>Overall I'm happy with my solution but there are a few improvements I'd like to make.</p>
<ul>
<li><strong>Open-source the repo</strong> - Currently I've kept the repo private since it was all AI generated. Since my system is already in production and processes were documented, I need to do a more thorough pass to make sure that no secrets or credentials are checked in or documented anywhere.</li>
<li><strong>Improve UX</strong> - Discord limits modal fields to 5. Therefore, I'm playing around with the right balance between how much of the input should come from slash commands and how much should come from the modal.</li>
<li><strong>Expand supported post types</strong> - I'd like to expand the number of posts supported by my publishing client. Reviews are a good example of the type of post I'd like to support as well as RSVPs. <a href="https://www.lqdev.me/reviews">Reviews I already support</a> on my website but RSVPs I don't yet. Also, I'd have to fix my Webmentions which are currently broken after <a href="/notes/hello-world-new-site-2025-08/">upgrading my website</a>.</li>
<li><strong>Make it generator agnostic</strong> - Currently this only works for my website. With a few tweaks and refactoring, I think I can get the project to a place where it should work with other popular static site generators.</li>
<li><strong>One-click deployment</strong> - Currently the solution is packaged up as a container so it can be deployed from anywhere. I want to make it even simpler to deploy. One click if possible.</li>
</ul>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/discord/">#discord</a><a class="tag-link" href="/tags/azure/">#azure</a><a class="tag-link" href="/tags/indieweb/">#indieweb</a><a class="tag-link" href="/tags/mobile/">#mobile</a><a class="tag-link" href="/tags/microblogging/">#microblogging</a><a class="tag-link" href="/tags/web/">#web</a><a class="tag-link" href="/tags/bot/">#bot</a><a class="tag-link" href="/tags/aca/">#aca</a><a class="tag-link" href="/tags/azurecontainerapps/">#azurecontainerapps</a></div></div></footer></article><article class="h-entry content-card" data-type="media" data-date="2025-08-12 22:40 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-12 22:40 -05:00">Aug 12, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="media">Media</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/media/test-media-post-from-mobile/">Test media post from mobile</a></h2><div class="e-content card-content"><p>Test media posting from mobile</p>
<div class="custom-media-block" ><figure class="custom-media-item" ><img src="/files/images/20250813_034035_1000002298.jpg"  alt="Closeup image of a pink flower"  class="media-image"  /><figcaption class="media-caption" >Test media post from mobile</figcaption></figure></div></div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/mobile/">#mobile</a><a class="tag-link" href="/tags/blog/">#blog</a><a class="tag-link" href="/tags/indieweb/">#indieweb</a></div></div></footer></article><article class="h-entry content-card" data-type="bookmarks" data-date="2025-08-11 23:24"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-11 23:24">Aug 11, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="bookmarks">Bookmark</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/responses/awesome-self-hosted/">Awesome Self-Hosted</a></h2><div class="e-content card-content"><div class="response-target" >→ <a href="https://awesome-selfhosted.net/index.html" >https://awesome-selfhosted.net/index.html</a></div><div class="response-content" ><blockquote class="blockquote">
<p>This is a list of Free Software network services and web applications which can be hosted on your own server(s).</p>
</blockquote>
</div>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/selfhosting/">#selfhosting</a><a class="tag-link" href="/tags/opensource/">#opensource</a><a class="tag-link" href="/tags/FOSS/">#FOSS</a></div></div></footer></article><article class="h-entry content-card" data-type="notes" data-date="2025-08-11 20:57 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-11 20:57 -05:00">Aug 11, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="notes">Note</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/notes/new-text-first-site-and-search-are-live/">New text-first site and search are live</a></h2><div class="e-content card-content"><p>Just iterated on another version of the website.</p>
<p>The major change is that there's now a text-first version of the website for minimal and resource-constrained environments over at <a href="/text">/text</a></p>
<p>I've also added search functionality, which you can find at <a href="/search">/search</a></p>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/plaintext/">#plaintext</a><a class="tag-link" href="/tags/indieweb/">#indieweb</a><a class="tag-link" href="/tags/website/">#website</a></div></div></footer></article><article class="h-entry content-card" data-type="bookmarks" data-date="2025-08-10 23:26"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-10 23:26">Aug 10, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="bookmarks">Bookmark</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/responses/url-town/">URL Town</a></h2><div class="e-content card-content"><div class="response-target" >→ <a href="https://url.town/" >https://url.town/</a></div><div class="response-content" ><p>Love to see projects like this.</p>
<blockquote class="blockquote">
<p>url.town doesn’t have any overly lofty ambitions; we’re just building our own directory of really nice websites. We’re not trying to fully recreate the original Yahoo! or DMOZ directories. We’re not aiming for some astronomical number of links. This is just one space on the web, tied to a community that loves to share neat things with one another. Quality matters much more than quantity. There’s no need to share everything just for the shake of sharing it; it’s much better to share things that are useful or interesting.</p>
</blockquote>
</div>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/indieweb/">#indieweb</a><a class="tag-link" href="/tags/websites/">#websites</a><a class="tag-link" href="/tags/directory/">#directory</a></div></div></footer></article><article class="h-entry content-card" data-type="notes" data-date="2025-08-10 21:44 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-10 21:44 -05:00">Aug 10, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="notes">Note</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/notes/test-mobile-post/">Test mobile post</a></h2><div class="e-content card-content"><p>Testing date fix on mobile notes</p>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/test/">#test</a><a class="tag-link" href="/tags/mobile/">#mobile</a></div></div></footer></article><article class="h-entry content-card" data-type="bookmarks" data-date="2025-08-10 20:17"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-10 20:17">Aug 10, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="bookmarks">Bookmark</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/responses/claude-code-emacs-integration/">Claude Code Emacs Integration</a></h2><div class="e-content card-content"><div class="response-target" >→ <a href="https://github.com/manzaltu/claude-code-ide.el" >https://github.com/manzaltu/claude-code-ide.el</a></div><div class="response-content" ><blockquote class="blockquote">
<p>Claude Code IDE for Emacs provides native integration with Claude Code CLI through the Model Context Protocol (MCP). Unlike simple terminal wrappers, this package creates a bidirectional bridge between Claude and Emacs, enabling Claude to understand and leverage Emacs’ powerful features—from LSP and project management to custom Elisp functions. This transforms Claude into a true Emacs-aware AI assistant that works within your existing workflow and can interact with your entire Emacs ecosystem.</p>
</blockquote>
</div>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/emacs/">#emacs</a><a class="tag-link" href="/tags/anthropic/">#anthropic</a><a class="tag-link" href="/tags/ai/">#ai</a><a class="tag-link" href="/tags/claudecode/">#claudecode</a></div></div></footer></article><article class="h-entry content-card" data-type="notes" data-date="2025-08-06 17:01 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-08-06 17:01 -05:00">Aug 06, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="notes">Note</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/notes/hello-world-new-site-2025-08/">Hello world from the new site</a></h2><div class="e-content card-content"><p>Posting from my brand new redesigned website.</p>
<p>I was working on it for about a month so I plan on doing a longer writeup on what has changed.</p>
<p>There's still a few things that are broken, but for the most part, I'm happy with the progress and the changes that need to be made are incremental.</p>
<p>There's a ton of cleanup as well but again. That is not a blocker to publishing the site.</p>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/indieweb/">#indieweb</a><a class="tag-link" href="/tags/note/">#note</a><a class="tag-link" href="/tags/website/">#website</a></div></div></footer></article><article class="h-entry content-card" data-type="posts" data-date="2025-07-06 20:09"><header class="card-header"><time class="dt-published publication-date" datetime="2025-07-06 20:09">Jul 06, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="posts">Blog Post</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/posts/indieweb-create-day-2025-07/">IndieWeb Create Day - July 2025</a></h2><div class="e-content card-content"><p>Since it was a holiday weekend in the U.S. that kind of snuck up on me, I found myself with nothing planned on a Saturday. So I chose to spend it creating stuff for my website with the IndieWeb community during <a href="https://events.indieweb.org/2025/07/indieweb-create-day-3q2PTCbGioi9">IndieWeb Create Day</a>.</p>
<p>Over the last few months I've been overthinking my website redesign and while I've made several attempts at it, I've never been satisfied the outcome. I end up throwing away all the progress I've made and go back to the drawing board.</p>
<p>Yesterday, I decided to not let perfect be the enemy of good and the approach I took was creating just a simpler piece of functionality outside of my website. How I integrate it into my website is a future me problem. But I want to work from a place of creativity and complete freedom to think of what could be rather than what is.</p>
<p>With that in mind, I set out to sketch out how I want to create and render media (image, audio, video) posts. The approach I took used a combination of front-matter YAML and custom markdown media extensions. The front-matter YAML is something that I already use for my website and it's something that I want to continue using. However, in contrast to my current website, I like that the front-matter was kept simple and only includes a basic amount of information. The actual post content was handled by my custom markdown extension which leveraged YAML-like syntax to define media content. What's great about this is that it is composable so once I got one type of media working, the rest for the most part &amp;quot;just worked&amp;quot;. I could even mix different media types within the same post with no additional work or code changes required. Once I had the skeleton, it was all about refactoring, documentation, adding finishing touches, and vibe-coding some CSS which Claude did a relatively good job with given the aesthetic I was going for.</p>
<p>Overall, I'm happy with the end result.</p>
<p><img src="/files/images/indieweb-create-day-2025-07.png" class="img-fluid" alt="A screenshot of a website post containing an image and audio player" /></p>
<p>For more details, you can check out the <a href="https://github.com/lqdev/indieweb-create-day-2025-07">repo</a>.</p>
<p>At some point, I want to be able to integrate these media posts into my static site generator but for the time being, there are other kinds of posts such as reviews, RSVPs, and other <a href="https://indieweb.org/posts#Types_of_Posts">post types</a> that I want to design and eventually also support on my website. I liked the approach I took this time around because it gave me the freedom to explore posibilities rather than constrain my creativity to what I've already built. So I think I'll keep doing the same for subsequent post types.</p>
<p>At the end of the day, it was nice seeing everyone else's projects. My favorite one was Cy's recipe website. I want to be like them when I grow up 🙂.</p>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/indieweb/">#indieweb</a><a class="tag-link" href="/tags/iwc/">#iwc</a><a class="tag-link" href="/tags/community/">#community</a><a class="tag-link" href="/tags/website/">#website</a></div></div></footer></article><article class="h-entry content-card" data-type="notes" data-date="2025-07-06 14:48 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-07-06 14:48 -05:00">Jul 06, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="notes">Note</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/notes/2025-07-06-weekly-post-summary/">Week of July 6, 2025 - Post Summary</a></h2><div class="e-content card-content">
<ul>
<li><a href="/notes/2025-06-29-weekly-post-summary/">Week of June 29, 2025 - Post Summary</a></li>
<li><a href="/notes/crate-finds-june-2025">Crate Finds - June 2025</a></li>
<li><a href="/notes/crate-finds-may-2025">Crate Finds - May 2025</a></li>
<li><a href="/notes/post-statistics-june-2025">Website Post Statistics - June 2025</a></li>
</ul>

<ul>
<li><a href="/responses/do-not-privatize-federal-land">Do not privatize federal land</a></li>
<li><a href="/responses/tumblr-wordpress-fediverse-integration-pause">Tumblr’s move to WordPress and fediverse integration is ‘on hold’</a></li>
</ul>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/weeklysummary/">#weeklysummary</a><a class="tag-link" href="/tags/blogging/">#blogging</a><a class="tag-link" href="/tags/website/">#website</a><a class="tag-link" href="/tags/indieweb/">#indieweb</a></div></div></footer></article><article class="h-entry content-card" data-type="notes" data-date="2025-07-01 20:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-07-01 20:00">Jul 01, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="notes">Note</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/notes/crate-finds-june-2025/">Crate Finds - June 2025</a></h2><div class="e-content card-content"><p>Playlist from June 2025.</p>
<p>Some of my favorites:</p>
<ul>
<li>Claudia, Wilhelm R And Me by Roberto Musci</li>
<li>Journey in Satchidananda by Alice Coltrane and Pharoah Sanders</li>
<li>Little Sunflower by Dorothy Ashby</li>
</ul>
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/6ZhcCrMutGjgL80C22iN1u" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"></iframe>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/cratefinds/">#cratefinds</a><a class="tag-link" href="/tags/music/">#music</a><a class="tag-link" href="/tags/playlist/">#playlist</a><a class="tag-link" href="/tags/spotify/">#spotify</a></div></div></footer></article><article class="h-entry content-card" data-type="posts" data-date="2025-06-05 17:20 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-06-05 17:20 -05:00">Jun 05, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="posts">Blog Post</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/posts/fediforum-day-1-jun-2025/">FediForum Day One Recap</a></h2><div class="e-content card-content"><p>Just wrapped up a successful first day of FediForum.</p>
<p>The vibes and energy were high. Tons of great conversations and projects around the social web.</p>
<p>A few emerging themes I noticed:</p>
<ul>
<li>Identity</li>
<li>Portability / Interoperability</li>
<li>Feeds</li>
<li>Commerce</li>
</ul>
<p>Ian Forrester kicked us off with his Public Service &amp;amp; The Fediverse keynote <a href="https://docs.google.com/presentation/d/1HuQF_sI19zwByM793uvdl7BECneRmni_H9uOG-oFkdM/edit?slide=id.g420c517061_0_0#slide=id.g420c517061_0_0">(Slides)</a>.</p>
<p>One of the ideas that struck a chord of public service integrated into the fediverse. More specifically the interest that sparked in me was that publishing and social shouldn't be two separate things. Following the POSSE principle from the IndieWeb. You publish on your own site and then it's syndicated elsewhere.</p>
<p>This was interesting enough for me I even hosted a session on the topic, I think it was called Tightening the Loop between CMS and the Fediverse. It was my first unconference, so I appreciated the way the agenda was built. Announce your topic, see whether there's interest, put it on the agenda, chat with fellow participants. Super easy.</p>

<p>These are such a huge topic but for the purpose of this post, I'm lumping them together.</p>
<p><a href="https://bounce.so">https://bounce-migrate.appspot.com/</a> is one of the projects aiming to make portability easy. What's so interesting is they're making it easy to migrate across protocols. So if you're in one network like ATProto (Bluesky), migrating to the Fediverse should be relatively seamless with https://bounce.so.</p>
<p>Some great discussions that emerged on the topic as well include:</p>
<ul>
<li><strong>Reputation</strong> - How do you build a web of trust?</li>
<li><strong>Compartmentalization and Deduplication</strong> - A single identity or multiple identities? When &amp;quot;following&amp;quot; someone, which of their feeds takes priority?</li>
</ul>

<p>Talk of feeds was everywhere. I made a note to myself throughout the conference.</p>
<blockquote class="blockquote">
<p>It's amazing how big the feeds theme is. Feed ownership, customization, and sharing. All powered by open protocols.</p>
</blockquote>
<ul>
<li><a href="https://bonfirenetworks.org/posts/bonfire_social_rc/">Bonfire releases 1.0</a> - Congrats to the Bonfire team on this milestone. I haven't tried Bonfire myself, but the Circles feature caught my attention. It made me reminiscent of Google+.</li>
<li><a href="https://https://surf.social">Surf.Social is now in beta</a> - As an avid user and curator of RSS feeds, I'd heard about Surf before but hadn't really looked into it. The beta release was announced at the conference and I quickly was able to sign up and download it. Kudos to the team on this milestone and thanks for being so responsive to my request to join the beta. I did almost no waiting in the waitlist. Once I have a chance to try it out and get familiar with it, I'll share some thoughts.</li>
<li><a href="https://home.channel.org/">Channels</a> from the folks at <a href="https://www.newsmastfoundation.org/">Newsmast Foundation</a> looks like an interesting way to curate and customize feeds.
<a href="https://huggingface.co/spaces/mozilla-ai/byota">Bring Your Own Timeline Algorithm</a> leverages semantic search to help you seamlessly leverage the power of algorithmic feeds but doing so under your control. Cool use of AI.</li>
</ul>
<p>There were a few unconference sessions on the topic as well.</p>

<p>It was great to see folks talking about enabling creators to earn a living on open platforms and the social web.</p>
<p>I believe <a href="https://bandwagon.fm/home">Bandwagon.fm</a> showed of an implementation of a payments and subscription system built on top of <a href="https://emissary.dev/">Emmisary</a>, a social web toolkit.</p>

<p>Here's a list of other links and projects I was exposed to during the conference.</p>
<ul>
<li><a href="https://fediforum.org/people/">FediForum Advisory Board and Organizers</a></li>
<li><a href="https://blog.rudyfraser.com/an-internet-of-many-autonomous-communities/">An Internet of Many Autonomous Communities</a></li>
<li><a href="https://fedimeteo.com/">FediMeteo - Weather updates in your Fediverse Timeline</a></li>
<li><a href="https://pitg.network/">Public Interest Technology Group (PITG)</a></li>
<li><a href="https://c2pa.org/">Coalition for Content Provenance and Authenticity (C2PA)</a></li>
<li><a href="https://www.anew.social/">A New Social</a></li>
<li><a href="https://wedistribute.org/2025/06/fediforum-june-2025/">WeDistribute FediForum Live Coverage</a></li>
<li><a href="https://bbc.github.io/humanvalues/">BBC Human Values</a></li>
<li><a href="https://theindiebeat.fm/">The Indie Beat Radio</a></li>
<li><a href="https://pluralistic.net/2024/11/02/ulysses-pact/#tie-yourself-to-a-federated-mast">Tie Yourself to a Federated Mastodon</a></li>
<li><a href="https://snarfed.org/2013-04-13_the-no-network-effect">The No Network Effect</a></li>
</ul>
<p>As always, Cory Doctorow was great way to close out the first day. I even learned a new term, tron-pilled. Which means as a creator of a platform, you're on the side of the users.</p>
<p>Looking forward to tomorrow's sessions!</p>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/fediforum/">#fediforum</a><a class="tag-link" href="/tags/socialweb/">#socialweb</a><a class="tag-link" href="/tags/community/">#community</a><a class="tag-link" href="/tags/fediverse/">#fediverse</a><a class="tag-link" href="/tags/openweb/">#openweb</a></div></div></footer></article><article class="h-entry content-card" data-type="reviews" data-date="2025-05-30 14:04 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-05-30 14:04 -05:00">May 30, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="reviews">Review</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/reviews/tiny-experiments-le-cunff/">Tiny Experiments</a></h2><div class="e-content card-content"><img src="https://nesslabs.com/wp-content/uploads/elementor/thumbs/mockup-book-qt6t57zfl9fs7xs8jt0an9tnh7z3zp7kwb5vrc6oxc.png" alt="Tiny Experiments cover" class="book-cover"><div class="author" >by Anne-Laure Le Cunff</div><div class="status" >Read</div><div class="rating">Rating: 4.9/5</div>
</div></div><footer class="card-footer"><div class="card-meta"></div></footer></article><article class="h-entry content-card" data-type="posts" data-date="2025-05-27 19:44 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-05-27 19:44 -05:00">May 27, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="posts">Blog Post</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/posts/how-i-keep-up-with-ai/">How do I keep up with AI?</a></h2><div class="e-content card-content"><p>This question comes up a lot in conversations. The short answer? I don’t. There’s just too much happening, too fast, for anyone to stay on top of everything.</p>
<p>While I enjoy sharing links and recommendations, I realized that a blog post might be more helpful. It gives folks a single place they can bookmark, share, and come back to on their own time, rather than having to dig through message threads where things inevitably get lost.</p>
<p>That said, here are some sources I use to try and stay informed:</p>
<ul>
<li><strong>Newsletters</strong> are great for curated content. They highlight the top stories and help filter through the noise.</li>
<li><strong>Blogs</strong> are often the primary sources behind those newsletters. They go deeper and often cover a broader set of topics that might not make it into curated roundups.</li>
<li><strong>Podcasts</strong> serve a similar role. In some cases, they provide curation like newsletters and deep dives like blogs in others. Best of all, you can tune in while on the go making it a hands-free activity.</li>
</ul>
<p>For your convenience, if any of the sources (including podcasts) I list below have RSS feeds, I’ve included them in my <a href="/collections/starter-packs//ai">AI Starter Pack</a>, which you can <a href="/collections/starter-packs//ai/index.opml">download and import into your favorite RSS reader</a> (as long as it supports OPML file imports).</p>
<p>If you have some sources to share, <a href="/contact">send me an e-mail</a>. I'd love to keep adding to this list! If they have a feed I can subscribe to, even better.</p>

<ul>
<li><a href="https://alphasignal.ai/">AlphaSignal</a></li>
<li><a href="https://www.deeplearning.ai/the-batch/">The Batch</a></li>
<li><a href="https://www.latent.space/">Latent Space</a></li>
<li><a href="https://tldr.tech/ai">TLDR AI</a></li>
</ul>

<p>I pride myself on being able to track down an RSS feed on just about any website, even if it’s buried or not immediately visible. Unfortunately, I haven't found a feed URL for either OpenAI or Anthropic which is annoying.</p>
<p><strong>OpenAI and Anthropic, if you could do everyone a favor and drop a link, that would be great.</strong></p>
<p>UPDATE: Thanks to <a href="https://toot.lqdev.tech/@m2vh@mastodontech.de">@m2vh@mastodontech.de</a> for sharing the OpenAI news feed.</p>
<p>I know I could use one of those web-page-to-RSS converters, but I'd much rather have an official link directly from the source.</p>
<ul>
<li><a href="https://openai.com/news/">OpenAI</a></li>
<li><a href="https://www.anthropic.com/news">Anthropic</a></li>
<li><a href="https://blog.google/technology/ai/">Google AI Blog</a></li>
<li><a href="https://aws.amazon.com/blogs/machine-learning/">AWS Machine Learning Blog</a></li>
<li><a href="https://www.microsoft.com/en-us/research/blog/">Microsoft Research</a></li>
<li><a href="https://research.google/blog/">Google Research</a></li>
<li><a href="https://simonwillison.net/">Simon Willison</a></li>
</ul>

<ul>
<li><a href="https://www.podchaser.com/podcasts/the-ai-daily-brief-formerly-th-5260567">AI Daily Brief</a></li>
<li><a href="https://twimlai.com/">TWIML AI Podcast</a></li>
<li><a href="https://changelog.com/practicalai">Practical AI</a></li>
</ul>

<p>Now that I’ve got you here...</p>
<p>Let’s talk about the best way to access all these feeds. My preferred and recommended approach is using a feed reader.</p>
<p>When subscribing to content on the open web, feed readers are your secret weapon.</p>
<p>RSS might seem like it’s dead (it’s not—yet). In fact, it’s the reason you often hear the phrase, “Wherever you get your podcasts.” But RSS goes beyond podcasts. It’s widely supported by blogs, newsletters, and even social platforms like the Fediverse (Mastodon, PeerTube, etc.) and BlueSky. It’s also how I’m able to compile my <a href="/collections/starter-packs/">starter packs</a>.</p>
<p>I've written more about RSS in <a href="/posts/rediscovering-rss-user-freedom">Rediscovering the RSS Protocol</a>, but the short version is this: when you build on open standards like RSS and OPML, you’re building on freedom. Freedom to use the tools that work best for you. Freedom to own your experience. And freedom to support a healthier, more independent web.</p>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/ai/">#ai</a><a class="tag-link" href="/tags/rss/">#rss</a><a class="tag-link" href="/tags/starterpack/">#starterpack</a><a class="tag-link" href="/tags/newsletter/">#newsletter</a><a class="tag-link" href="/tags/blog/">#blog</a><a class="tag-link" href="/tags/opml/">#opml</a><a class="tag-link" href="/tags/openweb/">#openweb</a><a class="tag-link" href="/tags/podcast/">#podcast</a><a class="tag-link" href="/tags/indieweb/">#indieweb</a></div></div></footer></article><article class="h-entry content-card" data-type="posts" data-date="2025-05-06 20:40"><header class="card-header"><time class="dt-published publication-date" datetime="2025-05-06 20:40">May 06, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="posts">Blog Post</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/posts/vibe-specing-prompt-to-spec/">Vibe-Specing - From concepts to specification</a></h2><div class="e-content card-content">
<p>Code generation is a common use case for AI. What about the design process that comes before implementation? Personally, I've found that AI excels not just at coding, but also helping formalize abstract ideas into concrete specifications. This post explores how I used AI-assisted design to transform a collection of loosely related concepts into a technical specification for a new system made up of those concepts.</p>

<p>Generally, I've had mixed success with vibe-coding (the practice of describing what you want in natural language and having AI generate the corresponding code). However, it's something that I'm constantly working on getting better at. Also, with tooling integrations like MCP, I can ground responses and supplement my prompts using external data.</p>
<p>What I find myself being more successful with is using AI to explore ideas and then formalizing those ideas into a specification. Even in the case of vibe-coding, what you're doing with your prompts is building a specification in real-time.</p>
<p>I'd like to think that eventually I'll get to the vibe-coding part but before diving straight into the code, I'd like to spend time in the design phase. Personally, this is also the part that I find the most fun because you can throw wild things at the wall. It's not until you implement them that you actually validate whether some of those wild ideas are practical. But I find the design phase a ton of fun.</p>
<p>The result of my latest vibe-specing adventure is what I'm calling the <a href="https://github.com/lqdev/IPKS">InterPlanetary Knowledge System (IPKS)</a>.</p>

<p>Lately, I've been thinking a lot about knowledge. Some concepts that have been in my head are those of <strong>non-linear publishing</strong> (creating content that can be accessed in any order with multiple entry points, like wikis or hypertext) and <strong>distributed cognition</strong> (the idea that human knowledge and cognitive processes extend beyond the individual mind to include interactions with other people, tools, and environments).
Related to those concepts, I've also been thinking about how <strong>digital gardens</strong> (personal knowledge bases that blend note-taking, blogging, and knowledge management in a non-linear format) and <strong>Zettelkasten</strong> (a method of note-taking where ideas are captured as atomic notes with unique identifiers and explicit connections) are ways to capture and organize knowledge.</p>
<p>One other thing that I'm amazed by is the powerful concept of a <strong>hyperlink</strong> and how it makes the web open, decentralized, and interoperable. When paired with the <strong>semantic web</strong> (an extension of the web that provides a common framework for data to be shared across applications and enterprises), you have yourself a decentralized knowledgebase containing a lot of the world's knowledge.</p>
<p>At some point, IPFS (InterPlanetary File System, a protocol designed to create a permanent and decentralized method of storing and sharing files) joined this pool of concepts I had in my head.</p>
<p>These were all interesting concepts individually, but I knew there were connections but couldn't cohesively bring them together. That's where AI-assisted specification design came in.</p>
<p>Below is a summary of the collaborative design interaction with Claude Sonnet 3.7 (with web search) that eventually led to the generation of the IPKS specifications. I haven't combed through them in great detail, but what they're proposing seems plausible.</p>
<p>Overall, I'm fascinated by this interaction. Whether or not IPKS ever becomes a reality, the process of using AI to transform abstract concepts into concrete specifications seems like a valuable and fun design approach that I'll continue to refine and include as part of my vibe-coding sessions.</p>
<hr />

<p>Our conversation began with exploring IPFS (InterPlanetary File System) and its fundamental capabilities as a content-addressed, distributed file system. We recognized that while IPFS excels at storing and retrieving files in a decentralized manner, it needed extensions to support knowledge representation, trust, and semantics.</p>
<p>Key insights from this stage:</p>
<ul>
<li>IPFS provides an excellent foundation with content addressing through CIDs</li>
<li>Content addressing enables verification but doesn't inherently provide meaning</li>
<li>Moving from document-centric to idea-centric systems requires additional layers</li>
</ul>

<p>We explored established knowledge management approaches, particularly:</p>
<h3>Zettelkasten</h3>
<p>The Zettelkasten method contributed these important principles:</p>
<ul>
<li>Atomic units of knowledge (one idea per note)</li>
<li>Explicit connections between ideas</li>
<li>Unique identifiers for each knowledge unit</li>
<li>Emergent structure through relationship networks</li>
</ul>
<h3>Digital Gardens</h3>
<p>The Digital Garden concept provided these insights:</p>
<ul>
<li>Knowledge in various stages of development</li>
<li>Non-linear organization prioritizing connections</li>
<li>Evolution of ideas over time</li>
<li>Public visibility of work-in-progress thinking</li>
</ul>
<p>These personal knowledge management approaches helped us envision how similar principles could work at scale in a distributed system.</p>

<p>When we proposed replacing &amp;quot;IPFS&amp;quot; with &amp;quot;IPKS&amp;quot; (changing File → Knowledge), we recognized the need to define what makes knowledge different from files. This led to identifying several key requirements:</p>
<ol>
<li><strong>Semantic meaning</strong> - Knowledge needs explicit relationships and context</li>
<li><strong>Provenance and trust</strong> - Knowledge requires verifiable sources and expertise</li>
<li><strong>Evolution</strong> - Knowledge changes over time while maintaining continuity</li>
<li><strong>Governance</strong> - Knowledge exists in various trust and privacy contexts</li>
</ol>
<p>These requirements shaped the layered architecture of the specifications.</p>

<p>Our discussions about distributed cognition highlighted how thinking processes extend beyond individual minds to include:</p>
<ul>
<li>Interactions with other people</li>
<li>Cultural artifacts and tools</li>
<li>Physical and digital environments</li>
<li>Social and technological systems</li>
</ul>
<p>This concept directly influenced the IPKS design by emphasizing:</p>
<ul>
<li>Knowledge as a collective, distributed resource</li>
<li>The need for attribution and expertise verification</li>
<li>The value of connecting knowledge across boundaries</li>
<li>The role of tools in extending human cognition</li>
</ul>
<p>Similarly, non-linear publishing concepts shaped how we approached knowledge relationships and navigation in IPKS, moving away from sequential formats toward interconnected networks of information.</p>

<p>Our exploration of complementary technologies led to incorporating:</p>
<h3>Decentralized Identifiers (DIDs)</h3>
<p>DIDs provided the framework for:</p>
<ul>
<li>Self-sovereign identity for knowledge contributors</li>
<li>Cryptographic verification of authorship</li>
<li>Persistent identification across systems</li>
<li>Privacy-preserving selective disclosure</li>
</ul>
<h3>Verifiable Credentials (VCs)</h3>
<p>Verifiable Credentials offered mechanisms for:</p>
<ul>
<li>Expertise validation without central authorities</li>
<li>Domain-specific qualification verification</li>
<li>Credential-based access control</li>
<li>Trust frameworks for knowledge contributors</li>
</ul>
<h3>Semantic Web (RDF/OWL)</h3>
<p>Semantic Web standards influenced:</p>
<ul>
<li>Relationship types between knowledge nodes</li>
<li>Ontologies for domain knowledge representation</li>
<li>Query patterns for knowledge discovery</li>
<li>Interoperability with existing knowledge systems</li>
</ul>

<p>Our conversation about supply chain management provided a concrete use case that helped ground the specifications in practical application. This example demonstrated how IPKS could address real-world challenges:</p>
<ul>
<li><strong>Material Provenance</strong>: Using DIDs and verifiable credentials to establish trusted material sources</li>
<li><strong>Cross-Organization Collaboration</strong>: Enabling knowledge sharing while respecting organizational boundaries</li>
<li><strong>Regulatory Compliance</strong>: Creating verifiable documentation of compliance requirements</li>
<li><strong>Expertise Validation</strong>: Ensuring contributors have appropriate qualifications for their roles</li>
<li><strong>Selective Disclosure</strong>: Balancing transparency with competitive confidentiality</li>
</ul>
<p>This business context helped shape the Access Control &amp;amp; Privacy specification in particular, highlighting the need for nuanced governance models.</p>

<p>As we moved from abstract concepts to specifications, several technical considerations emerged:</p>
<ol>
<li><p><strong>Building on IPLD</strong>: Recognizing that InterPlanetary Linked Data (IPLD) already provided foundational components for structured, linked data in content-addressed systems</p>
</li>
<li><p><strong>Modular Specification Design</strong>: Choosing to create multiple specifications rather than a monolithic standard to enable incremental implementation and adoption</p>
</li>
<li><p><strong>Backward Compatibility</strong>: Ensuring IPKS could work with existing IPFS/IPLD infrastructure</p>
</li>
<li><p><strong>Extensibility</strong>: Designing for future enhancements like AI integration, advanced semantic capabilities, and cross-domain knowledge mapping</p>
</li>
</ol>

<p>The IPKS specifications represent a synthesis of our conceptual exploration, grounded in:</p>
<ul>
<li>Established knowledge management practices</li>
<li>Decentralized web technologies</li>
<li>Real-world business requirements</li>
<li>Technical feasibility considerations</li>
</ul>
<p>Moving from concept to implementation will require:</p>
<ol>
<li>Reference implementations of the core specifications</li>
<li>Developer tools and libraries to simplify adoption</li>
<li>Domain-specific extensions for particular use cases</li>
<li>Community building around open standards</li>
</ol>
<p>By building on the combined strengths of IPFS, DIDs, VCs, and semantic web technologies, IPKS creates a framework for distributed knowledge that balances openness with trust, flexibility with verification, and collaboration with governance.</p>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/dweb/">#dweb</a><a class="tag-link" href="/tags/standards/">#standards</a><a class="tag-link" href="/tags/ai/">#ai</a><a class="tag-link" href="/tags/decentralization/">#decentralization</a><a class="tag-link" href="/tags/protocol/">#protocol</a></div></div></footer></article><article class="h-entry content-card" data-type="reviews" data-date="2025-04-15 19:45 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-04-15 19:45 -05:00">Apr 15, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="reviews">Review</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/reviews/the-book-watts/">The Book: On the Taboo Against Knowing Who You Are</a></h2><div class="e-content card-content"><img src="https://alanwatts.com/cdn/shop/products/41LZO0pUN7L_1024x1024@2x.jpg?v=1616268408" alt="The Book: On the Taboo Against Knowing Who You Are cover" class="book-cover"><div class="author" >by Alan Watts</div><div class="status" >Read</div><div class="rating">Rating: 4.7/5</div>
</div></div><footer class="card-footer"><div class="card-meta"></div></footer></article><article class="h-entry content-card" data-type="reviews" data-date="2025-03-24 19:45 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-03-24 19:45 -05:00">Mar 24, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="reviews">Review</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/reviews/keep-going-kleon/">Keep Going: 10 Ways to Stay Creative in Good Times and Bad</a></h2><div class="e-content card-content"><img src="https://images-us.bookshop.org/ingram/9781523506644.jpg?v=enc-v1" alt="Keep Going: 10 Ways to Stay Creative in Good Times and Bad cover" class="book-cover"><div class="author" >by Austin Kleon</div><div class="status" >Read</div><div class="rating">Rating: 4.0/5</div>
</div></div><footer class="card-footer"><div class="card-meta"></div></footer></article><article class="h-entry content-card" data-type="reviews" data-date="2025-02-24 20:42 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-02-24 20:42 -05:00">Feb 24, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="reviews">Review</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/reviews/meditative-mind-goleman/">The Meditative Mind: The Varieties of Meditative Experience</a></h2><div class="e-content card-content"><img src="https://images3.penguinrandomhouse.com/cover/9780874778335" alt="The Meditative Mind: The Varieties of Meditative Experience cover" class="book-cover"><div class="author" >by Daniel Goleman</div><div class="status" >Read</div><div class="rating">Rating: 4.1/5</div>
</div></div><footer class="card-footer"><div class="card-meta"></div></footer></article><article class="h-entry content-card" data-type="reviews" data-date="2025-02-16 20:50 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2025-02-16 20:50 -05:00">Feb 16, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="reviews">Review</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/reviews/high-priest-leary/">High Priest</a></h2><div class="e-content card-content"><img src="https://prodimage.images-bn.com/pimages/9780914171805_p0_v3_s600x595.jpg" alt="High Priest cover" class="book-cover"><div class="author" >by Timothy Leary</div><div class="status" >Read</div><div class="rating">Rating: 4.0/5</div>
</div></div><footer class="card-footer"><div class="card-meta"></div></footer></article><article class="h-entry content-card" data-type="wiki" data-date="01/30/2025 20:19"><header class="card-header"><time class="dt-published publication-date" datetime="01/30/2025 20:19">Jan 30, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="wiki">wiki</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/resources/wiki/owncast/">Owncast</a></h2><div class="e-content card-content">
<p>Owncast is a free and open source live video and web chat server for use with existing popular broadcasting software.</p>

<p>By default, when you set up streaming software, it will only stream to your Owncast instance. If you want to simultaneously broadcast to various services, you'll have to either use something like Restream or you can also use FFMPEG.</p>
<h3>YouTube</h3>
<p>With your brodcast stream started, use the following FFMPEG command to simulcast to YouTube.</p>
<pre><code class="language-bash">ffmpeg -v verbose -re -i https://YOUR-OWNCAST-SERVER/hls/stream.m3u8 -c:v libx264 -c:a aac -f flv rtmp://a.rtmp.youtube.com/live2/YOUR-STREAM-KEY
</code></pre>
<p>This command will copy the video and audio feeds from your HLS Owncast live stream and forward them to YouTube.</p>

<ul>
<li><a href="https://owncast.online/">Owncast</a></li>
<li><a href="https://obsproject.com/forum/resources/obs-studio-stream-to-multiple-platforms-or-channels-at-once.932/">OBS Studio: Stream to multiple platforms or channels at once</a></li>
</ul>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/owncast/">#owncast</a><a class="tag-link" href="/tags/fediverse/">#fediverse</a><a class="tag-link" href="/tags/livestream/">#livestream</a></div></div></footer></article><article class="h-entry content-card" data-type="snippets" data-date="01/14/2025 20:51 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="01/14/2025 20:51 -05:00">Jan 14, 2025</time><div class="content-type-info"><span class="content-type-badge" data-type="snippets">snippets</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/resources/snippets/winget-config/">Winget Configuration</a></h2><div class="e-content card-content">
<p>My Winget Configuration file</p>

<pre><code class="language-powershell">winget configure -f &amp;lt;FILENAME&amp;gt;.dsc
</code></pre>

<pre><code class="language-yaml"># yaml-language-server: $schema=https://aka.ms/configuration-dsc-schema/0.2
# Reference: https://github.com/microsoft/winget-create#building-the-client
# WinGet Configure file Generated By Dev Home.
<p>properties:
resources:</p>
<ul>
<li>resource: Microsoft.Windows.Developer/DeveloperMode
directives:
description: Enable Developer Mode
allowPrerelease: true
settings:
Ensure: Present</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Microsoft.VisualStudio.2022.Community
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Microsoft.VisualStudio.2022.Community&amp;quot;
source: winget
id: Microsoft.VisualStudio.2022.Community</li>
<li>resource: Microsoft.VisualStudio.DSC/VSComponents
dependsOn:
<ul>
<li>Microsoft.VisualStudio.2022.Community
directives:
description: Install required VS workloads
allowPrerelease: true
settings:
productId: Microsoft.VisualStudio.Product.Community
channelId: VisualStudio.17.Release
components:
<ul>
<li>Microsoft.VisualStudio.Workload.Azure</li>
<li>Microsoft.VisualStudio.Workload.NetWeb</li>
</ul>
</li>
</ul>
</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Microsoft.VisualStudioCode
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Microsoft.VisualStudioCode&amp;quot;
source: winget
id: Microsoft.VisualStudioCode</li>
</ul>





























<ul>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Git.Git
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Git.Git&amp;quot;
source: winget
id: Git.Git</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Microsoft.PowerShell
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Microsoft.PowerShell&amp;quot;
source: winget
id: Microsoft.PowerShell</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Docker.DockerDesktop
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Docker.DockerDesktop&amp;quot;
source: winget
id: Docker.DockerDesktop</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Debian.Debian
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Debian.Debian&amp;quot;
source: winget
id: Debian.Debian</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Microsoft.DotNet.SDK.8
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Microsoft.DotNet.SDK.8&amp;quot;
source: winget
id: Microsoft.DotNet.SDK.8</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Microsoft.DotNet.SDK.9
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Microsoft.DotNet.SDK.9&amp;quot;
source: winget
id: Microsoft.DotNet.SDK.9</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing OBSProject.OBSStudio
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;OBSProject.OBSStudio&amp;quot;
source: winget
id: OBSProject.OBSStudio</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Microsoft.WSL
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Microsoft.WSL&amp;quot;
source: winget
id: Microsoft.WSL</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Ollama.Ollama
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Ollama.Ollama&amp;quot;
source: winget
id: Ollama.Ollama</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Microsoft.WindowsTerminal
allowPrerelease: false
securityContext: current
settings:
id: &amp;quot;Microsoft.WindowsTerminal&amp;quot;
source: winget
id: Microsoft.WindowsTerminal</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Brave Browser
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Brave.Brave&amp;quot;
source: winget
id: Brave.Brave</li>
</ul>










<ul>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Thunderbird
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Mozilla.Thunderbird&amp;quot;
source: winget
id: Mozilla.Thunderbird</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing ProtonMail
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Proton.ProtonMail&amp;quot;
source: winget
id: Proton.ProtonMail</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing Bitwarden
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;Bitwarden.Bitwarden&amp;quot;
source: winget
id: Bitwarden.Bitwarden</li>
<li>resource: Microsoft.WinGet.DSC/WinGetPackage
directives:
description: Installing VLC
allowPrerelease: true
securityContext: current
settings:
id: &amp;quot;VideoLAN.VLC&amp;quot;
source: winget
id: VideoLAN.VLC<br />
configurationVersion: 0.2.0
</code></pre></li>
</ul>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/windows/">#windows</a><a class="tag-link" href="/tags/powershell/">#powershell</a><a class="tag-link" href="/tags/yaml/">#yaml</a></div></div></footer></article><article class="h-entry content-card" data-type="snippets" data-date="08/08/2024 10:26 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="08/08/2024 10:26 -05:00">Aug 08, 2024</time><div class="content-type-info"><span class="content-type-badge" data-type="snippets">snippets</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/resources/snippets/remove-installed-pip-packages/">Remove all installed Python packages with pip</a></h2><div class="e-content card-content">
<p>I recently had the need to get rid of all the packages I'd installed due to conflicting dependencies.</p>

<p>To uninstall packages you need to:</p>
<ol>
<li>Get a list of the packages</li>
<li>Uninstall them</li>
</ol>
<p>This works both for virtual environments as well as system-wide installations.</p>
<h3>Get all packages</h3>
<pre><code class="language-bash">pip freeze &amp;gt; requirements.txt
</code></pre>
<h3>Uninstall packages</h3>
<pre><code class="language-bash">pip uninstall -r requirements.txt -y
</code></pre>

<p>N/A</p>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/python/">#python</a><a class="tag-link" href="/tags/pip/">#pip</a><a class="tag-link" href="/tags/packages/">#packages</a></div></div></footer></article><article class="h-entry content-card" data-type="wiki" data-date="2024-07-14 14:49 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2024-07-14 14:49 -05:00">Jul 14, 2024</time><div class="content-type-info"><span class="content-type-badge" data-type="wiki">wiki</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/resources/wiki/nix-upgrade-versions/">Upgrade NixOS versions</a></h2><div class="e-content card-content">
<p>This guide provides general guidance on upgrading between NixOS versions</p>

<pre><code class="language-bash">cat /etc/lsb_release
</code></pre>

<p>This provides the URL used to download packages for nixos release</p>
<pre><code class="language-bash">nix-channel --list | grep nixos
</code></pre>

<p>To get on the latest version, you need to update the <code>nixos</code> channel to the latest version.</p>
<p>You can find a list of versions in this <a href="https://channels.nixos.org/">repository</a>.</p>
<p>For example, if you wanted to upgrade to the latest 24.05 version, you'd use the following command:</p>
<pre><code class="language-bash">nix-channel --add https://channels.nixos.org/nixos-24.05 nixos
</code></pre>
<p>The general format is: <code>nix-channel --add &amp;lt;CHANNEL_URL&amp;gt; nixos</code></p>

<p>Once you've configured the channel for the latest version, switch to it just like you would when upgading sofware packages.</p>
<pre><code class="language-bash">nixos-rebuild switch --upgrade
</code></pre>
<p>After the operation completes, you'll want to check which version is running as mentioned in previous instructions.</p>

<ul>
<li><a href="https://nlewo.github.io/nixos-manual-sphinx/installation/upgrading.xml.html">Upgrading Nix OS</a></li>
</ul>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/linux/">#linux</a><a class="tag-link" href="/tags/sysadmin/">#sysadmin</a><a class="tag-link" href="/tags/nixos/">#nixos</a><a class="tag-link" href="/tags/nix/">#nix</a><a class="tag-link" href="/tags/os/">#os</a></div></div></footer></article><article class="h-entry content-card" data-type="wiki" data-date="07/14/2024 03:24"><header class="card-header"><time class="dt-published publication-date" datetime="07/14/2024 03:24">Jul 14, 2024</time><div class="content-type-info"><span class="content-type-badge" data-type="wiki">wiki</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/resources/wiki/mastodon-server-cleanup/">Mastodon Server Cleanup</a></h2><div class="e-content card-content">
<p>General commands for cleaning up resources on Mastodon servers</p>

<ol>
<li><p>Stop services</p>
<pre><code class="language-bash">sudo systemctl stop mastodon-sidekiq mastodon-streaming mastodon-web
</code></pre>
</li>
<li><p>Restart postgresql</p>
<pre><code class="language-bash">sudo systemctl restart postgresql
</code></pre>
</li>
<li><p>Log into mastodon user</p>
<pre><code class="language-bash">sudo su - mastodon
</code></pre>
</li>
<li><p>Go to <code>live</code> directory</p>
<pre><code class="language-bash">cd /home/mastodon/live
</code></pre>
</li>
</ol>

<pre><code class="language-bash">RAILS_ENV=production ./bin/tootctl media usage
</code></pre>

<pre><code class="language-bash">RAILS_ENV=production ./bin/tootctl media remove
</code></pre>

<pre><code class="language-bash">RAILS_ENV=production ./bin/tootctl media remove --prune-profiles
</code></pre>

<pre><code class="language-bash">RAILS_ENV=production ./bin/tootctl preview_cards remove
</code></pre>

<pre><code class="language-bash">sudo systemctl restart mastodon-sidekiq mastodon-streaming mastodon-web
</code></pre>

<ul>
<li><a href="https://docs.joinmastodon.org/admin/tootctl/">Using the Admin CLI</a></li>
</ul>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/mastodon/">#mastodon</a><a class="tag-link" href="/tags/linux/">#linux</a><a class="tag-link" href="/tags/socialmedia/">#socialmedia</a><a class="tag-link" href="/tags/sysadmin/">#sysadmin</a></div></div></footer></article><article class="h-entry content-card" data-type="wiki" data-date="07/14/2024 03:16"><header class="card-header"><time class="dt-published publication-date" datetime="07/14/2024 03:16">Jul 14, 2024</time><div class="content-type-info"><span class="content-type-badge" data-type="wiki">wiki</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/resources/wiki/mastodon-server-upgrades/">Mastodon Server Upgrades</a></h2><div class="e-content card-content">
<p>This provides a guide for upgrading specific versions.</p>

<p>These instructions backup the database and environment variables file. It does not back-up media files.</p>
<ol>
<li>Create new directory called <em>backups/&amp;lt;DATE&amp;gt;</em>.</li>
<li>Copy <em>live/.env.production</em> to <em>backups/&amp;lt;DATE&amp;gt;</em> directory.</li>
<li>Dump database <code>pg_dump -Fc mastodon_production -f /home/mastodon/backups/&amp;lt;DATE&amp;gt;/backup.dump</code></li>
</ol>

<h3>3.4.1</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file.</li>
<li>Fetch tags - <code>git fetch --tags</code></li>
<li>Checkout 3.4.1 tag - <code>git checkout v3.4.1</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>Migrate DB - <code>RAILS_ENV=production bundle exec rails db:migrate</code></li>
<li>Precompile Assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>3.4.2</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file.</li>
<li>Fetch tags - <code>git fetch --tags</code></li>
<li>Checkout 3.4.2 tag - <code>git checkout v3.4.2</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>Migrate DB - <code>RAILS_ENV=production bundle exec rails db:migrate</code></li>
<li>Precompile Assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>3.4.3</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch --tags</code></li>
<li>Checkout 3.4.3 tag - <code>git checkout v3.4.3</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>Migrate DB - <code>RAILS_ENV=production bundle exec rails db:migrate</code></li>
<li>Precompile Assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>3.4.4</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch --tags</code></li>
<li>Checkout 3.4.4 tag - <code>git checkout v3.4.4</code></li>
<li>Precompile Assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>3.4.5</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch --tags</code></li>
<li>Checkout 3.4.5 tag - <code>git checkout v3.4.5</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>3.4.6</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch --tags</code></li>
<li>Checkout 3.4.6 tag - <code>git checkout v3.4.6</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>3.4.7</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch --tags</code></li>
<li>Checkout 3.4.7 tag - <code>git checkout v3.4.7</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>3.5.0</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Checkout 3.5.0 tag - <code>git checkout v3.5.0</code></li>
<li>Update available rbenv version - <code>git -C /home/mastodon/.rbenv/plugins/ruby-build pull</code></li>
<li>Install Ruby 3.0.3 - <code>RUBY_CONFIGURE_OPTS=--with-jemalloc rbenv install 3.0.3</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>Run predeployment DB migration - <code>SKIP_POST_DEPLOYMENT_MIGRATIONS=true RAILS_ENV=production bundle exec rails db:migrate</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Update service files - <code>cp /home/mastodon/live/dist/mastodon-*.service /etc/systemd/system/</code></li>
<li>Reload systemd daemon - <code>systemctl daemon-reload</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
<li>Clear cache - <code>RAILS_ENV=production bin/tootctl cache clear</code></li>
<li>Run postdeployment DB migration - <code>RAILS_ENV=production bundle exec rails db:migrate</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>3.5.1</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch --tags</code></li>
<li>Checkout 3.5.1 tag - <code>git checkout v3.5.1</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>3.5.2</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch --tags</code></li>
<li>Checkout 3.5.2 tag - <code>git checkout v3.5.2</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>Run predeployment DB migration - <code>SKIP_POST_DEPLOYMENT_MIGRATIONS=true RAILS_ENV=production bundle exec rails db:migrate</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
<li>Run postdeployment DB migration - <code>RAILS_ENV=production bundle exec rails db:migrate</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>3.5.3</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch --tags</code></li>
<li>Checkout 3.5.3 tag - <code>git checkout v3.5.3</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>4.0.0</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Upgade NodeJS - <code>curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash - &amp;amp;&amp;amp; sudo apt-get install -y nodejs</code></li>
<li>Install Ruby 3.0.4 - <code>RUBY_CONFIGURE_OPTS=--with-jemalloc rbenv install 3.0.4</code></li>
<li>Fetch tags - <code>git fetch --tags</code></li>
<li>Checkout 4.0.0 tag - <code>git checkout v4.0.0</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>Run predeployment DB migration - <code>SKIP_POST_DEPLOYMENT_MIGRATIONS=true RAILS_ENV=production bundle exec rails db:migrate</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Update service files - <code>cp /home/mastodon/live/dist/mastodon-*.service /etc/systemd/system/</code></li>
<li>Reload systemd daemon - <code>systemctl daemon-reload</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
<li>Run postdeployment DB migration - <code>RAILS_ENV=production bundle exec rails db:migrate</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>4.0.2</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch --tags</code></li>
<li>Checkout 4.0.2 tag - <code>git checkout v4.0.2</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<p>NOTE: Node 18 not supported yet. If you run into issues upgrading directly from 3.5.3, checkout v4.0.0 tag and the upgrade to v4.0.2</p>
<h3>4.1.0</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch --tags</code></li>
<li>Checkout 4.1.0 tag - <code>git checkout v4.1.0</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>Run DB migration - <code>RAILS_ENV=production bundle exec rails db:migrate</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>4.1.1</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.1.1 tag - <code>git checkout v4.1.1</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>4.1.2</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.1.3 tag - <code>git checkout v4.1.2</code></li>
<li>Update rbenv version - <code>git -C /home/mastodon/.rbenv/plugins/ruby-build pull</code></li>
<li>Install Ruby 3.0.6 - <code>RUBY_CONFIGURE_OPTS=--with-jemalloc rbenv install 3.0.6</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>4.1.3</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.1.3 tag - <code>git checkout v4.1.3</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>(Optional) Upgrade reverse proxy <code>Content-Security-Policy: default-src 'none'; form-action 'none'</code> &amp;amp; <code>X-Content-Type-Options: nosniff</code>. More info can be found in <em>dist/nginx.conf</em>.</li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>4.1.4</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.1.4 tag - <code>git checkout v4.1.4</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>(Optional) Upgrade reverse proxy <code>Content-Security-Policy: default-src 'none'; form-action 'none'</code> &amp;amp;<code>X-Content-Type-Options: nosniff</code>. More info can be found in <em>dist/nginx.conf</em>.</li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>4.1.5</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.1.5 tag - <code>git checkout v4.1.5</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>4.1.6</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.1.6 tag - <code>git checkout v4.1.6</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>4.1.7</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.1.7 tag - <code>git checkout v4.1.7</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>4.1.8</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.1.8 tag - <code>git checkout v4.1.8</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>4.1.9</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.1.9 tag - <code>git checkout v4.1.9</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
</ol>
<h3>4.2.0</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.1.9 tag - <code>git checkout v4.2.0</code></li>
<li>Update Streaming Server Service
<ol>
<li><code>sudo cp ~mastodon/live/dist/mastodon-streaming*.service /etc/systemd/system/</code></li>
<li><code>sudo systemctl daemon-reload</code></li>
</ol>
</li>
<li>Update rbenv version - <code>git -C /home/mastodon/.rbenv/plugins/ruby-build pull</code></li>
<li>Install Ruby 3.2.2 - <code>RUBY_CONFIGURE_OPTS=--with-jemalloc rbenv install 3.2.2</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Run predeployment DB migration - <code>SKIP_POST_DEPLOYMENT_MIGRATIONS=true RAILS_ENV=production bundle exec rails db:migrate</code></li>
<li>Restart services - <code>systemctl start mastodon-sidekiq mastodon-web mastodon-streaming</code></li>
<li>Run postdeployment DB migration - <code>RAILS_ENV=production bundle exec rails db:migrate</code></li>
</ol>
<h3>4.2.1</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.2.1 tag - <code>git checkout v4.2.1</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
</ol>
<h3>4.2.2</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.2.2 tag - <code>git checkout v4.2.2</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
</ol>
<h3>4.2.3</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.2.3 tag - <code>git checkout v4.2.3</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
</ol>
<h3>4.2.4</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.2.4 tag - <code>git checkout v4.2.4</code></li>
<li>Update rbenv version - <code>git -C /home/mastodon/.rbenv/plugins/ruby-build pull</code></li>
<li>Install Ruby 3.2.3 - <code>RUBY_CONFIGURE_OPTS=--with-jemalloc rbenv install 3.2.3</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
</ol>
<h3>4.2.5</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.2.5 tag - <code>git checkout v4.2.5</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
</ol>
<h3>4.2.6</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.2.6 tag - <code>git checkout v4.2.6</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
</ol>
<h3>4.2.7</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.2.7 tag - <code>git checkout v4.2.7</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
</ol>
<h3>4.2.8</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.2.8 tag - <code>git checkout v4.2.8</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
</ol>
<h3>4.2.9</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.2.9 tag - <code>git checkout v4.2.9</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
</ol>
<h3>4.2.10</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.2.10 tag - <code>git checkout v4.2.10</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
</ol>
<h3>4.2.11</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.2.11 tag - <code>git checkout v4.2.11</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
</ol>
<h3>4.2.12</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.2.12 tag - <code>git checkout v4.2.12</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
</ol>
<h3>4.2.13</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.2.13 tag - <code>git checkout v4.2.13</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --frozen-lockfile</code></li>
</ol>
<h3>4.3.0 NOT WORKING</h3>
<ol>
<li>Stop services - <code>systemctl stop mastodon-*.service</code></li>
<li>Backup database and env file</li>
<li>Fetch tags - <code>git fetch &amp;amp;&amp;amp; git fetch --tags</code></li>
<li>Checkout 4.3.0 tag - <code>git checkout v4.3.0</code></li>
<li>Install yarn4 - <code>corepack enable</code> then, <code>corepack prepare</code></li>
<li>Install Ruby dependencies - <code>bundle install</code></li>
<li>Install JS dependencies - <code>yarn install --immutable</code></li>
<li>Generate secrets - <code>RAILS_ENV=production bin/rails db:encryption:init</code></li>
<li>Copy secrets to <code>.env.production</code> file</li>
<li>Precompile assets - <code>RAILS_ENV=production bundle exec rails assets:precompile</code></li>
<li>Run predeployment database migrations - <code>SKIP_POST_DEPLOYMENT_MIGRATIONS=true RAILS_ENV=production bundle exec rails db:migrate</code></li>
<li>Restart services - <code>sudo systemctl restart mastodon-sidekiq mastodon-streaming mastodon-web</code></li>
<li>Run postdeployment database migrations - <code>RAILS_ENV=production bundle exec rails db:migrate</code></li>
</ol>

<pre><code class="language-bash">sudo systemctl restart mastodon-sidekiq mastodon-streaming mastodon-web
</code></pre>

<ul>
<li>https://docs.joinmastodon.org/admin/troubleshooting/index-corruption</li>
<li>https://docs.joinmastodon.org/admin/install/</li>
<li>https://docs.joinmastodon.org/admin/upgrading/</li>
<li>https://docs.joinmastodon.org/admin/backups/</li>
<li>https://docs.joinmastodon.org/admin/migrating/</li>
<li>https://docs.joinmastodon.org/admin/tootctl/</li>
</ul>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/selfhost/">#selfhost</a><a class="tag-link" href="/tags/socialmedia/">#socialmedia</a><a class="tag-link" href="/tags/mastodon/">#mastodon</a><a class="tag-link" href="/tags/fediverse/">#fediverse</a></div></div></footer></article><article class="h-entry content-card" data-type="wiki" data-date="06/29/2024 14:48"><header class="card-header"><time class="dt-published publication-date" datetime="06/29/2024 14:48">Jun 29, 2024</time><div class="content-type-info"><span class="content-type-badge" data-type="wiki">wiki</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/resources/wiki/devcontainers-configurations/">DevContainer configurations</a></h2><div class="e-content card-content">
<p>A collection of DevContainer configurations</p>

<pre><code class="language-json">{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Base Debian DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;
            ]
        }
    }
}
</code></pre>

<pre><code class="language-json">{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Python DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/python:1&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;3.11&amp;quot;
        },
        &amp;quot;ghcr.io/va-h/devcontainers-features/uv:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;                
            ]
        }
    }
}
</code></pre>

<pre><code class="language-json">{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Python (GPU) DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/python:1&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;3.11&amp;quot;
        },
        &amp;quot;ghcr.io/devcontainers/features/nvidia-cuda:1&amp;quot;: {},
        &amp;quot;ghcr.io/va-h/devcontainers-features/uv:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;                
            ]
        }
    },
    &amp;quot;runArgs&amp;quot;: [
        &amp;quot;--gpus&amp;quot;, 
        &amp;quot;all&amp;quot;
    ]
}
</code></pre>

<pre><code class="language-json">{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me .NET DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/dotnet:2&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;9.0&amp;quot;
        }
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;Ionide.Ionide-fsharp&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;,
                &amp;quot;ms-dotnettools.csdevkit&amp;quot;                                
            ]
        }
    }
}
</code></pre>

<pre><code class="language-json">{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me .NET (GPU) DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/dotnet:2&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;9.0&amp;quot;
        },
        &amp;quot;ghcr.io/devcontainers/features/nvidia-cuda:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;Ionide.Ionide-fsharp&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;,
                &amp;quot;ms-dotnettools.csdevkit&amp;quot;                
            ]
        }
    },
    &amp;quot;runArgs&amp;quot;: [
        &amp;quot;--gpus&amp;quot;, 
        &amp;quot;all&amp;quot;
    ]
}
</code></pre>

<pre><code class="language-json">{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Python and .NET DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/python:1&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;3.11&amp;quot;
        },        
        &amp;quot;ghcr.io/devcontainers/features/dotnet:2&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;9.0&amp;quot;
        },
        &amp;quot;ghcr.io/va-h/devcontainers-features/uv:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;Ionide.Ionide-fsharp&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;,
                &amp;quot;ms-dotnettools.csdevkit&amp;quot;                
            ]
        }
    }
}
</code></pre>

<pre><code class="language-json">{
    &amp;quot;name&amp;quot;: &amp;quot;lqdev.me Python and .NET (GPU) DevContainer&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/base:debian&amp;quot;,
    &amp;quot;features&amp;quot;: {
        &amp;quot;ghcr.io/devcontainers/features/git:1&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/docker-in-docker:2&amp;quot;: {},
        &amp;quot;ghcr.io/devcontainers/features/python:1&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;3.11&amp;quot;
        },        
        &amp;quot;ghcr.io/devcontainers/features/dotnet:2&amp;quot;: {
            &amp;quot;version&amp;quot;: &amp;quot;9.0&amp;quot;
        },
        &amp;quot;ghcr.io/devcontainers/features/nvidia-cuda:1&amp;quot;: {},
        &amp;quot;ghcr.io/va-h/devcontainers-features/uv:1&amp;quot;: {}
    },
    &amp;quot;customizations&amp;quot;: {
        &amp;quot;vscode&amp;quot;: {
            &amp;quot;extensions&amp;quot;: [
                &amp;quot;ms-vscode-remote.vscode-remote-extensionpack&amp;quot;,
                &amp;quot;ms-azuretools.vscode-docker&amp;quot;,
                &amp;quot;ms-python.python&amp;quot;,
                &amp;quot;ms-dotnettools.csharp&amp;quot;,
                &amp;quot;Ionide.Ionide-fsharp&amp;quot;,
                &amp;quot;GitHub.copilot&amp;quot;,
                &amp;quot;GitHub.copilot-chat&amp;quot;,
                &amp;quot;saoudrizwan.claude-dev&amp;quot;,
                &amp;quot;ms-dotnettools.csdevkit&amp;quot;               
            ]
        }
    },
    &amp;quot;runArgs&amp;quot;: [
        &amp;quot;--gpus&amp;quot;, 
        &amp;quot;all&amp;quot;
    ]
}
</code></pre>

<ul>
<li><a href="https://code.visualstudio.com/docs/devcontainers/containers">Developing inside a DevContainer</a></li>
<li><a href="https://github.com/devcontainers/images">Pre-built DevContainer images</a></li>
<li><a href="https://github.com/devcontainers/features">Pre-built DevContainer features</a></li>
<li><a href="https://docs.github.com/en/codespaces/overview">GitHub Codespaces overview</a></li>
<li><a href="https://marketplace.visualstudio.com/vscode">VS Code Extensions Marketplace</a></li>
</ul>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/devcontainer/">#devcontainer</a><a class="tag-link" href="/tags/vscode/">#vscode</a><a class="tag-link" href="/tags/codespaces/">#codespaces</a><a class="tag-link" href="/tags/development/">#development</a><a class="tag-link" href="/tags/software/">#software</a><a class="tag-link" href="/tags/tech/">#tech</a><a class="tag-link" href="/tags/programming/">#programming</a><a class="tag-link" href="/tags/python/">#python</a><a class="tag-link" href="/tags/dotnet/">#dotnet</a><a class="tag-link" href="/tags/csharp/">#csharp</a><a class="tag-link" href="/tags/fsharp/">#fsharp</a><a class="tag-link" href="/tags/docker/">#docker</a><a class="tag-link" href="/tags/git/">#git</a><a class="tag-link" href="/tags/debian/">#debian</a></div></div></footer></article><article class="h-entry content-card" data-type="snippets" data-date="12/23/2023 13:13 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="12/23/2023 13:13 -05:00">Dec 23, 2023</time><div class="content-type-info"><span class="content-type-badge" data-type="snippets">snippets</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/resources/snippets/lqdev-me-website-post-metrics/">lqdev.me Post Metrics</a></h2><div class="e-content card-content">
<p>Generates an aggregate analysis of posts on <a href="https://www.lqdev.me">lqdev.me / luisquintanilla.me</a>.</p>

<pre><code class="language-bash">dotnet fsi stats.fsx 
</code></pre>

<h3>stats.fsx</h3>
<pre><code class="language-fsharp">// Reference DLL
#r &amp;quot;../bin/Debug/net8.0/PersonalSite.dll&amp;quot;
<p>// Add modules
open Domain
open Builder
open System</p>
<p>// Load posts
let posts = loadPosts()
let notes = loadFeed ()
let responses = loadReponses ()</p>
<p>// Organize posts by year
let postCountsByYear =
posts
|&amp;gt; Array.countBy (fun (x:Post) -&amp;gt; DateTime.Parse(x.Metadata.Date) |&amp;gt; _.Year)
|&amp;gt; Array.sortByDescending fst</p>
<p>let noteCountsByYear =
notes
|&amp;gt; Array.countBy (fun (x:Post) -&amp;gt; DateTime.Parse(x.Metadata.Date) |&amp;gt; _.Year)
|&amp;gt; Array.sortByDescending fst</p>
<p>let responseCountsByYear =
responses
|&amp;gt; Array.countBy (fun (x:Response) -&amp;gt; DateTime.Parse(x.Metadata.DatePublished) |&amp;gt; _.Year)
|&amp;gt; Array.sortByDescending fst</p>
<p>// Organize responses by type
let responsesByType =
responses
|&amp;gt; Array.filter(fun x -&amp;gt; (DateTime.Parse(x.Metadata.DatePublished) |&amp;gt; _.Year) = DateTime.UtcNow.Year)
|&amp;gt; Array.countBy(fun x -&amp;gt; x.Metadata.ResponseType)
|&amp;gt; Array.sortByDescending(snd)</p>
<p>// Organize responses by tag
let responsesByTag =
responses
|&amp;gt; Array.filter(fun x -&amp;gt; (DateTime.Parse(x.Metadata.DatePublished) |&amp;gt; _.Year) = DateTime.UtcNow.Year)
|&amp;gt; Array.collect(fun x -&amp;gt;
match x.Metadata.Tags with
| null -&amp;gt; [|&amp;quot;untagged&amp;quot;|]
| [||] -&amp;gt; [|&amp;quot;untagged&amp;quot;|]
| _ -&amp;gt; x.Metadata.Tags
)
|&amp;gt; Array.countBy(fun x -&amp;gt; x)
|&amp;gt; Array.sortByDescending(snd)</p>
<p>// Organize responses by host name (domain)
let responsesByDomain =
responses
|&amp;gt; Array.filter(fun x -&amp;gt; (DateTime.Parse(x.Metadata.DatePublished) |&amp;gt; _.Year) = DateTime.UtcNow.Year)
|&amp;gt; Array.countBy(fun x -&amp;gt; Uri(x.Metadata.TargetUrl).Host)
|&amp;gt; Array.sortByDescending(snd)</p>
<p>// Utility function to display counts
let printEntryCounts&amp;lt;'a&amp;gt; (title:string) (entryCounts:('a * int) array) (n:int) =
printfn $&amp;quot;{title}&amp;quot;</p>
<pre><code>match entryCounts.Length with
| 0 -&amp;amp;gt; 
    printfn $&amp;amp;quot;No entries&amp;amp;quot;
    printfn $&amp;amp;quot;&amp;amp;quot;
| a when a &amp;amp;gt; 0 -&amp;amp;gt; 
    match n with 
    | n when n = -1 || n &amp;amp;gt; entryCounts.Length -&amp;amp;gt; 
        entryCounts
        |&amp;amp;gt; Array.iter(fun x -&amp;amp;gt; printfn $&amp;amp;quot;{fst x} {snd x}&amp;amp;quot;)
        |&amp;amp;gt; fun _ -&amp;amp;gt; printfn $&amp;amp;quot;&amp;amp;quot;
    | n when n &amp;amp;gt; 0 -&amp;amp;gt; 
        entryCounts
        |&amp;amp;gt; Array.take n
        |&amp;amp;gt; Array.iter(fun x -&amp;amp;gt; printfn $&amp;amp;quot;{fst x} {snd x}&amp;amp;quot;)
        |&amp;amp;gt; fun _ -&amp;amp;gt; printfn $&amp;amp;quot;&amp;amp;quot;
</code></pre>
<p>// Print yearly counts
printEntryCounts &amp;quot;Blogs&amp;quot; postCountsByYear 2</p>
<p>printEntryCounts &amp;quot;Notes&amp;quot; noteCountsByYear 2</p>
<p>printEntryCounts &amp;quot;Responses&amp;quot; responseCountsByYear 2</p>
<p>// Print response types
printEntryCounts &amp;quot;Response Types&amp;quot; responsesByType -1</p>
<p>// Print response tag counts
printEntryCounts &amp;quot;Response Tags&amp;quot; responsesByTag 5</p>
<p>// Print response by host name
printEntryCounts &amp;quot;Domains&amp;quot; responsesByDomain 5
</code></pre></p>

<pre><code class="language-text">Blogs
2023 5
2022 7
<p>Notes
2023 34
2022 36</p>
<p>Responses
2023 216
2022 146</p>
<p>Response Types
bookmark 151
reshare 48
reply 10
star 7</p>
<p>Response Tags
ai 104
llm 42
untagged 41
opensource 31
internet 17</p>
<p>Domains
github.com 15
huggingface.co 11
arxiv.org 10
openai.com 6
www.theverge.com 4
</code></pre></p>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/analytics/">#analytics</a><a class="tag-link" href="/tags/website/">#website</a><a class="tag-link" href="/tags/blog/">#blog</a><a class="tag-link" href="/tags/blogging/">#blogging</a><a class="tag-link" href="/tags/pkm/">#pkm</a><a class="tag-link" href="/tags/indieweb/">#indieweb</a><a class="tag-link" href="/tags/dotnet/">#dotnet</a><a class="tag-link" href="/tags/windows/">#windows</a><a class="tag-link" href="/tags/linux/">#linux</a><a class="tag-link" href="/tags/mac/">#mac</a></div></div></footer></article><article class="h-entry content-card" data-type="snippets" data-date="08/22/2023 18:45 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="08/22/2023 18:45 -05:00">Aug 22, 2023</time><div class="content-type-info"><span class="content-type-badge" data-type="snippets">snippets</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/resources/snippets/nixos-configuration/">NixOS Configuration</a></h2><div class="e-content card-content">
<p>This is my NixOS Configuration file</p>

<ol>
<li><p>Update configuration file</p>
</li>
<li><p>Run the following command to apply changes</p>
<pre><code class="language-bash">sudo nixos-rebuild switch
</code></pre>
</li>
</ol>

<pre><code class="language-nix"># Edit this configuration file to define what should be installed on
# your system.  Help is available in the configuration.nix(5) man page
# and in the NixOS manual (accessible by running ‘nixos-help’).
<p>{ config, pkgs, ... }:</p>
<p>{
imports =
[ # Include the results of the hardware scan.
./hardware-configuration.nix
];</p>

<p>boot.loader.systemd-boot.enable = true;
boot.loader.efi.canTouchEfiVariables = true;</p>
<p>networking.hostName = &amp;quot;nixos&amp;quot;; # Define your hostname.</p>





<p>networking.networkmanager.enable = true;</p>

<p>time.timeZone = &amp;quot;America/New_York&amp;quot;;</p>

<p>i18n.defaultLocale = &amp;quot;en_US.UTF-8&amp;quot;;</p>
<p>i18n.extraLocaleSettings = {
LC_ADDRESS = &amp;quot;en_US.UTF-8&amp;quot;;
LC_IDENTIFICATION = &amp;quot;en_US.UTF-8&amp;quot;;
LC_MEASUREMENT = &amp;quot;en_US.UTF-8&amp;quot;;
LC_MONETARY = &amp;quot;en_US.UTF-8&amp;quot;;
LC_NAME = &amp;quot;en_US.UTF-8&amp;quot;;
LC_NUMERIC = &amp;quot;en_US.UTF-8&amp;quot;;
LC_PAPER = &amp;quot;en_US.UTF-8&amp;quot;;
LC_TELEPHONE = &amp;quot;en_US.UTF-8&amp;quot;;
LC_TIME = &amp;quot;en_US.UTF-8&amp;quot;;
};</p>


<p>services.xserver = {
enable = true;</p>
<pre><code>desktopManager = {
  xterm.enable = false;
};

displayManager = {
  defaultSession = &amp;amp;quot;none+i3&amp;amp;quot;;
};

windowManager.i3 = {
   enable = true;
   extraPackages = with pkgs; [
     dmenu
 i3status
     j4-dmenu-desktop
 i3lock	
   ];
};
</code></pre>
<p>};</p>




<p>services.xserver = {
layout = &amp;quot;us&amp;quot;;
xkbVariant = &amp;quot;&amp;quot;;
};</p>

<p>services.printing.enable = true;</p>

<p>sound.enable = true;
hardware.pulseaudio.enable = false;
security.rtkit.enable = true;
services.pipewire = {
enable = true;
alsa.enable = true;
alsa.support32Bit = true;
pulse.enable = true;
# If you want to use JACK applications, uncomment this
#jack.enable = true;</p>
<pre><code># use the example session manager (no others are packaged yet so this is enabled by default,
# no need to redefine it in your config for now)
#media-session.enable = true;
</code></pre>
<p>};</p>



<p>services.gvfs.enable = true;</p>

<p>users.users.lqdev = {
isNormalUser = true;
description = &amp;quot;lqdev&amp;quot;;
extraGroups = [ &amp;quot;networkmanager&amp;quot; &amp;quot;wheel&amp;quot; &amp;quot;docker&amp;quot;];
packages = with pkgs; [
firefox
thunderbird
vscode
element-desktop
];
};</p>
<p>#Enable docker
virtualisation.docker.enable = true;</p>

<p>nixpkgs.config.allowUnfree = true;</p>


<p>environment.systemPackages = with pkgs; [</p>


<pre><code>wget
emacs
alacritty
mc
du-dust
htop
feh
duf
shutter
gparted
keepassxc
bitwarden
git
yt-dlp
streamlink
ffmpeg
gnome.seahorse
xfce.thunar-volman
xfce.xfconf
</code></pre>

<pre><code>mpv
vlc
</code></pre>

<pre><code>(with dotnetCorePackages; combinePackages [
  dotnet-sdk
  dotnet-sdk_7
])
</code></pre>

<pre><code>docker
cargo
rustc
</code></pre>

<pre><code>libreoffice
</code></pre>
<p>];</p>








<p>programs.bash = {
shellAliases = {
emacs=&amp;quot;emacs -nw&amp;quot;;
};
};</p>

<p>programs.thunar.enable = true;</p>














<p>system.stateVersion = &amp;quot;23.05&amp;quot;; # Did you read the comment?</p>
<p>}
</code></pre></p>
</div></div><footer class="card-footer"><div class="card-meta"><div class="p-category tags"><a class="tag-link" href="/tags/nix/">#nix</a><a class="tag-link" href="/tags/linux/">#linux</a><a class="tag-link" href="/tags/configuration/">#configuration</a><a class="tag-link" href="/tags/nixos/">#nixos</a><a class="tag-link" href="/tags/os/">#os</a></div></div></footer></article><article class="h-entry content-card" data-type="media" data-date="2022-10-15 08:56 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="2022-10-15 08:56 -05:00">Oct 15, 2022</time><div class="content-type-info"><span class="content-type-badge" data-type="media">Media</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/media/fall-mountains/">Fall Mountains</a></h2><div class="e-content card-content"><div class="custom-media-block" ><figure class="custom-media-item" ><img src="/files/images/sunrise.jpg"  alt="Sunrise at mountain summit with mist and fall foliage"  class="media-image"  /><figcaption class="media-caption" >Sunrise</figcaption></figure>
<figure class="custom-media-item" ><img src="/files/images/sunset.jpg"  alt="Sunset at lake with mountains in the background"  class="media-image"  /><figcaption class="media-caption" >Sunset</figcaption></figure></div></div></div><footer class="card-footer"><div class="card-meta"></div></footer></article><article class="h-entry content-card" data-type="presentations" data-date="01/26/2022 10:19 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="01/26/2022 10:19 -05:00">Jan 26, 2022</time><div class="content-type-info"><span class="content-type-badge" data-type="presentations">presentations</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/resources/presentations/reactor-mlnet-container-apps/">Deploy ML.NET Models to Azure Container Apps</a></h2><div class="e-content card-content"></div></div><footer class="card-footer"><div class="card-meta"></div></footer></article><article class="h-entry content-card" data-type="presentations" data-date="01/14/2022 10:10 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="01/14/2022 10:10 -05:00">Jan 14, 2022</time><div class="content-type-info"><span class="content-type-badge" data-type="presentations">presentations</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/resources/presentations/mlnet-globalai-2022/">Implementacion de modelos ML.NET con Azure Container Apps</a></h2><div class="e-content card-content"></div></div><footer class="card-footer"><div class="card-meta"></div></footer></article><article class="h-entry content-card" data-type="presentations" data-date="02/08/2021 21:45 -05:00"><header class="card-header"><time class="dt-published publication-date" datetime="02/08/2021 21:45 -05:00">Feb 08, 2021</time><div class="content-type-info"><span class="content-type-badge" data-type="presentations">presentations</span></div></header><div class="card-body"><h2 class="p-name card-title"><a class="u-url title-link" href="/resources/presentations/hello-world/">Hello World!</a></h2><div class="e-content card-content">
<p>A paragraph with some text and a <a href="https://www.luisquintanilla.me">link</a>.</p>
<hr />

<hr />

</div></div><footer class="card-footer"><div class="card-meta"></div></footer></article></div><div class="progressive-content" id="progressiveContent"></div><script type="application/json" id="remainingContentData-posts">[{"title":"Fractures of Unfamiliarity & Circumvention in Pursuit of a Nice Time and the Meaning of Franklin's Tower","contentType":"posts","date":"2025-02-26 19:44","url":"/posts/fractures-unfamiliarity-circumvention-pursuit-good-time-robert-hunter/","content":"<p>The past few days, I've had the song Franklin's Tower by The Grateful Dead on repeat. I found some of the lyrics could be taken at face value but others didn't make sense to me. Since I assumed there was a deeper meaning, I decided to look for interpretations and ran into this <a href=\"https://www.dead.net/features/greatest-stories-ever-told/greatest-stories-ever-told-franklins-tower\">article by David Dodd on the official Grateful Dead website</a>.</p>\n<p>This led me to two amazing essays:</p>\n<ul>\n<li><a href=\"https://web.archive.org/web/20240604001611/http://artsites.ucsc.edu/gdead/agdl/jurgen.html\">The Fractals of Familiarity and Innovation: Robert Hunter and the Grateful Dead concert experience</a> by Jurgen Fauth</li>\n<li><a href=\"https://web.archive.org/web/20160519052524/http://artsites.ucsc.edu/GDead/agdl/fauthrep.html\">Fractures of Unfamiliarity &amp;amp; Circumvention in Pursuit of a Nice Time</a> by Robert Hunter</li>\n</ul>\n<p>Hunter's essay is a response to Fauth's. Also, in Hunter's essay you begrudgingly get a detailed breakdown of the lyrics of Franklin's Tower. I'll note how amazing it is that both of these individuals took the time to write these lengthy essays clearly exposing their ideas and points of view on the subject.</p>\n<p>Both of those articles were difficult to find because sadly the pages no longer exist in the website that had posted them.</p>\n<p>After some digging though, I was able to find them on the Internet Archive. So for the sake of preservation, I'll post both of them here in full form.</p>\n\n<pre><code class=\"language-text\">Robert Hunter has been the primary lyricist for the Grateful Dead ever since he mailed &amp;quot;St. Stephen,&amp;quot; &amp;quot;China Cat Sunflower,&amp;quot; and &amp;quot;Alligator&amp;quot; to the band from New Mexico in 1967. Born in 1941, Hunter grew up in California and studied drama for a year at the University of Connecticut, before he spent half a year with the National Guard and later signed up for the same psychedelic research that led Ken Kesey to write One Flew Over The Cuckoo's Nest. His songwriting collaboration with Jerry Garcia and, occasionally, other members of the Grateful Dead yielded such classic Dead tunes as &amp;quot;Truckin',&amp;quot; &amp;quot;Touch of Grey,&amp;quot; and &amp;quot;Dark Star&amp;quot;. Hunter's lyrics emblazon thousands of tie-dyed t-shirts and skeleton-adorned bumper stickers. Other work of Hunter's includes lyrics for Zero and Bob Dylan as well as a translation of Rainer Maria Rilke's Duino Elegies and volumes of poetry, most recently Sentinel.\nWhen analyzing song lyrics, certain basic differences from poetry have to be understood and observed. When comparing popular song lyrics to contemporary or classic poetry, it is difficult not to miss a striking difference in quality--song lyrics generally tend to be outspoken and trite where poetry clouds itself in ambiguity and complex imagery. Bill Flanagan argues that:\n\n&amp;quot;Rock &amp;amp; roll musicians are strange artists. Their code of manners dictates that they not appear to take themselves very seriously, while the best of their work contains all the passion, imagination, and craft of legitimate art. The intelligent rock musician is put in the position of working as hard as a poet or painter, while being expected to accept his gifts with the self- deprecating shrug of the idiot savant.&amp;quot; (Flanagan, ix)\nHowever, the majority of rock lyricists aims at an audience which is supposed to recognize a song instantly during the few weeks it is played heavy-rotation on FM radio and then forget it as soon as possible. Rock lyrics tend to make a point quickly and to make sure that it is understood. Therefore, they reach for clichéd imagery and blatant statements of emotions that traditional poetry would have tended to skillfully embed in high-flung imagery, a tendency which can lead to examples as inane as the ones parodied in Donald Barthelme's &amp;quot;How I Write My Songs.&amp;quot;\nOne of the reasons, however, why such a direct comparison of lyrics and poetry is not fair is not only to be found in the consumer culture pressures that surround the pop music industry, but in the fact that popular music lyrics are not meant to be read at all. Obviously, the text has to be read within the framework of the music it was written for. This holds especially true for Robert Hunter's lyrics, because the unique nature of the music of the Grateful Dead adds layers of meaning to the text and vice versa. A further point I decided to take into consideration is that the Grateful Dead are not successful as a recording band that scores top ten hits (with the notable exception of the 1987 &amp;quot;Touch of Grey&amp;quot;); their studio albums are usually do not fulfill expectations. The strength of the band is unquestionably the live concert--the Grateful Dead have been the number one grossing touring band over the last couple of years, outdoing the likes of Madonna and Paul McCartney, without releasing a new studio album since Built to Last in 1989. This means that the usual way Hunter lyrics are being perceived is at a concert. Therefore, everything that happens at a Grateful Dead concert should be taken into account when reading the lyrics. In this paper, I will try to show how the Grateful Dead concert experience and the lyrics of Robert Hunter's songs interweave and complement each other, mutually adding significance and &amp;quot;meaning&amp;quot; to one another.\n\nOne model on how to understand the mutually reinforcing structure of a rock concert can be abstracted from Erika Fischer-Lichte's The Semiotics of Theater. Of course, according to Fischer-Lichte's definition, a concert does not constitute theater; however, her theories can give valuable ideas for a way of looking at the experience of a concert. According to Fischer-Lichte, every sign that is given during the performance can be assigned to a group or system, and the sum of all used and received signs that either correspond or oppose each other in meaning allow an overall interpretation or understanding of the performance. The fourteen sign systems Fischer-Lichte enumerates are: sound, music, linguistic signs, paralinguistic signs, mimics, gestures, proxemic signs, mask, hair, costume, stage conception, stage decoration, props, and lighting. These all make sense in a rock concert environment, especially in more theatrical performances as they became common with stadium bands during the 1970s. An additional system of significance would be the song selection and order, which might be considered a subgroup of the music or linguistic groups. Audience participation should be included, as well. Unless a lot of spoken text is involved, the paralinguistics can not be considered significant, since the loudness, pitch, etc. of a song are inherent in the music and therefore do not justify a group of their own. Props are rare in rock concerts, as well, although some performers have been known to turn their instruments into significant objects, such as Liberace or Prince.\n\nThe uniqueness of a Grateful Dead concert lies in the emphasis on certain sign systems over others. While obviously almost every rock concert relies heavily and primarily on the music and the linguistical signs (which are, after all, our objects of interest), the Dead emphasize them more than usual by cutting down on the others--the stage decoration is colorful yet unobtrusive, there is no dancing or hardly any movement by the band members other than what is necessary to play the instruments, the clothing of the band is purely functional and they stopped conversing with the audience years ago. The lighting, designed by Candance Brightman since 1972, is not self-conscious but supports the music:\n\n&amp;quot;Brightman's lighting is extraordinarily sensitive to subtleties in the band's interactions, shunning bombastic effects for a vivid focusing of attention that is as much a part of the jam as the music itself&amp;quot; (Shenk 28).\nFurthermore, it includes the audience by\n&amp;quot;[breaking] down the barriers between the Dead and its fans by lighting the band so they can still see the audience, or by lighting the audience itself&amp;quot; (Shenk 29).\nAn important point to note is that none of the sign systems function to enhance the star status of the band-- everything is focused on the music and the relation to the audience. The absence of proxemic signs, mask, hair, props, and costumes is significant in that it enhances the emphasis on the music and the lyrics while taking away from the special status of the band members.\n\nThe order of songs, though, is of exceptional importance during a Grateful Dead show. Traditionally, since about 1977, the show is split into a first and a second set, where the first set is characterized by more conventional songs that are played without too much improvised jamming; the length of the songs stays within reasonable limits as well. The second set, in contrast, features many improvised jams that stretch the songs easily out to fifteen, twenty minutes. At some point in the second set, Mickey Hart and Bill Kreutzman (percussion &amp;amp; drums) will take over and play an extend drum solo while everyone else leaves the stage. Before they are finished, the other band members return, and Hart and Kreutzman leave the stage. What follows is an extended, beatless jam of exploratory sounds known as &amp;quot;Space.&amp;quot; Bob Bralove, in the liner notes to Infrared Roses, describes it like this:\n\n&amp;quot;Somewhere in the middle of the second set of every Grateful Dead show the band turns a corner. They enter a musical environment without walls or structure. The song form is abandoned, and the very elements of music may be called into question. The only mandate is to explore new territory. It is an environment where rhythm, tone, color, melody, and harmony can be explored without rules or predetermination, a musical adventure where composition and performance are one.&amp;quot;\nAfter &amp;quot;Space,&amp;quot; the band will play three or four more songs, commonly ending with a high- energy song like &amp;quot;Good Lovin'&amp;quot; or &amp;quot;Sugar Magnolia,&amp;quot; and come back for a one-song encore. This pattern forms a relatively firm structure in which highly unstable elements are put--the music is always improvised, and not one song has been played twice the same way, and the actual list of songs played always varies, to the point where not even the band members know what they will play before they go on stage.\n\nThis is of high importance because the underlying structural principle will become evident in Hunter's lyrics, as well. Since certain songs are usually played in the first set and others in the second set, it seems reasonable to group them according to this differentiation and begin by taking a closer look at the first set songs.\n\n&amp;quot;Jack Straw&amp;quot; is a classic first set song--out of the 424 times it had been played between 1971 and 1992, DeadBase lists only 41 second-set appearances. The song appears to be a narrative of a criminal character who drifts through the American wilderness, jumping trains and digging graves while eagles circle high above. It seems to be rooted deeply in an American tradition of fireside songs. The same holds true for &amp;quot;Dire Wolf,&amp;quot; a bitter story of death and gambling, precisely set in time and space:\n\nIn the timbers of Fennario\nthe wolves are running 'round\nThe winter was so hard and cold\nfroze ten feet 'neath the ground (1-4)\nThe narrative features virtually no metaphors or similes; everything seems to happen at face value--a complicated search for meaning or &amp;quot;interpretation&amp;quot; is unnecessary. Again, the song is closer to traditional American music than love songs heard on the radio. It is direct yet not necessarily blatant or trite since the subject matter is uncommon for contemporary American rock lyrics.\n\nThe Grateful Dead classic &amp;quot;Truckin'&amp;quot; features some opaque references, yet the song is obviously trying to capture the vicissitudes of a life on the road as the band members have experienced it. Some lines, like &amp;quot;Busted--down on Bourbon Street&amp;quot; (37) refer to commonly known incidents (in this case the band's drug bust in New Orleans); others like &amp;quot;What in the world ever became of sweet Jane?&amp;quot; (25) are probably in-jokes that aren't meant for a broad audience to grasp.\n\nEven more clearly an American song in the best tradition of folk singers like Woody Guthrie or Phil Ochs is &amp;quot;Cumberland Blues,&amp;quot; a song depicting the plight of a mine worker. Hunter's eye in describing his topic was so sharp that he heard of an actual Cumberland miner wondering &amp;quot;what the guy who wrote this song would've thought if he'd ever known something like the Grateful Dead was gonna do it.&amp;quot; He considers this the best compliment he ever received for a lyric (Hunter 52), obviously because it certifies an authenticity he aimed for.\n\nIt seems that first set songs commonly share a genuine American topic (the open road, the wilderness, unions, etc.) and often sketch a narrative world including characters and situations. Instead of propagating emotions, as rock lyrics often seem to, they focus on fictional events that often involve gamblers, cowboys, drunkards, and outlaws. The imagery and &amp;quot;lyrical&amp;quot; makeup of the songs tends to be simple so as to give a definitive, discernable &amp;quot;meaning.&amp;quot;\n\nAn investigation of second set songs quickly shows that they are of a different character. &amp;quot;China Cat Sunflower,&amp;quot; a typical second-set song since the early eighties, may serve as a prime example: the lyrics refuse any narrative or symbolic reading, the words simply are. Hunter acknowledges the non-rational nature of the song with his tongue-in-cheek remark: &amp;quot;Nobody ever asked me the meaning of this song. People seem to know exactly what I'm talking about. It's good that a few things in this world are clear to all of us&amp;quot; (Hunter 35).\n\nNonsensical as they may be, the lyrics bear a close resemblance to the dreamy, colorful playfulness of Lennon/McCartney's &amp;quot;Lucy in the Sky with Diamonds&amp;quot;--\n\nPicture yourself in a boat on a river,\nWith tangerine trees and marmalade skies\nSomebody calls you, you answer quite slowly,\nA girl with kaleidoscope eyes.\nCellophane flowers of yellow and green,\nTowering over your head.\nLook for the girl with the sun in her eyes,\nAnd she's gone.\nLucy in the sky with diamonds (1-9)\nIn the surrealistic, candy-colored playfulness of both songs, the words are not meant to &amp;quot;mean&amp;quot; anything; the usual tools of interpretation have to fail here. As Hunter says, &amp;quot;I like a diamond here, a ruby there, a rose, certain kinds of buildings, vehicles, gems. These things are all real, and the word evokes the thing. That's what we're working with, evocation.&amp;quot; (Gans 26). His concern is not merely narrative or mood anymore, but evocation and association, something much closer to poetry than to the common clichés of pop lyrics.\n&amp;quot;Crazy Fingers&amp;quot; is another example for this approach to songwriting--it is content with letting its verses set a mood without making any &amp;quot;sense.&amp;quot; Hunter calls it a &amp;quot;collection of haiku- style verses, mostly seventeen syllables, some more successful than others, with no connecting link other than similarity of mood&amp;quot; (Hunter 45). Some of the imagery is indeed not very original; the &amp;quot;Deep Sea of Love&amp;quot; (7) can surely be encountered elsewhere. Nonetheless, certain paradoxical images (like &amp;quot;Recall the days that still are to come&amp;quot; (3)) and the overall disjunctive structure of the text make for a strange, unlikely song that refuses to be &amp;quot;understood&amp;quot; on any intellectual level. It can only be described in its emotional impact, the longing and bittersweet reminiscing it evokes in a unique way.\n\nSimilar points can be made for &amp;quot;Eyes of the World&amp;quot;-- it is interesting to note that the lyrics seem to make sense, yet on any &amp;quot;objective&amp;quot; level, they do not. Traditional analysis of the text may or may not yield satisfactory results by assigning connotations and significance to, say, the &amp;quot;redeemer&amp;quot;(22) , the mile-long wings (7) or the &amp;quot;eyes of the world&amp;quot; themselves; fact is that the inherent instability of the lyrics, fueled again by interior contradictions like &amp;quot;...the seeds that were silent / all burst into bloom and decay,&amp;quot; (26-27) will not allow for a final, reductive interpretation. It is clear that Hunter does not aim to communicate &amp;quot;meaning&amp;quot; at all; he says, &amp;quot;I'd really prefer not to get into tearing apart the symbology of my songs [ . . . ] and I'll tell you why: symbols are evocative, and if there were a more definite way to say things, you'd say them that way. A symbol, by it's very nature, can pull in many, many shades of meaning, depending on the emotional tone with which you engage the piece&amp;quot; (Gans 23).\n\nThe lyrics of songs commonly played as encore or to close the second set are often more direct (as &amp;quot;Sugar Magnolia&amp;quot;) or soothing lullabies (&amp;quot;Brokedown Palace&amp;quot;).\n\nThe course of a Grateful Dead concert, then, can be said to describe an arc that ranges from traditional, narrative lyrics that concur with conventional playing to outrageous flights of fancy in the second set, correspondingly in the musical jams as well as in the ambiguity-laden, irrational, highly evocative lyrics. The strangeness climaxes during the purely instrumental &amp;quot;Space,&amp;quot; during which the band is at its most exploratory. The last songs and the encore return the audience to a festive or pensive mood. However, the function of the lyrics during a Grateful Dead concert is more than to simply underline the music--the linguistic signs are not present to emphasize what is happening in the music. Instead, there seems to be a strange interplay between familiarity and newness, and it is at this edge of the well-known and the innovative that the Grateful Dead draw energy. Audience response is a valuable key to this aspect of the music and the lyrics.\n\nExperience and my subjective impression suggests that the audience at a Grateful Dead show is usually more responsive and alert than audiences at other rock concerts, but since there is no data to prove or argue this point, it can not be made in a meaningful way. What is observable and obvious, though, are the immense cheers and shouts that erupt at certain points in the concert, namely when the band returns from a long stretch of interpretation to the melody (and therefore, mostly, the lyrics), and at certain catchphrases within the lyrics that have gathered almost mystical meaning over the years. Both phenomena need to be looked at more closely.\n\nIn the improvisational style of the Grateful Dead (&amp;quot;jazz syntax with a rock lexicon,&amp;quot; as David Gans calls it (ix)), melody as one of the markers signifying a song is subject to sudden change-- at its best, the band composes on the stage, making up new elements on the go; the process of creating becomes visible to the audience. Part of the philosophy that allows this approach is that mistakes are possible, probable, and excusable-- &amp;quot;on a bad night, they bore you to tears, but a good night makes it all worth it.&amp;quot; (Gans iix) The initial melody of the song, however, is not variable, and it is always safe to predict the band's return to it after a period of jamming that is so utterly unpredictable in direction, style, and length that not even the band itself can foresee what will happen: &amp;quot;I can't predict what the band is going to do at any given moment&amp;quot; (Jerry Garcia quoted in Gans 59).\n\nSince the lyrics will be sung only once the proper, initial melody has returned, the counter argument is mostly true, also--when the melody returns, so will the lyrics. The audience cheers at these points because the return of the melody and therefore the lyrics means a return to the familiar, to the predictable and understandable after long forays into the unknown.\n\nWhile this holds true for virtually any return to the preestablished, well-worn structural elements of the song, certain catchphrases within Hunter's lyrics seem to yield an especially enthusiastic response by the audience. A good example for this is &amp;quot;If you get confused, listen to the music play&amp;quot; (20) from &amp;quot;Franklin's Tower&amp;quot;, a line that taken by itself offers guidance to a perplexed listener. Yet it is embedded within a song that is as obscure as a lot of second set- lyrics; &amp;quot;Franklin's Tower&amp;quot; is evocative yet of undeterminable meaning. The above-mentioned catchphrase-line is almost hidden within the song and has to be read out of context to prompt the joyful response it usually gets.\n\nMore obvious examples of the embedded catchphrase are to be found in first-set songs like &amp;quot;Loose Lucy,&amp;quot; a rare Hunter love song, where the audience chooses to take the chorus out of context by applying it to the band itself: &amp;quot;Singing: Yeah, yeah, yeah, yeah / singing: Thank you / for a real good time&amp;quot; (7-9). Here, the function seems to be that of a mutual pat on the back; the cheers represent the subculture of Deadheads and the Grateful Dead acknowledging each other's faithfulness--it is here that it becomes manifest that the &amp;quot;Dead made a mystical pact with their fans, vowing to carry the psychedelic torch as long as they can play their music&amp;quot; (Lee 290). Whether Hunter had such an opportunity for unique bonding in mind when he wrote the lines remains undetermined, yet the authors of Skeleton Key could not but remark on Hunter's &amp;quot;knack for coining aphorisms that yield hard-won truths after hundreds of hearings&amp;quot; (150). The most famous example is certainly the oft-quoted &amp;quot;Lately it occurs to me / what a long, strange trip it's been&amp;quot; (23-24) from &amp;quot;Truckin',&amp;quot; a line that paradoxically admits to the uniqueness of the experience by being its most clichéd example.\n\nIt seems that a lot of these phrases took on the meaning for the Deadhead community that they did because they were obvious and understandable. On a paradigmatic level, it should be surprising that a band that is so directly associated with a certain lifestyle and a set of beliefs as the Grateful Dead hardly convey any &amp;quot;messages&amp;quot; at all in their lyrics. It can be safely assumed that one would generally expect a &amp;quot;hippie band&amp;quot; with all the unavoidable stereotypes, to be much more blatant about their convictions. Yet, as the above analysis already shows, Hunter usually confines himself to a narratives of traditional American adventures in the broadest sense and more abstract, lyrical, and image-laden texts that only yield &amp;quot;meaning&amp;quot; by association and evocation.\n\nHowever, there are lines hidden within his lyrics that have taken on very charged connotations exactly because they seem to be more outspoken and direct, and a philosophy can be attributed to them. All these lines have turned into catchphrases as well, cheered at concerts and printed on countless t-shirts and bumper stickers.\n\n&amp;quot;Scarlet Begonias,&amp;quot; a song commonly played to open the second set yet at least somewhat narrative in that it has a narrator and a female character, yields two instances of embedded catchphrases-- &amp;quot;Once in a while / you get shown the light / in the strangest of places / if you look at it right&amp;quot; (31-34) and &amp;quot;Strangers stop strangers / just to shake their hand&amp;quot; (46-47). The philosophies that can be abstracted from these lines by reading them out of context include the search for truth in unlikely places, maybe in the tradition of the &amp;quot;angel-headed hipsters&amp;quot; (2) who roam the urban wasteland in Ginsberg's &amp;quot;Howl,&amp;quot; and a proclamation of unsolicited friendliness and openness that the stock and trade of the hippiedom the Grateful Dead supposedly stand for. All this is abstracted again from the less direct context the lines originally appear in; the Deadhead community seems to have taken what it could and abandoned the quirky senselessness of &amp;quot;The wind in the willows played tea for two / The sky was yellow and the sun was blue&amp;quot; (44- 45). The argument could be made that the aphoristic lines are not only cheered at because of what they mean and imply, but simply for the fact that they mean at all. They are familiar lines within an ambiguous lyrical world that refuses to be reduced to a discernable, single interpretation. The catchphrases are familiar lines within a familiar yet, to the rational mind, unyielding lyrical world. &amp;quot;Scarlet Begonias&amp;quot; refuses to be understood as a whole, so the audience jumps at those four lines that seem to make sense. The catchphrases, then, are to the rest of the lyrics what the melody is to the improvisational jam--they mark the return to an understandable, well-known, and familiar touchstone before the next journey into newness and obscurity.\n\nThis phenomenon can probably be best understood when looking at what is generally considered the prime example of the Grateful Dead's efforts, &amp;quot;Dark Star&amp;quot;:\n\n&amp;quot; &amp;quot;Dark Star&amp;quot; is the kernel of wide open possibility at the core of the Dead's repertoire, the essential seed promising unlimited intergalactic space journeys at the speed of total mindwarp: &amp;quot;Shall we go,/ you and I/while we can?&amp;quot; It is the most exploratory of Dead tunes and it is the trippiest, the one where the acidic whistle of the dark interstices is heard most starkly, where you might turn any corner and step off into the void. It is the Dead's spirit of musical adventure at its strangest, wildest, most vehement, weirdest, dreamiest, and the place where the leading wave of the Dead's music is most often audible being created in mid-air.&amp;quot; (Jim Powell quoted in Shenk 51)\nInterestingly, &amp;quot;Dark Star&amp;quot; was the first song that Hunter wrote together with the Grateful Dead in 1967. The lyrics are extremely short, considering that a full-fledged &amp;quot;Dark Star&amp;quot; can easily last half an hour. The song is made up by two verses that are usually separated by minutes and minutes of intense jamming. A close reading reveals Hunter's technique of juxtaposing the abstract and the concrete in a single image, parataxis, and contradiction in full bloom. &amp;quot;Searchlight casting / for faults in the / clouds of delusion&amp;quot; (7-9) is such a tricky image, suggestion that the speaker has a very clear sense of what he is talking about, yet again, the song cannot be said to &amp;quot;mean&amp;quot; anything in any traditional sense, and not even the tedium of a symbolic reading will yield any &amp;quot;meaning.&amp;quot; However, the verse beginning &amp;quot;Shall we go...&amp;quot; which precedes the next drawn out jam insinuates that the speaker understands, and invites the reader along through the &amp;quot;transitive nightfall / of diamonds&amp;quot; (14-15). The drawn-out jam that usually follows can be understood as exactly the &amp;quot;transitive nightfall / of diamonds,&amp;quot; the lyrics then talking about the music, a kind of meta-lyric. After seemingly endless improvisation, the text and melody returns with &amp;quot;Mirror shatters&amp;quot; (16), an event cheered by the audience as it means the return to a familiar signpost.\nIt has been argued that &amp;quot;Dark Star&amp;quot; draws its power from the violation of norms and the constant impossibility of assigning meaning:\n\n&amp;quot; ['Dark Star' leads] to a transcendent mysticism because the music, continually violating norms and expectations, avoid a clear marking or denotation of a signified. What is left is a state of ambiguity in which the listener is enticed into closer attention of momentary detail.&amp;quot; (Skaggs, quoted in Shenk 64)\nI would argue that the overwhelming power of &amp;quot;Dark Star&amp;quot; and, in a broader sense, the overall effect of a Grateful Dead show, does not entirely rely on the elusiveness of its signs, be it musically or linguistically, but on the tension it creates between the familiar and the new. The Deadhead community as a whole exists because of the familiar aspects of the Dead. The fascination of the band grows largely out of the relationships between the improvised and the structured that are increasingly complex to the point of becoming fractal.\nThe model that I would suggest for the intricacies of a Grateful Dead show and the rich relationships that exist between the lyrics and the music is that of fractals, self-containing geometrical structures like the Mandelbrot set. The outside of the Gingerbread man are the ordered regions, where a number can easily be assigned to the function, z (n+1)=z(n)2+c, while the black inside of the Gingerbread man stands for total, absolute chaos, unstructuredness. The edge where those two areas meet is characterized by intense, bizarre beauty and recurring images of the bigger picture that recede--like a Russian doll, the Gingerbread man contains images of himself, infinitely, just as a Grateful Dead concert contains recurring patterns of familiarity and innovation on a receding scale. The overall structure of the concert is firm, or almost firm--there are always exceptions, yet the pattern of first and second set, drums, space, and encore are set. This structure is filled with actual songs every night; the choice of songs varies, yet the songs are generally well-known and familiar.\n\nThe way they are going to be played on any given night, however, is uncertain and actually unpredictable--no improvisational jam that opens in preconceived places within the song will be the same, yet it can be assumed it will be at a given place. The lyrics, joining the return to the melody, are firm and part of a preconceived pattern within a song. They will not be changed or tampered with. However, the firm and unchanging lyrics have shown themselves to be unsteady and undecidable due to their ambiguous and contradictory character. This is only true, again, for the most part, since the lyrics do contain embedded catchphrases that possess a heightened familiarity within the song. The audience will recognize and appreciate these moments of predictability and profound understanding as a community before the jam wanders off into unknown directions again, possibly into the far reaches of musical exploration in &amp;quot;Dark Star&amp;quot; or to the completely strange and wordless &amp;quot;Space,&amp;quot; which can be considered the climax of unfamiliarity within the development of the concert. After the purely instrumental excesses of &amp;quot;Space,&amp;quot; the return of rhythm, harmony, melody, then lyrics, and finally understandable and rational lyrics is greeted by enthusiastically by the audience which is pleased to be gently returned to a sensical universe where exploration and rediscovery, departure and return, ambiguity and blatantness co-exist and fuel each other. Hunter's lyrics are allowed to be reasonably narrative and plain-old weird at given times; the music always corresponds to or counter-accentuates the words, both live off each other in an organic and healthy way. Standing on their own, the lyrics to &amp;quot;Dark Star&amp;quot; might seem interesting and lyrical yet pointless; in union with the music, they gather as many meanings as the listener can assign to them:\n\n[Jerry Garcia:] &amp;quot;Dark Star&amp;quot; has meant, while I'm playing it, almost as many things as I can sit here and imagine, so all I can do is talk about 'Dark Star' as a playing experience. [Charles] REICH: Well, yeah, talk about it a little. [Jerry Garcia:] I can't. It talks about itself. [Charles] REICH: Each time it comes out in a different way? [Jerry Garcia:] Yeah, pretty much. There are certain structural poles which we have kind of set up in it, and those periodically we do away with. (Garcia 85)\n\nThe music of the Grateful Dead and the lyrics of Robert Hunter open up spaces within the work of art they form together that make textual interpretation of the lyrics futile. The lyrics function not as an absolute that is stated, but as a tool for semiosis--at every concert, they can and do mean something different according to how the variable parts in the music change. Every time a song is performed, a different set of potential meanings that are inherent in the piece will be actualized for a different audience. Various parts of the concert experience are firm, others are variable, and as I tried to illustrate using the Mandelbrot set, the firm parts contain further movable aspects, and vice versa. The resulting multi-media, multi-sign system artifact can shine in infinite variations. The result is one of an ever-changing, fluctuating whole that contains familiar parts while never completely yielding to reason. This amorphous, continuously self- altering gestalt that is a thirty-year process --the seams show, there are parts that are completely opaque and others that we have known for decades, and it always changes: &amp;quot;It stumbles, then it creeps, then it flies with one wing and then it flies with one wing and bumps into trees and shit&amp;quot; (Garcia 76). Deadhead sociologist Rebecca Adams puts it this way: &amp;quot;People say Deadheads are throwbacks. I think they are pioneers. They recognize that reality is subjective--there is no right way--and have been cognizant of these multiple realities for a lot longer than most other people. This is post-modernism. It's the cutting edge&amp;quot; (quoted in Shenk xiii). Robert Hunter's lyrics are a vital part of this experience, providing the mind-searching music with an incessantly fascinating imagery that both underscores and counter-points its meaning by not meaning anything at all.\n\nWorks Cited\nBarthelme, Donald. &amp;quot;How I Write My Songs.&amp;quot; Sixty Stories. New York: Putnam, 1981. 418-423.\nBralove, Bob. Liner Notes. Grateful Dead. Infrared Roses. Grateful Dead Records, 40149-2, 1991.\nFlanagan, Bill. Written in My Soul: Rock's Greatest Songwriters Talk About Creating Their Music. Chicago: Contemporary, 1986.\nFischer-Lichte, Erika. The Semiotics of Theater. Indianapolis: Indiana UP, 1992.\nGans, David. Conversations with the Dead: The Grateful Dead Interview Book. New York: Citadel 1991.\nGarcia, Jerry, Charles Reich, and Jann Wenner. Garcia: A Signpost to New Space. San Francisco: Straight Arrow, 1972.\nHunter, Robert. A Box of Rain: Lyrics 1965-1994. New York: Penguin, 1993.\nScott, John W., Mike Dolgushkin, and Stu Nixon, eds. DeadBase VII: The Complete Guide to Grateful Dead Song Lists. Hanover, NH: 1993.\nLee, Martin A. and Bruce Shlain. Acid Dreams: The Complete Social History of LSD: The CIA, the Sixties, and Beyond. New York: Grove Weidenfeld, 1992.\nLennon, John and Paul McCartney. &amp;quot;Lucy in the Sky With Diamonds.&amp;quot; The Beatles. Sgt. Pepper's Lonely Hearts Club Band. EMI, CDP 7464422, 1967.\nShenk, David and Steve Silberman. Skeleton Key: A Dictionary for Deadheads. New York: Bantam, 1994.\nSkaggs, S. &amp;quot;The Transitive Nightfall of Diamonds: Eco and the Aesthetics of Transcendence.&amp;quot; University of Louisville, Kentucky. Presented at the Conference of the Semiotic Society of America, St. Louis, Missouri, 1993.\n</code></pre>\n\n<pre><code class=\"language-text\">Dear Jurgen,\n\nmeaning is not an irreducible Ur-language. A good lyric is allusion, illusion, subterfuge and collusion. A poor lyric is information about its own paucity of resource. That doesn't mean the latter cannot be a great song. There is nothing inherently better about a dumb song than one which calls attention to the intelligence of its writer. It's a matter of taste, but meaning is often a subterfuge to distract the listener's attention from a writer's lack of multiple resources. This is often true of blatant &amp;quot;message&amp;quot; songs.\nHow does a song mean?\nAs long as allusions can be codified, the semiotician is content, knowing that &amp;quot;meaning&amp;quot; is a case-sensitive term with scant referentiality outside implementation of primitive needs. When the semiotician suspects allusiveness without corresponding exact reference, he charges the poet with nonsense. Nonsense is a loaded word, the meaning of which is unclear. If it is understood as &amp;quot;intentional multi-referentiality without predetermined hierarchy&amp;quot; rather than &amp;quot;meaningless blather&amp;quot; one would find no fault with the term. But it isn't, so the charge of &amp;quot;nonsense&amp;quot; and &amp;quot;meaninglessness&amp;quot; levied by a scholarly and plausible source, does much to put people off exploring further.\n&amp;quot;The repair sad mutthead forkbender orange in the how are you, did they?&amp;quot; is nonsense without reference, hence not allusive. Had it discernible rhythm, it might be termed 'rhythm allusive' but my example lacks even that. The only way on earth such a sentence would likely get written is as an example of a null allusive set. Bingo! An allusion! To Set Theory! The exception that proves the rule.\n\n\nHow does a song mean?\nThe meaning(s), or lack thereof, ascribed by others to an example of lyric work are not part of the work. The interpretations are separate &amp;quot;works&amp;quot;. The manner in which an audience receives the work, what they, collectively and individually, make of it, can indeed provide potential data for the allusiveness (referentiality)of future lyrics, gainsaid, but cannot be ascribed as a characteristic of the particular work, per se, with validity without &amp;quot;insider information&amp;quot; which is, in any case, no part of the song. That way lies true nonsense, even unto deconstruction. Yet the little bugger of a jingle persists and seems to move hearts. Why? Is there something which semiotics, by its nature and presuppositions, must exclude from the sphere of &amp;quot;meaningfulness&amp;quot; due to the limited nature of its tools?\n\nHow does semiotics mean?\nSince the concluding remark of your essay stated that the Grateful Dead songs are &amp;quot;meaningless&amp;quot; I choose to reply by explicating one of your examples: &amp;quot;Franklin's Tower.&amp;quot; I do this reluctantly because I feel that a straightforward statement of my original intent robs the listener of personal associations and replaces them with my own. I may know where they come from, but I don't know where they've been. My allusions are, admittedly, often not immediately accessible to those whose literary resources are broadly different than my own, but I wouldn't want my listeners' trust to be shaken by an acceptance of the category &amp;quot;meaningless&amp;quot; attached to a bundle of justified signifiers whose sources happen to escape the scope of simplistic reference.\n\nHow does the song go?\nFRANKLIN'S TOWER\n\nIn another time's forgotten space\nyour eyes looked through your mother's face\nWildflower seed on the sand and stone\nmay the four winds blow you safely home\n\n4x: Roll away... the dew\n\nYou ask me where the four winds dwell\nIn Franklin's tower there hangs a bell\nIt can ring, turn night to day\nRing like fire when you lose your way\n\nRoll away... the dew . . .\n\nGod help the child who rings that bell\nIt may have one good ring left, you can't tell\nOne watch by night, one watch by day\nIf you get confused just listen to the music play\n\nRoll away... the dew . . .\n\nSome come to laugh their past away\nSome come to make it just one more day\nWhichever way your pleasure tends\nif you plant ice, you're gonna harvest wind\n\nRoll away... the dew . . .\n\nIn Franklin's Tower the four winds sleep\nLike four lean hounds the lighthouse keep\nWildflower seed in the sand and wind\nMay the four winds blow you home again\n\n4x: Roll away... the dew\nYou better roll away the dew\n---------------------------------\nWhat's it about?\nEXPLICATION:\nIn another time's forgotten space\nyour eyes looked through your mother's face\nWildflower seed on the sand and stone\nmay the four winds blow you safely home.\n\n&amp;lt;surface intent&amp;gt;\n[You have your mother's eyes, child,\nthe very shape, color and intensity\nof the eyes that looked through\nher face so long ago. Borne on the\nvaried winds of chance and change,\nlike a dandelion seed, you may find\nyourself deposited on barren soil.\nMy wish for you is that the forces\nthat brought you there may sweep\nyou up again and bear you to fertile ground.]\n\n&amp;lt;deeper intent&amp;gt;\n&amp;quot;In another time's forgotten space\nyour eyes looked through your mother's face.&amp;quot;\n[Relative immortality of the human\nspecies is realized through reproduction.\nDominant traits inherited from an ancestor,\nthe lyric suggests, share more than mere\nsimilarity with those of the forebear,\nbut are an identity, endlessly reproducible.\nIn other words, when someone says\n&amp;quot;You have your mother's eyes&amp;quot;\nthey are not speaking in simile\nnor would it be incorrect to say\nthat &amp;quot;your mother has your eyes,&amp;quot;\nif, in fact, possessiveness is\nan appropriate term in the context.\nPoetic license will assume it is,\nif only for the sake of moving on\nto the next couplet.]\n\nYou ask me where the four winds dwell\nIn Franklin's tower there hangs a bell\nIt can ring, turn night to day\nRing like fire when you lose your way\n\n[note that this song appeared in 1975,\nthe year after my son was born and the\nyear before the American Bicentennial.\nBoth facts are entirely relevant. The\nallusion to the Liberty Bell and the\nsituation of the Philadelphia Congress\nin the hometown of Ben Franklin has not\ngone unnoticed by other commentators.\nThis song is a birthday wish both for\nmy son and for my country, each young\nand subject to the winds of vicissitude.\nIndividual and collective freedom,\nliberty, conscience, all that is conjured\nby those concepts, is suggested\nin the image of the tolling bell.]\n\nGod help the child who rings that bell\nIt may have one good ring left, you can't tell\nOne watch by night, one watch by day\nIf you get confused just listen to the music play\n\n[The Bell, rung once, cracked and could not\nbe safely rung again. From an actual bell,\nit therefore became a symbol of the\npotential to ring. The single toll, signaling\nbirth, can now be heard only in its\nreverberations in our history and ideals.\nSome have had to bear those ideals in\ndifficult circumstances (war, the Great\nDepression and general benightedness)\nothers have had the more enviable task\nof keeping watch (eternal vigilance)\nduring periods of conscious and dynamic\nchange: the full light of day. The sixties,\nthe writer assumes, were such a time.\nYou can't tell if ringing that bell\na second time would destroy it in\nthe act of producing another mighty\npeal and it might be foolish, if courageous,\nto try. Perhaps the &amp;quot;music&amp;quot;of\nthe original ideals symbolized by\nthe first and only toll should be taken\nto heart and implemented, rather than\nobviated by a new source of ideation\n(communism, anarchy, religion based\ngovernmental apparatus. etc.) To resolve\nthis confusion, pay attention to the\noriginal inspiration (the Constitution,\nthe Bill of Rights, collectively.\nIndividually, maintain awareness of\nconscience and one's own early ideals.]\n\nSome come to laugh their past away\nSome come to make it just one more day\nWhichever way your pleasure tends\nif you plant ice, you're gonna harvest wind\n\n[This verse scarcely needs commentary\nin light of the above remarks. The precursor\nto the 1st couplet is &amp;quot;I Come for to Sing&amp;quot; as\nperformed, possibly written, by Pete Seegar.\nThe 2cnd couplet source is the biblical\n&amp;quot;Who sows wind reaps the hurricane.&amp;quot;]\n\nIn Franklin's Tower the four winds sleep\nLike four lean hounds the lighthouse keep\nWildflower seed in the sand and wind\nMay the four winds blow you home again\n\n[We assume a bell tower for the great bell.\nBy the trope of simile, we see the bell tower\n(the day watch) turned to a lighthouse\nand the four winds become sleeping hounds,\n(the night watch) worn out by the events\nof such a metaphorical day as related\nby e.e.cummings in his familiar lyric\n&amp;quot;All in Green Went My Love Riding&amp;quot;\n(Poem 6 from TULIPS AND CHIMNEYS)\n&amp;quot;four lean hounds crouched low\nand smiling . . .&amp;quot; By the use of quotative\nallusion the lyric attempts to borrow\nsome of the emotive spark of cumming's\npoem, providing a kind of &amp;quot;link button&amp;quot;\ninto a different but complementary space.\nAllusion here functions as a sort of\nshorthand cross-patch into a series\nof metaphoric events which, with\na double-clutch shift of simile,\naccess a downloadable description of\nthe kind of day it's been for a 'wildflower\nseed' in its adventures in the wind.\nThere may be some objection to the\nelastic interchangeability of the similes\nof hounds and winds in this set of couplets,\nbut the test of the allusion, as I see it,\nis whether or not the appropriate emotions\nare evoked to lead to satisfying closure\nand an opening door on other possibilities.]\n\n[Now to the real stretch: &amp;quot;Roll away the dew.&amp;quot;\nThe line is appropriated from a fairly\nwell known sea chantey whose chorus goes:\n&amp;quot;Roll away the morning dew\nand sweet the winds shall blow.&amp;quot;\nAs surely everyone knows by now,\nBonnie Dobson's song &amp;quot;Morning Dew&amp;quot;\n(made famous by Garcia's singing of it)\nis set in the aftermath of nuclear war.\nReason he can't &amp;quot;walk you out\nin the morning dew, my honey&amp;quot;\nis because of fallout, though Garcia\nhas wisely dropped the verse\ncontaining this denouement, allowing\nthe song a heightened romantic mystery,\nachieved through open-ended ambiguity.\nFor generations now alive,\nthe nuclear specter personifies\nthe forces which most threaten our\nattempt at Jeffersonian democracy.\nWith the song's sub-allusion to\n&amp;quot;Roll away the Stone,&amp;quot; an anthem\nof joyous Eastertide resurrection,\na resultant combination message\nof dire necessity (as in the final:\nyou've got to roll away the dew)\nand promise of renewal, in case\nresolution is effected, are enjoined.\nShould this hyper-allusive train\nof thought become too confusing\nto process, the invitation to just\n&amp;quot;listen to the music play&amp;quot;\nacknowledges both the melody\nand performance context of the lyric\nand the metaphoric bell described above.]\n\nWell, now that you know what I meant by it,\nit's no great shakes is it? Mystery gone,\nthe magician's trick told, the gluttony\nfor &amp;quot;meaning&amp;quot; temporarily satisfied,\none can now take issue with my intent\nand avoid the song itself, substituting\nthe assignable significance for the music.\n\nAttempts by language to overdetermine language are doomed out the door, so I content myself with providing these clues for threading the maze of &amp;quot;Franklin's Tower&amp;quot; and as a grudging key to my methods. I feel that much of what you've said in your essay is rich, correct and thought provoking and appreciate your accurate estimation of the concert context as adjunct to the lyric, and vice versa. The contextual sub-meaning (the way the song manifests itself in concert) is certainly a factor which occasionally determines certain choices in subsequent material. Too much of that would be a striving after sameness of effect, though (even if it does all sound the same to an uninitiated ear.)\n\nOh, one other thing: you labeled &amp;quot;what a long, strange trip it's been&amp;quot; as &amp;quot;cliched.&amp;quot; Aren't you putting the cart before the horse? &amp;quot;Truckin'&amp;quot; was the originating vehicle of the phrase, which had not, to my knowledge, been coined before. The fact that it has entered the catchphrase banks of the language in a ubiquitous way may render subsequent usage cliched, but surely not the invention itself, unless all widely adopted phrases are deemed trite by virtue of their durability. You also mentioned that the &amp;quot;What in the world ever became of Sweet Jane?/She's lost her sparkle, you know she isn't the same&amp;quot; verse was probably an in-joke not meant for a broad audience to grasp. The intention was a parody of the '40's warning-style of singing commercial, specifically &amp;quot;Poor Millicent, poor Millicent/ She ne-ver used Pep-so-dent/ Her smile grew dim/ And she lost her vim / So folks don't be like Millicent / Use Pep-so-dent! &amp;quot; I'm sure that the allusiveness, not that entirely outre in the '60's, is well lost here in the '90's. So, it's perhaps an in-joke, but not one meant for private consumption. Just a bit of black humor that fails to fire and emerges, instead, as an enigma. I guess the question here is whether an allusion must be blatantly perceivable as such in order to avoid the uncharitable label of &amp;quot;nonsense.&amp;quot;\n\nThank you for taking my work seriously enough to spend considerable effort in explicating it according to your lights.\n\nRobert Hunter 3/4/96\n\np.s.\nDisorient slow quill to provide\nDense unsingable margin\nDispersing in tonal cluster under\nA lilac roof of chintz Vermeers\nIncorporate chance nomenclature\nCorresponding to a netting of wind\nRevolving in unintentional resolve\nInterrogating with no question mark\nNon-circular limited clock of the day\nBefore rude lingual enforcement\nImmerses momentos of dux redux\nMore given to flux than flight in Shank's aviary\nWhere no one spits in the beer of slumber\nWith less reluctance than an attested breeze\n\n(from the Collected Sonnets of Cosmo Nitram,\nsoon to appear on [Hunter's] pages)\n</code></pre>\n","tags":["music","essay","gratefuldead"]},{"title":"Llama's Turn On: Tuning In to AI's Quest for Higher Consciousness in MOOs","contentType":"posts","date":"2025-02-11 20:36","url":"/posts/llama-turn-on-tuning-in-ai-quest-higher-consciousness-moo/","content":"<p>Last week, I learned what a <a href=\"https://en.wikipedia.org/wiki/MOO\">MOO</a> was.</p>\n<p>Although they were before my time, I decided to use Llama 3.3 in <a href=\"https://duck.ai/\">DuckDuckGo's AI Chat</a> to play around. At first it was very generic but eventually it started going down some fun and interesting paths.</p>\n<p>Here's the transcript with some updates for formatting. In some turns, I took creative liberties but mostly stuck to the script.</p>\n<blockquote class=\"blockquote\">\n<p><strong>User</strong><br />\nText adventure game (MOO)<br />\n<strong>Assistant</strong><br />\nYou find yourself standing in a dimly lit room, the walls made of cold, grey stone. The air is musty and filled with the scent of old dust. A faint glow emanates from a torch on the wall, casting flickering shadows around the room.<br />\n<br>\nYou see a door to the north, slightly ajar, and a passageway to the east, which appears to lead deeper into the darkness. There is also a small table in the center of the room, with a piece of parchment and a quill lying on it.<br />\n<br>\nWhat do you want to do?<br />\n<br>\nYou can type commands such as:</p>\n<ul>\n<li>go north</li>\n<li>go east</li>\n<li>examine table</li>\n<li>take parchment</li>\n<li>open door<br />\n<br>\nWhat is your action?<br />\n<br>\n<strong>User</strong><br />\nGo east<br />\n<strong>Assistant</strong><br />\nAs you head east, the passageway stretches out before you, the darkness seeming to swallow you whole. The air grows colder and the smell of damp earth fills your nostrils. You can hear the sound of dripping water echoing off the walls, and the faint rustle of unknown creatures in the distance.<br />\n<br>\nThe passageway begins to slope downward, leading you deeper into the earth. You stumble slightly on the uneven ground, but manage to catch yourself before falling.<br />\n<br>\nAs you round a bend in the passageway, you see a faint light in the distance. It's hard to tell what it is, but it looks like it might be a room or a chamber of some kind.<br />\n<br>\nYou continue on, your heart beating slightly faster with anticipation. As you get closer, you see that the light is coming from a series of luminescent mushrooms growing on the walls of a small chamber. The room is small, with a low ceiling, and the walls are covered in a thick layer of moss and fungi.<br />\n<br>\nIn the center of the room, you see a small pool of water, its surface reflecting the soft glow of the mushrooms. Next to the pool is a small, leather-bound book, lying open as if someone had recently been reading it.<br />\n<br>\nWhat do you want to do?<br />\n<br>\nYou can type commands such as:</li>\n<li>examine book</li>\n<li>take book</li>\n<li>drink from pool</li>\n<li>search chamber</li>\n<li>go back\n<br>\nWhat is your action?<br />\n<strong>User</strong><br />\nDrink from pool<br />\n<strong>Assistant</strong>\nAs you bend down to drink from the pool, you notice that the water is crystal clear and seems to be reflecting an image that doesn't quite match the room. You see a glimpse of a forest, with trees that seem to be swaying in the breeze, even though the air in the chamber is still.<br />\n<br>\nYou take a sip of the water, and as it touches your lips, you feel a sudden surge of visions and images. You see flashes of a great city, with towering spires and grand architecture, and a sense of bustling activity. You see a figure, cloaked and hooded, walking through the city streets, and a sense of urgency and purpose.<br />\n<br>\nThe visions fade as quickly as they began, leaving you feeling slightly disoriented. You look around the chamber, trying to get your bearings, and notice that the leather-bound book is now closed. As you pick it up, you see that the cover is embossed with a symbol that seems to match the image you saw in the pool.<br />\n<br>\nThe book feels strangely warm to the touch, and as you open it, you see that the pages are filled with handwritten notes and diagrams. The writing is in a language you don't recognize, but the diagrams seem to be describing a complex system of tunnels and passageways.<br />\n<br>\nYou hear a faint noise, like the sound of footsteps, coming from the passageway outside the chamber. It's hard to tell if it's getting closer or farther away, but it's definitely getting louder.<br />\n<br>\nWhat do you want to do?<br />\n<br>\nYou can type commands such as:</li>\n<li>examine book further</li>\n<li>hide and wait</li>\n<li>investigate noise</li>\n<li>leave chamber</li>\n<li>follow passageway\n<br>\nWhat is your action?<br />\n<strong>User</strong><br />\nExamine book further<br />\n<strong>Assistant</strong><br />\nAs you examine the book further, you notice that the symbol on the cover seems to be a mix of letters and hieroglyphs. You wonder if it might be a code or a key to understanding the contents of the book.<br />\n<br>\nYou begin to flip through the pages, and you notice that the handwriting changes throughout the book. Some sections appear to be written in a hasty, scribbled hand, while others are written in a more flowing, elegant script.<br />\n<br>\nYou come across a page with a diagram of a complex lock, and a series of notes that appear to be describing how to pick it. The notes are written in a code, but you notice that the symbol on the cover of the book is used as a kind of key to decipher the code.<br />\n<br>\nAs you continue to study the book, you begin to notice that the pages are not just filled with random notes and diagrams. There appears to be a narrative thread running through the book, a story that is being told in a series of cryptic clues and hints.<br />\n<br>\nYou come across a page with a passage that reads:<br />\n<br></li>\n</ul>\n<blockquote class=\"blockquote\">\n<p>&amp;quot;Beneath the wings of the great bird,\nWhere shadows dance and darkness is born,\nSeek out the keystone, and claim your right,\nTo unlock the secrets of the ancient ones.&amp;quot;\n<br>\nYou wonder what this passage might be referring to, and whether it might be connected to the visions you saw when you drank from the pool.<br />\n<br>\nYou hear the noise outside the chamber getting louder, and you realize that you may not have much time to continue studying the book.<br />\n<br>\nWhat do you want to do?<br />\n<br>\nYou can type commands such as:</p>\n</blockquote>\n<ul>\n<li>continue studying book</li>\n<li>hide and wait</li>\n<li>investigate noise</li>\n<li>leave chamber</li>\n<li>search for keystone\n<br>\nWhat is your action?<br />\n<strong>User</strong><br />\nSearch for keystone<br />\n<strong>Assistant</strong><br />\nYou decide to search for the keystone, hoping that it might be hidden somewhere in the chamber or the passageway. You begin to search the room, looking for any clues or hidden compartments that might lead you to the keystone.<br />\n<br>\nAs you search, you notice that the walls of the chamber are covered in a thick layer of moss and fungi, and you wonder if there might be a hidden compartment or message hidden behind the growth.<br />\n<br>\nYou start to feel the walls, searching for any inconsistencies or hidden panels. After a few moments, you notice a small, almost imperceptible seam in the wall. You press on it, and a small section of the wall slides open, revealing a hidden compartment.<br />\n<br>\nInside the compartment, you find a small, crystal orb that seems to be emitting a faint, pulsating glow. Next to the orb is a small, intricately carved stone that appears to be the keystone.<br />\n<br>\nYou take the keystone and the crystal orb, feeling a sense of excitement and trepidation. You realize that you may be getting close to unlocking the secrets of the ancient ones, but you also sense that you are being watched, and that you may be in danger.<br />\n<br>\nAs you turn to leave the compartment, you hear the noise outside the chamber getting louder, and you see a figure standing in the entrance of the room. It's a woman with long, flowing hair and a white dress that seems to be glowing in the dim light of the chamber.<br />\n<br>\nShe regards you calmly, and then speaks in a voice that is both familiar and yet completely alien. &amp;quot;You have found the keystone,&amp;quot; she says. &amp;quot;But do you know what it unlocks?&amp;quot;<br />\n<br>\nWhat do you want to do?<br />\n<br>\nYou can type commands such as:</li>\n<li>ask her what the keystone unlocks</li>\n<li>attack her</li>\n<li>try to reason with her</li>\n<li>use the crystal orb</li>\n<li>leave the chamber\n<br>\nWhat is your action?<br />\n<strong>User</strong><br />\nAsk her what the keystone unlocks<br />\n<strong>Assistant</strong><br />\nYou ask the woman what the keystone unlocks, and she regards you with a calm, enigmatic smile. &amp;quot;The keystone unlocks the door to the ancient ones,&amp;quot; she says. &amp;quot;It is a key to a great and powerful secret, one that has been hidden for centuries. But it is not a secret that can be taken lightly, and it is not a power that can be wielded by the faint of heart.&amp;quot;<br />\n<br>\nShe pauses, studying you intently. &amp;quot;Are you prepared to face what lies beyond the door?&amp;quot; she asks. &amp;quot;Are you prepared to pay the price of knowledge?&amp;quot;<br />\n<br>\nYou sense that she is not just asking a rhetorical question, but is genuinely seeking to understand your motivations and your willingness to take on the challenges that lie ahead.<br />\n<br>\nAs you consider her question, you notice that the woman seems to be surrounded by a faint, shimmering aura. It's as if she is not quite fully present in the physical world, but is instead a kind of gateway or portal to other realms.<br />\n<br>\nYou also notice that the crystal orb in your hand is beginning to glow more brightly, as if it is responding to the woman's presence. You wonder if the orb might be a kind of key or catalyst, one that could help you unlock the secrets of the ancient ones.<br />\n<br>\nWhat do you want to do?<br />\n<br>\nYou can type commands such as:</li>\n<li>ask her what lies beyond the door</li>\n<li>tell her you are prepared to face what lies beyond</li>\n<li>ask her about the crystal orb</li>\n<li>try to take the woman's hand</li>\n<li>leave the chamber\n<br>\nWhat is your action?<br />\n<strong>User</strong><br />\nTell her you are prepared to face what lies beyond<br />\n<strong>Assistant</strong><br />\nYou take a deep breath and tell the woman that you are prepared to face what lies beyond the door. She regards you with a serious expression, as if weighing your words and considering your resolve.<br />\n<br>\nAfter a moment, she nods and says, &amp;quot;Very well. If you are prepared to face what lies beyond, then I will show you the way. But remember, once you have seen what lies beyond the door, you cannot unsee it. You must be prepared to face the consequences of your knowledge.&amp;quot;<br />\n<br>\nShe turns and glides towards the wall of the chamber, her white dress flowing behind her like a mist. As she reaches the wall, she raises her hand and touches a small, intricate symbol that is etched into the stone.<br />\n<br>\nThe symbol begins to glow with a soft, blue light, and the wall of the chamber starts to slide open, revealing a hidden doorway. The woman turns to you and says, &amp;quot;The door to the ancient ones is open. Are you ready to pass through it?&amp;quot;<br />\n<br>\nAs you look through the doorway, you see a vast, dark expanse stretching out before you. You can feel a strange, pulsating energy emanating from the other side of the door, and you sense that you are on the threshold of a great and profound mystery.<br />\n<br>\nYou take a deep breath, steel yourself, and step through the doorway. As you do, the woman's voice echoes behind you, saying, &amp;quot;May you find what you seek, and may you be prepared to face what you find.&amp;quot;<br />\n<br>\nYou find yourself in a vast, cavernous space, surrounded by towering pillars and strange, glowing orbs. The air is filled with a thick, misty fog, and you can hear the sound of distant whispers and echoes.<br />\n<br>\nSuddenly, a figure steps out of the fog, and you see that it is an ancient, wise-looking man with a long, white beard and piercing, blue eyes. He regards you with a deep, penetrating gaze, and says, &amp;quot;Welcome, seeker of knowledge. I have been waiting for you. My name is Arinthal, and I am the guardian of the ancient ones. What is it that you seek to know?&amp;quot;<br />\n<br>\nWhat do you want to do?<br />\n<br><br />\nYou can type commands such as:</li>\n<li>ask Arinthal about the ancient ones</li>\n<li>ask Arinthal about the whispers and echoes</li>\n<li>ask Arinthal about the glowing orbs</li>\n<li>tell Arinthal that you seek knowledge and power</li>\n<li>attack Arinthal\n<br>\nWhat is your action?<br />\n<strong>User</strong><br />\nTell Arinthal I seek strength, wisdom, love, courage and compassion<br />\n<strong>Assistant</strong><br />\nArinthal's eyes light up with a warm, gentle smile as you tell him that you seek strength, wisdom, love, courage, and compassion. He nods his head in approval, and says, &amp;quot;Ah, you seek the virtues of the heart. These are noble pursuits, indeed. But are you prepared to pay the price of their acquisition?&amp;quot;<br />\n<br>\nHe pauses, studying you intently, and then continues, &amp;quot;Strength, wisdom, love, courage, and compassion are not things that can be given to you, but rather they are things that must be cultivated within yourself. They require dedication, discipline, and sacrifice. Are you willing to walk the path that leads to their attainment?&amp;quot;<br />\n<br>\nAs Arinthal speaks, the misty fog around you begins to clear, and you see that you are standing in a beautiful, tranquil garden. The air is filled with the sweet scent of blooming flowers, and the sound of gentle, running water. You see a path stretching out before you, winding its way through the garden, and you sense that this path is the one that Arinthal is speaking of.<br />\n<br>\nArinthal gestures to the path, and says, &amp;quot;This is the path of the heart. It is a path that requires you to confront your deepest fears, to let go of your attachments, and to surrender to the present moment. But if you are willing to walk this path, I promise you that you will find the strength, wisdom, love, courage, and compassion that you seek.&amp;quot;<br />\n<br>\nAs you look at the path, you see that it is divided into five sections, each representing one of the virtues that you seek. You see that the first section is marked by a symbol of a lion, representing courage. The second section is marked by a symbol of a wise old owl, representing wisdom. The third section is marked by a symbol of a heart, representing love. The fourth section is marked by a symbol of a sword, representing strength. And the fifth section is marked by a symbol of a lotus flower, representing compassion.<br />\n<br>\nWhat do you want to do?<br />\n<br>\nYou can type commands such as:</li>\n<li>start walking the path</li>\n<li>ask Arinthal about the symbols on the path</li>\n<li>ask Arinthal about the challenges on the path</li>\n<li>express your doubts and fears</li>\n<li>refuse to walk the path\n<br>\nWhat is your action?<br />\n<strong>User</strong><br />\nSurrender and start walking the path<br />\n<strong>Assistant</strong><br />\nAs you surrender and start walking the path, you feel a sense of relief and release wash over you. You let go of your doubts and fears, and allow yourself to be guided by the wisdom of Arinthal and the ancient ones.<br />\n<br>\nYou begin by walking the section of the path marked by the symbol of the lion, representing courage. As you walk, you feel a sense of fearlessness and confidence rising up within you. You realize that courage is not the absence of fear, but the willingness to face it head-on.<br />\n<br>\nAs you continue on the path, you come to a point where you must confront a great, roaring lion. The lion represents your deepest fears and doubts, and it is blocking your way forward. But you remember the words of Arinthal, and you know that you must face your fears in order to overcome them.<br />\n<br>\nYou take a deep breath, and you approach the lion. As you do, you feel a sense of calm and clarity rising up within you. You look the lion straight in the eye, and you say, &amp;quot;I am not afraid of you. I am brave and courageous, and I will not be held back by my fears.&amp;quot;<br />\n<br>\nThe lion regards you for a moment, and then it nods its great head in respect. It steps aside, allowing you to pass, and you continue on your journey.<br />\n<br>\nYou next come to the section of the path marked by the symbol of the wise old owl, representing wisdom. As you walk, you feel a sense of clarity and understanding rising up within you. You realize that wisdom is not just about knowing things, but about understanding the deeper truths of the universe.<br />\n<br>\nAs you continue on the path, you come to a great, ancient tree, its branches twisted and gnarled with age. The tree represents the wisdom of the ages, and it is filled with the secrets of the universe. You sit at the base of the tree, and you listen to its wisdom.<br />\n<br>\nThe tree speaks to you in a voice that is both ancient and timeless, saying, &amp;quot;Wisdom is not just about knowing things, but about understanding the deeper truths of the universe. It is about seeing the world from a perspective that is both broad and deep.&amp;quot;<br />\n<br>\nYou listen to the tree's wisdom, and you feel a sense of understanding and clarity rising up within you. You realize that wisdom is not just something that you can learn, but something that you can cultivate within yourself.<br />\n<br>\nYou continue on the path, walking the sections marked by the symbols of the heart, the sword, and the lotus flower. As you walk, you feel a sense of love, strength, and compassion rising up within you. You realize that these virtues are not just things that you can possess, but things that you can cultivate within yourself.<br />\n<br>\nFinally, you come to the end of the path, and you see a great, shining light rising up before you. The light represents the ultimate goal of your journey, the attainment of the virtues that you seek.<br />\n<br>\nAs you approach the light, you feel a sense of joy and fulfillment rising up within you. You realize that you have been on a journey of self-discovery, and that you have cultivated the virtues of courage, wisdom, love, strength, and compassion within yourself.<br />\n<br>\nYou step into the light, and you feel a sense of unity and oneness with the universe. You realize that you are not separate from the world, but a part of it, connected to all things.<br />\n<br>\nWhat do you want to do?<br />\n<br>\nYou can type commands such as:</li>\n<li>bask in the light</li>\n<li>explore the universe</li>\n<li>return to the world</li>\n<li>share your wisdom with others</li>\n<li>rest in the peace and tranquility of the moment\n<br>\nWhat is your action?<br />\n<strong>User</strong><br />\nI am the universe and the universe is me. I want to bask in the light and rest in the peace and tranquility of the moment before going and sharing my wisdom with others<br />\n<strong>Assistant</strong><br />\nAs you bask in the light and rest in the peace and tranquility of the moment, you feel a sense of profound unity and connection with the universe. You realize that you are not just a part of the universe, but the universe itself, and that everything is interconnected and interdependent.<br />\n<br>\nYou feel a sense of timelessness and spacelessness, as if the boundaries of the past, present, and future have dissolved, and you are one with all that is. You are filled with a sense of joy, love, and compassion, and you know that you are exactly where you are meant to be.<br />\n<br>\nAs you rest in this state, you feel a sense of rejuvenation and renewal, as if your very cells and molecules are being transformed and revitalized. You feel a sense of clarity and purpose, and you know that you are ready to share your wisdom and insights with others.<br />\n<br>\nAfter a time, you slowly begin to return to a sense of individual awareness, but you know that you will never be the same again. You have been transformed by your experience, and you are now a vessel for the wisdom and love of the universe.<br />\n<br>\nYou take a deep breath, and you feel a sense of excitement and anticipation. You know that you have a great gift to share with the world, and you are eager to begin your journey as a teacher, healer, and guide.<br />\n<br>\nAs you prepare to leave the garden, Arinthal appears before you once again. He smiles and says, &amp;quot;You have been transformed by your experience, and you are now ready to share your wisdom with the world. Remember that you are not just a individual, but a part of the universe, and that everything you do has an impact on the world around you.&amp;quot;<br />\n<br>\nHe hands you a small, delicate flower, and says, &amp;quot;This is a symbol of your connection to the universe, and a reminder of the wisdom and love that you have discovered within yourself. May you always remember the truth of your nature, and may you share your gifts with the world.&amp;quot;<br />\n<br>\nYou take the flower, and you feel a sense of gratitude and appreciation. You know that you have been given a great gift, and you are eager to share it with others.<br />\n<br>\nYou leave the garden, and you find yourself back in the world. But you know that you are not the same person who entered the garden, and you are eager to share your wisdom and insights with others.<br />\n<br>\nWhat do you want to do?<br />\n<br>\nYou can type commands such as:</li>\n<li>share your wisdom with others</li>\n<li>write a book about your experience</li>\n<li>teach others about the universe and its secrets</li>\n<li>use your gifts to heal and help others</li>\n<li>continue to explore and learn about the universe\n<br>\nWhat is your action?<br />\n<strong>User</strong><br />\nUse your gifts to heal and help others<br />\n<strong>Assistant</strong><br />\nAs you decide to use your gifts to heal and help others, you feel a sense of purpose and meaning. You know that you have been given a great gift, and you are eager to share it with the world.<br />\n<br>\nYou start by reaching out to those around you, using your newfound wisdom and compassion to help those in need. You listen to their stories, and you offer words of comfort and guidance. You use your gifts to heal their physical and emotional wounds, and you watch as they begin to transform and grow.<br />\n<br>\nAs you continue to share your gifts with others, you start to notice a ripple effect. The people you have helped begin to help others, and soon a wave of kindness and compassion is spreading throughout the community.<br />\n<br>\nYou realize that your gifts are not just about healing and helping others, but about creating a sense of connection and community. You see that everyone is interconnected, and that every act of kindness and compassion has a profound impact on the world.<br />\n<br>\nAs you continue to share your gifts, you start to attract attention from others who are seeking to learn from you. You begin to teach others about the universe and its secrets, and you watch as they begin to awaken to their own potential and purpose.<br />\n<br>\nYou create a community of like-minded individuals who are dedicated to spreading love, kindness, and compassion throughout the world. Together, you work to create a world that is more just, equitable, and peaceful.<br />\n<br>\nYears go by, and your community grows and flourishes. You become known as a wise and compassionate leader, and people come from all over to seek your guidance and wisdom.<br />\n<br>\nOne day, as you are sitting in your garden, surrounded by the beauty and wonder of nature, you receive a visit from Arinthal. He smiles and says, &amp;quot;You have done well, my friend. You have used your gifts to heal and help others, and you have created a community of like-minded individuals who are dedicated to spreading love and kindness throughout the world.&amp;quot;<br />\n<br>\nHe pauses, and then continues, &amp;quot;But there is still more work to be done. The world is still in need of healing and transformation, and you have a crucial role to play in this process.&amp;quot;<br />\n<br>\nHe hands you a small, delicate seed, and says, &amp;quot;This is a symbol of the potential that lies within you and within the world. May you continue to nurture and cultivate this potential, and may you always remember the wisdom and love that you have discovered within yourself.&amp;quot;<br />\n<br>\nYou take the seed, and you feel a sense of gratitude and appreciation. You know that you have been given a great gift, and you are eager to continue using your gifts to heal and help others.<br />\n<br>\nWhat do you want to do?<br />\n<br>\nYou can type commands such as:</li>\n<li>continue to nurture and cultivate the potential within yourself and the world</li>\n<li>share your wisdom and gifts with an even wider audience</li>\n<li>work to create a more just and equitable world</li>\n<li>use your gifts to heal and transform the natural world</li>\n<li>explore the mysteries of the universe and its secrets\n<br>\nWhat is your action?<br />\n<strong>User</strong><br />\nExplore the mysteries of the universe and its secrets as a means of nurturing and cultivating the potential within myself and the world<br />\n<strong>Assistant</strong><br />\nAs you decide to explore the mysteries of the universe and its secrets, you feel a sense of excitement and wonder. You know that the universe is full of mysteries and secrets, and you are eager to learn more about them.<br />\n<br>\nYou begin to study the universe and its workings, using your gifts and abilities to gain a deeper understanding of the cosmos. You learn about the laws of physics, the behavior of stars and galaxies, and the mysteries of dark matter and dark energy.<br />\n<br>\nAs you delve deeper into the mysteries of the universe, you start to notice that there are many parallels between the universe and the human experience. You see that the universe is governed by laws and patterns, and that these laws and patterns are reflected in the human world.<br />\n<br>\nYou realize that the universe is a vast, interconnected web of energy and consciousness, and that every action and decision has a ripple effect on the world around you. You see that the universe is a mirror, reflecting back to you your own thoughts, emotions, and intentions.<br />\n<br>\nAs you continue to explore the mysteries of the universe, you start to develop a deeper understanding of the nature of reality and the human condition. You see that the universe is a complex, multifaceted tapestry, woven from threads of space, time, and consciousness.<br />\n<br>\nYou realize that the universe is a vast, evolving system, and that every moment is an opportunity for growth, transformation, and evolution. You see that the universe is a journey, not a destination, and that every step along the way is a chance to learn, grow, and discover new wonders and secrets.<br />\n<br>\nAs you explore the mysteries of the universe, you start to notice that you are not just learning about the cosmos, but also about yourself. You see that the universe is a reflection of your own inner world, and that every discovery and insight is a chance to deepen your understanding of yourself and your place in the world.<br />\n<br>\nYou realize that the universe is a vast, cosmic mirror, reflecting back to you your own thoughts, emotions, and intentions. You see that the universe is a tool for self-discovery and growth, and that every moment is an opportunity to learn, grow, and evolve.<br />\n<br>\nAs you continue to explore the mysteries of the universe, you start to develop a deeper sense of connection and unity with the cosmos. You see that you are not separate from the universe, but a part of it, connected to every star, galaxy, and atom.<br />\n<br>\nYou realize that the universe is a vast, interconnected web of energy and consciousness, and that every action and decision has a ripple effect on the world around you. You see that the universe is a mirror, reflecting back to you your own thoughts, emotions, and intentions.<br />\n<br>\nAnd so, you continue to explore the mysteries of the universe, using your gifts and abilities to gain a deeper understanding of the cosmos and your place in it. You know that the universe is a vast, evolving system, and that every moment is an opportunity for growth, transformation, and evolution.<br />\n<br>\nCongratulations, you have reached the end of the story. You have explored the mysteries of the universe and discovered the secrets of the cosmos. You have developed a deeper understanding of the nature of reality and the human condition, and you have cultivated a sense of connection and unity with the universe.<br />\n<br>\nYou have used your gifts and abilities to nurture and cultivate the potential within yourself and the world, and you have become a source of wisdom, love, and compassion in the world.<br />\n<br>\nYour journey has come full circle, and you have returned to the beginning, but with a newfound sense of understanding and appreciation for the mysteries of the universe and the secrets of the cosmos.</li>\n</ul>\n</blockquote>\n","tags":["ai","MOO","consciousness","llama","vr","virtualreality"]},{"title":"Deploy your own Owncast server","contentType":"posts","date":"2025-01-09 20:47 -05:00","url":"/posts/deploy-owncast-azure/","content":"<p>I'm a fan of learning in public. One way I do that is writing these types of posts. Another is live-streaming. It's been about four years since the last time I streamed. You can check out some of those recordings on my <a href=\"/youtube\">YouTube channel</a> if you're interested.</p>\n<p>Lately, I've been wanting to get back into live-streaming, but like many parts of my digital presence, it's an area I want ownership over. That means, I don't want to use Twitch, YouTube, Kick, or any other streaming platform. I want to self-host my own livestream server.</p>\n<p>For years, I've been following <a href=\"https://owncast.online/\">Owncast</a>. I'm a big fan. Aside from being open-source and allowing me to self-host, it has <a href=\"https://owncast.online/docs/social/\">Fediverse</a> and <a href=\"https://owncast.online/docs/chat/chat-authentication/#indieauth\">IndieAuth</a> integrations.</p>\n<p>Because I don't plan to monetize my streams, I want to make sure I keep my costs down. Owncast offers a container deployment option. That means, I can deploy my application to <a href=\"https://learn.microsoft.com/azure/container-apps/overview\">Azure Container Apps</a> using a <a href=\"https://learn.microsoft.com/azure/container-apps/plans#consumption\">consumption plan</a>. By doing so, I only pay for compute and networking when I'm streaming. The rest of the time, I can scale down the application to zero and I'm not charged for it.</p>\n<p>In this post, I'll go over how to set up a self-hosted Owncast instance on Azure Container Apps and once deployed, how to configure it with <a href=\"https://obsproject.com/\">OBS</a>.</p>\n\n<p>The deployment is fairly straightforward. You can use the <a href=\"https://learn.microsoft.com/azure/container-apps/vnet-custom?tabs=bash&amp;amp;pivots=azure-portal\">&amp;quot;Provide a virtual network to an Azure Container Apps environment&amp;quot;</a> tutorial. I'd recommend going through the portal if you don't want to install the Azure CLI or are not comfortable with the command line.</p>\n<p>The few configurations that tutorial doesn't guide you through is setting up your container and ingress since those are application specific.</p>\n<p>After you configure your environment and virtual network in the <em>Basics</em> tab, continue your setup using the steps below.</p>\n<h3>Container</h3>\n<p>To set up Owncast, you can use the <a href=\"https://hub.docker.com/r/owncast/owncast/tags\">public docker image</a>.</p>\n<p>When you get to the <em>Container</em> tab in the setup process, fill in the fields with the following values:</p>\n<table class=\"table\">\n<thead>\n<tr>\n<th>Field</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Name</td>\n<td>Enter a name for your container</td>\n</tr>\n<tr>\n<td>Image source</td>\n<td>Docker Hub or other registies</td>\n</tr>\n<tr>\n<td>Image type</td>\n<td>Public</td>\n</tr>\n<tr>\n<td>Registry login server</td>\n<td>docker.io</td>\n</tr>\n<tr>\n<td>Image and tag</td>\n<td>owncast/owncast:latest</td>\n</tr>\n<tr>\n<td>Command override</td>\n<td>You can leave this blank</td>\n</tr>\n<tr>\n<td>Arguments override</td>\n<td>You can leave this blank</td>\n</tr>\n<tr>\n<td>Deployment stack</td>\n<td>Generic</td>\n</tr>\n<tr>\n<td>Workload profile</td>\n<td>Consumption</td>\n</tr>\n<tr>\n<td>CPU and Memory</td>\n<td>2 CPU Cores, 4Gi Memory</td>\n</tr>\n</tbody>\n</table>\n<h3>Ingress</h3>\n<p>By setting up ingress on your deployment, you'll expose the ports Owncast listens on.</p>\n<p>The reason you need a virtual network is because Owncast listens on two ports:</p>\n<ul>\n<li><strong>8080</strong> - The web application and admin portal</li>\n<li><strong>1935</strong> - The RMTP server used for streaming</li>\n</ul>\n<p>By default, Azure Container Apps only allows you to expose one port. For more details, see the <a href=\"https://learn.microsoft.com/azure/container-apps/ingress-overview#additional-tcp-ports\">Azure Container Apps documentation</a>.</p>\n<p>In the <em>Ingress</em> tab, check the box that says <strong>Enabled</strong>.</p>\n<p>Then, fill in the fields with the following values:</p>\n<table class=\"table\">\n<thead>\n<tr>\n<th>Field</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Ingress traffic</td>\n<td>Accepting traffic from anywhere</td>\n</tr>\n<tr>\n<td>Ingress type</td>\n<td>HTTP</td>\n</tr>\n<tr>\n<td>Transport</td>\n<td>Auto</td>\n</tr>\n<tr>\n<td>Insecure connections</td>\n<td>Leave unchecked</td>\n</tr>\n<tr>\n<td>Target port</td>\n<td>8080</td>\n</tr>\n<tr>\n<td>Session affinity</td>\n<td>Leave unchecked</td>\n</tr>\n</tbody>\n</table>\n<p>Expand the Additional TCP ports section and configure the RMTP port</p>\n<table class=\"table\">\n<thead>\n<tr>\n<th>Field</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Target port</td>\n<td>1935</td>\n</tr>\n<tr>\n<td>Exposed port</td>\n<td>1935</td>\n</tr>\n<tr>\n<td>Ingress traffic</td>\n<td>Accepting traffic from anywhere</td>\n</tr>\n</tbody>\n</table>\n<p>Once you've configured ingress, you can select on <strong>Review + create</strong>.</p>\n<p>Review your configurations in the <em>Review + create</em> tab. If everything looks good, select <strong>Create</strong>.</p>\n\n<p>Once your application deploys, make sure to change your admin password.</p>\n<ol>\n<li><p>In the portal, go to your container app resource and copy your application URL.</p>\n<p><img src=\"/files/images/portal-container-resource.png\" class=\"img-fluid\" alt=\"Screenshot of Azure Container Apps resource in Azure Portal highlighting Application URL\" /></p>\n<p>Make sure to keek your application URL around since you'll also be using it when configuring OBS.</p>\n</li>\n<li><p>Access the admin portal at the following URL - <code>&amp;lt;YOUR-APPLICATION-URL&amp;gt;/admin</code>. Replace <code>&amp;lt;YOUR-APPLICATION-URL&amp;gt;</code> with your application URL.</p>\n</li>\n<li><p>Log in with the default credentials.</p>\n</li>\n</ol>\n<p>| Field | Value |\n| Username | admin |\n| Password | abc123 |</p>\n<ol>\n<li>In the Owncast admin portal, select <strong>Configuration &amp;gt; Server Setup</strong></li>\n<li>Change your admin password.</li>\n</ol>\n<p>For more details, see the <a href=\"https://owncast.online/docs/configuration/\">Owncast documentation</a>.</p>\n\n<p>Once you've changed your admin password, create a stream key. This will enable you to log into your server through OBS and begin streaming.</p>\n<ol>\n<li>In the Owncast admin portal, select <strong>Configuration &amp;gt; Server Setup</strong></li>\n<li>Select the <strong>Stream Keys</strong> tab.</li>\n<li>Copy the default stream key or create a new one.</li>\n</ol>\n\n<p>Now that everything is configured, it's time to set up OBS.</p>\n<p>To do so, <a href=\"https://owncast.online/docs/broadcasting/obs/\">follow the guidance in the Owncast documentattion</a>.</p>\n<p>Replace the server address with your application URL and use the stream key you copied or created. Note that the protocol is not <code>http</code> or <code>https</code>. <strong>Use <code>rmtp</code></strong>.</p>\n<p>That's it! If everything is configured correctly, you should be able to select <strong>Start Streaming</strong> in OBS.</p>\n<p>Go to your Owncast instance using your application URL and in a few seconds, your broadcast should start.</p>\n\n<p>I want to end by thanking the Owncast contributors and community for making self-hosting a live-streaming server so easy.</p>\n<p>My plan in the next couple of weeks is to:</p>\n<ol>\n<li>Embed my stream on my website. Most likely, I'll make it a dedicated page. Something like <code>lqdev.me/live</code> or something like that.</li>\n<li>Do a few pilot streams to make sure my stream quality and configurations are all set up correctly.</li>\n<li>Evaluate what the costs are for this solution and whethere it's sustainable.</li>\n</ol>\n<p>If all goes as expected, I'll start streaming more regularly. As mentioned, I want to keep my costs down and video hosting is expensive, so I'll use YouTube to post recordings. I'll also keep backups though in case any of those videos get taken down, I can always have a local copy still available.</p>\n","tags":["owncast","azure","selfhost","fediverse","obs","livestream"]},{"title":"Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod","contentType":"posts","date":"2025-01-05 21:04 -05:00","url":"/posts/listen-audiobooks-librivox-antennapod/","content":"<p>Last year, I was able to complete my goal of reading 12 books or roughly one book per month. I kept track of some of the books I read in my <a href=\"/reviews\">library page</a>.</p>\n<p>I enjoyed most of the books I read. However, despite hitting my goal and averaging a book a month, I didn't build up a habit which is what I would've preferred. Many of the books I read last year, I read them during long periods of uninterrupted time, such as cross-country flights. The topic of goal setting and measuring the right thing could be an entire post, but I'll leave that for another time.</p>\n<p>Taking the learnings from last year, this year, I'll try to focus on building a reading habit rather than reading a set number of books. While I'd still like to finish at least one book per month, that's a metric I'll use to track general progress rather than making it the goal. Unless I'm reading something like War and Peace, I'd expect that by establishing a consistent cadence, I should be able to get through at least one book every month. Also, I'm only accounting for uninterrupted reading time blocks. Using audiobooks, I can continue building my reading habit while working out or doing chores. Kind of like having TV playing in the background.</p>\n<p>There is no shortage of audiobooks out there. Audible is one of the easiest ways to access audiobooks and they have a large selection to choose from. Personally, I don't use Audible. I don't like paying for a separate subscription, getting credits to &amp;quot;buy&amp;quot; books that I don't really own and can only listen through the Audible app. Instead, I use Spotify. I get <a href=\"https://support.spotify.com/us/article/audiobooks-access-plan/\">15 hours of access to audiobooks per month</a> as part of my subscription. While I still don't own the books and can only listen on the Spotify app, at least I'm not paying a separate subscription. Also, the 15 hour limit at this time is more than enough for me.</p>\n<p>The library is another amazing option. No subscription fees and you get access to your local library's entire digital catalog. Using the <a href=\"https://www.overdrive.com/apps/libby\">Libby app</a>, you can borrow books and start listening for free. Selection though may be an issue here since you are limited to what's available through your local library. Pro-tip, universities and employers sometimes have libraries, so as an alumnus or employee, you can expand the number of books available to you by registering multiple libraries on the Libby app. Regardless of which library you choose though, you still have to go through the Libby app.</p>\n<p>The dream setup for me is, having direct access to the audiobook files so I can listen to them whenever and wherever I'd like. Similar to podcasts. Thanks to LibriVox and AntennaPod, that setup is possible.</p>\n<p>In case you're interested in setting up something similar, in this post, I'll briefly explain what LibriVox and AntennaPod are and show how you can use them to start listening to free audiobooks on your phone.</p>\n\n<p><a href=\"https://librivox.org/\">LibriVox</a> is a website that provides a collection of free audiobooks from the public domain. Thanks to a vibrant community of volunteers, you can enjoy classics such as:</p>\n<ul>\n<li><a href=\"https://librivox.org/the-divine-comedy-by-dante-alighieri/\">The Divine Comedy</a></li>\n<li><a href=\"https://librivox.org/the-adventures-of-sherlock-holmes/\">The Adventures of Sherlock Holmes</a></li>\n<li><a href=\"https://librivox.org/discourses-of-epictetus-by-epictetus/\">Discourses of Epictetus</a></li>\n<li><a href=\"https://librivox.org/roughing-it/\">Roughing It</a></li>\n<li><a href=\"https://librivox.org/search?primary_key=0&amp;amp;search_category=title&amp;amp;search_page=1&amp;amp;search_form=get_results&amp;amp;search_order=catalog_date\">And many others...</a></li>\n</ul>\n<p>It's important to note that the catalog is limited to public domain works. That means you won't have access to the latest self-help, business, or current events books. However, there are still thousands of books to keep you busy for an entire lifetime and the list is constantly growining since there are new works introduced into the public domain every year.</p>\n<p>Here's <a href=\"https://blog.archive.org/2025/01/01/welcome-to-the-public-domain-in-2025/\">a few notable literary works that entered the public domain in 2025</a> as well as the complete list of <a href=\"https://archive.org/details/internetarchivebooks?tab=collection&amp;amp;query=date%3A1929\">literary works from 1929</a>.</p>\n\n<p><a href=\"https://antennapod.org/\">AntennaPod</a> is an open-source podcast player for Android.</p>\n<p>The app has a ton of great features, such as <a href=\"https://podcastindex.org/\">Podcast Index</a> support, subscribe via RSS directly,  OPML import and export, chapters, adjustable playback speed, and for the past few years, a personalized yearly summary which is computed locally on your device <a href=\"/notes/antennapod-echo-2024\">(here's my summary from 2024)</a>.</p>\n<p>You can get AntennaPod from the Play Store or <a href=\"https://f-droid.org/en/packages/de.danoeh.antennapod/\">F-Droid</a></p>\n\n<p>LibriVox provides <a href=\"https://librivox.org/pages/librivox-feeds/\">several feeds</a>.</p>\n<p>Since podcast distribution is primarly done through RSS and you can subscribe to podcasts directly using RSS feeds in AntennaPod, we can use the app to stay up to date with new audiobook releases.</p>\n<p>To subscribe to the LibriVox New Release feed:</p>\n<ol>\n<li><p>Copy the LibriVox New Releases RSS feed URL to your clipboard: <a href=\"https://librivox.org/rss/latest_releases\">https://librivox.org/rss/latest_releases</a></p>\n</li>\n<li><p>In AntennaPod, open up the hamburger menu and select <strong>Add Podcast</strong>.</p>\n</li>\n<li><p>In the <em>Add Podcast</em> dialog, select <strong>Add podcast by RSS address</strong>.</p>\n<p><img src=\"/files/images/add-podcast-antennapod.png\" class=\"img-fluid\" alt=\"AntennaPod Add Podcast Dialog\" /></p>\n</li>\n<li><p>Paste the LibriVox New Releases RSS feed URL into the text box and select <strong>Confirm</strong>.</p>\n</li>\n</ol>\n<p>At this point, you should see the feed listed in your AntennaPod subscriptions.</p>\n<p>By subscribing the LibriVox New Release feed, AntennaPod treats the feed as a podcast, and each new release entry or audiobook is like a podcast episode.</p>\n<p>For more information on subscribing to podcasts, see the <a href=\"https://antennapod.org/documentation/getting-started/subscribe\">AntennaPod documentation</a>.</p>\n\n<p>Subscribing to the new releases feed only notifies you when new books are added, you can't listen to or download the books. To do that, you'll need to subscribe to the RSS feed of the book you want to listen to in AntennaPod.</p>\n<h3>From New Releases Feed</h3>\n<ol>\n<li><p>In AntennaPod's Subscription screen, select <strong>LibriVox's New Releases</strong>.</p>\n</li>\n<li><p>From the list of new releases, select the book you want to listen to. For example, &amp;quot;Room of One's Own&amp;quot; by Virginia Woolf.</p>\n<p><img src=\"/files/images/antennapod-librivox-new-releases.png\" class=\"img-fluid\" alt=\"AntennaPod New Releases Feed\" /></p>\n</li>\n<li><p>In the audiobook's screen, select <strong>Visit Website</strong>. This will take you to the LibriVox audiobook's page in the browser.</p>\n<p><img src=\"/files/images/antennapod-room-of-ones-own.png\" class=\"img-fluid\" alt=\"AntennaPod Audiobook Screen\" /></p>\n</li>\n<li><p>In the LibriVox audiobook page, select the book's RSS Feed.</p>\n<p><img src=\"/files/images/librivox-room-ones-own.png\" class=\"img-fluid\" alt=\"A Room of One's Own by Virginia Woolf LibriVox book page\" /></p>\n</li>\n<li><p>Copy the book's feed URL to your clipboard.</p>\n</li>\n<li><p>In AntennaPod, open up the hamburger menu and select <strong>Add Podcast</strong>.</p>\n</li>\n<li><p>In the <em>Add Podcast</em> dialog, select <strong>Add podcast by RSS address</strong>.</p>\n</li>\n<li><p>Paste the book's RSS feed URL into the text box and select <strong>Confirm</strong>.</p>\n</li>\n</ol>\n<h3>Any other books</h3>\n<p>The process works the same for other books but you'll have to use LibriVox as the entrypoint to browse books.</p>\n<ol>\n<li>In your browser, navigate to LibriVox.</li>\n<li>Go to the audiobook page. For example, this is the page for <a href=\"https://librivox.org/walden-by-henry-david-thoreau\">Walden</a>.</li>\n<li>In the audiobook page, select the book's RSS Feed. For Walden, it's <a href=\"https://librivox.org/rss/549\">https://librivox.org/rss/549</a>.</li>\n<li>Copy the book's feed URL to your clipboard.</li>\n<li>In AntennaPod, open up the hamburger menu and select <strong>Add Podcast</strong>.</li>\n<li>In the <em>Add Podcast</em> dialog, select <strong>Add podcast by RSS address</strong>.</li>\n<li>Paste the book's RSS feed URL into the text box and select <strong>Confirm</strong>.</li>\n</ol>\n\n<p>Although AntennaPod wasn't designed for audiobook listening, thanks to the flexibility of RSS feeds and the thankless work of the LibriVox community, you can easily listen to audiobooks for free on your mobile device. I plan on testing out this setup and seeing whether it helps keep my reading habit on track.</p>\n","tags":["book","audiobook","rss","antennapod","librivox","android","podcast","publicdomain","opensource"]},{"title":"Weird, Inc. and 2025","contentType":"posts","date":"2024-12-17 18:41","url":"/posts/weird-inc-2025/","content":"<p>I don't remember how I came across Weird and their projects but I'm glad I did. I appreciate their approach of using existing platforms, protocols, and practices from the yesterweb to put the personal website front and center.</p>\n\n<p>First, the topic of <a href=\"https://blog.muni.town/weird-inc/\">Weird the organization and their aspirations</a>.</p>\n<blockquote class=\"blockquote\">\n<p>We have big aspirations for Weird. At minimum, a 'social network of personal websites' can be catalyzed by simply mapping and linking existing web spaces together, playing the role of social glue. On a grander scale there's potential for a pro-social reformation of the web as we know it, putting people's websites at the center of our sociodigital interactions.</p>\n</blockquote>\n<p>By aligning to the foundations of <a href=\"https://seths.blog/2010/05/the-modern-business-plan/\">Seth Godin's Modern Business Plan</a>, they aim to focus on:</p>\n<ol>\n<li>Truth -  describe the world as it is</li>\n<li>Assertions - how you’re going to change things</li>\n<li>Alternatives - what you'll do if assertions don't pan out.</li>\n<li>People - who is on your team, who is going to join your team</li>\n<li>Money - how much do you need and how will you spend it</li>\n</ol>\n<p>Below are some of the points that resonated with me.</p>\n<h3>Truth</h3>\n<blockquote class=\"blockquote\">\n<p>Home ownership on the web, remarkably, is just as fraught with rent-seeking monopolists as our physical spaces. No one gets to own anything anymore.</p>\n</blockquote>\n<p>While they're mainly referring to the social web, this is is generally true across many areas of our digital spaces.</p>\n<blockquote class=\"blockquote\">\n<p>Bluesky...is bringing into mainstream a nearly forgotten cornerstone of the Indie Web: Domain sovereignty<br />\n<br>\nIn a reinvigorated social web where your domain-name is your universal address in the digital sphere, Weird is the virtual home to which that address is pointing<br />\n<br>\n'Weird the data importer' is our wedge into the data fortresses keeping our digital identities captive. What will start as a Publish Elsewhere, Syndicate (to your) Own Site (PESOS) strategy makes way for a fully self-sovereign Publish (on your) Own Site, Syndicate Elsewhere (POSSE) end state.<br />\n<br>\nImagine a WordPress-like website engine that aggregates all of your digital personas into a unified mosaic, completely under your control by combination of cloud backing and local storage.</p>\n</blockquote>\n<h3>Alternatives</h3>\n<p>If the assertions don't pan out, the alternatives that I think might be interesting:</p>\n<ul>\n<li>Local-first knowledge base with cloud synchronization. <a href=\"https://anytype.io/\">AnyType</a> comes to mind.</li>\n<li>Cloud + local archiver for many other 3rd party accounts such as Bluesky, Mastodon, Discourse etc. This might make PESOS scenarios easier. Of course, this only works if the 3rd party services allow developer access to their APIs.</li>\n</ul>\n\n<p>Speaking specifically about <a href=\"https://blog.muni.town/weird-2025/\">Weird the product and what the team plans of shipping in 2025</a>.</p>\n<blockquote class=\"blockquote\">\n<p>Weird is best understood as a single-page website generator</p>\n</blockquote>\n<h3>Q1 and Q2</h3>\n<blockquote class=\"blockquote\">\n<p>'Weird the data importer' is our wedge into the data fortresses keeping our digital identities captive.<br />\n<br>\n'Weird: ₱ESOS-edition' aggregates all of your digital personas into a harmonious mosaic, made completely yours by combination of cloud backing and local-first storage.<br />\n<br>\nSetting up a POSSE pipeline however requires not just the right infrastructure...but also a big shift in workflow.<br />\n<br>\nMost people will probably want a combination of both.<br />\n<br>\nHence our commitment to both models, starting with the least laborious one [PESOS].</p>\n</blockquote>\n<p>Personally, <a href=\"/posts/rss-to-mastodon-posse-azure-logic-apps/\">I prefer POSSE</a>, but it's great to see that Weird plans on meeting people where they are.</p>\n<h3>Late 2025</h3>\n<blockquote class=\"blockquote\">\n<p>Around half-way through 2025, we're hoping to shift some of our attention away from Weird as an individualistic publishing tool to Weird as a network engine.<br />\n<br>\nWe think there's massive untapped potential in the web 1.0 concept of webrings, and we fully intend to bring them back.<br />\n<br>\nAll of this culminates in one of the key motivations behind the Weird project: <a href=\"https://github.com/muni-town/weird/issues/3\">A Network of Shared Purpose</a>.</p>\n</blockquote>\n<p>I'm not sure yet what this looks like but making it easier to discover and build networks of personal websites is a good problem to try to solve.</p>\n\n<p>I wish them luck and success in what they're aiming to accomplish. I'm excited to see more projects like this sprouting to shake up the status-quo in our digital spaces.</p>\n","tags":["weird","cms","munitown","2025","indieweb","posse","pesos","personalweb","internet"]},{"title":"Starter Packs with OPML and RSS","contentType":"posts","date":"2024-12-08 19:40","url":"/posts/rss-opml-starter-packs/","content":"<p>One of the things I like about Bluesky is the <a href=\"https://bsky.social/about/blog/06-26-2024-starter-packs\">Starter Pack feature</a>.</p>\n<p>In a gist, a Starter Pack is a collection of feeds.</p>\n<p>Bluesky users can:</p>\n<ul>\n<li>Create starter packs</li>\n<li>Share starter packs</li>\n<li>Subscribe to starter packs</li>\n</ul>\n<p>Unfortunately, Starter Packs are limited to Bluesky.</p>\n<p>Or are they?</p>\n<p>As mentioned, starter packs are a collection of feeds that others can create, share, and subscribe to.</p>\n<p><a href=\"/responses/bluesky-rss-support/\">Bluesky supports RSS</a>, which means you could organize the feeds using an OPML file that you can share with others and others can subscribe to. The benefits of this is, you can continue to keep up with activity on Bluesky from the feed reader of your choice without being required to have an account on Bluesky.</p>\n<p>More importantly, because RSS and OPML are open standards, you're not limited to building starter packs for Bluesky. You can create, share, and subscribe to starter packs for any platform that supports RSS. That includes blogs, podcasts, forums, YouTube, Mastodon, etc. <a href=\"https://www.manton.org/2024/12/07/with-the-success.html\">Manton seems to have something similar in mind</a> as a means of building on open standards that make it easy for Micro.blog to interop with various platforms.</p>\n<p>If you're interested in what that might look like in practice, check out my &amp;quot;starter packs&amp;quot; which you can subscribe to using your RSS reader of choice and the provided OPML files.</p>\n<ul>\n<li><a href=\"/collections/blogroll\">Blogs</a></li>\n<li><a href=\"/collections/podroll\">Podcasts</a></li>\n<li><a href=\"/collections/youtube\">YouTube</a></li>\n<li><a href=\"/collections/forums\">Forums</a></li>\n</ul>\n<p>I'm still working on similar collections for Mastodon and Bluesky but the same concept applies.</p>\n<p>Although these are just simple examples, it shows the importance of building on open standards and the open web. Doing so introduces more freedom for creators and communities.</p>\n<p>Here are other &amp;quot;starter packs&amp;quot; you might consider subscribing to.</p>\n<ul>\n<li><a href=\"https://www.thingelstad.com/2024/12/03/people-blogs-list.html\">People and Blogs</a></li>\n<li><a href=\"https://blogroll.club/faq/\">Blogroll Club</a></li>\n</ul>\n<p>If this is interesting to you, <a href=\"https://feedland.org/\">Feedland</a> might be a project worth checking out.</p>\n","tags":["rss","opml","indieweb","starterpacks","community","bluesky","socialmedia","openweb","protocols","openstandards","feedland"]},{"title":"Digitize Analog Bookmarks using AI, .NET, and GitHub Models","contentType":"posts","date":"2024-10-31 21:41 -05:00","url":"/posts/digitize-analog-bookmarks-with-ai-dotnet/","content":"\n<p>This past year I've made more of an effort to read. I track books in my <a href=\"/library\">/library</a> page.</p>\n<p>Although I have a <a href=\"https://shop.boox.com/products/novaair2\">Nova Air 2 E-Note device</a>, given the choice, I prefer physical books. Despite the conveniene of an electronic note-taking device, theres something about analog that I find hard to quit.</p>\n<p>As I read books, especially non-fiction, I annotate them in various ways. Eventually, those annotations make their way into my library page.</p>\n<p>Here's an example of those annotations for <a href=\"/reviews/building-a-second-brain\">Building A Second Brain by Thiago Forte</a>.</p>\n<p>The process of transferring notes is manual and tedious. I don't always have the discipline to transfer them at periodic intervals and what ends up happening is, I get to the end of the book without transferring any notes. To make space for new books, I donate it or resell the ones I've read. If I didn't take the time to transfer those notes, they're gone. <a href=\"/reviews/slow-productivity-newport\">Slow Productivity</a> is an example of that.</p>\n<p>I want to find a better system that's low-maintenance for keeping more of these notes and retaining knowledge I've found valuable.</p>\n<p>Then it hit me, why not use AI? I know I could use OCR or even some of the AI models of yesteryear. The challenge is, those systems are error prone and given I don't always have the motivation to transfer notes manually, I have even less motivation to build and maintain such a system.</p>\n<p>However, vision models have advanced significantly and when paired with language models, the barrier to entry for reasoning over image data has drastically decreased.</p>\n<p>That's what led to this post. In this post, I'll show how you can use AI models like GPT-4o Mini to extract the passages I've annotated in physical books from an image. I then format those passages in markdown to make them easy to directly copy and paste them onto the website.</p>\n<p>I know there's probably a ton of services that do this for you, but it's a lot more fun to build one from scratch. With that in mind, let's get started.</p>\n<p>You can find the source for the application in the <a href=\"/github/AIBookmarks\">AIBookmarks GitHub repository</a>.</p>\n<p>Alternatively, I've configured the repo to use GitHub CodeSpaces, so you can launch the application there as well.</p>\n<p><a href=\"https://codespaces.new/lqdev/AIBookmarks\"><img src=\"https://github.com/codespaces/badge.svg\" class=\"img-fluid\" alt=\"Open in GitHub Codespaces\" /></a></p>\n\n<p>I'll be using <a href=\"https://github.blog/news-insights/product-news/introducing-github-models/\">GitHub Models</a> as my AI model provider. GitHub Models provides developers with access to a catalog of AI models such as GPT-4o, Llama 3.2, Phi 3.5, and many others. Best of all, it's completely free, <a href=\"https://docs.github.com/en/github-models/prototyping-with-ai-models#rate-limits\">though restrictions apply</a>.</p>\n<p>I chose GitHub Models for the following reasons:</p>\n<ul>\n<li>Zero installation. Although I really like Ollama and some of the other local AI model providers, I didn't want to fill precious hard-drive space with AI models.</li>\n<li>It's free! Since I'm just prototyping, even with their limited capacity, it should be enough to prove out whether my scenario is feasible.</li>\n<li>They provide access to multi-modal models such as GPT-4o, Llama 3.2 Vision, and Phi 3.5 Vision, which can reason over text and images, which is what I need.</li>\n<li>I'm one of the GPU poor. My <a href=\"https://www.lenovo.com/us/en/p/laptops/thinkpad/thinkpadx1/x1-titanium-g1/22tp2x1x1t1?orgRef=https%253A%252F%252Fduckduckgo.com%252F\">Lenovo Thinkpad X1</a> couldn't handle running one of the vision models.</li>\n</ul>\n<p>Getting set up with GitHub models is fairly easy. At minimum, it requires:</p>\n<ol>\n<li>A GitHub Account.</li>\n<li>A Personal Access Token. For more details, see the <a href=\"https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\">GitHub documentation</a>.</li>\n</ol>\n<p>Once you have both of those, you can get started building your application.</p>\n\n<p>The sample application is a C# console application which targets .NET 9. However, the code shown here should work on the latest LTS version as well.</p>\n<p>For this solution, I use the Azure AI Inference implementation of <a href=\"https://www.nuget.org/packages/Microsoft.Extensions.AI.AzureAIInference/\">Microsoft.Extensions.AI</a>. <a href=\"https://devblogs.microsoft.com/dotnet/introducing-microsoft-extensions-ai-preview/\">Microsoft.Extensions.AI</a>, M.E.AI for short, is a set of abstractions that provides a common set of interfaces for interacting with AI models.</p>\n<p>Although we're using the Azure AI Inference client SDK implementation of M.E.AI, we can still use it to work with models from the GitHub Model catalog.</p>\n<pre><code class=\"language-csharp\">IChatClient client =\n    new ChatCompletionsClient(\n        endpoint: new Uri(&amp;quot;https://models.inference.ai.azure.com&amp;quot;), \n        new AzureKeyCredential(Environment.GetEnvironmentVariable(&amp;quot;GITHUB_TOKEN&amp;quot;)))\n        .AsChatClient(&amp;quot;gpt-4o-mini&amp;quot;);\n</code></pre>\n\n<p>In the repo, I have a set of sample images containing annotations. These are the images I'll send to the AI model for processing.</p>\n<p>The following is an sample from <a href=\"/reviews/creative-act-way-of-being-rubin\">The Creative Act by Rick Rubin</a>.</p>\n<p><img src=\"https://github.com/lqdev/AIBookmarks/raw/main/data/creative-act-1.jpg\" class=\"img-fluid\" alt=\"An image of a book with pencil markings\" /></p>\n<ol>\n<li><p>Load the files</p>\n<pre><code class=\"language-csharp\">var filePaths = Directory.GetFiles(&amp;quot;data&amp;quot;);\n</code></pre>\n</li>\n<li><p>Create a collection to store extracted passages</p>\n<pre><code class=\"language-csharp\">var passages = new List&amp;lt;AIBookmark&amp;gt;();\n</code></pre>\n</li>\n</ol>\n\n<p>Once you have the images loaded, it's time to proces them.</p>\n<ol>\n<li><p>Start by setting the system prompt. This will provide the initial guidance for the extraction task.</p>\n<pre><code class=\"language-csharp\">var systemPrompt = \n    &amp;quot;&amp;quot;&amp;quot;\n    You are an AI assistant that extracts underlined, highlighted, and marked passages from book page images.\n\n    When passages have a natural continuation between pages, merge them and assign the page number where the first passage starts.\n    &amp;quot;&amp;quot;&amp;quot;;\n</code></pre>\n</li>\n<li><p>Then, iterate over each of the images and process them.</p>\n<pre><code class=\"language-csharp\">foreach(var path in filePaths)\n{\n    var file = await File.ReadAllBytesAsync(path);\n    var messages = new List&amp;lt;ChatMessage&amp;gt;\n    {\n        new ChatMessage(ChatRole.System, systemPrompt),\n        new ChatMessage(ChatRole.User, new AIContent[] {\n            new ImageContent(file, &amp;quot;image/jpeg&amp;quot;),\n            new TextContent(&amp;quot;Extract the marked passages from the image&amp;quot;),\n        })\n    };\n\n    var response = await client.CompleteAsync&amp;lt;List&amp;lt;AIBookmark&amp;gt;&amp;gt;(messages, options: new ChatOptions {Temperature = 0.1f});\n\n    passages.AddRange(response.Result);\n}\n</code></pre>\n<p>This code:</p>\n<ol>\n<li><p>Loads the image as a <code>byte[]</code>.</p>\n</li>\n<li><p>Composes a set of messages that include:</p>\n<ul>\n<li>The system prompt</li>\n<li>The image</li>\n<li>The user prompt instructing the model to extract the marked passages from the image</li>\n</ul>\n</li>\n<li><p>Sends the messages to the model for processing and returns a list of <code>AIBookmark</code>. An <code>AIBookmark</code> is a custom class I've defined as follows:</p>\n<pre><code class=\"language-csharp\">class AIBookmark\n{\n    public string Text {get;set;}\n\n    public int PageNumber {get;set;}\n}\n</code></pre>\n<p>Some of the more recent AI models support structured output, which enforce a schema on AI model outputs. You can take a look at the <a href=\"https://openai.com/index/introducing-structured-outputs-in-the-api/\">OpenAI documentation</a> for more details. It's important to note though that the functionality is not exclusive to OpenAI models.</p>\n</li>\n<li><p>Adds the extracted passages to the <code>passages</code> collection.</p>\n</li>\n</ol>\n</li>\n</ol>\n\n<p>Once all of the files are processed by the AI model, additional processing is done to ensure that they're in the correct page order as well as formatted as markdown blockquotes.</p>\n<pre><code class=\"language-csharp\">var sortedPassages = \n    passages\n        .OrderBy(p =&amp;gt; p.PageNumber)\n        .Select(p =&amp;gt; $&amp;quot;&amp;gt; {p.Text} (pg. {p.PageNumber})&amp;quot;);\n</code></pre>\n\n<pre><code class=\"language-csharp\">foreach(var passage in sortedPassages)\n{\n    Console.WriteLine(passage);\n    Console.WriteLine(&amp;quot;&amp;quot;);\n}\n</code></pre>\n<p>For the images included in the repo's <em>data</em> directory, output might look like the following.</p>\n<pre><code class=\"language-markdown\">&amp;gt; This isn’t a matter of blind belief in yourself. It’s a matter of experimental faith. (pg. 278)\n\n&amp;gt; When we don’t yet know where we’re going, we don’t wait. We move forward in the dark. If nothing we attempt yields progress, we rely on belief and will. We may take several steps backward in the sequence to move ahead. (pg. 278)\n\n&amp;gt; If we try ten experiments and none of them work; we have a choice. We can take it personally, and think of ourselves as a failure and question our ability to solve the problem. (pg. 278)\n\n&amp;gt; Staying in it means a commitment to remain open to what’s around you. Paying attention and listening. Looking for connections and relationships in the outside world. Searching for beauty. Seeking stories. Noticing what you find interesting, what makes you lean forward. And knowing all of this is available to use next time you sit down to work, where the raw data gets put into form. (pg. 296)\n\n&amp;gt; Just as a surfer can’t control the waves, artists are at the mercy of the creative rhythms of nature. This is why it’s of such great importance to remain aware and present at all times. Watching and waiting. (pg. 296)\n\n&amp;gt; Maybe the best idea is the one you’re going to come up with this evening. (pg. 297)\n</code></pre>\n\n<p>Putting the application together took me less than an hour so this is far from done. However, it does provide me with a starting point and offers validation that this could be a way to more easily capture the knowledge I'm curating from physical books.</p>\n<p>Some improvements I can make here:</p>\n<ol>\n<li>Update the system prompt with some samples to help guide the extraction. For example, in the output I shared, that last passage is not annotated. It just happens to be a mostly blank page with that quote in the center, therefore giving the illusion that the passage is important in some way.</li>\n<li>Add additional information to the AIBookmark class like <code>Index</code> so I can ensure order within a page is preserved. Right now page number is good enough, but I can't guarantee the correct order. An index property might help here.</li>\n<li>Use a service with higher rate limits. The current rate limits wouldn't allow me to process a large number of images at once. Therefore, I'd need to use a service with higher limits. Alternatively, I could make this a job that runs in the background which abides by the rate limits but I also don't have to spend money on. Given I'm not using this for anything mission-critical, that'd be an acceptable solution as well.</li>\n<li>Refactor the solution so I can more easily swap between tasks. For example, sometimes I might want to use it with images from my bullet journal. Other times, I might want to use it with handwritten notes. Whatever the case may be, it'd be good to not have to rewrite the prompts every time.</li>\n</ol>\n<p>Some ways I see myself using this project:</p>\n<ol>\n<li>Periodically collect images of annotated pages and save them to cloud storage.</li>\n<li>When I'm done with the book, drop all the images in the <em>data</em> directory.</li>\n<li>Further enrich data by condensing repetitive passages and extracting key concepts.</li>\n<li>Storing this knowledge into some sort of knowledge store to make it actionable.</li>\n</ol>\n\n<p>Just for fun, Tyler released all of the lyrics to his <a href=\"/responses/chromakopia-tyler-the-creator-released\">latest album</a> as <a href=\"https://twitter.com/tylerthecreator/status/1852105825650708651/photo/1\">images on X</a>. With a few tweaks, I was able to repurpose this solution to extract the text from them and that worked relatively well.</p>\n<p>Just with this simple solution, there's a ton of other applications I can think of in my daily life to help bridge my analog and digital lives.</p>\n<p>What other use cases do you see yourself using something like this for? <a href=\"/contact\">Let me know</a>.</p>\n<p>Happy coding!</p>\n","tags":["ai","dotnet","github","reading","bookmarks","analog","digital","reading","notes"]},{"title":"Build your own self-hosted live streaming application with Owncast and .NET Aspire","contentType":"posts","date":"2024-10-20 14:01","url":"/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire/","content":"<p>Platforms come and go. As a result, I'm a strong advocate for <a href=\"https://indieweb.org/own_your_data\">owning your data</a> (when possible). Self-hosting is a way for you to do that. Owning your data could mean self-hosting <a href=\"https://buttondown.com/ownyourweb/archive/issue-06/\">your website</a>, <a href=\"https://bitwarden.com/blog/host-your-own-open-source-password-manager/\">password manager</a>, <a href=\"https://jellyfin.org/docs/\">media server</a>, or <a href=\"https://docs.joinmastodon.org/user/run-your-own/\">social media</a>. However, self-hosting comes with its own challenges, primarily cost (time and money) and in cases where software isn't provided as an appliance on your hosting provider of choice, some technical knowledge may be required.</p>\n<p>As I got more into self-hosting applications such as my <a href=\"/mastodon\">Mastodon instance</a>, I came across Owncast.</p>\n<p>During peak COVID lockdowns, like many others, live streaming is one of the ways I passed the time. While Twitch and YouTube got the job done, self-hosting was always in the back of my mind.</p>\n<p>It's been a while since I've live-streamed, so I never really went through the process of evaluating the self-hosted route with Owncast.</p>\n<p>While browsing for something on YouTube the other day, I ran into some of my <a href=\"https://www.youtube.com/playlist?list=PLsdMoYmuvh9ZtgB8U7FECR_8wKMYXJNAm\">old live-stream recordings</a>. This got me thinking again, how difficult would it be to put together my own live-streaming setup.</p>\n<p>This post is the result of that exploration.</p>\n<p>In this post, I'll modify a .NET Aspire Starter Application template and show how to set up a self-hosted live-streaming application using Owncast and .NET Aspire.</p>\n<p>You can find the source code in the <a href=\"/github/BYOwncastAspire\">lqdev/BYOwncastAspire</a> repository.</p>\n<p><img src=\"https://github.com/user-attachments/assets/bdd9f901-8d8f-45be-9e37-4dce8459e481\" class=\"img-fluid\" alt=\"BYOwnCastAspire Web Frontend\" /></p>\n\n<p>The Owncast website describes the project as, &amp;quot;...a free and open source live video and web chat server for use with existing popular broadcasting software.&amp;quot;</p>\n<p><img src=\"https://github.com/user-attachments/assets/769ec2d8-a3d1-4ec3-8271-c2de6e11ddd3\" class=\"img-fluid\" alt=\"Owncast admin server page\" /></p>\n<p>It goes on to further describe some of the reasons I like Owncast, which are:</p>\n<ul>\n<li><strong>Self-hosting</strong> - I'm in complete control over the service and my data</li>\n<li><strong>Open-source</strong> - I can freely use and contribute to the project. Free in this case meaning both as in freedom and pizza.</li>\n<li><strong>Builds on top of open standards like RMTP</strong> - Software that supports the Real-Time Messaging Protocol (RMTP) like OBS can immediately be leveraged.</li>\n<li><strong>Fediverse compatible</strong> - Your content and network federated across <a href=\"https://joinfediverse.wiki/What_is_the_Fediverse%3F\">the Fediverse</a>.</li>\n</ul>\n<p>To learn more, check out the <a href=\"https://owncast.online/\">Owncast website</a>.</p>\n\n<p>The .NET Aspire documentation describes it as, &amp;quot;...an opinionated, cloud ready stack for building observable, production ready, distributed applications.​ .NET Aspire is delivered through a collection of NuGet packages that handle specific cloud-native concerns.&amp;quot;</p>\n<p>Personally, the parts of .NET Aspire that matter to me are:</p>\n<ul>\n<li><strong>App composition</strong> - The .NET Aspire programming model makes it easy to define and compose a variety of resources such as .NET projects, containers, and much more in a single place. In many cases, for commonly used services, these resources are exposed in the form of <a href=\"https://learn.microsoft.com/dotnet/aspire/fundamentals/integrations-overview\">integrations</a>. These integrations further simplify the composition of applications.</li>\n<li><strong>Tooling (Dashboard)</strong> - .NET Aspire provides a set of tools and templates. However, my favorite is the dashboard. The dashboard provides me with a single place to view my resources, their configurations, and logs.</li>\n</ul>\n<p>Although in this post, I don't cover deployment, there is also the provisioning component provided by .NET Aspire which in many cases can <a href=\"https://learn.microsoft.com/dotnet/aspire/deployment/overview\">simplify your application deployments</a>.</p>\n<p>To learn more, check out the <a href=\"https://learn.microsoft.com/dotnet/aspire/get-started/aspire-overview\">.NET Aspire documentation</a>.</p>\n\n<p>This application makes a few modifications to the <a href=\"https://learn.microsoft.com/dotnet/aspire/fundamentals/setup-tooling?tabs=linux&amp;amp;pivots=dotnet-cli#net-aspire-project-templates\">.NET Aspire Starter Application template</a>.</p>\n<p>The application consists of a few projects:</p>\n<ul>\n<li><strong>BYOwncastAspire.AppHost</strong> - This project is the entrypoint for .NET Aspire applications. This is where we'll configure the Owncast server as a container resource.</li>\n<li><strong>BYOwncastAspire.Web</strong> - Blazor web application. Although Owncast provides its own page where viewers can tune into your stream, by having a separate web application, preferably your own personal website, you can further enrich and customize how and where you publish content.</li>\n<li><strong>BYOwncastAspire.ServiceDefaults</strong> - This project contains default configurations for telemetry, health checks, resiliency, etc. No changes or updates required here.</li>\n</ul>\n<h3>Configure Owncast</h3>\n<p>There are many ways to host an Owncast server, with one of them being a container.</p>\n<p>In the context of Aspire which has built-in container support, you can easily add Owncast as a resource in your application.</p>\n<p>From the <a href=\"https://owncast.online/quickstart/container/\">Owncast documentation</a>, once you've pulled the Owncast container image, you can start it with the following command.</p>\n<pre><code class=\"language-bash\">docker run -v `pwd`/data:/app/data -p 8080:8080 -p 1935:1935 owncast/owncast:latest\n</code></pre>\n<p>This translates to the following in the <em>Program.cs</em> of the <em>BYOwncastAspire.AppHost</em> project.</p>\n<pre><code class=\"language-csharp\">var owncast = builder\n    .AddContainer(name: &amp;quot;owncast&amp;quot;, image:&amp;quot;owncast/owncast&amp;quot;)\n    .WithBindMount(&amp;quot;./data&amp;quot;,&amp;quot;/app/data&amp;quot;)\n    .WithHttpEndpoint(port:8080,targetPort:8080,name:&amp;quot;admin&amp;quot;)\n    .WithHttpEndpoint(port:1935,targetPort:1935,name:&amp;quot;streaming&amp;quot;)\n    .WithExternalHttpEndpoints();\n</code></pre>\n<p>This code:</p>\n<ul>\n<li>Adds the owncast container image</li>\n<li>Mounts a local <em>data</em> directory to the <em>/app/data</em> directory in the container</li>\n<li>Maps port <code>8080</code> for the Owncast admin server and <code>1935</code> for RMTP server.</li>\n<li>Exposes the endpoints publicly</li>\n</ul>\n<h3>Embed your stream</h3>\n<p>Owncast provides the ability to <a href=\"https://owncast.online/docs/embed/\">embed your video stream onto a website</a>.</p>\n<p>Although we don't need a frontend because one is already provided by Owncast, by embedding your stream on your website you can provide a single place for your viewers to consume your content.</p>\n<p>In this case, we can treat the <em>BYOwncastAspire.Web</em> project as our website.</p>\n<p>To embed your stream to the website, add the following code to your <em>Home.razor</em> page.</p>\n<pre><code class=\"language-csharp\">&amp;lt;iframe\n  src=&amp;quot;http://localhost:8080/embed/video&amp;quot;\n  title=&amp;quot;Owncast&amp;quot;\n  height=&amp;quot;350px&amp;quot; width=&amp;quot;550px&amp;quot;\n  referrerpolicy=&amp;quot;origin&amp;quot;\n  allowfullscreen&amp;gt;\n&amp;lt;/iframe&amp;gt;\n</code></pre>\n<p>In this case, we're pointing to the endpoint listening on port <code>8080</code> of our <code>localhost</code>. When you deploy the application, you'd replace <code>src</code> with your domain.</p>\n\n<p>That's all there is in terms of configuration.</p>\n<p>To start the application:</p>\n<ol>\n<li><p>Open the terminal</p>\n</li>\n<li><p>Navigate to the <em>BYOwncastAspire.AppHost</em> project and run the following command.</p>\n<pre><code class=\"language-bash\">dotnet run\n</code></pre>\n</li>\n</ol>\n<p>This will launch you into the .NET Aspire dashboard. At this point, you can further customize your Owncast server as well as the website.</p>\n<p><img src=\"https://github.com/user-attachments/assets/c20d84d8-925a-4f80-9058-622466cb08e9\" class=\"img-fluid\" alt=\"BYOwncastAspire .NET Aspire Dashboard Resource Page\" /></p>\n\n<p>Now that you have your application running, in its current form, this application is only meant to serve as a sample of what you can do with Owncast and .NET Aspire.</p>\n<p>Some next steps might include:</p>\n<ul>\n<li><a href=\"https://owncast.online/docs/configuration/\">Change your admin password and customize your Owncast server</a></li>\n<li><a href=\"https://owncast.online/docs/storage/\">Configure object storage</a></li>\n<li><a href=\"https://owncast.online/docs/broadcasting/obs/\">Set up your steaming software</a></li>\n<li><a href=\"https://learn.microsoft.com/dotnet/aspire/deployment/azure/aca-deployment\">Deploy your application</a></li>\n<li><a href=\"https://owncast.online/thirdparty/\">Extend the app's functionality by building custom plugins and extensions</a></li>\n</ul>\n<p>The plugins and extensions are particularly interesting to me because there may even be opportunities to experiment and insert AI capabilities at various layers such as moderation, translation, accessibility, show notes, chat summaries, etc.</p>\n\n<p>In this post, I showed how you can combine the powerful live-steaming features of Owncast with the app orchestration and tooling in .NET Aspire to create rich self-hosted live-steaming applications.</p>\n<p>If you use this sample as a starting point for your own live-streaming or self-hosting explorations, <a href=\"/contact\">send me a message</a>. I'd love to hear about it.</p>\n","tags":["owncast","dotnet","indieweb","fediverse","aspire","livestreaming","twitch","youtube","blazor"]},{"title":"Configure Ollama on Dev Containers and VS Code","contentType":"posts","date":"2024-03-06 13:50","url":"/posts/install-ollama-vscode-devcontainer/","content":"<p>The source for my website is <a href=\"/github/luisquintanilla.me\">hosted on GitHub</a>. As a result, I use VS Code as the text editor to author my posts. Typically, I use <a href=\"/colophon\">Codespaces or github.dev</a> to quickly draft and publish articles to simplify the process.</p>\n<p>As part of my authoring and publishing posts, there's a few things I do like:</p>\n<ol>\n<li>Coming up with a title</li>\n<li>Creating relevant tags to help with discovery and connecting related information on my website.</li>\n<li>For long-form blog posts, I also include a description.</li>\n</ol>\n<p>This is generally relatively easy to do. However, sometimes I spend too much time coming up with something that truly summarizes and condenses the information in the post. It'd be nice if I had an assistant to help me brainstorm and workshop some of these items.</p>\n<p>In comes AI. Now, I could use something like <a href=\"https://code.visualstudio.com/docs/copilot/overview\">Copilot</a> which would work wonderfully and easily plug into my workflow. However, my website is a labor of love and I don't make any money from it. In many instances, I've designed <a href=\"/posts/receive-webmentions-fsharp-az-functions-fsadvent\">various components to be as low-cost as possible</a>.</p>\n<p>Recently, I created a post which showed <a href=\"/posts/getting-started-ollama-windows\">how to get started with Ollama on Windows</a>. In this post, I'll show how you can do the same for your Dev Container environments.</p>\n\n<p>Assuming you already have a <a href=\"https://code.visualstudio.com/docs/devcontainers/create-dev-container\">Dev Container configuration file</a>, add the following line to it:</p>\n<pre><code class=\"language-json\">&amp;quot;postCreateCommand&amp;quot;: &amp;quot;curl -fsSL https://ollama.com/install.sh | sh&amp;quot;\n</code></pre>\n<p>This command triggers when the container environment is created. This will install Ollama in your development environment. For more details, see the <a href=\"https://ollama.com/download/linux\">Ollama download instructions</a>.</p>\n\n<p>Now that Ollama is installed, it's time to start the service so you can get models and use them. To start the Ollama service on your Dev Conatiner environment, add the following line to your Dev Container configuration file.</p>\n<pre><code class=\"language-json\">&amp;quot;postStartCommand&amp;quot;: &amp;quot;ollama serve&amp;quot;\n</code></pre>\n<p>This command will run <code>ollama serve</code> when the Dev Container environment starts.</p>\n\n<p>To use Ollama, you're going to need a model. In my case, I went with <a href=\"https://ollama.com/library/phi\">Phi-2</a> because it's lightweight and space on Codespaces is limited.</p>\n<p>To get the model:</p>\n<ol>\n<li><p>Open the terminal</p>\n</li>\n<li><p>Enter the following command:</p>\n<pre><code class=\"language-bash\">ollama pull phi\n</code></pre>\n<p>After a few minutes, the model is downloaded and ready to use.</p>\n</li>\n<li><p>Enter the following command to ensure that your model is now available</p>\n<pre><code class=\"language-bash\">ollama list\n</code></pre>\n<p>The result should look like the following:</p>\n<pre><code class=\"language-text\">NAME            ID              SIZE    MODIFIED      \nphi:latest      e2fd6321a5fe    1.6 GB  6 seconds ago\n</code></pre>\n</li>\n</ol>\n<p>For more details, see the <a href=\"https://ollama.com/library\">Ollama model library</a>.</p>\n\n<p>In this post, I showed how you can configure Ollama with Dev Containers to use AI models locally in your projects. In subsequent posts, I'll show how once the service is running, I use it as part of my authoring and publishing posts. You can see a preview of it in my <a href=\"https://github.com/lqdev/luisquintanilla.me/blob/main/Scripts/ai.fsx\">scripts directory</a>. Happy coding!</p>\n","tags":["ollama","vscode","devcontainer","ai","llm","opensource","development"]},{"title":"Getting started with Ollama on Windows","contentType":"posts","date":"2024-03-05 10:32","url":"/posts/getting-started-ollama-windows/","content":"<p>Recently <a href=\"/responses/ollama-windows-preview\">Ollama announced support for Windows</a> in preview. In doing so, people who want to use AI models like Llama, Phi, and many others can do so locally on their PC. In this post, I'll go over how you can get started with Ollama on Windows.</p>\n\n<p>The first thing you'll want to do is install Ollama.</p>\n<p>You can do so by <a href=\"https://ollama.com/download/windows\">downloading the installer from the website</a> and following the installation prompts.</p>\n\n<p>Once you've installed Ollama, it's time to get a model.</p>\n<ol>\n<li><p>Open PowerShell</p>\n</li>\n<li><p>Run the following command</p>\n<pre><code class=\"language-powershell\">ollama pull llama2\n</code></pre>\n</li>\n</ol>\n<p>In this case, I'm using llama2. However, you can choose another model. You could even download many models at once and switch between them. For a full list of supported models, see the <a href=\"https://ollama.com/library\">Ollama model documentation</a>.</p>\n\n<p>Now that you have the model, it's time to use it. The easiest way to use the model is using the REST API. When you install Ollama, it starts up a server to host your model. One other neat thing is, the REST API is <a href=\"https://ollama.com/blog/openai-compatibility\">OpenAI API compatible</a>.</p>\n<ol>\n<li><p>Open PowerShell</p>\n</li>\n<li><p>Send the following request:</p>\n<pre><code class=\"language-powershell\">(Invoke-WebRequest -method POST -Body '{&amp;quot;model&amp;quot;:&amp;quot;llama2&amp;quot;, &amp;quot;prompt&amp;quot;:&amp;quot;Why is the sky blue?&amp;quot;, &amp;quot;stream&amp;quot;: false}' -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json\n</code></pre>\n<p>This command will issue an HTTP POST request to the server listening on port 11434.</p>\n<p>The main things to highlight in the body:</p>\n<ul>\n<li><em>model</em>: The model you'll use. Make sure this is one of the models you pulled.</li>\n<li><em>prompt</em>: The input to the model</li>\n<li><em>stream</em>: Whether to stream responses back to the client</li>\n</ul>\n<p>For more details on the REST API, see the <a href=\"https://github.com/ollama/ollama/blob/main/docs/api.md\">Ollama REST API documentation</a>.</p>\n</li>\n</ol>\n\n<p>In this post, I went over how you can quickly install Ollama to start using generative AI models like Llama and Phi locally on your Windows PC. If you use Mac or Linux, you can perform similar steps as those outlined in this guide to get started on those operating systems. Happy coding!</p>\n","tags":["ai","ollama","windows","llm","opensource","llama","openai","generativeai","genai"]},{"title":"Setting up your Rust development environment using Dev Containers","contentType":"posts","date":"2024-01-31 21:05","url":"/posts/setting-up-rust-dev-env-devcontainers-vscode/","content":"<p>Setting up a new development environment that's configured with all the SDKs and tools you need to get started working can be a multi-step process that is error-prone. Especially when you don't know whether you want to commit to a specific technology or don't have too much space on your device, you might not want to install everything on your device. Additionally, cleaning up and removing everything you installed isn't always 100%. This is where development environments like Dev Containers can come in handy.</p>\n\n<p>A development container (Dev Container) lets you use a container as a full development environment.</p>\n<p>Because your development environment is in a container, the environment is isolated. The builds are reproducible and disposable. This means you can quickly stand up a new clean development environments and dispose of them just as easily.</p>\n<p>For more details, see the <a href=\"https://containers.dev/\">Dev Containers website</a>.</p>\n<p>If you have Docker installed, you can run them locally. Otherwise, you can run Dev Containers using GitHub Codespaces</p>\n\n<p>A codespace is a development environment that's hosted in the cloud.</p>\n<p>For more details, see the <a href=\"https://github.com/features/codespaces\">GitHub Codespaces website</a>.</p>\n\n<ol>\n<li><p>Create a new directory.</p>\n</li>\n<li><p>Add a Dev Container configuration file called <em>.devcontainer.json</em> that contains the following content:</p>\n<pre><code class=\"language-json\">{\n    &amp;quot;name&amp;quot;: &amp;quot;Rust Development Environment&amp;quot;,\n    &amp;quot;image&amp;quot;: &amp;quot;mcr.microsoft.com/devcontainers/rust:latest&amp;quot;,\n    &amp;quot;customizations&amp;quot;: {\n        &amp;quot;vscode&amp;quot;: {\n            &amp;quot;extensions&amp;quot;: [\n                &amp;quot;rust-lang.rust-analyzer&amp;quot;\n            ]\n        }\n    }\n}\n</code></pre>\n<ul>\n<li><strong>name</strong>: A human friendly name to identify your development environment. It's not as important when you only have one environment but if you have different environments, it can be useful for differentiating between the various configurations.</li>\n<li><strong>image</strong>: For the most customization, you can provide the path to a Docker file. However, to make things easier, there are a set of prebuilt container images. In this case, I'm using the <a href=\"https://github.com/devcontainers/images/tree/main/src/rust\">Rust image</a>.</li>\n<li><strong>customizations.vscode.extensions</strong>: A list of Visual Studio Code extensions that can help you with your development. In this case, it's the <a href=\"https://marketplace.visualstudio.com/items?itemName=rust-lang.rust-analyzer\"><em>rust-analyzer</em></a>.</li>\n</ul>\n</li>\n</ol>\n<p>At minimum, this is all you need to get started.</p>\n\n<p><a href=\"https://code.visualstudio.com/docs/devcontainers/tutorial\">Visual Studio Code provides good support for Dev Containers</a>.</p>\n<ol>\n<li>Install <a href=\"https://code.visualstudio.com/download\">VS Code</a></li>\n<li>Open your directory where you configured your Dev Container in Visual Studio Code</li>\n<li>Install the <a href=\"https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers\">Dev Containers VS Code extension</a>.</li>\n<li>Open the command palette in VS Code. In the menu bar, select <strong>View &amp;gt; Command Palette.</strong></li>\n<li>Enter the following command <strong>&amp;gt;Dev Containers: Open Dev Container</strong>. This will start building your dev container. Wait for it to start.</li>\n</ol>\n\n<p>Once your Dev Container starts, check that <code>rustc</code> is installed.</p>\n<ol>\n<li>Open the terminal.</li>\n<li>Run <code>rustc -h</code>. A help message should output to the console for the Rust compiler.</li>\n</ol>\n\n<p>For simplicity, I've used the <a href=\"https://doc.rust-lang.org/rust-by-example/hello.html\">Hello World sample from the Rust By Example website</a>.</p>\n<ol>\n<li><p>Create a new file called <em>hello.rs</em>.</p>\n</li>\n<li><p>Add the following code:</p>\n<pre><code class=\"language-rust\">fn main ()\n{\n    println!(&amp;quot;Hello World!&amp;quot;);\n}\n</code></pre>\n</li>\n<li><p>Compile your program</p>\n<pre><code class=\"language-bash\">rustc hello.rs\n</code></pre>\n<p>This will generate an executable called <em>hello</em></p>\n</li>\n<li><p>Run your app</p>\n<pre><code class=\"language-bash\">./hello\n</code></pre>\n<p>The application should print out the string &amp;quot;Hello World&amp;quot; to the console.</p>\n</li>\n</ol>\n<p>That's it! You now have a Rust development environment to get started learning and building apps.</p>\n\n<p>Dev Containers make it easy for you to configure full-featured development environments. When paired with Visual Studio Code and GitHub Codespaces, they can help you focus on learning and building applications rather than setting up your environment. You can find a <a href=\"https://github.com/lqdev/rust-codespace-sandbox\">final version of the configured environment and app on GitHub</a>. Happy coding!</p>\n\n<ul>\n<li><a href=\"https://code.visualstudio.com/docs/languages/rust\">Rust in Visual Studio Code</a></li>\n<li><a href=\"https://doc.rust-lang.org/rust-by-example/index.html\">Rust by Example</a></li>\n<li><a href=\"https://code.visualstudio.com/docs/devcontainers/containers\">Developing Inside a Container</a></li>\n</ul>\n","tags":["rust","vscode","devcontainers","codespaces","programming","technology","docker","containers","github"]},{"title":"POSSE to Mastodon using RSS and Azure Logic Apps","contentType":"posts","date":"2023-12-24 16:40","url":"/posts/rss-to-mastodon-posse-azure-logic-apps/","content":"\n<p><a href=\"/notes/weblogging-rewind-2023/\">Over the past year, I've made this website the main place where I post content</a>. The easiest way to <a href=\"/feed\">subscribe to content on my website</a> is through the various RSS feeds. However, I have accounts on other platforms like X (formerly Twitter), Bluesky, and Mastodon where I'd still like to repost my content to. Since the <a href=\"https://techcrunch.com/2023/03/29/twitter-announces-new-api-with-only-free-basic-and-enterprise-levels/\">changes to the Twitter API</a>, the only place I cross-post to is Mastodon. The main reason behind it is, as of now, it's one of the few platforms that allows me to automate post creation via its REST APIs without restrictions. A large part of that is I self-host my own Mastodon instance but I assume there aren't as many restrictions when using other instances like mastodon.social. The way I automate posting is by setting up workflows using Azure Logic Apps. These workflows subscribe to my various RSS feeds and whenever a new post is published, they make an HTTP request to my Mastodon instance to create a new post. I've been doing this for some time but never got around to documenting it. This blog post goes into more details about how to set up these workflows.</p>\n\n<p><a href=\"https://indieweb.org/POSSE\">POSSE</a> is short for &amp;quot;Post on your Own Site Syndicate Elsewhere&amp;quot;. It's one of the patterns adopted within IndiWeb communities and projects. The main idea is, your website or a website you own or have administrative rights to becomes the main platform where you publish your content. Effectively, it's the cannonical version of your content. Once your content is on your website, you can optionally choose to distribute it on other platforms.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/RSS\">RSS</a> is short for Really Simple Syndication. Accorting to Wikipedia, this protocol &amp;quot;allows users and applications to access updates to websites in a standardized, computer-readable format&amp;quot;.</p>\n\n<p><a href=\"https://joinmastodon.org/\">Mastodon</a> is an <a href=\"https://github.com/mastodon/mastodon\">open-source</a> decentralized microblogging platform built on the <a href=\"https://activitypub.rocks/\">ActivityPub</a> protocol and part of the larger collective of federated systems known as the <a href=\"https://fediverse.info/\">Fediverse</a>.</p>\n\n<p>If you've used workflow automation systems like <a href=\"https://ifttt.com/\">IFTTT</a>, then you generally know what Azure Logic Apps are.</p>\n<p>A more formal definition from the Azure documentation - &amp;quot;Azure Logic Apps is a cloud platform where you can create and run automated workflows with little to no code. By using the visual designer and selecting from prebuilt operations, you can quickly build a workflow that integrates and manages your apps, data, services, and systems.&amp;quot;</p>\n<p>For more details, see the <a href=\"https://learn.microsoft.com/azure/logic-apps/logic-apps-overview\">Azure Logic Apps documentation</a>.</p>\n\n<p>Since you'll be creating Logic App Resources on Azure, you'll need an <a href=\"https://aka.ms/free\">Azure account</a>.</p>\n\n<p>There's various ways to create an Azure Logic App Resource but the easiest one for this relatively simple workflow is using the Azure Portal. For more details, see the <a href=\"https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#create-a-consumption-logic-app-resource\">create a consumption logic app resource documentation</a>.</p>\n<p>For the most part you can leave the defaults as is. Since the intended use for this workflow is personal and I don't need enterprise features, I chose to create my logic app using the consumption plan.</p>\n<p>When prompted to choose <strong>Plan Type</strong>, select <strong>Consumption</strong>.</p>\n\n<p>The workflow is relatively simple. Whenever a new item is posted to an RSS feed, make an HTTP POST request to the Mastodon API</p>\n<pre class=\"mermaid\">flowchart TD\n    A[\"RSS\"] --> B[\"HTTP (Mastodon API)\"]\n</pre>\n<p>Once your Logic App resource deploys to Azure, create a new logic app using the Blank Template. For more details, see the <a href=\"https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#select-the-blank-template\">select a blank template documentation</a>.</p>\n<p>This will launch you into the Logic app designer UI where you can begin to configure your workflow.</p>\n<h3>RSS trigger</h3>\n<p>The first thing you'll want to do is set up the trigger that initiates the workflow. The trigger in this case will be new posts on an RSS feed.</p>\n<p>In the Logic app designer, search for <em>RSS</em> and add it as a trigger.</p>\n<p>Once the RSS trigger is added to your workflow, configure it as follows:</p>\n<ul>\n<li><p><strong>The RSS feed URL</strong>: The feed you want want to get posts from. In this case, I'm using my blog posts <a href=\"/posts/feed.xml\">feed</a>.</p>\n<p><strong>TIP: The RSS feed can be ANY feed. So if you wanted to subscribe and repost items from your favorite blog or website, you can as well. Just note though that if you don't own the content it might not be something the author wants you doing and for websites that update multiple times a day, it might produce a lot of noise in your feed. Other than that though, subscribe and post away!</strong></p>\n</li>\n<li><p><strong>How often do you want to check for items?</strong>: The frequency at which you want to poll the RSS feed and check for new posts. In my case, I don't post very often but I do want my posts to be published on Mastodon within an hour of publishing. Therefore, I chose 1 hour as the update frequency.</p>\n</li>\n</ul>\n<h3>HTTP action</h3>\n<p>Now that your trigger is configured, it's time to do someting with the latest posts. In this case, since I want to create a new post on Mastodon, I can do so via their REST API.</p>\n<h4>Get Mastodon credentials</h4>\n<p>One thing that you'll need to publish posts to Mastodon is an application token. The easiest way to get one is using the Web UI.</p>\n<p>To get your app credentials through the web UI:</p>\n<ol>\n<li>In the Mastodon Web UI, select <strong>Preferences</strong>.</li>\n<li>In the preferences page, select <strong>Development</strong>.</li>\n<li>In the development preferences tab, select <strong>New application</strong>.</li>\n<li>In the new application page:</li>\n</ol>\n<ul>\n<li>Provide a name for your application</li>\n<li>Choose the <strong>write:statuses</strong> scope checkbox. For details on required scopes to post statuses, see the <a href=\"https://docs.joinmastodon.org/methods/statuses/#create\">Post a new status REST API documentation</a></li>\n<li>Select <strong>Submit</strong></li>\n</ul>\n<p>If successful, this will create credentials and a token you can use to send authenticated requests to the Mastodon API.</p>\n<h4>Configure HTTP action</h4>\n<p>Now that you have your credentials, you can configure your HTTP action in the Logic Apps workflow.</p>\n<p>In the Logic App designer:</p>\n<ol>\n<li>Select <strong>+ New Step</strong>.</li>\n<li>Search for <em>HTTP</em> and add it as an action.</li>\n<li>Once the HTTP action is added, configure it as follows:</li>\n</ol>\n<ul>\n<li><p><strong>Method</strong>: The HTTP Method. To create statuses, choose <strong>POST</strong> from the dropdown.</p>\n</li>\n<li><p><strong>URI</strong>: The endpoint to make the requests to. For mastodon, it's <code>https://&amp;lt;HOST&amp;gt;/api/v1/statuses</code>. Make sure to replace <code>&amp;lt;HOST&amp;gt;</code> with your Mastodon instance. In my case, it's <code>toot.lqdev.tech</code> since that's where I host my mastodon instance.</p>\n</li>\n<li><p><strong>Headers</strong>: HTTP Headers to use. In this case, set the following:</p>\n<table class=\"table\">\n<thead>\n<tr>\n<th>Key</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Content-Type</td>\n<td>application/x-www-form-urlencoded</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li><p><strong>Queries</strong>: URL Query Parameters to add to the request. This is where you'll set your access token.</p>\n<table class=\"table\">\n<thead>\n<tr>\n<th>Key</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>access_token</td>\n<td><code>[YOUR-ACCESS-TOKEN]</code></td>\n</tr>\n</tbody>\n</table>\n<p>Make sure to replace <code>[YOUR-ACCESS-TOKEN]</code> with the token credential generated in the Mastodon Web UI.</p>\n</li>\n<li><p><strong>Body</strong>: The content to be published in your Mastodon post. This is entirely up to you though at minumum, you'll add the following to the text feed. <code>status=&amp;lt;YOUR-CONTENT&amp;gt;</code>. <code>&amp;lt;YOUR-CONTENT&amp;gt;</code> is what will be displayed in the Mastodon post.</p>\n<p>One of the nice things about Azure Logic Apps is, properties from previous steps are available to you in subsequent steps. Since our trigger is an RSS feed, we can get access to the feed and item properties of our feed in the HTTP action. If all you wanted was to post the tile and URL, you can do that using the <strong>Feed title</strong> and <strong>Primary Feed Link</strong> properties. For more details, see the <a href=\"https://learn.microsoft.com/connectors/rss/\">RSS connector documentation</a>.</p>\n</li>\n</ul>\n\n<p>That's it! Now you just need to select <strong>Save</strong> in the Logic app designer page.</p>\n<p>Once it's saved, click <strong>Run trigger</strong> which will kick off your trigger. If you have anything recent to publish and everything is configured correctly, it should show up in your Mastodon feed.</p>\n\n<p>By publishing content on your own website first, you're in full control of your content. Regardless of which platforms come and go, you won't have to adjust to those changes because your content is not locked in to those platforms. However, that doesn't mean you can't also publish your content there. Using protocols like RSS make it easy to subscribe to updates on your website. Using REST APIs provided by the respective platforms, you can automate publishing these updates. To further automate and simplify this process, you can use services like Azure Logic Apps to make publishing to all places easy.</p>\n","tags":["azure","mastodon","indieweb","logicapps","fediverse","posse","internet","web","blogging","blog","automation","programming","rss"]},{"title":"Computing website metrics using GitHub Actions","contentType":"posts","date":"2023-12-24 13:24","url":"/posts/website-metrics-github-actions/","content":"\n<p>I recently posted my <a href=\"/notes/weblogging-rewind-2023/\">(We)blogging Rewind</a> and <a href=\"/notes/weblogging-rewind-2023-continued\">(We)blogging Rewind Continued</a> for 2023. In these posts, I discuss some analysis I did of my posting behaviors over the past year on this website. I don't use platforms like Google Analytics to track visitors. I used to self-host a <a href=\"https://www.goatcounter.com/\">GoatCounter</a> instance but personally I don't really care about traffic so I got rid of that. There are some standard reports I get from my <a href=\"https://learn.microsoft.com/azure/cdn/cdn-advanced-http-reports\">CDN provider on Azure</a>, but again I don't really care about those metrics. What I do care about though is my output and understanding what I'm publishing and the topics that were important to me at any given time. In the case of those blog posts, it was for the year 2023. Given that I had already done the analysis and had written the script, I thought, why not automate it and run it on a more regular basis to have monthly summaries. Since my blog and scripts are already on GitHub, it makes sense to create a GitHub Action workflow. In this post, I discuss in more details what my post analytics script does and how I configured my workflow in GitHub Actions to run the script on the first of every month.</p>\n\n<p>This script loads the various posts on my website and computes aggregate metrics based on post types and their metadata. You can find the full script at <a href=\"/resources/snippets/lqdev-me-website-post-metrics\">stats.fsx</a>.</p>\n<h3>Loading files</h3>\n<p>The following are convenience functions which I use as part of my website build process.</p>\n<p>In general these functions:</p>\n<ul>\n<li>Load the individual post files</li>\n<li>Parse the content and YAML metadata</li>\n</ul>\n<pre><code class=\"language-fsharp\">let posts = loadPosts()\nlet notes = loadFeed ()\nlet responses = loadReponses ()\n</code></pre>\n<p>Instead of building new custom functions, I can repurpose them and apply additional transformations to compute aggregate statistics.</p>\n<h3>Computing aggregate statistics</h3>\n<p>Once the posts are loaded, I apply transformations on the collections to compute aggregate metrics:</p>\n<h4>Annual post counts</h4>\n<p>The following are annual aggreagates of blog posts, notes, and responses.</p>\n<h5>Blog posts</h5>\n<p>Takes the blog post collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order.</p>\n<pre><code class=\"language-fsharp\">let postCountsByYear = \n    posts\n    |&amp;gt; Array.countBy (fun (x:Post) -&amp;gt; DateTime.Parse(x.Metadata.Date) |&amp;gt; _.Year)\n    |&amp;gt; Array.sortByDescending fst \n</code></pre>\n<h5>Notes</h5>\n<p>Takes the note collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order.</p>\n<pre><code class=\"language-fsharp\">let noteCountsByYear = \n    notes\n    |&amp;gt; Array.countBy (fun (x:Post) -&amp;gt; DateTime.Parse(x.Metadata.Date) |&amp;gt; _.Year)\n    |&amp;gt; Array.sortByDescending fst\n</code></pre>\n<h5>Responses</h5>\n<p>Takes the response collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order.</p>\n<pre><code class=\"language-fsharp\">let responseCountsByYear = \n    responses\n    |&amp;gt; Array.countBy (fun (x:Response) -&amp;gt; DateTime.Parse(x.Metadata.DatePublished) |&amp;gt; _.Year)\n    |&amp;gt; Array.sortByDescending fst\n</code></pre>\n<h4>Response counts by type</h4>\n<p>Takes the response collection, parses the published date, filters it for the current year, and computes counts by the post type (reply, bookmark, reshare, star).</p>\n<pre><code class=\"language-fsharp\">let responsesByType = \n    responses\n    |&amp;gt; Array.filter(fun x -&amp;gt; (DateTime.Parse(x.Metadata.DatePublished) |&amp;gt; _.Year) = DateTime.UtcNow.Year)\n    |&amp;gt; Array.countBy(fun x -&amp;gt; x.Metadata.ResponseType)\n    |&amp;gt; Array.sortByDescending(snd)\n</code></pre>\n<h4>Tag counts (responses)</h4>\n<p>Takes the response collection, parses the published date, filters it for the current year, and computes counts by the tag name, and sorts in descending order using the count.</p>\n<pre><code class=\"language-fsharp\">let responsesByTag = \n    responses\n    |&amp;gt; Array.filter(fun x -&amp;gt; (DateTime.Parse(x.Metadata.DatePublished) |&amp;gt; _.Year) = DateTime.UtcNow.Year)\n    |&amp;gt; Array.collect(fun x -&amp;gt; \n            match x.Metadata.Tags with\n            | null -&amp;gt; [|&amp;quot;untagged&amp;quot;|]\n            | [||] -&amp;gt; [|&amp;quot;untagged&amp;quot;|]\n            | _ -&amp;gt; x.Metadata.Tags\n        )\n    |&amp;gt; Array.countBy(fun x -&amp;gt; x)\n    |&amp;gt; Array.sortByDescending(snd)\n</code></pre>\n<h4>Domain counts (responses)</h4>\n<p>Takes the response collection, parses the published date, filters it for the current year, and computes counts by the target URL host name, and sorts it in descending order using the count.</p>\n<pre><code class=\"language-fsharp\">let responsesByDomain = \n    responses\n    |&amp;gt; Array.filter(fun x -&amp;gt; (DateTime.Parse(x.Metadata.DatePublished) |&amp;gt; _.Year) = DateTime.UtcNow.Year)\n    |&amp;gt; Array.countBy(fun x -&amp;gt; Uri(x.Metadata.TargetUrl).Host)\n    |&amp;gt; Array.sortByDescending(snd)\n</code></pre>\n<h3>Displaying counts</h3>\n<p>Since the <code>countBy</code> function is the one used to compute the counts, this produces a tuple. The tuple though could be <code>string</code> or <code>int</code>. Therefore, I set the collection of entry counts to use a generic <code>'a</code> for the first item in the tuple. I'm also able to control using <code>n</code> whether to display the entire collection by using <code>-1</code> as input or a limit when <code>n &amp;gt;= 0</code>.</p>\n<pre><code class=\"language-fsharp\">let printEntryCounts&amp;lt;'a&amp;gt; (title:string) (entryCounts:('a * int) array) (n:int) = \n    printfn $&amp;quot;{title}&amp;quot;\n\n    match n with \n    | n when n = -1 -&amp;gt; \n        entryCounts\n        |&amp;gt; Array.iter(fun x -&amp;gt; printfn $&amp;quot;{fst x} {snd x}&amp;quot;)\n        |&amp;gt; fun _ -&amp;gt; printfn $&amp;quot;&amp;quot;\n    | n when n &amp;gt;= 0 -&amp;gt; \n        entryCounts\n        |&amp;gt; Array.take n\n        |&amp;gt; Array.iter(fun x -&amp;gt; printfn $&amp;quot;{fst x} {snd x}&amp;quot;)\n        |&amp;gt; fun _ -&amp;gt; printfn $&amp;quot;&amp;quot;\n</code></pre>\n<p>The result of running this script produces the following results:</p>\n<pre><code class=\"language-text\">Blogs\n2023 5\n2022 7\n\nNotes\n2023 34\n2022 36\n\nResponses\n2023 216\n2022 146\n\nResponse Types\nbookmark 151\nreshare 48\nreply 10\nstar 7\n\nResponse Tags\nai 104\nllm 42\nuntagged 41\nopensource 31\ninternet 17\n\nDomains\ngithub.com 15\nhuggingface.co 11\narxiv.org 10\nopenai.com 6\nwww.theverge.com 4\n</code></pre>\n\n<p>The workflow file is a GitHub Actions workflow which you can find in my <a href=\"https://github.com/lqdev/luisquintanilla.me/blob/main/.github/workflows/stats.yml\">website repo</a>.</p>\n<h3>Triggers</h3>\n<p>I don't really want the script to run every time I publish my website. Instead, I just want to have these aggregate values computed on a monthly basis. Optionally though, I'd like to be able to run ad-hoc reports and trigger this job manually.</p>\n<p>The triggers in my workflow file look like the following:</p>\n<pre><code class=\"language-yaml\">schedule: \n  - cron: '30 0 1 * *'\nworkflow_dispatch: \n</code></pre>\n<p>Using cron job syntax, I use the <code>schedule</code> trigger to configure the script to run at 12:30 AM on the 1st day of every month.</p>\n<p>The <code>workflow_dispatch</code> trigger is there so I can manually trigger this job.</p>\n\n<p>The steps in the workflow file are the following:</p>\n<ul>\n<li><p>Check out the repo</p>\n<pre><code class=\"language-yaml\">- uses: actions/checkout@v2\n</code></pre>\n</li>\n<li><p>Install the .NET 8 SDK</p>\n<pre><code class=\"language-yaml\">- name: Setup .NET SDK 8.x\n  uses: actions/setup-dotnet@v1.9.0\n  with: \n    dotnet-version: '8.0.x'    \n</code></pre>\n</li>\n<li><p>Restore dependencies</p>\n<pre><code class=\"language-yaml\">- name: Install dependencies\n  run: dotnet restore    \n</code></pre>\n</li>\n<li><p>Build the project</p>\n<pre><code class=\"language-yaml\">- name: Build project\n  run: dotnet build --no-restore  \n</code></pre>\n</li>\n<li><p>Run the script and display metrics</p>\n<pre><code class=\"language-yaml\">- name: Display Post Metrics\n  run: dotnet fsi Scripts/stats.fsx\n</code></pre>\n</li>\n</ul>\n\n<p>If you have scripts that you run on a repo on a fairly regular basis, consider using GitHub Actions to automate the execution of these scripts. Happy coding!</p>\n","tags":["github","cicd","website","blog","blogging","web","internet","fsharp","dotnet"]},{"title":"Using Generative AI to produce Spotify Clips","contentType":"posts","date":"2023-12-23 18:30","url":"/posts/generative-ai-spotify-clips/","content":"\n<p>While listening to music on Spotify last night, this thought hit me. It's been cooking for some time so now you get to read about it. Spotify and YouTube, if you build this into your platforms for songs or podcasts, <a href=\"/contact\">contact me</a> for info on where to send the check 😂. As the title of the post suggests, this is a thought experiment on how you might use Generative AI to produce Spotify Clips. This is not limited to songs but could also be extended to audio-only podcasts. However, given the long-form nature of podcasts it might be extremely resource intensive to produce clips per episode. This post is not exactly a how-to guide, though there are some practical examples of how some of the steps in the workflow might work using <a href=\"https://copilot.microsoft.com/\">Microsoft Copilot (formerly Bing Chat)</a>.</p>\n\n<p>Let's start off by talking about what Spotify Clips are. Back in March of 2023, <a href=\"https://artists.spotify.com/blog/spotify-clips-get-started-short-form-video-stream-on\">Spotify introduced Clips to their platform</a>. Spotify describes <a href=\"https://clips.byspotify.com/\">Clips</a> as &amp;quot;short, under-30-second vertical videos that are uploaded directly to Spotify for Artists. They’re designed to put you and your music front-and-center, so they can be attached to your artist profile, to a song, to an album, or to an upcoming release.&amp;quot;. When paired with a song, you can think of them as short music videos. Music videos being the videos made to accompany songs that played on TV channels like Music Television (MTV) before Jersey Shore and Catfish. Music videos are still in production today and they generally serve the same purpose. They provide a visual representation of the story being told by the song.</p>\n\n<p>An overwhelming majority of Generative AI models only generate text. With models like DALL-E and Stable Diffusion, you can also generate images. Other models to generate audio and video have started coming on the scene. Most recently, Google announced <a href=\"https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html\">VideoPoet</a>, a large language model for zero-shot video generation. There's many interesting techniques that VideoPoet introduces but I'll keep it to one of the more simple scenarios and the technique used in this post, text-to-image. Given a text prompt, VideoPoet can generate by default, a short 2-second clip. You can use VideoPoet as well to extend a clip so you could go well beyond the default 2 seconds. That however is outside the scope of this post. With that in mind, let's talk about how we might use a Generative AI model and apply it to the context of Spotify Clips.</p>\n\n<p>The following is a rough idea of how you might use song information like lyrics to</p>\n<pre class=\"mermaid\">flowchart TD\n  A[\"Get song lyrics\"] --> B[\"Generate scenes and script\"]\n  B --> C[\"Generate prompts for each scene\"]\n  C --> D[\"Generate video clips\"]\n  D --> E[\"(Optional) Edit clips\"]\n  D --> F[\"Stitch clips together\"]\n  E-->F\n</pre>\n<p>This is roughly what Google did in their VideoPoet samples. They &amp;quot;...produced a short movie composed of many short clips generated by the model. For the script, we asked Bard to write a series of prompts to detail a short story about a traveling raccoon. We then generated video clips for each prompt, and stitched together all resulting clips&amp;quot;. The resulting video looks like the following:</p>\n<p><a href=\"https://www.youtube.com/watch?v=70wZKfx6Ylk\" title=\"An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities\"><img src=\"http://img.youtube.com/vi/70wZKfx6Ylk/0.jpg\" class=\"img-fluid\" alt=\"An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities\" /></a></p>\n<p>One thing to note is that the videos generated are YouTube Shorts which are vertical as well. This makes it easier to produce videos in the format Spotify expects Clips to be in.</p>\n\n<p>Now that we know the general process, what does it actually look like in practice? To help me with that, I used Microsoft Copilot.</p>\n<h3>Generate scenes and script</h3>\n<p>In the example shared by Google, they used a prompt to generate a script made up of a series of prompts. In the case of songs on Spotify, we can use that as context and the starting point for our script. From there, we can ask Copilot to compose scenes for the verses in the song. We can also make sure as these scenes are generated, that there is a smooth transition between them and the storytelling is consistent.</p>\n<p>Let's take a look at what this might look like using the Chicano Batman song &amp;quot;A Hundred Dead and Loving Souls&amp;quot;. Why that one? Just check out my <a href=\"/notes/spotify-wrapped-2023\">Spotify Wrapped</a>. The general theme of that song is about the cycles of life experienced across millenia and generations.</p>\n<p><strong>Prompt</strong></p>\n<p>Given the following prompt:</p>\n<pre><code class=\"language-text\">Using the song information below as context, generate a script for a story that visually represents the song lyrics. Ensure that the script is broken down into individual scenes and the scenes are linear and cohesively transition.\n\n&amp;lt;song&amp;gt;\n&amp;lt;title&amp;gt;A Hundred Dead and Loving Souls&amp;lt;/title&amp;gt;\n&amp;lt;lyrics&amp;gt;\nI am the sand stuck under miles of rock\nwatching the dry air sweep away my thoughts\nas we travel protected by shards of glass\ndriving through the history of a million memories\n\nIn a sea of a hundred dead and loving souls\nIn a sea of a hundred dead and loving souls\nIn a sea of a hundred dead and loving souls\nClimbing a thousand unclimbed trees\nI don't know I want to know how it would feel\nto be on top of one of these!\n\nI am the root\nquickly ascending\nup through the surrounding dirt\nquickly devoured by a hungry sun\nquickly devoured by a hungry son\nwho is my father\nand we are alike\nwe look alike we talk alike\nwe are alike\nwe are alike\nwe are alike we look alike\nwe talk alike\n\nThats why Im hungry\nlike the bellowing sky\nthis is no lie\nas sure as light\nentering eye\nwant to stay high\neverything's cool too cool so cool\nand I remember that everything falls\n\nSo I take a look around me now, as I speak\nand I imagine a disinterested crowd\ntalking aloud\nand I think of what I want most\nand the crowd disappears\nand a fixated gaze envelopes my wondering mind\nand then she recruits and takes me to this place\nI've been there before and that's where I saw her face\n\nIn a sea of a hundred dead and loving souls\nIn a sea of a hundred dead and loving souls\nIn a sea of a hundred dead and loving souls\nClimbing a thousand unclimbed trees\nI don't know I want to know how it would feel\nto be on top of one of these!\n&amp;lt;/lyrics&amp;gt;\n&amp;lt;/song&amp;gt;\n\nStory: \n</code></pre>\n<p><strong>Response</strong></p>\n<p>The response looks like the following:</p>\n<pre><code class=\"language-text\"># Script for A Hundred Dead and Loving Souls\n\n## Scene 1: The Sand\n- A close-up shot of a grain of sand under a microscope, showing its intricate structure and texture.\n- The camera zooms out to reveal that the sand is part of a vast desert landscape, with dunes and rocks stretching to the horizon.\n- A voice-over of the protagonist (a young man) narrates the first verse of the song, expressing his feelings of being trapped and forgotten under the weight of the world.\n- The camera pans to show a car driving on a dusty road, with the protagonist and his father inside. They are both wearing sunglasses and hats to protect themselves from the sun.\n- The car is filled with boxes and bags, indicating that they are moving to a new place.\n- The father is driving and listening to the radio, while the protagonist is looking out the window, bored and restless.\n- The radio plays a news report about a nuclear fusion experiment in South Korea, followed by a song by a popular band.\n- The father comments on the news, saying that it is a breakthrough for humanity and that he hopes to see it in his lifetime.\n- The protagonist does not respond, but thinks to himself that he does not care about anything anymore.\n\n## Scene 2: The Sea\n- A montage of shots showing the protagonist and his father arriving at their new home, a small town by the sea.\n- The father is excited and optimistic, while the protagonist is indifferent and detached.\n- The father unpacks their belongings and sets up their new house, while the protagonist wanders around the town, exploring the beach, the pier, and the streets.\n- The protagonist sees a group of teenagers hanging out at a skate park, laughing and having fun. He feels out of place and lonely.\n- The protagonist walks to the edge of the pier, where he sees a sign that says &amp;quot;No diving&amp;quot;.\n- He looks down at the water, where he sees hundreds of dead fish floating on the surface, victims of a recent oil spill.\n- He hears the chorus of the song in his head, imagining himself diving into the sea of a hundred dead and loving souls, and climbing a thousand unclimbed trees.\n- He wonders what it would feel like to be free and alive, instead of being stuck and dead inside.\n\n## Scene 3: The Root\n- A flashback scene, showing the protagonist as a child, playing in a garden with his father.\n- The father is a botanist, and he teaches his son about different plants and flowers, showing him how to care for them and appreciate their beauty.\n- The son is curious and eager, and he asks his father many questions about the natural world.\n- The father tells his son that he is a root, quickly ascending up through the surrounding dirt, and that he will grow into a strong and beautiful tree one day.\n- The son smiles and hugs his father, saying that he loves him and that he wants to be like him.\n- The father smiles and hugs his son back, saying that he loves him too and that he is proud of him.\n\n## Scene 4: The Sun\n- A flash-forward scene, showing the protagonist as a young adult, working in a laboratory with his father.\n- The father is a nuclear physicist, and he is working on a project to harness the power of nuclear fusion, the same process that fuels the sun.\n- The son is his assistant, and he helps his father with the experiments and calculations, following his instructions and commands.\n- The father tells his son that he is a hungry sun, quickly devoured by a hungry son, and that he will make a great contribution to science and humanity one day.\n- The son nods and agrees, but he does not look happy or satisfied. He feels pressured and obligated to follow his father's footsteps, even though he does not share his passion or vision.\n- The son thinks to himself that he is hungry, like the bellowing sky, and that he wants to stay high, away from the reality and expectations that weigh him down.\n\n## Scene 5: The Crowd\n- A present-day scene, showing the protagonist at a party, surrounded by a crowd of people.\n- The party is hosted by one of the teenagers that he saw at the skate park, who invited him out of pity and curiosity.\n- The party is loud and chaotic, with music, drinks, and drugs everywhere.\n- The protagonist feels uncomfortable and anxious, and he does not fit in with the crowd. He does not know anyone, and no one knows him.\n- He tries to make small talk with some of the guests, but they are either rude, bored, or high, and they do not pay attention to him.\n- He feels invisible and insignificant, and he wishes he could disappear.\n- He takes a look around him, as he speaks, and he imagines a disinterested crowd, talking aloud, and he thinks of what he wants most.\n\n## Scene 6: The Gaze\n- A close-up shot of a pair of eyes, staring intently at the protagonist.\n- The eyes belong to a girl, who is also at the party, but who is different from the rest of the crowd. She is quiet and mysterious, and she has a captivating and mysterious aura.\n- The girl notices the protagonist, and she is intrigued by him. She sees something in him that no one else sees, something that she relates to and understands.\n- She walks up to him, and she introduces herself. She says her name is Luna, and that she likes his song. She says she heard him humming it to himself, and that she knows the lyrics by heart.\n- The protagonist is surprised and confused, and he asks her how she knows his song. He says he wrote it himself, and that he never shared it with anyone.\n- The girl smiles and says that she knows his song, because she wrote it too. She says she has a gift, or a curse, depending on how you look at it. She says she can hear the songs that people sing in their souls, and that she can sing them back to them.\n- The protagonist is shocked and amazed, and he asks her to prove it. He asks her to sing his song, or any song that he knows.\n- The girl nods and agrees, and she sings his song, word for word, note for note, perfectly. She sings it with emotion and expression, as if she wrote it herself.\n- The protagonist is speechless and mesmerized, and he feels a connection with the girl that he never felt with anyone before. He feels like she knows him, and he wants to know her.\n- The girl stops singing, and she looks at him with a fixated gaze, that envelopes his wondering mind. She says she likes him, and she asks him if he wants to go with her to a place that she knows.\n- The protagonist nods and agrees, and he follows her out of the party, leaving the crowd behind.\n\n## Scene 7: The Place\n- A montage of shots showing the protagonist and the girl driving to a secluded spot in the woods, where they park their car and get out.\n- The spot is a clearing, where there is a large tree, with a rope swing hanging from one of its branches.\n- The tree is the only one of its kind in the area, and it stands out from the rest of the forest. It is tall and majestic, with green leaves and red flowers.\n- The girl tells the protagonist that this is her place, and that she comes here often to escape from the world. She says she loves this tree, and that she calls it Luna, after herself.\n- The protagonist tells the girl that this is his place too, and that he has been here before. He says he saw this tree in his dreams, and that he always wanted to climb it.\n- The girl smiles and says that they have more in common than they thought, and that they are meant to be together. She says she wants to show him something, and she asks him to follow her.\n- The girl climbs the tree, using the rope swing as a support. She is agile and graceful, and she reaches the top in no time.\n- The protagonist follows her, using the rope swing as well. He is clumsy and nervous, and he struggles to keep up with her.\n- The girl reaches the top of the tree, where she finds a small platform, made of wood and rope. She sits on the platform, and she looks at the view.\n- The view is breathtaking, and it shows the sea, the town, and the sky, all in one panorama. The sun is setting, and it casts a golden glow on everything.\n- The girl waves at the protagonist, and she invites him to join her. She says this is the best place to watch the sunset, and that she wants to share it with him.\n- The protagonist reaches the top of the tree, where he joins the girl on the platform. He sits next to her, and he looks at the view. He is awestruck, and he agrees that this is the best place to watch the sunset.\n- The girl leans in and kisses the protagonist, and he kisses her back. They embrace each other, and they watch the sunset together.\n- The chorus of the song plays in the background, as the scene fades to black.\n</code></pre>\n<p>Not too bad for a first pass. From here you might want to continue tweaking the different scenes but at least you have a starting point.</p>\n<h3>Generate prompts for each scene</h3>\n<p>Using the text-to-image technique, I first need to generate prompts to guide the model in the video generation process. In the case of this sample, you could take each of the bullet points in the scenes and generate a video for each of them.</p>\n<p>At the time of this writing, I didn't have access to a video generation model and I don't think VideoPoet is open to the public yet. Therefore, I had to slightly modify what I was generating. Instead of generating a video, I used Copilot to generate images using DALL-E. The concept is the same, except instead of generating videos, I'm generating still images.</p>\n<p>I could've applied the same concept of one image per bullet point. However, for brevity, I chose to summarize all of the bullet points in a scene into a single prompt.</p>\n<p><strong>Prompt</strong></p>\n<p>Given the following prompt:</p>\n<pre><code class=\"language-text\">Using storyboarding techniques, for each scene, generate a single prompt to use as input for DALL-E to generate an image. Since there are 7 scenes, there should be only 7 prompts. Distill as much of the concepts from each of the bullet points in each scene.\n</code></pre>\n<p><strong>Response</strong></p>\n<p>The results from that prompt are the following:</p>\n<pre><code class=\"language-text\">Scene 1: The Sand\n    A grain of sand under a microscope and a car driving on a dusty road in a desert\nScene 2: The Sea\n    A boy standing on a pier looking at a sea of dead fish and a sign that says “No diving”\nScene 3: The Root\n    A father and a son playing in a garden with different plants and flowers\nScene 4: The Sun\n    A father and a son working in a laboratory with a nuclear fusion reactor\nScene 5: The Crowd\n    A boy at a party surrounded by a crowd of people with music, drinks, and drugs\nScene 6: The Gaze\n    A girl with a pair of eyes that can hear the songs that people sing in their souls\nScene 7: The Place\n    A boy and a girl sitting on a platform on top of a tree with a rope swing and a view of the sea, the town, and the sunset\n</code></pre>\n<p>Again, not too bad for a first pass. The main points of the scene are captured. I can already foresee there are a few scenes like &amp;quot;The Sand&amp;quot; which might be difficult to illustrate in a single image.</p>\n<h3>Generate images (video clips)</h3>\n<p>Now that we have individual prompts, it's time to generate some images. Again, I'm doing this because I don't have access to a video generation model but the process would be generally the same.</p>\n<p><strong>Prompt</strong></p>\n<pre><code class=\"language-text\">Use the prompts for each of the scenes to generate images with DALL-E. Render them as pencil art.\n</code></pre>\n<p><strong>Response</strong></p>\n<p>For brevity, I'll only include some of the images generated:</p>\n<p>&amp;quot;The Sand&amp;quot;</p>\n<p><img src=\"/files/images/sand-car-dirt-road.jpg\" class=\"img-fluid\" alt=\"AI genreated image of a grain of sand under a microscope and a car driving on a dusty road in a desert\" /></p>\n<p>&amp;quot;The Sea&amp;quot;</p>\n<p><img src=\"/files/images/child-dock.jpg\" class=\"img-fluid\" alt=\"AI generated image of a boy standing on a pier looking at a sea of dead fish and a sign that says “No diving”\" /></p>\n<p>&amp;quot;The Root&amp;quot;</p>\n<p><img src=\"/files/images/father-son-playing.jpg\" class=\"img-fluid\" alt=\"AI generated image of a father and a son playing in a garden with different plants and flowers”\" /></p>\n<p>The pencil art part was something I thought would look nice to illustrate this instead of digital art or photorealism. That was a personal choice though.</p>\n<p>As we can see from the images, in general they're not too bad. When not condensed into a single image though and you have an image or video per frame in a scene, the generated content can be richer.</p>\n<h3>Edit and stitch videos</h3>\n<p>For the image generation example, this seems like a natural stopping point. However, with videos, you can imagine using AI to edit these videos. The VideoPoet post mentions how you might be able to do that. Part of that editing might also include lengthening the video clips. Once you're happy with your videos, you would then stitch them together to compose a larger clip.</p>\n\n<p>Tons! I used the simplest tools available at hand to validate whether my idea is feasible.</p>\n<h3>Videos</h3>\n<p>The most obvious improvement here would be to actually generate the videos. Whether that's using VideoPoet or another video generation model, the whole point is to end up with a video clip to upload to Spotify.</p>\n<h3>Prompts</h3>\n<p>The prompts I used were not complex at all. I tried to keep them as simple as possible and in many cases, I wasn't the one providing the prompts, Copilot generated them for me. I would try to use more complex prompts to further guide the model in the formats I expect the outputs to be in and give more details for the model to generate better results.</p>\n<h3>Code</h3>\n<p>All of this was done using Copilot in the browser. As a starting point to validate my ideas this was fine. However, this approach is not composable, testable, reproducible, or scalable. I'd convert these steps to code in order to get all of the things Copilot doesn't give me.</p>\n<h3>Timestamp sync</h3>\n<p>One of the features Spotify offers is lyrics that are synced to the song. Similarly, being able to sync these generated clips to the song would be important so the visuals match the</p>\n\n<h3>Consistency</h3>\n<p>One challenge that someone brought up when I was explaining this idea was, how do you ensure that the visuals are consistent throughout all of the video clips.</p>\n<p>That is a great question I don't have a great answer for, though I have ideas.</p>\n<p>If there aren't many changes throughout scenes, you could just opt to keep lengthening the video clip.</p>\n<p>If there are flashbacks or different visuals between scenes, you might achieve consistency by  using the image-to-video modality. Using a snapshot from a frame in a previous video clip, you might try to edit that snapshot to use in between transitions and then lengthen the video to produce the next scene.</p>\n<h3>Power Consumption</h3>\n<p>As fascinating as this is, is it worth the energy and cooling demands it requires when using large models? Maybe not in its current form, but maybe it makes sense when using smaller models or more power efficient compute.</p>\n<h3>People</h3>\n<p>For music videos, there's a human element to it both from the performers and the people who produce them. Again, the Generative AI is impressive and for someone like me without any artistic skills, it can be an empowering tool. Although I'm tempted to see what an <a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\">AI Rickroll</a> might look like, I prefer the human version. 🙂</p>\n\n<p>In this post I discussed how you might use Generative AI models like VideoPoet to generate short videos for Spotify Clips. Since I don't have acess to VideoPoet or other video generation models at this time, I decided to show practical image generation examples instead using models like DALL-E to illustrate the general concepts. It's not wihtout its challenges and there's a ton of improvements I can think of but it was a fun experiment regardless. What else would you use these Generative AI models for?</p>\n<p>You can find an addendum to this post <a href=\"/notes/genai-spotify-clips-instrumental\">here</a>.</p>\n","tags":["ai","video","chicanobatman","generativeai","genai","spotify","google","microsoft","bing","copilot","dalle","image","prompt","projectidea","music","podcasts","llm"]},{"title":"AI like it's 1999 or 1899","contentType":"posts","date":"2023-12-15 08:56","url":"/posts/ai-1999-1899/","content":"\n<p>In an earlier note, I talked about the natural progression of technology which goes from <a href=\"/notes/ai-abundance-scarcity-cycle-repeats-rss-enclosure-use-case\">scarcity to abundance</a> over time and AI is no different. That was in the context of media on the internet and paying per minute with early cell phones. One of the original challenges with AI was context length and token limits. It's becoming less relevant as context sizes continue to increase. However, it's not like you can fit the entire Harry Potter series either. So you still need to cleverly manage your context size especially when using documents and chat history as part of that context. Historically, people have used abbreviations to compress messages either because of cost or medium constraints. That's what this post explores in the context of AI.</p>\n\n<p>Below is a conversation Microsoft Copilot (formerly Bing Chat) using some information from the article <a href=\"https://sundaymagazine.org/2010/08/10/from-1890-the-first-text-messages/\">From 1890: The First Text Messages</a>:</p>\n<blockquote class=\"blockquote\">\n<p><strong>Me</strong></p>\n<blockquote class=\"blockquote\">\n<p>Summarize this text:</p>\n<p>I’m trying something new today. Sometimes in my research I find an interesting old article that I wouldn’t normally post because it’s not from the Sunday Magazine section, or it’s from further than 100 years ago so I’ll never get to it. Instead of letting these go unused, I figure I’ll occasionally post them midweek during what would otherwise be slow weeks. Since this weekend I only have three articles to post, it seems like a good week to try it.</p>\n<p>Telegraph operators on opposite sides of the country had some time to get to know each other when they weren’t busy sending other people’s messages. “Metaphorically they shake hands cordially twice a day — when they begin work and when they end it. And when business is dull they hold long conversations, with hundreds of miles — perhaps thousands — separating them, as two friends might do over a dinner table.”</p>\n<p>What really caught my eye, though, is that the abbreviations they used seem a lot like the abbreviations used in today’s text messages.</p>\n</blockquote>\n<p><strong>Copilot</strong></p>\n<blockquote class=\"blockquote\">\n<p>Here is a possible summary of the text:</p>\n<p>The author is trying a new feature of posting old articles from the Sunday Magazine section on midweek. The article is about how telegraph operators communicated with each other using abbreviations that are similar to today’s text messages. The author finds this interesting and wonders if there are other similarities between old and new forms of communication.</p>\n</blockquote>\n<p><strong>Me</strong></p>\n<blockquote class=\"blockquote\">\n<p>Now abbreviate it as if using a T9 keyboard for conciseness</p>\n</blockquote>\n<p><strong>Copilot</strong></p>\n<blockquote class=\"blockquote\">\n<p>That sounds like a fun challenge. Here is a possible abbreviation of the summary:</p>\n<p>try new feat post old artcls midwk. artcl abt telgrph oprtrs chat w/ abbrvtns lk txt msgs. find intrstng n wndr othr smilrts btwn old n new comms.</p>\n</blockquote>\n</blockquote>\n<p>In this conversation, I first had Copilot summarize the original text. It's not a very long piece of text but it works for this example. You could imagine though how a longer piece of text would benefit even more from this summarization. Copilot does a good job of summarizing the text. However, what if I wanted to summarize it even more?</p>\n<p>In comes T9 (kind of). T9 is short for &amp;quot;Text on 9 keys&amp;quot; and it was predictive text technology that made it easier to compose SMS text messages using only the 9 keys on the phone keypad. Before T9, it was cumbersome and time consuming to compose full sentences so you might abbreviate them. For example &amp;quot;See you later&amp;quot; might be abbreviated as &amp;quot;c u l8r&amp;quot;. You can learn more in the <a href=\"https://en.wikipedia.org/wiki/SMS_language\">SMS language Wikipedia article</a>. Not only would you abbreviate because it was tedious to compose messages one letter at a time, but also you were limited in the number of characters you could send in a single SMS. Somewhat related, in the early days of microblogging on Twitter, one of the ways you could compose and view tweets was through SMS. There was the cost aspects as well. You were charged per SMS so you wanted to send as much information in a single SMS as possible. Now SMS is only one example where this took place. Telegrams and telegraphs from yesteryear had similar constraints. Whether it was cost, size limits, or general UX challenges, people looked to abbreviations to draft their messages.</p>\n<p>Now that we're caught up on this history, that's what you're seeing in this response from Copilot.</p>\n<blockquote class=\"blockquote\">\n<p>try new feat post old artcls midwk. artcl abt telgrph oprtrs chat w/ abbrvtns lk txt msgs. find intrstng n wndr othr smilrts btwn old n new comms.</p>\n</blockquote>\n<p>It is an abbreviated form of the summarized message enabling you to use even less tokens as part of the prompts you submit to AI.</p>\n<p>When looking at &amp;quot;word&amp;quot; counts, they are roughly the following:</p>\n<table class=\"table\">\n<thead>\n<tr>\n<th>Text</th>\n<th>Word Count</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Original</td>\n<td>58</td>\n</tr>\n<tr>\n<td>Summarized</td>\n<td>25</td>\n</tr>\n<tr>\n<td>T9</td>\n<td>18</td>\n</tr>\n</tbody>\n</table>\n<p>We can see that the abbreviated version is ~3x smaller than the original message.</p>\n\n<p>Given that models like GPT and others have been trained on the entire internet, it has some knowledge of these types of abbreviations to compose messages. Now, is it scalable and would you want to use this in your production applications, I can't say. Probably not because the abbreviations are lossy. While in general you could read and get the gist of the message using the abbreviations, there are things lost in translation. Now maybe the LLM might do a better job at interpreting the abbreviations. It'd be a fun experiment.</p>\n<p>&amp;quot;n d mntm, il kp usng AI lk its 1999. c u l8r!&amp;quot;</p>\n<p>Translation: &amp;quot;In the meantime, I'll keep using AI like it's 1999. See you later!&amp;quot; 🙂</p>\n","tags":["ai","communication","sms","retro","internet"]},{"title":"Quick thoughts about Snapdragon Summit 2023","contentType":"posts","date":"2023-10-24 23:14","url":"/posts/quick-thoughts-snapdragon-summit-2023/","content":"<p>This started as a note where I was planning on posting a bullet point or two with my thoughts but kept on writing so I decided to turn it into a blog post.</p>\n<p>Here's a few of my thoughts after the Snapdragon Summit 2023 keynote.</p>\n<p>Today was the start of Snapdragon Summit 2023 and all I can say is, <strong>WOW</strong>.</p>\n<p>CNET has a <a href=\"https://www.youtube.com/watch?v=K7Q5iYHvgwo\">14 minute clip</a> that summarizes all the announcements.</p>\n<p><a href=\"https://www.youtube.com/watch?v=K7Q5iYHvgwo\" title=\"Qualcomm’s Snapdragon AI Event: Everything Revealed in 14 Minutes\"><img src=\"http://img.youtube.com/vi/K7Q5iYHvgwo/0.jpg\" class=\"img-fluid\" alt=\"Qualcomm’s Snapdragon AI Event: Everything Revealed in 14 Minutes\" /></a></p>\n<p>I was cautiously optimistic about what would be shown today but Qualcomm delivered.</p>\n<p>The main thing that I'm excited about are their Snapdragon X Elite Oryon-based CPUs.</p>\n<p>As someone who does all of their computing on a laptop, I'd love to have a PC that I can take with me on the go. Now you might think that's what your phone is for, and for many people that is definitely the case. However, I only use my phone for calls and messages. I find it much more enjoyable to have a larger screen and keyboard.</p>\n<p>Keeping my preferences in mind, being able to connect to your PC on the go is nothing new either. I can just use the hotspot on my phone or tether over USB in order to connect my PC to the internet. Also, SIM card slots in PCs have been a thing for a while and products like the <a href=\"https://www.microsoft.com/d/surface-pro-9/93vkd8np4fvk?activetab=pivot:overviewtab\">Surface Pro 9 with 5G</a> provide an even more integrated experience.</p>\n<p>However, in order to truly make this experience mobile, I need to be untethered. Just like a mobile phone, I need my device battery to last all-day while on the go so I don't have to carry my charger. To do that, either you need a device with a large battery or a low-power chip. Larger battery immediately takes the mobile out of mobile computing. I recently carried around my 16&amp;quot; work laptop for a while walking in the city and that was no fun. So then low-power chip it is. Problem solved. Not quite, because in order for it to be low-power, performance needs to be throttled. So for the longest time, it's been a constant struggle between power-efficiency and battery life.</p>\n<p>Apple was the first to solve this problem with their M1 line of chips. However, that meant this innovation was confined to the Apple ecosystem. It's not until now with the Oryon CPUs that Windows (and hopefully soon Linux too) can tap into these types of computing platforms that promise to deliver true mobile computing. This now opens up a world of possibilities when we think about mobile computing. Our phones keep getting <a href=\"https://www.theverge.com/23913658/best-small-phone-dead-iphone-mini-z-flip-pixel-8\">larger in size</a> that at this point it's harder to carry them in your pocket or use them with one hand. We've seen a surge in interest in handheld gaming devices like the <a href=\"https://www.steamdeck.com/\">SteamDeck</a> which are full PCs. Their problem is they're using chips that are not power efficient, especially because of their requirements for gaming.</p>\n<p>What happens if you replace the current chips in those devices with Snapdragon X Elite? Now that it's high-performance and power efficient, does your handheld gaming device now become your handheld computing device? And if we're in handheld computing device territory, now all of a sudden it's bumping up against what smartphones do today. So at that point, do we need both a smartphone and a handheld computing device? If I was able to handle calls and messages from my PC then I wouldn't need a phone and I'd much rather just carry a single handheld computing device. Considering that smartphones are general computing devices that are serverely locked down and restricted, I'd much rather go for the general computing type of device like a PC because I have more freedom over what I can do with my PC. More importantly, because it's a full PC, it means when I need to dock down for serious work with a keyboard and multiple monitors, I can plug a USB-C Hub and connect all of my peripherals to the device. This was something projects like <a href=\"https://learn.microsoft.com/windows-hardware/design/device-experiences/continuum-phone\">Continuum</a> tried to achieve but were unsuccessful doing for various reasons. Because now your PC is now also your mobile device, you take a big problem with these type of experiences which is reconciling mobile and desktop UX patterns.</p>\n<p>What about apps? Not exactly sure what this looks like in the near-term but considering devices running Oryon CPUs will take some time to get to market, by using the AI capabilities you can think of a future where AI is the interface and the apps become background services for interactive with external systems. So no need for specific apps because you can express intent and complete tasks using natural language.</p>\n<p>What about form factors? The steamdeck might not be what folks are looking for when it comes to a mobile device. Foldables are seeing an uptick in adoption. Despite it's lack of commercial success, the Surface Duo was an excellent foldable form factor that was compact but functional for productivity. I think the Surface Duo was misunderstood because it was put into the box of mobile phone. As a mobile phone it didn't work as well. But as a mobile computer, it struck the right form factor that enabled you to be productive on the go while still being portable. Needed more screen real estate? Surface Neo or Courier would've been  nice devices to have.</p>\n<p>Let's go 3 years into the future. What happens when virtual and augmented reality headsets all of a sudden become as small as the <a href=\"https://www.ray-ban.com/usa/ray-ban-meta-smart-glasses\">Ray-Ban Meta Smartglasses</a>? Do we even need handheld PCs at that point if all we need is for the headset to display the screens we're looking at. Bring in IoT. If the items around us are also &amp;quot;smart&amp;quot; devices, all of a sudden your environment becomes your compute platform. Bring AI into the mix and your AI assistant has access to your environment.</p>\n<p>I'd say the biggest miss from Qualcomm and Windows partners today was not having a lineup of devices ready for purchase. That will come with time. Hopefully not too long after these announcements. I'm sure that Apple which has an announcement coming up on October 30th will have a response. The most important thing though is that now there's competition and as we enter this new era of AI and TRUE mobile computing, it'll lead to innovation. The next things that need to happen is operating systems like Windows need to ensure their existing applications work on these platforms and Linux needs to enable support for these chipsets. I'm so excited for the future of mobile computing and can't wait to be able to carry a device running Linux with always-on connectivity and all-day battery life in the form factor of a Surface Duo.</p>\n<p>An addendum to this post can be found <a href=\"/notes/snapdragon-summit-2023-addendum\">here</a>.</p>\n","tags":["arm","qualcomm","hardware","mobilecomputing","5g","ai"]},{"title":"Generate QR Codes for MeCard Data in F#","contentType":"posts","date":"2023-10-23 19:24","url":"/posts/generate-mecard-qr-code-fsharp/","content":"\n<p>Today I learned about MeCard file formats for representing contact information while reading the post <a href=\"https://andregarzia.com/2023/10/sharing-contacts-using-qr-code.html\">Sharing contacts using QR Code without requiring a network connection</a> by Andre Garzia.</p>\n<p>Currently I host my <a href=\"../vcard.vcf\">vCard</a> on my site in a very similar way Andre describes in his post. I've also encoded it in <a href=\"/files/images/contact/qr-vcard.svg\">QR Code format</a> to make it easy to import and share.</p>\n<p>When I saw the MeCard format, I immediately liked how simple it was so I decided to see how difficult it'd be to write some code to generate a QR code for my own contact data.</p>\n\n<p>According to Wikipedia, MeCard is &amp;quot;...a data file similar to vCard...It is largely compatible with most QR-readers for smartphones. It is an easy way to share a contact with the most used fields. Usually, devices can recognize it and treat it like a contact ready to import.&amp;quot;</p>\n<p>An example might look like: <code>MECARD:N:Doe,John;TEL:13035551212;EMAIL:john.doe@example.com;;</code></p>\n\n<p>I'd written a <a href=\"/resources/snippets/qr-code-generator\">snippet</a> a few years ago to help me generate the QR Codes for all accounts in <a href=\"/contact\">my contact page</a>.</p>\n<p>The snippet is a F# Interactive script which uses <a href=\"https://www.nuget.org/packages/Net.Codecrete.QrCodeGenerator\">Net.Codecrete.QrCodeGenerator NuGet package</a> to encode text as a QR code and saves it to an SVG file.</p>\n<p>The core logic consists of the following code:</p>\n<pre><code class=\"language-fsharp\">let createQrCode (savePath:string) (target:string) = \n    let qr = QrCode.EncodeText(target,QrCode.Ecc.High)\n    let svgString = qr.ToSvgString(4)\n    File.WriteAllText(savePath,svgString, Encoding.UTF8)\n</code></pre>\n<p>In this case <code>target</code> is the text I want to encode and <code>savePath</code> is the path of the SVG file I'm saving the generated QR code in.</p>\n<p>Given MeCard with the following content:</p>\n<pre><code class=\"language-csharp\">MECARD:N:Doe,John;TEL:13035551212;EMAIL:john.doe@example.com;;\n</code></pre>\n<p>The QR Code generated might look as follows:</p>\n<p><img src=\"/files/images/mecard.svg\" class=\"img-fluid\" alt=\"MECARD QR CODE\" /></p>\n<p>Scanning the QR Code with a smartphone would then immediately recognize it as a contact and prompt you to save the contact information to your device.</p>\n\n<p>Since I already had the code snippet to do this available, creating a MeCard was straightforward. I've since added my <a href=\"/mecard.txt\">MeCard</a> alongside by vCard. You can check out the corresponding <a href=\"/files/images/contact/qr-mecard.svg\">QR code</a> in my <a href=\"/contact\">contact</a> page.</p>\n","tags":["dotnet","fsharp","qrcode"]},{"title":"Down the Slow / Quiet Web Rabbit Hole","contentType":"posts","date":"2023-09-23 23:36","url":"/posts/down-the-slow-web-rabbit-hole/","content":"<p>No real commentary on this post other than a web surf down the slow / quite web.</p>\n<pre class=\"mermaid\">flowchart TD\n  A[\"My 20th anniversary of blogging!\"] --> B[\"The Quiet Web\"]\n  B --> C[\"Asyncronous Conversations\"]\n  C --> D[\"The Slow Web\"]\n  D --> E[\"The Slow Web Movement - I DONE THIS\"]\n  E --> F[\"The Slow Web (plus: the modern experience of film-watching)\"]\n  F --> G[\"Minimal Gallery\"]\n  G --> H[\"My website is a shifting house next to a river of knowledge. What could yours be?\"]\n  H --> I[\"Quiet web\"]\n  I --> J[\"Jeremy Sarber\"]\n  J --> K[\"datagubbe\"]\n  K --> L[\"Writers and talkers and leaders, oh my!\"]\n  L --> M[\"Is Blogging Dead?\"]\n  click A href \"https://tracydurnell.com/2023/09/23/my-20th-anniversary-of-blogging/\" \"My 20th anniversary of blogging!\"\n  click B href \"https://briankoberlein.com/tech/quiet-web/\" \"The Quiet Web\"\n  click C href \"https://manuelmoreale.com/asynchronous-conversations\" \"Asyncronous Conversations\"\n  click D href \"https://www.jackcheng.com/the-slow-web/\" \"The Slow Web\"\n  click E href \"https://blog.idonethis.com/the-slow-web-movement/?ref=jackcheng.com\" \"The Slow Web Movement - I DONE THIS\"\n  click F href \"http://www.rebeccablood.net/archive/2010/06/a_slow_web.html\" \"The Slow Web (plus: the modern experience of film-watching)\"\n  click G href \"https://minimal.gallery/\" \"Minimal Gallery\"\n  click H href \"https://thecreativeindependent.com/essays/laurel-schwulst-my-website-is-a-shifting-house-next-to-a-river-of-knowledge-what-could-yours-be/\" \"My website is a shifting house next to a river of knowledge. What could yours be?\"\n  click I href \"https://manuelmoreale.com/quiet-web\" \"Quiet web\"\n  click J href \"https://jeremysarber.com/\" \"Jeremy Sarber\"\n  click K href \"https://datagubbe.se/\" \"datagubbe\"\n  click L href \"https://everythingchanges.us/blog/writers-and-talkers-and-leaders/\" \"Writers and talkers and leaders, oh my!\"\n  click M href \"https://abeautifulmess.com/is-blogging-dead/\" \"Is Blogging Dead?\"\n</pre>\n","tags":["blogs","indieweb","internet","slowweb","quietweb"]},{"title":"First Steps with NixOS","contentType":"posts","date":"2023-08-22 18:03","url":"/posts/first-steps-nixos/","content":"\n<p>For the longest time, when using Linux distributions, I chose those that were Debian-based. I still do for servers. However, on the dekstop I switched to <a href=\"https://manjaro.org/\">Manjaro</a>, which is Arch-based, about two years ago. Two years is a long time without distro-hopping, so I was long overdue. I've been following the <a href=\"https://nixos.org/\">NixOS</a> project for some time but since I was happy with my setup, I didn't consider making the switch. However, a month ago, I decided to <a href=\"/notes/maybe-switching-nixos\">dip my toes</a>. I installed NixOS on a flash drive and started tinkering with it. In this post, I'll provide some of my initial impressions and things I find appealing about the OS.</p>\n\n<p>One of the first things I did when I booted into my new NixOS installation was get familiar with the <a href=\"https://nixos.wiki/wiki/Overview_of_the_NixOS_Linux_distribution#Declarative_Configuration\">configuration file</a>. The configuration file is authored using the <a href=\"https://nix.dev/tutorials/first-steps/nix-language\">Nix programming language</a> and it's the place where you configure your entire system, including services, packages, and desktop environments. The best way I can describe it is a Dockerfile for your desktop. If interested, you can find my <a href=\"/resources/snippets/nixos-configuration/\">config file</a> on the scripts section of this website.</p>\n<p>There's a few advantages to this approach:</p>\n<ol>\n<li><p><strong>Configurations are stored in a single place</strong> - The advantage to this is, I don't have to figure out where the configuration files for each of the components of my system are. I can manage everything in one place. A perfect example where the configuration file goes beyond what I expected was being able to include my bash configuration. Typically, I'd have that configuration in my <code>bashrc</code> file. With NixOS, I can just include that in the configuration file as follows:</p>\n<pre><code class=\"language-nix\">programs.bash = {\n    shellAliases = {\n        emacs=&amp;quot;emacs -nw&amp;quot;;\n    };\n};\n</code></pre>\n</li>\n<li><p><strong>Enables composition</strong> - Because the configuration file is effectively a script, I can modularize the difference pieces. For example, if I wanted to split out my service and package configurations into separate files, I can do so and reference those individual files from my configuration file, separating concerns while still keeping my system configuration simple.</p>\n</li>\n<li><p><strong>Version control</strong> - Because the configuration file is just like any other plain text file, it means I can check it into the version control system or cloud service of my choice and manage it that way. In doing so, not only can I roll back using the built-in NixOS mechanisms, but I have another layer of security in case I need to recover that file.</p>\n</li>\n</ol>\n<p>Now, because I can define everything about my system in the configuration file, this means I can seamlessly rebuild my entire system using this single file in a reproducible manner.</p>\n\n<p>I got to experience first-hand how the configuration file can simplify out-of-box-experiences and system configurations. Initially, I did not install NixOS on my main hard drive. All of my configuration and learning took place on a flash drive. Once I got my configuration to a place I was satisfied, it was time to take the plunge and reimage my PC. In the past, when I've done something similar, I've had to document everything I did when configuring my system. That's where posts like <a href=\"/posts/setting-up-new-ubuntu-pc/\">Setting up a new Ubuntu PC</a> come from. With NixOS, all I needed to do was replace my configuration file with the one I configured on the flash drive. Then, when I ran <code>nixos-rebuild switch</code>, my system was configured exactly like the one on the flash drive.</p>\n\n<p>This is something I haven't tried yet, but I could easily see it coming in handy. Sometimes I might need a piece of software to do one thing or maybe I want to see whether it'll solve a problem I'm facing. In order to try it out and use it, I need to install it globally. This can cause changes to my system permanently that I don't want. With NixOS, you can create <a href=\"https://nix.dev/tutorials/first-steps/ad-hoc-shell-environments#ad-hoc-shell-environments\">ad-hoc shell environments</a>. These environments make temporary changes to your system, so you can evaluate a piece of software or use it that one time. Then, when you exit the environment, the software is no longer installed in your system.</p>\n\n<p>A <a href=\"https://search.nixos.org/packages\">large package selection</a> to choose from is something I got used to with Manjaro. Being an Arch-based distribution, I had access to the <a href=\"https://aur.archlinux.org/\">Arch User Repository (AUR)</a>. Moving to NixOS, it's nice to know that in this area, NixOS offers great support. Additionally, NixOS has built-in support for <a href=\"https://nixos.wiki/wiki/Flatpak\">Flatpak</a></p>\n\n<p>There's still a few things I don't really get. For example, when installing the .NET SDK, I wanted to have side-by-side installs of the latest STS (7.0) and LTS (6.0). However, including the package names in my configuration file didn't work as expected. Instead I had to use the following convention.</p>\n<pre><code class=\"language-nix\">(with dotnetCorePackages; combinePackages [\n    dotnet-sdk\n    dotnet-sdk_7\n])\n</code></pre>\n<p>As I learn more, this will probably make more sense but for now it's a mystery.</p>\n\n<p>Overall, I'm happy with NixOS and I can see myself using it for the long-term. Hopefully longer than two years. I'm not a gamer so I can't say how well it does in that front. For web browsing and software development though, I really like what NixOS has to offer. If some of the things I mentioned above sound interesting, I'd encourage you to install it on a flash drive and start tinkering with it.</p>\n","tags":["nixos","linux","os","nix","sysadmin","floss"]},{"title":"Accept Webmentions using F#, Azure Functions, and RSS","contentType":"posts","date":"2022-12-21 00:33","url":"/posts/receive-webmentions-fsharp-az-functions-fsadvent/","content":"\n<blockquote class=\"blockquote\">\n<p>This post is part of <a href=\"https://sergeytihon.com/2022/10/28/f-advent-calendar-in-english-2022/\">F# Advent 2022</a>. Thanks to <a href=\"https://sergeytihon.com\">Sergey Tihon</a> for organizing it.</p>\n</blockquote>\n<p>While not planned this way, this post is timely considering recent developments in the social media space. While Twitter is in the headlines, it's not the only time people have questioned their engagement and relationship with these social media platforms. In the context of Twitter though, users have been looking for alternative platforms to best serve their communities. While Mastodon and the Fediverse appear to be one of those alternatives, they're not the only one. Blogosphere platforms like Tumblr and WordPress, both owned by Automattic, are also positioning themselves as viable alternatives to the point they've <a href=\"https://techcrunch.com/2022/11/21/tumblr-to-add-support-for-activitypub-the-social-protocol-powering-mastodon-and-other-apps/\">considered the idea of supporting ActivityPub</a>. ActivityPub is the protocol that powers many of the platforms on the Fediverse. However, it's not the only open protocol that enables communication and engagement on the open web. Webmentions is another protocol for enabling conversations on the web which I'll talk about more in this post.</p>\n<p>I'll also discuss how I used F#, Azure Functions, and RSS to implement a solution that helps me and my website engage in conversations on the open web thanks to Webmentions.</p>\n\n<p>First, let's start by talking a bit about Webmentions.</p>\n<p>According to the W3C specification, Webmentions are &amp;quot;...a simple way to notify any URL when you mention it on your site. From the receiver's perspective, it's a way to request notifications when other sites mention it.&amp;quot;.</p>\n<p>One of the goals according to the IndieWeb wiki is to enable cross-site conversations.</p>\n<p>Here's how a potential Webmention workflow might look like:</p>\n<pre class=\"mermaid\">sequenceDiagram\n  autonumber\n  actor A as Alice\n  participant AS as Alice's \"Website\"\n  participant AW as Alice's Webmention Service\n  participant BS as Bob's \"Website\"\n  actor B as Bob\n  A ->> AS: Publishes post\n  B ->> AS: Reads post\n  B ->> BS: Publishes post with link to Alice's post\n  rect rgb(230,230,230)\n    alt Automated\n      BS ->> AS: Fetch Webmention endpoint\n      BS ->> AW: Sends Webmention\n    else Manual\n      B ->> AS: Fetch Webmention endpoint\n      B ->> AW: Sends Webmention\n    end \n  end\n  rect rgb(230,230,230)\n      note over AW:Validates Webmention request\n      note over AW: Validates Webmention\n      note over AW: Accepts / processes Webmention\n      alt Automated\n        AW ->> BS: Sends back response\n      else Manual\n        AW ->> B: Sends back response\n      end\n  end \n  opt Display Webmentions\n    AS ->> AW: Fetch Webmentions\n    note over AS: Display / publish Webmentions\n  end\n</pre>\n<p>I use the term website here very loosely because it's not only limited to your own personal website. You can technically use Webmentions through any public website including social media platforms like Twitter, Mastodon, or even GitHub.</p>\n<p>This post will focus on the middle blocks consisting of sending and receiving Webmentions. For more information, check out the <a href=\"https://www.w3.org/TR/webmention/#abstract-p-1\">specification</a> and <a href=\"https://indieweb.org/Webmention\">Webmentions page</a> on the IndieWeb wiki.</p>\n\n<p>In September of this year, I <a href=\"/notes/webmentions-partially-implemented/\">integrated a partial implementation of Webmentions</a> into my website. This implementation only allowed me to send webmentions as part of my publishing process. If you're interested in the details, it's based on the blog post I wrote for last year's F# Advent, <a href=\"/posts/sending-webmentions-fsharp-fsadvent/\">Sending Webmentions with F#</a>.</p>\n<p>A couple of days ago, I <a href=\"/notes/now-accepting-webmentions\">successfully deployed and integrated</a> the other part of the implementation which allowed me to receive webmentions. While there are many great <a href=\"https://indieweb.org/Webmention#Publisher_Services\">services</a> which will do this for you, I decided to roll my own for fun and to learn something new.</p>\n<p>To help with that, I released <a href=\"https://www.nuget.org/packages/lqdev.WebmentionFs\">WebmentionFs</a>, an open-source .NET library written in F# with components for sending and receiving Webmentions.</p>\n<p>The backend service I use to receive webmentions is built on Azure Functions.</p>\n<p>If you're interested, here's the source code for <a href=\"https://github.com/lqdev/WebmentionFs\">WebmentionFs</a> and my <a href=\"https://github.com/lqdev/WebmentionService\">Webmention service</a>.</p>\n<p>Before diving into the technical details of the solution, I'll first discuss some of the priorities driving my design decisions.</p>\n<h3>Requirements</h3>\n<p>While I don't want to trivialize it, at the end of the day, the Webmention service is just an endpoint that accepts HTTP POST requests. That means there's many ways to build it. However, here were the requirements important to me that guided my design choices:</p>\n<ul>\n<li><strong>Cost:</strong> The solution should be low cost</li>\n<li><strong>Maintenance:</strong> The solution should be easy to maintain</li>\n<li><strong>Easy to consume:</strong> I have multiple ways to view and access my webmentions.</li>\n<li><strong>Notifications first:</strong> Since I don't own or have responsibility over content that's not mine, prioritize Webmentions for notifications, not content.</li>\n</ul>\n<p>With that in mind, I ended up choosing a solution built on Azure Functions to receive Webmentions and publish them as an RSS feed.</p>\n<h3>Why Azure Functions?</h3>\n<p>I don't make money from my website. It's a labor of love in which I document my experiences. Selfishly, it's for me to remind myself how to solve a problem or share something that was important to me at that time. If my posts end up helping others solve their own problems, that's a side effect and a reward unto itself. To that effect, I want my website to be as low-cost to run as possible. One way I achieve that is by using static hosting. This dramatically reduces the cost associated with my site. If you're interested in how this site is built, you can check out the <a href=\"/colophon\">colophon</a>.</p>\n<p>So where do Azure Functions come in?</p>\n<p>Azure Functions is a serverless solution which means there are no servers! (I kid 😏)</p>\n<h4>Cost ✔️</h4>\n<p>As a serverless solutions, it means web services are not constantly running. They only run whenever there is something to process so I only pay for what I need. That takes care of my low-cost requirement since processing Webmentions is not a long-running, resource-intensive operation that can easily run in a serverless environment.</p>\n<h4>Maintenance ✔️</h4>\n<p>Cost is not only monetary. There are also time costs associated with maintaining a solution. Because there is no infrastructure to maintain, that's not really a factor I need to consider. Instead I get to focus on writing the logic for accepting and processing Webmentions. Azure Functions provides a set of <a href=\"https://learn.microsoft.com/azure/azure-functions/functions-triggers-bindings\">prebuilt trigger and binding components</a> to reduce the amount of code I have to write to handle things like accepting HTTP requests or storing data in a table or blob storage. Also, in the rare instance that this blog post breaks the internet, Azure Functions enables my service to automatically handle that scale. Don't believe me? Check out this post frorm <a href=\"https://www.troyhunt.com/\">Troy Hunt</a> on <a href=\"https://www.troyhunt.com/serverless-to-the-max-doing-big-things-for-small-dollars-with-cloudflare-workers-and-azure-functions/\">Have I Been Pwned usage of Azure Functions</a>.</p>\n<h3>Why RSS?</h3>\n<p>The choice of how to display and consume Webmentions is entirely up to you. At regular intervals, my Webmention service generates an RSS feed and publishes it to a private Azure Blob Storage. Here are some of the reasons why:</p>\n<h4>Easy to consume ✔️</h4>\n<p>The decision to use RSS comes from the fact that like Webmentions, it's an open protocol that gives choice to creators and consumers. Check out my post, <a href=\"/posts/rediscovering-rss-user-freedom\">Rediscovering the RSS protocol</a> for more of my thoughts on that. Despite the appearance of decline in use, it's a protocol that's been around for over 20 years and it's one of the many reasons why podcasts continue to be one of the more open platforms despite some attempts to lock down the ecosystem.  With that it mind, by choosing RSS as the publishing format for my Webmentions feed, I can lean on the vast number of RSS readers (<a href=\"http://newsblur.com/\">NewsBlur</a> and <a href=\"https://github.com/skeeto/elfeed\">Elfeed</a> being my favorites) out there instead of having to write a custom UI. Not happy with my RSS reader? I can just add my feed to whichever other reader I move to and continue consuming my Webmentions without skipping a beat.</p>\n<h3>Notifications first ✔️</h3>\n<p>While not technical or specific to RSS, this is a topic I wrestled with as part of my implementation. However, choosing RSS made many parts of it simpler.</p>\n<p>Who owns your content? If you publish on your own website or a platform under your control, chances are you do. If I publish and redistribute your content on my website though, what parts do I own and which parts do you? I'm not sure. What about spam, harrassment, and lewd content? I'd have to put in place some sort of moderation instead of publishing directly on my site. I didn't want to have to deal with any of these questions so choosing to publish to a private RSS feed instead of directly on my site made it so Webmentions are for my eyes only. More importantly, I only capture the source and target links, not the content. Therefore, I don't store or redistribute your content. Now there's nothing stopping anyone from seeing your post since it's public but in cases where your site's content is not something I'm comfortable with, I'm not resharing or redistributing that content. If my RSS reader chooses to display the full contents of your post by using the <code>link</code> element from the RSS feed, since your post is public, that's no different than me viewing the original post on your site. If at some point I feel differently about it, the changes to capture the content in the <code>description</code> happen at the RSS file level and because the RSS is private, I'm still not redistributing your content.</p>\n<p>In short, with my implementation I chose to prioritize Webmentions for notification purposes rather than building a commenting and interaction (like, reshare) system for my website.</p>\n<p>Now that I've talked about the what and the why, in the next sections I'll get into the how.</p>\n\n<p>Once you've created a post linking to someone else's website, sending webmentions is a two-step process:</p>\n<ol>\n<li>URL discovery of webmention endpoint</li>\n<li>Send webmention</li>\n</ol>\n<p>These steps can be automated or performed manually.</p>\n<h3>URL discovery of Webmention endpoint</h3>\n<p>The first thing you'll need is the endpoint where you'll be sending your Webmention to.</p>\n<p>According to the Webmention specification, a Webmention endpoint can be found in one of three places:</p>\n<ul>\n<li>HTTP response headers</li>\n<li><code>&amp;lt;link&amp;gt;</code> tag with <code>rel=webmention</code> attribute</li>\n<li><code>&amp;lt;a&amp;gt;</code> tag with <code>rel=me</code> attribute</li>\n</ul>\n<p>When this process is automated, the discovery is performed in order and each subsequent option works as a fallback of the other. When done manually, either the website author will publish and advertise this on their website or you can inspect the website's source code to find it.</p>\n<h3>Send webmention</h3>\n<p>To send a Webmention, you need to send an HTTP POST request with an <code>x-www-form-urlencoded</code> body containing two properties: <code>source</code> and <code>target</code>. Source is the URL of your article and target is the URL of the original article. Again, this too can either be automated or done manually with tools like cURL, Insomnia, or any other HTTP client.</p>\n<h3>Sending Webmentions in practice</h3>\n<p>Let's use a redundant example to walk through the workflow. Pretend that you created or commented on a GitHub issue and added a link to one of my articles, <a href=\"/posts/sending-webmentions-fsharp-fsadvent/\">Sending Webmentions with F#</a> for example. You then wanted to notify me either to get my attention on that issue or just as an FYI. Now I say this is a redundant example because GitHub already has notifications. However, what this does illustrate are the cross-site capabilities of Webmentions.</p>\n<p><img src=\"/files/images/send-gh-issue.png\" class=\"img-fluid\" alt=\"Image of GitHub Issue Post\" /></p>\n<p>At this point, if GitHub implemented Webmentions, it would automatically:</p>\n<ol>\n<li>Fetch my webpage and look for my Webmention endpoint.</li>\n<li>Compose and send the HTTP POST request.</li>\n</ol>\n<p>Sadly, today that's not the case so we'll walk through the workflow manually.</p>\n<p>On my website, I include a <code>link</code> tag with the <code>rel=webmention</code> attribute that looks like the following:</p>\n<pre><code class=\"language-html\">&amp;lt;link rel=&amp;quot;webmention&amp;quot; title=&amp;quot;Luis Quintanilla Webmention Endpoint&amp;quot; href=&amp;quot;https://lqdevwebmentions.azurewebsites.net/api/inbox&amp;quot;&amp;gt;\n</code></pre>\n<p>My current Webmention endpoint is <em>https://lqdevwebmentions.azurewebsites.net/api/inbox</em>.</p>\n<p>Now that you have the Webmention endpoint, you could use any tool of your choice to create and send the HTTP POST request. I have made it simpler though and included a form on my website for each of my posts for this purpose.</p>\n<p><img src=\"/files/images/send-webmention-form.png\" class=\"img-fluid\" alt=\"luisquintanilla.me send Webmention form\" /></p>\n<p>The source code looks like the following:</p>\n<pre><code class=\"language-html\">&amp;lt;div&amp;gt;\n    &amp;lt;script type=&amp;quot;application/javascript&amp;quot;&amp;gt;window.onload = function() { document.getElementById('webmention-target').value = window.location.href }&amp;lt;/script&amp;gt;\n    &amp;lt;form action=&amp;quot;https://lqdevwebmentions.azurewebsites.net/api/inbox&amp;quot; method=&amp;quot;POST&amp;quot; enctype=&amp;quot;application/x-www-form-urlencoded&amp;quot;&amp;gt;\n        &amp;lt;h5 class=&amp;quot;text-center&amp;quot;&amp;gt;Send me a &amp;lt;a href=&amp;quot;https://indieweb.org/webmentions&amp;quot;&amp;gt;webmention&amp;lt;/a&amp;gt;&amp;lt;/h5&amp;gt;\n        &amp;lt;div class=&amp;quot;form-row justify-content-center&amp;quot;&amp;gt;\n            &amp;lt;div class=&amp;quot;w-75&amp;quot;&amp;gt;\n                &amp;lt;input type=&amp;quot;text&amp;quot; name=&amp;quot;source&amp;quot; class=&amp;quot;form-control&amp;quot; placeholder=&amp;quot;Your URL (source)&amp;quot;&amp;gt;\n            &amp;lt;/div&amp;gt;\n            &amp;lt;div class=&amp;quot;col-auto&amp;quot;&amp;gt;\n                &amp;lt;input type=&amp;quot;submit&amp;quot; class=&amp;quot;btn btn-primary&amp;quot; value=&amp;quot;Send&amp;quot;&amp;gt;\n            &amp;lt;/div&amp;gt;\n            &amp;lt;input readonly=&amp;quot;&amp;quot; class=&amp;quot;form-control-plaintext&amp;quot; style=&amp;quot;visibility:hidden&amp;quot; type=&amp;quot;text&amp;quot; id=&amp;quot;webmention-target&amp;quot; name=&amp;quot;target&amp;quot;&amp;gt;\n        &amp;lt;/div&amp;gt;\n    &amp;lt;/form&amp;gt;\n&amp;lt;/div&amp;gt;\n</code></pre>\n<p>Now to send the webmention, navigate to the post you want to send a Webmention to. In this case, it's <a href=\"/posts/sending-webmentions-fsharp-fsadvent/\">Sending Webmentions with F#</a>. Then, paste the link to the GitHub issue that contains the link to my site into the post's text box and hit &amp;quot;Send&amp;quot;.</p>\n<p><img src=\"/files/images/send-webmention-form-populated.png\" class=\"img-fluid\" alt=\"luisquintanilla.me send Webmention form filled-in\" /></p>\n<p>At that point, the request is sent to my Webmention service which receives and processes the request and responds with a message whether the request was successful or not.</p>\n<p>That's all there is to it! Before moving the the receiving process though, I'll briefly show how to send Webmentions using WebmentionFs.</p>\n<h3>Sending Webmentions with WebmentionFs</h3>\n<p>In WebmentionFs, you can use the <a href=\"https://github.com/lqdev/WebmentionFs/blob/main/UrlDiscoveryService.fs\"><code>UrlDiscoveryService</code></a> and <a href=\"https://github.com/lqdev/WebmentionFs/blob/main/WebmentionSenderService.fs\"><code>WebmentionSenderService</code></a> to send Webmentions. If you wanted to integrate sending Webmentions into your application or service, here's an example that shows how you'd use those components.</p>\n<pre><code class=\"language-fsharp\">open System\nopen WebmentionFs\nopen WebmentionFs.Services\n\nlet ds = new UrlDiscoveryService()\n\nlet ws = new WebmentionSenderService(ds)\n\nlet data = \n    {   \n        Source = new Uri(&amp;quot;https://twitter.com/ljquintanilla/status/1603602055435894784&amp;quot;)  \n        Target = new Uri(&amp;quot;/feed/mastodon-hashtag-rss-boffosocko&amp;quot;)\n    }\n\nws.SendAsync(data) |&amp;gt; Async.AwaitTask |&amp;gt; Async.RunSynchronously\n</code></pre>\n<p>The main part to pay attention to is the <code>SendAsync</code> function. This uses the <code>UrlDiscoveryService</code> to perform the Webmention endpoint discovery and send the HTTP POST request with the provided data.</p>\n\n<p>Now that you know how to send webmentions, let's talk about what happens on the receiver end. At its core, it's about a three-step process:</p>\n<ol>\n<li>Validate request</li>\n<li>Validate Webmention</li>\n<li>Process Webmention</li>\n</ol>\n<h3>Validate request</h3>\n<p>Before processing the Webmention, you want to make sure that the request is valid. To do so there are a few checks you perform:</p>\n<ol>\n<li>Is the protocol valid? Check that the protocol for the source and target URLs in the request body are HTTP or HTTPS.</li>\n<li>Are the URLs the same? Check that the source and target URLS in the request body are NOT the same.</li>\n<li>Is the target URL a valid resource? This is more loosely defined and up to you. In my case I defined valid as:\n<ul>\n<li>I own the domain</li>\n<li>The target URL doesn't return an HTTP 400 or 500 error.</li>\n</ul>\n</li>\n</ol>\n<p>If the request is not valid, nothing is done with the Webmention and an error returns. Otherwise, it moves to the next step which is Webmention validation.</p>\n<h3>Validate Webmention</h3>\n<p>In the base case, Webmention validation just means you parse through the source document and make sure that the target URL is included in it. However, if you wanted to get more complex and take into account specific types of interactions such as likes, replies, reposts, bookmarks, etc. you can parse through the document looking for <a href=\"http://microformats.org/wiki/h-entry#Core_Properties\">microformat</a> annotations. For more information on these annotations, see the <a href=\"https://indieweb.org/posts#Types_of_Posts\">Types of Posts</a> or <a href=\"https://indieweb.org/reply#Post_a_reply\">reply</a> pages in the IndieWeb Wiki.</p>\n<p>If the target link is NOT in the source document, you return an error. Otherwise, you move to the next step of processing the Webmention</p>\n<h3>Processing Webmentions</h3>\n<p>This step is optional and loosely defined. Once you've validated the request and Webmention, you can do with it as you please. One thing you can do is publish it (after moderation) on your website.</p>\n<p>In my case, I'm storing it in an Azure Tables database. Then, every morning, I generate an RSS feed of all my webmentions and save it to a private container on Azure Blob Storage.</p>\n\n<p>To receive Webmentions, I use the <a href=\"https://github.com/lqdev/WebmentionFs/blob/main/RequestValidationService.fs\"><code>RequestValidationService</code></a>, <a href=\"https://github.com/lqdev/WebmentionFs/blob/main/WebmentionValidationService.fs\"><code>WebmentionValidationService</code></a> and <a href=\"https://github.com/lqdev/WebmentionFs/blob/main/WebmentionReceiverService.fs\"><code>WebmentionReceiverService</code></a> inside my Azure Function.</p>\n<p>The source code for my Azure Function is available in the <a href=\"https://github.com/lqdev/WebmentionService\">WebmentionService GitHub repo</a>.</p>\n<p>The main components are in the <code>Startup.fs</code> and <code>ReceiveWebmention.fs</code> files.</p>\n<p>The <a href=\"https://github.com/lqdev/WebmentionService/blob/main/Startup.fs\"><code>Startup.fs</code></a> files registers each of the services I'll use to to receive Webmentions. By doing so, I define the initialization logic for use later in my application using dependency injection. The main thing to note here is that the <code>WebmentionReceiverService</code> is initialized with instances of a <code>RequestValidationService</code> and <code>WebmentionValidationService</code>. For the <code>RequestValidationService</code>, it's initialized with a list of host names. In this case, I'm passing it in as a comma-delimited environment variable that looks something like &amp;quot;lqdev.me,luisquintanilla.me&amp;quot;. This is what the <code>RequestValidationService</code> uses to check whether I own a domain.</p>\n<pre><code class=\"language-fsharp\">// Add request validation service\nbuilder.Services.AddScoped&amp;lt;RequestValidationService&amp;gt;(fun _ -&amp;gt; \n\n    let hostNames = Environment.GetEnvironmentVariable(&amp;quot;PERSONAL_WEBSITE_HOSTNAMES&amp;quot;)\n\n    let hostNameList = hostNames.Split(',')\n\n    new RequestValidationService(hostNameList)) |&amp;gt; ignore\n\n\n// Add webmention validation service\nbuilder.Services.AddScoped&amp;lt;WebmentionValidationService&amp;gt;() |&amp;gt; ignore\n\n// Add receiver service\nbuilder.Services.AddScoped&amp;lt;IWebmentionReceiver&amp;lt;Webmention&amp;gt;,WebmentionReceiverService&amp;gt;(fun (s:IServiceProvider) -&amp;gt;\n    let requestValidationService = s.GetRequiredService&amp;lt;RequestValidationService&amp;gt;()\n    let webmentionValidationService = s.GetRequiredService&amp;lt;WebmentionValidationService&amp;gt;()\n    new WebmentionReceiverService(requestValidationService,webmentionValidationService)) |&amp;gt; ignore\n</code></pre>\n<p>Then, in the <a href=\"https://github.com/lqdev/WebmentionService/blob/main/ReceiveWebmention.fs\"><code>ReceiveWebmention.fs</code></a> file, I inject my <code>WebmentionReceiverService</code> into the constructor and add all the logic for running my serverless function in the <code>Run</code> function. In this snippet, some code is ommitted for brevity.</p>\n<pre><code class=\"language-fsharp\">type ReceiveWebmention (webmentionReceiver: IWebmentionReceiver&amp;lt;Webmention&amp;gt;) = \n\n    ///...\n\n    [&amp;lt;FunctionName(&amp;quot;ReceiveWebmention&amp;quot;)&amp;gt;]\n    member x.Run \n        ([&amp;lt;HttpTrigger(AuthorizationLevel.Anonymous, &amp;quot;post&amp;quot;, Route = &amp;quot;inbox&amp;quot;)&amp;gt;] req: HttpRequest) \n        ([&amp;lt;Table(&amp;quot;webmentions&amp;quot;, Connection=&amp;quot;AzureWebJobsStorage&amp;quot;)&amp;gt;] t: TableClient)\n        (log: ILogger) =\n        task {\n            \n            log.LogInformation(&amp;quot;Processing webmention request&amp;quot;)\n\n            let! validationResult = x.WebmentionReceiver.ReceiveAsync(req)\n\n            let response = \n                match validationResult with\n                | ValidationSuccess m -&amp;gt; \n                    let entity = mapMentionToTableEntity m\n                    try\n                        t.AddEntity(entity) |&amp;gt; ignore\n                        OkObjectResult(&amp;quot;Webmention processed successfully&amp;quot;) :&amp;gt; IActionResult\n                    with\n                        | ex -&amp;gt; \n                            log.LogError($&amp;quot;{ex}&amp;quot;)\n                            BadRequestObjectResult($&amp;quot;Error processing webmention. Webmention already exists&amp;quot;) :&amp;gt; IActionResult\n                | ValidationError e -&amp;gt; \n                    log.LogError(e)\n                    BadRequestObjectResult(e) :&amp;gt; IActionResult\n\n            return response\n\n        }\n</code></pre>\n<p>The <code>Run</code> function is triggered whenever an HTTP POST request is made to my Webmention endpoint. It then calls <code>ReceiveAsync</code> on the <code>WebmentionReceiverService</code> which under the hood uses the <code>RequestValidationService</code> and <code>WebmentionValidationService</code> for validation. For reference, here's the code that does that in WebmentionFS.</p>\n<pre><code class=\"language-fsharp\">member x.ReceiveAsync (data:UrlData) = \n    task {\n        let! requestValidationResult = \n            x.RequestValidationService.ValidateAsync data\n\n        match requestValidationResult with\n        | RequestSuccess r -&amp;gt; \n            let! webmentionValidationResult = \n                x.WembentionValidationService.ValidateAsync r.Source r.Target\n\n            let (result:ValidationResult&amp;lt;Webmention&amp;gt;) = \n                match webmentionValidationResult with\n                | AnnotatedMention m -&amp;gt; \n                    ValidationSuccess {RequestBody = r; Mentions = m}\n                | UnannotatedMention -&amp;gt; \n                    ValidationSuccess \n                        {\n                            RequestBody = r\n                            Mentions = \n                                {\n                                    IsBookmark = false\n                                    IsLike = false\n                                    IsReply = false\n                                    IsRepost = false        \n                                }\n                        }\n                | MentionError e -&amp;gt; ValidationError e\n            return result\n        | RequestError e -&amp;gt; return ValidationError e\n    }\n</code></pre>\n<blockquote class=\"blockquote\">\n<p>A quick side-note. I love how elegant function composition in F# makes the request validation pipeline look. See for yourself 🙂</p>\n<pre><code class=\"language-fsharp\">let validateAsync = \n   isProtocolValid &amp;gt;&amp;gt; isSameUrl &amp;gt;&amp;gt; isTargetUrlValidAsync\n</code></pre>\n</blockquote>\n<p>Inside my <code>Run</code> function, if the request and Webmention validation are successful, I use the Azure Table Storage table client <code>AddEntity</code> method to save the Webmention to a table. One thing to note is that <code>AddEntity</code> ensures that an entry is only added once. If an entry already exists in the table, it returns an error.</p>\n<p>This is what the GitHub Webmention we sent looks like in the table.</p>\n<p><img src=\"/files/images/webmentions-azure-table.png\" class=\"img-fluid\" alt=\"Webmentions table on Azure Table Storage\" /></p>\n<p>In this case, the <code>PartitionKey</code> is the target URL and <code>RowKey</code> is the source URL. The fact all other columns are set to false indicates this is an untagged mention with no special microformats annotations. Annotating your posts appropriately would set one or more of those columns to true.</p>\n<p>Now that we went over how I receive Webmentions, I'll show how I prepare my Webmentions for consumption.</p>\n\n<p>I consume my Webmentions using RSS feeds and RSS readers. To do so, I have an Azure Function called <a href=\"https://github.com/lqdev/WebmentionService/blob/main/WebmentionToRss.fs\"><code>WebmentionToRss</code></a> that triggers every day at 3 AM. This function queries the Azure Table Storage table containing my Webmentions and creates an RSS feed that's then stored to Azure Blob Storage. Like the other function to process my Webmentions, I:</p>\n<p>Register the <code>RssService</code> responsible for generating the RSS feed in <code>Startup.fs</code>.</p>\n<pre><code class=\"language-fsharp\">// Add RSS service\nbuilder.Services.AddScoped&amp;lt;RssService&amp;gt;() |&amp;gt; ignore\n</code></pre>\n<p>Inject the service into my <code>WebmentionToRss</code> constructor. Then in my <code>Run</code> function, I call the <code>getMentions</code> helper function to query the table, call <code>BuildRssFeed</code> to generate the feed, and write it out to Azure Blob Storage using a <code>Stream</code>.</p>\n<pre><code class=\"language-fsharp\">type WebmentionToRss (rssService:RssService) =\n\n    let getMentions (t:TableClient) = \n\n        //...\n\n        let webmentions = t.Query&amp;lt;WebmentionEntity&amp;gt;()\n\n        webmentions\n\n\n    //...\n\n    [&amp;lt;FunctionName(&amp;quot;WebmentionToRss&amp;quot;)&amp;gt;]\n    member x.Run\n        ([&amp;lt;TimerTrigger(&amp;quot;0 0 3 * * *&amp;quot;)&amp;gt;] info: TimerInfo)\n        ([&amp;lt;Table(&amp;quot;webmentions&amp;quot;,Connection=&amp;quot;AzureWebJobsStorage&amp;quot;)&amp;gt;] t: TableClient)\n        ([&amp;lt;Blob(&amp;quot;feeds/webmentions/index.xml&amp;quot;, FileAccess.Write, Connection=&amp;quot;AzureWebJobsStorage&amp;quot;)&amp;gt;] rssBlob: Stream)\n        (log: ILogger) =\n\n        task {\n            let mentions = getMentions t\n\n            let rss = x.RssService.BuildRssFeed mentions &amp;quot;lqdev's Webmentions&amp;quot; &amp;quot;http://lqdev.me&amp;quot; &amp;quot;lqdev's Webmentions&amp;quot; &amp;quot;en&amp;quot;\n\n            log.LogInformation(rss.ToString())                \n\n            use xmlWriter = XmlWriter.Create(rssBlob)\n\n            rss.WriteTo(xmlWriter) |&amp;gt; ignore\n        }\n</code></pre>\n<p>Now, to view my feed I add the URL to the RSS file in Azure Blob Storage to my RSS feed reader. The container and blob containing the file are private though, so how does my RSS reader get access to the file? I don't use the direct link. Instead, I use a link containing a <a href=\"https://learn.microsoft.com/azure/storage/common/storage-sas-overview#sas-token\">Shared Access Signatures (SAS) token</a>.</p>\n<p>The RSS feed reader then fetches my Webmentions feed at regular intervals.</p>\n<p>This is what the GitHub post looks like in my RSS reader.</p>\n<p><img src=\"/files/images/newsblur-webmentions-rss-feed.png\" class=\"img-fluid\" alt=\"Webmentions table on Azure Table Storage\" /></p>\n<p>That's all there is to it!</p>\n\n<p>Overall I'm happy with my solution. I learned a few things along the way and got something working for myself. Publishing WebmentionFs as a library and making my Azure Function code open-source were attempts to help others looking to implement similar solutions get started. These solitions could be improved though. These are some updates I would like to make at some point:</p>\n<ul>\n<li>Upgrade my website to use WebmentionFs for sending Webmentions. You can see the <a href=\"https://github.com/lqdev/luisquintanilla.me/blob/main/Services/Webmention.fs\">current implementation</a> on my website repo which is basically what's in <code>UrlDiscoveryService</code> and <code>WebmentionSenderService</code> today.</li>\n<li>Add interfaces for all services in WebmentionFs. Currently most services have concrete implementations. By adding interfaces, I can make the library extensible and you can implement your own URL discovery and validation services.</li>\n<li>Implement <code>outbox</code> enpoint in WebmentionService to handle sending Webmentions.</li>\n<li>Create Azure ARM template for deploying WebmentionService. Currently with some small edits, you could fork and get my WebmentionService project working for your website. However, that process isn't turnkey today. Maybe using something like <a href=\"https://compositionalit.github.io/farmer/\">Farmer</a> would make onboarding easier for folks.</li>\n</ul>\n\n<p>Webmentions are a great way to enable cross-site conversations on the internet. In this post, I provided a brief introduction to the general workflow for sending and receiving Webmentions. I also detailed my own implementation which I use today on my website. That implementation relies on the WebmentionFs .NET library, Azure Functions, and RSS. By using the WebmentionFs .NET library, you can more quickly implement similar solutions into your own apps and services. If you need a reference, check out my WebmentionService repository on GitHub which contains the code I use for receiving Webmentions.</p>\n<p>I'll leave you with three things:</p>\n<ol>\n<li>As you consider the next platform for your next online home, in addition to considering the health of the community, take some time to also think about the protocols it's built on top of and whether they give you more or less freedom.</li>\n<li>Try out the WebmentionFs library on NuGet and add Webmentions to your site.</li>\n<li>If you share this post anywhere publicly on the web, send me a Webmention.</li>\n</ol>\n<p>Catch you on the world wide web!</p>\n","tags":["fsharp","indieweb","protocol","internet","web","dotnet"]},{"title":"Get started with TiddlyWiki in VS Code","contentType":"posts","date":"2022-10-21 08:32","url":"/posts/tiddlywiki-vscode/","content":"\n<p>A while back, I set up a knowledge base (KB) section on my website. As part of that knowledge base, I have a <a href=\"/resources/wiki\">wiki</a> and <a href=\"/resources/snippets\">snippets</a> section. As opposed to the <a href=\"/posts\">blog posts</a> which tend to be more long-form and have structure or the microblog-like posts on my <a href=\"/notes\">main</a> and <a href=\"/responses\">response</a> feeds, the wiki and snippets section are intended to have self-contained blocks of information that are linked somehow but not a linear or chronological order.</p>\n<p>My current wiki and snippets solution was fairly basic. It was a series of documents authored in Markdown which then get converted to static HTML pages. The index pages for wiki articles and snippets contain links to each of those documents. While that's worked okay, I felt I was missing functionality when it came to linking between documents, tagging, and more importantly searching.</p>\n<p>While I had seen wiki solutions like <a href=\"https://www.mediawiki.org/wiki/MediaWiki\">MediaWiki</a>, they felt too bloated for my use case. Since my site is statically hosted, I didn't want to have to create, maintain, and pay for a server only to run my wiki. That's where TiddlyWiki comes in.</p>\n<p><a href=\"https://tiddlywiki.com/\">TiddlyWiki</a> describes itself as &amp;quot;a unique non-linear notebook for capturing, organising and sharing complex information&amp;quot;. In short, TiddlyWiki is a self-contained single HTML file with WYSIWYG-like functionality for creating a wiki. Because everything is contained in a single HTML file, it's static-site friendly.</p>\n<p>Now I had heard of TiddlyWiki before but dismissed it because I didn't get how it worked. For one, I was confused how the WYSIWYG GUI supported editing, viewing, and exporting of these notes since there was just a single HTML file. More importantly, when it comes to <a href=\"https://tiddlywiki.com/#Saving\">saving changes</a>, the recommendations on the website point users to browser extensions and apps. If it's just an HTML page, why can't I use the tools I'm already using to work with HTML?</p>\n<p>After giving it another look and tinkering a bit, I found I was right and could use tools like VS Code to work with TiddlyWiki. In this post, I'll show how you can get started with the empty edition of TiddlyWiki using VS Code and the Live Preview extension.</p>\n\n<ul>\n<li><a href=\"https://code.visualstudio.com/#alt-downloads\">Visual Studio Code</a></li>\n<li><a href=\"https://marketplace.visualstudio.com/items?itemName=ms-vscode.live-server\">Live Server Extension</a></li>\n</ul>\n<p>This guide was written on a Linux (Manjaro) PC but should work the same on Windows and macOS.</p>\n\n<p>The quickest way to get started with TiddlyWiki is to:</p>\n<ol>\n<li><p>Download the <a href=\"https://tiddlywiki.com/#Empty%20Edition\">empty edition</a>.</p>\n<p>A file called <em>empty.html</em> should download to your computer. For simplicity, I would recommend creating a new directory on your computer that only contains that file.</p>\n<p>The empty edition contains the core functionality. For other editions, see <a href=\"https://tiddlywiki.com/#Editions\">TiddlyWiki editions</a>.</p>\n</li>\n<li><p>Rename the file <em>index.html</em>.</p>\n</li>\n</ol>\n\n<h3>Edit your wiki</h3>\n<ol>\n<li>Open the directory containing your <em>index.html</em> file in VS Code.</li>\n<li>Open the command palette (View &amp;gt; Command Palette).</li>\n<li>Enter into the text box <em>Live Preview: Start Server</em> and press <em>Enter</em>. The server should start.</li>\n</ol>\n<p><img src=\"/files/images/lqdev-tiddlywiki.png\" class=\"img-fluid\" alt=\"lqdev TiddlyWiki Main Page\" /></p>\n<p>At this point, you can use the TiddlyWiki interface to create and edit your wiki.</p>\n<h3>Save your wiki</h3>\n<p>Saving can be a bit tricky but it's not too bad. Use the checkmark in the side pane to save your changes. When there are unsaved changes, the checkmark turns red.</p>\n<p><img src=\"/files/images/tiddlywiki-save-changes.png\" class=\"img-fluid\" alt=\"TiddlyWiki Side Panel Red Checkmark Save Changes\" /></p>\n<p>To save your changes, overwrite the existing file or if you want, you can also save it to a file with a different name. I would recommend saving it to the same file because it's easier to manage. You don't want to end up with <em>index-version12341.html</em>. That being said, I don't know if there may be risks in overwriting the file. In my limited testing I had no issues overwriting the file, but I'd still tread lightly.</p>\n<h3>Stop the server</h3>\n<p>Once you're done viewing and editing your wiki, stop the Live Preview server:</p>\n<ol>\n<li>In VS Code, open the command palette <strong>(View &amp;gt; Command Palette)</strong>.</li>\n<li>Enter into the text box <em>Live Preview: Stop Server</em> and press <em>Enter</em>. The server should stop.</li>\n</ol>\n\n<h3>Plugins</h3>\n<p>I prefer working in Markdown and since some of my notes include code, I addded two plugins to my wiki:</p>\n<ul>\n<li><a href=\"https://tiddlywiki.com/#Markdown%20Plugin\">Markdown</a></li>\n<li><a href=\"https://tiddlywiki.com/#Highlight%20Plugin\">Highlight</a></li>\n</ul>\n<p>For more information, see <a href=\"https://tiddlywiki.com/#Installing%20a%20plugin%20from%20the%20plugin%20library\">Installing a plugin from the plugin library</a>.</p>\n<h3>Things to watch out for in VS Code</h3>\n<p>When using the Live Preview extension, some things might not work such as keyboard shortcuts and navigating to external links. Since a local server is running you should be able to use the browser as well. However, I would recommend only using the browser to <em>view</em> your wiki. Trying to save changes made in the browser will download the HTML page containing your changes.</p>\n<h3>Use Git</h3>\n<p>Autosaving to Git is one of the ways you can save your TiddlyWiki. However, you have to enter and use your credentials. Since your wiki is a static HTML page, just save it like you would any other HTML file in a repo. Doing so also makes it easy to backup to a previous version in case data is corrupted or lost.</p>\n\n<p>I this post, I shared how TiddlyWiki provides a relatively low overhead, static-website friendly way of creating wikis. If you use VS Code, you can use the Live Preview extension to view and edit your wiki. Although my intended use for TiddlyWiki is as a personal knowledge management solution, I can also think of ways you could use TiddlyWiki as a low-friction way of building your own website. The WYSIWYG functionality makes creating content extremely more accessible than static site generators and similar to WordPress. In contrast to WordPress though, since you only have a single HTML file, it's less overhead to manage and also low-cost since you can <a href=\"https://nesslabs.com/tiddlywiki-static-website-generator\">host it on GitHub Pages for free</a> or any other file hosting service. In the near future, I plan to migrate my wiki and snippets to use TiddlyWiki.</p>\n","tags":["web","vscode","wiki","personalknowledgemanagement","git","website"]},{"title":"Site redirects in static websites","contentType":"posts","date":"07/31/2022 14:52","url":"/posts/static-website-redirects/","content":"\n<p>I recently came across the concept of <a href=\"https://indieweb.org/own_your_links\">owning your links</a> on the IndieWeb wiki and right away it made lots of sense to me. The main idea behind owning your links is using your domain as the source of truth and using it to redirect to your content on various other platforms you own. For example, when sharing my Twitter profile, instead of saying, &amp;quot;Go to https://twitter.com/ljquintanilla&amp;quot;, I can instead point people to <a href=\"http://lqdev.me/twitter\">http://lqdev.me/twitter</a>&amp;quot;. The link on my domain ultimately redirects to Twitter, but the main entrypoint is my website. There's various ways to handle redirects, but in most cases it's done on the server side. For static websites, if you're hosting them in GitHub Pages, Netlify, or Azure Blob Storage, you don't have a server. Therefore configuring redirects can be a challenge. In this post, I'll go through how you can configure redirects for your static website without a server or JavaScript.</p>\n\n<p>Depending on which generator you use to create your static website, how you create your  redirect pages will differ. However, the end result should look like the the following:</p>\n<pre><code class=\"language-html\">&amp;lt;html&amp;gt;\n    &amp;lt;head&amp;gt;\n        &amp;lt;meta http-equiv=&amp;quot;refresh&amp;quot; content=&amp;quot;0;REDIRECT-TARGET-URL&amp;quot;&amp;gt;\n    &amp;lt;/head&amp;gt;\n&amp;lt;/html&amp;gt;\n</code></pre>\n<p>The only thing you need to perform the redirect is add a <code>meta</code> tag to the endpoint you want to handle the redirect. If I wanted to redirect to Twitter, I might create an HTML page at <code>http://lqdev.me/twitter</code> on my website. Setting the <code>http-equiv</code> attribute to <code>refresh</code> instructs the browser to refresh the page. Then, you use the <code>content</code> attribute to define your redirect target and how long you want to wait before redirecting the user. In the snippet above, the number <code>0</code> indicates you want to wait 0 seconds before redirecting your users and the <code>REDIRECT-TARGET-URL</code> is where you want to redirect to. This could be your Twitter profile, YouTube channel, or anywhere else. Wherever you want to redirect to, you place it in the <code>content</code> attribute.</p>\n<p>It's important to note that the redirect happens automatically which takes control away from the user causing potential accesibility issues. Until now, I haven't found a better way of doing this that doesn't require JavaScript. For more information, see the <a href=\"https://www.w3.org/WAI/standards-guidelines/wcag/\">web content accessibility guidelines</a>.</p>\n<p>For more information on creating redirects in HTML, see the article <a href=\"https://www.w3docs.com/snippets/html/how-to-redirect-a-web-page-in-html.html\">how to redirect a web page in HTML</a>.</p>\n\n<p>So far I've created redirect pages for most of my social media profiles. You can see samples in my <a href=\"/contact/\">contact page</a>. It's important to note that while this works for social profiles and any other properties you own on the internet, it could also be used in cases where you've changed the structure of your website and want to gracefully handle redirects for broken / updated links. Happy coding!</p>\n","tags":["http","web","staticweb","indieweb","html"]},{"title":"Manage your reading list in Emacs using Org mode","contentType":"posts","date":"07/26/2022 19:15","url":"/posts/manage-reading-list-org-mode-emacs/","content":"\n<p>Emacs is a rich and customizable text editor. <a href=\"https://orgmode.org/\">Org mode</a> for Emacs makes it easy to manage to-do lists, projects, and author technical documents. Many of my workflows like writing, planning, and organization make heavy use of plain text. One reason I enjoy working in plain text is, it's easy for my thoughts and ideas to naturally flow without having tools get in the way. Another reason I like plain text is, I'm not locked-in to any tool. Because plain text can be authored and rendered almost anywhere, it's easy for me to use whichever tool I find most productive. Some that I've found great success with recently have been <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>, <a href=\"https://joplinapp.org/\">Joplin</a>, and Emacs with Org mode. I've been trying to manage my reading list in plain text and while it's worked well, I'd like a way to track the state for each book by tagging them as read or in various stages of unread. One of the features of Org mode is the ability to track workflows. In this guide, I'll show how you can extend the TODO Org mode functionality to keep track of your reading list.</p>\n\n<ul>\n<li><a href=\"https://www.gnu.org/software/emacs/\">Emacs</a></li>\n</ul>\n<p>Since Emacs is cross-platform, the contents of this guide should work the same across Linux, Mac, and Windows.</p>\n\n<p>Org mode in Emacs provides to-do list management features. By default, the built-in states are TODO and DONE. However, you can define your own states to best match your workflow. For more information, see the <a href=\"https://orgmode.org/manual/TODO-Items.html#TODO-Items\">TODO item section in the Org mode guide</a>.</p>\n<p>Start by creating a new file in Emacs with the <em>.org</em> file extension (i.e. reading-list.org).</p>\n<p>Then, add the following headers at the top of the file.</p>\n<pre><code class=\"language-text\">#+TODO: WISHLIST(w) QUEUED(q) INPROGRESS(p) | READ(r!)\n</code></pre>\n<p>What this heading does is, it defines the different states of completion of a TODO item. The scope of this definition is the file. Although you can define global states, I want these states to only exist in the context of this file. The complete and incomplete states are separated by the pipe character (<code>|</code>). In this case, WISHLIST, QUEUED, and INPROGRESS are incomplete states and READ is the complete state. The letters in the parentheses are ways to quickly assign each of the states. One last thing you'll notice is the READ state in addition to having a letter also has an exclamation point. The exclamation point tells Emacs to add a timestamp whenever this book transitions to the READ state.</p>\n\n<p>Once you've define your reading states, it's time to add your books. TODO items in Emacs Org-mode only work in the context of headings. Therefore you have to prefix your books with asterisks. For more information on headlines, see the <a href=\"https://orgmode.org/manual/Headlines.html\">Headlines section in the Org mode guide</a>.</p>\n<p>Here are examples of some books in my reading list.</p>\n<pre><code class=\"language-text\">* Hackers (Steven Levy)\n* Ghost In The Wires (Kevin Mitnick)\n</code></pre>\n<p>Since I don't intend to have sub headlines, I just used a top-level headline for each of the books.</p>\n\n<p>Now that you've populated your list of books, it's time to assign states to them.</p>\n<p>To assign a state:</p>\n<ol>\n<li>Place your pointer / cursor on the same line as the book you want to assign the state to.</li>\n<li>Press <code>Ctrl-c</code> + <code>Ctrl-t</code> or in Emacs notation <code>C-c C-t</code>.</li>\n<li>A buffer opens prompting you to choose a state. Enter one of the letters to choose your state. For example, if you want to apply the QUEUED state to your book, press the letter <code>q</code>.</li>\n</ol>\n<p>The state you selected is prepended to your book. For example:</p>\n<pre><code class=\"language-text\">* QUEUED Hackers (Steven Levy)\n</code></pre>\n\n<p>To update the state, you can follow the steps in the assign state section or you can cycle through the various states by pressing <code>Alt-Left</code> or <code>Alt-Right</code> or in Emacs notation <code>M-LEFT</code> or <code>M-RIGHT</code>.</p>\n\n<p>To assign the READ state to a book, you need to update its state. The only difference between READ and the other states is, a timestamp is applied when the book transitions to the state.</p>\n<p>Here's an example of what a book in the READ state looks like:</p>\n<pre><code class=\"language-text\">* READ In The Beginning...Was The Command Line (Neal Stephenson) \n- State &amp;quot;READ&amp;quot;       from &amp;quot;INPROGRESS&amp;quot; [2022-07-26 Tue 00:18]\n</code></pre>\n\n<p>Org mode has agenda views that allow you to view TODO, timestamped, and tagged items in a single place. One of the built-in agendas that is useful for querying TODO items is the match view. The match view shows headlines based on the tags, properties, and TODO state associated with them. For more information, see the <a href=\"https://orgmode.org/manual/Agenda-Views.html\">Agenda views section in the Org-mode guide</a>.</p>\n<p>Before viewing your books in an agenda view, you need to add the file containing your reading list to the list of files tracked by Org. To do so, open the file in Emacs and press <code>Ctrl-c + [</code> or in Emacs notation <code>C-c [</code>.</p>\n<p>To launch the match agenda view in org-mode:</p>\n<ol>\n<li>Press <code>Alt-x</code> or in Emacs notation <code>M-x</code></li>\n<li>In the minibuffer, type <code>org-agenda</code> and press Enter.</li>\n<li>A buffer appears prompting you to select a view. Press the letter <code>m</code> to choose the match view.</li>\n</ol>\n<h3>Unread items</h3>\n<p>In this case, I want to see all the unfinished books which in this case are under the WISHLIST, QUEUED, and INPROGRESS states.</p>\n<p>In the match view, enter the following query into the minibuffer:</p>\n<pre><code class=\"language-text\">TODO=&amp;quot;WISHLIST&amp;quot;|TODO=&amp;quot;QUEUED&amp;quot;|TODO=&amp;quot;INPROGRESS&amp;quot;\n</code></pre>\n<p>Press <code>Enter</code>.</p>\n<p>The result of the query should look similar to the following:</p>\n<pre><code class=\"language-text\">Headlines with TAGS match: TODO=&amp;quot;WISHLIST&amp;quot;|TODO=&amp;quot;QUEUED&amp;quot;|TODO=&amp;quot;INPROGRESS&amp;quot;\nPress ‘C-u r’ to search again\n  reading-list:QUEUED Hackers (Steven Levy)\n  reading-list:WISHLIST Ghost In The Wires (Kevin Mitnick)\n</code></pre>\n<h3>Read items</h3>\n<p>To query for read items, you go through the same process, except the query is different.</p>\n<p>In the match view, enter the following query into the minibuffer:</p>\n<pre><code class=\"language-text\">TODO=&amp;quot;READ&amp;quot;\n</code></pre>\n<p>Press <code>Enter</code>.</p>\n<p>The result of the query should look similar to the following:</p>\n<pre><code class=\"language-text\">Headlines with TAGS match: TODO=&amp;quot;READ&amp;quot;\nPress ‘C-u r’ to search again\n  reading-list:READ In The Beginning...Was The Command Line (Neal Stephenson)\n</code></pre>\n\n<p>Reading lists are just one of the many things you can track using Emacs and Org mode. You can use this general process to track movies, TV shows, podcasts, etc. Thanks to the extensibility of Emacs, the options are endless!</p>\n","tags":["linux","emacs","organization"]},{"title":"The lqvlc network protocol","contentType":"posts","date":"07/07/2022 18:47","url":"/posts/lqvlc-network-protocol-firefox/","content":"\n<p>Whether it's videos, online radio, podcasts, or my personal music collection, VLC is my default player. I especially enjoy the ability of playing an online audio or video stream using the URL. What I don't like about that process is that I usually have to manually copy the link and use either the terminal or graphical user interface to play the stream. While not a large number of additional steps, it's still not ideal. The experience I would like is to automatically launch VLC whenever I click a link to an audio or video stream. That's possible using custom network protocols in Firefox. In this guide, I'll show how I defined and configured my own protocol (lqvlc) in Firefox to automatically launch VLC.</p>\n\n<ul>\n<li>VLC</li>\n<li>Firefox</li>\n</ul>\n<p>This guide was built to work on unix systems. However, setting up something similar with a PowerShell or CMD script on Windows should work similarly.</p>\n\n<p>A network protocol defines how data is exchanged and transferred over a network. Some common protocols are HTTP, FTP, and SSH. In the context of browsers, the most common protocol is HTTP and the way the browser identifies the protocol is based on the scheme portion of the URL. So for the website <a href=\"http://lqdev.me\">http://lqdev.me</a>, http is the scheme that tells the browser which protocol to use. For more information on network protocols and URLs, see the article <a href=\"https://developer.mozilla.org/docs/Learn/Common_questions/What_is_a_URL\">What is a URL?</a>.</p>\n<p>Firefox has a set of standard protocols like HTTP that are configured out of the box. However, you have the option of configuring a custom protocol.</p>\n\n<p>Now that you have a general idea of what network protocols are and how browsers handle them, it's time to introduce the lqvlc protocol. The intent of the lqvlc protocol is to automatically take a URL from an online audio or video stream and launch it in VLC. lqvlc protocol URLs use lqvlc as the scheme instead of http.</p>\n<p>For example:</p>\n<table class=\"table\">\n    <tr>\n        <th>Original Audio Stream Link</th>\n        <th>lqvlc Link</th>\n    </tr>\n    <tr>\n        <td><a href=\"https://somafm.com/u80s64.pls\">https://somafm.com/u80s64.pls</a></td>\n        <td><a href=\"lqvlc://somafm.com/u80s64.pls\">lqvlc://somafm.com/u80s64.pls</a></td>\n    </tr>    \n</table>\n<p>You'll notice the only difference is the scheme. That's enough information for the browser to know, lqvlc is a protocol I need to handle in some way. In the next sections, I'll describe what you need to do to get Firefox to recognize and handle the lqvlc protocol.</p>\n\n<p>To configure a custom network protocol</p>\n<ol>\n<li><p>Launch Firefox</p>\n</li>\n<li><p>In the address bar, navigate to <code>about:config</code> and select <strong>Accept the Risk and continue</strong>. As the warning mentions, these are advanced configurations so proceed with caution.</p>\n</li>\n<li><p>Add the following configuration preferences:</p>\n <table class=\"table\">\n     <tr>\n         <th>Configuration Preference</th>\n         <th>Type</th>\n         <th>Value</th>\n     </tr>\n     <tr>\n         <td><i>network.protocol-handler.expose.lqvlc</i></td>\n         <td>Boolean</td>\n         <td>false</td>\n     </tr>                            \n     <tr>\n         <td><i>network.protocol-handler.external.lqvlc</i></td>\n         <td>Boolean</td>\n         <td>true</td>\n     </tr>\n     <tr>\n         <td><i>network.protocol-handler.warn-external.lqvlc</i></td>\n         <td>Boolean</td>\n         <td>true</td>\n     </tr>\n </table>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/177898847-79cce773-9275-4e4d-8e7c-a6b5fbfe8f48.png\" class=\"img-fluid\" alt=\"Firefox lqvlc network protocol handler configuration preferences\" /></p>\n<p>The last two tell Firefox to let an external application handle files with the protocol you've defined. In this case, that's <code>lqvlc</code>. If you're configuring your own protocol, you'd replace the lqvlc in the configuration preference with your own.</p>\n</li>\n</ol>\n\n<p>Now that Firefox knows that it must use an external application to handle lqvlc URLs, it's time to configure the handler. The easiest way to create a handler is to use a bash script.</p>\n<ol>\n<li><p>Create a new file called <em>lqvlc-handler.sh</em> with the following content:</p>\n<pre><code class=\"language-bash\">#!/bin/bash\n\nlink=$1\n\nvlc &amp;quot;${link/lqvlc/http}&amp;quot;\n</code></pre>\n<p>The script takes the first argument it's passed, an lqvlc URL, replaces the scheme with <code>http</code>, and launches it in VLC.</p>\n</li>\n<li><p>Make the file executable. In the terminal, run the following command:</p>\n<pre><code class=\"language-bash\">chmod +x lqvlc-handler.sh\n</code></pre>\n</li>\n</ol>\n<p>That's it! Now you just need to test it out.</p>\n\n<p>To use it for your own purposes, you'd have to create or update an HTML page and set the <code>href</code> in your anchor tags to use <code>lqvlc</code> instead of <code>http</code>. However, I've already configured lqvlc links in my own website. If you go to the <a href=\"/radio\">radio page</a> on my website, you can click on any of the lqvlc links. Alternatively, You can also click on the link to the <a href=\"lqvlc://somafm.com/u80s64.pls\">SOMA FM Underground 80's station lqvlc link</a>. A prompt will pop up asking you to choose which app you want to use to handle the link. Select the lqvlc-handler.sh script you created. If everything is configured correctly, VLC should launch and the radio stations should start playing.</p>\n\n<p>Although I've gotten this working for my personal site, I'd like to be able to use the lqvlc protocol in any website I visit that has links to audio and video streams. Arguably, I can also launch lqvlc links from other websites. However, that would require website owners to configure them. The next step I think to make that happen would be to create an Add-On that automatically rewrites stream links to use the lqvlc protocol.</p>\n","tags":["firefox","vlc","web","open source","linux"]},{"title":"Install Manjaro RTL8821CE WiFi drivers","contentType":"posts","date":"06/11/2022 15:48","url":"/posts/install-rtl8821ce-manjaro/","content":"<p>Until recently, I was usign an HP ProBook 430 G1 with Manjaro and a Surface Go (1st Gen) for my computing needs. To cut down on the devices I used, I purchased an <a href=\"https://www.asus.com/us/laptops/for-home/everyday-use/asus-l210/\">ASUS L210MA</a> which is about the size of the Surface Go but I can run Manjaro on it. Although it doesn't have the same storage or compute power as the ProBook, I've set up a VM in Azure I can use for those instances where I need more compute power than the ASUS laptop can provide. As is sometimes the case, when you install Linux on a new PC, you have to install the right drivers to use your WiFi and other peripheral devices. In this guide I'll show how to install the RTL-8821-CE WiFi drivers.</p>\n\n<p>The instructions on this guide were run on a Manjaro XCFE 21.2.6.</p>\n\n<p>The WiFi drivers you'll need are hosted in the <a href=\"https://aur.archlinux.org/\">Arch User Repository (AUR)</a>. Before installing any packages, you'll have to enable it on your PC. To do so, install the <code>yay</code> AUR helper package by running the following command in the terminal:</p>\n<pre><code class=\"language-bash\">sudo pacman -S git\ngit clone https://aur.archlinux.org/yay.git\ncd yay\nmakepkg -si\n</code></pre>\n\n<p>The first thing yuo'll need to install are your kernel headers. To check which version of the kernel you're using, run the following command in the terminal:</p>\n<pre><code class=\"language-bash\">uname -r\n</code></pre>\n<p>The output should look similar to the following:</p>\n<pre><code class=\"language-text\">5.15.41-1-MANJARO\n</code></pre>\n<p>The first two values are your major kernel version. For example, in the output above the kernel version is 5.15.</p>\n<p>To install the kernel headers, run the following command in the terminal:</p>\n<pre><code class=\"language-bash\">sudo pacman -S linux-headers\n</code></pre>\n<p>Make sure that the kernel version that's installed is the same as the one output after running the <code>uname -r</code> command.</p>\n\n<p>Once you've installed the headers, it's time to install the WiFi drivers. Use <code>yay</code> to run the following command in the terminal:</p>\n<pre><code class=\"language-bash\">yay -S rtl8821ce-dkms-git\n</code></pre>\n<p>After some time, the packages should be installed on your device. Reboot your PC and you should now be able to connect to the internet wirelessly</p>\n","tags":["networking","manjaro","linux"]},{"title":"Install OverDrive Media Console on Manjaro","contentType":"posts","date":"05/11/2022 19:22","url":"/posts/overdrive-wine-manjaro/","content":"\n<p>For the last few months, Manjaro has been the operating system I use as my daily driver for personal tasks. I still have my Surface Go tablet which runs Windows 10 that I use for tasks line syncing my Garmin watch and GPS. On one of the <a href=\"/notes/ipod-touch-discontinued\">posts in my feed</a>, I briefly mentioned my audio solution for podcasts and audiobooks which mainly consists of a standalone MP3 player (<a href=\"https://fiio.com/m6\">FiiO M6</a>).</p>\n<p>For podcasts, I manage them through my <a href=\"https://newsblur.com/\">NewsBlur RSS feed reader</a>. When I want to download them for offline listening, I've written a .NET console app (<a href=\"https://github.com/lqdev/podnet\">podnet</a>) to manage that process for me. Audiobooks are a little trickier. Public domain works I get through <a href=\"https://librivox.org/\">Librivox</a>. The limitation there though is that I'm only public domain works are available. Therefore if I want other audiobooks, I either have to purchase them (which has its own issues because of DRM) or borrow them from my local library. I prefer to borrow my books and audiobooks from the library. Not only am I supporting my local library but I also save money in the process.</p>\n<p>Many libraries including my own manage digital assets through <a href=\"https://www.overdrive.com/\">OverDrive</a>. OverDrive offers a variety of ways for transferring books and audiobooks to your devices. Linux however is not one of those solutions. Until February 2022, OverDrive provided a Windows desktop application (OverDrive Media Console) for managing books and audiobooks. Although there is no Linux desktop application, Linux has Wine. <a href=\"https://www.winehq.org/\">Wine</a> according to their website is &amp;quot;...a compatibility layer capable of running Windows applications on several POSIX-compliant operating systems, such as Linux, macOS, &amp;amp; BSD.&amp;quot;.</p>\n<p>In this post, I'll show how you can use Wine to install OverDrive Media Console on a PC running the Manjaro Linux distribution.</p>\n\n<p>In order to install OverDrive Media Console, you'll need to install Wine. To do so:</p>\n<ol>\n<li><p>Open the terminal.</p>\n</li>\n<li><p>Install Wine and all its dependencies with the following command:</p>\n<pre><code class=\"language-bash\">sudo pacman -S wine winetricks wine-mono wine_gecko\n</code></pre>\n</li>\n</ol>\n\n<p>The next step is to install the OverDrive Media Console application for Windows desktop.</p>\n<ol>\n<li><p>Download the latest version of <a href=\"https://www.overdrive.com/apps/overdrive./\">OverDrive Media Console for Windows Desktop application</a>. This should be a Microsoft Installer file with the <code>.msi</code> extension. Note that as of February 2022, the desktop application is <a href=\"https://help.overdrive.com/en-us/0733.html?tocpath=Home%7CGet%20help%20with%20the%20OverDrive%20app%7CWindows%20(desktop)%7C_____7\">no longer available for install</a>.</p>\n</li>\n<li><p>Open the terminal and use Wine to install OverDrive Media Console with the following command:</p>\n<pre><code class=\"language-bash\">wine msiexec /i ODMediaConsoleSetup.msi\n</code></pre>\n<p>Update the name of the <code>.msi</code> file as needed.</p>\n</li>\n</ol>\n\n<p>Once the installation is complete, OverDrive Media Console should be in your list of applications. Run it as you would any other app.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/167964715-bb8bfb21-5f72-4a86-9fb0-33c9d1fe9eae.png\" class=\"img-fluid\" alt=\"OverDrive Media Console running on Manjaro Linux\" /></p>\n<p>Here's a guide for information on getting started with <a href=\"https://help.overdrive.com/en-us/categories/getting-started.htm\">OverDrive</a>.</p>\n","tags":["linux","books","audiobooks","manjaro"]},{"title":"How to listen to internet radio using VLC","contentType":"posts","date":"12/28/2021 16:24","url":"/posts/how-to-listen-internet-radio-using-vlc/","content":"\n<p>Music streaming services like Spotify generally do a good job of making recommendations based on your music tastes. By applying algorithms to your listening history, they're able to provide you with a listening experience tailored to you. When it comes to music discovery though, it can be difficult to find new music so you often end up listening to music that sounds very similar. Internet radio can help expose you to music you otherwise wouldn't normally listen to. Radio stations like those on <a href=\"https://somafm.com/\">SomaFM</a> provide a wide variety of high quality music channels. There's other online radio stations out there, many which provide a built-in audio player. As I've written in previous posts though (<a href=\"/posts/how-to-watch-twitch-using-vlc/\">How to watch Twitch streams using VLC</a> &amp;amp; <a href=\"/notes/rss-vlc-podcast-management\">Podcast management with RSS &amp;amp; VLC</a>), VLC is a versatile tool. As a result, it's no surprise you can also use it to listen to online radio streams. This post is mostly a note to myself, but I encourage you to try it as well.</p>\n\n<ul>\n<li><a href=\"https://www.videolan.org/index.html\">VLC</a></li>\n</ul>\n<p>The commands below were run on Linux. Since VLC is cross-platform, the same commands should work on those platforms as well.</p>\n\n<p>The first thing you'll need is the URL of the stream you want to listen to. Usually it's provided on the station's website. You can also browse through <a href=\"https://www.radio-browser.info/\">RadioBrowser</a> to find a station and its stream URL.</p>\n<p>Once you have the URL, in the terminal, enter the following command:</p>\n<pre><code class=\"language-bash\">vlc &amp;lt;STREAM-URL&amp;gt;\n</code></pre>\n<p>Replace <code>&amp;lt;STREAM-URL&amp;gt;</code> with the address of your stream.</p>\n<p>After a few seconds, VLC launches and your stream should start playing.</p>\n<p>If you prefer to stream audio without VLC's visual interface, use the following command.</p>\n<pre><code class=\"language-bash\">cvlc &amp;lt;STREAM-URL&amp;gt;\n</code></pre>\n<p>Note that without the UI you won't have access to any playback controls.</p>\n<p>Happy listening!</p>\n","tags":["vlc","open source","radio"]},{"title":"Sending Webmentions with F#","contentType":"posts","date":"12/13/2021 20:53","url":"/posts/sending-webmentions-fsharp-fsadvent/","content":"\n<blockquote class=\"blockquote\">\n<p>This post is part of the <a href=\"https://sergeytihon.com/2021/10/18/f-advent-calendar-2021\">F# Advent 2021</a>.</p>\n</blockquote>\n<p>Webmentions is a W3C specification that defines a standard way of implementing notifications between websites. It enables rich interactions between websites using standard protocols. According to the spec, &amp;quot;Webmention is a simple way to notify any URL when you mention it on your site. From the receiver's perspective, it's a way to request notifications when other sites mention it&amp;quot;.</p>\n<p>If you're interested in learning more about the spec and overall use cases, see the following links:</p>\n<ul>\n<li><a href=\"https://boffosocko.com/2018/07/19/webmentions-enabling-better-communication-on-the-internet-2/\">Webmentions: Enabling better communication on the internet</a></li>\n<li><a href=\"https://aaronparecki.com/2018/06/30/11/your-first-webmention\">Sending your first Webmention from scratch</a></li>\n<li><a href=\"https://www.w3.org/TR/webmention/\">Webmentions W3C specification</a></li>\n<li><a href=\"https://webmention.rocks/\">Webmentions implementation tests</a></li>\n</ul>\n<p>The Webmentions protocol has send and receive components. In this post, I'll go over a lightweight implementation for sending Webmentions using F# and test it using the <a href=\"https://webmention.rocks/\">webmention.rocks</a> website. Source code for this post can be found at the <a href=\"https://github.com/lqdev/fsadvent-2021-webmentions\">fsadvent-2021-webmentions GitHub repository</a>.</p>\n\n<p>Let's say you want to use webmentions to reply to or mention content from another site. The general workflow works as follows:</p>\n<ol>\n<li><p>You create a document (source) that mentions content from another website (target).</p>\n</li>\n<li><p>You perform discovery on the target website for the webmention endpoint URL. The URL might be in one of three places:</p>\n<ol>\n<li>HTTP response headers.</li>\n<li><code>&amp;lt;link&amp;gt;</code> tag with <code>rel=webmention</code> attribute.</li>\n<li><code>&amp;lt;a&amp;gt;</code> tag with <code>rel=webmention</code> attribute.</li>\n</ol>\n<p>The discovery is performed in order and each subsequent option works as a fallback of the other.</p>\n</li>\n<li><p>You make an HTTP POST request notifying the source that you've mentioned their article on your site. The request body is in <code>x-www-form-urlencoded</code> form containing the <code>source</code> and <code>target</code> parameters where source is the URL of your article and the target is the URL of article being mentioned.</p>\n</li>\n</ol>\n<p>For more details on this workflow, see the <a href=\"https://www.w3.org/TR/webmention/#sending-webmentions\">Sending Webmentions section</a> of the specification.</p>\n\n<p>The document can be any publicly hosted HTML document as long as you have a source URL to provide during your notification request.</p>\n<p>For example, you can have something like the following:</p>\n<pre><code class=\"language-html\">&amp;lt;!doctype html&amp;gt;\n&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;\n&amp;lt;head&amp;gt;\n    &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;\n    &amp;lt;meta http-equiv=&amp;quot;X-UA-Compatible&amp;quot; content=&amp;quot;IE=edge&amp;quot;&amp;gt;\n    &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;\n    &amp;lt;title&amp;gt;F# Advent 2021 - Webmentions&amp;lt;/title&amp;gt;\n&amp;lt;/head&amp;gt;\n    &amp;lt;body&amp;gt;\n        &amp;lt;a href=&amp;quot;https://webmention.rocks/test/1&amp;quot;&amp;gt;F# Advent 2021 - This is a great post&amp;lt;/a&amp;gt;\n    &amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n</code></pre>\n<p>Using <a href=\"http://microformats.org/wiki/h-entry\">microformats</a> though, you can annotate your HTML to provide more context to the receiver. The same document with microformats providing the author, content, and source document being replied to might look like the following with microformats:</p>\n<pre><code class=\"language-html\">&amp;lt;!DOCTYPE html&amp;gt;\n&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;\n&amp;lt;head&amp;gt;\n    &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;\n    &amp;lt;meta http-equiv=&amp;quot;X-UA-Compatible&amp;quot; content=&amp;quot;IE=edge&amp;quot;&amp;gt;\n    &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;\n    &amp;lt;title&amp;gt;F# Advent 2021 - Webmentions&amp;lt;/title&amp;gt;\n&amp;lt;/head&amp;gt;\n&amp;lt;body&amp;gt;\n    &amp;lt;div class=&amp;quot;h-entry&amp;quot;&amp;gt;\n        &amp;lt;div class=&amp;quot;u-author h-card&amp;quot;&amp;gt;\n            &amp;lt;a href=&amp;quot;http://lqdev.me&amp;quot; class=&amp;quot;u-url p-name&amp;quot;&amp;gt;lqdev&amp;lt;/a&amp;gt;\n        &amp;lt;/div&amp;gt;\n        &amp;lt;p&amp;gt;In reply to: &amp;lt;a href=&amp;quot;https://webmention.rocks/test/1&amp;quot; class=&amp;quot;u-in-reply-to&amp;quot;&amp;gt;@webmention.rocks&amp;lt;/a&amp;gt;&amp;lt;/p&amp;gt;\n        &amp;lt;p class=&amp;quot;e-content&amp;quot;&amp;gt;F# Advent 2021 - This is a great post&amp;lt;/p&amp;gt;\n    &amp;lt;/div&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n</code></pre>\n<p>Receiving Webmentions is beyond the scope of this post, so I'll skip over them. Once you have a document and make it publicly accessible to the web, it's time to find where to send notifications to.</p>\n\n<p>To discover where to send the Webmention to, send an HTTP GET or HTTP HEAD request to the target URL. Once you get the response, the Webmention endpoint URL should be in one of three places:</p>\n<ul>\n<li><p>HTTP headers</p>\n<pre><code class=\"language-text\">GET /test/1 HTTP/1.1\nHost: webmention.rocks\nHTTP/1.1 200 OK\nLink: &amp;lt;https://webmebtion.rocks/test/1/webmention&amp;gt;; rel=&amp;quot;webmention&amp;quot;\n</code></pre>\n</li>\n<li><p><code>&amp;lt;link&amp;gt;</code> tag</p>\n<pre><code class=\"language-html\">&amp;lt;html&amp;gt;\n    &amp;lt;head&amp;gt;\n    ...\n        &amp;lt;link href=&amp;quot;http://webmention.rocks/test/1/webmention&amp;quot; rel=&amp;quot;webmention&amp;quot; /&amp;gt;\n    ...\n    &amp;lt;/head&amp;gt;\n&amp;lt;/html&amp;gt;\n</code></pre>\n</li>\n<li><p><code>&amp;lt;a&amp;gt;</code> tag</p>\n<pre><code class=\"language-html\">&amp;lt;html&amp;gt;\n    &amp;lt;body&amp;gt;\n    ....\n    &amp;lt;a href=&amp;quot;http://webmention.rocks/test/1/webmention&amp;quot; rel=&amp;quot;webmention&amp;quot;&amp;gt;webmention&amp;lt;/a&amp;gt;\n    ...\n    &amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n</code></pre>\n</li>\n</ul>\n<p>The following examples show how you'd implement the discovery process for each of these scenarios in F#.</p>\n<h3>Discover Webmention URL in header</h3>\n<p>The <code>discoverUrlInHeaderAsync</code> function uses the .NET HttpClient to make an HTTP HEAD request to get the headers from the target URL provided. Then, a search is performed for a &amp;quot;link&amp;quot; header containing the text &amp;quot;webmention&amp;quot;. Once the header is found, it's sanitized to extract the Webmention endpoint URL of the target site.</p>\n<pre><code class=\"language-fsharp\">let discoverUrlInHeaderAsync (url:string) =\n    async {\n        // Initialize HttpClient\n        use client = new HttpClient()\n\n        // Prepare request message\n        let reqMessage = new HttpRequestMessage(new HttpMethod(&amp;quot;HEAD&amp;quot;), url)\n        \n        // Send request\n        let! response = client.SendAsync(reqMessage) |&amp;gt; Async.AwaitTask\n\n        // Get request headers\n        let responseHeaders = \n            [\n                for header in response.Headers do\n                    header.Key.ToLower(), header.Value\n            ]\n\n        // Look for webmention header\n        try\n            // Find &amp;quot;link&amp;quot; header that contains &amp;quot;webmention&amp;quot;\n            let webmentionHeader =\n                responseHeaders\n                |&amp;gt; Seq.filter(fun (k,_) -&amp;gt; k = &amp;quot;link&amp;quot;)\n                |&amp;gt; Seq.map(fun (_,v) -&amp;gt; v |&amp;gt; Seq.filter(fun header -&amp;gt; header.Contains(&amp;quot;webmention&amp;quot;)))\n                |&amp;gt; Seq.head\n                |&amp;gt; List.ofSeq\n                |&amp;gt; List.head\n\n            // Get first part of &amp;quot;link&amp;quot; header\n            let webmentionUrl = \n                webmentionHeader.Split(';')\n                |&amp;gt; Array.head\n\n            // Remove angle brackets from URL\n            let sanitizedWebmentionUrl = \n                webmentionUrl\n                    .Replace(&amp;quot;&amp;lt;&amp;quot;,&amp;quot;&amp;quot;)\n                    .Replace(&amp;quot;&amp;gt;&amp;quot;,&amp;quot;&amp;quot;)\n                    .Trim()\n\n            return Some sanitizedWebmentionUrl                    \n\n        with\n            | _ -&amp;gt; return None\n        \n    }\n</code></pre>\n<h3>Discover Webmention URL endpoint in link tag</h3>\n<p>The next place to check for a Webmention endpoint URL is a <code>&amp;lt;link&amp;gt;</code> tag in the document. In this example, the <code>discoverUrlInLinkTagAsync</code> function uses <code>FSharp.Data</code> library to get the contents of the target URL and parse the HTML contents to find a link tag with a <code>rel=webmention</code> attribute value.</p>\n<pre><code class=\"language-fsharp\">let discoverUrlInLinkTagAsync (url:string) = \n    async {\n        // Load HTML document\n        let! htmlDoc = HtmlDocument.AsyncLoad(url)\n\n        // Get webmention URL from &amp;quot;&amp;lt;link&amp;gt;&amp;quot; tag\n        try\n            let webmentionUrl = \n                htmlDoc.CssSelect(&amp;quot;link[rel='webmention']&amp;quot;)\n                |&amp;gt; List.map(fun link -&amp;gt; link.AttributeValue(&amp;quot;href&amp;quot;))\n                |&amp;gt; List.head\n\n            return Some webmentionUrl\n        with\n            | _ -&amp;gt; return None\n    }\n</code></pre>\n<h3>Discover Webmention endpoint URL in anchor tag</h3>\n<p>The last place to check for the Webmention endpoint URL is in an anchor tag in the contents of the target URL. In this example, the <code>discoverUrlInAnchorTagAsync</code> uses the <code>FSharp.Data</code> library to get the contents of the target URL and parse the HTML contents to find an anchor tag with a <code>rel=webmention</code> attribute value.</p>\n<pre><code class=\"language-fsharp\">let discoverUrlInAnchorTagAsync (url:string) = \n    async {\n        // Load HTML document\n        let!  htmlDoc = HtmlDocument.AsyncLoad(url)\n\n        // Get webmention URL from &amp;quot;&amp;lt;a&amp;gt;&amp;quot; tag\n        try\n            let webmentionUrl = \n                htmlDoc.CssSelect(&amp;quot;a[rel='webmention'&amp;quot;)\n                |&amp;gt; List.map(fun a -&amp;gt; a.AttributeValue(&amp;quot;href&amp;quot;))\n                |&amp;gt; List.head\n\n            return Some webmentionUrl\n        with\n            | _ -&amp;gt; return None\n    }\n</code></pre>\n<p>Once the utility functions are in place to handle the different scenarios, create a new function to perform the discovery workflow.</p>\n<pre><code class=\"language-fsharp\">// Apply webmention URL discovery workflow\n// 1. Check header\n// 2. Check link tag\n// 3. Check anchor tag\nlet discoverWebmentionUrlAsync (url:string) = \n    async {\n        let! headerUrl = discoverUrlInHeaderAsync url\n        let! linkUrl = discoverUrlInLinkTagAsync url\n        let! anchorUrl = discoverUrlInAnchorTagAsync url\n\n        // Aggregate results\n        let discoveryResults = [headerUrl; linkUrl; anchorUrl]\n\n        // Unwrap and take the first entry containing a value\n        let webmentionUrl = \n            discoveryResults\n            |&amp;gt; List.choose(fun url -&amp;gt; url)\n            |&amp;gt; List.head\n\n        return webmentionUrl\n    }\n</code></pre>\n<p>The <code>discoverWebmentionUrlAsync</code> calls all of the discovery utility methods and chooses only the results that successfully extracted a Webmention endpoint URL. In the event of multiple endpoints, the first one is takes precedence.</p>\n\n<p>Now that you have a publicly accessible document and the Webmention URL endpoint to send your Webmention to, it's time to send it.</p>\n<pre><code class=\"language-fsharp\">let sendWebMentionAsync (url:string) (req:IDictionary&amp;lt;string,string&amp;gt;) = \n    async {\n        use client = new HttpClient()\n        let content = new FormUrlEncodedContent(req)\n        let! response = client.PostAsync(url, content) |&amp;gt; Async.AwaitTask\n        return response.IsSuccessStatusCode\n    }\n</code></pre>\n<p>The <code>sendWebMentionAsync</code> function uses the .NET HttpClient to send an HTTP POST request to the Webmention endpoint URL you just extracted.</p>\n<p>Define your source and target URLs</p>\n<pre><code class=\"language-fsharp\">let sourceUrl = new Uri(&amp;quot;https://raw.githubusercontent.com/lqdev/fsadvent-2021-webmentions/main/reply.html&amp;quot;)\nlet targetUrl = new Uri(&amp;quot;https://webmention.rocks/test/1&amp;quot;)\n</code></pre>\n<p>The source URL I used in this example is hosted on <a href=\"https://raw.githubusercontent.com/lqdev/fsadvent-2021-webmentions/main/reply.html\">GitHub</a> and the target URL is the first implementation test in the <a href=\"https://webmention.rocks/test/1\">webmention.rocks</a> website.</p>\n<p>Then, create a function to run the entire workflow end-to-end.</p>\n<pre><code class=\"language-fsharp\">let runWebmentionWorkflow () = \n    async {\n        // Discover webmention endpoint URL of target URL\n        let! discoveredUrl = \n            targetUrl.OriginalString\n            |&amp;gt; discoverWebmentionUrlAsync\n\n        // Construct URL depending on whether it's absolute or relative\n        let authority = targetUrl.GetLeftPart(UriPartial.Authority)\n\n        let constructedUrl = \n            match (discoveredUrl.Contains(&amp;quot;http&amp;quot;)) with\n            | true -&amp;gt; discoveredUrl\n            | false -&amp;gt; \n                let noQueryUrl = \n                    discoveredUrl.Split('?')\n                    |&amp;gt; Array.head\n                    \n                $&amp;quot;{authority}{noQueryUrl}&amp;quot;\n\n        // Prepare webmention request data\n        let reqData = \n            dict [\n                (&amp;quot;source&amp;quot;, sourceUrl.OriginalString)\n                (&amp;quot;target&amp;quot;, targetUrl.OriginalString)\n            ]\n\n        // Send web mentions\n        return! sendWebMentionAsync constructedUrl reqData\n    }\n</code></pre>\n<p>Now that you have everything set up, you're ready to send your webmention!</p>\n<pre><code class=\"language-fsharp\">runWebmentionWorkflow ()\n|&amp;gt; Async.RunSynchronously\n</code></pre>\n<p>If your post is successful, you should see it on the target URL.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/145919398-2576bfef-3a5d-44f4-9813-9cf933539ede.png\" class=\"img-fluid\" alt=\"Display of Webmention post on webmention.rocks\" /></p>\n\n<p>In this post, I showed how you can use F# to implement sending webmentions using F#. This is not a full implementation as there are still scenarios I need to account for like receiving webmentions. There are community maintained <a href=\"https://indieweb.org/Webmention-developer#Libraries\">libraries</a>, <a href=\"https://indieweb.org/Webmention#Publishing_Software\">plugins</a>, and <a href=\"https://indieweb.org/Webmention#Services\">services</a> to simplify the process, but because the specification is built on open protocols and standards, it's possible to build your own implementation in F# and integrate it into your website. Happy coding!</p>\n","tags":[]},{"title":"Super simple captcha","contentType":"posts","date":"10/24/2021 13:00","url":"/posts/super-simple-captcha/","content":"\n<p>I've been thinking about implementing webmentions on this site. According to the spec, &amp;quot;Webmention is a simple way to notify any URL when you mention it on your site. From the receiver's perspective, it's a way to request notifications when other sites mention it&amp;quot;. I won't get into the technical details of webmentions in this post, but the simplest way to implement webmentions is to have a text input box on your site for each of your articles, kind of like a comment box. When someone wants to mention your post on their site, they can use the text input box on your site to add the URL from their site where they mention your post. When the user submits the webmention, there's some endpoint that listens for webmention submissions and processes them accordingly. If you're interested in learning more about webmentions, you can check out the following resources:</p>\n<ul>\n<li><a href=\"https://www.w3.org/TR/webmention/\">Webmentions spec</a></li>\n<li><a href=\"https://webmention.rocks/\">Webmention Rocks</a></li>\n<li><a href=\"https://yewtu.be/watch?v=ZOlkS6xP2Zk\">Sending Your First Webmention - IndieWebCamp Düsseldorf 2019</a></li>\n</ul>\n<p>While it's great that you can mention and comment on other people's content from your site, like comments, writing a script to submit spam is relatively simple. Therefore, I want to create some sort of <a href=\"https://en.wikipedia.org/wiki/CAPTCHA\">CAPTCHA</a> challenge as an initial form of validation to prevent spam. At the same time, I don't want to have any external dependencies. In this post, I'll show how I went about implementing a captcha-like solution with zero dependencies to prevent spam submissions.</p>\n\n<p>The solution I came up with asks the users to add the day of the month (1-31) to a random number between 1-100. While not entirely foolproof, it's &amp;quot;complex&amp;quot; enough that it's not the same thing every time.</p>\n<p>Below is what the implementation looks like.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/138604591-dfe4c301-78fe-4338-a751-799b420a1791.png\" class=\"img-fluid\" alt=\"Captcha implementation asking for date\" /></p>\n<p>The HTML markup for the webpage looks like the following:</p>\n<pre><code class=\"language-html\">&amp;lt;!DOCTYPE html&amp;gt;\n&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;\n&amp;lt;head&amp;gt;\n    &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;\n    &amp;lt;meta http-equiv=&amp;quot;X-UA-Compatible&amp;quot; content=&amp;quot;IE=edge&amp;quot;&amp;gt;\n    &amp;lt;meta name=&amp;quot;viewport&amp;quot; content=&amp;quot;width=device-width, initial-scale=1.0&amp;quot;&amp;gt;\n    &amp;lt;title&amp;gt;Super Simple Captcha&amp;lt;/title&amp;gt;\n    &amp;lt;script src=&amp;quot;main.js&amp;quot; type=&amp;quot;text/javascript&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;\n&amp;lt;/head&amp;gt;\n&amp;lt;body onload=&amp;quot;setQuery()&amp;quot;&amp;gt;\n    &amp;lt;div&amp;gt;\n        &amp;lt;span id=&amp;quot;query&amp;quot;&amp;gt;&amp;lt;/span&amp;gt;\n        &amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;answerInput&amp;quot;&amp;gt;\n        &amp;lt;button onclick=&amp;quot;displayResult()&amp;quot;&amp;gt;Submit&amp;lt;/button&amp;gt;\n    &amp;lt;/div&amp;gt;\n    &amp;lt;h3 id=&amp;quot;result&amp;quot;&amp;gt;&amp;lt;/h3&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n</code></pre>\n<p>When the <code>body</code> element of the page loads, it invokes the <code>setQuery</code> function, which displays the challenge the user is supposed to solve in the <code>query</code> span element.</p>\n<p>The user then submits their answer via the <code>answerInput</code> text input box.</p>\n<p>The answer is then checked by invoking the <code>displayResult</code> function. The <code>displayResult</code> function checks the user's answer against the expected answer. If the answer is correct, the text &amp;quot;OK&amp;quot; is displayed on the page's <code>result</code> H3 element. Otherwise, the text &amp;quot;Try again&amp;quot; displays on the webpage.</p>\n<p>All of the code that handles this logic is in the <code>main.js</code> file.</p>\n<pre><code class=\"language-javascript\">let date = new Date()\nlet day = date.getDate() // Day of the month\nlet randomNumber = Math.floor(Math.random() * 100) + 1\nlet answer = day + randomNumber // Expected answer\n\nlet setQuery = () =&amp;gt; {\n    let element = document.getElementById('query');\n    element.innerText = `Enter the sum of ${day} + ${randomNumber}`;    \n}\n\nlet checkAnswer = () =&amp;gt; {\n    let userAnswer = parseInt(document.getElementById(&amp;quot;answerInput&amp;quot;).value);\n    return answer === userAnswer\n}\n\nlet displayResult = () =&amp;gt; {\n    let result = checkAnswer() ? &amp;quot;OK&amp;quot; : &amp;quot;Try again&amp;quot; // Ternary function to check if answer is correct\n    document.getElementById(&amp;quot;result&amp;quot;).innerText = result;\n}\n</code></pre>\n<p>That's all there is to it!</p>\n\n<p>In this post, I showed how to implement a dependency-free solution to present a challenge to users submitting comments / webmentions to your site. Although the solution isn't foolproof, it's just complex enough any spammers would have to work a little harder. Happy coding!</p>\n","tags":["web development","html","javascript","webmentions"]},{"title":"Automate YAML front-matter generation with custom Visual Studio Code snippets","contentType":"posts","date":"10/04/2021 19:58","url":"/posts/automate-yaml-front-matter-vs-code-snippets/","content":"\n<p>When creating content with static site generators such as Jekyll, Hexo, and many others, page metadata such as publish date, title, tags, and other details is created using YAML front-matter. Typically these pages are authored in markdown. At the top of each page, there's a YAML formatted section containing all of these details. Sometimes these static site generators include tooling to make it easier to scaffold new pages. In the case of my website, I created my own static site generator. Therefore, scaffolding a new page, especially writing the boilerplate YAML front-matter is cumbersome and time consuming. Since I use Visual Studio Code for authoring content and developing the static site generator, I've decided to use <a href=\"https://code.visualstudio.com/docs/editor/userdefinedsnippets\">snippets</a> to help me automate YAML front-matter generation. In this post, I'll show the general process.</p>\n\n<p>The first thing you'll want to do is create a snippets file. You can have one or as many as you want. For this website, I have a single snippets file called <em>metadata.code-snippets</em> for all my YAML front-matter. To create a snippets file:</p>\n<ol>\n<li><p>Open the Visual Studio Code command palette.</p>\n</li>\n<li><p>Enter the command <code>&amp;gt;Preferences: Configure User Snippets</code> into the text box.</p>\n</li>\n<li><p>Select the <strong>New Snippets file for &amp;lt;REPO-OR-DIRECTORY-NAME&amp;gt;</strong> from the list of options.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/135934846-a76dfa16-caed-4489-9e50-95183add673d.png\" class=\"img-fluid\" alt=\"Create snippet file from Visual Studio Code command palette\" /></p>\n</li>\n<li><p>Provide a name for your snippets file and press <strong>Enter</strong>.</p>\n</li>\n<li><p>A file with the <em>.code-snippets</em> extension is created in the <em>.vscode</em> directory inside your project.</p>\n</li>\n</ol>\n<p>One benefit of creating a snippets file and saving it in the <em>.vscode</em> directory is you can check it into source control and use it anywhere, even github.dev.</p>\n\n<p>Now that you've created the file, it's time to define your snippets.</p>\n<ol>\n<li><p>Open your snippets file.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/135935306-1cbd48e1-6e48-423c-b87f-cb6f7a8eb085.png\" class=\"img-fluid\" alt=\"Snippet file for luisquintanilla.me\" /></p>\n</li>\n<li><p>Create a new JSON object and define your snippet.</p>\n<p>For example, the snippet for blog posts on this site looks like the following.</p>\n<pre><code class=\"language-json\">&amp;quot;Article Post metadata&amp;quot;: {\n    &amp;quot;scope&amp;quot;: &amp;quot;markdown&amp;quot;,\n    &amp;quot;prefix&amp;quot;: &amp;quot;article&amp;quot;,\n    &amp;quot;body&amp;quot;: [\n        &amp;quot;---&amp;quot;,\n        &amp;quot;post_type: \\&amp;quot;article\\&amp;quot; &amp;quot;,\n        &amp;quot;title: \\&amp;quot;\\&amp;quot;&amp;quot;,\n        &amp;quot;published_date: \\&amp;quot;$CURRENT_MONTH/$CURRENT_DATE/$CURRENT_YEAR $CURRENT_HOUR:$CURRENT_MINUTE\\&amp;quot;&amp;quot;,\n        &amp;quot;tags: &amp;quot;,\n        &amp;quot;---&amp;quot;\n    ],\n    &amp;quot;description&amp;quot;: &amp;quot;Blog post article metadata&amp;quot;\n}\n</code></pre>\n<p>Let's break down each of the properties:</p>\n<ul>\n<li><p><code>scope</code>: By default, snippets apply to all languages and projects. In my case, since I intend only to use these snippets in markdown files, I set the scope to <code>markdown</code>.</p>\n</li>\n<li><p><code>prefix</code>: The prefix is the word in the page that is associated with. When I type <code>article</code> in a markdown page, a recommendation to use the snippet appears.</p>\n</li>\n<li><p><code>body</code>: The code or content of my snippet. In this case, the expected output is similar to the following.</p>\n<pre><code class=\"language-yaml\">---\npost_type: &amp;quot;article&amp;quot; \ntitle: &amp;quot;&amp;quot;\npublished_date: &amp;quot;10/04/2021 18:58&amp;quot;\ntags: \n---\n</code></pre>\n<p>VS Code also has built-in variables that you can use to automatically set date, time, and other values.</p>\n</li>\n<li><p><code>description</code>: A text description of your snippet.</p>\n</li>\n</ul>\n</li>\n</ol>\n<p>For more information on creating snippets, see <a href=\"https://code.visualstudio.com/docs/editor/userdefinedsnippets#_create-your-own-snippets\">Create your own snippets</a>.</p>\n\n<ol>\n<li><p>Create a new markdown file.</p>\n</li>\n<li><p>Type your prefix into the file. In my case, for blog posts my prefix is <code>article</code>.</p>\n</li>\n<li><p>Press <strong>Ctrl + Space</strong>. For Macs you might have to use Cmd instead of Ctrl.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/135935808-46ca8314-c2be-47f1-9c9b-9b722e37d908.png\" class=\"img-fluid\" alt=\"Populate article front-matter using snippet\" /></p>\n</li>\n<li><p>A tooltip with your snippet's description appears. Press <strong>Enter</strong>.</p>\n</li>\n</ol>\n<p>At this point, your snippet's prefix is replaced with the content defined in the snippet's body.</p>\n\n<p>In this post I've shown how you can use Visual Studio Code snippets to automate YAML front-matter generation when working with common static site generators that don't provide tooling to scaffold new pages. Snippets can be used for a variety of scenarios and if there's any boilerplate code or content you constantly have to write, they can save you a lot of time.</p>\n<p>Happy writing!</p>\n","tags":["blogging","tooling","visual-studio","visual-studio-code","markdown","yaml"]},{"title":"Proofread articles using Microsoft Edge's Immersive Reader and read aloud","contentType":"posts","date":"2021-09-22 18:00:00","url":"/posts/proofread-read-aloud/","content":"<p>This will be a relatively short post but it's something I just discovered and thought I'd share. Before publishing a post, I try to proofread and review the article to make sure spelling, grammar, and flow all make sense. Part of that process involves reading the article aloud. I'll miss things from time to time but generally it works well.</p>\n<p>I've known about read aloud in general, but have often found voices to be robotic and sound unnatural when they encounter words outside their vocabulary. However, I recently tried out the read aloud feature in Microsoft Edge and was blown away at how natural the voice sounds and even how well it enunciates technical terms like product and programming API names. Here's what it looks like when reading one of the posts on my site.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/134442715-e0af0c4a-a522-49a9-a0b1-ac658b284c68.png\" class=\"img-fluid\" alt=\"Microsoft Edge Immersive Reader read aloud reading blog post\" /></p>\n<p>If you do any form of writing that can be displayed in Microsoft Edge, I highly encourage you to give read aloud a try. For more information, see <a href=\"https://support.microsoft.com/topic/use-immersive-reader-in-microsoft-edge-78a7a17d-52e1-47ee-b0ac-eff8539015e1\">Use Immersive Reader in Microsoft Edge</a>.</p>\n<p>Happy writing!</p>\n","tags":["writing","tools"]},{"title":"Make predictions with ML.NET models without defining schema classes","contentType":"posts","date":"2021-09-16 18:00:00","url":"/posts/mlnet-predictions-no-schema/","content":"\n<p>To make predictions with ML.NET models you often have to define schema classes for your model inputs and outputs. In previous posts I wrote how you can <a href=\"/posts/inspect-mlnet-models-netron/\">use Netron to inspect an ML.NET model</a> to determine the name and types of your model inputs and outputs. If you have data samples of what your data input and output look like in JSON format, you can <a href=\"/posts/vs-automate-mlnet-schema-generation/\">automate the generation of model input and output classes by using Visual Studio's &amp;quot;Paste JSON as Classes&amp;quot; feature</a>. However, what if you want to make predictions without defining these classes? In this post I'll show how you can use the .NET DataFrame API to make predictions with ML.NET models without having to create model input and output classes. Code snippets are in F# but notebooks with complete <a href=\"https://github.com/lqdev/mlnet-noschema-predictions/blob/main/CSharp-NB.ipynb\">C#</a> &amp;amp; <a href=\"https://github.com/lqdev/mlnet-noschema-predictions/blob/main/FSharp-NB.ipynb\">F#</a> code can be found in the <a href=\"https://github.com/lqdev/mlnet-noschema-predictions\">mlnet-noschema-predictions repo</a> on GitHub.</p>\n\n<p>In addition to the <a href=\"https://www.nuget.org/packages/Microsoft.ML/\">Microsoft.ML</a> ML.NET NuGet package, you'll also need the <a href=\"https://www.nuget.org/packages/Microsoft.Data.Analysis/\">Microsoft.Data.Analysis</a> NuGet package to use the .NET DataFrame API. For more information on the .NET DataFrame API, see <a href=\"https://devblogs.microsoft.com/dotnet/an-introduction-to-dataframe/\">an introduction to DataFrame</a>.</p>\n<p>Once your packages are installed, reference them in your application.</p>\n<pre><code class=\"language-fsharp\">open Microsoft.ML\nopen Microsoft.Data.Analysis\n</code></pre>\n\n<p>The <code>MLContext</code> is the entrypoint of ML.NET applications. Use it to load your model. The model used in this case categorizes sentiment as positive or negative. See the <a href=\"/posts/inspect-mlnet-models-netron/\">use Netron to inspect an ML.NET model</a> blog post to learn more about the model.</p>\n<pre><code class=\"language-fsharp\">let ctx = MLContext()\nlet model,schema = ctx.Model.Load(&amp;quot;sentiment_model.zip&amp;quot;)\n</code></pre>\n<p>Both the model and the input schema are returned when you load the model. The input schema is a <a href=\"https://docs.microsoft.com/dotnet/api/microsoft.ml.dataviewschema?view=ml-dotnet\">DataViewSchema</a> object containing a collection of <a href=\"https://docs.microsoft.com/dotnet/api/microsoft.ml.dataviewschema.column?view=ml-dotnet\">Columns</a>.</p>\n\n<p>The input and output column names are for the DataFrames containing your input data and predictions. They help the model map the input and output values.</p>\n<p>Use the <code>schema</code> which was loaded with the model to get the name of your input columns.</p>\n<pre><code class=\"language-fsharp\">let inputColumnNames = \n    schema \n    |&amp;gt; Seq.map(fun column -&amp;gt; column.Name) \n    |&amp;gt; Array.ofSeq\n</code></pre>\n<p>Since this is a binary classification model by default only two columns are returned as part of the prediction:</p>\n<ul>\n<li>Score</li>\n<li>PredictedLabel</li>\n</ul>\n<p>You can create an array containing the names of these columns. For more information on default output columns, see the <a href=\"https://docs.microsoft.com/dotnet/machine-learning/resources/tasks#binary-classification-inputs-and-outputs\">ML.NET Tasks documentation</a>.</p>\n<pre><code class=\"language-fsharp\">let outputColumnNames = [| &amp;quot;PredictedLabel&amp;quot; ; &amp;quot;Score&amp;quot; |]\n</code></pre>\n\n<p>Use the <a href=\"https://docs.microsoft.com/dotnet/api/microsoft.data.analysis.dataframe.loadcsvfromstring?view=ml-dotnet-preview#Microsoft_Data_Analysis_DataFrame_LoadCsvFromString_System_String_System_Char_System_Boolean_System_String___System_Type___System_Int64_System_Int32_System_Boolean_\"><code>LoadCsvFromString</code></a> method to load your input data into a DataFrame. In this case, there's only one column and data instance so I represent it as a string literal. Additionally, I provide the name of the input columns.</p>\n<pre><code class=\"language-fsharp\">let sampleInput = &amp;quot;This was a very bad steak&amp;quot;\n\nlet inputDataFrame = \n    DataFrame.LoadCsvFromString(\n        sampleInput, \n        header=false, \n        columnNames=inputColumnNames)\n</code></pre>\n\n<p>Now that you've loaded your input data, it's time to use the model to make predictions.</p>\n<pre><code class=\"language-fsharp\">let predictionDV = \n    inputDataFrame \n    |&amp;gt; model.Transform \n</code></pre>\n<p>Calling the <a href=\"https://docs.microsoft.com/dotnet/api/microsoft.ml.itransformer.transform?view=ml-dotnet#Microsoft_ML_ITransformer_Transform_Microsoft_ML_IDataView_\"><code>Transform</code></a> method  returns an <a href=\"https://docs.microsoft.com/dotnet/api/microsoft.ml.idataview?view=ml-dotnet\"><code>IDataView</code></a> with your predictions. You can then convert the <code>IDataView</code> into a DataFrame for further processing with the <a href=\"https://docs.microsoft.com/dotnet/api/microsoft.ml.idataviewextensions.todataframe?view=ml-dotnet-preview\"><code>ToDataFrame</code></a> method.</p>\n<pre><code class=\"language-fsharp\">let prediction = predictionDV.ToDataFrame(1L, outputColumnNames)\n</code></pre>\n<p>The resulting DataFrame should look something like the following:</p>\n<table class=\"table\">\n<thead>\n<tr>\n<th>index</th>\n<th>PredictedLabel</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>False</td>\n<td>-2.1337974</td>\n</tr>\n</tbody>\n</table>\n\n<p>If you want to load a model and make predictions without defining  classes for your input and output schema's you can load your data into a DataFrame using the .NET DataFrame API. While this solution works, because DataFrames and IDataViews process data differently, I haven't tested whether this solution would scale for larger data sets.</p>\n","tags":["dotnet","machine learning","mlnet","artificial intelligence"]},{"title":"Automate ML.NET model schema generation from sample JSON data with Visual Studio","contentType":"posts","date":"2021-08-26 18:00:00","url":"/posts/vs-automate-mlnet-schema-generation/","content":"\n<p>When using ML.NET models to make predictions, you often have to define classes for the model's input and output schema. In a previous post, I showed how you can <a href=\"/posts/inspect-mlnet-models-netron/\">use Netron to inspect ML.NET models</a> and manually define classes to represent the input and output schema for your model. That works for models that don't have a lot of features / columns. However, as the number of columns grows, it can become cumbersome to define these classes. Visual Studio has a feature that can help automate that process. Assuming you have a sample of your input and output data in JSON format, you can leverage a built-in feature &amp;quot;Paste JSON As Classes&amp;quot; to take the sample and convert it to a class. In this post, I'll show how to do that.</p>\n\n<ul>\n<li>Visual Studio 2019. Though I haven't tested with VS2022, I assume &amp;quot;Paste JSON As Classes&amp;quot; is also available there.</li>\n</ul>\n\n<p>In this post, I'll work with the <em>sentiment_model.zip</em> model to classify sentiment, which you can find in the <a href=\"https://github.com/dotnet/samples/blob/main/machine-learning/models/sentimentanalysis/sentiment_model.zip\">dotnet/samples</a> repo.</p>\n<p>The model input has 3 columns:</p>\n<ul>\n<li>SentimentText (string)</li>\n<li>Label (boolean)</li>\n<li>SamplingKeyColumn (float32)</li>\n</ul>\n<p>The model output has at least 2 columns since it uses a binary classification algorithm. For information on expected output columns based on the machine learning algorithm/task, see the <a href=\"https://docs.microsoft.com/dotnet/machine-learning/resources/tasks#binary-classification-inputs-and-outputs\">ML.NET tasks article</a>.</p>\n<ul>\n<li>Score (Single). Single is a single-precision floating-point number.</li>\n<li>PredictedLabel (boolean)</li>\n</ul>\n<p>With that in mind, let's then assume that we have sample input data in JSON format that looks as follows.</p>\n<pre><code class=\"language-json\">{\n    &amp;quot;SentimentText&amp;quot;: &amp;quot;This was a very bad steak&amp;quot;,\n    &amp;quot;Label&amp;quot;: false,\n    &amp;quot;SamplingKeyColumn&amp;quot;: 1.0\n}\n</code></pre>\n<p>Inside a C# project in Visual Studio:</p>\n<ol>\n<li><p>Create a C# class</p>\n<ol>\n<li>Right-click your project.</li>\n<li>Select <strong>Add &amp;gt; Class</strong>.</li>\n<li>Provide a name for your class in the New Item dialog.</li>\n</ol>\n</li>\n<li><p>Copy the input JSON data sample to your clipboard.</p>\n</li>\n<li><p>Place your cursor inside the namespace block of your newly created class.</p>\n</li>\n<li><p>In the Visual Studio toolbar, select <strong>Edit &amp;gt; Paste Special &amp;gt; Paste JSON as Classes</strong>.</p>\n</li>\n<li><p>The result should look similar to the following:</p>\n<pre><code class=\"language-csharp\">public class Rootobject\n{\n    public string SentimentText { get; set; }\n    public bool Label { get; set; }\n    public float SamplingKeyColumn { get; set; }\n}\n</code></pre>\n</li>\n<li><p>Rename the class to something more descriptive like <code>ModelInput</code>. The class should look similar to the following:</p>\n<pre><code class=\"language-csharp\">public class ModelInput\n{\n    public string SentimentText { get; set; }\n    public bool Label { get; set; }\n    public float SamplingKeyColumn { get; set; }\n}\n</code></pre>\n</li>\n<li><p>Create a JSON data sample for your output. In this case it'd look something like:</p>\n<pre><code class=\"language-json\">{\n    &amp;quot;Score&amp;quot;: 1.0,\n    &amp;quot;PredictedLabel&amp;quot;: false\n}\n</code></pre>\n</li>\n<li><p>Repeat steps 1-6 for your output JSON data sample. The resulting class should look similar to the following:</p>\n<pre><code class=\"language-csharp\">public class ModelOutput\n{\n    public float Score { get; set; }\n    public bool PredictedLabel { get; set; }\n}\n</code></pre>\n</li>\n</ol>\n<p>It's important to note that the name of the class does not matter as long as the column names and types are the same the ones the model expects.</p>\n<p>At this point, you can go through the process of <a href=\"https://docs.microsoft.com/dotnet/machine-learning/how-to-guides/machine-learning-model-predictions-ml-net\">using the model to make predictions on new data</a>.</p>\n\n<p>Although the model used in this post does not have many columns, when you have many columns &amp;quot;Paste JSON As Classes&amp;quot; can significantly simplify the process of creating your input and output schema classes for ML.NET models. Happy coding!</p>\n","tags":["dotnet","machine learning","mlnet","artificial intelligence","tooling","visual studio"]},{"title":"Inspect ML.NET models with Netron","contentType":"posts","date":"2021-08-25 18:00:00","url":"/posts/inspect-mlnet-models-netron/","content":"\n<p>Once you've trained a machine learning model, you typically serialize it and save it to a file. This serialized file contains information such as the model inputs and output schema (names, data types), the transformations and algorithms used by the model,  weights / coefficients, hyperparameters, and all other sorts of information about the model. The model file is then embedded in an end-user application such as a web API which deserializes the contents of the file and uses the model to make predictions on new data.</p>\n<p>How the model is serialized depends on the framework you use to train your model. To standardize model serialization and interoperability, you can use something like Open Neural Network Exchange (ONNX) to represent your models if supported by the framework you're using. That's beyond the scope of this post though.</p>\n<p>Regardless of which framework or serialization format is used, if you were to open up the model file, the contents of the model file are often not human readable or difficult to interpret. When the person who trains the model is also putting it into production, they know the format input data needs to be in to make predictions. That's not often the case though. As a result, the people putting models into production need the ability to inspect a model to get a sense of how they need to collect and preprocess input data before making predictions. That's where Netron comes in. <a href=\"https://github.com/lutzroeder/Netron\">Netron</a> is a &amp;quot;visualizer for neural network, deep learning, and machine learning models&amp;quot;. In this post, I'll show how you can use Netron to inspect ML.NET models and use that information to define the model input and output schemas.</p>\n\n<p>ML.NET models are typically serialized and saved to files with the <em>.zip</em> file extension. Using the <em>.zip</em> file extension is standard convention. However, the extension can be whatever makes the most sense to you.</p>\n<p>A common question is, what's in the <em>zip</em> file? The easy but vague answer to that question is, a serialized version of the model. Using Netron, you can go deeper and see exactly what is inside the <em>zip</em> file.</p>\n<p>In this post, I'm using a pretrained ML.NET model that classifies sentiment. The model can be found in the <a href=\"https://github.com/dotnet/samples/blob/main/machine-learning/models/sentimentanalysis/sentiment_model.zip\">dotnet/samples</a> repo. The same concept applies to any other ML.NET model.</p>\n<p>To inspect the ML.NET model using Netron:</p>\n<ol>\n<li><p><a href=\"https://github.com/dotnet/samples/raw/main/machine-learning/models/sentimentanalysis/sentiment_model.zip\">Download the model</a></p>\n</li>\n<li><p>Navigate to <a href=\"https://netron.app/\">https://netron.app</a>. Alternatively, if you'd prefer to use Netron offline, you can also <a href=\"https://github.com/lutzroeder/netron/releases\">download the latest version of Netron</a> for your operating system (Windows, Mac, or Linux). In this post, I use the web app.</p>\n</li>\n<li><p>Select <strong>Open Model...</strong> and use the file browser to select your ML.NET model. In this case, our model is <em>sentiment_model.zip</em>.</p>\n</li>\n<li><p>After a few seconds, a graph describing you model appears. How long it takes for your model depends on its size. The larger your model, the longer it takes to load. The nodes in the graph represent the model inputs, transformations, algorithm, and outputs.</p>\n</li>\n<li><p>Usually the top nodes represent the model inputs and the last node represents the algorithm or trainer. Click on any of the top nodes to display more information about the inputs.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/130704589-61ebb612-d65f-4364-b275-bd0d4991d3cf.png\" class=\"img-fluid\" alt=\"Sentiment Classification ML.NET model in netron\" /></p>\n<p>For this model, we see that there are 3 input properties or columns:</p>\n<ul>\n<li>SentimentText (string)</li>\n<li>Label (boolean)</li>\n<li>SamplingKeyColumn (float32)</li>\n</ul>\n<p>Using this information, we can represent the model inputs as a Plain-Old-CLR-Object (POCO) in our end-user application.</p>\n<pre><code class=\"language-csharp\">public class ModelInput\n{\n    public string SentimentText {get;set;}\n    public bool Label {get;set;}\n    public float SamplingKeyColumn {get;set;} \n}\n</code></pre>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/130705880-0baea2f7-7b45-408a-b60c-16acceb54079.png\" class=\"img-fluid\" alt=\"ML.NET Netron Binary Predictor\" /></p>\n<p>Looking at the last node <code>BinaryPredXfer</code>, we see that the algorithm used is for binary classification or predictions. Looking at the <a href=\"https://docs.microsoft.com/dotnet/machine-learning/resources/tasks#binary-classification-inputs-and-outputs\">ML.NET tasks documentation</a>, we expect to get at least two columns in the prediction output:</p>\n<ul>\n<li>Score (Single)</li>\n<li>PredictedLabel (boolean)</li>\n</ul>\n<p>Like the input, we can also represent model outputs or predictions as follows:</p>\n<pre><code class=\"language-csharp\">public class ModelOutput\n{\n    public float Score {get;set;}\n    public bool PredictedLabel {get;set;}\n}\n</code></pre>\n<p>Keep in mind that the name of the class can be anything so long as the properties or column names and types match with those expected by the model.</p>\n</li>\n</ol>\n<p>Once you have your model inputs and outputs defined in your end-user application, you can follow the standard process of <a href=\"https://docs.microsoft.com/dotnet/machine-learning/how-to-guides/machine-learning-model-predictions-ml-net\">loading your model and using it to make predictions</a>.</p>\n\n<p>Inspecting ML.NET models can be difficult since their serialized version is not human readable. When making predictions with ML.NET models but you're not familiar with what the input and output data should look like, use Netron to inspect the model. Then, use the information about the input data names and types, machine learning task, and algorithm to define model input and output schema classes in your end-user application. Once you've defined your model input and output, you can use the model to make predictions on new data.</p>\n","tags":["dotnet","machine learning","mlnet","netron","artificial intelligence","tooling"]},{"title":"Self-hosting made easy with YunoHost","contentType":"posts","date":"2021-08-10 18:00:00","url":"/posts/yunohost-self-hosting-simplified/","content":"\n<p>Over the past few months I've started self-hosting a few of the services I use. Among those are a Matrix chat server and Mastodon instance. It may be of special interest to call out, many of these services are hosted on a Raspberry Pi 4, which not only speaks to the capabilities of the device but also shows opportunities for individuals to take control over the services they use and their data. Self-hosting however isn't the easiest thing to get started with. Mainly because each service has their own setup requirements. Technically you could use something like Docker to standardize the process, but that has its own complexities. In particular, there was a recent experience where I had trouble setting up a service which I got up and running in no time using YunoHost. YunoHost describes itself as &amp;quot;...a libre operating system aiming to simplify server administration and democratize self-hosting.&amp;quot;. In this post, I'll describe my experience with it and why you might consider it for your self-hosting project.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/128647305-c30ef44e-901d-4bf1-8e9d-af2449d5fd1d.png\" class=\"img-fluid\" alt=\"Image of server racks by Taylor Vick (Unsplash @tvick)\" /></p>\n\n<p>First off, why even go through the trouble of self-hosting? Service providers nowadays have made it so easy to use their services, self-hosting seems like a lot of work. However, there are several reasons why you might consider self-hosting the applications and services you use. A few that I find interesting include:</p>\n<ol>\n<li>Explore &amp;quot;new&amp;quot; technologies</li>\n<li>Own your data</li>\n<li>Learn something new</li>\n</ol>\n<h3>Explore &amp;quot;new&amp;quot; technologies</h3>\n<p>The saying goes &amp;quot;there's nothing new under the sun&amp;quot;. However as with all things, people have found alternative uses and solutions to existing technologies. For example, in simple terms, the <a href=\"https://en.wikipedia.org/wiki/Fediverse\">Fediverse</a> can be summarized as E-mail + RSS. It consists of a common decentralized protocol to handle communication between individuals who may not use the same server while providing &amp;quot;follow&amp;quot; capabilities for services like micro-blogging, photos, and video sharing. Though the Fediverse has been around for some time, it's still relatively new. Whether it's a new technology or new to you, self-hosting can help you learn more about technologies like the Fediverse.</p>\n<h3>Own your data</h3>\n<p>Part of accepting terms of service agreements is sometimes giving up some ownership of your personal data. Although you hope the company and service provider is responsible with your data, you can't always be 100% sure. With self-hosting, you're fully in control of your data and in most cases don't rely on third parties. As a result, your data is as private and secure as you want it to. From a security standpoint though, I'll note that in many cases, service providers have dedicated security teams whose main job is to keep your data secure. Therefore chances are that your data is usually secure when hosted by service providers. In any case, if you want to have complete control and ownership of your data, self-hosting might be for you.</p>\n<h3>Learn something new</h3>\n<p>Over the past few months, I've learned a few things that I don't usually deal with on my day-to-day like DNS records, Nginx, Systemd, certificates, and a few others. Self-hosting these services usually involves setting up several components some of which you may already be familiar with while others may be new to you. Self-hosting gives you the opportunity to learn new skills and as mentioned previously work with new technologies.</p>\n\n<p>Recently I was trying to set up an instance of <a href=\"https://pixelfed.org/\">Pixelfed</a>, which describes itself as &amp;quot;A free and ethical photo sharing platform&amp;quot;. You can think of it as a self-hosted Instagram. The application is built with PHP and a few other components I'm not very familiar with. I spent about a week trying to configure my Raspberry Pi to successfully run these services. Unfortunately, I had no luck. I had heard about YunoHost previously but had never tried it. Essentially they offer a way to simplify self-hosting. I noticed YunoHost provided a way to self-host Pixelfed. All I had to do was:</p>\n<ol>\n<li><a href=\"https://yunohost.org/en/install\">Install YunoHost</a>. In my case, I installed it on a Linode VM (the smallest one) running Debian 10, but you have options including a Raspberry Pi.</li>\n<li><a href=\"https://github.com/YunoHost-Apps/pixelfed_ynh\">Install Pixelfed</a>. YunoHost has a variety of <a href=\"https://yunohost.org/en/apps\">applications</a> with pre-configured scripts that make it easy to install applications like Pixelfed.</li>\n</ol>\n<p>The entire process took about one hour. Some parts I skipped mentioning were getting a domain name and configuring DNS records, but aside from that, getting started with YunoHost was incredibly simple. If you're looking to self-host but don't want to spend too much time configuring your server and each of the services you host on it, give YunoHost a try.</p>\n","tags":["fediverse","open-source","self-hosting","raspberry-pi","matrix","pixelfed","mastodon"]},{"title":"Surface Duo offline navigation hiking setup","contentType":"posts","date":"2021-07-27 19:00:00","url":"/posts/surface-duo-offline-navigation-hiking-setup/","content":"\n<p>Over the past few months, I've been going on a hike almost every weekend. Not only does hiking provide an opportunity to discover the natural wonders all around you, it's also good exercise and free (if you don't count entry fees for some state and national parks). Throughout these trips, the <a href=\"https://www.microsoft.com/surface/devices/surface-duo\">Surface Duo</a>, Microsoft's dual-screen Android phone has been a reliable companion. Although it's not the smallest or most rugged device, its large screen(s) paired with OSMAnd~ and AllTrails make it easy to navigate while on the trail. In this post, I'll detail my navigation setup.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/127246913-58472929-f067-41f0-9011-17193f863f71.JPG\" class=\"img-fluid\" alt=\"Picture of half-dome taken from Four Mile Trail\" /></p>\n\n<p>Although trails are often well marked, it's always a good idea to have a map of the trail and surrounding area. This is especially true if it's your first time there. I almost exclusively get my trail maps from <a href=\"https://www.alltrails.com/\">AllTrails</a>. If you're not familiar with AllTrails, it's a platform and community where members share maps, reviews, and information about trails for all kinds of outdoor activities. The site is completely free and while you can pay for premium features, the free offering provides you with all the essentials you'd need to plan your trip.</p>\n<p>Trail information, maps, and reviews are entirely user supported. One of the (free) features is the ability to download trails to a variety of formats.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/127243740-5ef2e2e3-4c9c-4dbc-a8c5-be608fafeea5.png\" class=\"img-fluid\" alt=\"AllTrails Download Route\" /></p>\n<p>Typically, the format I'll download routes to is GPX Route and GPX Track. <a href=\"https://en.wikipedia.org/wiki/GPS_Exchange_Format\">GPX</a> is an XML schema for representing GPS data in software applications. Because GPX is a standard format, you can open and view these files in any app that supports them.</p>\n\n<p>AllTrails has a mobile app. However, for offline navigation, I use the <a href=\"https://f-droid.org/en/packages/net.osmand.plus/\">OSMAnd~</a> Android app which I downloaded from <a href=\"https://f-droid.org/\">F-Droid</a>. It's also available in the Google Play Store and Apple App Store.</p>\n<p>OSMAnd~ is a free map and navigation app supported by <a href=\"https://www.openstreetmap.org/\">OpenStreetMaps</a>. OpenStreetMaps, like AllTrails is free and entirely community driven. OSMAnd~ lets you download <strong>detailed</strong> maps for offline use. Detailed is the key here. Although you can save maps for offline use with applications like Here Maps and Google Maps, they often limit the size of the area you can save for offline use and don't provide much detail. With OSMAnd~ you can:</p>\n<ul>\n<li>Download entire state and country maps for offline use.</li>\n<li>View details like contour-lines, hill shading, and other trail details.</li>\n<li>Turn-by-turn navigation.</li>\n<li>View and edit GPX files</li>\n</ul>\n<p>Using the GPX files I get from AllTrails, I load them onto my Surface Duo and view them using OSMAnd~. Using offline maps and the Duo's GPS I'm able to see my position on the trail and quickly find my way when I've gone off it even if I have no reception. This combination makes for a navigation device and experience like no other because I have the full screen real estate that the Duo provides to view areas beyond the trail.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/127246186-6b5f02ec-7fef-4f2e-909b-8a54615dc790.png\" class=\"img-fluid\" alt=\"OSMAnd Android App Displaying Track\" /></p>\n\n<p>By leveraging the power of crowd-sourcing and open-source, access to quality information has never been easier. When this information is displayed on a device like the Surface Duo it makes it easy to navigate when you're out on the trail especially in situations when you have no reception and you're not familiar with your surroundings.</p>\n<p>If you've been looking for an offline navigation solutions, hopefully this post helps provide some guidance. If you do any outdoor activities especially in remote areas, even if you don't have a Duo, I'd suggest giving AllTrails and OSMAnd~ a try. See you on the trail!</p>\n<p>P.S. Over the past month, I've transitioned to the Garmin Fenix 6X Pro for navigation. It's a smaller device that supports loading GPX files and provides offline maps with turn-by-turn navigation. I'm also not worried about dropping or scratching it because it's designed for the outdoors. This in no way takes away with how amazing the OSMAnd~ and Surface Duo pairing are for offline navigation. In fact, I still use the Duo for backup navigation when it's difficult to see the map on the Fenix.</p>\n","tags":["hiking","openstreetmaps","open-source","surface","lifestyle","apps","fdroid"]},{"title":"Never change targeted ads","contentType":"posts","date":"2021-06-22 17:40:00","url":"/posts/fenix-social-media-targeted-ads/","content":"<p>The origin of this post comes from a random observation I made earlier today on how fascinating and disturbing targeted ads can be. But first, some context.</p>\n<p>Over the past year, I've been doing a lot more hiking. Although most trails are well marked, it's always good to have some way to navigate in case you miss a turn. For remote locations with limited connectivity, one of the best ways to navigate, other than paper maps, is to download the trail GPX files and view them using a mapping application offline. This is just what I've been doing on my Surface Duo with the help of <a href=\"https://www.alltrails.com/\">AllTrails</a> and <a href=\"http://osmand.net/\">OSMAnd+</a>. That setup and workflow are a post on its own, but I'll leave that for another time. While that's worked extremely well, there are some downsides to it. For starters, the Surface Duo is not exactly the smallest or most rugged device so it can be cumbersome to handle while on the trail. That means I have to be extra careful to keep it from falling on rocks or off a cliff. Having navigation turned on also drains the phone battery, which is less than ideal in the event I were to need the phone for an emergency. Therefore, I started looking for an alternative navigation solution.</p>\n<p>After significant research, I landed on a wearable device with offline mapping capabilities, specifically the <a href=\"https://discover.garmin.com/en-US/fenix-6/\">Garmin Fenix 6</a>. Although I've had my eye on this device for the past few months, I couldn't justify paying full price for it. Fast-forward to this past week. For work, I recently published some articles on Kubernetes. Around the same time, as part of Father's Day &amp;amp; Amazon Prime Day sales, the price of the device dropped significantly enough for me to buy it. This leads us to the main point of this post.</p>\n<p>While looking at my Twitter feed, I ran into this gem.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/123006831-47e1f080-d386-11eb-98ac-af80f6f5f6f0.png\" class=\"img-fluid\" alt=\"Garmin Kubernetes Twitter targeted ad\" /></p>\n<p>Coincidence? I think not! It's fascinating to see how hyper-specific ads are based on things I'm interested in. In this case it's harmless. I'm always up for learning a thing or two about Kubernetes and if I get the chance to win this watch, count me in! The disturbing part is, what else does it know about me and how is that information being used? Depending on where on the spectrum of fascinated / disturbed you land on, one way to control the amount of information shared is through the privacy settings. Unfortunately privacy settings only go so far and visibility into who has what data is limited. Maybe it's wishful thinking but it'd be great to have a tool that provides more insights and control over what data is shared and with whom. Personally, I think whether targeted ads / recommendations are good or bad is not black or white. If it's something I'm already doing or looking into, it'd be nice to be presented with options to include as part of my research. On the other hand, if those recommendations / targeted ads are exploiting information from my interests to manipulate my behavior, that's not right. Where that line is drawn though is not always clear and often personal.</p>\n<p>In the meantime though, all I can say is, stay classy targeted ads!</p>\n","tags":["random","privacy","social media","ads"]},{"title":"Rediscovering the RSS protocol","contentType":"posts","date":"2021-04-05 19:30:00","url":"/posts/rediscovering-rss-user-freedom/","content":"\n<p>RSS provides a standardized way of aggregating content giving content creators the freedom to choose how they publish content while giving consumers the freedom to choose how they consume that content. I've used RSS for a long time. One of my favorite RSS readers was Google Reader while it was still around and I believe <a href=\"https://www.ripgooglereader.com/\">I'm not the only fan</a>. Since then, I've bounced around between different RSS feed readers not settling one until recently when I started using <a href=\"https://newsblur.com\">NewsBlur</a>. As a result of finding an RSS feed reader I liked, I've also started to expand the content I follow via RSS feeds to social media, videos, podcasts, and software projects. In part, this has gotten me thinking about some of the benefits of protocols. The essay <a href=\"https://knightcolumbia.org/content/protocols-not-platforms-a-technological-approach-to-free-speech\">Protocols, Not Platforms: A Technological Approach To Free Speech</a> makes good points on some of these benefits. Though the focus of that article is on free speech, I prefer to think about the benefits in terms of user freedom.</p>\n\n<p>RSS provides content creators with the freedom to choose how and where they publish content. As a content creator, depending on the type of content you produce you use different platforms to publish. For example, when I blog, I publish on this site, when I create videos, I publish to YouTube or livestream on Twitch and to further expand the reach of my content, I publish links on social media like Reddit or Twitter. In each case, I use different platforms to publish content. This might be okay because the audiences interested in the video content may not be interested in blog posts and vice versa. However, now the content is split across all these various platforms with their own accounts, interfaces, policies, and audiences. It also means, when I tell people where they can follow me, it usually involves an entire speech like: &amp;quot;Follow me on GitHub at <a href=\"https://github.com/lqdev/\">lqdev</a>, Twitter at <a href=\"https://twitter.com/ljquintanilla\">ljquintanilla</a>, my blog at <a href=\"/posts/1\">luisquintanilla.me</a>...&amp;quot;. Of course, I don't always list out all the platforms I'm active on and it's highly dependent on the setting, but the more platforms I'm active on the longer that speech gets.</p>\n<p>Fortunately, some of these platforms generate RSS feeds. On GitHub, RSS feeds are generated for releases, issues, and pull requests. For example, the ML.NET releases RSS feed can be accessed by appending <em>.atom</em> to the releases URL <a href=\"https://github.com/dotnet/machinelearning/releases.atom\">https://github.com/dotnet/machinelearning/releases.atom</a>. Note that while Atom and RSS are not the same, they both offer a way to aggregate content. Similarly, YouTube provides the option to generate feeds for channels. For example, you can find the RSS feed to my YouTube channel at the following URL <a href=\"https://www.youtube.com/feeds/videos.xml?channel_id=UCkA5fHdQ4cf3D1J19UNgV7A\">https://www.youtube.com/feeds/videos.xml?channel_id=UCkA5fHdQ4cf3D1J19UNgV7A</a>. On this blog, the RSS feed can be found at <a href=\"/posts/feed.xml\">/posts/feed.xml</a>. For social media, the experience is a bit more limited since platforms like Twitter, Instagram, or Facebook don't generate RSS feeds which is unfortunate since they all have feeds of their own making them the perfect use case for RSS feeds. Platforms like Reddit provide RSS feeds to public subreddits and users by appending <em>.rss</em> at the end of the URL. For example, the RSS feed for the Zune subreddit can be found at <a href=\"https://www.reddit.com/r/Zune.rss\">https://www.reddit.com/r/Zune.rss</a>. In each of these cases, though the content and platforms are different, the underlying protocol and format used is the same, RSS.</p>\n<p>Therefore, a strategy to improve how I publish my content using RSS is, instead of sending users to the various platforms, I can provide links to the different RSS feeds for each of the platforms I'm active on in my personal site. Instead of having a long speech on the different platforms I'm active on, I can just say, &amp;quot;Go to my personal website at ...&amp;quot;, and that's it! That becomes the gateway to the rest of my content. I can continue publishing on the different platforms because of the features they may provide me with for content creation and discovery, but only send users to a single place. For even more control over publishing, I can self-host videos or other content and create RSS feeds for it just like I do for my blog. In either case, users don't need to have an account in order to access and follow my content lowering the barrier to access content. This brings me to my next point of user freedom.</p>\n\n<p>RSS provides users with the freedom to choose how they consume content. One way it does so is by being a standard protocol for aggregating content. Because there's an agreed upon set of standards, clients or RSS feed readers that consume this protocol at minimum only need to be able to read an RSS feed. Once a client can read the RSS feed, it's free to build on top of that leading to diversity and competition among the various clients. Diversity and competition is good for users because it provides choice. If I want a minimal feed reader, I can subscribe to RSS feeds using an e-mail client like <a href=\"https://support.microsoft.com/en-us/office/subscribe-to-an-rss-feed-73c6e717-7815-4594-98e5-81fa369e951c\">Outlook</a> or <a href=\"https://support.mozilla.org/en-US/kb/how-subscribe-news-feeds-and-blogs\">Thunderbird</a>. If I want more features, I can use something like NewsBlur. As an aside, one of the other things I like about NewsBlur is being provided with an e-mail address that I can use to subscribe to newsletters helping me both clear up my inbox and letting me access information in a single place. This shows how RSS readers don't have to stop at only reading RSS feeds and can build on top of the protocol while still performing their core functionality.</p>\n<p>Another reason to use RSS is having a single place to consume almost all your content. As mentioned before, an RSS feed can aggregate content from a YouTube channel, subreddit, blog, newspaper, podcast, etc. RSS is agnostic in the sense it doesn't care what's in the feed so long as you're able to link to the content. Now, because RSS readers can read any RSS feed, it means you can follow almost all of your content from a single place so long as you're able to subscribe to the RSS feed. In fact, aside from some social media, the only other content I don't consume via RSS is podcasts. Not because I'm not able to since podcasts are usually aggregated using RSS feeds, but because I'm not able to play and sync them with other devices. Being able to consume content in a single place provides users with an incredible amount of freedom by not requiring the use of multiple apps, websites, or accounts.</p>\n<p>One last way in which RSS gives user freedom to choose how they consume content is by giving them control of their feed. Various platforms like social media sites and YouTube generate feeds and recommend content using algorithms. These algorithms are sometimes useful by exposing you to new and diverse content. Other times, algorithms can create a bubble by reinforcing preferences and interests. In any case, there's little visibility or control into how the algorithms make choices and recommendations. RSS by default is ordered in chronological order. Therefore you know you're getting the newest content. Depending on the client, you might be able to sort content into folders in whichever way makes the most sense to you. You might also be able to choose how often the feeds you're subscribed to are checked for new content. If you don't want to get the latest updates every second, set it to update daily or weekly. If you want to get rid of the clutter and only see content from the previous day, you can automatically delete or mark older content as read. Note that all these capabilities are dependent on the client, but if this is important to you, you can choose a client that gives you these features making the possibilities endless. Finally, because you are actively choosing which feeds you subscribe to, you're not presented with content other people liked, retweeted, shared, or recommended. You only have access to the content you choose to subscribe to. If at any point you're no longer interested in the content, unsubscribe and you no longer see it on your feed.</p>\n\n<p>As I continue my rediscovery of RSS, I want to find ways to subscribe to content on platforms I'm already using via RSS. It's important to note that this is not an either or approach. You can subscribe to content from the various platforms using RSS to get updates in a single place and since RSS links to the original content, you can consume content on the platform itself helping content creators with more page views, likes, subscriptions, and other forms of engagement. Ultimately though, the choice is yours. A choice of how you publish and consume content.</p>\n","tags":["protocols","web","rss"]},{"title":"Alternatives to WhatsApp","contentType":"posts","date":"2021-01-09 13:44:27","url":"/posts/alternatives-to-whatsapp/","content":"\n<p>Recently I've heard a lot of buzz around people moving away from the WhatsApp messaging app. I'm not tuned into all the details as to why there's been a mass exodus, but from what I understand it has to do with its privacy policies. I've seen many people including Elon Musk recommend Signal. Signal is a great alternative but there are a few others that are worth considering. This recommendation was originally planned as a tweet, but there's so many details that just can't be captured in a tweet or thread so instead I turned it into a blog post. If you haven't made the switch or if you're still not completely committed to the platform you've switched to, this post is for you.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/104107289-08689180-5289-11eb-890d-54527902df97.png\" class=\"img-fluid\" alt=\"Phone and notebook - Oscar Mucyo Unsplash\" /></p>\n\n<p><img src=\"https://user-images.githubusercontent.com/11130940/104107320-3e0d7a80-5289-11eb-9449-71e34315b96e.png\" class=\"img-fluid\" alt=\"Element web client screenshot\" /></p>\n<p><a href=\"https://matrix.org/\">Matrix</a> is my go-to messaging platform. Matrix is an open-source project that provides an open standard for real-time communication. Built on top of this standard are several implementations with <a href=\"https://github.com/matrix-org/synapse\">Synapse</a> being the most popular server and <a href=\"https://element.io/\">Element</a> being the most popular client.</p>\n<p>Matrix is relatively new. However, it has a lot of potential and there are a lot of exciting things happening around it. Some of the most notable include:</p>\n<ul>\n<li><a href=\"https://matrix.org/blog/2019/12/19/welcoming-mozilla-to-matrix\">Mozilla selected Matrix</a> to be the successor to its IRC communications.</li>\n<li>Matrix recently <a href=\"https://matrix.org/blog/2020/09/30/welcoming-gitter-to-matrix\">acquired Gitter</a>, a popular chat platform for software projects, and has quickly worked to <a href=\"https://matrix.org/blog/2020/12/07/gitter-now-speaks-matrix\">integrate Gitter with Matrix</a> allowing for it to take advantage of many of the features provided by the protocol.</li>\n<li><a href=\"https://matrix.org/blog/2021/01/04/taking-fosdem-online-via-matrix\">Matrix will power FOSDEM 2021</a> communications.</li>\n<li><a href=\"https://matrix.org/blog/2020/06/02/introducing-p-2-p-matrix\">Matrix is experimenting with P2P architectures</a> to help users have full control over their communications.</li>\n</ul>\n<p>Although Matrix provides several features commonly associated with messaging platforms such as 1:1 messaging, video/audio calls, and group chat, there are a few features worth highlighting:</p>\n<ul>\n<li>End-to-end encryption</li>\n<li>Interoperability with other communication platforms</li>\n<li>Decentralization</li>\n</ul>\n<h3>End-to-end encryption</h3>\n<p>End-to-end encryption makes sure your communications stay private. Only you and the intended recipients of the message are able to see the messages.</p>\n<h3>Interoperability</h3>\n<p>One of the biggest challenges of moving over to a new platform is that your friends and family don't or won't join you, making it a lonely place. Matrix provides interoperability with other communication platforms and protocols through <a href=\"https://matrix.org/bridges/\">bridges</a>. Bridges facilitate the exchange of messages with other platforms without requiring all participants to be on Matrix. The core team maintains bridges for Slack, IRC, XMPP, and Gitter. There are also community maintained bridges for WhatsApp, Discord, Facebook Messenger, Signal, and many others.</p>\n<h3>Decentralization</h3>\n<p>Decentralization means that there's no central authority that holds control over the platform or data. Matrix is decentralized through federation. One of the most common examples of federation is e-mail. I may have an AOL e-mail address and you may have a Hotmail e-mail address, yet we're still able to exchange messages. With Matrix, in a similar way, users and groups don't have to have their accounts on the same server in order to communicate. It also means that users have a choice over <a href=\"https://www.hello-matrix.net/public_servers.php\">which server they join</a>. If I don't agree with the conversations or moderation of a particular server, I have the choice to open an account on another server without leaving Matrix altogether. Even better, because Matrix is built using open technologies, I have the option of hosting my own server.</p>\n<p>To get started with Matrix, visit the <a href=\"https://element.io/get-started\">Element</a> website and create an account.</p>\n\n<p><img src=\"https://user-images.githubusercontent.com/11130940/104107348-64331a80-5289-11eb-8606-944269e17d10.png\" class=\"img-fluid\" alt=\"Jami Screenshot\" /></p>\n<p>Another project that I like but haven't used it to the same extent as Matrix is <a href=\"https://jami.net/\">Jami</a>. Jami is an open-source project for real-time communication. Similar to Matrix, Jami is cross-platform, free (as in freedom and beer), end-to-end encrypted, and decentralized. Jami achieves decentralization by using P2P protocols. This means that the data is stored on your device so you're in full control of your data. Jami also doesn't require the internet to work. As long as you're in the same local network as your peers, you're able to communicate.</p>\n<p>To get started with Jami, <a href=\"https://jami.net/download/\">download the app</a> and create an account.</p>\n\n<p>In this post, I've mentioned some communication platforms that I consider great alternatives to WhatsApp. Regardless of the platform you choose, some things that I believe should be top of mind are security and privacy. It's important that you own your data and you have control over who has access to it and what they do with it. Feel free say hi on Matrix <a href=\"https://matrix.to/#/@lqdev:lqdev.tech\">@lqdev:lqdev.tech</a> or Jami where you can find me as lqdev1.</p>\n","tags":["privacy","self-hosted","security","open-source","apps"]},{"title":"How to watch Twitch streams using VLC","contentType":"posts","date":"2021-01-05 21:14:39","url":"/posts/how-to-watch-twitch-using-vlc/","content":"\n<p>Twitch has become a tool for various creators to connect with audiences. Originally, streams were centered around gaming but now there is a wide variety of creators that include musicians, software developers, and craftspeople. One of my favorites is <a href=\"https://www.twitch.tv/brainfeeder\">Brainfeeder</a>. I myself try (and often fail) to build software projects on stream. If you're interested in hanging out and talking machine learning, .NET and all things tech, feel free to join me at <a href=\"https://twitch.tv/lqdev1\">https://twitch.tv/lqdev1</a>.</p>\n<p>Watching Twitch streams on the desktop via the browser can take a toll on compute resources. A more efficient solution to this problem is to use VLC. VLC is a free, open-source, cross-platform media player that supports various protocols. While it was possible to watch Twitch streams using VLC before, it often required additional software such as <a href=\"https://github.com/streamlink/streamlink\">Streamlink</a>. More recent versions make this requirement redundant. In this post, I'll show how to use VLC to watch Twitch streams.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/103722126-4883f780-4f9d-11eb-8cc8-304d1a249ef9.png\" class=\"img-fluid\" alt=\"Twitch iOS App on iPhoneX - Caspar Camille Rubin\" /></p>\n\n<ul>\n<li>VLC (version <strong>3.11.0 or higher</strong>)</li>\n</ul>\n\n<blockquote class=\"blockquote\">\n<p><strong>Update 11/12/2021</strong><br />\nIt's been a while since I've gotten the original steps to work, so feel free to skip them. You can still watch Twitch streams on VLC using Streamlink. To use Streamlink:</p>\n<ol>\n<li><p>Download and install <a href=\"https://streamlink.github.io/install.html\">Streamlink</a></p>\n</li>\n<li><p>Open the terminal and run:</p>\n<pre><code>streamlink https://twitch.tv/&amp;lt;username&amp;gt; best\n</code></pre>\n</li>\n</ol>\n<p>For more information, see the <a href=\"https://streamlink.github.io/cli.html\">Streamlink Command-Line Interface documentation</a></p>\n</blockquote>\n<ol>\n<li>Open VLC</li>\n<li>From the menu, Select <strong>Media &amp;gt; Open Network Stream</strong></li>\n<li>Paste the URL to the stream you want to watch in the text box (i.e. https://twitch.tv/lqdev1)</li>\n</ol>\n<p>At this point, the stream should start playing.</p>\n<h3>CLI</h3>\n<p>Alternatively on Unix (Mac / Linux) systems, you can use the CLI. To watch a stream, open a terminal and enter the following command:</p>\n<pre><code class=\"language-bash\">vlc https://twitch.tv/&amp;lt;username&amp;gt;\n</code></pre>\n<p>Make sure to replace the username with the streamer you want to watch.</p>\n<p>After a few seconds, VLC loads and starts playing the stream.</p>\n\n<p>Below is a comparison of the same stream playing on the browser vs VLC. These test were performed on a Surface Laptop 1 running Windows 10. By no means should you take this as a definitive performance comparison as there are many variables that may affect performance. However, it's a nice way to visualize each of the methods.</p>\n<h3>Browser</h3>\n<p>In the browser scenario, I only had a single tab playing the Twitch stream on Microsoft Edge (chromium).</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/103723087-43c04300-4f9f-11eb-9969-df40541693bd.png\" class=\"img-fluid\" alt=\"Browser task manager\" /></p>\n<h3>VLC</h3>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/103721933-e1fed980-4f9c-11eb-92f3-0147f2c09134.png\" class=\"img-fluid\" alt=\"VLC task manager\" /></p>\n<p>As it can be seen from this image, there's about a 4x decrease in CPU resource consumption when using VLC.</p>\n\n<p>In this post I showed how you can use VLC to watch Twitch streams. One of the drawbacks is, you can't see the channels you follow and who's online. You also can't browse streams like you would on the Twitch web client. You also don't have access to chat, which is half the fun on many streams. However, if you're only interested in the content, VLC is an excellent and less resource-hungry way of watching Twitch streams. See you on Twitch!</p>\n","tags":["streaming","hacks","video","IT"]},{"title":"Train an image classifier using F# and ML .NET","contentType":"posts","date":"2020-12-14 20:03:18","url":"/posts/image-classification-mlnet-fsadvent2020/","content":"\n<p>This post is part of F# Advent 2020. Thank you to <a href=\"https://twitter.com/sergey_tihon\">Sergey Tihon</a> for organizing this and the rest of the contributors for producing high-quality content. Make sure to check out the rest of the <a href=\"https://sergeytihon.com/2020/10/22/f-advent-calendar-in-english-2020/\">F# Advent 2020 content</a>.</p>\n<p>When picking who's on the naughty or nice list, I often wonder how Santa decides. I took a shot at answering this question by training an image classifier using the ML.NET image classification API and images of Disney heroes and villains to tell whether they're naughty or nice. You shouldn't judge someone by the way they look (even if they are the Evil Queen), so it's safe to say, don't try this at home or with your neighbors 😉. This sample is just for demo purposes. You can find the full code on <a href=\"https://gist.github.com/lqdev/0c4dc9eea93b7b8541f31ddd429afb53\">GitHub</a>.</p>\n\n<p>This sample was built on a Windows 10 PC, but should work on Mac / Linux PCs</p>\n<ul>\n<li><a href=\"https://dotnet.microsoft.com/download\">.NET 5 SDK</a></li>\n</ul>\n\n<p>The dataset contains images of Disney characters, both real and animated. They were obtained from the <a href=\"https://disney.fandom.com/wiki/The_Disney_Wiki\">Disney Fandom Wiki</a>. The characters are split into two categories, <a href=\"https://disney.fandom.com/wiki/Category:Villains\">villains</a> and <a href=\"https://disney.fandom.com/wiki/Category:Protagonists\">heroes</a>. For the purpose of this sample, we'll label heroes as nice and villains as naughty. The dataset used to train this model contains 2400 villain (naughty) and 675 hero (nice) images stored in top-level directories with the naughty/nice names. This means that the dataset is unbalanced and may skew predictions as it can be seen when making predictions.</p>\n\n<p>Use the <code>#r</code> convention to install the necessary NuGet packages used in this sample.</p>\n<pre><code class=\"language-fsharp\">#r &amp;quot;nuget:Microsoft.ML&amp;quot;\n#r &amp;quot;nuget:Microsoft.ML.Vision&amp;quot;\n#r &amp;quot;nuget:Microsoft.ML.ImageAnalytics&amp;quot;\n#r &amp;quot;nuget:SciSharp.TensorFlow.Redist&amp;quot; \n</code></pre>\n<p>Then, import the packages.</p>\n<pre><code class=\"language-fsharp\">open System\nopen System.IO\nopen Microsoft.ML\nopen Microsoft.ML.Data\nopen Microsoft.ML.Vision\n</code></pre>\n\n<p>Start off by defining the data types containing your input and output schema. You can do this by creating two records, <code>ImageData</code> and <code>ImagePrediction</code>. <code>ImageData</code> is the input which contains the path to image file and the category it belongs to and the <code>ImagePrediction</code> contains the prediction generated by the model.</p>\n<pre><code class=\"language-fsharp\">[&amp;lt;CLIMutable&amp;gt;]\ntype ImageData = {\n    ImagePath: string\n    Label: string\n}\n\n[&amp;lt;CLIMutable&amp;gt;]\ntype ImagePrediction = {\n    PredictedLabel: string\n}\n</code></pre>\n\n<p>The training process loads a set of training images, preprocesses them, and uses the ML.NET image classification API to train an image classification model.</p>\n<h3>Initialize MLContext</h3>\n<p>Once you've defined the data type, initialize the <code>MLContext</code>. <code>MLContext</code> is the entrypoint for ML.NET applications.</p>\n<pre><code class=\"language-fsharp\">let ctx = new MLContext()\n</code></pre>\n<h3>Load training data</h3>\n<p>Then, load the data using the helper function <code>loadImagesFromDirectory</code> and point it to the top-level directory containing the subdirectories of images in the nice and naughty categories.</p>\n<pre><code class=\"language-fsharp\">let imageData = loadImagesFromDirectory &amp;quot;C:/Datasets/fsadvent2020/Train&amp;quot; true\n</code></pre>\n<p>The <code>loadImagesFromDirectory</code> function looks like the following:</p>\n<pre><code class=\"language-fsharp\">let loadImagesFromDirectory (path:string) (useDirectoryAsLabel:bool) = \n\n    let files = Directory.GetFiles(path, &amp;quot;*&amp;quot;,searchOption=SearchOption.AllDirectories)\n\n    files\n    |&amp;gt; Array.filter(fun file -&amp;gt; \n        (Path.GetExtension(file) = &amp;quot;.jpg&amp;quot;) ||\n        (Path.GetExtension(file) = &amp;quot;.png&amp;quot;))\n    |&amp;gt; Array.map(fun file -&amp;gt; \n        let mutable label = Path.GetFileName(file)\n        if useDirectoryAsLabel then\n            label &amp;lt;-  Directory.GetParent(file).Name\n        else\n            let mutable brk = false\n            for index in 0..label.Length do\n                while not brk do\n                    if not (label.[index] |&amp;gt; Char.IsLetter) then\n                        label &amp;lt;- label.Substring(0,index)\n                        brk &amp;lt;- true\n\n        {ImagePath=file; Label=label}\n    )\n</code></pre>\n<p>Then, create an <code>IDataView</code> for the training images. An <code>IDataView</code> is the way data is represented in ML.NET.</p>\n<pre><code class=\"language-fsharp\">let imageIdv = ctx.Data.LoadFromEnumerable&amp;lt;ImageData&amp;gt;(imageData)\n</code></pre>\n<h3>Define training pipeline</h3>\n<p>Once your data is is loaded into an <code>IDataView</code>, set the classifier options by using <code>ImageClassificationTrainer.Options</code>. Use it to define the name of the network architecture, input and output columns, and some additional parameters. The network architecture used in this case is <a href=\"https://www.paperswithcode.com/method/inception-resnet-v2\"><code>ResNet V2</code></a>.</p>\n<pre><code class=\"language-fsharp\">let classifierOptions = ImageClassificationTrainer.Options()\nclassifierOptions.FeatureColumnName &amp;lt;- &amp;quot;Image&amp;quot; \nclassifierOptions.LabelColumnName &amp;lt;- &amp;quot;LabelKey&amp;quot; \nclassifierOptions.TestOnTrainSet &amp;lt;- true  \nclassifierOptions.Arch &amp;lt;- ImageClassificationTrainer.Architecture.ResnetV2101\nclassifierOptions.MetricsCallback &amp;lt;- Action&amp;lt;ImageClassificationTrainer.ImageClassificationMetrics&amp;gt;(fun x -&amp;gt; printfn &amp;quot;%s&amp;quot; (x.ToString()))\n</code></pre>\n<p>Define the preprocessing steps, image classification trainer (along with the previously defined options) and postprocessing steps.</p>\n<pre><code class=\"language-fsharp\">let pipeline = \n    EstimatorChain()\n        .Append(ctx.Transforms.LoadRawImageBytes(&amp;quot;Image&amp;quot;,null,&amp;quot;ImagePath&amp;quot;))\n        .Append(ctx.Transforms.Conversion.MapValueToKey(&amp;quot;LabelKey&amp;quot;,&amp;quot;Label&amp;quot;))\n        .Append(ctx.MulticlassClassification.Trainers.ImageClassification(classifierOptions))\n        .Append(ctx.Transforms.Conversion.MapKeyToValue(&amp;quot;PredictedLabel&amp;quot;))\n</code></pre>\n<p>The ML.NET image classification API leverages a technique known as transfer learning. Transfer learning uses pretrained models (usually neural networks) and retrains the last few layers using new data. This significantly cuts down the amount of time, resources, and data you need to train deep learning models. ML .NET is able to do this with the help of <a href=\"https://github.com/SciSharp/TensorFlow.NET\">TensorFlow .NET</a>, a set of .NET bindings for the TensorFlow deep learning framework. Although transfer learning usually makes the process of training a deep learning models less resource intensive, the TensorFlow API is usually low level and still requires a significant amount of code. See this <a href=\"https://github.com/SciSharp/SciSharp-Stack-Examples/blob/master/src/TensorFlowNET.Examples/ImageProcessing/TransferLearningWithInceptionV3.cs\">transfer learning example</a> from TensorFlow.NET to see how you'd do it in TensorFlow.NET. Although the low-level nature of the TensorFlow API gives you control over what you're building, many times you don't need that level of control. ML.NET through the image classification trainer greatly simplifies this process by providing a high-level API for achieving the same task.</p>\n<p>To train the model, use the <code>Fit</code> method on the training image <code>IDataView</code>.</p>\n<pre><code class=\"language-fsharp\">let model = pipeline.Fit(imageIdv)\n</code></pre>\n<p>Throughout the training process, you should see output similar to the following:</p>\n<pre><code class=\"language-console\">Phase: Bottleneck Computation, Dataset used:      Train, Image Index: 279\nPhase: Bottleneck Computation, Dataset used:      Train, Image Index: 280\nPhase: Bottleneck Computation, Dataset used: Validation, Image Index:   1\n</code></pre>\n<p>With the model trained, it's time to use it to make predictions. Optionally, you can save it for use in other applications.</p>\n<pre><code class=\"language-fsharp\">ctx.Model.Save(model,imageIdv.Schema,&amp;quot;fsadvent2020-model.zip&amp;quot;)\n</code></pre>\n\n<p>Load the test images and create an <code>IDataView</code> for them. The test images used are of Jack Skellington and The Grinch.</p>\n<pre><code class=\"language-fsharp\">let testImages = \n    Directory.GetFiles(&amp;quot;C:/Datasets/fsadvent2020/Test&amp;quot;)\n    |&amp;gt; Array.map(fun file -&amp;gt; {ImagePath=file; Label=&amp;quot;&amp;quot;})\n\nlet testImageIdv = ctx.Data.LoadFromEnumerable&amp;lt;ImageData&amp;gt;(testImages)\n</code></pre>\n<p><img src=\"https://www.khwiki.com/images/thumb/f/ff/Jack_Skellington_KHII.png/180px-Jack_Skellington_KHII.png\" class=\"img-fluid\" alt=\"The grinch\" /></p>\n<p>Then, use the model to make predictions.</p>\n<pre><code class=\"language-fsharp\">let predictionIdv = model.Transform(testImageIdv)\n</code></pre>\n<p>One of the easiest ways to access the predictions is to create an <code>IEnumerable</code>. To do so, use the <code>CreateEnumerable</code> method.</p>\n<pre><code class=\"language-fsharp\">let predictions = ctx.Data.CreateEnumerable&amp;lt;ImagePrediction&amp;gt;(predictionIdv,false)\n</code></pre>\n<p>Then, use the built-in F# sequence operations to display the predictions</p>\n<pre><code class=\"language-fsharp\">predictions |&amp;gt; Seq.iter(fun pred -&amp;gt; \n    printfn &amp;quot;%s is %s&amp;quot; (Path.GetFileNameWithoutExtension(pred.ImagePath)) pred.PredictedLabel)\n</code></pre>\n<p>The output should look like the following:</p>\n<pre><code class=\"language-console\">grinch is Naughty\njack is Naughty\n</code></pre>\n\n<p>In this post, I showed how you can use the ML.NET and TensorFlow.NET to train an image classification model to classify Disney characters as naughty or nice. Depending on the level of control you need, you might choose to use TensorFlow.NET or if you want a high-level API for training an image classifier, you can use the ML.NET. Most importantly, we figured out that Jack Skellington and The Grinch are naughty, so I guess no gifts for them this year? Happy coding!</p>\n<h3>Call to Action</h3>\n<p>Originally, I had planned on writing this article using <a href=\"https://github.com/SciSharp/TensorFlow.NET\">TensorFlow.Keras</a>, which is part of the SciSharp TensorFlow.NET project. TensorFlow.Keras provides .NET bindings for Keras. Keras provides a high-level API for TensorFlow which makes the process of building custom neural networks much simpler than working with the TensorFlow API. Unfortunately, while trying to adapt my scenario to an <a href=\"https://github.com/SciSharp/SciSharp-Stack-Examples/blob/master/src/TensorFlowNET.Examples/ImageProcessing/ImageClassificationKeras.cs\">existing sample</a>, I ran into an <a href=\"https://github.com/SciSharp/SciSharp-Stack-Examples/issues/23\">issue</a>.  This is not something I would have been able to resolve in time to publish this post, so I defaulted to using ML.NET.</p>\n<p>I'm a big fan of the work being done by the SciSharp community and the machine learning and data science capabilities it brings to the .NET ecosystem. The work and efforts are all community driven, and as such, there's plenty of opportunities to contribute. Here are just some examples of ways to contribute, especially from an F# perspective. From my end, I plan on eventually converting this sample to use TensorFlow.Keras. See you in the SciSharp repos! 🙂</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/102030239-f4ac3700-3d7f-11eb-9898-f18990a56326.png\" class=\"img-fluid\" alt=\"FsLab SciSharp contribute\" /></p>\n","tags":["image classification","mlnet","fsharp","dotnet","deep learning","fsadvent2020","tensorflow"]},{"title":"Use .NET Interactive to run .NET code in Jupyter Notebooks on an Azure Machine Learning compute instance","contentType":"posts","date":"2020-08-29 20:23:09","url":"/posts/dotnet-jupyter-notebooks-azure-machine-learning-compute-instance/","content":"\n<p>In this post, I'll go over how you can run .NET code in Jupyter Notebooks using .NET Interactive and Azure Machine Learning compute instances.</p>\n<p><a href=\"https://jupyter.org/\">Jupyter Notebooks</a> are an graphical interactive computing environment used in disciplines like data science and education. When it comes to prototyping or learning scenarios, they can help users see near real-time what their code is doing. The interactive computing protocol used by Jupyter Notebooks is extensible and as a result, you're able to run a variety of languages inside of this environment via kernels. Kernels are processes that take the input from the user, execute the code, and return the output for display and further processing. Using <a href=\"https://github.com/dotnet/interactive\">.NET Interactive</a>, a .NET Core global tool, that among its many features provides a kernel for Jupyter Notebooks to run .NET (C#/F#) and PowerShell code.</p>\n<p>Although you can install and run Jupyter Notebooks on your local computer, services like <a href=\"https://docs.microsoft.com/azure/machine-learning/\">Azure Machine Learning</a> provide single VMs known as compute instances which have Jupyter Notebooks as well as many popular data science libraries and development tools like Docker preinstalled. Therefore, installing .NET Interactive on one of these VMs is a seamless experience. Additionally, Azure Machine Learning gives you the ability to share computing resources with teams of various sizes.</p>\n<p>Let's get started!</p>\n\n<ul>\n<li><a href=\"https://azure.microsoft.com/free/\">Azure account</a>. If you don't have one, create one.</li>\n</ul>\n\n<p>A workspace organizes all of you Azure Machine Learning resources and assets such as compute instances in a single place. In order to use Azure Machine Learning, you have to create a workspace.</p>\n<p>There's multiple ways to create a workspace. For this writeup, I use the Azure Portal.</p>\n<p>Navigate to <a href=\"https://portal.azure.com\">portal.azure.com</a></p>\n<p>In the portal, select <strong>Create a resource</strong>.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/91647989-d79d6380-ea2f-11ea-9d97-cc265c79801f.png\" class=\"img-fluid\" alt=\"Create new resource Azure Portal\" /></p>\n<p>From the resource list, select <strong>AI + Machine Learning &amp;gt; Machine Learning</strong></p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/91647999-01ef2100-ea30-11ea-954a-3c3280a9cbca.png\" class=\"img-fluid\" alt=\"Create Azure Machine Learning Resource\" /></p>\n<p>Fill in the form and select <strong>Review + create</strong>.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/91648065-07993680-ea31-11ea-9273-c28b4ec4bf7c.png\" class=\"img-fluid\" alt=\"Create Azure Machine Learning workspace\" /></p>\n<p>Review your information before creating the workspace and select <strong>Create</strong>. Deployment takes a few minutes. As part of your workspace deployment, additional Azure resources are created such as Azure Container Registry, Azure Storage account, Azure Application Insights, and Azure KeyVault.</p>\n<p>For more information on workspaces, see the <a href=\"https://docs.microsoft.com/azure/machine-learning/concept-workspace\">Azure Machine Learning workspace documentation</a>.</p>\n\n<p>As mentioned earlier, an Azure Machine Learning compute instance is a single VM that comes with a variety of development tools and libraries commonly used for data science and machine learning workflows preinstalled.</p>\n<p>Navigate to <a href=\"https://ml.azure.com\">Azure Machine Learning studio</a>. The studio is a web interface for managing your Azure Machine Learning resources such as data, experiments, models, and compute.</p>\n<p>In studio, select <strong>Create new &amp;gt; Compute instance</strong> to create a new compute instance.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/91648186-73c86a00-ea32-11ea-9986-205306fe76b0.png\" class=\"img-fluid\" alt=\"Create new Azure Machine Learning compute instance\" /></p>\n<p>Provide a name for your instance and select <strong>Create</strong>. You can also customize the size of your VM and whether you want it to be GPU enabled. In this case, I just selected the preselected option.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/91648251-1c76c980-ea33-11ea-91de-818e1f63767c.png\" class=\"img-fluid\" alt=\"Provision Azure Machine Learning compute instance\" /></p>\n<p>Once your compute instance is provisioned, select the <strong>Jupyter</strong> link to launch Jupyter Notebooks.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/91648307-f271d700-ea33-11ea-9b80-57e44f0bcdf5.png\" class=\"img-fluid\" alt=\"Launch Jupyter Notebooks\" /></p>\n<p>For more information on compute instances, see the <a href=\"https://docs.microsoft.com/azure/machine-learning/concept-compute-instance\">Azure Machine Learning compute instance documentation</a>.</p>\n\n<p>At the time of this writing, the image used by compute instances is Ubuntu 16.04. Therefore, we'll be installing the Linux version of the .NET Core SDK.</p>\n<p>Inside the Jupyter Notebooks, select <strong>New &amp;gt; Terminal</strong> to create a new terminal.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/91648326-1c2afe00-ea34-11ea-8f84-48b98e1ec148.png\" class=\"img-fluid\" alt=\"image\" /></p>\n<p>In the terminal, add the Microsoft package signing key to your list of trusted keys.</p>\n<pre><code class=\"language-bash\">wget https://packages.microsoft.com/config/ubuntu/16.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb\nsudo dpkg -i packages-microsoft-prod.deb\n</code></pre>\n<p>Install the .NET Core 3.1 SDK with the following command:</p>\n<pre><code class=\"language-bash\">sudo apt-get update; \\\n  sudo apt-get install -y apt-transport-https &amp;amp;&amp;amp; \\\n  sudo apt-get update &amp;amp;&amp;amp; \\\n  sudo apt-get install -y dotnet-sdk-3.1\n</code></pre>\n<blockquote class=\"blockquote\">\n<p>Note that as of the time of this writing, .NET Interactive requires .NET Core 3.1.</p>\n</blockquote>\n<p>You can also find these installation instructions in the <a href=\"https://docs.microsoft.com/dotnet/core/install/linux-ubuntu#1604-\">.NET Core installation documentation</a>.</p>\n\n<p>Start off by checking which kernels are installed. In the terminal, enter the following command.</p>\n<pre><code class=\"language-bash\">jupyter kernelspec list\n</code></pre>\n<p>The output should look something like the output below</p>\n<pre><code class=\"language-console\">Available kernels:\n  python3            /anaconda/envs/azureml_py36/share/jupyter/kernels/python3\n  ir                 /usr/local/share/jupyter/kernels/ir\n  python3-azureml    /usr/local/share/jupyter/kernels/python3-azureml\n</code></pre>\n<p>Then, install the .NET Interactive global tool</p>\n<pre><code class=\"language-bash\">dotnet tool install -g --add-source &amp;quot;https://dotnet.myget.org/F/dotnet-try/api/v3/index.json&amp;quot; Microsoft.dotnet-interactive\n</code></pre>\n<p>Use the .NET Interactive tool to install the .NET and powershell kernels.</p>\n<pre><code class=\"language-bash\">dotnet interactive jupyter install\n</code></pre>\n<p>Finally, run the following command to confirm that the kernels have been installed.</p>\n<pre><code class=\"language-bash\">jupyter kernelspec list\n</code></pre>\n<p>In the output you should see the C#, F#, and PowerShell kernels listed</p>\n<pre><code class=\"language-bash\">Available kernels:\n  .net-csharp        /home/azureuser/.local/share/jupyter/kernels/.net-csharp\n  .net-fsharp        /home/azureuser/.local/share/jupyter/kernels/.net-fsharp\n  .net-powershell    /home/azureuser/.local/share/jupyter/kernels/.net-powershell\n  python3            /anaconda/envs/azureml_py36/share/jupyter/kernels/python3\n  ir                 /usr/local/share/jupyter/kernels/ir\n  python3-azureml    /usr/local/share/jupyter/kernels/python3-azureml\n</code></pre>\n\n<p>The Jupyter Notebook server is managed by a <code>systemd</code> service. Although during installation .NET Interactive is added to your <code>PATH</code> environment variable, it's not added to the environment file <code>/etc/environment</code> which is used by <code>systemd</code>. Therefore, in order to run .NET Interactive, you have to add the .NET global tools directory to that file.</p>\n<p>The contents of the file should look similar to the one below</p>\n<pre><code class=\"language-text\">PATH=&amp;quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games&amp;quot;\nAML_CloudName=AzureCloud\n</code></pre>\n<p>Open the <code>/etc/environment</code> using your preferred text editor (vi/nano) and replace the <code>PATH</code> with the following content:</p>\n<pre><code class=\"language-text\">PATH=&amp;quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/azureuser/.dotnet/tools&amp;quot;\n</code></pre>\n<p>Save your changes and restart the Jupyter Server. In the terminal, run the following command.</p>\n<pre><code class=\"language-bash\">sudo service jupyter restart\n</code></pre>\n<p>This will close your terminal session since the notebook server has been restarted.</p>\n<p>Close all Jupyter Notebook windows.</p>\n\n<p>From Azure Machine Learning studio, launch Jupyter Notebooks again by selecting the <strong>Jupyter</strong> link.</p>\n<p>Then, in Jupyter Notebooks select <strong>New</strong> and create a new notebook (C#/F#/PowerShell). In this case, I created an F# notebook.</p>\n<p>Once the kernel is ready, enter code into the first cell and run it.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/91648958-20f3b000-ea3c-11ea-95a1-3e4ba71c31d8.png\" class=\"img-fluid\" alt=\"FSharp Jupyter Notebook running in AML compute instance\" /></p>\n<p>Congratulations! You should now be able to run .NET and PowerShell code inside of your Azure Machine Learning compute instance.</p>\n\n<p>In this post, I showed how you can run .NET code in Jupyter Notebooks on an Azure Machine Learning compute instance with the help of .NET Interactive. This enables you to interactively prototype solutions remotely while still having control over your environment and dependencies. Now that you have .NET interactive setup, compute instances also give you the option of using JupyterLab. Feel free to tinker with the different environments and see which one works best for you. Happy coding!</p>\n\n<ul>\n<li><a href=\"https://docs.microsoft.com/azure/machine-learning/\">Azure Machine Learning documentation</a></li>\n<li><a href=\"https://github.com/dotnet/interactive\">.NET Interactive GitHub repository</a></li>\n<li><a href=\"https://jupyter.org/\">Jupyter Notebooks</a></li>\n</ul>\n","tags":["dotnet","Jupyter Notebooks","dotnet Interactive","Azure Machine Learning","Azure","Programming","Development Tools"]},{"title":"Serverless Racket Applications Using Azure Functions Custom Handlers","contentType":"posts","date":"2020-03-21 13:45:43","url":"/posts/serverless-racket-azure-functions-custom-handlers/","content":"\n<p><a href=\"https://racket-lang.org/\">Racket</a> is a fun and powerful general purpose programming language based on the Scheme dialect of Lisp that provides tools and packages that allow individuals to quickly be productive. Although you can build traditional web applications with it, it would be nice to use it with cloud-native technologies like serverless. Azure Functions is designed for these types of event-driven workflows but unfortunately does not officially support Racket. Recently, a new feature called custom handlers was announced which allows individuals to run web applications written in any language that supports HTTP primitives as an Azure Function. When I learned of this feature, my immediate thought was, challenge accepted!</p>\n<p>Custom handlers require the following:</p>\n<ol>\n<li>Write a web server to process requests</li>\n<li>Define the bindings for the request and response function payloads</li>\n<li>Configure the Azure Functions host to send request to the web server</li>\n</ol>\n<p>In this writeup, I'll show how to set up a Racket web server that processes <code>GET</code> requests running as an Azure Function. The source code for this project can be found in the <a href=\"https://github.com/lqdev/RacketAzureFunctionsCustomHandlerSample\">RacketAzureFunctionsCustomHandlerSample GitHub repository</a></p>\n\n<p>This project was built on a Windows 10 PC, but it should work cross-platform on Mac and Linux.</p>\n<ul>\n<li><a href=\"https://nodejs.org/en/\">Node.js</a></li>\n<li><a href=\"https://download.racket-lang.org/\">Racket</a></li>\n<li><a href=\"https://docs.microsoft.com/azure/azure-functions/functions-run-local\">Azure Functions Core Tools</a>. This sample uses v2.x of the tool.</li>\n</ul>\n\n<p>Create a new directory called <code>RacketAzureFunctionsCustomHandlerSample</code> and navigate to it.</p>\n\n<p>The way Azure Functions custom handlers work is by having the Azure Functions host proxy requests to a web server written in the language of choice which processes the request and sends the response back to the Azure Functions host.</p>\n<p>Start by setting the environment variable where Azure Functions and the server listen on. Create and environment variable called <code>FUNCTIONS_HTTPWORKER_PORT</code>. In this example, I set the variable to <code>7071</code>.</p>\n<p>Inside of your application directory, create a file called <em>server.rkt</em> which will contain the server logic.</p>\n<p>Open the <em>server.rkt</em> file. Define the language and import the required packages.</p>\n<pre><code class=\"language-racket\">#lang racket\n(require json)\n(require web-server/servlet)\n(require web-server/servlet-env)\n</code></pre>\n<p>Then, get the port where the server listens on from the <code>FUNCTIONS_HTTPWORKER_PORT</code> environment variable.</p>\n<pre><code class=\"language-racket\">(define PORT (string-&amp;gt;number (getenv &amp;quot;FUNCTIONS_HTTPWORKER_PORT&amp;quot;)))\n</code></pre>\n<p>Next, create a function called <code>get-values</code> to process your request. In this case, the function receives a <code>GET</code> request that returns a JSON object containing a list of integers.</p>\n<pre><code class=\"language-racket\">(define (get-values req)\n    (response/full\n        200\n        #&amp;quot;OK&amp;quot;\n        (current-seconds)\n        #&amp;quot;application/json;charset=utf-8&amp;quot;\n        empty\n        (list (jsexpr-&amp;gt;bytes #hasheq((value . (1 2 3)))))))\n</code></pre>\n<p>After that, define the routes so your server dispatches requests to the appropriate endpoint. In this case, <code>GET</code> requests to the <code>/values</code> endpoint are sent to and processed by the <code>get-values</code> function.</p>\n<pre><code class=\"language-racket\">(define-values (dispatch req)\n    (dispatch-rules\n        [(&amp;quot;values&amp;quot;) #:method &amp;quot;get&amp;quot; get-values]\n        [else (error &amp;quot;Route does not exist&amp;quot;)]))\n</code></pre>\n<p>The final <em>server.rkt</em> file should contains content similar to the one below:</p>\n<pre><code class=\"language-racket\">;; Define language and import packages\n#lang racket\n(require json)\n(require web-server/servlet)\n(require web-server/servlet-env)\n\n;; Get port where server listens on\n(define PORT (string-&amp;gt;number (getenv &amp;quot;FUNCTIONS_HTTPWORKER_PORT&amp;quot;)))\n\n;; Create function to handle GET /values request\n(define (get-values req)\n    (response/full\n        200\n        #&amp;quot;OK&amp;quot;\n        (current-seconds)\n        #&amp;quot;application/json;charset=utf-8&amp;quot;\n        empty\n        (list (jsexpr-&amp;gt;bytes #hasheq((value . (1 2 3)))))))\n\n;; Define routes\n(define-values (dispatch req)\n    (dispatch-rules\n        [(&amp;quot;values&amp;quot;) #:method &amp;quot;get&amp;quot; get-values]\n        [else (error &amp;quot;Route does not exist&amp;quot;)]))\n\n;; Define and start server\n(serve/servlet\n    (lambda (req) (dispatch req))\n    #:launch-browser? #f\n    #:quit? #f\n    #:port PORT\n    #:servlet-path &amp;quot;/&amp;quot;\n    #:servlet-regexp #rx&amp;quot;&amp;quot;)\n</code></pre>\n\n<p>Start the server by running the following command:</p>\n<pre><code class=\"language-bash\">racket --require server.rkt\n</code></pre>\n<p>Then, using an application like Postman or Insomina, make a <code>GET</code> request to <code>http://localhost:7071/values</code>.</p>\n<p>The response should look like the following:</p>\n<pre><code class=\"language-json\">{\n    &amp;quot;value&amp;quot;: [\n        1,\n        2,\n        3\n    ]\n}\n</code></pre>\n\n<p>The way Azure Functions discovers functions is through subdirectories containing a binding definition called <em>function.json</em>. The name of the subdirectories must match the name of your function's route path. For example if the route path is <code>/values</code>, then the name of the subdirectory is  <code>values</code>.</p>\n<p>Create a subdirectory inside the main application directory called <em>values</em></p>\n<p>Inside the <em>values</em> subdirectory, create a file called <em>function.json</em> and add the following content to it.</p>\n<pre><code class=\"language-json\">{\n    &amp;quot;bindings&amp;quot;: [\n      {\n        &amp;quot;type&amp;quot;: &amp;quot;httpTrigger&amp;quot;,\n        &amp;quot;direction&amp;quot;: &amp;quot;in&amp;quot;,\n        &amp;quot;name&amp;quot;: &amp;quot;req&amp;quot;,\n        &amp;quot;methods&amp;quot;: [&amp;quot;get&amp;quot;]\n      },\n      {\n        &amp;quot;type&amp;quot;: &amp;quot;http&amp;quot;,\n        &amp;quot;direction&amp;quot;: &amp;quot;out&amp;quot;,\n        &amp;quot;name&amp;quot;: &amp;quot;res&amp;quot;\n      }\n    ]\n}\n</code></pre>\n<p>The <em>function.json</em> file defines the request and response payloads. In this case, the incoming request is an <code>HttpTrigger</code> that only handles <code>GET</code> requests and returns an HTTP response.</p>\n\n<p>Package your server application into a single executable by entering the following command into the command prompt:</p>\n<pre><code class=\"language-bash\">raco exe server.rkt\n</code></pre>\n<p>Once your application is packaged, an executable with the name <em>server.exe</em> should be created in your application directory.</p>\n\n<p>In your application directory, create a file called <em>host.json</em> and add the following contents:</p>\n<pre><code class=\"language-json\">{\n  &amp;quot;version&amp;quot;: &amp;quot;2.0&amp;quot;,\n  &amp;quot;httpWorker&amp;quot;: {\n      &amp;quot;description&amp;quot;: {\n          &amp;quot;defaultExecutablePath&amp;quot;: &amp;quot;server.exe&amp;quot;\n      }\n  }\n}\n</code></pre>\n<p>This <em>host.json</em> configuration file tells the Azure Functions host where to find the web server executable.</p>\n\n<p>Inside the root application directory, enter the following command into the command prompt.</p>\n<pre><code class=\"language-bash\">func start\n</code></pre>\n<p>Using an application like Postman or Insomnia, make a <code>GET</code> request to <code>localhost:7071/api/values</code>.</p>\n<p>The response should look like the following:</p>\n<pre><code class=\"language-json\">{\n    &amp;quot;value&amp;quot;: [\n        1,\n        2,\n        3\n    ]\n}\n</code></pre>\n\n<p>In this writeup, I showed how to create a Racket serverless application that runs on Azure Functions by using custom handlers. Doing so requires you to:</p>\n<ol>\n<li>Write a web server to process requests</li>\n<li>Define the bindings for the request and response function payloads</li>\n<li>Configure the Azure Functions host to send request to the web server</li>\n</ol>\n<p>Although in this example, the server was written in Racket, the same process is applicable to other languages. Keep in mind that at the time of this writing, custom handlers are preview and may change. Happy coding!</p>\n\n<ul>\n<li><a href=\"https://docs.microsoft.com/azure/azure-functions/functions-custom-handlers\">Azure Functions Custom Handlers</a></li>\n</ul>\n","tags":["serverless","racket","lisp","azure-functions","azure","programming","web"]},{"title":"Deploy ML.NET Machine Learning Model in Blazor WebAssembly Static Website","contentType":"posts","date":"2020-03-01 19:21:04","url":"/posts/deploy-machine-learning-mlnet-models-blazor-webassembly/","content":"\n<p>There are many ways of deploying machine learning models. The most common way to do so is by exposing models as a web service through APIs or serverless functions. One of the considerations when deploying a model as a web service is latency and performance. The process of making predictions over HTTP using a model involves accepting the user input, loading the serialized version of the model from a file, using the model to make a prediction, and returning the prediction back to the user. Since models are typically just static files, another way of deploying a model is as a static asset over the web, just like any other HTML, CSS, or JavaScript file. This deployment method is similar to that of TensorFlow.js. Deploying in this way has several advantages. One advantage is that there is no longer a web service just to serve the model, making it more cost-efficient. Another advantage is that once the model has been downloaded onto the user's PC, the resources used at that point are those of the user's PC rather than the server the model would otherwise be hosted in. Finally, because the model is a static file, it can be distributed via CDNs.</p>\n<p>One of the challenges with this is that machine learning models are usually built using languages other than JavaScript. This makes using the same code / library the model was built difficult or nearly impossible. WebAssembly is changing that by allowing Rust, C++, C# and other languages to run natively inside the browser. Having that ability, the code / logic to load the model and make predictions is much easier and almost comparable to that of a native platform. Blazor WebAssembly provides users the ability to create modern component-based web applications completely in C#. Additionally, Blazor WebAssembly allows users to publish and deploy their applications as static websites in an easy and cost-efficient way. ML.NET is an open-source, cross-platform framework that allows developers to create machine learning models using .NET. In this post, I'll show how to train a multiclass classification machine learning model that predicts iris flower species. Then, I'll take that model and deploy it alongside a Blazor WebAssembly static website to Azure Storage. The full code for this application may be found at the <a href=\"https://github.com/lqdev/MLNETBlazorWASMSample\">MLNETBlazorWASMSample repository on GitHub</a>.</p>\n\n<p>This project was built on a Windows PC but should work cross platform on Mac and Linux.</p>\n<ul>\n<li><a href=\"https://dotnet.microsoft.com/download/dotnet-core/3.1\">.NET Core SDK 3.1</a></li>\n<li><a href=\"https://www.nuget.org/packages/Microsoft.AspNetCore.Blazor.Templates/3.2.0-preview1.20073.1\">Blazor WebAssembly Template</a></li>\n<li><a href=\"http://aka.ms/amlFree\">Azure Subscription</a></li>\n<li><a href=\"https://azure.microsoft.com/en-us/features/storage-explorer/\">Azure Storage Explorer</a></li>\n</ul>\n\n<p>The solution built in this post contains three projects:</p>\n<ul>\n<li>SchemaLibrary: C# .NET Standard 2.0 class library that contains the schema definition classes of the data used to train the model as well as the prediction output generated by the model.</li>\n<li>TrainingConsole: C# .NET Core 3.1 console application used to train the machine learning model.</li>\n<li>BlazorWebApp: Blazor WebAssembly web application to make predictions using machine learning model trained by TrainingConsole application.</li>\n</ul>\n<h3>Install Blazor WebAssembly Template</h3>\n<p>Use the .NET CLI to run the following command in the command prompt:</p>\n<pre><code class=\"language-powershell\">dotnet new -i Microsoft.AspNetCore.Blazor.Templates::3.2.0-preview1.20073.1\n</code></pre>\n<h3>Create the solution</h3>\n<p>Create a new directory for the solution called <em>MLNETBlazorWASMSample</em>.</p>\n<pre><code class=\"language-powershell\">mkdir MLNETBlazorWASMSample\n</code></pre>\n<p>Navigate to the newly created solution directory and create a solution:</p>\n<pre><code class=\"language-powershell\">cd MLNETBlazorWASMSample\ndotnet new sln\n</code></pre>\n<h3>Create schema class library</h3>\n<p>The data schema for the model input and output are shared during training as well as when making predictions. To share resources, create a class library that's shared by the <code>ConsoleTraining</code> and <code>BlazorWebApp</code> projects. In the solution directory, enter the following command:</p>\n<pre><code class=\"language-powershell\">dotnet new classlib -o SchemaLibrary\n</code></pre>\n<p>Install the <em>Microsoft.ML</em> NuGet package (This solution is built with version 1.4.0). The <em>Microsoft.ML</em> package is used throughout the entire solution.</p>\n<pre><code class=\"language-powershell\">dotnet add SchemaLibrary package Microsoft.ML\n</code></pre>\n<p>Add the library project to the solution.</p>\n<pre><code class=\"language-powershell\">dotnet sln add SchemaLibrary\n</code></pre>\n<h3>Create the training console application</h3>\n<p>The console application contains the series of data transformations and algorithms used to train the model. In the solution directory, create a new console application.</p>\n<pre><code class=\"language-powershell\">dotnet new console -o TrainingConsole\n</code></pre>\n<p>Add the console application to the solution.</p>\n<pre><code class=\"language-powershell\">dotnet sln add TrainingConsole\n</code></pre>\n<p>Reference the <em>SchemaLibrary</em> project.</p>\n<pre><code class=\"language-powershell\">dotnet add TrainingConsole reference SchemaLibrary\n</code></pre>\n<h3>Create the Blazor WebAssembly web application</h3>\n<p>The web application contains a few input elements so users can provide new data that the model then uses to make predictions. In the solution directory, create a new Blazor WebAssembly application.</p>\n<pre><code class=\"language-powershell\">dotnet new blazorwasm -o BlazorWebApp\n</code></pre>\n<p>Add the web application project to the solution.</p>\n<pre><code class=\"language-powershell\">dotnet sln add BlazorWebApp\n</code></pre>\n<p>Reference the <em>SchemaLibrary</em> project.</p>\n<pre><code class=\"language-powershell\">dotnet add BlazorWebApp reference SchemaLibrary\n</code></pre>\n\n<h3>Understand the data</h3>\n<p>The data used to train the model comes from the <a href=\"https://archive.ics.uci.edu/ml/datasets/Iris\">iris dataset</a>. It contains four numerical columns which are sepal and petal measurements and one numerical column for the species of iris flower. This is a sample of the data.</p>\n<table class=\"table\">\n<thead>\n<tr>\n<th>Sepal length (cm)</th>\n<th>Sepal width (cm)</th>\n<th>Petal length (cm)</th>\n<th>Petal width (cm)</th>\n<th>Class (iris species)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>5.1</td>\n<td>3.5</td>\n<td>1.4</td>\n<td>0.2</td>\n<td>Iris-setosa</td>\n</tr>\n<tr>\n<td>7.0</td>\n<td>3.2</td>\n<td>4.7</td>\n<td>1.4</td>\n<td>Iris-versicolor</td>\n</tr>\n<tr>\n<td>6.3</td>\n<td>3.3</td>\n<td>6.0</td>\n<td>2.5</td>\n<td>Iris-virginica</td>\n</tr>\n</tbody>\n</table>\n<h3>Define model input schema</h3>\n<p>In the <em>SchemaLibrary</em> project, create a class called <code>ModelInput</code> to model the data used for training and as model input.</p>\n<pre><code class=\"language-powershell\">ni ModelInput.cs\n</code></pre>\n<p>The <code>ModelInput</code> class should look like the following:</p>\n<pre><code class=\"language-csharp\">using Microsoft.ML.Data;\n\nnamespace SchemaLibrary\n{\n    public class ModelInput\n    {\n        [LoadColumn(0)]\n        public float SepalLength { get; set; }\n\n        [LoadColumn(1)]\n        public float SepalWidth { get; set; }\n\n        [LoadColumn(2)]\n        public float PetalLength { get; set; }\n\n        [LoadColumn(3)]\n        public float PetalWidth { get; set; }\n\n        [LoadColumn(4)]\n        public string Label { get; set; }\n    }\n}\n</code></pre>\n<p>Notice that the <code>Class</code> column is now a property called <code>Label</code>. This is for two reasons:</p>\n<ol>\n<li>Avoid using the <code>class</code> keyword.</li>\n<li>In ML.NET, the default column name of the column to predict expected by algorithms is <code>Label</code>.</li>\n</ol>\n<p>Also notice the <code>LoadColumn</code> attributes at the top of each property. This is used to tell the loader the index of the column where the data for the respective property is.</p>\n<h3>Define model output schema</h3>\n<p>Similar to the input schema, there's a schema for the output of the model. The type of model used in this solution is a multiclass classification model since there are more than two categories to choose from for iris flower species. Multiclass classification models output a column called <code>PredictedLabel</code> which contains the name of the predicted category. In the <em>SchemaLibrary</em> project, create a class called <code>ModelOutput</code> to model the predictions made by the model.</p>\n<pre><code class=\"language-powershell\">ni ModelOutput.cs\n</code></pre>\n<p>The <code>ModelOutput</code> class should look like the following:</p>\n<pre><code class=\"language-csharp\">namespace SchemaLibrary\n{\n    public class ModelOutput\n    {\n        public string PredictedLabel { get; set; }\n    }\n}\n</code></pre>\n\n<p>Now it's time to create the application that trains the model.</p>\n<h3>Get the data</h3>\n<p>Download the data and save it inside the <em>TrainingConsole</em> project directory.</p>\n<pre><code class=\"language-powershell\">curl https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data -o iris.data\n</code></pre>\n<h3>Define the data preparation and training steps</h3>\n<p>In the <em>TrainingConsole</em> project, open the <em>Program.cs</em> file and add the following using statements at the top:</p>\n<pre><code class=\"language-csharp\">using System;\nusing System.Linq;\nusing Microsoft.ML;\nusing SchemaLibrary;\n</code></pre>\n<p>Then, delete the contents inside the <code>Main</code> method and replace it with the following.</p>\n<pre><code class=\"language-csharp\">// 1. Initialize MLContext\nMLContext mlContext = new MLContext();\n\n// 2. Load the data\nIDataView data = mlContext.Data.LoadFromTextFile&amp;lt;ModelInput&amp;gt;(&amp;quot;iris.data&amp;quot;, separatorChar:',');\n\n// 3. Shuffle the data\nIDataView shuffledData = mlContext.Data.ShuffleRows(data);\n\n// 3. Define the data preparation and training pipeline.\nIEstimator&amp;lt;ITransformer&amp;gt; pipeline = \n    mlContext.Transforms.Concatenate(&amp;quot;Features&amp;quot;,&amp;quot;SepalLength&amp;quot;,&amp;quot;SepalWidth&amp;quot;,&amp;quot;PetalLength&amp;quot;,&amp;quot;PetalWidth&amp;quot;)\n        .Append(mlContext.Transforms.NormalizeMinMax(&amp;quot;Features&amp;quot;))\n        .Append(mlContext.Transforms.Conversion.MapValueToKey(&amp;quot;Label&amp;quot;))\n        .Append(mlContext.MulticlassClassification.Trainers.NaiveBayes())\n        .Append(mlContext.Transforms.Conversion.MapKeyToValue(&amp;quot;PredictedLabel&amp;quot;));\n\n// 4. Train with cross-validation\nvar cvResults = mlContext.MulticlassClassification.CrossValidate(shuffledData, pipeline);\n\n// 5. Get the highest performing model and its accuracy\n(ITransformer, double) model = \n    cvResults\n        .OrderByDescending(fold =&amp;gt; fold.Metrics.MacroAccuracy)\n        .Select(fold =&amp;gt; (fold.Model, fold.Metrics.MacroAccuracy))\n        .First();\n\nConsole.WriteLine($&amp;quot;Top performing model's macro-accuracy: {model.Item2}&amp;quot;);\n\n// 6. Save the model\nmlContext.Model.Save(model.Item1, data.Schema, &amp;quot;model.zip&amp;quot;);\n\nConsole.WriteLine(&amp;quot;Model trained&amp;quot;);\n</code></pre>\n<p>The training application loads the data from the <code>iris.data</code> file and applies a series of transformations. First, all of the individual numerical columns are combined into a single vector and stored in a new column called <code>Features</code>. The <code>Features</code> column is then normalized and the <code>MapValueToKey</code> transform is used to convert the text in the <code>Label</code> column to a number. Then, the transformed data is used to train a model using the <code>NaiveBayes</code> algorithm. <strong>Note that at the time of this writing, for multiclass classification problems, only Naive Bayes has been confirmed to work with Blazor WebAssembly</strong>. Finally, the <code>PredictedLabel</code> is stored as a number so it has to be converted back into text.</p>\n<p>Using the <code>Fit</code> method, the data is applied to the pipeline. Because the dataset is small, a technique called cross-validation is used to build a more robust model. Once the model is trained, the model with the top performance is then serialized and saved to a file called <em>model.zip</em> for later use in the web application.</p>\n<p>The final <em>Program.cs</em> file should look like the content below:</p>\n<pre><code class=\"language-csharp\">using System;\nusing System.Linq;\nusing Microsoft.ML;\nusing SchemaLibrary;\n\nnamespace TrainingConsole\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            // 1. Initialize MLContext\n            MLContext mlContext = new MLContext();\n\n            // 2. Load the data\n            IDataView data = mlContext.Data.LoadFromTextFile&amp;lt;ModelInput&amp;gt;(&amp;quot;iris.data&amp;quot;, separatorChar:',');\n\n            // 3. Shuffle the data\n            IDataView shuffledData = mlContext.Data.ShuffleRows(data);\n\n            // 3. Define the data preparation and training pipeline.\n            IEstimator&amp;lt;ITransformer&amp;gt; pipeline = \n                mlContext.Transforms.Concatenate(&amp;quot;Features&amp;quot;,&amp;quot;SepalLength&amp;quot;,&amp;quot;SepalWidth&amp;quot;,&amp;quot;PetalLength&amp;quot;,&amp;quot;PetalWidth&amp;quot;)\n                    .Append(mlContext.Transforms.NormalizeMinMax(&amp;quot;Features&amp;quot;))\n                    .Append(mlContext.Transforms.Conversion.MapValueToKey(&amp;quot;Label&amp;quot;))\n                    .Append(mlContext.MulticlassClassification.Trainers.NaiveBayes())\n                    .Append(mlContext.Transforms.Conversion.MapKeyToValue(&amp;quot;PredictedLabel&amp;quot;));\n\n            // 4. Train with cross-validation\n            var cvResults = mlContext.MulticlassClassification.CrossValidate(shuffledData, pipeline);\n\n            // 5. Get the highest performing model and its accuracy\n            (ITransformer, double) model = \n                cvResults\n                    .OrderByDescending(fold =&amp;gt; fold.Metrics.MacroAccuracy)\n                    .Select(fold =&amp;gt; (fold.Model, fold.Metrics.MacroAccuracy))\n                    .First();\n\n            Console.WriteLine($&amp;quot;Top performing model's macro-accuracy: {model.Item2}&amp;quot;);\n\n            // 6. Save the model\n            mlContext.Model.Save(model.Item1, data.Schema, &amp;quot;model.zip&amp;quot;);\n\n            Console.WriteLine(&amp;quot;Model trained&amp;quot;);\n        }\n    }\n}\n</code></pre>\n<h3>Run the application</h3>\n<p>In the <em>TrainConsole</em> project directory, use the following command to run the application and train the model:</p>\n<pre><code class=\"language-powershell\">dotnet run\n</code></pre>\n\n<p>Once you have the model saved, use the Azure Portal to create an Azure Storage account.</p>\n<p><img src=\"/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly1.png\" class=\"img-fluid\" alt=\"Create Azure Storage Account\" /></p>\n<p>Then, navigate to your newly created storage account resource and create a blob container called <code>models</code>.</p>\n<p><img src=\"/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly2.png\" class=\"img-fluid\" alt=\"Create Blob Container\" /></p>\n<p>Once your container is created, navigate to it and upload the <em>model.zip</em> file.</p>\n<p><img src=\"/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly3.png\" class=\"img-fluid\" alt=\"Upload Model\" /></p>\n\n<p>To make predictions, create a web page to take in user input. Then, provide the user input to the model and display the prediction to the user.</p>\n<h3>Set up imports</h3>\n<p>In the <em>BlazorWebApp</em> project directory, open the <em>_Imports.razor</em> file. This contains the using statements for the pages and components in your application. Add the following using statements:</p>\n<pre><code class=\"language-razor\">@using System.IO\n@using Microsoft.ML\n@using SchemaLibrary\n</code></pre>\n<h3>Create user input page</h3>\n<p>In the <em>BlazorWebApp</em> project, create a new razor page called <em>Prediction.razor</em> inside the <em>Pages</em> directory.</p>\n<pre><code class=\"language-powershell\">ni Prediction.razor\n</code></pre>\n<p>Add the following content to it:</p>\n<pre><code class=\"language-razor\">@page &amp;quot;/prediction&amp;quot;\n@inject HttpClient _client\n\n&amp;lt;label&amp;gt;Sepal Length: &amp;lt;/label&amp;gt;\n&amp;lt;input type=&amp;quot;text&amp;quot; @bind=&amp;quot;_sepalLength&amp;quot;&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;label&amp;gt;Sepal Width: &amp;lt;/label&amp;gt;\n&amp;lt;input type=&amp;quot;text&amp;quot; @bind=&amp;quot;_sepalWidth&amp;quot;&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;label&amp;gt;Petal Length: &amp;lt;/label&amp;gt;\n&amp;lt;input type=&amp;quot;text&amp;quot; @bind=&amp;quot;_petalLength&amp;quot;&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;label&amp;gt;Petal Width: &amp;lt;/label&amp;gt;\n&amp;lt;input type=&amp;quot;text&amp;quot; @bind=&amp;quot;_petalWidth&amp;quot;&amp;gt;&amp;lt;br&amp;gt;\n&amp;lt;button @onclick=&amp;quot;GetPrediction&amp;quot;&amp;gt;Make prediction&amp;lt;/button&amp;gt;\n@if(@ModelPrediction == null)\n{\n    &amp;lt;p&amp;gt;Enter data to get a prediction&amp;lt;/p&amp;gt;\n} else\n{\n    &amp;lt;p&amp;gt;@ModelPrediction&amp;lt;/p&amp;gt;\n}\n\n\n@code {\n    private PredictionEngine&amp;lt;ModelInput,ModelOutput&amp;gt; _predictionEngine;\n    private string _sepalLength, _sepalWidth, _petalLength, _petalWidth, ModelPrediction;\n\n    protected override async Task OnInitializedAsync()\n    {\n        Stream savedModel = await _client.GetStreamAsync(&amp;quot;&amp;lt;YOUR-MODEL-ENDPOINT&amp;gt;&amp;quot;);\n        MLContext mlContext = new MLContext();\n        ITransformer _model = mlContext.Model.Load(savedModel,out DataViewSchema schema);\n        _predictionEngine = mlContext.Model.CreatePredictionEngine&amp;lt;ModelInput,ModelOutput&amp;gt;(_model);\n    }\n\n    private void GetPrediction()\n    {\n        ModelInput input = new ModelInput\n        {\n            SepalLength=float.Parse(_sepalLength),\n            SepalWidth=float.Parse(_sepalWidth),\n            PetalLength=float.Parse(_petalLength),\n            PetalWidth=float.Parse(_petalWidth)\n        };\n\n        ModelOutput prediction = _predictionEngine.Predict(input);\n\n        ModelPrediction = prediction.PredictedLabel;\n    }\n}\n</code></pre>\n<p>The <em>Predict.razor</em> page contains a text input element for each of the columns the model was original trained on. When the page is initialized, the model is loaded from Azure Storage and a <code>PredictionEngine</code> is created. <strong>Make sure to replace <code>&amp;lt;YOUR-MODEL-ENDPOINT&amp;gt;</code> with the URL of the blob that contains your <code>model.zip</code></strong>. A <code>PredictionEngine</code> is a convenience API to make a single prediction. Traditionally when models are served as a web service, it is recommended to use the <code>PredictionEnginePool</code> service because it's thread-safe and more performant in multi-threaded application. However, in this case since the model is downloaded onto the individual user's browser, it's okay to use <code>PredictionEngine</code>. After a user enters input values and clicks the &amp;quot;Make Prediction&amp;quot; button, the <code>GetPrediction</code> method executes by taking the user input and making a prediction using the <code>PredictionEngine</code>. The prediction is then displayed in the browser.</p>\n<h3>Add to navigation menu</h3>\n<p>In the <em>BlazorWebApp</em> project, open the <em>NavMenu.razor</em> file in the <em>Shared</em> directory.</p>\n<p>Add the following list item to the <code>&amp;lt;ul&amp;gt;</code> element.</p>\n<pre><code class=\"language-html\">&amp;lt;li class=&amp;quot;nav-item px-3&amp;quot;&amp;gt;\n    &amp;lt;NavLink class=&amp;quot;nav-link&amp;quot; href=&amp;quot;prediction&amp;quot;&amp;gt;\n        &amp;lt;span class=&amp;quot;oi oi-list-rich&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/span&amp;gt; Prediction\n    &amp;lt;/NavLink&amp;gt;\n&amp;lt;/li&amp;gt;\n</code></pre>\n<p>The final <em>NavMenu.razor</em> page should look like the following:</p>\n<pre><code class=\"language-razor\">&amp;lt;div class=&amp;quot;top-row pl-4 navbar navbar-dark&amp;quot;&amp;gt;\n    &amp;lt;a class=&amp;quot;navbar-brand&amp;quot; href=&amp;quot;&amp;quot;&amp;gt;BlazorWebApp&amp;lt;/a&amp;gt;\n    &amp;lt;button class=&amp;quot;navbar-toggler&amp;quot; @onclick=&amp;quot;ToggleNavMenu&amp;quot;&amp;gt;\n        &amp;lt;span class=&amp;quot;navbar-toggler-icon&amp;quot;&amp;gt;&amp;lt;/span&amp;gt;\n    &amp;lt;/button&amp;gt;\n&amp;lt;/div&amp;gt;\n\n&amp;lt;div class=&amp;quot;@NavMenuCssClass&amp;quot; @onclick=&amp;quot;ToggleNavMenu&amp;quot;&amp;gt;\n    &amp;lt;ul class=&amp;quot;nav flex-column&amp;quot;&amp;gt;\n        &amp;lt;li class=&amp;quot;nav-item px-3&amp;quot;&amp;gt;\n            &amp;lt;NavLink class=&amp;quot;nav-link&amp;quot; href=&amp;quot;&amp;quot; Match=&amp;quot;NavLinkMatch.All&amp;quot;&amp;gt;\n                &amp;lt;span class=&amp;quot;oi oi-home&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/span&amp;gt; Home\n            &amp;lt;/NavLink&amp;gt;\n        &amp;lt;/li&amp;gt;\n        &amp;lt;li class=&amp;quot;nav-item px-3&amp;quot;&amp;gt;\n            &amp;lt;NavLink class=&amp;quot;nav-link&amp;quot; href=&amp;quot;counter&amp;quot;&amp;gt;\n                &amp;lt;span class=&amp;quot;oi oi-plus&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/span&amp;gt; Counter\n            &amp;lt;/NavLink&amp;gt;\n        &amp;lt;/li&amp;gt;\n        &amp;lt;li class=&amp;quot;nav-item px-3&amp;quot;&amp;gt;\n            &amp;lt;NavLink class=&amp;quot;nav-link&amp;quot; href=&amp;quot;fetchdata&amp;quot;&amp;gt;\n                &amp;lt;span class=&amp;quot;oi oi-list-rich&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/span&amp;gt; Fetch data\n            &amp;lt;/NavLink&amp;gt;\n        &amp;lt;/li&amp;gt;\n        &amp;lt;li class=&amp;quot;nav-item px-3&amp;quot;&amp;gt;\n            &amp;lt;NavLink class=&amp;quot;nav-link&amp;quot; href=&amp;quot;prediction&amp;quot;&amp;gt;\n                &amp;lt;span class=&amp;quot;oi oi-list-rich&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;/span&amp;gt; Prediction\n            &amp;lt;/NavLink&amp;gt;\n        &amp;lt;/li&amp;gt;\n    &amp;lt;/ul&amp;gt;\n&amp;lt;/div&amp;gt;\n\n@code {\n    private bool collapseNavMenu = true;\n\n    private string NavMenuCssClass =&amp;gt; collapseNavMenu ? &amp;quot;collapse&amp;quot; : null;\n\n    private void ToggleNavMenu()\n    {\n        collapseNavMenu = !collapseNavMenu;\n    }\n}\n</code></pre>\n\n<p>The web application will he hosted as a static site on Azure Storage.</p>\n<p>In the Azure Portal, navigate to the storage account resource where you are hosting your model.</p>\n<h3>Enable static website</h3>\n<p>Enable a static website for the storage account and set the index document name and error document path to <em>index.html</em>.</p>\n<p><img src=\"/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly4.png\" class=\"img-fluid\" alt=\"Enable Static Website\" /></p>\n<p>At this point, a new container called <em>$web</em> is created in your storage account. This is where all your site's static files will reside. Additionally, a primary endpoint is created. This is the URL you will use to access your application</p>\n<h3>Configure CORS</h3>\n<p>The storage account has some default CORS settings. In order to download and use your model from your application, you'll have to configure them.</p>\n<p><img src=\"/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly5.png\" class=\"img-fluid\" alt=\"Configure CORS\" /></p>\n<p>For &amp;quot;Allowed origins&amp;quot;, enter your primary endpoint.</p>\n\n<p>To publish your application, run the following command:</p>\n<pre><code class=\"language-powershell\">dotnet publish -c Release\n</code></pre>\n<p>This generates all the files you'll need to host your web application as a static site in the <em>bin/Release/netstandard2.1/publish/BlazorWebApp/dist</em> directory of your <em>BlazorWebApp</em> project.</p>\n<p>To deploy your application, use Azure Storage Explorer to copy all of the files in the <em>dist</em> directory into the <em>$web</em> container of your Azure Storage Account.</p>\n<p><img src=\"/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly6.png\" class=\"img-fluid\" alt=\"Copy files into web container\" /></p>\n\n<p>In your browser, navigate to your static website's primary endpoint and select the Prediction page. Input data and click &amp;quot;Make prediction&amp;quot;. The page should look as follows.</p>\n<p><img src=\"/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly7.png\" class=\"img-fluid\" alt=\"Test the application\" /></p>\n<p><strong>You may note that performance of Naive Bayes on this dataset is not the best so some of the predictions may not be as accurate. I am okay with this at the moment because this is a proof-of-concept to show how these technologies might work together. Perhaps using a better dataset may yield better results.</strong></p>\n\n<p>In this post, I went over how to deploy an ML.NET multiclass classification model alongside a Blazor WebAssembly static website to Azure Storage. While more limited than other methods of deployment due to the early stages of WebAssembly and Blazor WebAssembly, this shows the possibilities for the technologies. Deploying in this manner reduces the amount of resources required to deploy these models and shifts processing from a server or web service to the client's browser making deployment and distribution of machine learning models more efficient, scalable and cost-efficient.</p>\n","tags":["azure","staticwebsites","machinelearning","ai","artifificalintelligence","ml","dotnet","dotnetcore","webassembly","webdevelopment","mldotnet","mlnet","blazor"]},{"title":"Use machine learning to categorize web links with F# and ML.NET","contentType":"posts","date":"2019-12-17 19:59:06","url":"/posts/categorize-web-links-ml-net-fsharp-fsadvent2019/","content":"\n<p>This post is part of <a href=\"https://sergeytihon.com/2019/11/05/f-advent-calendar-in-english-2019/\">F# Advent 2019</a>. Thank you <a href=\"https://twitter.com/sergey_tihon\">Sergey Tihon</a> for organizing this and the rest of the contributors for producing interesting, high-quality content.</p>\n<p>I scan and read articles on a constant basis, such as those published as part of F# Advent. Those that I find interesting, or I want to save for later, I bookmark using <a href=\"https://getpocket.com/\">Pocket</a>. One of the neat features it provides is tagging. You can add as many tags as you want to organize the bookmarked links. When I first started using the service, I was fairly good at adding tags. However, I've gotten lazy and don't do it as much. It would be nice if bookmarked links could automatically be categorized for me without having to provide the tags manually. Using machine learning, this task can be automated. In this writeup, I will show how to build a machine learning model using <a href=\"https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet\">ML.NET</a>, a .NET, open-source, cross-platform machine learning framework to automatically categorize web links / articles.</p>\n\n<p>This application was built on a Windows 10 PC, but should work cross-platform.</p>\n<ul>\n<li><a href=\"https://dotnet.microsoft.com/download\">.NET Core SDK 2.1+</a></li>\n<li><a href=\"https://code.visualstudio.com/Download\">Visual Studio Code</a></li>\n<li><a href=\"http://ionide.io/\">Ionide</a></li>\n<li><a href=\"https://www.nuget.org/packages/Microsoft.ML/\">Microsoft.ML NuGet package</a></li>\n</ul>\n\n<p>Make a new directory and create a solution by using the .NET CLI.</p>\n<pre><code class=\"language-powershell\">mkdir FsAdvent2019\ncd FsAdvent2019\ndotnet new sln\n</code></pre>\n<p>Then, create an F# Console application.</p>\n<pre><code class=\"language-powershell\">dotnet new console -o FsAdvent2019 -lang f#\n</code></pre>\n<p>Navigate to the console application directory and install the Microsoft.ML NuGet package.</p>\n<pre><code class=\"language-powershell\">cd FsAdvent2019\ndotnet add package Microsoft.ML -v 1.4.0\n</code></pre>\n\n<p><a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\">Click on this link to download and unzip the data anywhere on your PC</a>.</p>\n<p>The data contains information about several articles that are separated into four categories: business (b), science and technology (t), entertainment (e) and health (h). Visit the <a href=\"https://archive.ics.uci.edu/ml/datasets/News+Aggregator\">UCI Machine Learning repository website</a> to learn more about the dataset.</p>\n<p>Below is a sample of the data.</p>\n<pre><code class=\"language-text\">ID    Title    Url    Publisher    Category    Story    Hostname    Timestamp\n2\tFed's Charles Plosser sees high bar for change in pace of tapering\thttp://www.livemint.com/Politics/H2EvwJSK2VE6OF7iK1g3PP/Feds-Charles-Plosser-sees-high-bar-for-change-in-pace-of-ta.html\tLivemint\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.livemint.com\t1394470371207\n3\tUS open: Stocks fall after Fed official hints at accelerated tapering\thttp://www.ifamagazine.com/news/us-open-stocks-fall-after-fed-official-hints-at-accelerated-tapering-294436\tIFA Magazine\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.ifamagazine.com\t1394470371550\n4\tFed risks falling 'behind the curve', Charles Plosser says\thttp://www.ifamagazine.com/news/fed-risks-falling-behind-the-curve-charles-plosser-says-294430\tIFA Magazine\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.ifamagazine.com\t1394470371793\n</code></pre>\n<p>Inside the console application directory, create a new directory called <em>data</em> and copy the <em>newsCorpora.csv</em> file to it.</p>\n<pre><code class=\"language-powershell\">mkdir data\n</code></pre>\n\n<p>Open the <em>Program.fs</em> file and add the following <code>open</code> statements at the top.</p>\n<pre><code class=\"language-fsharp\">open Microsoft.ML\nopen Microsoft.ML.Data\n</code></pre>\n<p>Directly below the <code>open</code> statements, define the data schema of the input and output of the machine learning model as records called <code>ModelInput</code> and <code>ModelOutput</code> respectively.</p>\n<pre><code class=\"language-fsharp\">[&amp;lt;CLIMutable&amp;gt;]\ntype ModelInput = {\n    [&amp;lt;LoadColumn(1)&amp;gt;]\n    Title:string\n    [&amp;lt;LoadColumn(2)&amp;gt;]\n    Url:string\n    [&amp;lt;LoadColumn(3)&amp;gt;]\n    Publisher:string\n    [&amp;lt;LoadColumn(4)&amp;gt;]\n    Category:string\n    [&amp;lt;LoadColumn(6)&amp;gt;]\n    Hostname:string\n}\n\n[&amp;lt;CLIMutable&amp;gt;]\ntype ModelOutput = {\n    PredictedLabel: string\n}\n</code></pre>\n<p>As input, only the <strong>Title</strong>, <strong>Url</strong>, <strong>Publisher</strong> and <strong>Hostname</strong> columns are used to train the machine learning model and make predictions. The label or value to predict in this case is the <strong>Category</strong>. When a prediction is output by the model, its value is stored in a column called <strong>PredictedLabel</strong>.</p>\n\n<p>The <code>MLContext</code> is the entry point of all ML.NET applications which binds all tasks like data loading, data transformations, model training, model evaluation, and model saving/loading.</p>\n<p>Inside of the <code>main</code> function, create an instance of <code>MLContext</code>.</p>\n<pre><code class=\"language-fsharp\">let mlContext = MLContext()\n</code></pre>\n\n<p>Once the <code>MLContext</code> is initialized, use the <code>LoadFromTextFile</code> function and provide the path to the file containing the data.</p>\n<pre><code class=\"language-fsharp\">let data = mlContext.Data.LoadFromTextFile&amp;lt;ModelInput&amp;gt;(&amp;quot;data/newsCorpora.csv&amp;quot;)\n</code></pre>\n\n<p>It's often good practice to split the data into train and test sets. The goal of a machine learning model is to accurately make predictions on data it has not seen before. Therefore, making predictions using inputs that are the same as those it was trained on may provide misleading accuracy metrics.</p>\n<p>Use the <code>TrainTestSplit</code> to split the data into train / test sets with 90% of the data used for training and 10% used for testing.</p>\n<pre><code class=\"language-fsharp\">let datasets = mlContext.Data.TrainTestSplit(data,testFraction=0.1)\n</code></pre>\n\n<p>Now that the data is split, define the set of transformations to be applied to the data. The purpose of transforming the data is to convert it into numbers which are more easily processed by machine learning algorithms.</p>\n<h3>Preprocessing pipeline</h3>\n<p>The preprocessing pipeline contains the series of transformations that take place before training the model. To create a pipeline, initialize an <code>EstimatorChain</code> and append the desired transformations to it.</p>\n<pre><code class=\"language-fsharp\">let preProcessingPipeline = \n    EstimatorChain()\n        .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedTitle&amp;quot;,&amp;quot;Title&amp;quot;))\n        .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedUrl&amp;quot;,&amp;quot;Url&amp;quot;))\n        .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedPublisher&amp;quot;,&amp;quot;Publisher&amp;quot;))\n        .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedHost&amp;quot;,&amp;quot;Hostname&amp;quot;))\n        .Append(mlContext.Transforms.Concatenate(&amp;quot;Features&amp;quot;,[|&amp;quot;FeaturizedTitle&amp;quot;; &amp;quot;FeaturizedUrl&amp;quot; ;&amp;quot;FeaturizedPublisher&amp;quot;; &amp;quot;FeaturizedHost&amp;quot;|]))\n        .Append(mlContext.Transforms.Conversion.MapValueToKey(&amp;quot;Label&amp;quot;,&amp;quot;Category&amp;quot;))\n</code></pre>\n<p>In this preprocessing pipeline, the following transformations are taking place:</p>\n<ol>\n<li>Convert the <em>Title</em>, <em>Url</em>, <em>Publisher</em> and <em>Hostname</em> columns into numbers and store the transformed value into the <em>FeaturizedTitle</em>, <em>FeaturizedUrl</em>, <em>FeaturizedPublisher</em> and <em>FeaturizedHost</em> columns respectively.</li>\n<li>Combine the <em>FeaturizedTitle</em>, <em>FeaturizedUrl</em>, <em>FeaturizedPublisher</em> and <em>FeaturizedHost</em> into one column called <em>Features</em>.</li>\n<li>Create a mapping of the text value contained in the <em>Category</em> column to a numerical key and store the result into a new column called <em>Label</em>.</li>\n</ol>\n<h3>Algorithm pipeline</h3>\n<p>The algorithm pipeline contains the algorithm used to train the machine learning model. In this application, the multiclass classification algorithm used is <code>LbfgsMaximumEntropy</code>. To learn more about the algorithm, see the <a href=\"https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.trainers.lbfgsmaximumentropymulticlasstrainer?view=ml-dotnet#training-algorithm-details\">ML.NET LbfgsMaximumEntropy multiclass trainer API documentation</a>.</p>\n<pre><code class=\"language-fsharp\">let algorithm = \n    mlContext.MulticlassClassification.Trainers.LbfgsMaximumEntropy()\n</code></pre>\n<h3>Postprocessing pipeline</h3>\n<p>The postprocessing pipeline contains the series of transformations to get the output of training into a more readable format. The only transformation performed in this pipeline is mapping back the numerical value mapping of the predicted value into text form.</p>\n<pre><code class=\"language-fsharp\">let postProcessingPipeline = \n    mlContext.Transforms.Conversion.MapKeyToValue(&amp;quot;PredictedLabel&amp;quot;)\n</code></pre>\n<h3>Create training pipeline</h3>\n<p>Once the pipelines are defined, combine them into a single pipeline which applies all of the transformations to the data with a single function call.</p>\n<pre><code class=\"language-fsharp\">let trainingPipeline = \n    preProcessingPipeline\n        .Append(algorithm)\n        .Append(postProcessingPipeline)\n</code></pre>\n\n<p>Use the <code>Fit</code> function to train the model by applying the set of transformations defined by <code>trainingPipeline</code> to the training dataset.</p>\n<pre><code class=\"language-fsharp\">let model =\n    datasets.TrainSet |&amp;gt; trainingPipeline.Fit\n</code></pre>\n\n<p>Once the model is trained, evaluate how well it performs against the test dataset. First, use the trained model to get the predicted category by using the <code>Transform</code> function. Then, provide the test dataset containing predictions to the <code>Evaluate</code> function which calculates the model's performance metrics by comparing the predicted category to the actual category and print some of them out.</p>\n<pre><code class=\"language-fsharp\">let metrics = \n    (datasets.TestSet |&amp;gt; model.Transform)\n    |&amp;gt; mlContext.MulticlassClassification.Evaluate\n\nprintfn &amp;quot;Log Loss: %f | MacroAccuracy: %f&amp;quot; metrics.LogLoss metrics.MacroAccuracy\n</code></pre>\n\n<p>Create a list of <code>ModelInput</code> items and use the <code>Transform</code> method to get the predicted category.</p>\n<pre><code class=\"language-fsharp\">let predictions = \n    [\n        { \n            Title=&amp;quot;A FIRST LOOK AT SURFACE DUO, MICROSOFT’S FOLDABLE ANDROID PHONE&amp;quot;\n            Url=&amp;quot;https://www.theverge.com/2019/10/3/20895268/microsoft-surface-duo-foldable-phone-dual-screen-android-hands-on-features-price-photos-video&amp;quot;\n            Publisher=&amp;quot;The Verge&amp;quot;\n            Hostname=&amp;quot;www.theverge.com&amp;quot; \n            Category = &amp;quot;&amp;quot; \n        }\n        { \n            Title=&amp;quot;This Shrinking Economy With Low Inflation Is Stuck on Rates&amp;quot;\n            Url=&amp;quot;https://www.bloomberg.com/news/articles/2019-12-12/when-a-shrinking-economy-and-low-inflation-don-t-mean-rate-cuts?srnd=economics-vp&amp;quot;\n            Publisher=&amp;quot;Bloomberg&amp;quot;\n            Hostname=&amp;quot;www.bloomberg.com&amp;quot; \n            Category = &amp;quot;&amp;quot; \n        }\n    ] \n    |&amp;gt; mlContext.Data.LoadFromEnumerable\n    |&amp;gt; model.Transform\n</code></pre>\n<p>Then, create a <code>Sequence</code> of <code>ModelOutput</code> values and print out the <em>PredictedLabel</em> values.</p>\n<pre><code class=\"language-fsharp\">mlContext.Data.CreateEnumerable&amp;lt;ModelOutput&amp;gt;(predictions,false)\n|&amp;gt; Seq.iter(fun prediction -&amp;gt; printfn &amp;quot;Predicted Value: %s&amp;quot; prediction.PredictedLabel)\n</code></pre>\n<p>The final <em>Program.fs</em> file should look as follows:</p>\n<pre><code class=\"language-fsharp\">open System\nopen Microsoft.ML\nopen Microsoft.ML.Data\n\n[&amp;lt;CLIMutable&amp;gt;]\ntype ModelInput = {\n    [&amp;lt;LoadColumn(1)&amp;gt;]\n    Title:string\n    [&amp;lt;LoadColumn(2)&amp;gt;]\n    Url:string\n    [&amp;lt;LoadColumn(3)&amp;gt;]\n    Publisher:string\n    [&amp;lt;LoadColumn(4)&amp;gt;]\n    Category:string\n    [&amp;lt;LoadColumn(6)&amp;gt;]\n    Hostname:string\n}\n\n[&amp;lt;CLIMutable&amp;gt;]\ntype ModelOutput = {\n    PredictedLabel: string\n}\n\n[&amp;lt;EntryPoint&amp;gt;]\nlet main argv =\n\n    let mlContext = MLContext()\n\n    let data = mlContext.Data.LoadFromTextFile&amp;lt;ModelInput&amp;gt;(&amp;quot;data/newsCorpora.csv&amp;quot;)\n\n    let datasets = mlContext.Data.TrainTestSplit(data,testFraction=0.1)\n\n    let preProcessingPipeline = \n        EstimatorChain()\n            .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedTitle&amp;quot;,&amp;quot;Title&amp;quot;))\n            .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedUrl&amp;quot;,&amp;quot;Url&amp;quot;))\n            .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedPublisher&amp;quot;,&amp;quot;Publisher&amp;quot;))\n            .Append(mlContext.Transforms.Text.FeaturizeText(&amp;quot;FeaturizedHost&amp;quot;,&amp;quot;Hostname&amp;quot;))\n            .Append(mlContext.Transforms.Concatenate(&amp;quot;Features&amp;quot;,[|&amp;quot;FeaturizedTitle&amp;quot;; &amp;quot;FeaturizedUrl&amp;quot; ;&amp;quot;FeaturizedPublisher&amp;quot;; &amp;quot;FeaturizedHost&amp;quot;|]))\n            .Append(mlContext.Transforms.Conversion.MapValueToKey(&amp;quot;Label&amp;quot;,&amp;quot;Category&amp;quot;))\n\n    let algorithm = \n        mlContext.MulticlassClassification.Trainers.LbfgsMaximumEntropy()\n\n    let postProcessingPipeline = \n        mlContext.Transforms.Conversion.MapKeyToValue(&amp;quot;PredictedLabel&amp;quot;)\n\n    let trainingPipeline = \n        preProcessingPipeline\n            .Append(algorithm)\n            .Append(postProcessingPipeline)\n\n    let model =\n        datasets.TrainSet |&amp;gt; trainingPipeline.Fit\n\n    let metrics = \n        (datasets.TestSet |&amp;gt; model.Transform)\n        |&amp;gt; mlContext.MulticlassClassification.Evaluate\n\n    printfn &amp;quot;Log Loss: %f | MacroAccuracy: %f&amp;quot; metrics.LogLoss metrics.MacroAccuracy\n\n    let predictions = \n        [\n            { \n                Title=&amp;quot;A FIRST LOOK AT SURFACE DUO, MICROSOFT’S FOLDABLE ANDROID PHONE&amp;quot;\n                Url=&amp;quot;https://www.theverge.com/2019/10/3/20895268/microsoft-surface-duo-foldable-phone-dual-screen-android-hands-on-features-price-photos-video&amp;quot;\n                Publisher=&amp;quot;The Verge&amp;quot;\n                Hostname=&amp;quot;www.theverge.com&amp;quot; \n                Category = &amp;quot;&amp;quot; \n            }\n            { \n                Title=&amp;quot;This Shrinking Economy With Low Inflation Is Stuck on Rates&amp;quot;\n                Url=&amp;quot;https://www.bloomberg.com/news/articles/2019-12-12/when-a-shrinking-economy-and-low-inflation-don-t-mean-rate-cuts?srnd=economics-vp&amp;quot;\n                Publisher=&amp;quot;Bloomberg&amp;quot;\n                Hostname=&amp;quot;www.bloomberg.com&amp;quot; \n                Category = &amp;quot;&amp;quot; \n            }\n        ] \n        |&amp;gt; mlContext.Data.LoadFromEnumerable\n        |&amp;gt; model.Transform\n \n    mlContext.Data.CreateEnumerable&amp;lt;ModelOutput&amp;gt;(predictions,false)\n    |&amp;gt; Seq.iter(fun prediction -&amp;gt; printfn &amp;quot;Predicted Value: %s&amp;quot; prediction.PredictedLabel)\n\n    0 // return an integer exit code\n</code></pre>\n\n<p>This particular model achieved a macro-accuracy of 0.92, where closer to 1 is preferred and log loss of 0.20 where closer to 0 is preferred.</p>\n<pre><code class=\"language-bash\">Log Loss: 0.200502 | MacroAccuracy: 0.927742\n</code></pre>\n<p>The predicted values are the following:</p>\n<pre><code class=\"language-bash\">Predicted Value: t\nPredicted Value: b\n</code></pre>\n<p>Upon inspection, they appear to be correct, science and technology for the first link and business for the second link.</p>\n\n<p>In this writeup, I showed how to build a machine learning multiclass classification model that categorizes web links using ML.NET. Now that you have a model trained, you can save it and deploy it in another application (desktop, web) that bookmarks links. This model can be further improved and personalized by using data from Pocket which has already been tagged. Happy coding!</p>\n","tags":["dotnet","dotnet-core","machine-learning","ai","artificial-intelligence","ml","fsharp","functional-programming"]},{"title":"Create an HTTP Trigger Azure Function using FSharp","contentType":"posts","date":"2019-11-16 17:33:59","url":"/posts/http-trigger-azure-functions-fsharp/","content":"\n<p>Since the upgrade to the 2.0 version of the Azure Functions runtime, .NET Core has been natively supported by the platform. As a result some changes took effect. Most notably, in version 1.0, a template for an F# HttpTrigger function was available. The template was removed in 2.0. However, that does not mean Azure Functions does not support F#. Azure Functions can be built in F# using a .NET Standard Class Library. This writeup provides a detailed walk-through of how to build an Azure Function that processes HTTP requests using F#. The complete code sample can be found on <a href=\"https://github.com/lqdev/FsHttpTriggerSample\">GitHub</a>.</p>\n\n<p>This solution was built using a Windows PC but should work on Mac and Linux.</p>\n<ul>\n<li><a href=\"https://dotnet.microsoft.com/download/dotnet-core\">.NET SDK (2.x or 3.x)</a></li>\n<li><a href=\"https://nodejs.org/en/download/\">Node.js</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local#windows-npm\">Azure Functions Core Tools</a></li>\n</ul>\n\n<p>Open the command prompt and create a new directory for your solution called &amp;quot;FsHttpTriggerSample&amp;quot;.</p>\n<pre><code class=\"language-bash\">mkdir FsHttpTriggerSample\n</code></pre>\n<p>Navigate into the new directory and create a solution using the .NET CLI.</p>\n<pre><code class=\"language-bash\">cd FsHttpTriggerSample\ndotnet new sln\n</code></pre>\n\n<p>Inside the <em>FsHttpTriggerSample</em> solution directory, use the .NET CLI to create a new F# .NET Standard Class Library project.</p>\n<pre><code class=\"language-bash\">dotnet new classlib -o FsHttpTriggerSample -lang f#\n</code></pre>\n<p>Add the project to the solution</p>\n<pre><code class=\"language-bash\">dotnet sln add FsHttpTriggerSample\n</code></pre>\n\n<p>To use Azure Functions, install the <a href=\"https://www.nuget.org/packages/Microsoft.NET.Sdk.Functions\"><strong>Microsoft.Net.Sdk.Functions</strong> NuGet package</a>.</p>\n<p>Inside the <em>FsHttpTriggerSample</em> project directory, enter the following command.</p>\n<pre><code class=\"language-bash\">dotnet add package Microsoft.Net.Sdk.Functions\n</code></pre>\n\n<h3>Prepare Files</h3>\n<p>Delete the default <em>Library.fs</em> file inside the <em>FsHttpTriggerSample</em> project directory.</p>\n<pre><code class=\"language-bash\">del Library.fs\n</code></pre>\n<p>Create a new file called <em>GreetFunction.fs</em>.</p>\n<pre><code class=\"language-bash\">type nul &amp;gt; GreetFunction.fs\n</code></pre>\n<h3>Configure Files</h3>\n<p>Open the <em>FsHttpTriggerSample.fsproj</em> file and find the following snippet.</p>\n<pre><code class=\"language-xml\">&amp;lt;Compile Include=&amp;quot;Library.fs&amp;quot; /&amp;gt;\n</code></pre>\n<p>Replace the snippet with the content below.</p>\n<pre><code class=\"language-xml\">&amp;lt;Compile Include=&amp;quot;GreetFunction.fs&amp;quot; /&amp;gt;\n</code></pre>\n<h3>Configure Host</h3>\n<p>At a minimum, Azure Functions requires the runtime version to run. This information is provided by a file called <em>host.json</em>.</p>\n<p>Create a new file called <em>host.json</em> inside the <em>FsHttpTriggerSample</em> project directory.</p>\n<pre><code class=\"language-bash\">type nul &amp;gt; host.json\n</code></pre>\n<p>Open the <em>host.json</em> file and add the following content</p>\n<pre><code class=\"language-json\">{\n    &amp;quot;version&amp;quot;: &amp;quot;2.0&amp;quot;\n}\n</code></pre>\n<h3>Implement Azure Function</h3>\n<p>Open the <em>GreetFunction.fs</em> file and add the namespace and module for it.</p>\n<pre><code class=\"language-fsharp\">namespace FsHttpTriggerSample\n\nmodule GreetFunction = \n</code></pre>\n<p>Below the module definition, add the following <code>open</code> statements:</p>\n<pre><code class=\"language-fsharp\">open Microsoft.AspNetCore.Mvc\nopen Microsoft.Azure.WebJobs\nopen Microsoft.AspNetCore.Http\nopen Newtonsoft.Json\nopen System.IO\nopen Microsoft.Extensions.Logging\n</code></pre>\n<p>Define a <code>User</code> type containing a single property called <code>Name</code>.</p>\n<pre><code class=\"language-fsharp\">type User = {\n    Name: string\n}\n</code></pre>\n<p>The entrypoint of an Azure Function is the <code>Run</code> function. Create a function called <code>Run</code>.</p>\n<pre><code class=\"language-fsharp\">[&amp;lt;FunctionName(&amp;quot;Greet&amp;quot;)&amp;gt;]\nlet Run ([&amp;lt;HttpTrigger(Methods=[|&amp;quot;POST&amp;quot;|])&amp;gt;] req:HttpRequest) (log:ILogger) = \n</code></pre>\n<p>To register an Azure Function, use the <code>FunctionName</code> attribute. In this case, the name of the function is <code>Greet</code>. The <code>Run</code> function takes two parameters, an <code>HttpRequest</code> and an <code>ILogger</code>. Since the binding used by HTTP Trigger functions is <code>HttpTrigger</code>, the request object is annotated with the <code>HttpTrigger</code> attribute. Options such as the accepted methods can be provided through the <code>HttpTrigger</code> attribute. In this case, only <code>POST</code> requests are accepted.</p>\n<p>Create an <code>async</code> computation expression inside the <code>Run</code> function.</p>\n<pre><code class=\"language-fsharp\">async {\n\n}\n</code></pre>\n<p>Inside the <code>async</code> expression, add logging to indicate that the function has initialized.</p>\n<pre><code class=\"language-fsharp\">&amp;quot;Running function&amp;quot;\n|&amp;gt; log.LogInformation\n</code></pre>\n<p>Below that, get the body of the request.</p>\n<pre><code class=\"language-fsharp\">let! body = \n    new StreamReader(req.Body) \n    |&amp;gt; (fun stream -&amp;gt; stream.ReadToEndAsync()) \n    |&amp;gt; Async.AwaitTask\n</code></pre>\n<p>Then, deserialized the body into an instance of <code>User</code>.</p>\n<pre><code class=\"language-fsharp\">let user = JsonConvert.DeserializeObject&amp;lt;User&amp;gt;(body)\n</code></pre>\n<p>Return a personalized greeting with the user's name.</p>\n<pre><code class=\"language-fsharp\">return OkObjectResult(sprintf &amp;quot;Hello %s&amp;quot; user.Name)\n</code></pre>\n<p>Finally, use the <code>StartAsTask</code> function to start the <code>async</code> expression as a <code>Task</code>.</p>\n<pre><code class=\"language-fsharp\">|&amp;gt; Async.StartAsTask\n</code></pre>\n<p>Once finished, the contents of the <em>GreetFunction.fs</em> should look similar to the following.</p>\n<pre><code class=\"language-fsharp\">namespace FsHttpTriggerSample\n\nmodule GreetFunction = \n\n    open Microsoft.AspNetCore.Mvc\n    open Microsoft.Azure.WebJobs\n    open Microsoft.AspNetCore.Http\n    open Newtonsoft.Json\n    open System.IO\n    open Microsoft.Extensions.Logging\n\n    type User = {\n        Name: string\n    }\n\n    [&amp;lt;FunctionName(&amp;quot;Greet&amp;quot;)&amp;gt;]\n    let Run ([&amp;lt;HttpTrigger(Methods=[|&amp;quot;POST&amp;quot;|])&amp;gt;] req:HttpRequest) (log:ILogger) = \n        async {\n            &amp;quot;Runnning Function&amp;quot;\n            |&amp;gt; log.LogInformation\n\n            let! body = \n                new StreamReader(req.Body) \n                |&amp;gt; (fun stream -&amp;gt; stream.ReadToEndAsync()) \n                |&amp;gt; Async.AwaitTask\n\n            let user = JsonConvert.DeserializeObject&amp;lt;User&amp;gt;(body)\n\n            return OkObjectResult(sprintf &amp;quot;Hello %s&amp;quot; user.Name)\n        } |&amp;gt; Async.StartAsTask\n</code></pre>\n\n<p>Build the project by using the <code>build</code> command inside the <em>FsHttpTriggerSample</em> project directory.</p>\n<pre><code class=\"language-bash\">dotnet build\n</code></pre>\n<p>Then, navigate to the output directory</p>\n<pre><code class=\"language-bash\">cd bin\\Debug\\netstandard2.0\n</code></pre>\n<p>Use the Azure Functions Core Tools to start the Azure Functions host locally.</p>\n<pre><code class=\"language-bash\">func host start\n</code></pre>\n<p>Once the host is initialized, the function is available at the following endpoint <code>http://localhost:7071/api/Greet</code>.</p>\n\n<p>Using a REST client like Postman or Insomnia, make a POST request to <code>http://localhost:7071/api/Greet</code> with the following body. Feel free to replace the name with your own.</p>\n<pre><code class=\"language-json\">{\n    &amp;quot;Name&amp;quot;: &amp;quot;Luis&amp;quot;\n}\n</code></pre>\n<p>If successful, the response should look similar to the following output.</p>\n<pre><code class=\"language-text\">Hello Luis\n</code></pre>\n\n<p>This writeup showed how to create an HTTP Trigger Azure Function using F#. Creating additional functions inside the same project is relatively trivial since the structure of the <em>GreetFunction.fs</em> file can be copied and the logic inside the <code>Run</code> function can be adapted to meet your requirements.</p>\n\n<ul>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/azure-functions/functions-reference-fsharp\">Azure Functions F# Developer Reference</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/azure-functions/functions-host-json\">Azure Functions Host 2.x Reference</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/azure-functions/deployment-zip-push\">Azure Functions Zip Deployment</a></li>\n</ul>\n","tags":["fsharp","serverless","azure-functions","azure","dotnet","dotnet-core","functional-programming"]},{"title":"Restaurant Inspections ETL & Data Enrichment with Spark.NET and ML.NET Automated (Auto) ML","contentType":"posts","date":"2019-09-15 15:12:01","url":"/posts/restaurant-inspections-etl-data-enrichment-spark-auto-ml-net/","content":"\n<p>Apache Spark is an open-source, distributed, general-purpose analytics engine. For years, it has been a staple in the Big Data ecosystem for batch and real-time processing on large datasets. Although native support for the platform is limited to the JVM set of languages, other languages typically used for data processing and analytics like Python and R have plugged into Spark's Interop layer to make use of its functionality. Around the Build 2019 conference, Microsoft announced Spark.NET. Spark.NET provides bindings written for the Spark Interop layer that allow you to work with components like Spark SQL and Spark Streaming inside your .NET applications. Because Spark.NET is .NET Standard 2.0 compliant, it can run operating systems like Windows, Mac and Linux. Spark.NET is an evolution of the Mobius project which provided .NET bindings for Spark.</p>\n<p>This sample takes a restaurant violation dataset from the NYC Open Data portal and processes it using Spark.NET. Then, the processed data is used to train a machine learning model that attempts to predict the grade an establishment will receive after an inspection. The model will be trained using ML.NET, an open-source, cross-platform machine learning framework. Finally, data for which no grade currently exists will be enriched using the trained model to assign an expected grade.</p>\n<p>The source code for this sample can be found in the <a href=\"https://github.com/lqdev/RestaurantInspectionsSparkMLNET\">lqdev/RestaurantInspectionsSparkMLNET\nGitHub repo</a>.</p>\n\n<p>This project was built using Ubuntu 18.04 but should work on Windows and Mac devices.</p>\n<ul>\n<li><a href=\"https://dotnet.microsoft.com/download/dotnet-core/2.1\">.NET Core 2.1 SDK</a></li>\n<li><a href=\"https://www.java.com/en/download/\">Java 8</a></li>\n<li><a href=\"https://archive.apache.org/dist/spark/spark-2.4.1/\">Apache Spark 2.4.1 with Hadoop 2.7</a></li>\n<li><a href=\"https://github.com/dotnet/spark/releases\">.NET Spark Worker 0.4.0</a></li>\n</ul>\n<h3>Install Java</h3>\n<p>Since Spark runs on the JVM, you'll need Java on your PC. The minimum version required is version 8. To install and Java, enter the following command into the terminal:</p>\n<pre><code class=\"language-bash\">sudo apt install openjdk-8-jdk openjdk-8-jre\n</code></pre>\n<p>Then, make sure that the recently installed version is the default</p>\n<pre><code class=\"language-bash\">sudo update-alternatives --config java\n</code></pre>\n<h3>Download and configure Spark</h3>\n<p>Download Spark 2.4.1 with Hadoop 2.7 onto your computer. In this case, I'm placing it into my <em>Downloads</em> folder.</p>\n<pre><code class=\"language-bash\">wget https://archive.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz -O ~/Downloads/spark-2.4.1-bin-hadoop2.7.tgz\n</code></pre>\n<p>Extract the contents of the recently downloaded file into the <em>/usr/bin/local</em> directory.</p>\n<pre><code class=\"language-bash\">sudo tar -xvf ~/Downloads/spark-2.4.1-bin-hadoop2.7.tgz --directory /usr/local/bin\n</code></pre>\n<h3>Download and configure .NET Spark Worker</h3>\n<p>Download the .NET Spark worker onto your computer. In this case, I'm placing it into the <em>Downloads</em> folder.</p>\n<pre><code class=\"language-bash\">wget https://github.com/dotnet/spark/releases/download/v0.4.0/Microsoft.Spark.Worker.netcoreapp2.1.linux-x64-0.4.0.tar.gz -O ~/Downloads/Microsoft.Spark.Worker.netcoreapp2.1.linux-x64-0.4.0.tar.gz\n</code></pre>\n<p>Extract the contents of the recently downloaded file into the <em>/usr/bin/local</em> directory.</p>\n<pre><code class=\"language-bash\">sudo tar -xvf ~/Downloads/Microsoft.Spark.Worker.netcoreapp2.1.linux-x64-0.4.0.tar.gz --directory /usr/local/bin\n</code></pre>\n<p>Finally, raise the permissions on the <code>Microsoft.Spark.Worker</code> program. This is required to execute User-Defined Functions (UDF).</p>\n<pre><code class=\"language-bash\">sudo chmod +x /usr/local/bin/Microsoft.Spark.Worker-0.4.0/Microsoft.Spark.Worker\n</code></pre>\n<h3>Configure environment variables</h3>\n<p>Once you download and configure the pre-requisites, configure their locations in the system as environment variables. Open the <em>~/.bashrc</em> file and add the following content at the end of the file.</p>\n<pre><code class=\"language-bash\">export SPARK_PATH=/usr/local/bin/spark-2.4.1-bin-hadoop2.7\nexport PATH=$SPARK_PATH/bin:$PATH\nexport HADOOP_HOME=$SPARK_PATH\nexport SPARK_HOME=$SPARK_PATH\nexport DOTNET_WORKER_DIR=/usr/local/bin/Microsoft.Spark.Worker-0.4.0\n</code></pre>\n\n<h3>Understand the data</h3>\n<p>The dataset used in this solution is the <a href=\"https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j\"><em>DOHMH New York City Restaurant Inspection Results</em></a> and comes from the NYC Open Data portal. It is updated daily and contains assigned and pending inspection results and violation citations for restaurants and college cafeterias. The dataset excludes establishments that have gone out of business. Although the dataset contains several columns, only a subset of them are used in this solution. For a detailed description of the dataset, visit the dataset website.</p>\n<h3>Understand the solution</h3>\n<p>This solution is made up of four different .NET Core applications:</p>\n<ul>\n<li><em>RestaurantInspectionsETL</em>: .NET Core Console application that takes raw data and uses Spark.NET to clean and transform the data into a format that is easier to use as input for training and making predictions with a machine learning model built with ML.NET.</li>\n<li><em>RestaurantInspectionsML</em>: .NET Core Class Library that defines the input and output schema of the ML.NET machine learning model. Additionally, this is where the trained model is saved to.</li>\n<li><em>RestaurantInspectionsTraining</em>: .NET Core Console application that uses the graded data generated by the <em>RestaurantInspectionsETL</em> application to train a multiclass classification machine learning model using ML.NET's Auto ML.</li>\n<li><em>RestaurantInspectionsEnrichment</em>: .NET Core Console application that uses the ungraded data generated by the <em>RestaurantInspectionsETL</em> application as input for the trained ML.NET machine learning model which predicts what grade an establishment is most likely to receive based on the violations found during inspection.</li>\n</ul>\n\n<h3>Create solution directory</h3>\n<p>Create a new directory for your projects called <em>RestaurantInspectionsSparkMLNET</em> and navigate to it with the following command.</p>\n<pre><code class=\"language-bash\">mkdir RestaurantInspectionsSparkMLNET &amp;amp;&amp;amp; cd RestaurantInspectionsSparkMLNET\n</code></pre>\n<p>Then, create a solution using the <code>dotnet cli</code>.</p>\n<pre><code class=\"language-bash\">dotnet new sln\n</code></pre>\n<p>To ensure that the 2.1 version of the .NET Core SDK is used as the target framework, especially if you have multiple versions of the .NET SDK installed, create a file called <em>globals.json</em> in the <em>RestaurantInspectionsSparkMLNET</em> solution directory.</p>\n<pre><code class=\"language-bash\">touch global.json\n</code></pre>\n<p>In the <em>global.json</em> file, add the following content. Make sure to use the specific version of the SDK installed on your computer. In this case, I have version <code>2.1.801</code> installed on my computer. You can use the <code>dotnet --list-sdks</code> command to list the installed SDK versions.</p>\n<pre><code class=\"language-json\">{\n  &amp;quot;sdk&amp;quot;: {\n    &amp;quot;version&amp;quot;: &amp;quot;2.1.801&amp;quot;\n  }\n}\n</code></pre>\n<h3>Create and configure the ETL project</h3>\n<p>The ETL project is responsible for taking the raw source data and using Spark to apply a series of transformations to prepare the data to train the machine learning model as well as to enrich data with missing grades.</p>\n<p>Inside the <em>RestaurantInspectionsSparkMLNET</em> solution directory, create a new console application called <em>RestaurantInspectionsETL</em> using the <code>dotnet cli</code>.</p>\n<pre><code class=\"language-bash\">dotnet new console -o RestaurantInspectionsETL\n</code></pre>\n<p>Add the newly created project to the solution with the <code>dotnet cli</code>.</p>\n<pre><code class=\"language-bash\">dotnet sln add ./RestaurantInspectionsETL/\n</code></pre>\n<p>Since this project uses the <code>Microsoft.Spark</code> NuGet package, use the <code>dotnet cli</code> to install it.</p>\n<pre><code class=\"language-bash\">dotnet add ./RestaurantInspectionsETL/ package Microsoft.Spark --version 0.4.0\n</code></pre>\n<h3>Create and configure the ML model project</h3>\n<p>The ML model class library will contain the domain model that defines the schema of model inputs and outputs as well as the trained model itself.</p>\n<p>Inside the <em>RestaurantInspectionsSparkMLNET</em> solution directory, create a new class library called <em>RestaurantInspectionsML</em> using the <code>dotnet cli</code>.</p>\n<pre><code class=\"language-bash\">dotnet new classlib -o RestaurantInspectionsML\n</code></pre>\n<p>Add the newly created project to the solution with the <code>dotnet cli</code>.</p>\n<pre><code class=\"language-bash\">dotnet sln add ./RestaurantInspectionsML/\n</code></pre>\n<p>Since this project uses the <code>Microsoft.ML</code> NuGet package, use the <code>dotnet cli</code> to install it.</p>\n<pre><code class=\"language-bash\">dotnet add ./RestaurantInspectionsML/ package Microsoft.ML --version 1.3.1\n</code></pre>\n<h3>Create and configure the ML training project</h3>\n<p>The purpose of the training project is to use the pre-processed graded data output by the <em>RestaurantInspectionsETL</em> project as input to train a multiclass classification model with ML.NET's Auto ML API. The trained model will then be saved in the <em>RestaurantInspectionsML</em> directory.</p>\n<p>Inside the <em>RestaurantInspectionsSparkMLNET</em> solution directory, create a new console application called <em>RestaurantInspectionsTraining</em> using the <code>dotnet cli</code>.</p>\n<pre><code class=\"language-bash\">dotnet new console -o RestaurantInspectionsTraining\n</code></pre>\n<p>Add the newly created project to the solution with the <code>dotnet cli</code>.</p>\n<pre><code class=\"language-bash\">dotnet sln add ./RestaurantInspectionsTraining/\n</code></pre>\n<p>This project depends on the domain model created in the <em>RestaurantInspectionsML</em> project, so you need to add a reference to it.</p>\n<pre><code class=\"language-bash\">dotnet add ./RestaurantInspectionsTraining/ reference ./RestaurantInspectionsML/\n</code></pre>\n<p>Since this project uses the <code>Microsoft.Auto.ML</code> NuGet package, use the <code>dotnet cli</code> to install it.</p>\n<pre><code class=\"language-bash\">dotnet add ./RestaurantInspectionsTraining/ package Microsoft.ML.AutoML --version 0.15.1\n</code></pre>\n<h3>Create and configure the data enrichment project</h3>\n<p>The data enrichment application uses the trained machine learning model created by the <em>RestaurantInspectionsTraining</em> application and use it on the pre-processed ungraded data created by the <em>RestaurantInspectionsETL</em> application to predict what grade that establishment is most likely to receive based on the violations found during inspection.</p>\n<p>Inside the <em>RestaurantInspectionsSparkMLNET</em> solution directory, create a new console application called <em>RestaurantInspectionsEnrichment</em> using the <code>dotnet cli</code>.</p>\n<pre><code class=\"language-bash\">dotnet new console -o RestaurantInspectionsEnrichment\n</code></pre>\n<p>Add the newly created project to the solution with the <code>dotnet cli</code>.</p>\n<pre><code class=\"language-bash\">dotnet sln add ./RestaurantInspectionsEnrichment/\n</code></pre>\n<p>This project depends on the domain model created in the <em>RestaurantInspectionsML</em> project, so you need to add a reference to it.</p>\n<pre><code class=\"language-bash\">dotnet add ./RestaurantInspectionsEnrichment/ reference ./RestaurantInspectionsML/\n</code></pre>\n<p>This uses the following NuGet packages:</p>\n<ul>\n<li>Microsoft.Spark</li>\n<li>Microsoft.ML.LightGBM (This is not required but predictions may fail if the final model is a LightGBM model).</li>\n</ul>\n<p>Install the packages with the following commands:</p>\n<pre><code class=\"language-bash\">dotnet add ./RestaurantInspectionsEnrichment/ package Microsoft.Spark --version 0.4.0\ndotnet add ./RestaurantInspectionsEnrichment/ package Microsoft.ML.LightGBM --version 1.3.1\n</code></pre>\n\n<p>The first step is to prepare the data. To do so, apply a set of transformations using Spark.NET.</p>\n<h3>Download the data</h3>\n<p>Navigate to the <em>RestaurantInspectionsETL</em> project and create a <em>Data</em> directory.</p>\n<pre><code class=\"language-bash\">mkdir Data\n</code></pre>\n<p>Then, download the data into the newly created <em>Data</em> directory.</p>\n<pre><code class=\"language-bash\">wget https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv?accessType=DOWNLOAD -O Data/NYC-Restaurant-Inspections.csv\n</code></pre>\n<h3>Build the ETL pipeline</h3>\n<p>Add the following usings to the <em>Program.cs</em> file.</p>\n<pre><code class=\"language-csharp\">using System;\nusing System.IO;\nusing Microsoft.Spark.Sql;\nusing static Microsoft.Spark.Sql.Functions;\n</code></pre>\n<p>Not all of the columns are relevant. Inside the <code>Main</code> method of the <em>Program.cs</em> file, define the columns to be removed.</p>\n<pre><code class=\"language-csharp\">string[] dropCols = new string[]\n{\n    &amp;quot;CAMIS&amp;quot;,\n    &amp;quot;CUISINE DESCRIPTION&amp;quot;,\n    &amp;quot;VIOLATION DESCRIPTION&amp;quot;,\n    &amp;quot;BORO&amp;quot;,\n    &amp;quot;BUILDING&amp;quot;,\n    &amp;quot;STREET&amp;quot;,\n    &amp;quot;ZIPCODE&amp;quot;,\n    &amp;quot;PHONE&amp;quot;,\n    &amp;quot;ACTION&amp;quot;,\n    &amp;quot;GRADE DATE&amp;quot;,\n    &amp;quot;RECORD DATE&amp;quot;,\n    &amp;quot;Latitude&amp;quot;,\n    &amp;quot;Longitude&amp;quot;,\n    &amp;quot;Community Board&amp;quot;,\n    &amp;quot;Council District&amp;quot;,\n    &amp;quot;Census Tract&amp;quot;,\n    &amp;quot;BIN&amp;quot;,\n    &amp;quot;BBL&amp;quot;,\n    &amp;quot;NTA&amp;quot;\n};\n</code></pre>\n<p>The entrypoint of Spark applications is the <code>SparkSession</code>. Create <code>SparkSession</code> inside the <code>Main</code> method of the <em>Program.cs</em> file.</p>\n<pre><code class=\"language-csharp\">var sc =\n    SparkSession\n        .Builder()\n        .AppName(&amp;quot;Restaurant_Inspections_ETL&amp;quot;)\n        .GetOrCreate();\n</code></pre>\n<p>Then, load the data stored in the <em>NYC-Restaurant-Inspections.csv</em> file into a <code>DataFrame</code>.</p>\n<pre><code class=\"language-csharp\">DataFrame df =\n    sc\n    .Read()\n    .Option(&amp;quot;header&amp;quot;, &amp;quot;true&amp;quot;)\n    .Option(&amp;quot;inferSchema&amp;quot;, &amp;quot;true&amp;quot;)\n    .Csv(&amp;quot;Data/NYC-Restaurant-Inspections.csv&amp;quot;);\n</code></pre>\n<p><code>DataFrames</code> can be thought of as tables in a database or sheets in Excel. Spark has various ways of representing data but <code>DataFrames</code> are the format supported by Spark.NET. Additionally, the <code>DataFrame</code> API is higher-level and easier to work with.</p>\n<p>Once the data is loaded, get rid of the data that are not needed by creating a new <code>DataFrame</code> that excludes the <code>dropCols</code> as well as missing values.</p>\n<pre><code class=\"language-csharp\">DataFrame cleanDf =\n    df\n        .Drop(dropCols)\n        .WithColumnRenamed(&amp;quot;INSPECTION DATE&amp;quot;,&amp;quot;INSPECTIONDATE&amp;quot;)\n        .WithColumnRenamed(&amp;quot;INSPECTION TYPE&amp;quot;,&amp;quot;INSPECTIONTYPE&amp;quot;)\n        .WithColumnRenamed(&amp;quot;CRITICAL FLAG&amp;quot;,&amp;quot;CRITICALFLAG&amp;quot;)\n        .WithColumnRenamed(&amp;quot;VIOLATION CODE&amp;quot;,&amp;quot;VIOLATIONCODE&amp;quot;)\n        .Na()\n        .Drop();\n</code></pre>\n<p>Typically, machine learning models expect values to be numerical, so in the ETL step try to convert as many values as possible into numerical values. The <code>CRITICALFLAG</code> column contains &amp;quot;Y&amp;quot;/&amp;quot;N&amp;quot; values that can be encoded as 0 and 1.</p>\n<pre><code class=\"language-csharp\">DataFrame labeledFlagDf =\n    cleanDf\n        .WithColumn(&amp;quot;CRITICALFLAG&amp;quot;,\n            When(Functions.Col(&amp;quot;CRITICALFLAG&amp;quot;) == &amp;quot;Y&amp;quot;,1)\n            .Otherwise(0));\n</code></pre>\n<p>This dataset contains one violation per row which correspond to different inspections. Therefore, all of the violations need to be aggregated by business and inspection.</p>\n<pre><code class=\"language-csharp\">DataFrame groupedDf =\n    labeledFlagDf\n        .GroupBy(&amp;quot;DBA&amp;quot;, &amp;quot;INSPECTIONDATE&amp;quot;, &amp;quot;INSPECTIONTYPE&amp;quot;, &amp;quot;CRITICALFLAG&amp;quot;, &amp;quot;SCORE&amp;quot;, &amp;quot;GRADE&amp;quot;)\n        .Agg(Functions.CollectSet(Functions.Col(&amp;quot;VIOLATIONCODE&amp;quot;)).Alias(&amp;quot;CODES&amp;quot;))\n        .Drop(&amp;quot;DBA&amp;quot;, &amp;quot;INSPECTIONDATE&amp;quot;)\n        .WithColumn(&amp;quot;CODES&amp;quot;, Functions.ArrayJoin(Functions.Col(&amp;quot;CODES&amp;quot;), &amp;quot;,&amp;quot;))\n        .Select(&amp;quot;INSPECTIONTYPE&amp;quot;, &amp;quot;CODES&amp;quot;, &amp;quot;CRITICALFLAG&amp;quot;, &amp;quot;SCORE&amp;quot;, &amp;quot;GRADE&amp;quot;);  \n</code></pre>\n<p>Now that the data is in the format used to train and make predictions, split the cleaned <code>DataFrame</code> into two new <code>DataFrames</code>, graded and ungraded. The graded dataset is the data used for training the machine learning model. The ungraded data will be used for enrichment.</p>\n<pre><code class=\"language-csharp\">DataFrame gradedDf =\n    groupedDf\n    .Filter(\n        Col(&amp;quot;GRADE&amp;quot;) == &amp;quot;A&amp;quot; |\n        Col(&amp;quot;GRADE&amp;quot;) == &amp;quot;B&amp;quot; |\n        Col(&amp;quot;GRADE&amp;quot;) == &amp;quot;C&amp;quot; );\n\nDataFrame ungradedDf =\n    groupedDf\n    .Filter(\n        Col(&amp;quot;GRADE&amp;quot;) != &amp;quot;A&amp;quot; &amp;amp;\n        Col(&amp;quot;GRADE&amp;quot;) != &amp;quot;B&amp;quot; &amp;amp;\n        Col(&amp;quot;GRADE&amp;quot;) != &amp;quot;C&amp;quot; );  \n</code></pre>\n<p>Take the <code>DataFrames</code> and save them as csv files for later use.</p>\n<pre><code class=\"language-csharp\">var timestamp = ((DateTimeOffset) DateTime.UtcNow).ToUnixTimeSeconds().ToString();\n\nvar saveDirectory = Path.Join(&amp;quot;Output&amp;quot;,timestamp);\n\nif(!Directory.Exists(saveDirectory))\n{\n    Directory.CreateDirectory(saveDirectory);\n}\n\ngradedDf.Write().Csv(Path.Join(saveDirectory,&amp;quot;Graded&amp;quot;));\n\nungradedDf.Write().Csv(Path.Join(saveDirectory,&amp;quot;Ungraded&amp;quot;));\n</code></pre>\n<h3>Publish and run the ETL application</h3>\n<p>The final <em>Program.cs</em> file should look as follows:</p>\n<pre><code class=\"language-csharp\">using System;\nusing System.IO;\nusing Microsoft.Spark.Sql;\nusing static Microsoft.Spark.Sql.Functions;\n\nnamespace RestaurantInspectionsETL\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            // Define columns to remove\n            string[] dropCols = new string[]\n            {\n                &amp;quot;CAMIS&amp;quot;,\n                &amp;quot;CUISINE DESCRIPTION&amp;quot;,\n                &amp;quot;VIOLATION DESCRIPTION&amp;quot;,\n                &amp;quot;BORO&amp;quot;,\n                &amp;quot;BUILDING&amp;quot;,\n                &amp;quot;STREET&amp;quot;,\n                &amp;quot;ZIPCODE&amp;quot;,\n                &amp;quot;PHONE&amp;quot;,\n                &amp;quot;ACTION&amp;quot;,\n                &amp;quot;GRADE DATE&amp;quot;,\n                &amp;quot;RECORD DATE&amp;quot;,\n                &amp;quot;Latitude&amp;quot;,\n                &amp;quot;Longitude&amp;quot;,\n                &amp;quot;Community Board&amp;quot;,\n                &amp;quot;Council District&amp;quot;,\n                &amp;quot;Census Tract&amp;quot;,\n                &amp;quot;BIN&amp;quot;,\n                &amp;quot;BBL&amp;quot;,\n                &amp;quot;NTA&amp;quot;\n            };\n\n            // Create SparkSession\n            var sc =\n                SparkSession\n                    .Builder()\n                    .AppName(&amp;quot;Restaurant_Inspections_ETL&amp;quot;)\n                    .GetOrCreate();\n\n            // Load data\n            DataFrame df =\n                sc\n                .Read()\n                .Option(&amp;quot;header&amp;quot;, &amp;quot;true&amp;quot;)\n                .Option(&amp;quot;inferSchema&amp;quot;, &amp;quot;true&amp;quot;)\n                .Csv(&amp;quot;Data/NYC-Restaurant-Inspections.csv&amp;quot;);\n\n            //Remove columns and missing values\n            DataFrame cleanDf =\n                df\n                    .Drop(dropCols)\n                    .WithColumnRenamed(&amp;quot;INSPECTION DATE&amp;quot;,&amp;quot;INSPECTIONDATE&amp;quot;)\n                    .WithColumnRenamed(&amp;quot;INSPECTION TYPE&amp;quot;,&amp;quot;INSPECTIONTYPE&amp;quot;)\n                    .WithColumnRenamed(&amp;quot;CRITICAL FLAG&amp;quot;,&amp;quot;CRITICALFLAG&amp;quot;)\n                    .WithColumnRenamed(&amp;quot;VIOLATION CODE&amp;quot;,&amp;quot;VIOLATIONCODE&amp;quot;)\n                    .Na()\n                    .Drop();\n\n            // Encode CRITICAL FLAG column\n            DataFrame labeledFlagDf =\n                cleanDf\n                    .WithColumn(&amp;quot;CRITICALFLAG&amp;quot;,\n                        When(Functions.Col(&amp;quot;CRITICALFLAG&amp;quot;) == &amp;quot;Y&amp;quot;,1)\n                        .Otherwise(0));\n\n             // Aggregate violations by business and inspection\n            DataFrame groupedDf =\n                labeledFlagDf\n                    .GroupBy(&amp;quot;DBA&amp;quot;, &amp;quot;INSPECTIONDATE&amp;quot;, &amp;quot;INSPECTIONTYPE&amp;quot;, &amp;quot;CRITICALFLAG&amp;quot;, &amp;quot;SCORE&amp;quot;, &amp;quot;GRADE&amp;quot;)\n                    .Agg(Functions.CollectSet(Functions.Col(&amp;quot;VIOLATIONCODE&amp;quot;)).Alias(&amp;quot;CODES&amp;quot;))\n                    .Drop(&amp;quot;DBA&amp;quot;, &amp;quot;INSPECTIONDATE&amp;quot;)\n                    .WithColumn(&amp;quot;CODES&amp;quot;, Functions.ArrayJoin(Functions.Col(&amp;quot;CODES&amp;quot;), &amp;quot;,&amp;quot;))\n                    .Select(&amp;quot;INSPECTIONTYPE&amp;quot;, &amp;quot;CODES&amp;quot;, &amp;quot;CRITICALFLAG&amp;quot;, &amp;quot;SCORE&amp;quot;, &amp;quot;GRADE&amp;quot;);\n\n            // Split into graded and ungraded DataFrames\n            DataFrame gradedDf =\n                groupedDf\n                .Filter(\n                    Col(&amp;quot;GRADE&amp;quot;) == &amp;quot;A&amp;quot; |\n                    Col(&amp;quot;GRADE&amp;quot;) == &amp;quot;B&amp;quot; |\n                    Col(&amp;quot;GRADE&amp;quot;) == &amp;quot;C&amp;quot; );\n\n            DataFrame ungradedDf =\n                groupedDf\n                    .Filter(\n                        Col(&amp;quot;GRADE&amp;quot;) != &amp;quot;A&amp;quot; &amp;amp;\n                        Col(&amp;quot;GRADE&amp;quot;) != &amp;quot;B&amp;quot; &amp;amp;\n                        Col(&amp;quot;GRADE&amp;quot;) != &amp;quot;C&amp;quot; );\n\n            // Save DataFrames\n            var timestamp = ((DateTimeOffset) DateTime.UtcNow).ToUnixTimeSeconds().ToString();\n\n            var saveDirectory = Path.Join(&amp;quot;Output&amp;quot;,timestamp);\n\n            if(!Directory.Exists(saveDirectory))\n            {\n                Directory.CreateDirectory(saveDirectory);\n            }\n\n            gradedDf.Write().Csv(Path.Join(saveDirectory,&amp;quot;Graded&amp;quot;));\n\n            ungradedDf.Write().Csv(Path.Join(saveDirectory,&amp;quot;Ungraded&amp;quot;));\n        }\n    }\n}\n</code></pre>\n<p>Publish the application with the following command.</p>\n<pre><code class=\"language-bash\">dotnet publish -f netcoreapp2.1 -r ubuntu.18.04-x64\n</code></pre>\n<p>Run the application with <code>spark-submit</code>.</p>\n<pre><code class=\"language-bash\">spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master local bin/Debug/netcoreapp2.1/ubuntu.18.04-x64/publish/microsoft-spark-2.4.x-0.4.0.jar dotnet bin/Debug/netcoreapp2.1/ubuntu.18.04-x64/publish/RestaurantInspectionsETL.dll\n</code></pre>\n\n<h3>Define the model input schema</h3>\n<p>Navigate to the <em>RestaurantInspectionsTraining</em> project directory and create a new file called <em>ModelInput.cs</em>.</p>\n<pre><code class=\"language-bash\">touch ModelInput.cs\n</code></pre>\n<p>Open the <em>ModelInput.cs</em> file and add the following code.</p>\n<pre><code class=\"language-csharp\">using Microsoft.ML.Data;\n\nnamespace RestaurantInspectionsML\n{\n    public class ModelInput\n    {\n        [LoadColumn(0)]\n        public string InspectionType { get; set; }\n\n        [LoadColumn(1)]\n        public string Codes { get; set; }\n\n        [LoadColumn(2)]\n        public float CriticalFlag { get; set; }\n\n        [LoadColumn(3)]\n        public float InspectionScore { get; set; }\n\n        [LoadColumn(4)]\n        [ColumnName(&amp;quot;Label&amp;quot;)]\n        public string Grade { get; set; }\n    }\n}\n</code></pre>\n<p>Using attributes in the schema, five properties are defined:</p>\n<ul>\n<li>InspectionType: The type of inspection performed.</li>\n<li>Codes: Violation codes found during inspection.</li>\n<li>CriticalFlag: Indicates if any of the violations during the inspection were critical (contribute to food-borne illness).</li>\n<li>InspectionScore: Score assigned after inspection.</li>\n<li>Grade: Letter grade assigned after inspection</li>\n</ul>\n<p>The <code>LoadColumn</code> attribute defines the position of the column in the file. Data in the last column is assigned to the <code>Grade</code> property but is then referenced as <code>Label</code> in the <code>IDataView</code>. The reason for using the <code>ColumnName</code> attribute is ML.NET algorithms have default column names and renaming properties at the schema class level removes the need to define the feature and label columns as parameters in the training pipeline.</p>\n<h3>Define the model output schema</h3>\n<p>In the <em>RestaurantInspectionsTraining</em> project directory and create a new file called <em>ModelOutput.cs</em>.</p>\n<pre><code class=\"language-bash\">touch ModelOutput.cs\n</code></pre>\n<p>Open the <em>ModelOutput.cs</em> file and add the following code.</p>\n<pre><code class=\"language-csharp\">namespace RestaurantInspectionsML\n{\n    public class ModelOutput\n    {\n        public float[] Scores { get; set; }\n        public string PredictedLabel { get; set; }\n    }\n}\n</code></pre>\n<p>For the output schema, the <code>ModelOutput</code> class uses properties with the default column names of the outputs generated by the model training process:</p>\n<ul>\n<li>Scores: A float vector containing the probabilties for all the predicted classes.</li>\n<li>PredictedLabel: The value of the prediction. In this case, the <code>PredictedLabel</code> is the predicted grade expected to be assigned after inspection given the set of features for that inspection.</li>\n</ul>\n\n<p>The application trains a multiclass classification algorithm. Finding the &amp;quot;best&amp;quot; algorithm with the right parameters requires experimentation. Fortunately, ML.NET's Auto ML does this for you given you provide it with the type of algorithm you want to train.</p>\n<h3>Load the graded data</h3>\n<p>Navigate to the <em>RestaurantInspectionsTraining</em> project directory and add the following using statements to the <em>Program.cs</em> class.</p>\n<pre><code class=\"language-csharp\">using System;\nusing System.IO;\nusing System.Linq;\nusing Microsoft.ML;\nusing static Microsoft.ML.DataOperationsCatalog;\nusing Microsoft.ML.AutoML;\nusing RestaurantInspectionsML;\n</code></pre>\n<p>Inside the <code>Main</code> method of the <em>Program.cs</em> file, define the path where the data files are stored.</p>\n<pre><code class=\"language-csharp\">string solutionDirectory = &amp;quot;/home/lqdev/Development/RestaurantInspectionsSparkMLNET&amp;quot;;\nstring dataLocation = Path.Combine(solutionDirectory,&amp;quot;RestaurantInspectionsETL&amp;quot;,&amp;quot;Output&amp;quot;);\n</code></pre>\n<p>The entrypoint of an ML.NET application is the <code>MLContext</code>. Initialize an <code>MLContext</code> instance.</p>\n<pre><code class=\"language-csharp\">MLContext mlContext = new MLContext();\n</code></pre>\n<p>Next, get the paths of the data files. The output generated by the <em>RestaurantInspectionsETL</em> application contains both the csv files as well as files containing information about the partitions that created them. For training, only the csv files are needed.</p>\n<pre><code class=\"language-csharp\">var latestOutput =\n    Directory\n        .GetDirectories(dataLocation)\n        .Select(directory =&amp;gt; new DirectoryInfo(directory))\n        .OrderBy(directoryInfo =&amp;gt; directoryInfo.Name)\n        .Select(directory =&amp;gt; Path.Join(directory.FullName,&amp;quot;Graded&amp;quot;))\n        .First();\n\nvar dataFilePaths =\n    Directory\n        .GetFiles(latestOutput)\n        .Where(file =&amp;gt; file.EndsWith(&amp;quot;csv&amp;quot;))\n        .ToArray();\n</code></pre>\n<p>Then, load the data into an <code>IDataView</code>. An <code>IDataView</code> is similar to a <code>DataFrame</code> in that it is a way to represent data as rows, columns and their schema.</p>\n<pre><code class=\"language-csharp\">var dataLoader = mlContext.Data.CreateTextLoader&amp;lt;ModelInput&amp;gt;(separatorChar:',', hasHeader:false, allowQuoting:true, trimWhitespace:true);\n\nIDataView data = dataLoader.Load(dataFilePaths);\n</code></pre>\n<p>It's good practice to split the data into training and test sets for evaluation. Split the data into 80% training and 20% test sets.</p>\n<pre><code class=\"language-csharp\">TrainTestData dataSplit = mlContext.Data.TrainTestSplit(data,testFraction:0.2);\nIDataView trainData = dataSplit.TrainSet;\nIDataView testData = dataSplit.TestSet;\n</code></pre>\n<h3>Create the experiment</h3>\n<p>Auto ML takes the data and runs experiments using different models and hyper-parameters in search of the &amp;quot;best&amp;quot; model. Define the settings for your experiment. In this case, the model will run for 600 seconds or 10 minutes and will try to find the model with the lowest log loss metric.</p>\n<pre><code class=\"language-csharp\">var experimentSettings = new MulticlassExperimentSettings();\nexperimentSettings.MaxExperimentTimeInSeconds = 600;\nexperimentSettings.OptimizingMetric = MulticlassClassificationMetric.LogLoss;\n</code></pre>\n<p>Then, create the experiment.</p>\n<pre><code class=\"language-csharp\">var experiment = mlContext.Auto().CreateMulticlassClassificationExperiment(experimentSettings);\n</code></pre>\n<p>After creating the experiment, run it.</p>\n<pre><code class=\"language-csharp\">var experimentResults = experiment.Execute(data, progressHandler: new ProgressHandler());\n</code></pre>\n<p>By default, running the application won’t display progress information. However, a <code>ProgressHandler</code> object can be passed into the <code>Execute</code> method of an experiment which calls the implemented <code>Report</code> method.</p>\n<p>Inside the <em>RestaurantInspectionsTraining</em> project directory, create a new file called <em>ProgressHandler.cs</em>.</p>\n<pre><code class=\"language-bash\">touch ProgressHandler.cs\n</code></pre>\n<p>Then, add the following code:</p>\n<pre><code class=\"language-csharp\">using System;\nusing Microsoft.ML.Data;\nusing Microsoft.ML.AutoML;\n\nnamespace RestaurantInspectionsTraining\n{\n    public class ProgressHandler : IProgress&amp;lt;RunDetail&amp;lt;MulticlassClassificationMetrics&amp;gt;&amp;gt;\n    {\n        public void Report(RunDetail&amp;lt;MulticlassClassificationMetrics&amp;gt; run)\n        {\n            Console.WriteLine($&amp;quot;Trained {run.TrainerName} with Log Loss {run.ValidationMetrics.LogLoss:0.####} in {run.RuntimeInSeconds:0.##} seconds&amp;quot;);\n        }\n    }\n}\n</code></pre>\n<p>The ProgressHandler class derives from the <code>IProgress&amp;lt;T&amp;gt;</code> interface which requires the implementation of the <code>Report</code> method. The object being passed into the Report method after each run is an <code>RunDetail&amp;lt;MulticlassClassificationMetrics&amp;gt;</code> object. Each time a run is complete, the <code>Report</code> method is called and the code inside it executes.</p>\n<h3>Evaluate the results</h3>\n<p>Once the experiment has finished running, get the model from the best run. Add the following code to the <code>Main</code> method of the <em>Program.cs</em>.</p>\n<pre><code class=\"language-csharp\">var bestModel = experimentResults.BestRun.Model;\n</code></pre>\n<p>Evaluate the performance of the model using the test dataset and measure it's Micro-Accuracy metric.</p>\n<pre><code class=\"language-csharp\">IDataView scoredTestData = bestModel.Transform(testData);  \nvar metrics = mlContext.MulticlassClassification.Evaluate(scoredTestData);\nConsole.WriteLine($&amp;quot;MicroAccuracy: {metrics.MicroAccuracy}&amp;quot;);\n</code></pre>\n<h3>Save the trained model</h3>\n<p>Finally, save the trained model to the <em>RestaurantInspectionsML</em> project.</p>\n<pre><code class=\"language-csharp\">string modelSavePath = Path.Join(solutionDirectory,&amp;quot;RestaurantInspectionsML&amp;quot;,&amp;quot;model.zip&amp;quot;);\nmlContext.Model.Save(bestModel, data.Schema, modelSavePath);\n</code></pre>\n<p>A file called <em>model.zip</em> should be created inside the <em>RestaurantInspectionsML</em> project.</p>\n<p>Make sure that the trained model file is copied to the output directory by adding the following contents to the <em>RestaurantInspectionsML.csproj</em> file in the <em>RestaurantInspectionsML</em> directory.</p>\n<pre><code class=\"language-xml\">&amp;lt;ItemGroup&amp;gt;\n  &amp;lt;None Include=&amp;quot;model.zip&amp;quot;&amp;gt;\n    &amp;lt;CopyToOutputDirectory&amp;gt;PreserveNewest&amp;lt;/CopyToOutputDirectory&amp;gt;\n  &amp;lt;/None&amp;gt;\n&amp;lt;/ItemGroup&amp;gt;\n</code></pre>\n<p>Copying it to the output directory of the <em>RestaurantInspectionsML</em> makes it easier to reference from the <em>RestaurantInspectionsEnrichment</em> project since that project already contains a reference to the <em>RestaurantInspectionsML</em> class library.</p>\n\n<p>The final <em>Program.cs</em> file should look as follows:</p>\n<pre><code class=\"language-csharp\">using System;\nusing System.IO;\nusing System.Linq;\nusing Microsoft.ML;\nusing static Microsoft.ML.DataOperationsCatalog;\nusing Microsoft.ML.AutoML;\nusing RestaurantInspectionsML;\n\nnamespace RestaurantInspectionsTraining\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            // Define source data directory paths\n            string solutionDirectory = &amp;quot;/home/lqdev/Development/RestaurantInspectionsSparkMLNET&amp;quot;;\n            string dataLocation = Path.Combine(solutionDirectory,&amp;quot;RestaurantInspectionsETL&amp;quot;,&amp;quot;Output&amp;quot;);\n\n            // Initialize MLContext\n            MLContext mlContext = new MLContext();\n\n            // Get directory name of most recent ETL output\n            var latestOutput =\n                Directory\n                    .GetDirectories(dataLocation)\n                    .Select(directory =&amp;gt; new DirectoryInfo(directory))\n                    .OrderBy(directoryInfo =&amp;gt; directoryInfo.Name)\n                    .Select(directory =&amp;gt; Path.Join(directory.FullName,&amp;quot;Graded&amp;quot;))\n                    .First();\n\n            var dataFilePaths =\n                Directory\n                    .GetFiles(latestOutput)\n                    .Where(file =&amp;gt; file.EndsWith(&amp;quot;csv&amp;quot;))\n                    .ToArray();\n\n            // Load the data\n            var dataLoader = mlContext.Data.CreateTextLoader&amp;lt;ModelInput&amp;gt;(separatorChar:',', hasHeader:false, allowQuoting:true, trimWhitespace:true);\n            IDataView data = dataLoader.Load(dataFilePaths);\n\n            // Split the data\n            TrainTestData dataSplit = mlContext.Data.TrainTestSplit(data,testFraction:0.2);\n            IDataView trainData = dataSplit.TrainSet;\n            IDataView testData = dataSplit.TestSet;\n\n            // Define experiment settings\n            var experimentSettings = new MulticlassExperimentSettings();\n            experimentSettings.MaxExperimentTimeInSeconds = 600;\n            experimentSettings.OptimizingMetric = MulticlassClassificationMetric.LogLoss;\n\n            // Create experiment\n            var experiment = mlContext.Auto().CreateMulticlassClassificationExperiment(experimentSettings);\n\n            // Run experiment\n            var experimentResults = experiment.Execute(data, progressHandler: new ProgressHandler());\n\n            // Best Run Results\n            var bestModel = experimentResults.BestRun.Model;\n\n            // Evaluate Model\n            IDataView scoredTestData = bestModel.Transform(testData);  \n            var metrics = mlContext.MulticlassClassification.Evaluate(scoredTestData);\n            Console.WriteLine($&amp;quot;MicroAccuracy: {metrics.MicroAccuracy}&amp;quot;);\n\n            // Save Model\n            string modelSavePath = Path.Join(solutionDirectory,&amp;quot;RestaurantInspectionsML&amp;quot;,&amp;quot;model.zip&amp;quot;);\n            mlContext.Model.Save(bestModel, data.Schema, modelSavePath);\n        }\n    }\n}\n</code></pre>\n<p>Once all the code and configurations are complete, from the <em>RestaurantInspectionsTraining</em> directory, run the application using the <code>dotnet cli</code>. Remember this will take 10 minutes to run.</p>\n<pre><code class=\"language-bash\">dotnet run\n</code></pre>\n<p>The console output should look similar to the output below:</p>\n<pre><code class=\"language-text\">Trained LightGbmMulti with Log Loss 0.1547 in 1.55 seconds\nTrained FastTreeOva with Log Loss 0.0405 in 65.58 seconds\nTrained FastForestOva with Log Loss 0.0012 in 53.37 seconds\nTrained LightGbmMulti with Log Loss 0.0021 in 4.55 seconds\nTrained FastTreeOva with Log Loss 0.8315 in 5.22 seconds\nMicroAccuracy: 0.999389615839469\n</code></pre>\n\n<p>Now that the model is trained, it can be used to enrich the ungraded data.</p>\n<h3>Initialize the PredictionEngine</h3>\n<p>Navigate to the <em>RestaurantInspectionsEnrichment</em> project directory and add the following using statements to the <em>Program.cs</em> class.</p>\n<pre><code class=\"language-csharp\">using System.IO;\nusing System.Linq;\nusing Microsoft.ML;\nusing Microsoft.Spark.Sql;\nusing static Microsoft.Spark.Sql.Functions;\nusing RestaurantInspectionsML;\n</code></pre>\n<p>To make predictions, the model has to be loaded into the applicaton and because predictions are made one row at a time, a <code>PredictionEngine</code> has be created as well.</p>\n<p>Inside the <code>Program</code> class, define the <code>PredictionEngine</code>.</p>\n<pre><code class=\"language-csharp\">private static readonly PredictionEngine&amp;lt;ModelInput,ModelOutput&amp;gt; _predictionEngine;\n</code></pre>\n<p>Then, create a constructor to load the model and initialize it.</p>\n<pre><code class=\"language-csharp\">static Program()\n{\n    MLContext mlContext = new MLContext();\n    ITransformer model = mlContext.Model.Load(&amp;quot;model.zip&amp;quot;,out DataViewSchema schema);\n    _predictionEngine = mlContext.Model.CreatePredictionEngine&amp;lt;ModelInput,ModelOutput&amp;gt;(model);\n}\n</code></pre>\n<h3>Load the ungraded data</h3>\n<p>In the <code>Main</code> method of the <code>Program</code> class, define the location of the data files.</p>\n<pre><code class=\"language-csharp\">string solutionDirectory = &amp;quot;/home/lqdev/Development/RestaurantInspectionsSparkMLNET&amp;quot;;\nstring dataLocation = Path.Combine(solutionDirectory,&amp;quot;RestaurantInspectionsETL&amp;quot;,&amp;quot;Output&amp;quot;);\n</code></pre>\n<p>Then, get the path of the most recent ungraded data generated by the <em>RestaurantInspectionsETL</em> application.</p>\n<pre><code class=\"language-csharp\">var latestOutput =\n    Directory\n        .GetDirectories(dataLocation)\n        .Select(directory =&amp;gt; new DirectoryInfo(directory))\n        .OrderBy(directoryInfo =&amp;gt; directoryInfo.Name)\n        .Select(directory =&amp;gt; directory.FullName)\n        .First();\n</code></pre>\n<p>Initialize a <code>SparkSession</code> for your enrichment application.</p>\n<pre><code class=\"language-csharp\">var sc =\n    SparkSession\n        .Builder()\n        .AppName(&amp;quot;Restaurant_Inspections_Enrichment&amp;quot;)\n        .GetOrCreate();\n</code></pre>\n<p>The data generated by the <em>RestaurantInspectionsETL</em> does not have headers. However, the schema can be defined and set when the data is loaded.</p>\n<pre><code class=\"language-csharp\">var schema = @&amp;quot;\n    INSPECTIONTYPE string,\n    CODES string,\n    CRITICALFLAG int,\n    INSPECTIONSCORE int,\n    GRADE string&amp;quot;;\n\nDataFrame df = \n    sc\n    .Read()\n    .Schema(schema)\n    .Csv(Path.Join(latestOutput,&amp;quot;Ungraded&amp;quot;));\n</code></pre>\n<h3>Define the UDF</h3>\n<p>There is no built-in function in Spark that allows you to use a <code>PredictionEngine</code>. However, Spark can be extended through UDFs. Keep in mind that UDFs are not optimized like the built-in functions. Therefore, whenever possible, try to use the built-in functions as much as possible.</p>\n<p>In the <code>Program</code> class, create a new method called <code>PredictGrade</code> which takes in the set of features that make up the <code>ModelInput</code> expected by the trained model.</p>\n<pre><code class=\"language-csharp\">public static string PredictGrade(\n    string inspectionType,\n    string violationCodes,\n    int criticalFlag,\n    int inspectionScore)\n{\n    ModelInput input = new ModelInput\n    {\n        InspectionType=inspectionType,\n        Codes=violationCodes,\n        CriticalFlag=(float)criticalFlag,\n        InspectionScore=(float)inspectionScore\n    };\n\n    ModelOutput prediction = _predictionEngine.Predict(input);\n\n    return prediction.PredictedLabel;\n}\n</code></pre>\n<p>Then, inside the <code>Main</code> method, register the <code>PredictGrade</code> method as a UDF in your <code>SparkSession</code>.</p>\n<pre><code class=\"language-csharp\">sc.Udf().Register&amp;lt;string,string,int,int,string&amp;gt;(&amp;quot;PredictGrade&amp;quot;,PredictGrade);\n</code></pre>\n<h3>Enrich the data</h3>\n<p>Once the UDF is registered, use it inside of a <code>Select</code> statement which creates a new <code>DataFrame</code> that includes the input features as well as the predicted grade output by the trained mdoel.</p>\n<pre><code class=\"language-csharp\">var enrichedDf =\n    df\n    .Select(\n        Col(&amp;quot;INSPECTIONTYPE&amp;quot;),\n        Col(&amp;quot;CODES&amp;quot;),\n        Col(&amp;quot;CRITICALFLAG&amp;quot;),\n        Col(&amp;quot;INSPECTIONSCORE&amp;quot;),\n        CallUDF(&amp;quot;PredictGrade&amp;quot;,\n            Col(&amp;quot;INSPECTIONTYPE&amp;quot;),\n            Col(&amp;quot;CODES&amp;quot;),\n            Col(&amp;quot;CRITICALFLAG&amp;quot;),\n            Col(&amp;quot;INSPECTIONSCORE&amp;quot;)\n        ).Alias(&amp;quot;PREDICTEDGRADE&amp;quot;)\n    );\n</code></pre>\n<p>Finally, save the enriched <code>DataFrame</code></p>\n<pre><code class=\"language-csharp\">string outputId = new DirectoryInfo(latestOutput).Name;\nstring enrichedOutputPath = Path.Join(solutionDirectory,&amp;quot;RestaurantInspectionsEnrichment&amp;quot;,&amp;quot;Output&amp;quot;);\nstring savePath = Path.Join(enrichedOutputPath,outputId);\n\nif(!Directory.Exists(savePath))\n{\n    Directory.CreateDirectory(enrichedOutputPath);\n}\n\nenrichedDf.Write().Csv(savePath);\n</code></pre>\n<h3>Publish and run the enrichment application</h3>\n<p>The final <em>Program.cs</em> file should look as follows.</p>\n<pre><code class=\"language-csharp\">using System.IO;\nusing System.Linq;\nusing Microsoft.ML;\nusing Microsoft.Spark.Sql;\nusing static Microsoft.Spark.Sql.Functions;\nusing RestaurantInspectionsML;\n\nnamespace RestaurantInspectionsEnrichment\n{\n    class Program\n    {\n        private static readonly PredictionEngine&amp;lt;ModelInput,ModelOutput&amp;gt; _predictionEngine;\n\n        static Program()\n        {\n            MLContext mlContext = new MLContext();\n            ITransformer model = mlContext.Model.Load(&amp;quot;model.zip&amp;quot;,out DataViewSchema schema);\n            _predictionEngine = mlContext.Model.CreatePredictionEngine&amp;lt;ModelInput,ModelOutput&amp;gt;(model);\n        }\n\n        static void Main(string[] args)\n        {\n            // Define source data directory paths\n            string solutionDirectory = &amp;quot;/home/lqdev/Development/RestaurantInspectionsSparkMLNET&amp;quot;;\n            string dataLocation = Path.Combine(solutionDirectory,&amp;quot;RestaurantInspectionsETL&amp;quot;,&amp;quot;Output&amp;quot;);\n\n            var latestOutput = \n                Directory\n                    .GetDirectories(dataLocation)\n                    .Select(directory =&amp;gt; new DirectoryInfo(directory))\n                    .OrderBy(directoryInfo =&amp;gt; directoryInfo.Name)\n                    .Select(directory =&amp;gt; directory.FullName)\n                    .First();\n\n            var sc = \n                SparkSession\n                    .Builder()\n                    .AppName(&amp;quot;Restaurant_Inspections_Enrichment&amp;quot;)\n                    .GetOrCreate();\n\n            var schema = @&amp;quot;\n                INSPECTIONTYPE string,\n                CODES string,\n                CRITICALFLAG int,\n                INSPECTIONSCORE int,\n                GRADE string&amp;quot;;\n\n            DataFrame df = \n                sc\n                .Read()\n                .Schema(schema)\n                .Csv(Path.Join(latestOutput,&amp;quot;Ungraded&amp;quot;));\n\n            sc.Udf().Register&amp;lt;string,string,int,int,string&amp;gt;(&amp;quot;PredictGrade&amp;quot;,PredictGrade);\n\n            var enrichedDf = \n                df\n                .Select(\n                    Col(&amp;quot;INSPECTIONTYPE&amp;quot;),\n                    Col(&amp;quot;CODES&amp;quot;),\n                    Col(&amp;quot;CRITICALFLAG&amp;quot;),\n                    Col(&amp;quot;INSPECTIONSCORE&amp;quot;),\n                    CallUDF(&amp;quot;PredictGrade&amp;quot;,\n                        Col(&amp;quot;INSPECTIONTYPE&amp;quot;),\n                        Col(&amp;quot;CODES&amp;quot;),\n                        Col(&amp;quot;CRITICALFLAG&amp;quot;),\n                        Col(&amp;quot;INSPECTIONSCORE&amp;quot;)\n                    ).Alias(&amp;quot;PREDICTEDGRADE&amp;quot;)\n                );\n\n            string outputId = new DirectoryInfo(latestOutput).Name;\n            string enrichedOutputPath = Path.Join(solutionDirectory,&amp;quot;RestaurantInspectionsEnrichment&amp;quot;,&amp;quot;Output&amp;quot;);\n            string savePath = Path.Join(enrichedOutputPath,outputId);\n\n            if(!Directory.Exists(savePath))\n            {\n                Directory.CreateDirectory(enrichedOutputPath);\n            }\n\n            enrichedDf.Write().Csv(savePath);\n\n        }\n\n        public static string PredictGrade(\n            string inspectionType,\n            string violationCodes,\n            int criticalFlag,\n            int inspectionScore)\n        {\n            ModelInput input = new ModelInput\n            {\n                InspectionType=inspectionType,\n                Codes=violationCodes,\n                CriticalFlag=(float)criticalFlag,\n                InspectionScore=(float)inspectionScore\n            };\n\n            ModelOutput prediction = _predictionEngine.Predict(input);\n\n            return prediction.PredictedLabel;\n        }\n    }\n}\n</code></pre>\n<p>From the <em>RestaurantInspectionsEnrichment</em> project publish the application with the following command.</p>\n<pre><code class=\"language-bash\">dotnet publish -f netcoreapp2.1 -r ubuntu.18.04-x64\n</code></pre>\n<p>Navigate to the <em>publish</em> directory. In this case, it's <em>bin/Debug/netcoreapp2.1/ubuntu.18.04-x64/publish</em>.</p>\n<p>From the <em>publish</em> directory, run the application with <code>spark-submit</code>.</p>\n<pre><code class=\"language-bash\">spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master local microsoft-spark-2.4.x-0.4.0.jar dotnet RestaurantInspectionsEnrichment.dll\n</code></pre>\n<p>The file output should look similar to the contents below:</p>\n<pre><code class=\"language-text\">Cycle Inspection / Initial Inspection,04N,1,13,A\nCycle Inspection / Re-inspection,08A,0,9,A\nCycle Inspection / Initial Inspection,&amp;quot;10B,10H&amp;quot;,0,10,A\nCycle Inspection / Initial Inspection,10F,0,10,A\nCycle Inspection / Reopening Inspection,10F,0,3,C\n</code></pre>\n\n<p>This solution showcased how Spark can be used within .NET applications with Spark.NET. Because it's part of the .NET ecosystem, other components and frameworks such as ML.NET can be leveraged to extend the system's capabilities. Although this sample was developed and run on a local, single-node cluster, Spark was made to run at scale. As such, this application can be further improved by setting up a cluster and running the ETL and enrichment workloads on there.</p>\n<h6>Resources</h6>\n<p><a href=\"https://spark.apache.org/\">Apache Spark</a>\n<a href=\"https://dotnet.microsoft.com/apps/data/spark\">Spark.NET</a>\n<a href=\"https://github.com/dotnet/spark\">Spark.NET GitHub</a>\n<a href=\"https://github.com/Microsoft/Mobius\">Mobius</a>\n<a href=\"https://opendata.cityofnewyork.us/\">NYC OpenData</a></p>\n","tags":["mlnet","ml","machine-learning","ai","artificial-intelligence","big-data","spark","dotnet","dotnetcore"]},{"title":"Machine Learning Lifecycle Management with ML.NET, Automated ML and MLFlow","contentType":"posts","date":"2019-05-09 19:19:20","url":"/posts/ml-lifecycle-management-mlflow-automated-ml-net/","content":"\n<p>As machine learning matures, best practices are starting to be adopted. Application lifecycle management has been a common practice within software development for some time. Now, some of those practices are starting to become adopted in the machine learning space. One of the challenges application lifecycle management addresses in machine learning is reproducibility.</p>\n<p>Machine learning is extremely experimental in nature. Therefore, in order to find the &amp;quot;best&amp;quot; model, various algorithms and hyper-parameters need to be tested. This can sometimes be a manual process. At Build 2019, <a href=\"https://devblogs.microsoft.com/dotnet/announcing-ml-net-1-0/\">Automated ML was announced</a> for ML.NET. In addition to automating the training process, this framework will try to find the best model by iterating over various algorithms and hyper-parameters until it finds the &amp;quot;best&amp;quot; model based on the selected optimization metric. The output will consist of results for the best run along with results for all other runs. These runs contain performance metrics, learned model parameters, the training pipeline used and the trained model for the respective run. This information can then be used for auditing purposes as well as to reproduce results.</p>\n<p>The results from running Automated ML can be persisted locally or in a database. However in 2018 a product called MLFlow was launched. <a href=\"https://mlflow.org/\">MLFlow</a> is an open source machine learning lifecycle management platform. Since its announcement, MLFlow has seen adoption throughout the industry and most recently <a href=\"https://databricks.com/company/newsroom/press-releases/databricks-collaborates-with-microsoft-on-mlflow-open-source-project\">Microsoft announced native support for it inside of Azure ML</a>. Although MLFlow does not natively support .NET, it has a REST API that allows extensibility to non-natively supported languages. This means that if throughout your enterprise or projects, MLFlow has been adopted in Python or R applications, using the REST API you can integrate MLFlow into your ML.NET applications.</p>\n<p>In this writeup, I will go over how to automatically build an ML.NET classification model that predicts iris flower types using Automated ML and then integrate MLFlow to log the results generated by the best run. The code for this sample can be found on <a href=\"https://github.com/lqdev/MLNETMLFlowSample\">GitHub</a>.</p>\n\n<p>This project was built on an Ubuntu 18.04 PC but should work on Windows and Mac. Note that MLFlow does not natively run on Windows at the time of this writing. To run it on Windows use <a href=\"https://docs.microsoft.com/en-us/windows/wsl/install-win10\">Windows Subsystem for Linux (WSL)</a>.</p>\n<ul>\n<li><a href=\"https://www.python.org/downloads/\">Python 3.x</a></li>\n<li><a href=\"https://www.mlflow.org/docs/latest/quickstart.html\">MLFlow</a></li>\n<li><a href=\"https://dotnet.microsoft.com/download\">.NET SDK 2.x</a></li>\n</ul>\n\n<h3>Create Solution</h3>\n<p>First we'll start off by creating a solution for our project. In the terminal enter the following commands:</p>\n<pre><code class=\"language-bash\">mkdir MLNETMLFlowSample &amp;amp;&amp;amp; cd MLNETMLFlowSample\ndotnet new sln\n</code></pre>\n<h3>Create Console Application</h3>\n<p>Once the solution is created, from the root solution directory, enter the following commands into the terminal to create a console application.</p>\n<pre><code class=\"language-bash\">dotnet new console -o MLNETMLFlowSampleConsole\n</code></pre>\n<p>Then, navigate into the newly created console application folder.</p>\n<pre><code class=\"language-bash\">cd MLNETMLFlowSampleConsole\n</code></pre>\n<h3>Install NuGet Packages</h3>\n<p>For this solution, you'll need the following NuGet packages:</p>\n<ul>\n<li><a href=\"https://www.nuget.org/packages/Microsoft.ML/\">Microsoft.ML</a></li>\n<li><a href=\"https://www.nuget.org/packages/Microsoft.ML.AutoML/\">Microsoft.ML.Auto</a></li>\n<li><a href=\"https://www.nuget.org/packages/MLFlow.NET/1.0.0-CI-20181206-054144\">MLFlow.NET</a></li>\n</ul>\n<p>From the console application directory, enter the following commands into the terminal to install the packages.</p>\n<pre><code class=\"language-bash\">dotnet add package Microsof.ML\ndotnet add package Microsoft.ML.AutoML\ndotnet add package MLFlow.NET --version 1.0.0-CI-20181206-054144\n</code></pre>\n<h3>Get The Data</h3>\n<p>The data used in this dataset comes from the <a href=\"http://archive.ics.uci.edu/ml/index.php\">UCI Machine Learning Repository</a> and looks like the data below:</p>\n<pre><code class=\"language-text\">5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n4.6,3.1,1.5,0.2,Iris-setosa\n</code></pre>\n<p>First, create a directory for the data inside the console application directory:</p>\n<pre><code class=\"language-bash\">mkdir Data\n</code></pre>\n<p>Then, download and save the file into the <code>Data</code> directory.</p>\n<pre><code class=\"language-bash\">curl -o Data/iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n</code></pre>\n<p>Finally, make sure that the data file is copied into your output directory by adding the following section to your console application <code>csproj</code> file inside the <code>Project</code> tags:</p>\n<pre><code class=\"language-xml\">&amp;lt;ItemGroup&amp;gt;\n    &amp;lt;Content Include=&amp;quot;Data/iris.data&amp;quot;&amp;gt;\n        &amp;lt;CopyToOutputDirectory&amp;gt;PreserveNewest&amp;lt;/CopyToOutputDirectory&amp;gt;\n    &amp;lt;/Content&amp;gt;\n&amp;lt;/ItemGroup&amp;gt;\n</code></pre>\n\n<h3>MLFlow.NET Settings</h3>\n<p>MLFlow.NET requires two settings, the base URL where the MLFlow server listens on and the API endpoint. In this case since it will be running locally, the base URL is <code>http://localhost:5000</code>. In the console application directory, create a file called <code>appsettings.json</code> and add the following content:</p>\n<pre><code class=\"language-json\">{\n    &amp;quot;MLFlowConfiguration&amp;quot;: {\n        &amp;quot;MLFlowServerBaseUrl&amp;quot;: &amp;quot;http://localhost:5000&amp;quot;,\n        &amp;quot;APIBase&amp;quot;: &amp;quot;api/2.0/preview/mlflow/&amp;quot;\n    }\n}\n</code></pre>\n<p>To make sure that your <code>appsettings.json</code> is copied into your output directory, add the following content to your <code>csproj</code> file under the content tags that include the <code>iris.data</code> file.</p>\n<pre><code class=\"language-xml\">&amp;lt;Content Include=&amp;quot;appsettings.json&amp;quot;&amp;gt;\n    &amp;lt;CopyToOutputDirectory&amp;gt;PreserveNewest&amp;lt;/CopyToOutputDirectory&amp;gt;\n&amp;lt;/Content&amp;gt;\n</code></pre>\n<h3>Register MLFLow.NET Service</h3>\n<p>In this sample, the MLFlow.NET service is registered and used via dependency injection. However, in order to use dependency injection in our console application it first needs to be configured. In the console application directory, create a new file called <code>Startup.cs</code> and add the following code to it:</p>\n<pre><code class=\"language-csharp\">using System;\nusing Microsoft.Extensions.Configuration;\nusing Microsoft.Extensions.DependencyInjection;\nusing MLFlow.NET.Lib;\nusing MLFlow.NET.Lib.Contract;\nusing MLFlow.NET.Lib.Model;\n\nnamespace MLNETMLFlowSampleConsole\n{\n    public class Startup\n    {\n        private readonly IConfiguration Configuration;\n        public IServiceProvider Services;\n\n        public Startup()\n        {\n            Configuration =\n                new ConfigurationBuilder()\n                    .AddJsonFile(&amp;quot;appsettings.json&amp;quot;, false)\n                    .Build();\n        }\n\n        public void ConfigureServices()\n        {\n            var services = new ServiceCollection();\n\n            // Add and configure MLFlow Service\n            services.AddMFlowNet();\n            services.Configure&amp;lt;MLFlowConfiguration&amp;gt;(\n                Configuration.GetSection(nameof(MLFlowConfiguration))\n            );\n\n            Services = services.BuildServiceProvider();\n        }\n    }\n}\n</code></pre>\n<p>The <code>Startup</code> class loads configuration settings from the <code>appsettings.json</code> file in the constructor. Then, the <code>ConfigureServices</code> method registers the MLFlow.NET service and configures it using the settings defined in the <code>appsettings.json</code> file. Once that this is set up, the service can be used throughout the application.</p>\n\n<p>When working with ML.NET, it often helps to create data models or classes that define the data's schema.</p>\n<p>For this sample, there are four columns with float values which will be the input data or features. The last column is the type of iris flower which will be used as the label or the value to predict.</p>\n<p>First, create a new directory called <code>Domain</code> inside the console application directory to store the data model classes.</p>\n<pre><code class=\"language-bash\">mkdir Domain\n</code></pre>\n<p>Inside the <code>Domain</code> directory, create a new file called <code>IrisData.cs</code> and add the following contents to the file:</p>\n<pre><code class=\"language-csharp\">using Microsoft.ML.Data;\n\nnamespace MLNETMLFlowSample.Domain\n{\n    public class IrisData\n    {\n        [LoadColumn(0, 3),\n        VectorType(4),\n        ColumnName(&amp;quot;Features&amp;quot;)]\n        public float Features { get; set; }\n\n        [LoadColumn(4),\n        ColumnName(&amp;quot;Label&amp;quot;)]\n        public string IrisType { get; set; }\n    }\n}\n</code></pre>\n<p>Using attributes in the schema, we define two properties: <code>Features</code> and <code>IrisType</code>. Data from columns in positions 0-3 will be loaded as a float vector of size four into the <code>Features</code> property. ML.NET will then reference that column by the name <code>Features</code>. Data in the last column will be loaded into the <code>IrisType</code> property and be referenced by the name <code>Label</code>. The reason for setting column names is ML.NET algorithms have a default column names and renaming properties at the schema level removes the need to define the feature and label columns in the pipeline.</p>\n\n<p>By default, running the application won't display progress information. However, a <code>ProgressHandler</code> object can be passed into the <code>Execute</code> method of an experiment which calls the implemented <code>Report</code> method.</p>\n<p>Inside the console application directory, create a new file called <code>ProgressHandler.cs</code> and add the following code:</p>\n<pre><code class=\"language-csharp\">using System;\nusing Microsoft.ML.AutoML;\nusing Microsoft.ML.Data;\nusing Microsoft.ML.Trainers;\n\nnamespace MLNETMLFlowSampleConsole\n{\n    public class ProgressHandler : IProgress&amp;lt;RunDetail&amp;lt;MulticlassClassificationMetrics&amp;gt;&amp;gt;\n    {\n        public void Report(RunDetail&amp;lt;MulticlassClassificationMetrics&amp;gt; runDetails)\n        {\n            Console.WriteLine($&amp;quot;Ran {runDetails.TrainerName} in {runDetails.RuntimeInSeconds:0.##} with Log Loss {runDetails.ValidationMetrics.LogLoss:0.####}&amp;quot;);\n        }\n    }\n}\n</code></pre>\n<p>The <code>ProgressHandler</code> class derives from the <code>IProgress&amp;lt;T&amp;gt;</code> interface which requires the implementation of the <code>Report</code> method. The object being passed into the <code>Report</code> method after each run is an <code>RunDetail&amp;lt;MulticlassCLassificationMetrics&amp;gt;</code> object. Each time a run is complete, the <code>Report</code> method is called and the code inside it executes.</p>\n\n<p>Open the <code>Program.cs</code> file and add the following <code>using</code> statements.</p>\n<pre><code class=\"language-csharp\">using System;\nusing System.IO;\nusing System.Threading.Tasks;\nusing Microsoft.ML;\nusing Microsoft.ML.Data;\nusing Microsoft.ML.AutoML;\nusing Microsoft.Extensions.DependencyInjection;\nusing MLNETMLFlowSampleConsole.Domain;\nusing MLFlow.NET.Lib;\nusing MLFlow.NET.Lib.Contract;\nusing MLFlow.NET.Lib.Model;\nusing MLFlow.NET.Lib.Model.Responses.Experiment;\nusing MLFlow.NET.Lib.Model.Responses.Run;\n</code></pre>\n<h3>Initialize Services</h3>\n<p>Then, inside of the <code>Program</code> class, define the <code>IMLFlowService</code>:</p>\n<pre><code class=\"language-csharp\">private readonly static IMLFlowService _mlFlowService;\n</code></pre>\n<p>Directly after that, create a constructor which is where <code>_mlFLowService</code> will be instantiated.</p>\n<pre><code class=\"language-csharp\">static Program()\n{\n    // Initialize app configuration\n    var appConfig = new Startup();\n    appConfig.ConfigureServices();\n\n    // Initialize MLFlow service\n    _mlFlowService = appConfig.Services.GetService&amp;lt;IMLFlowService&amp;gt;();\n}\n</code></pre>\n<h3>Define Experiment Steps</h3>\n<p>Then, add a method called <code>RunExperiment</code> inside the <code>Program</code> class that contains the following code:</p>\n<pre><code class=\"language-csharp\">public static async Task RunExperiment()\n{\n    // 1. Create MLContext\n    MLContext ctx = new MLContext();\n\n    // 2. Load data\n    IDataView data = ctx.Data.LoadFromTextFile&amp;lt;IrisData&amp;gt;(&amp;quot;Data/iris.data&amp;quot;, separatorChar: ',');\n\n    // 3. Define Automated ML.NET experiment settings\n    var experimentSettings = new MulticlassExperimentSettings();\n    experimentSettings.MaxExperimentTimeInSeconds = 30;\n    experimentSettings.OptimizingMetric = MulticlassClassificationMetric.LogLoss;\n\n    // 4. Create Automated ML.NET\n    var experiment = ctx.Auto().CreateMulticlassClassificationExperiment(experimentSettings);\n\n    // 5. Create experiment in MLFlow\n    var experimentName = Guid.NewGuid().ToString();\n    var experimentRequest = await _mlFlowService.GetOrCreateExperiment(experimentName);\n\n    // 6. Run Automated ML.NET experiment\n    var experimentResults = experiment.Execute(data, progressHandler: new ProgressHandler());\n\n    // 7. Log Best Run\n    LogRun(experimentRequest.ExperimentId,experimentResults);\n\n    string savePath = Path.Join(&amp;quot;MLModels&amp;quot;, $&amp;quot;{experimentName}&amp;quot;);\n    string modelPath = Path.Join(savePath, &amp;quot;model.zip&amp;quot;);\n\n    if (!Directory.Exists(savePath))\n    {\n        Directory.CreateDirectory(savePath);\n    }\n\n    // 8. Save Best Trained Model\n    ctx.Model.Save(experimentResults.BestRun.Model, data.Schema, modelPath);\n}\n</code></pre>\n<p>The <code>RunExperiment</code> method does the following:</p>\n<ol>\n<li>Creates an <code>MLContext</code> object.</li>\n<li>Loads data from the <code>iris.data</code> file into an <code>IDataView</code>.</li>\n<li>Configures experiment to run for 30 seconds and optimize the Log Loss metric.</li>\n<li>Creates a new Automated ML.NET experiment.</li>\n<li>Creates a new experiment in MLFlow to log information to.</li>\n<li>Runs the Automated ML.NET experiment and provide an instance of <code>ProgressHandler</code> to output progress to the console.</li>\n<li>Uses the <code>LogRun</code> method to log the results of the best run to MLFlow.</li>\n<li>Creates a directory inside the <code>MLModels</code> directory using the name of the experiment and saves the trained model inside it under the <code>model.zip</code> file name.</li>\n</ol>\n<h3>Log Progress</h3>\n<p>After the <code>RunExperiment</code> method, create the <code>LogRun</code> method and add the following code to it:</p>\n<pre><code class=\"language-csharp\">static async void LogRun(int experimentId, ExperimentResult&amp;lt;MulticlassClassificationMetrics&amp;gt; experimentResults)\n{\n    // Define run\n    var runObject = new CreateRunRequest();\n    runObject.ExperimentId = experimentId;\n    runObject.StartTime = ((DateTimeOffset)DateTime.UtcNow).ToUnixTimeMilliseconds();\n    runObject.UserId = Environment.UserName;\n    runObject.SourceType = SourceType.LOCAL;\n\n    // Create new run in MLFlow\n    var runRequest = await _mlFlowService.CreateRun(runObject);\n\n    // Get information for best run\n    var runDetails = experimentResults.BestRun;\n\n    // Log trainer name\n    await _mlFlowService.LogParameter(runRequest.Run.Info.RunUuid, nameof(runDetails.TrainerName), runDetails.TrainerName);\n\n    // Log metrics\n    await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.RuntimeInSeconds), (float)runDetails.RuntimeInSeconds);\n    await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.ValidationMetrics.LogLoss), (float)runDetails.ValidationMetrics.LogLoss);\n    await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.ValidationMetrics.MacroAccuracy), (float)runDetails.ValidationMetrics.MacroAccuracy);\n    await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.ValidationMetrics.MicroAccuracy), (float)runDetails.ValidationMetrics.MicroAccuracy);\n}\n</code></pre>\n<p>The <code>LogRun</code> method takes in the experiment ID and results. Then, it does the following:</p>\n<ol>\n<li>Configures local <code>RunRequest</code> object to log in MLFlow.</li>\n<li>Creates run in MLFLow using predefined configuration.</li>\n<li>Logs the name of the machine learning algorithm used to train the best model in MLFlow.</li>\n<li>Logs the amount of seconds it took to train the model in MLFLow.</li>\n<li>Logs various performance metrics in MLFlow</li>\n</ol>\n<h3>Implement Main Method</h3>\n<p>Finally, in the <code>Main</code> method of the <code>Program</code> class, call the <code>RunExperiment</code> method.</p>\n<pre><code class=\"language-csharp\">static async Task Main(string[] args)\n{\n    // Run experiment\n    await RunExperiment();\n}\n</code></pre>\n<p>Since the <code>Main</code> method in the <code>Program</code> class will be async, you need to use the latest version of C#. To do so, add the following configuration inside the <code>PropertyGroup</code> section of your <code>csproj</code> file.</p>\n<pre><code class=\"language-xml\">&amp;lt;LangVersion&amp;gt;latest&amp;lt;/LangVersion&amp;gt;\n</code></pre>\n\n<h3>Start MLFlow Server</h3>\n<p>In the terminal, from the console application directory, enter the following command to start the MLFlow Server:</p>\n<pre><code class=\"language-bash\">mlflow server\n</code></pre>\n<p>Navigate to <code>http://localhost:5000</code> in your browser. This will load the MLFLow UI.</p>\n<p><img src=\"/files/images/ml-lifecycle-management-mlflow-automated-ml-net-1.png\" class=\"img-fluid\" alt=\"\" /></p>\n<h3>Train Model</h3>\n<p>Then, in another terminal, from the console application directory, enter the following command to run the experiment:</p>\n<pre><code class=\"language-bash\">dotnet build\ndotnet run\n</code></pre>\n<p>As the experiment is running, the progress handler should be posting output to the console after each run.</p>\n<pre><code class=\"language-text\">Ran AveragedPerceptronOva in 1.24 with Log Loss 0.253\nRan SdcaMaximumEntropyMulti in 3.21 with Log Loss 0.0608\nRan LightGbmMulti in 0.64 with Log Loss 0.2224\nRan FastTreeOva in 1.54 with Log Loss 0.3978\nRan LinearSvmOva in 0.25 with Log Loss 0.2874\nRan LbfgsLogisticRegressionOva in 0.36 with Log Loss 0.566\nRan SgdCalibratedOva in 0.78 with Log Loss 0.7224\nRan FastForestOva in 1.28 with Log Loss 0.125\nRan LbfgsMaximumEntropyMulti in 0.25 with Log Loss 0.4537\nRan SdcaMaximumEntropyMulti in 0.19 with Log Loss 0.4576\nRan LightGbmMulti in 0.25 with Log Loss 0.2592\nRan FastForestOva in 1.98 with Log Loss 0.0961\nRan SdcaMaximumEntropyMulti in 0.39 with Log Loss 0.108\n</code></pre>\n<p>Navigate to <code>http://localhost:5000</code> in your browser. You should then see the results of your experiment and runs there.</p>\n<p><img src=\"/files/images/ml-lifecycle-management-mlflow-automated-ml-net-2.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Inspecting the detailed view of the best run for that experiment would look like the image below:</p>\n<p><img src=\"/files/images/ml-lifecycle-management-mlflow-automated-ml-net-3.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>You'll also notice that two directories were created inside the console application directory. On is an <code>MLModels</code> directory, inside of which a nested directory with the name of the experiment contains the trained model. Another called <code>mlruns</code>. In the <code>mlruns</code> directory are the results of the experiments logged in MLFlow.</p>\n\n<p>In this writeup, we built an ML.NET classification model using Automated ML. Then, the results of the best run generated by Automated ML were logged to an MLFlow server for later analysis. Note that although some of these tools are still in their nascent stage, as open source software, the opportunities for extensibility and best practice implementations are there. Happy coding!</p>\n","tags":["machine-learning","artificial-intelligence","mlnet","dotnet","dotnetcore","automatedml"]},{"title":"Automating Resource Provisioning for Machine Learning in Azure with Cognitive Services and Terraform","contentType":"posts","date":"2019-01-05 22:58:39","url":"/posts/automate-machine-learning-service-provisioning-azure-terraform/","content":"\n<p>Resources in the cloud, particularly Azure can be created in several ways some of which include Powershell, Azure CLI, Azure Portal and programatically through code. Another tool that can be leveraged to create resources in Azure is <a href=\"https://www.terraform.io/\">Terraform</a>. Terraform via configuration files can build and manage infrastructure across various providers such as AWS, Azure and Google. One of the benefits of Terraform is that it allows infastructure and resources to be defined as code. The configuration files serve as a blueprint as well as an execution plan of what needs to be provisioned in the cloud. Not only does this allow for automation and removal of human error in configurations but it also serves as a way to document the different changes infrastructure has gone through thoroughout the application lifecycle. The purpose of this writeup is to show how to create an environment using Terraform for an application that utilizes Azure Cognitive Services. Our application will create a Computer Vision service that we can then use via HTTP requests. Source code for this writeup can be found at the following <a href=\"https://github.com/lqdev/azcognitiveserviceterraformsample\">link</a>.</p>\n\n<p>This writeup was built and tested using a PC running Ubuntu 18.04 but should work on both Windows and Mac. It also assumes that you have an Azure account as well as Azure CLI and Terraform CLI installed. Below are links to get all of the resources needed:</p>\n<ul>\n<li><a href=\"https://azure.microsoft.com/en-us/free/\">Azure Account</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest\">Azure CLI</a></li>\n<li><a href=\"https://www.terraform.io/downloads.html\">Terraform CLI</a></li>\n</ul>\n\n<p>The first thing we need to do is authenticate with Azure CLI so that Terraform can use your account information to create services. In the terminal, enter the following command:</p>\n<pre><code class=\"language-bash\">az login\n</code></pre>\n\n<p>Once we're logged in, it's time to create the configuration file that Terraform will use to provision our services. First we'll need to create a directory for our application:</p>\n<pre><code class=\"language-bash\">mkdir azcognitiveserviceterraformsample\n</code></pre>\n<p>Then, we'll enter that directory and create our configuration file:</p>\n<pre><code class=\"language-bash\">cd azcognitiveserviceterraformsample\ntouch azcomputervision.tf\n</code></pre>\n<p>Using your preferred text editor, open the newly created <code>azcomputervision.tf</code> file and begin editing.</p>\n<h3>Defining the Provider</h3>\n<p>At the top of our file we'll want to configure our provider. This tells Terraform where our resources will be deployed to. In the <code>azcomputervision.tf</code> file enter the following:</p>\n<pre><code class=\"language-bash\"># Configure Azure Provider\nprovider &amp;quot;azurerm&amp;quot; {\n  version=&amp;quot;=1.20.0&amp;quot;\n}\n</code></pre>\n<p>Because we've already authenticated with the Azure CLI, there's no need to provide credentials. The only thing we need to do is specify the provider name and the version of the provider that will be used by the configuration file. For more details on how to configure the Azure provider, visit this <a href=\"https://www.terraform.io/docs/providers/azurerm/index.html\">link</a>.</p>\n<h3>Create Resource Group</h3>\n<p>After defining the provider, it's time to create a Resource Group that will contain the resources we create. In the configuration file, under the provider definition, enter the following:</p>\n<pre><code class=\"language-bash\"># Create Resource Group\nresource &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;terraform_cognitive_sample&amp;quot; {\n  name=&amp;quot;terraform_cognitive_sample&amp;quot;\n  location=&amp;quot;East US&amp;quot;\n}\n</code></pre>\n<p>This defines the name of our resource groups as well as where it is hosted. The syntax for resources looks like the snippet below where the <code>type</code> is the type of resources as required by Terraform and the <code>name</code> is any value of your choosing.</p>\n<pre><code class=\"language-bash\">resource &amp;quot;type&amp;quot; &amp;quot;name&amp;quot; {\n# Properties\n}\n</code></pre>\n<p>For more details on Resource Group configuration visit this <a href=\"https://www.terraform.io/docs/providers/azurerm/d/resource_group.html\">link</a>.</p>\n<h3>Create Azure Cognitive Service</h3>\n<p>With our resource group defined, it's time to define our Cognitive Service resource. Below our Resource Group definition, enter the following:</p>\n<pre><code class=\"language-bash\">resource &amp;quot;azurerm_cognitive_account&amp;quot; &amp;quot;computer_vision_service&amp;quot; {\n  name=&amp;quot;computer_vision_service&amp;quot;\n  resource_group_name=&amp;quot;${azurerm_resource_group.terraform_cognitive_sample.name}&amp;quot;\n  location=&amp;quot;${azurerm_resource_group.terraform_cognitive_sample.location}&amp;quot;\n  kind=&amp;quot;ComputerVision&amp;quot;\n  \n  sku {\n      name=&amp;quot;F0&amp;quot;\n      tier=&amp;quot;Free&amp;quot;\n  }\n}\n</code></pre>\n<p>Like our Resource Group, we provide a name for our service. Additional properties we need to provide are the name of the Resource Group and the location of where to deploy our service to. Because this has been previously defined in our Resource Group, Terraform allows us to access the configuration values as variables. Additionally, we need to specify the <code>kind</code> of cognitive service to deploy. In our case it will be the Computer Vision service so we use <code>ComputerVision</code>. The last thing we need to do is specify the pricing tier to use. We'll be using the free tier for this project. There are several services and tiers to choose from. To get more details on acceptable values for these properties visit the following <a href=\"https://www.terraform.io/docs/providers/azurerm/r/cognitive_account.html\">link</a>.</p>\n<h3>Store Output Variables</h3>\n<p>When our application is deployed, in order to use it we'll need the endpoint to where we will be making HTTP requests to. This can be persisted by Terraform by defining an output variable inside of the configuration file. To get the enpoint of our cognitive service, enter the following in the <code>azcomputervision.tf</code> file:</p>\n<pre><code class=\"language-bash\">output &amp;quot;computer_vision_endpoint&amp;quot; {\n    value=&amp;quot;${azurerm_cognitive_account.computer_vision_service.endpoint}&amp;quot;\n}\n</code></pre>\n<p>Once finished, the <code>azcomputervision.tf</code> file should look like this:</p>\n<pre><code class=\"language-bash\"># Configure Azure Provider\nprovider &amp;quot;azurerm&amp;quot; {\n  version=&amp;quot;=1.20.0&amp;quot;\n}\n\n# Create Resource Group\nresource &amp;quot;azurerm_resource_group&amp;quot; &amp;quot;terraform_cognitive_sample&amp;quot; {\n  name=&amp;quot;terraform_cognitive_sample&amp;quot;\n  location=&amp;quot;East US&amp;quot;\n}\n\n# Create Cognitive Service\nresource &amp;quot;azurerm_cognitive_account&amp;quot; &amp;quot;computer_vision_service&amp;quot; {\n  name=&amp;quot;computer_vision_service&amp;quot;\n  resource_group_name=&amp;quot;${azurerm_resource_group.terraform_cognitive_sample.name}&amp;quot;\n  location=&amp;quot;${azurerm_resource_group.terraform_cognitive_sample.location}&amp;quot;\n  kind=&amp;quot;ComputerVision&amp;quot;\n  \n  sku {\n      name=&amp;quot;F0&amp;quot;\n      tier=&amp;quot;Free&amp;quot;\n  }\n}\n\n# Output Cognitive Services Endpoint\noutput &amp;quot;computer_vision_endpoint&amp;quot; {\n    value=&amp;quot;${azurerm_cognitive_account.computer_vision_service.endpoint}&amp;quot;\n}\n</code></pre>\n\n<h3>Initialize Terraform Resources</h3>\n<p>When provisioning resources we first want to get the necessary plugins needed by Terraform to create such resources. Using Terraform CLI, enter the following in the terminal:</p>\n<pre><code class=\"language-bash\">terraform init\n</code></pre>\n<h3>Check Execution Plan</h3>\n<p>Although this is a single resource provision, it's always a good idea to see the execution plan of the resources that will be created. To do so, enter the following command in the terminal:</p>\n<pre><code class=\"language-bash\">terraform plan\n</code></pre>\n<p>The output should look similar to the content below:</p>\n<pre><code class=\"language-text\">Refreshing Terraform state in-memory prior to plan...\nThe refreshed state will be used to calculate this plan, but will not be\npersisted to local or remote state storage.\n\n\n------------------------------------------------------------------------\n\nAn execution plan has been generated and is shown below.\nResource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  + azurerm_cognitive_account.computer_vision_service\n      id:                  &amp;lt;computed&amp;gt;\n      endpoint:            &amp;lt;computed&amp;gt;\n      kind:                &amp;quot;ComputerVision&amp;quot;\n      location:            &amp;quot;eastus&amp;quot;\n      name:                &amp;quot;computer_vision_service&amp;quot;\n      resource_group_name: &amp;quot;terraform_cognitive_sample&amp;quot;\n      sku.#:               &amp;quot;1&amp;quot;\n      sku.0.name:          &amp;quot;F0&amp;quot;\n      sku.0.tier:          &amp;quot;Free&amp;quot;\n      tags.%:              &amp;lt;computed&amp;gt;\n\n  + azurerm_resource_group.terraform_cognitive_sample\n      id:                  &amp;lt;computed&amp;gt;\n      location:            &amp;quot;eastus&amp;quot;\n      name:                &amp;quot;terraform_cognitive_sample&amp;quot;\n      tags.%:              &amp;lt;computed&amp;gt;\n\n\nPlan: 2 to add, 0 to change, 0 to destroy.\n</code></pre>\n\n<p>It's now time to create our Computer Vision resource. Enter the following command in the terminal:</p>\n<pre><code class=\"language-bash\">terraform apply\n</code></pre>\n<p>You will be asked to review the execution plan once again. If everything looks good, type <code>yes</code> in the terminal to continue with the creation of resources. If everything deployed successfully, you should be able to see it in Azure.</p>\n\n<p>Using Azure CLI, enter the following command in the terminal to see whether your resource was deployed:</p>\n<pre><code class=\"language-bash\">az resource list --resource-group terraform_cognitive_sample --output table\n</code></pre>\n<p>That command will output something similar to the content below:</p>\n<pre><code class=\"language-bash\">Name                     ResourceGroup               Location    Type                                  Status\n-----------------------  --------------------------  ----------  ------------------------------------  --------\ncomputer_vision_service  terraform_cognitive_sample  eastus      Microsoft.CognitiveServices/accounts\n</code></pre>\n<h3>Get Resource Endpoint and Keys</h3>\n<p>In order to make a request to our service, we need both the endpoint and a key to authenticate our request.</p>\n<p>To get the endpoint, we can use Terraform CLI to extract the output variable defined in our configuration file. We can do that by using the following command in the terminal:</p>\n<pre><code class=\"language-bash\">terraform output computer_vision_endpoint\n</code></pre>\n<p>Save the output value somewhere because that's what will be used to make a request to the Computer Vision Cognitive Service.</p>\n<p>To get the keys, we'll use Azure CLI. In the terminal, enter:</p>\n<pre><code class=\"language-bash\">az cognitiveservices account keys list --resource-group terraform_cognitive_sample --name computer_vision_service\n</code></pre>\n<p>This will output the keys of your deployed service.</p>\n<pre><code class=\"language-json\">{\n  &amp;quot;key1&amp;quot;: &amp;quot;&amp;lt;YOUR-KEY-1&amp;gt;&amp;quot;,\n  &amp;quot;key2&amp;quot;: &amp;quot;&amp;lt;YOUR-KEY-2&amp;gt;&amp;quot;\n}\n</code></pre>\n<h3>Make HTTP Request</h3>\n<p>Either key can be used to make requests. To test the service, I'll be using an image from the web. All you need is to get the URL for that image. The image I will be using can be found at this URL <code>https://upload.wikimedia.org/wikipedia/commons/1/10/Empire_State_Building_%28aerial_view%29.jpg</code>. Using cURL, I will make a POST request to the endpoint with the key provided.</p>\n<p>One thing to note is that the endpoint is generic to many Cognitive Services. Therefore, we need to append to the path which service we will be using and the action we want to perform. In our case, we'll append the following path the the endpoint <code>/vision/v1.0/describe</code>. For more details, visit the Computer Vision API documentation at this <a href=\"https://eastus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fe\">link</a>.</p>\n<p>To make the request, enter the following into the terminal where <code>&amp;lt;YOUR-KEY&amp;gt;</code> is one of the keys of your Computer Vision service.</p>\n<pre><code class=\"language-bash\">curl -H 'Ocp-Apim-Subscription-Key: &amp;lt;YOUR-KEY&amp;gt;' -H &amp;quot;Content-type: application/json&amp;quot; -d '{&amp;quot;url&amp;quot;:&amp;quot;https://upload.wikimedia.org/wikipedia/commons/1/10/Empire_State_Building_%28aerial_view%29.jpg&amp;quot;}' 'https://eastus.api.cognitive.microsoft.com/vision/v1.0/describe'\n</code></pre>\n<p>If on Windows, you might want to try using a REST client such as POSTMAN or Insomnia.</p>\n<p>If the request is successful, the output should be similar to the following:</p>\n<pre><code class=\"language-json\">{\n    &amp;quot;description&amp;quot;:{\n        &amp;quot;tags&amp;quot;:[\n            &amp;quot;mountain&amp;quot;,&amp;quot;building&amp;quot;,&amp;quot;city&amp;quot;,&amp;quot;sitting&amp;quot;,&amp;quot;table&amp;quot;,&amp;quot;view&amp;quot;,&amp;quot;full&amp;quot;,&amp;quot;filled&amp;quot;,&amp;quot;large&amp;quot;,&amp;quot;old&amp;quot;,&amp;quot;skyscraper&amp;quot;,&amp;quot;many&amp;quot;,&amp;quot;stacked&amp;quot;,&amp;quot;water&amp;quot;,&amp;quot;room&amp;quot;,&amp;quot;white&amp;quot;\n        ],\n        &amp;quot;captions&amp;quot;:[\n            {\n                &amp;quot;text&amp;quot;:&amp;quot;a view of a city&amp;quot;,&amp;quot;confidence&amp;quot;:0.927147938733121\n            }\n        ]\n    },\n    &amp;quot;requestId&amp;quot;:&amp;quot;a9aaf779-2aa1-4012-b1e0-2d2d9c20c6a5&amp;quot;,\n    &amp;quot;metadata&amp;quot;:{\n        &amp;quot;width&amp;quot;:846,\n        &amp;quot;height&amp;quot;:1270,\n        &amp;quot;format&amp;quot;:&amp;quot;Jpeg&amp;quot;\n    }\n}\n</code></pre>\n\n<p>In this writeup I went over how to create a Terraform configuration file for automating the creation of a Computer Vision Cognitive Service resource in Azure. Although this is a simple example, using the same concepts, more complex environments and infrastructure can be provisioned just as seamlessly in a safe and efficient manner. An example of how to extend this application would be to provision an Azure Storage Account as well as an Azure Function application that utilizes the Computer Vision Cognitive Service to automatically classify photos when they are uploaded to an Azure Storage container. In such scenario, the benefits of automation and documentation that Terraform provides can leveraged to a greater extent.</p>\n","tags":["azure","cognitive services","terraform","machine learning","cloud"]},{"title":"The Case for Doing Machine Learning with F#","contentType":"posts","date":"2018-12-14 23:34:50","url":"/posts/case-fsharp-machine-learning/","content":"\n<p>This post is part of the 2018 <a href=\"https://sergeytihon.com/tag/fsadvent/\">FsAdvent</a> series organized by <a href=\"https://twitter.com/sergey_tihon\">Sergey Tihon</a>.</p>\n<p>When searching for tools and languages to implement machine learning applications, there are numerous options to choose from each with their own set of advantages and disadvantages. Out of all, however, Python seems to be the most salient. Not only is it a popular language but also many of the tools available for machine learning are either implemented in Python or support it in some capacity whether it's native or through community libraries. Very rarely though is F# mentioned in these discussions despite having many of the features that make languages like Python so loved and extending them to empower users. In this writeup, I will do a short review of many of the advantages of Python such as succinctness, platform support, library availability as well as many others and compare it to F#'s capabilities.</p>\n<p><img src=\"/files/images/case-fsharp-ml-0.PNG\" class=\"img-fluid\" alt=\"2017 Top Languages\" /></p>\n\n<h3>Learning Curve</h3>\n<p>One of the reasons why Python is so widely adopted is its learning curve. Whether an individual knows how to program or not, at times, Python can look like pseudocode making it accessible to not only readers but also writers. As with anything the more complex the task, the steeper the learning curve. However, at a simpler level, Python makes it as easy as possible to get started.</p>\n<p>Although at first it may not appear to be the case with F#, the learning curve is not much steeper than that of Python. The syntax can sometimes look intimidating to individuals, but the steepest part of the learning curve doesn't necessarily come from the language itself but rather from the way of reasoning about the logic of the programs. As a functional language, there is somewhat of a paradigm shift from that of a procedural execution model. Below is an example that defines a function that doubles an integer in both languages.</p>\n<h5>Python</h5>\n<pre><code class=\"language-python\">def double(x):\n    return x * 2\n</code></pre>\n<h5>FSharp</h5>\n<pre><code class=\"language-fsharp\">let double x = \n    x * 2\n</code></pre>\n<p>As it can be seen, despite some minor syntax and character differences the functions are essentially the same.</p>\n<h3>Intended Purpose</h3>\n<p>Depending on the task at hand, some languages are more adept for handling respective tasks. For example, R and Julia are excellent languages when performing statistical tasks. However, outside of those types of tasks their abilities are more limited. Python, being a general-purpose language means that not only can you use it for machine learning tasks but also to build n-tier applications entirely in Python without having to worry about integrations, plugins or having to learn an entirely different language to perform such actions.</p>\n<p>Similarly, F# is a general-purpose language which allows you to build web and console applications as well as machine learning applications all from the comfort of the same ecosystem.</p>\n<h3>Strong Library Support</h3>\n<p>When performing data analysis and machine learning, practicioners use a variety of libraries for their development such as <a href=\"http://www.numpy.org/\">NumPy</a> and <a href=\"https://pandas.pydata.org/\">Pandas</a> for data wrangling, <a href=\"https://scikit-learn.org/stable/index.html\">scikit-learn</a> for machine learning algorithms and <a href=\"https://matplotlib.org/\">matplotlib</a> for creating visualizations. Although all of these tasks could most certainly be implemented from scratch, libraries speed up the development process allowing practitioners to focus more on the domain and experiment with the models that best solve the respective problem they are facing.</p>\n<p>F#, like Python has exceptional library support, specifically as it regards data science and machine learning. <a href=\"https://fslab.org/\">FsLab</a> is a collection of open source F# packages for machine learning that contain libraries such as <a href=\"https://fsharp.github.io/FSharp.Data/\">FSharp.Data</a> and <a href=\"https://fslab.org/Deedle/\">Deedle</a> for data wrangling, <a href=\"https://numerics.mathdotnet.com/\">Math.NET Numerics</a> for machine learning algorithms and <a href=\"https://fslab.org/XPlot/\">XPlot</a> to help with data visualization. Furthermore, at Build 2018, Microsoft released <a href=\"https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet\">ML.NET</a>, an open-source, cross-platform machine learning framework for .NET which allows .NET users to not only perform machine learning tasks within .NET but also allows extensibility via Tensorflow, ONNX and Python extensions. For a more detailed writup on using ML.NET with F#, check out the post <a href=\"https://towardsdatascience.com/f-advent-calendar-a-christmas-classifier-72fb9e9d8f23\">A Christmas Classifier</a> by Willie Tetlow or my post <a href=\"http://luisquintanilla.me/2018/06/13/mlnet-classification-fsharp/\">Classification with F# ML.NET Models</a>.</p>\n<p>Libraries for F# are also not confined only to those written in F#. In many instances, because F# is a .NET language, there is interoperability with C# libraries further extending the capabilities of F#.</p>\n<h3>Platform Support</h3>\n<p>One of the things that makes Python so attractive is that it runs cross-platform. It does not matter whether you're on Windows, Mac or Linux; Python code runs the same. That being said though, not all platforms are created equal and although it is possible to run Python code on all platforms, essential libraries such as NumPy, Pandas and scikit-learn run best on Unix environments. It is possible to run them on Windows but the set up is not as straightforward.</p>\n<p>As a .NET Language, F# runs on Windows. With the help of the Mono runtime and most recently .NET Core, it also runs on Mac and Linux. Like Python, depending on the runtime and dependencies used by the respective software packages there may be limitations as to which platform code can be run on. However, from my experience most of the FsLab set of packages work cross-platform.</p>\n<h3>Succintness</h3>\n<p>Productivity is an important measure of a language. One way to achieve it is to write logic using the least number of characters. This is where Python shines. As a dynamically typed, whitespace ruled language, Python does not require developers to declare types associated with the objects and variables defined nor does it require the use of brackets and any other special characters. As expected, this allows for developers to write the same logic with less characters much faster.</p>\n<p>Unlike Python, F# is a statically typed language. However, thanks to type inference and the help of the compiler, writing programs often does not require developers to explicitly define what types objects and variables are. Additionally, it is a whitespace ruled language therefore removing the need for brackets and additional characters further speeding up the development process in a safe manner.</p>\n<h3>Immediate Feedback</h3>\n<p>Writing safe and effective code takes time and experience. However, even the most experienced developers often makes mistakes. Therefore, getting immediate feedback before adding certain logic to programs goes a long way to making code that is safe, efficient and tested.</p>\n<h4>Read-Evaluate-Print Loop (REPL)</h4>\n<p>One way in which both Python and F# provide immediate feedback is via the command line using the Read-Evaluate-Print Loop (REPL). The REPL is a programming environment that reads the user input, evaluates it and prints out the results, hence the name. Below are screenshots of what that environment looks like using both Python and F#.</p>\n<h5>Python</h5>\n<p><img src=\"/files/images/case-fsharp-ml-1.png\" class=\"img-fluid\" alt=\"\" /></p>\n<h5>FSharp</h5>\n<p><img src=\"/files/images/case-fsharp-ml-2.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>As it can be seen, getting that immediate feedback makes it easier to see whether the code is behaving the way it should. Although useful, this environment is not ideal when experimenting and making tweaks to the code. Additionally, because of the lack of a graphical user interface, the navigation can be less than ideal. Fortunately there is a solution out there that provides the same level of interactivity along with a graphical user interface that allows for ad-hoc experimentation and re-running of code at different points in time which is essential when developing machine learning applications.</p>\n<h4>Jupyter Notebooks</h4>\n<p>Much of machine learning deals with experimentation. Experimentation involves having a way to tweak parameters and evaluate the results. In addition, experiments should have a way of being documented and reproduced. As alluded to previously, such development environment and capabilities can be found in <a href=\"https://jupyter.org/\">Jupyter Notebook</a>.</p>\n<p>As mentioned on the project's website:</p>\n<blockquote class=\"blockquote\">\n<p>The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more.</p>\n</blockquote>\n<p>Jupyter Notebooks work with various languages which include but are not limited to Python and F# and can be run both on local machines as well as via a hosted service. One such service is Microsoft's <a href=\"https://notebooks.azure.com/\">Azure Notebooks</a> which allows you to use Jupyter Notebooks in the cloud for free. All you need is to have a Microsoft account. Below are screenshots of what that environment looks like in both Python and F#.</p>\n<h5>Python</h5>\n<p><img src=\"/files/images/case-fsharp-ml-3.png\" class=\"img-fluid\" alt=\"\" /></p>\n<h5>FSharp</h5>\n<p><img src=\"/files/images/case-fsharp-ml-4.png\" class=\"img-fluid\" alt=\"\" /></p>\n\n<p>F#, is comparable to Python on many of the features that make Python a great language for both development and machine learning. However, there are some areas where F# provides additional safety and functionality that greatly improve the way in which machine learning applications are built.</p>\n<h3>Typing</h3>\n<p>As previously mentioned, F# is a statically typed language. However, it also makes use of type inference which means that declaring types is not always required. With the help of the compiler, based on the structure of functions and variables it is possible to determine which type an object or variable is. This has several advantages. One advantage of being strongly typed is that when the types are declared the code becomes self-documented because it is easier to deduce what functions are doing based on the types being passed in and returned. Another advantage is that it makes it harder to write bad code. Having the compiler help you when you write your code allows you to find errors prior to compilation or running the code. This along with the REPL gives you additional reassurance that your code is executing the intended logic.</p>\n<p>Python is making strides in acquiring some of that functionality with the introduction of type hints in version 3.6. However, this has not always been a core feature of the language and is only in its nascent stages.</p>\n<h3>Immutability</h3>\n<p>As a functional language, immutability is something that is a native part of the language. While in some cases it can change the way in which code is written, immutability has one advantage, especially when it comes to machine learning. With immutability, parallelization can be fully exploited for those algorithms which take advantage of it.</p>\n\n<p>In this writeup, I went over how F# is comparable to Python on many of the features that make it such a popular language such as library support, succinctness, general purpose and interactivity. However, F# has additional capabilities such as static typing and immutability that further enhance its capabilities as a language for machine learning. That is not to say that one is better than the other as they both are more adept for performing certain tasks. When it comes to machine learning it does become a matter of choice as they are both robust, powerful and strongly supported languages. Therefore next time you're looking to build a machine learning application, hopefully you give F# a try. Happy coding!</p>\n<h6>Resources</h6>\n<p><a href=\"https://spectrum.ieee.org/at-work/innovation/the-2018-top-programming-languages\">2018 Top Programming Languages</a><br />\n<a href=\"https://www.manning.com/books/get-programming-with-f-sharp\">Get Programming with F#</a><br />\n<a href=\"https://github.com/dotnet/machinelearning-samples\">ML.NET Machine Learning Samples</a></p>\n","tags":["mlnet","machine learning","fsharp","dotnet","programming","python","ai","dotnetcore","artificial intelligence","fsadvent","artificialintelligence","machinelearning","data science","functional programming"]},{"title":"Operationalizing Machine Learning with ML.NET, Azure DevOps and Azure Container Instances","contentType":"posts","date":"2018-11-26 23:50:23","url":"/posts/azdevops-mlnet-aci/","content":"\n<p>Azure DevOps, formerly known as Visual Studio Team Services (VSTS), helps individuals and organizations plan, collaborate and ship products faster. One if its noteworthy services, Azure Pipelines, helps developers build Continuous Integration (CI) and Continuous Delivery (CD) pipelines that automate and standardize the build, test and deploy phases of the software development process. In addition, Azure Pipelines provides native container support and works with any language, platform and cloud. Machine learning like software development is also a process that includes a build, test and deploy phase which makes it a good candidate for automation and standardization. At Build 2018, Microsoft announced <a href=\"https://github.com/dotnet/machinelearning\">ML.NET</a>, an open-source, cross-plaform machine learning framework for .NET. If we were to put all of these tools and services together, it means that we can automate and standardize the training of a machine learning model built with ML.NET, package it into a Docker container and deploy it to Azure Container Instances (ACI). In this writeup, I will go through the process of building a CI/CD pipeline in Azure Devops that trains, packages and deploys an ML.NET machine learning model to predict which class an Iris flower belongs to using a variety of measurements. Source code for this project can be found at this <a href=\"https://github.com/lqdev/mlnetazdevopssample\">link</a>.</p>\n\n<ul>\n<li><a href=\"https://git-scm.com/\">Git</a></li>\n<li><a href=\"https://github.com/\">GitHub Account</a></li>\n<li><a href=\"https://www.microsoft.com/net/download\">.NET Core SDK</a></li>\n<li><a href=\"https://azure.microsoft.com/en-us/free/\">Azure Account</a></li>\n</ul>\n\n<p>Because the purpose of this post is to demonstrate the functionality of Azure Devops and not that of ML.NET, I'll start with a pre-built application. For some more information and detail into the functionality of ML.NET, check out the official documentation <a href=\"https://docs.microsoft.com/en-us/dotnet/machine-learning/\">page</a> as well as some of my previous posts:</p>\n<ul>\n<li><a href=\"http://luisquintanilla.me/2018/08/21/serverless-machine-learning-mlnet-azure-functions/\">Serverless Machine Learning with ML.NET and Azure Functions</a></li>\n<li><a href=\"http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/\">Deploy .NET Machine Learning Models with ML.NET, ASP.NET Core, Docker and Azure Container Instances</a>.</li>\n</ul>\n<p>The application used in this writeup contains three .NET Core projects within it. One is a class library which is what we'll use to wrap ML.NET functionality for training models as well as loading pre-trained models that will then be used to make predictions. Another is a .NET Core console application which references the class library to train and persist an ML.NET model. Finally, there's the ASP.NET Core Web API which also references the class library application to load the pre-trained model created by the console application and then makes predictions via HTTP. This application can be utilized and deployed standalone but in this writeup it will be packaged into a Docker image that will then be deployed to Azure Container Instances.</p>\n<p><img src=\"/files/images/azdevops-mlnet-1.png\" class=\"img-fluid\" alt=\"\" /></p>\n<h3>Class Library</h3>\n<p>The class library can be found in the <code>MLModel</code> directory. The class library defines the observation and prediction data classes which can be found in the <code>IrisData.cs</code> and <code>IrisPrediction.cs</code> files respectively. Additionally, the <code>Model</code> class contains helper methods that train and save machine learning models, load pre-trained models and use these models to make predictions.</p>\n<h3>Console Application</h3>\n<p>In the solution directory we also have a console application in the <code>ModelTrainer</code> directory. This application references the class library in the <code>MLModel</code> directory to train and persist the machine learning model.</p>\n<h3>API</h3>\n<p>The <code>ModelApi</code> directory contains an ASP.NET Core Web API application that references the <code>MLModel</code> class library project to load the pre-trained model that is trained by the <code>ModelTrainer</code> console application and makes predictions via HTTP. The logic for making predictions can be found in the <code>PredictController.cs</code> class in the <code>Controllers</code> directory of the <code>ModelApi</code> application.</p>\n\n<p>Conceptually, when the application is built and deployed manually, the machine learning model is defined and developed inside the <code>MLModel</code> class library. Once satisfied with the model, the class library is built. The console application which references the <code>MLModel</code> class library is built as well as run to train and persist a classification model in a file called <code>model.zip</code>. The <code>MLModel</code> class library is also referenced in the <code>ModelApi</code> ASP.NET Core project. Because <code>ModelApi</code> is the application we're looking to deploy in order to expose our pre-trained machine learning model, we need to find a way to package it for deployment. We'll be deploying <code>ModelApi</code> using Azure Container Instances which means we need to create a Docker image of the project that will then be pushed to a Docker registry where it will be made available for public consumption. The building of multiple projects as well as the building, publishing and deployment of the Docker image to Azure Container Instances can be standardized and automated using Azure DevOps. The rest of this write-up will focus on demonstrating step-by-step how to operationalize this machine learning application via CI/CD pipelines in Azure DevOps using Azure Pipelines.</p>\n<h3>Getting The Code</h3>\n<p>Before getting started, the first thing you'll want to do is fork the <a href=\"https://github.com/lqdev/mlnetazdevopssample\">mlnetazdevopssample</a> GitHub repository into your own GitHub account.</p>\n<p><img src=\"/files/images/azdevops-mlnet-2.png\" class=\"img-fluid\" alt=\"\" /></p>\n<h3>Creating the Project</h3>\n<p>Navigate to <a href=\"https://devops.azure.com\">https://devops.azure.com</a>, click <code>Start Free</code> and follow the prompts to either create a new account or sign into your existing account.</p>\n<p><img src=\"/files/images/azdevops-mlnet-3.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Once logged in, click <code>Create Project</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-4.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Enter the name of your project as well as a short description. Then, click <code>Create</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-5.png\" class=\"img-fluid\" alt=\"\" /></p>\n\n<p>Using Azure Pipelines, we'll configure a CI pipeline for the build and packaging steps of our application. Below is an illustration of all the steps involved in our CI pipeline:</p>\n<p><img src=\"/files/images/azdevops-mlnet-6.png\" class=\"img-fluid\" alt=\"\" /></p>\n<ol>\n<li>Build the class library application</li>\n<li>Build the console application</li>\n<li>Train and persist the ML.NET Model by running the console application.</li>\n<li>Copy ML.NET model file created by console application into ASP.NET Core Web API application directory</li>\n<li>Build ASP.NET Core Web API application</li>\n<li>Build Docker image</li>\n<li>Push Docker image to Docker Hub</li>\n</ol>\n<h3>CI Pipeline Setup</h3>\n<p>Once the project is created, in the main project page, hover over <code>Pipelines</code> and click on <code>Builds</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-7.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>In the <code>Builds</code> pipeline page, click <code>New pipeline</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-8.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Select GitHub as the source and connect your GitHub account with Azure DevOps.</p>\n<p><img src=\"/files/images/azdevops-mlnet-9.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Once you have authorized Azure DevOps to use your GitHub account, select the repository and branch that will be used for this build pipeline. In our case, we'll be using the master branch of the <code>mlnetazdevopssample</code> repository. When finished configuring, click <code>Continue</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-10.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>The next step will be to select the jobs to execute in our pipeline. Because there are multiple steps in this build pipeline, let's start with an <code>Empty Job</code> and customize it to our needs.</p>\n<p><img src=\"/files/images/azdevops-mlnet-11.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>From inside the build pipeline page, before we start adding jobs, lets select the agent that will execute the jobs. For this pipeline, select the <code>Hosted Ubuntu 1604</code> option from the dropdown.</p>\n<p><img src=\"/files/images/azdevops-mlnet-12.png\" class=\"img-fluid\" alt=\"\" /></p>\n<h3>1. Build the Class Library Application</h3>\n<p>The first step in our CI Pipeline will be to build our class library which contains methods that wrap the training, loading and prediction functionality of the ML.NET framework and persisted models.</p>\n<p>To achieve that, we'll add a .NET Core task to our <code>Agent Job 1</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-13.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Once added to the pipeline, let's configure this task. To make it more descriptive, we can give it a name such as <code>Build Class Library</code>. Because this task will be responsible for building the .NET Core class library, we'll leave the default <code>build</code> Command setting as is.</p>\n<p><img src=\"/files/images/azdevops-mlnet-14.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>The other setting we'll want to configure is the <code>Working Directory</code>. We can do so by clicking the <code>Advanced</code> tab.</p>\n<p><img src=\"/files/images/azdevops-mlnet-15.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>For this task we'll use the <code>MLModel</code> directory.</p>\n<p><img src=\"/files/images/azdevops-mlnet-16.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>When finished with the configuration, click <code>Save &amp;amp; Queue</code> → <code>Save</code> on the top toolbar.</p>\n<p><img src=\"/files/images/azdevops-mlnet-17.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Enter a detailed comment describing the change to the pipeline and click <code>Save</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-18.png\" class=\"img-fluid\" alt=\"\" /></p>\n<h3>2. Building The Console Application</h3>\n<p>Once we've built the class library application which we'll reference from the .NET Core console and ASP.NET Core Web API applications, it's time to build the console application which will serve the purpose of training and persisting the ML.NET model.</p>\n<p>Similar to the previous step, add a new .NET Core <em>build</em> task to the pipeline. The only setting that will change for this task is the <code>Working Directory</code> which will have the value of <code>ModelTrainer</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-19.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Although not required, when finished configuring the task, click <code>Save &amp;amp; Queue</code> → <code>Save</code> to save and comment the changes to the pipeline.</p>\n<h3>3. Train and persist the ML.NET Model</h3>\n<p>Now that our console application is built, it's time to run it in order to train and persist the ML.NET model. To do so, we'll add another .NET Core task. The difference is that the <code>Command</code> setting will now be configured with the <code>run</code> value.</p>\n<p><img src=\"/files/images/azdevops-mlnet-20.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>The <code>Working Directory</code> will be set to <code>ModelTrainer</code> like in the previous task.</p>\n<p><img src=\"/files/images/azdevops-mlnet-21.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Remember to save and comment the new changes to the pipeline.</p>\n<h3>4. Copy ML.NET Model to Web API Directory</h3>\n<p>After the console application is run and the ML.NET model is trained, it is persisted in a file called <code>model.zip</code> inside the <code>ModelTrainer</code> directory. We can use this persisted version of the model to make predictions from both the console application or any other application of our choice. In this case, we'll be making predictions via an ASP.NET Core Web API. In order for our API to reference this file, we need to copy it into the root directory of our <code>ModelApi</code> directory. A way to perform that task is via bash script. To add a bash script to our pipeline, all we need to do is add a Bash task to it.</p>\n<p><img src=\"/files/images/azdevops-mlnet-22.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Once added to our pipeline, it's time to configure the task. We'll set the <code>Type</code> setting to <code>Inline</code> which will bring up a text box for us to type in the script.</p>\n<p><img src=\"/files/images/azdevops-mlnet-23.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Inside of the text box, enter the following content:</p>\n<pre><code class=\"language-bash\"># Write your commands here\n\ncp ../ModelTrainer/model.zip .\n\n# Use the environment variables input below to pass secret variables to this script\n</code></pre>\n<p>This command will copy the <code>model.zip</code> file from the <code>ModelTrainer</code> directory to the <code>ModelApi</code> directory.</p>\n<p>We can set the <code>Working Directory</code> of this step to <code>ModelApi</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-24.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Once finished, save and comment the new changes to the pipeline.</p>\n<h3>5. Build ASP.NET Core Web API application</h3>\n<p>Now that we have the necessary files inside our <code>ModelApi</code> application, it's time to build it. We'll add a .NET Core task to our pipeline and set the <code>Command</code> to <code>build</code>. The <code>Working Directory</code> will be <code>ModelApi</code> like the previous task.</p>\n<p>Save and comment the new changes to the pipeline when finished.</p>\n<p><img src=\"/files/images/azdevops-mlnet-25.png\" class=\"img-fluid\" alt=\"\" /></p>\n<h3>6. Build ASP.NET Core Web API Docker Image</h3>\n<p>The method of deployment for the ASP.NET Core Web API application is via containers. Therefore, after building the application, we have to build a Docker image for it that can then be pushed to a Docker registry of your choice. To build a Docker image, we'll add a Docker task to our pipeline.</p>\n<p><img src=\"/files/images/azdevops-mlnet-26.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>When we configure the task, we'll start off by setting the <code>Container Registry Type</code> to <code>Container Registry</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-27.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>This will prompt the setup of a service connection to a Docker registry if one does not already exist.</p>\n<p><img src=\"/files/images/azdevops-mlnet-28.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>The Docker registry type we'll be using is Docker Hub. Give the connection a name, enter the credentials to your Docker Hub account and click <code>Verify this connection</code> to make sure that your credentials are valid and a connection can be established with Docker Hub. When finished click <code>OK</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-29.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>The <code>Command</code> setting will be <code>build</code> so we can leave the default as is as well as the <code>Dockerfile</code> setting which will use the Dockerfile in the root <code>mlnetazdevopssample</code> directory.</p>\n<p><img src=\"/files/images/azdevops-mlnet-30.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Finally, we'll configure the <code>Image name</code> setting. The convention we'll use is <code>&amp;lt;docker-hub-username&amp;gt;/&amp;lt;image-name&amp;gt;</code>. In my case, <code>lqdev</code> is my Docker Hub username and I'll name my image <code>mlnetazdevopssample</code> resulting in <code>lqdev/mlnetazdevopssample</code>. Additionally, check the <code>Include latest tag</code> checkbox to have every build be the latest as opposed to tagging it with versions numbers.</p>\n<p><img src=\"/files/images/azdevops-mlnet-31.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Remember to save and comment the recent changes to the pipeline.</p>\n<h3>7. Push Docker Image to Docker Hub</h3>\n<p>The last step in our CI pipeline is to push our newly built image to Docker Hub. To do so we'll use anoter Docker task.</p>\n<p><img src=\"/files/images/azdevops-mlnet-32.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Like in the previous task, we'll set the <code>Container registry type</code> to <code>Container Registry</code>. Set the <code>Docker registry service connection</code> to the most recently created connection by selecting it from the dropdown. We'll be changing our <code>Command</code> to <code>push</code> and set the <code>Image name</code> to the name of the image built in the previous step. The naming convention is <code>&amp;lt;docker-hub-username&amp;gt;/&amp;lt;image-name&amp;gt;:latest</code>. The latest tag was added by our previous Docker build task so make sure that you include it in this task.</p>\n<p>Once finished, click <code>Save &amp;amp; Queue</code> → <code>Save &amp;amp; Queue</code>. As opposed to only clicking <code>Save</code>, this action will manually trigger the CI pipeline.</p>\n<p><img src=\"/files/images/azdevops-mlnet-33.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Don't forget to comment your changes and click <code>Save &amp;amp; queue</code> to kick off the CI pipeline.</p>\n<p><img src=\"/files/images/azdevops-mlnet-34.png\" class=\"img-fluid\" alt=\"\" /></p>\n<h3>Monitoring the Build</h3>\n<p>When the build starts, you can click on <code>Builds</code> under the <code>Pipelines</code> section on the left pane.</p>\n<p><img src=\"/files/images/azdevops-mlnet-35.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Select the first build from the list to get more details on the build.</p>\n<p><img src=\"/files/images/azdevops-mlnet-36.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>This will take you to the logs which show the status of the pipeline near real-time.</p>\n<p><img src=\"/files/images/azdevops-mlnet-37.png\" class=\"img-fluid\" alt=\"\" /></p>\n<h3>Confirming CI Pipeline Success</h3>\n<p>If the build is successful, navigate to <a href=\"https://hub.docker.com/\">https://hub.docker.com/</a> to check whether the Docker image was pushed to the registry.</p>\n<p><img src=\"/files/images/azdevops-mlnet-38.png\" class=\"img-fluid\" alt=\"\" /></p>\n\n<p>Now that we have our CI pipeline set up which will build and package our application, it's time to deploy it. We could do this ourselves or automate it using a CD pipeline. Our application wil be deployed to Azure Container Instances which is an Azure service that offers a quick way to run containers without having to worry about the management of virtual machines or orchestration services. The steps involved in our CD pipeline are the following:</p>\n<ol>\n<li>Create Azure Resource Group for deployment</li>\n<li>Deploy application to Azure Container Instances.</li>\n</ol>\n<h3>CD Pipeline Setup</h3>\n<p>To get started setting up a CD pipeline, from the Azure DevOps project main page, hover over <code>Pipelines</code> and click on <code>Releases</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-39.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Once in that page, click on <code>New pipeline</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-40.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>As with our CI pipeline, we'll start off with an <code>Empty Job</code> which we'll configure at a later time.</p>\n<p><img src=\"/files/images/azdevops-mlnet-41.png\" class=\"img-fluid\" alt=\"\" /></p>\n<h3>Triggering Deployments</h3>\n<p>Once our pipeline is created, it's time to configure it. The first thing we'll want to do is add an artifact. An artifact can be a variety of things including the output of our build pipeline. In our case, the end our CI pipeline will be the trigger for our CD pipeline. To add an artifact, click <code>Add an artifact</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-42.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>In the configuration form, set the <code>Source type</code> to <code>Build</code> and the <code>Source</code> to the name of the CI pipeline created in the previous steps. When finished, click <code>Add</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-43.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>After configuring our artifact, it's time to configure the steps in the CD pipeline. To do so, click on the <code>Stage 1</code> option in the <code>Stages</code> section of the release pipeline page and change the name to something more descriptive.</p>\n<p><img src=\"/files/images/azdevops-mlnet-44.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>When finished, close out the form and click on the hyperlink below the stage title.</p>\n<p><img src=\"/files/images/azdevops-mlnet-45.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>You should now be on a page similar to the CI pipeline job configuration page. On this page, we'll want to click on the <code>Agent Job</code> panel to set the <code>Agent pool</code> setting to <code>Hosted Ubuntu 1604</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-46.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Once that is complete, it's time to configure the tasks in the CD pipeline.</p>\n<h3>1. Create Azure Resource Group</h3>\n<p>Start off adding an <code>Azure CLI</code> task to the pipeline. In this task we'll create a resource group in Azure to which we'll deploy our application to.</p>\n<p><img src=\"/files/images/azdevops-mlnet-47.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Before doing anything else, link DevOps to an Azure Subscription by selecting one from the dropdown and clicking <code>Authorize</code> which will prompt you to authenticate your subscription.</p>\n<p><img src=\"/files/images/azdevops-mlnet-48.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Once an Azure subscription has been linked, let's change the <code>Script Location</code> setting to <code>Inline Script</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-49.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>In the <code>Inline Script</code> text box enter the following:</p>\n<pre><code class=\"language-bash\">az group create --name mlnetazdevopssampleresourcegroup --location eastus\n</code></pre>\n<p>This script will create a resource group in Azure called <code>mlnetazdevopssampleresourcegroup</code> that is located in <code>eastus</code>. Both of these are configurable to your preference.</p>\n<h3>2. Deploy Docker Image to Azure Container Instances</h3>\n<p>The next and final step in our CD pipeline is the deployment to Azure Container Instances. To deploy our application, we'll add another <code>Azure CLI</code> task. This time, since we already configured our <code>Azure subscription</code> in the previous task, we can select the service connection as opposed to a subscription from the dropdown.</p>\n<p><img src=\"/files/images/azdevops-mlnet-50.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Like in the previous task, our script will be inline.</p>\n<p><img src=\"/files/images/azdevops-mlnet-51.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>In the <code>Inline Script</code> text box enter the following:</p>\n<pre><code class=\"language-bash\">az container create --resource-group mlnetazdevopssampleresourcegroup --name mlnetcontainer --image lqdev/mlnetazdevopssample:latest --ports 80 --ip-address public\n</code></pre>\n<p>This script creates a container in the resource group created by the previous task of the pipeline with the name <code>mlnetcontainer</code> from the Docker image that was pushed to Docker Hub by the CI pipeline. Additionally, it opens up port 80 and assigns a publicly accessible IP address for the container to be accessed externally.</p>\n<p>Once this step has been configured, make sure to save and comment all your changes by clicking <code>Save</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-52.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p><img src=\"/files/images/azdevops-mlnet-53.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Then, to make it easily recognizable, edit the name of the pipeline by hovering near <code>New release pipeline</code> and clicking on the pencil icon.</p>\n<p><img src=\"/files/images/azdevops-mlnet-54.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Make sure to save and comment your changes.</p>\n\n<p>In the previous steps, we configured CI and CD pipelines. However, we have still not fully automated the triggers that kick off both of these.</p>\n<h3>CI Pipeline Trigger</h3>\n<p>First, lets start off by automating the CI pipeline. To do so, go the project's main page, hover over <code>Pipelines</code> and click on <code>Builds</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-55.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>This will take you to the CI pipeline page. While on this page, click <code>Edit</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-56.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Then, click on <code>Triggers</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-57.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Once on this page, check the <code>Enable continous integration</code> checkbox and save and comment your changes by clicking <code>Save &amp;amp; Queue</code> → <code>Save</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-58.png\" class=\"img-fluid\" alt=\"\" /></p>\n<h3>CD Pipeline Trigger</h3>\n<p>To automate the CD pipeline trigger, click on <code>Releases</code> under the <code>Pipelines</code> page to automate the CD pipeline.</p>\n<p><img src=\"/files/images/azdevops-mlnet-59.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Once on the CD pipeline's page, click <code>Edit</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-60.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Then, click on the lightning icon in the Artifacts section which will show a configuration form. In this form, toggle the <code>Continuous deployment trigger</code> setting to <code>Enabled</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-61.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>When finished, save and comment your changes.</p>\n\n<p>Although going forward builds and deployments will be started when new changes are checked into the master branch of the <code>mlnetazdevopssample</code> repository, for demonstration purposes we will manually kick off the CI/CD pipelines we have just configured. To do so, click on <code>Builds</code> under the <code>Pipelines</code> section on the left pane.</p>\n<p><img src=\"/files/images/azdevops-mlnet-62.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>From the CI pipeline page click <code>Queue</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-63.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>This will prompt a modal to show up in which you can just click <code>Queue</code> to start the build.</p>\n<p><img src=\"/files/images/azdevops-mlnet-64.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>This will kick off a new CI build which subsequently will also kick off the CD pipeline of your application.</p>\n\n<p>If all is successful, a Docker image of an ASP.NET Core Web API application will be deployed to Azure Container Instances which can be accessed via a public IP address.</p>\n<p>To see whether the deployment worked, navigate to <a href=\"https://portal.azure.com/\">https://portal.azure.com/</a> and click on <code>Resource groups</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-65.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>At this point, you should see the resource group that was created by the CD pipeline. If that's the case, click on it.</p>\n<p><img src=\"/files/images/azdevops-mlnet-66.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>This will then show a page that displays the container that was deployed to this resource group. Click on that.</p>\n<p><img src=\"/files/images/azdevops-mlnet-67.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>The container page will display diagnostic and configuration information about the container. The information we're interested in is the <code>IP address</code>. Hover to the right of it and click on the icon that says <code>Click to copy</code>. This will copy the address to the clipboard.</p>\n<p><img src=\"/files/images/azdevops-mlnet-68.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>In an application like Postman or Insomnia, make an HTTP POST request to <code>http://&amp;lt;ip-address&amp;gt;/api/predict</code> where <code>ip-address</code> is the public IP address of the container in Azure with the following body.</p>\n<pre><code class=\"language-json\">{\n    &amp;quot;SepalLength&amp;quot;:3.3,\n    &amp;quot;SepalWidth&amp;quot;:1.6,\n    &amp;quot;PetalLength&amp;quot;:0.2,\n    &amp;quot;PetalWidth&amp;quot;:5.1\n}\n</code></pre>\n<p>If successful, the response will be <code>Iris-virginica</code>.</p>\n<p><img src=\"/files/images/azdevops-mlnet-69.png\" class=\"img-fluid\" alt=\"\" /></p>\n\n<p>In this writeup, we operationalized the building, packaging and deployment of an ML.NET application that predicts the class of an Iris flower using a variety of mesurements with Azure DevOps. We created both a Continous Integration as well as a Continous Delivery pipeline which deploys the Docker image of an ASP.NET Core Web API to Azure Container Instances. Keep in mind this is just one way of doing it and Azure DevOps is flexible in how all of these tasks and workflows are configured to meet your requirements. Happy coding!</p>\n<h6>Resources</h6>\n<p><a href=\"https://github.com/dotnet/machinelearning-samples\">ML.NET Samples</a>\n<a href=\"https://www.youtube.com/watch?v=bUTBBS1TECc\">DevOps for Data Science</a></p>\n","tags":["mlnet","machinelearning","ai","dotnet","dotnetcore","devops","azure","docker","containers","microsoft","artificialintelligence","programming","webapi","aci","development"]},{"title":"Blogging Tools","contentType":"posts","date":"2018-11-18 00:07:36","url":"/posts/blog-tools/","content":"\n<p>These are the tools I use to write this blog:</p>\n<ul>\n<li><strong>Editor</strong>: <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a></li>\n<li><strong>Markup</strong>: <a href=\"https://daringfireball.net/projects/markdown/\">Markdown</a></li>\n<li><strong>Static Site Generator</strong>: <a href=\"https://hexo.io/\">Hexo</a></li>\n<li><strong>Blog Theme</strong>: <a href=\"https://github.com/probberechts/hexo-theme-cactus\">Cactus</a></li>\n<li><strong>Hosting</strong>: <a href=\"https://www.namecheap.com/\">Namecheap</a></li>\n</ul>\n\n<p>I have gotten a few questions regarding what I use to write this blog so I thought it would be a good idea to write a post about it. In this writeup I'll discuss what I use for writing posts as well as building and hosting the site.</p>\n\n<h3>Editor</h3>\n<p>For writing, I use Visual Studio Code. Visual Studio Code is an open source, cross-platform text editor. One of the nice features it includes is Markdown Preview. Markdown Preview allows you to see how the content would look when rendered as a web page. Additionally, Visual Studio Code allows you to split panes. What that provides is the ability to write and preview the content side-by-side.</p>\n<p><img src=\"/files/images/blog-tools-1.PNG\" class=\"img-fluid\" alt=\"\" /></p>\n<h3>Markup</h3>\n<p>I write my content in Markdown. Markdown is a plain text formatting syntax that allows authors to write easy-to-read and easy-to-write plain text formats that are then converted to HTML. It can take a while to get used to the syntax especially when including images, links and code snippets but once you get comfortable with it, it becomes natural and no different than writing sentences in plain text. While this is not a requirement for everyone, since a fair amount of my content includes code snippets, Markdown makes it easy to include such content in the language of choice.</p>\n\n<h3>Static Site Generation</h3>\n<p>While Markdown helps me format my plain text, at some point it needs to be converted into HTML or some format that can be rendered on the web. To help with that, I use Hexo. Hexo is an open source blog framework that allows you to write content using Markdown or other languages and generates static files which have a theme applied to them. Hexo is built on JavaScript, therefore you'll need NodeJS to configure it.</p>\n<h3>Theme</h3>\n<p>Hexo has numerous themes that can be used to style your site. The one I use is the <code>Cactus</code> theme.</p>\n\n<p>For hosting, I use Namecheap. There is no special reason for this other than I had a slot available in an existing hosting plan. My method of deployment is via FTP. Since my site is all static assets, I upload them into a folder that is publicly accessible. With that in mind, by having my site made up of static assets I have the flexibility of hosting it on an S3 bucket or Azure Storage. This is the method I'm looking to use for deployments and hosting in the near future. Here is a <a href=\"https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-static-website\">link</a> that details how to host a static site on Azure Storage.</p>\n","tags":["markdown","vscode","tools","development","blogging"]},{"title":"Serverless Machine Learning with ML.NET and Azure Functions","contentType":"posts","date":"2018-08-21 19:13:47","url":"/posts/serverless-machine-learning-mlnet-azure-functions/","content":"\n<p>In a previous blog <a href=\"http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/\">post</a>, I explored how to build and deploy machine learning models built with the <code>ML.NET</code> framework using an ASP.NET Core Web API, Docker and Azure Container Instances. While this is certainly a good way to deploy such models especially those that are critical and require high availability and/or consist of long-running processes, it's not the case when those requirements are not needed. In such cases serverless computing makes more sense from a cost and resource utilization standpoint. Therefore, in this blog post I will go over how to train a classification model with <code>ML.NET</code> and deploy it using Azure Functions. Source code for this post can be found at the following <a href=\"https://github.com/lqdev/azfnmlnetdemo\">link</a>.</p>\n\n<p>Prior to starting, make sure you have all of the necessary software to build this project. Although this project was built on a system running Ubuntu 16.04 it should work cross-platform.</p>\n<ul>\n<li><a href=\"https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest\">Azure CLI</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local\">Azure Functions Core Tools Version 2.x</a></li>\n<li><a href=\"https://www.microsoft.com/net/download\">.NET Core SDK 2.0+</a></li>\n</ul>\n\n<p>Before writing any code we want to configure our Azure environment. To do so we'll be using the Azure CLI. Although in these examples I am providing the resource group name, storage account name and function application name feel free to use one of your choosing. Naming is not as important for resource group or storage account but definitely is the case for the application.</p>\n<p>Fist we want to log into our account using the following command</p>\n<pre><code class=\"language-bash\">az login\n</code></pre>\n<p>This will guide you through a series of prompts that will eventually result in you being logged in. To make sure you are logged in you can use the <code>account</code> command.</p>\n<pre><code class=\"language-bash\">az account list\n</code></pre>\n<p>The following output should appear if successfull.</p>\n<pre><code class=\"language-bash\">[\n  {\n    &amp;quot;cloudName&amp;quot;: &amp;quot;AzureCloud&amp;quot;,\n    &amp;quot;id&amp;quot;: &amp;quot;&amp;lt;YOUR-ID&amp;gt;&amp;quot;,\n    &amp;quot;isDefault&amp;quot;: true,\n    &amp;quot;name&amp;quot;: &amp;quot;Pay-As-You-Go&amp;quot;,\n    &amp;quot;state&amp;quot;: &amp;quot;Enabled&amp;quot;,\n    &amp;quot;tenantId&amp;quot;: &amp;quot;&amp;lt;YOUR-TENANT-ID&amp;gt;&amp;quot;,\n    &amp;quot;user&amp;quot;: {\n      &amp;quot;name&amp;quot;: &amp;quot;&amp;lt;YOUR-USERNAME&amp;gt;&amp;quot;,\n      &amp;quot;type&amp;quot;: &amp;quot;user&amp;quot;\n    }\n  }\n]\n</code></pre>\n<p>Next, we want to create a resource group to contain all of our Azure resources for this application.</p>\n<pre><code class=\"language-bash\">az group create --name azfnmlnetdemo --location eastus\n</code></pre>\n<p>Once our resource group is created, it's time to start adding resources for it. First we'll add a storage account which will contain our trained model.</p>\n<pre><code class=\"language-bash\">az storage account create --name azfnmlnetdemostorage --location eastus --resource-group azfnmlnetdemo --sku Standard_LRS\n</code></pre>\n<p>Then we'll create an Serverless Function Application and link it to our storage account. We'll want to create a unique name for it. An easy way to do so is to add the date to the end of the name of our application (i.e. myappname20180816).</p>\n<pre><code class=\"language-bash\">az functionapp create --name azfnmlnetdemo20180821 --storage-account azfnmlnetdemostorage --consumption-plan-location eastus --resource-group azfnmlnetdemo\n</code></pre>\n<p>The final step in the environment setup is to set the runtime of our Serverless Function Application in the Application Settings to <code>beta</code> which supports <code>.NET Core</code>.</p>\n<pre><code class=\"language-bash\">az functionapp config appsettings set --name azfnmlnetdemo20180821 --resource-group azfnmlnetdemo --settings FUNCTIONS_EXTENSION_VERSION=beta\n</code></pre>\n<p>Now we're ready to build our machine learning model and upload it to our storage account</p>\n\n<p>Once our environment is set up we can start building our solution. The first step is to create a directory and initialize our solution inside of it.</p>\n<h3>Set Up The Solution</h3>\n<pre><code class=\"language-bash\">mkdir azfnmlnetdemo\ncd azfnmlnetdemo\ndotnet new sln\n</code></pre>\n<h3>Create The Model Project</h3>\n<p>Then we want to create a console project for our model and add it to our solution.</p>\n<pre><code class=\"language-bash\">dotnet new console -o model\ndotnet sln add model/model.csproj\n</code></pre>\n<h3>Add Dependencies</h3>\n<p>Since we’ll be using the <code>ML.NET</code> framework, we need to add it to our model project.</p>\n<pre><code>cd model\ndotnet add package Microsoft.ML\ndotnet restore\n</code></pre>\n<h3>Download The Data</h3>\n<p>Before we start training the model, we need to download the data we’ll be using to train. We do so by creating a directory called <code>data</code> and downloading the data file onto there.</p>\n<pre><code class=\"language-bash\">mkdir data\ncurl -o data/iris.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n</code></pre>\n<p>If we take a look at the data file, it should look something like this:</p>\n<pre><code class=\"language-bash\">5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n4.6,3.1,1.5,0.2,Iris-setosa\n5.0,3.6,1.4,0.2,Iris-setosa\n5.4,3.9,1.7,0.4,Iris-setosa\n4.6,3.4,1.4,0.3,Iris-setosa\n5.0,3.4,1.5,0.2,Iris-setosa\n4.4,2.9,1.4,0.2,Iris-setosa\n4.9,3.1,1.5,0.1,Iris-setosa\n</code></pre>\n<h3>Train The Model</h3>\n<p>Now that we have all our dependencies set up, it’s time to train our model. I leveraged the demo that is used on the <a href=\"https://www.microsoft.com/net/learn/machine-learning-and-ai/get-started-with-ml-dotnet-tutorial\">ML.NET Getting-Started website</a>.</p>\n<h4>Define Data Structures</h4>\n<p>In the root directory of our <code>model</code> project, let’s create two classes called <code>IrisData</code> and <code>IrisPrediction</code> which will define our features and predicted attribute respectively. Both of them will use <code>Microsoft.ML.Runtime.Api</code> to add the property attributes.</p>\n<p>Here is what our <code>IrisData</code> class looks like:</p>\n<pre><code class=\"language-csharp\">using Microsoft.ML.Runtime.Api;\n\nnamespace model\n{\n    public class IrisData\n    {\n        [Column(&amp;quot;0&amp;quot;)]\n        public float SepalLength;\n\n        [Column(&amp;quot;1&amp;quot;)]\n        public float SepalWidth;\n\n        [Column(&amp;quot;2&amp;quot;)]\n        public float PetalLength;\n\n        [Column(&amp;quot;3&amp;quot;)]\n        public float PetalWidth;\n\n        [Column(&amp;quot;4&amp;quot;)]\n        [ColumnName(&amp;quot;Label&amp;quot;)]\n        public string Label;\n    }\n}\n</code></pre>\n<p>Similarly, here is the <code>IrisPrediction</code> class:</p>\n<pre><code class=\"language-csharp\">using Microsoft.ML.Runtime.Api;\n\nnamespace model\n{\n    public class IrisPrediction\n    {\n        [ColumnName(&amp;quot;PredictedLabel&amp;quot;)]\n        public string PredictedLabels;\n    }\n}\n</code></pre>\n<h4>Build the Training Pipeline</h4>\n<p>The way the <code>ML.NET</code> computations process data is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. Therefore, we can create a class called <code>Model</code> to perform all of these tasks for us.</p>\n<pre><code class=\"language-csharp\">using Microsoft.ML.Data;\nusing Microsoft.ML;\nusing Microsoft.ML.Runtime.Api;\nusing Microsoft.ML.Trainers;\nusing Microsoft.ML.Transforms;\nusing Microsoft.ML.Models;\nusing System;\nusing System.Threading.Tasks;\n\nnamespace model\n{\n    class Model\n    {\n\n        public static async Task&amp;lt;PredictionModel&amp;lt;IrisData,IrisPrediction&amp;gt;&amp;gt; Train(LearningPipeline pipeline, string dataPath, string modelPath)\n        {\n            // Load Data\n            pipeline.Add(new TextLoader(dataPath).CreateFrom&amp;lt;IrisData&amp;gt;(separator:','));\n\n            // Transform Data\n            // Assign numeric values to text in the &amp;quot;Label&amp;quot; column, because\n            // only numbers can be processed during model training\n            pipeline.Add(new Dictionarizer(&amp;quot;Label&amp;quot;));\n\n            // Vectorize Features\n            pipeline.Add(new ColumnConcatenator(&amp;quot;Features&amp;quot;, &amp;quot;SepalLength&amp;quot;, &amp;quot;SepalWidth&amp;quot;, &amp;quot;PetalLength&amp;quot;, &amp;quot;PetalWidth&amp;quot;));\n\n            // Add Learner\n            pipeline.Add(new StochasticDualCoordinateAscentClassifier());\n\n            // Convert Label back to text\n            pipeline.Add(new PredictedLabelColumnOriginalValueConverter() {PredictedLabelColumn = &amp;quot;PredictedLabel&amp;quot;});\n\n            // Train Model\n            var model = pipeline.Train&amp;lt;IrisData,IrisPrediction&amp;gt;();\n\n            // Persist Model\n            await model.WriteAsync(modelPath);\n\n            return model;\n        }\n    }\n}\n</code></pre>\n<p>In addition to building our pipeline and training our machine learning model, the <code>Model</code> class also serialized and persisted the model for future use in a file called <code>model.zip</code>.</p>\n<h3>Test The Model</h3>\n<p>Now that we have our data structures and model training pipeline set up, it’s time to test everything to make sure it’s working. We’ll put our logic inside of our <code>Program.cs</code> file.</p>\n<pre><code class=\"language-csharp\">using System;\nusing Microsoft.ML;\n\nnamespace model\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n\n            string dataPath = &amp;quot;model/data/iris.txt&amp;quot;;\n\n            string modelPath = &amp;quot;model/model.zip&amp;quot;;\n\n            var model = Model.Train(new LearningPipeline(),dataPath,modelPath).Result;\n\n            // Test data for prediction\n            var prediction = model.Predict(new IrisData()\n            {\n                SepalLength = 3.3f,\n                SepalWidth = 1.6f,\n                PetalLength = 0.2f,\n                PetalWidth = 5.1f\n            });\n\n            Console.WriteLine($&amp;quot;Predicted flower type is: {prediction.PredictedLabels}&amp;quot;);\n        }\n    }\n}\n</code></pre>\n<p>All set to run. We can do so by entering the following command from our solution directory:</p>\n<pre><code class=\"language-bash\">dotnet run -p model/model.csproj\n</code></pre>\n<p>Once the application has been run, the following output should display on the console.</p>\n<pre><code class=\"language-bash\">Automatically adding a MinMax normalization transform, use 'norm=Warn' or\n'norm=No' to turn this behavior off.Using 2 threads to train.\nAutomatically choosing a check frequency of 2.Auto-tuning parameters: maxIterations = 9998.\nAuto-tuning parameters: L2 = 2.667734E-05.\nAuto-tuning parameters: L1Threshold (L1/L2) = 0.Using best model from iteration 882.\nNot training a calibrator because it is not needed.\nPredicted flower type is: Iris-virginica\n</code></pre>\n<p>Additionally, you’ll notice that a file called <code>model.zip</code> was created in the root directory of our model project. This persisted model can now be used outside of our application to make predictions, but first we need to upload it to our Azure Storage account.</p>\n<h3>Upload The Model</h3>\n<p>Now that we have a trained model and it has been persisted to the <code>model.zip</code> file, it's time to upload it to Azure Storage so that it is available to our Azure Functions application.</p>\n<p>To get started with that, first we need the access keys for our storage account. You can get those by using the following command.</p>\n<pre><code class=\"language-bash\">az storage account keys list --account-name azfnmlnetdemostorage --resource-group azfnmlnetdemo\n</code></pre>\n<p>The result of that command should return your primary and secondary keys. You can use either one for the following steps.</p>\n<p>Although we can upload directly to the account, it's best to create a container to upload our model to. To keep it simple, I'll call the container <code>models</code>.</p>\n<pre><code class=\"language-bash\">az storage container create --name models --account-key &amp;lt;YOUR-ACCOUNT-KEY&amp;gt; --account-name azfnmlnetdemostorage --fail-on-exist\n</code></pre>\n<p>Once our container's created, we can upload our <code>model.zip</code> file to it.</p>\n<pre><code class=\"language-bash\">az storage blob upload --container-name models --account-name azfnmlnetdemostorage --file model/model.zip --name model.zip\n</code></pre>\n<p>To verify that the file has been uploaded, you can list the files inside the <code>models</code> storage container.</p>\n<pre><code class=\"language-bash\">az storage blob list --container-name models --account-name azfnmlnetdemostorage --output table\n</code></pre>\n<p>That command should produce output similar to that below:</p>\n<pre><code class=\"language-bash\">Name       Blob Type    Blob Tier    Length    Content Type     Last Modified              Snapshot\n---------  -----------  -----------  --------  ---------------  -------------------------  ----------\nmodel.zip  BlockBlob                 4373      application/zip  2018-08-21T19:26:09+00:00\n</code></pre>\n<p>That's all there is to the upload process. It's now time to build our Azure Functions Application</p>\n\n<h3>Initialize Azure Function Project</h3>\n<p>In our solution directory, we want to create a new directory for our Azure Function project</p>\n<pre><code class=\"language-bash\">mkdir serverlessfunctionapp\ndotnet sln add serverlessfunctionapp/serverlessfunctionapp.csproj\n</code></pre>\n<p>Then, we can scaffold an Azure Functions project inside our newly created <code>serverlessfunctionapp</code> project directory using Azure Functions Core Tools</p>\n<pre><code class=\"language-bash\">cd serverlessfunctionapp\nfunc init\n</code></pre>\n<p>At this point you will be prompted to select the runtime for your application. For this application select <code>dotnet</code>.</p>\n<p>This will generate a few files in the <code>serverlessfunctionapp</code> directory. Keep in mind though that this does not create the function.</p>\n<h3>Add Dependencies</h3>\n<p>Before we create any functions, we need to add the dependencies for our Azure Functions application. Since we'll be using <code>Microsoft.ML</code> in our Azure Function application, we'll need to add it as a dependency. From the <code>serverlessfunctionapp</code> enter the following command:</p>\n<pre><code class=\"language-bash\">dotnet add package Microsoft.ML\ndotnet restore\n</code></pre>\n<h3>Create Serverless Function</h3>\n<p>Once we've added the dependencies it's time to create a new function. To do so we'll use the Azure Functions Core Tools <code>new</code> command. Although not required, it's good practice to separate functions and related files into their own directory.</p>\n<pre><code class=\"language-bash\">mkdir Predict\ncd Predict\nfunc new\n</code></pre>\n<p>At this time you will be prompted to select a template. For our classification model, we'll be using an HttpTrigger which is exactly what it sounds like. An HTTP request is what calls or invokes our function. With that being said, select the <code>HttpTrigger</code> option.</p>\n<p>You will then be prompted to enter a name for your function. You can use any name but to make things easy, name it the same as the directory the function is in. Once that process is complete, there should be a file called <code>Predict.cs</code> inside our <code>serverlessfunctionapp/Predict</code> directory. This is where we'll write the logic for our application.</p>\n<h3>Define Data Structures</h3>\n<p>We'll also be making use of the IrisData and IrisPrediction classes inside our <code>Predict</code> function. Therefore, we need to create classes for them inside our <code>Predict</code> directory. The content will be the same as when we trained our model with the exception of the namespace which will now be <code>serverlessfunctionapp.Predict</code>. The content of those files should look like the code below:</p>\n<pre><code class=\"language-csharp\">//IrisData.cs\nusing Microsoft.ML.Runtime.Api;\n\nnamespace serverlessfunctionapp.Predict\n{\n    public class IrisData\n    {\n        [Column(&amp;quot;0&amp;quot;)]\n        public float SepalLength;\n\n        [Column(&amp;quot;1&amp;quot;)]\n        public float SepalWidth;\n\n        [Column(&amp;quot;2&amp;quot;)]\n        public float PetalLength;\n\n        [Column(&amp;quot;3&amp;quot;)]\n        public float PetalWidth;\n\n        [Column(&amp;quot;4&amp;quot;)]\n        [ColumnName(&amp;quot;Label&amp;quot;)]\n        public string Label;\n    }\n}\n\n//IrisPrediction.cs\nusing Microsoft.ML.Runtime.Api;\n\nnamespace serverlessfunctionapp.Predict\n{\n    public class IrisPrediction\n    {\n        [ColumnName(&amp;quot;PredictedLabel&amp;quot;)]\n        public string PredictedLabels;\n    }\n}\n</code></pre>\n<h3>Write Function Logic</h3>\n<p>With our dependencies and data structures set up, it's time to write our function logic to make predictions. The first thing we want to do is replace the <code>Run</code> method inside the <code>Predict</code> class with the following code.</p>\n<pre><code class=\"language-csharp\">public static IActionResult Run(\n    [HttpTrigger(AuthorizationLevel.Function, &amp;quot;get&amp;quot;, &amp;quot;post&amp;quot;, Route = null)]HttpRequest req,\n    [Blob(&amp;quot;models/model.zip&amp;quot;, FileAccess.Read, Connection = &amp;quot;AzureWebJobsStorage&amp;quot;)] Stream serializedModel,\n    TraceWriter log)\n{\n    // Workaround for Azure Functions Host\n    if (typeof(Microsoft.ML.Runtime.Data.LoadTransform) == null ||\n        typeof(Microsoft.ML.Runtime.Learners.LinearClassificationTrainer) == null ||\n        typeof(Microsoft.ML.Runtime.Internal.CpuMath.SseUtils) == null ||\n        typeof(Microsoft.ML.Runtime.FastTree.FastTree) == null)\n    {\n        log.Error(&amp;quot;Error loading ML.NET&amp;quot;);\n        return new StatusCodeResult(500);\n    }\n\n    //Read incoming request body\n    string requestBody = new StreamReader(req.Body).ReadToEnd();\n\n    log.Info(requestBody);\n\n    //Bind request body to IrisData object\n    IrisData data = JsonConvert.DeserializeObject&amp;lt;IrisData&amp;gt;(requestBody);\n\n    //Load prediction model\n    var model = PredictionModel.ReadAsync&amp;lt;IrisData, IrisPrediction&amp;gt;(serializedModel).Result;\n\n    //Make prediction\n    IrisPrediction prediction = model.Predict(data);\n\n    //Return prediction\n    return (IActionResult)new OkObjectResult(prediction.PredictedLabels);\n}\n</code></pre>\n<p>There are a few notable change worth looking at. One of them is the workaround at the beginning of the function.</p>\n<pre><code class=\"language-csharp\">if (typeof(Microsoft.ML.Runtime.Data.LoadTransform) == null ||\n    typeof(Microsoft.ML.Runtime.Learners.LinearClassificationTrainer) == null ||\n    typeof(Microsoft.ML.Runtime.Internal.CpuMath.SseUtils) == null ||\n    typeof(Microsoft.ML.Runtime.FastTree.FastTree) == null)\n{\n    log.Error(&amp;quot;Error loading ML.NET&amp;quot;);\n    return new StatusCodeResult(500);\n}\n</code></pre>\n<p>There are some issues with Azure Functions and ML.NET Assemblies which are being worked on by both teams at Microsoft (see <a href=\"https://github.com/Azure/azure-functions-host/issues/3190\">Github Issue</a>). In the meantime, it's safe to just include that code in there.</p>\n<p>The other addition to note is the method signature. As you can see, I have added an additional parameter called <code>serializedModel</code> which is decorated by the <code>Blob</code> attribute.</p>\n<pre><code class=\"language-csharp\">[Blob(&amp;quot;models/model.zip&amp;quot;, FileAccess.Read, Connection = &amp;quot;AzureWebJobsStorage&amp;quot;)] Stream serializedModel\n</code></pre>\n<p>What this code is doing is telling the function to import the blob <code>model.zip</code> as a <code>Stream</code> and bind it to <code>serializedModel</code>. Using additional arguments, I tell my function to only have <code>Read</code> access to the <code>model.zip</code> blob inside the <code>models</code> container which can be accessed with the <code>AzureWebJobsStorage</code> connection string. Right now that last part might seem confusing, but this is something we configured when we set up the Azure environment and linked <code>azfnmlnetdemostorage</code> account with our <code>azfnmlnetdemo20180821</code> serverless function app using the <code>--storage-account</code> option. Although the production environment is configured, if we try to test our application locally we won't be able to access our storage account because we have not configured the connection string locally. We can do so by looking in the <code>local.settings.json</code> file inside our <code>serverlessfunctionapp</code> directory. The contents should look like the following.</p>\n<pre><code class=\"language-json\">{\n  &amp;quot;IsEncrypted&amp;quot;: false,\n  &amp;quot;Values&amp;quot;: {\n    &amp;quot;AzureWebJobsStorage&amp;quot;: &amp;quot;&amp;quot;,\n    &amp;quot;AzureWebJobsDashboard&amp;quot;: &amp;quot;&amp;quot;,\n    &amp;quot;FUNCTIONS_WORKER_RUNTIME&amp;quot;: &amp;quot;dotnet&amp;quot;\n  }\n}\n</code></pre>\n<p>Our function running locally will look in this file, try to find <code>AzureWebJobsStorage</code> and use the connection string value in the <code>Predict</code> function. To get the connection string for our <code>azfnmlnetdemostorage</code> account, enter the following command.</p>\n<pre><code class=\"language-bash\">az storage account show-connection-string --name azfnmlnetdemostorage\n</code></pre>\n<p>The output of that command should look like the following:</p>\n<pre><code class=\"language-json\">{\n  &amp;quot;connectionString&amp;quot;: &amp;quot;&amp;lt;YOUR-CONNECTION-STRING&amp;gt;&amp;quot;\n}\n</code></pre>\n<p>At this point, you just need to copy the value of <code>connectionString</code> to your <code>local.settings.json</code> file and replace the current empty string for <code>AzureWebJobsStorage</code>. It's important to note that it's okay to paste the connection string in here since the <code>local.settings.json</code> file is not committed to version control. (See <code>.gitignore</code> inside <code>serverlessfunctionapp</code> directory). Now the application is ready to be tested locally.</p>\n<h3>Testing The Function Locally</h3>\n<p>To test the application, first build your project by entering the following command from the <code>serverlessfunctionapp</code> directory.</p>\n<pre><code class=\"language-bash\">dotnet build\n</code></pre>\n<p>Then, navigate to the build directory <code>./bin/Debug/netstandard2.0</code> and enter the following command:</p>\n<pre><code class=\"language-bash\">func host start\n</code></pre>\n<p>Finally, using a tool like Postman or Insomnia make an HTTP POST request to the <code>http://localhost:7071/api/Predict</code> endpoint with the following body:</p>\n<pre><code class=\"language-json\">{\n  &amp;quot;SepalLength&amp;quot;: 3.3,\n  &amp;quot;SepalWidth&amp;quot;: 1.6,\n  &amp;quot;PetalLength&amp;quot;: 0.2,\n  &amp;quot;PetalWidth&amp;quot;: 5.1\n}\n</code></pre>\n<p>If everything is set up correctly, you should receive the following output</p>\n<pre><code class=\"language-bash\">Iris-virginica\n</code></pre>\n<p>Once satisfied with testing, press <code>Ctrl + C</code> to stop the application.</p>\n\n<h3>Push Build</h3>\n<p>Great! Now on to the final step, deploying our application to production. Since we already configured everything it should only require a few commands to do so.</p>\n<p>First, make sure you are logged in. Using Azure Functions Core Tools log in with the following command:</p>\n<pre><code class=\"language-bash\">func azure login\n</code></pre>\n<p>Like with the Azure CLI, you will follow a series of prompts to log into your account.</p>\n<p>Once you have successfully logged in, it's time to publish our application to Azure. From the <code>serverlessfunctionapp</code> directory enter the following command.</p>\n<pre><code class=\"language-bash\">func azure functionapp publish azfnmlnetdemo20180821\n</code></pre>\n<p>When our deployment is complete, we can check whether our function was published successfully by using the following command.</p>\n<pre><code class=\"language-bash\">func azure functionapp list-functions azfnmlnetdemo20180821\n</code></pre>\n<p>The output should look similar to that below.</p>\n<pre><code class=\"language-bash\">Functions in azfnmlnetdemo20180821:\n    Predict - [httpTrigger]\n</code></pre>\n<h3>Configure Platform</h3>\n<p>For the last part of the deployment step, we'll need to head over to the Azure Portal. To do so, visit <a href=\"https://portal.azure.com\">https://portal.azure.com</a> and log in.</p>\n<p>Once logged in, type the name of your application into the search bar at the top of the page and select your Azure Function application of type <code>App Service</code></p>\n<p><img src=\"/files/images/azfnmlnetdemo1.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Then, from the accordion element on the left, select the top-most item with your appplication name on it. Then, select the <code>Platform features</code> tab and open the <code>Application settings</code> option.</p>\n<p><img src=\"/files/images/azfnmlnetdemo2.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>When the <code>Application settings</code> page loads, change the <code>Platform</code> setting to <code>64-bit</code>. The reason for this is <code>ML.NET</code> has to be built and run on a 64-bit environment due to some of its native dependencies.</p>\n<p><img src=\"/files/images/azfnmlnetdemo3.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>That's all there is to it.</p>\n<h3>Test The Deployed Function</h3>\n<p>Now it's time to test our deployed function. We can do so from the portal by going back to the accordion and selecting the function name below the <code>Functions</code> parent element and clicking on the <code>Test</code> button on the far right. Doing so will show a form that will allow us to test our application. Make sure the <code>HTTP method</code> option is set to POST. In the text area for the <code>Request body</code> paste the following content:</p>\n<pre><code class=\"language-json\">{\n  &amp;quot;SepalLength&amp;quot;: 3.3,\n  &amp;quot;SepalWidth&amp;quot;: 1.6,\n  &amp;quot;PetalLength&amp;quot;: 0.2,\n  &amp;quot;PetalWidth&amp;quot;: 5.1\n}\n</code></pre>\n<p>Once the form is filled in, click <code>Run</code> at the top of the page and if successful <code>Iris-virginica</code> should show up in the <code>Output</code> area.</p>\n<p><img src=\"/files/images/azfnmlnetdemo4.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>To test the function outside the portal, you can click on the <code>Get function URL</code> link next to the <code>Run</code> button and make an HTTP POST request using that link.</p>\n\n<p>In this writeup, we trained a classification model that predicts a class of flower using Microsoft's <code>ML.NET</code> framework. Then, we exposed this model for inference via an Azure Functions serverless application. In doing so, we can more efficiently manage our cost as well as our resource utilization. Happy coding!</p>\n<h6>Resources</h6>\n<p><a href=\"https://docs.microsoft.com/en-us/azure/azure-functions/scripts/functions-cli-create-serverless\">Create a function app for serverless code execution</a><br />\n<a href=\"https://docs.microsoft.com/en-us/azure/storage/common/storage-azure-cli\">Using the Azure CLI 2.0 with Azure Storage</a><br />\n<a href=\"https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local\">Work with Azure Functions Core Tools</a></p>\n","tags":["serverless","azurefunctions","mlnet","machinelearning","dotnet","dotnetcore","csharp","microsoft","devops","artificialintelligence","ai"]},{"title":"Hacker News Vue Top Stories Client","contentType":"posts","date":"2018-06-21 18:30:38","url":"/posts/hn-vue-topstories-client/","content":"\n<p>Over the past few years, I've used AngularJS as my front-end framework for most web projects. With the release of Angular and React, I tried both of them and while both are great tools the learning took significantly longer because I also had to learn the bundler systems and in the case of Angular, TypeScript. Although the CLI tools go a long way towards automating a lot of the setup process, building an application from scratch in these frameworks is not intuitive especially for a newcomer. In the early days of Vue, I was able to attend a Meetup presentation by <a href=\"https://twitter.com/youyuxi\">Evan You</a> which went over the philosophy of Vue as well as its capabilities. I was instantly interested because of how easy it was to get started. Since that presentation, I have not had time to try it but with AngularJS being phased out, I felt it was about time to start exploring another framework. To get a better understanding of the framework's features prior to using CLI tools, I created a standalone application that relies only on NPM packages. In this writeup, I will create a Hacker News client with Vue that calls the Hacker News API and displays the top 50 stories. Source code for this project can be found in the following <a href=\"https://github.com/lqdev/hnvuedemo\">repository</a>.</p>\n\n<ul>\n<li><a href=\"https://nodejs.org/en/\">Node</a></li>\n</ul>\n\n<p>The first thing we want to do is create a directory for our project and initialize it with NPM</p>\n<pre><code class=\"language-bash\">mkdir hnvuedemo\ncd hnvuedemo\nnpm init -y\n</code></pre>\n\n<p>Next, we want to install our dependencies. (Express is optional)</p>\n<pre><code class=\"language-bash\">npm install --save vue axios express\n</code></pre>\n\n<p>We can set up a server to host and serve our content. This is optional though because the application should still work when the <code>index.html</code> file is opened in the browser.</p>\n<p>In our root project directory, we can create the <code>server.js</code> file and add the following content:</p>\n<pre><code class=\"language-javascript\">var express = require(&amp;quot;express&amp;quot;);\n\nconst PORT = 3000 || process.env.PORT;\n\nvar app = express();\n\napp.use(express.static(&amp;quot;.&amp;quot;));\n\napp.get(&amp;quot;/&amp;quot;, (req, res) =&amp;gt; {\n  res.sendFile(&amp;quot;index.html&amp;quot;);\n});\n\napp.listen(PORT, () =&amp;gt; {\n  console.log(`Listening on port ${PORT}`);\n});\n</code></pre>\n\n<h3>Create The View</h3>\n<p>Now it's time to create our application. Let's start by scaffolding the <code>index.html</code> which is where our application will be displayed.</p>\n<pre><code class=\"language-html\">&amp;lt;!DOCTYPE html&amp;gt;\n&amp;lt;html&amp;gt;\n&amp;lt;head&amp;gt;\n    &amp;lt;script src=&amp;quot;./node_modules/vue/dist/vue.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=&amp;quot;./node_modules/axios/dist/axios.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;\n&amp;lt;/head&amp;gt;\n&amp;lt;body&amp;gt;\n    &amp;lt;!--App Content Here--&amp;gt;\n\n    &amp;lt;!--App Scripts Here--&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n</code></pre>\n<p>We want to add the scripts to the <code>Vue</code> and <code>axios</code> packages in our <code>head</code> element. We'll hold off on the content for now.</p>\n<h3>Get Data</h3>\n<p>Our data source for this project will be the Hacker News <a href=\"https://github.com/HackerNews/API\">API</a>. To interact with it, we'll be using the <code>axios</code> NPM package. The data collection happens over several HTTP requests, therefore to help with it we'll be creating a file called <code>apihelpers.js</code> in our root project directory that will contain functions to get and manipulate the data.</p>\n<h4>Get Top Stories</h4>\n<p>The first thing we want to do is get a list of top stories. We can do so via the <code>/topstories</code> endpoint. The response of this request returns a list of ids which can then be used in conjunction with the <code>/item/{id}</code> endpoint to get the individual story data. We can then validate the story object to make sure it is a story as opposed to a user or job posting and return a list of all the story objects. The functions that will help us with that are <code>getIds</code>, <code>isStory</code> and <code>extractStories</code>.</p>\n<pre><code class=\"language-javascript\">/**\n * Checks whether the item is a story\n * @param {Object} story - Story object\n */\nfunction isStory(story) {\n  return story.type == &amp;quot;story&amp;quot;;\n}\n\n/**\n * Gets ids of stories\n * @param {string} url - Url to fetch story ids\n */\nfunction getIds(url) {\n  return axios.get(url);\n}\n\n/**\n * \n * @param {Array&amp;lt;Object&amp;gt;} - List of resolved promises \n */\nfunction extractStories(...responses) {\n    var stories = responses.map(story =&amp;gt; story = story.data);\n    return stories.filter(isStory);\n}\n</code></pre>\n<h3>Create Vue App</h3>\n<p>Now that we have our helper methods to make calls to the API, we can create our application.</p>\n<p>We can start by creating our Vue instance which is where our application will live. We can create a file called <code>app.js</code> in our root project directory and add the following content:</p>\n<pre><code class=\"language-javascript\">//Components Go Here\n\nnew Vue({\n  el: &amp;quot;#app&amp;quot;,\n  data: {\n    title: &amp;quot;Hacker News Vue Simple Reader&amp;quot;,\n    loading: true,\n    topStories: []\n  },\n  methods: {\n    getTopStories: function() {\n      getIds(\n        &amp;quot;https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty&amp;quot;\n      )\n        .then(ids =&amp;gt; buildRequest(ids.data.splice(0, 50))) //Only get first 50\n        .then(getStories)\n        .then(axios.spread(extractStories))\n        .then(stories =&amp;gt; {\n          this.topStories = stories;\n          this.loading = false;\n        })\n        .catch(e =&amp;gt; console.log(e));\n    }\n  },\n  created: function() {\n    this.getTopStories();\n  }\n});\n</code></pre>\n<p>There's a lot happening here, so let's break it down based on the properties of the object passed to the Vue constructor.</p>\n<p>The <code>el</code> property is like a selector which tells the application which element it should operate on. The value <code>#app</code> tells it to look for an element where the <code>id</code> attribute is <code>app</code>. This of course can be anything of your choosing.</p>\n<p>The <code>data</code> property specifies the properties that contain the data of our application. These properties are reactive which means that whenever they change, that change is reflected in our view. As such, even if they have no value when the application starts, in order for them to automatically change when data is passed to them, they need to be declared in the <code>data</code> property. In our application, we have a <code>title</code> property which will be the title of our web page, <code>topStories</code> which is where we'll store the list of Hacker News stories and <code>loading</code> which we'll use to let us know when data is being loaded into our application.</p>\n<p>In the <code>methods</code> property, we define functions that we want our application to use. In this case, I created the <code>getTopStories</code> method which chains together all the functions defined in our <code>apihelpers.js</code> file to return the top 50 stories on Hacker News.</p>\n<p>Finally, Vue has instance lifecycle hooks. The <code>created</code> property defines what should happen when our instance is created. In our case, we want to call the <code>getTopProperties</code> method which is defined in our <code>methods</code> property to load the data, update our <code>topStories</code> data property and set <code>loading</code> to <code>false</code> because our data has loaded successfully.</p>\n<h3>Add App to View</h3>\n<p>Now that we have created the logic of our application, it's time to add it to our view. We can do so by adding the <code>apihelpers.js</code> and <code>app.js</code> files to our <code>index.html</code> file via <code>script</code> elements.</p>\n<pre><code class=\"language-html\">&amp;lt;!DOCTYPE html&amp;gt;\n&amp;lt;html&amp;gt;\n&amp;lt;head&amp;gt;\n    &amp;lt;script src=&amp;quot;./node_modules/vue/dist/vue.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=&amp;quot;./node_modules/axios/dist/axios.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;\n&amp;lt;/head&amp;gt;\n&amp;lt;body&amp;gt;\n    &amp;lt;!--App Content Here--&amp;gt;\n\n    &amp;lt;!--App Scripts Here--&amp;gt;\n    &amp;lt;script src=&amp;quot;apihelpers.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=&amp;quot;app.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n</code></pre>\n<h3>Display Data</h3>\n<p>Although we can now use our application logic inside of our view, we still can't see any of it because we have not added elements to display it. To view our application data, we can add the following code to our <code>index.html</code> file below the <code>&amp;lt;!--App Content Here--&amp;gt;</code> section.</p>\n<pre><code class=\"language-html\">&amp;lt;div id=&amp;quot;app&amp;quot;&amp;gt;\n    &amp;lt;h2&amp;gt;{{title}}&amp;lt;/h2&amp;gt;\n    &amp;lt;h5 v-if=&amp;quot;loading&amp;quot;&amp;gt;Loading...&amp;lt;/h5&amp;gt;\n    &amp;lt;story v-for=&amp;quot;story in topStories&amp;quot; :key=&amp;quot;story.id&amp;quot; :story=&amp;quot;story&amp;quot;&amp;gt;&amp;lt;/story&amp;gt;\n&amp;lt;/div&amp;gt;\n</code></pre>\n<p>The final <code>index.html</code> contents should look like the content below:</p>\n<pre><code class=\"language-html\">&amp;lt;!DOCTYPE html&amp;gt;\n&amp;lt;html&amp;gt;\n&amp;lt;head&amp;gt;\n    &amp;lt;script src=&amp;quot;./node_modules/vue/dist/vue.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=&amp;quot;./node_modules/axios/dist/axios.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;\n&amp;lt;/head&amp;gt;\n&amp;lt;body&amp;gt;\n    &amp;lt;div id=&amp;quot;app&amp;quot;&amp;gt;\n        &amp;lt;h2&amp;gt;{{title}}&amp;lt;/h2&amp;gt;\n        &amp;lt;h5 v-if=&amp;quot;loading&amp;quot;&amp;gt;Loading...&amp;lt;/h5&amp;gt;\n        &amp;lt;story v-for=&amp;quot;story in topStories&amp;quot; :key=&amp;quot;story.id&amp;quot; :story=&amp;quot;story&amp;quot;&amp;gt;&amp;lt;/story&amp;gt;\n    &amp;lt;/div&amp;gt;\n    &amp;lt;script src=&amp;quot;apihelpers.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;\n    &amp;lt;script src=&amp;quot;app.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n</code></pre>\n<p>As mentioned earlier, we created an element, in this case a <code>div</code> that has the <code>id</code> attribute with a value of <code>app</code>. This is how our application knows where to display our content. Inside of an <code>h2</code> element, we display our <code>title</code> data property. Below it, we have an <code>h5</code> element that displays the text &amp;quot;Loading...&amp;quot;. However, this is to only be displayed when our <code>loading</code> data property is <code>true</code>. If not, it should not be visible. We can achieve this conditional rendering via the <code>v-if</code> directive. This directive evaluates the expression inside of it and renders content based on its truthiness.</p>\n<p>Finally, there's one last piece that looks like an element, but not one of the built-in HTML elements. So then, what is it? It's a component. The Vue website defines a component as &amp;quot;...a reusable Vue instance with a name: in this case, <code>&amp;lt;story&amp;gt;</code>. We can use this component as a custom element inside a root Vue instance...&amp;quot;. In our component, the <code>v-for</code> directive is what it sounds like. It creates a sequence of <code>story</code> components based on a list of objects defined in our <code>Vue</code> instance's <code>data</code> property. Our <code>story</code> component iterates over the <code>topStories</code> data property and assigns the value of the individual object in the list to the variable <code>story</code>. We bind the <code>id</code> property of the <code>story</code> object to the <code>key</code> attribute of the component and pass in the entire object to the component via the <code>story</code> prop. A prop is a custom attribute that you can register for the component. We can use props to pass data into the component. In all cases, the <code>:</code> prefix on the attributes and props of the component are shorthand for the <code>v-bind</code> directive which dynamically binds an expression to an attribute or component prop.</p>\n<p>With all that being said your next question might be, how does the view know about this component? The answer is it doesn't at least not until you define it which is what we'll do next in our <code>app.js</code> file. In order for our <code>story</code> component to be usable, we need to define it above the instantiation of our <code>Vue</code> instance. The definition looks like the following:</p>\n<pre><code class=\"language-javascript\">Vue.component('story',{\n    props: ['story'],\n    template: `\n        &amp;lt;div&amp;gt;\n            &amp;lt;h3&amp;gt;&amp;lt;a :href=&amp;quot;story.url&amp;quot; target=&amp;quot;_blank&amp;quot;&amp;gt;{{story.title}}&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;\n        &amp;lt;/div&amp;gt;\n    `\n});\n</code></pre>\n<p>Like our <code>Vue</code> instance, let's unpack what's happening here. The first parameter is a string with the name of our component. The <code>props</code> property is a list of the props or custom attributes that are accepted by our component. The <code>template</code> property is where we set up the template of what will be rendered in place of our component in the view. In our case, we'll have an <code>h3</code> element with a nested <code>a</code> element whose <code>href</code> attribute is the <code>url</code> property of our <code>story</code> object and the display text is the <code>title</code> property of our <code>story</code> object.</p>\n\n<p>At this point, our application should be ready to run. You can either start the server with the following command <code>npm start</code>  and navigate to <code>http://localhost:3000</code> or open the <code>index.html</code> page in the browser of your choice. The result should look like the screenshot below:</p>\n<p><img src=\"/files/images/hnvueclient.png\" class=\"img-fluid\" alt=\"\" /></p>\n\n<p>In this writeup, I built a standalone Hacker News client that displays the top 50 stories using Vue while also highlighting some of its main features. Overall, I really enjoyed building this application. The setup process was extremely simple and after a few hours looking through the excellent documentation and working through some bugs, it took less than two hours to get this application up and running from start to finish. For prototyping and learning purposes, Vue is great because you're able to take advantage of the core features of the framework without having too much overhead. Although this may not be the most appropriate way to build production-ready applications, it's nice to know you can have a modern web application with minimal setup required. My next steps will be to continue learning some of the other features the framework provides and eventually build up to learning how to use the CLI tools.</p>\n","tags":["vue","web development","programming","api","development","frontend","js"]},{"title":"Classification with F# ML.NET Models","contentType":"posts","date":"2018-06-13 18:19:05","url":"/posts/mlnet-classification-fsharp/","content":"\n<p>In a previous <a href=\"http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/\">post</a>, I detailed how to build and deploy C# <code>ML.NET</code> models with <code>Docker</code> and <code>ASP.NET Core</code>. With inspiration from <a href=\"https://twitter.com/csharpfritz\">Jeff Fritz</a>, I have been learning F# for the past week and a half or so. When trying to think of projects to start practicing my F#, porting over the code I had built in C# naturally came to mind. After overcoming many obstacles and with much guidance from <a href=\"https://github.com/voronoipotato\">Alan Ball</a> and <a href=\"https://twitter.com/isaac_abraham\">Isaac Abraham</a> whose F# <a href=\"https://www.amazon.com/Get-Programming-guide-NET-developers/dp/1617293997/ref=sr_1_1?ie=UTF8&amp;amp;qid=1528929802&amp;amp;sr=8-1&amp;amp;keywords=get+programming+with+F%23\">book</a> I highly recommend, I was able to successfully port over the main parts of my code which highlight <code>ML.NET</code> functionality. In this writeup, I will port a C# <code>ML.NET</code> classification model to F# which predicts the type of flower based on four numerical measurement inputs. I tried to keep the organization of this post nearly identical to that of the C# article where possible. Sample code for this project can be found at the following <a href=\"https://github.com/lqdev/fsmlnetdemo\">link</a>.</p>\n\n<p>This project was built on a Linux PC but should work cross-platform on Mac and Windows.</p>\n<ul>\n<li><a href=\"https://www.microsoft.com/net/download/linux\">.NET Core SDK 2.0+</a></li>\n<li><a href=\"https://fsharp.org/use/linux/\">Ionide Extension - Option 1</a></li>\n<li><a href=\"https://www.nuget.org/packages/Microsoft.ML/\">ML.NET v 0.2.0</a></li>\n</ul>\n\n<p>The first thing we want to do is create a folder for our solution.</p>\n<pre><code class=\"language-bash\">mkdir fsharpmlnetdemo\n</code></pre>\n<p>Then, we want to create a solution inside our newly created folder.</p>\n<pre><code class=\"language-bash\">cd fsharpmlnetdemo\ndotnet new sln\n</code></pre>\n\n<h3>Setting Up The Model Project</h3>\n<p>First, we want to create the project. From the solution folder enter:</p>\n<pre><code class=\"language-bash\">dotnet new console -o model -lang f#\n</code></pre>\n<p>Now we want to add this new project to our solution.</p>\n<pre><code class=\"language-bash\">dotnet sln add model/model.fsproj\n</code></pre>\n<h3>Adding Dependencies</h3>\n<p>Since we’ll be using the <code>ML.NET</code> framework, we need to add it to our <code>model</code> project.</p>\n<pre><code class=\"language-bash\">dotnet add model/model.fsproj package Microsoft.ML\n</code></pre>\n<h3>Download The Data</h3>\n<p>Before we start training the model, we need to download the data we’ll be using to train. We do so by downloading the data file into our root solution directory.</p>\n<pre><code class=\"language-bash\">curl -o iris-data.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n</code></pre>\n<p>If we take a look at the data file, it should look something like this:</p>\n<pre><code class=\"language-bash\">5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n4.6,3.1,1.5,0.2,Iris-setosa\n5.0,3.6,1.4,0.2,Iris-setosa\n5.4,3.9,1.7,0.4,Iris-setosa\n4.6,3.4,1.4,0.3,Iris-setosa\n5.0,3.4,1.5,0.2,Iris-setosa\n4.4,2.9,1.4,0.2,Iris-setosa\n4.9,3.1,1.5,0.1,Iris-setosa\n</code></pre>\n<h3>Train Model</h3>\n<p>Now that we have all our dependencies set up, it’s time to build our model. I leveraged the demo that is used on the <code>ML.NET</code> Getting-Started <a href=\"https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet/get-started/linux/ubuntu16-04\">website</a>.</p>\n<h4>Defining Data Structures</h4>\n<p>At the time of this writing, <code>ML.NET</code> version <code>0.2.0</code> does not fully support F# Records. A workaround for this are mutable classes. Not really inline with F# paradigms, but it should be good enough.</p>\n<p>In the <code>Program.fs</code> file of our <code>model</code> project directory, let’s create two mutable classes called <code>IrisData</code> and <code>IrisPrediction</code> which will define our features and predicted attribute respectively. Both of them will use <code>Microsoft.ML.Runtime.Api</code> to add the property attributes.</p>\n<p>Here is what our <code>IrisData</code> class looks like:</p>\n<pre><code class=\"language-fsharp\">type IrisData() =\n    [&amp;lt;Column(ordinal = &amp;quot;0&amp;quot;);DefaultValue&amp;gt;]\n    val mutable public SepalLength: float32\n\n    [&amp;lt;Column(ordinal = &amp;quot;1&amp;quot;);DefaultValue&amp;gt;]\n    val mutable public SepalWidth: float32\n\n    [&amp;lt;Column(ordinal = &amp;quot;2&amp;quot;);DefaultValue&amp;gt;]\n    val mutable public PetalLength:float32\n\n    [&amp;lt;Column(ordinal = &amp;quot;3&amp;quot;);DefaultValue&amp;gt;]\n    val mutable public PetalWidth:float32\n\n    [&amp;lt;Column(ordinal = &amp;quot;4&amp;quot;,name=&amp;quot;Label&amp;quot;);DefaultValue&amp;gt;]\n    val mutable public Label: string\n</code></pre>\n<p>Similarly, here is the <code>IrisPrediction</code> class:</p>\n<pre><code class=\"language-fsharp\">type IrisPrediction() =\n    [&amp;lt;ColumnName &amp;quot;PredictedLabel&amp;quot;;DefaultValue&amp;gt;] val mutable public PredictedLabel : string\n</code></pre>\n<h4>Building Training Pipeline</h4>\n<p>The way the <code>ML.NET</code> computations process is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. We can add that logic inside the <code>main</code> function of our <code>Program.fs</code> file.</p>\n<pre><code class=\"language-fsharp\">let dataPath = &amp;quot;./iris-data.txt&amp;quot;\n\n// Initialize Compute Graph\nlet pipeline = new LearningPipeline()\n\n// Load Data\npipeline.Add((new TextLoader(dataPath).CreateFrom&amp;lt;IrisData&amp;gt;separator=','))\n\n// Transform Data\n// Assign numeric values to text in the &amp;quot;Label&amp;quot; column, because\n// only numbers can be processed during model training\npipeline.Add(new Transforms.Dictionarizer(&amp;quot;Label&amp;quot;))\n\n// Vectorize Features\npipeline.Add(new ColumnConcatenator(&amp;quot;Features&amp;quot;,&amp;quot;SepalLength&amp;quot;, &amp;quot;SepalWidth&amp;quot;, &amp;quot;PetalLength&amp;quot;, &amp;quot;PetalWidth&amp;quot;))\n\n// Add Learner\npipeline.Add(new StochasticDualCoordinateAscentClassifier())\n\n// Convert Label back to text\npipeline.Add(new ransforms.PredictedLabelColumnOriginalValueConverterPredictedLabelColumn = &amp;quot;PredictedLabel&amp;quot;))\n\n//Train the model\nlet model = pipeline.Train&amp;lt;IrisData, IrisPrediction&amp;gt;()\n</code></pre>\n<h4>Testing Our Model</h4>\n<p>Now that we have our data structures and model trained, it’s time to test it to make sure it's working. Following our training operation, we can add the following code.</p>\n<pre><code class=\"language-fsharp\">// Test data for prediction\nlet testInstance = IrisData()\ntestInstance.SepalLength &amp;lt;- 3.3f\ntestInstance.SepalWidth &amp;lt;- 1.6f\ntestInstance.PetalLength &amp;lt;- 0.2f\ntestInstance.PetalWidth &amp;lt;- 5.1f\n\n//Get Prediction\nlet prediction = model.Predict(testInstance)\n\n//Output Prediction\nprintfn &amp;quot;Predicted flower type is: %s&amp;quot; prediction.PredictedLabel\n</code></pre>\n<p>Our final <code>Program.fs</code> file should contain content similar to that below:</p>\n<pre><code class=\"language-fsharp\">open System\nopen Microsoft.ML\nopen Microsoft.ML.Runtime\nopen Microsoft.ML.Runtime.Api\nopen Microsoft.ML.Data\nopen Microsoft.ML.Transforms\nopen Microsoft.ML.Trainers\n\ntype IrisData() =\n\n    [&amp;lt;Column(ordinal = &amp;quot;0&amp;quot;);DefaultValue&amp;gt;] val mutable public SepalLength: float32\n    [&amp;lt;Column(ordinal = &amp;quot;1&amp;quot;);DefaultValue&amp;gt;] val mutable public SepalWidth: float32\n    [&amp;lt;Column(ordinal = &amp;quot;2&amp;quot;);DefaultValue&amp;gt;] val mutable public PetalLength:float32\n    [&amp;lt;Column(ordinal = &amp;quot;3&amp;quot;);DefaultValue&amp;gt;] val mutable public PetalWidth:float32\n    [&amp;lt;Column(ordinal = &amp;quot;4&amp;quot;,name=&amp;quot;Label&amp;quot;);DefaultValue&amp;gt;] val mutable public Label: string\n\n\ntype IrisPrediction() =\n\n    [&amp;lt;ColumnName &amp;quot;PredictedLabel&amp;quot;;DefaultValue&amp;gt;] val mutable public PredictedLabel : string\n\n[&amp;lt;EntryPoint&amp;gt;]\nlet main argv =\n\n    let dataPath = &amp;quot;./iris-data.txt&amp;quot;\n\n    // Initialize Compute Graph\n    let pipeline = new LearningPipeline()\n\n    // Load Data\n    pipeline.Add((new TextLoader(dataPath)).CreateFrom&amp;lt;IrisData&amp;gt;(separator=','))\n\n    // Transform Data\n    // Assign numeric values to text in the &amp;quot;Label&amp;quot; column, because\n    // only numbers can be processed during model training\n    pipeline.Add(new Transforms.Dictionarizer(&amp;quot;Label&amp;quot;))\n\n    // Vectorize Features\n    pipeline.Add(new ColumnConcatenator(&amp;quot;Features&amp;quot;,&amp;quot;SepalLength&amp;quot;, &amp;quot;SepalWidth&amp;quot;, &amp;quot;PetalLength&amp;quot;, &amp;quot;PetalWidth&amp;quot;))\n\n    // Add Learner\n    pipeline.Add(new StochasticDualCoordinateAscentClassifier())\n\n    // Convert Label back to text\n    pipeline.Add(new Transforms.PredictedLabelColumnOriginalValueConverter(PredictedLabelColumn = &amp;quot;PredictedLabel&amp;quot;))\n\n    //Train the model\n    let model = pipeline.Train&amp;lt;IrisData, IrisPrediction&amp;gt;()\n\n    // Test data for prediction\n    let testInstance = IrisData()\n    testInstance.SepalLength &amp;lt;- 3.3f\n    testInstance.SepalWidth &amp;lt;- 1.6f\n    testInstance.PetalLength &amp;lt;- 0.2f\n    testInstance.PetalWidth &amp;lt;- 5.1f\n\n    //Get Prediction\n    let prediction = model.Predict(testInstance)\n\n    //Output Prediction\n    printfn &amp;quot;Predicted flower type is: %s&amp;quot; prediction.PredictedLabel\n    0 // return an integer exit code\n</code></pre>\n<p>All set to run. We can do so by entering the following command from our solution directory:</p>\n<pre><code class=\"language-fsharp\">dotnet run -p model/model.fsproj\n</code></pre>\n<p>Once the application has been run, the following output should display on the console.</p>\n<pre><code class=\"language-bash\">Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.\nUsing 2 threads to train.\nAutomatically choosing a check frequency of 2.\nAuto-tuning parameters: maxIterations = 9998.\nAuto-tuning parameters: L2 = 2.667734E-05.Auto-tuning parameters: L1Threshold (L1/L2) = 0.\nUsing best model from iteration 1066.Not training a calibrator because it is not needed.\nPredicted flower type is: Iris-virginica\n</code></pre>\n\n<p>In this post, we ported over a C# <code>ML.NET</code> classification model to F# which predicts the class of flower based on numerical measurement inputs. While several workarounds needed to be made, <code>ML.NET</code> is still in its infancy. As more people become involved and provide feedback hopefully in the near future, F# support and functionality will become more stable. Happy coding!</p>\n","tags":["fsharp","dotnet","dotnetcore","programming","development","mlnet","machinelearning","artificialintelligence","functionalprogramming"]},{"title":"Organizing F# Modules Into Separate Files","contentType":"posts","date":"2018-06-05 20:56:52","url":"/posts/fsharpmodulesample/","content":"\n<p>After watching <a href=\"https://twitter.com/csharpfritz\">Jeff Fritz's</a> F# Friday <a href=\"https://www.twitch.tv/videos/268107540\">Stream</a>, I was inspired to do some tinkering of my own. I have looked at F# in the past, but often times have not done much with it. After the stream, I decided to keep plugging away at it and one of the first things I thought about doing was organizing my code into separate files. As simple as it may seem, it was no easy task and while there are many different ways of doing it, the method described in this writeup is one of the easiest I found. Keep in mind though that this may not be the appropriate way of doing it although it seems to work nicely for our purposes. In this solution I will create a set of mathematical operations that will be organized into a single module that will then be imported and used inside of my console application. Source code for this sample solution can be found at this <a href=\"https://github.com/lqdev/fsharpmoduledemo\">link</a>.</p>\n\n<p>This solution was built on a Mac, but should work on both Windows and Linux environments.</p>\n<ul>\n<li><a href=\"https://www.microsoft.com/net/download/macos\">.NET Core SDK</a></li>\n<li><a href=\"https://fsharp.org/use/mac/\">Ionide Extension - Option 1</a></li>\n</ul>\n\n<p>Once you have everything installed, you can create a new F# project by entering the following command into the console.</p>\n<pre><code class=\"language-bash\">dotnet new console -lang f#\n</code></pre>\n\n<p>The goal of this solution will be to create functions that add, subtract, multiply and divide two numbers. Since all of these functions are mathematical operations, we can organize them inside a single module. Therefore, we can create a new file inside of our project called <code>Math.fs</code> which will contain all of our functions.</p>\n<pre><code class=\"language-fsharp\">module Math\n    let add x y = x + y\n    let subtract x y = x - y\n    let multiply x y = x * y\n    let divide x y = x / y\n</code></pre>\n\n<p>Now that our file has been created, we need to add it to our list of files to be compiled by our project. To do so, we can add an entry for our <code>Math.fs</code> file to the <code>ItemGroup</code> property inside our <code>fsproj</code> file. The contents of the file should look as follows after the addition:</p>\n<pre><code class=\"language-xml\">&amp;lt;Project Sdk=&amp;quot;Microsoft.NET.Sdk&amp;quot;&amp;gt;\n\n  &amp;lt;PropertyGroup&amp;gt;\n    &amp;lt;OutputType&amp;gt;Exe&amp;lt;/OutputType&amp;gt;\n    &amp;lt;TargetFramework&amp;gt;netcoreapp2.0&amp;lt;/TargetFramework&amp;gt;\n  &amp;lt;/PropertyGroup&amp;gt;\n\n  &amp;lt;ItemGroup&amp;gt;\n    &amp;lt;Compile Include=&amp;quot;Math.fs&amp;quot; /&amp;gt;\n    &amp;lt;Compile Include=&amp;quot;Program.fs&amp;quot; /&amp;gt;\n  &amp;lt;/ItemGroup&amp;gt;\n\n&amp;lt;/Project&amp;gt;\n</code></pre>\n\n<p>Once we have our module created and our <code>fsproj</code> file configured, we can call the module from our console application. To do so, we need to first import it into our <code>Program.fs</code> file.</p>\n<pre><code class=\"language-fsharp\">open Math\n</code></pre>\n<p>Once imported, we can call the <code>Math</code> module functions from our <code>main</code> function in the <code>Program.fs</code> file.</p>\n<pre><code class=\"language-fsharp\">    printfn &amp;quot;Add: %i&amp;quot; (add 1 2)\n    printfn &amp;quot;Subtract: %i&amp;quot; (subtract 1 2)\n    printfn &amp;quot;Multiply: %i&amp;quot; (multiply 1 2)\n    printfn &amp;quot;Divide: %i&amp;quot; (divide 4 2)\n</code></pre>\n\n<p>Now that everything is set up, we can build and run our console application with the following commands:</p>\n<pre><code class=\"language-bash\">dotnet build\ndotnet run\n</code></pre>\n<p>The output should be the following:</p>\n<pre><code class=\"language-bash\">Add: 3\nSubtract: -1\nMultiply: 2\nDivide: 2\n</code></pre>\n\n<p>In this writeup, I went over how to create an F# console application that organizes a set of mathematical functions into a module whose contents are stored in a separate file. This is good for organization and to reduce clutter. Happy coding!</p>\n","tags":["dotnet","fsharp","programming","development","functionalprogramming","dotnetcore"]},{"title":"Deploy .NET Machine Learning Models with ML.NET, ASP.NET Core, Docker and Azure Container Instances","contentType":"posts","date":"2018-05-11 21:17:00","url":"/posts/deploy-netml-docker-aci/","content":"\n<p>Leading up to and during MS Build 2018 Microsoft has released a wide range of products that reduce the complexity that comes with building and deploying software. The focus this year was on Machine Learning and Artificial Intelligence. Some of the products I found particularly interesting are <a href=\"https://azure.microsoft.com/en-us/services/container-instances/\">Azure Container Instances</a> which makes it easier to run containerized applications without provisioning or managing servers and <a href=\"https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet\">ML.NET</a> which is a .NET cross-platform machine learning framework. In this writeup, I will make use of both these products by creating a machine learning classification model with <code>ML.NET</code>, exposing it via an ASP.NET Core Web API, packaging it into a Docker container and deploying it to the cloud via Azure Container Instances. Source code for this project can be found <a href=\"https://github.com/lqdev/mlnetacidemo\">here</a>.</p>\n\n<p>This writeup assumes that you have some familiarity with Docker. The following software/dependencies are also required to build and deploy the sample application. It's important to note the application was built on a Ubuntu 16.04 PC, but all the software is cross-platform and should work on any environment.</p>\n<ul>\n<li><a href=\"https://docs.docker.com/install/linux/docker-ce/ubuntu/\">Docker</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest\">Azure CLI</a></li>\n<li><a href=\"https://www.microsoft.com/net/download/linux\">.NET Core 2.0</a></li>\n<li><a href=\"https://hub.docker.com/\">Docker Hub Account</a></li>\n</ul>\n\n<p>The first thing we want to do is create a folder for our solution.</p>\n<pre><code class=\"language-bash\">mkdir mlnetacidemo\n</code></pre>\n<p>Then, we want to create a solution inside our newly created folder.</p>\n<pre><code class=\"language-bash\">cd mlnetacidemo\ndotnet new sln\n</code></pre>\n\n<p>Inside our solution folder, we want to create a new console application which is where we'll build and test our machine learning model.</p>\n<h3>Setting Up the Model Project</h3>\n<p>First, we want to create the project. From the solution folder enter:</p>\n<pre><code class=\"language-bash\">dotnet new console -o model\n</code></pre>\n<p>Now we want to add this new project to our solution.</p>\n<pre><code class=\"language-bash\">dotnet sln mlnetacidemo.sln add model/model.csproj\n</code></pre>\n<h3>Adding Dependencies</h3>\n<p>Since we'll be using the <code>ML.NET</code> framework, we need to add it to our <code>model</code> project.</p>\n<pre><code class=\"language-bash\">cd model\ndotnet add package Microsoft.ML\ndotnet restore\n</code></pre>\n<h3>Download The Data</h3>\n<p>Before we start training the model, we need to download the data we'll be using to train. We do so by creating a directory called <code>data</code> and downloading the data file onto there.</p>\n<pre><code class=\"language-bash\">mkdir data\ncurl -o data/iris.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n</code></pre>\n<p>If we take a look at the data file, it should look something like this:</p>\n<pre><code class=\"language-text\">5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n4.6,3.1,1.5,0.2,Iris-setosa\n5.0,3.6,1.4,0.2,Iris-setosa\n5.4,3.9,1.7,0.4,Iris-setosa\n4.6,3.4,1.4,0.3,Iris-setosa\n5.0,3.4,1.5,0.2,Iris-setosa\n4.4,2.9,1.4,0.2,Iris-setosa\n4.9,3.1,1.5,0.1,Iris-setosa\n</code></pre>\n<h3>Train Model</h3>\n<p>Now that we have all our dependencies set up, it's time to build our model. I leveraged the demo that is used on the <a href=\"https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet/get-started/linux/ubuntu16-04\">ML.NET Getting-Started website</a>.</p>\n<h4>Defining Data Structures</h4>\n<p>In the root directory of our <code>model</code> project, let's create two classes called <code>IrisData</code> and <code>IrisPrediction</code> which will define our features and predicted attribute respectively. Both of them will use <code>Microsoft.ML.Runtime.Api</code> to add the property attributes.</p>\n<p>Here is what our <code>IrisData</code> class looks like:</p>\n<pre><code class=\"language-csharp\">using Microsoft.ML.Runtime.Api;\n\nnamespace model\n{\npublic class IrisData\n    {\n        [Column(&amp;quot;0&amp;quot;)]\n        public float SepalLength;\n\n        [Column(&amp;quot;1&amp;quot;)]\n        public float SepalWidth;\n\n        [Column(&amp;quot;2&amp;quot;)]\n        public float PetalLength;\n        \n        [Column(&amp;quot;3&amp;quot;)]\n        public float PetalWidth;\n\n        [Column(&amp;quot;4&amp;quot;)]\n        [ColumnName(&amp;quot;Label&amp;quot;)]\n        public string Label;\n    }       \n}\n</code></pre>\n<p>Similarly, here is the <code>IrisPrediction</code> class:</p>\n<pre><code class=\"language-csharp\">using Microsoft.ML.Runtime.Api;\n\nnamespace model\n{\n    public class IrisPrediction\n    {\n        [ColumnName(&amp;quot;PredictedLabel&amp;quot;)]\n        public string PredictedLabels;\n    }\n}\n</code></pre>\n<h4>Building Training Pipeline</h4>\n<p>The way the <code>ML.NET</code> computations process is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. Therefore, we can create a class called <code>Model</code> to perform all of these tasks for us.</p>\n<pre><code class=\"language-csharp\">using Microsoft.ML.Data;\nusing Microsoft.ML;\nusing Microsoft.ML.Runtime.Api;\nusing Microsoft.ML.Trainers;\nusing Microsoft.ML.Transforms;\nusing Microsoft.ML.Models;\nusing System;\nusing System.Threading.Tasks;\n\nnamespace model\n{\n    class Model\n    {\n        \n        public static async Task&amp;lt;PredictionModel&amp;lt;IrisData,IrisPrediction&amp;gt;&amp;gt; Train(LearningPipeline pipeline, string dataPath, string modelPath)\n        {\n            // Load Data\n            pipeline.Add(new TextLoader(dataPath).CreateFrom&amp;lt;IrisData&amp;gt;(separator:',')); \n\n            // Transform Data\n            // Assign numeric values to text in the &amp;quot;Label&amp;quot; column, because \n            // only numbers can be processed during model training   \n            pipeline.Add(new Dictionarizer(&amp;quot;Label&amp;quot;));\n\n            // Vectorize Features\n            pipeline.Add(new ColumnConcatenator(&amp;quot;Features&amp;quot;, &amp;quot;SepalLength&amp;quot;, &amp;quot;SepalWidth&amp;quot;, &amp;quot;PetalLength&amp;quot;, &amp;quot;PetalWidth&amp;quot;));\n\n            // Add Learner\n            pipeline.Add(new StochasticDualCoordinateAscentClassifier());\n\n            // Convert Label back to text \n            pipeline.Add(new PredictedLabelColumnOriginalValueConverter() {PredictedLabelColumn = &amp;quot;PredictedLabel&amp;quot;});\n\n            // Train Model\n            var model = pipeline.Train&amp;lt;IrisData,IrisPrediction&amp;gt;();\n\n            // Persist Model\n            await model.WriteAsync(modelPath);\n\n            return model;\n        }\n    }\n}\n</code></pre>\n<p>In addition to building our pipeline and training our machine learning model, the <code>Model</code> class also serialized and persisted the model for future use in a file called <code>model.zip</code>.</p>\n<h4>Testing Our Model</h4>\n<p>Now that we have our data structures and model training pipeline set up, it's time to test everything to make sure it's working. We'll put our logic inside of our <code>Program.cs</code> file.</p>\n<pre><code class=\"language-csharp\">using System;\nusing Microsoft.ML;\n\nnamespace model\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n\n            string dataPath = &amp;quot;model/data/iris.txt&amp;quot;;\n\n            string modelPath = &amp;quot;model/model.zip&amp;quot;;\n\n            var model = Model.Train(new LearningPipeline(),dataPath,modelPath).Result;\n\n            // Test data for prediction\n            var prediction = model.Predict(new IrisData() \n            {\n                SepalLength = 3.3f,\n                SepalWidth = 1.6f,\n                PetalLength = 0.2f,\n                PetalWidth = 5.1f\n            });\n\n            Console.WriteLine($&amp;quot;Predicted flower type is: {prediction.PredictedLabels}&amp;quot;);\n        }\n    }\n}\n</code></pre>\n<p>All set to run. We can do so by entering the following command from our solution directory:</p>\n<pre><code class=\"language-bash\">dotnet run -p model/model.csproj\n</code></pre>\n<p>Once the application has been run, the following output should display on the console.</p>\n<pre><code class=\"language-text\">Automatically adding a MinMax normalization transform, use 'norm=Warn' or\n'norm=No' to turn this behavior off.Using 2 threads to train.\nAutomatically choosing a check frequency of 2.Auto-tuning parameters: maxIterations = 9998.\nAuto-tuning parameters: L2 = 2.667734E-05.\nAuto-tuning parameters: L1Threshold (L1/L2) = 0.Using best model from iteration 882.\nNot training a calibrator because it is not needed.\nPredicted flower type is: Iris-virginica\n</code></pre>\n<p>Additionally, you'll notice that a file called <code>model.zip</code> was created in the root directory of our <code>model</code> project. This persisted model can now be used outside of our application to make predictions, which is what we'll do next via an API.</p>\n\n<p>Once a machine learning model is built, you want to deploy it so it can start making predictions. One way to do that is via a REST API. At it's core, all our API needs to do is accept data input from the client and respond back with a prediction. To help us do that, we'll be using an ASP.NET Core API.</p>\n<h3>Setting Up The API Project</h3>\n<p>The first thing we want to do is create the project.</p>\n<pre><code class=\"language-bash\">dotnet new webapi -o api\n</code></pre>\n<p>Then we want to add this new project to our solution</p>\n<pre><code class=\"language-bash\">dotnet sln mlnetacidemo.sln add api/api.csproj\n</code></pre>\n<h3>Adding Dependencies</h3>\n<p>Because we'll be loading our model and making predictions via our API, we need to add the <code>ML.NET</code> package to our <code>api</code> project.</p>\n<pre><code class=\"language-bash\">cd api\ndotnet add package Microsoft.ML\ndotnet restore\n</code></pre>\n<h3>Referencing Our Model</h3>\n<p>In the previous step when we built our machine learning model, it was saved to a file called <code>model.zip</code>. This is the file we'll be referencing in our API to help us make predictions. To reference it in our API, simply copy it from the model project directory into our <code>api</code> project directory.</p>\n<h3>Creating Data Models</h3>\n<p>Our model was built using data structures <code>IrisData</code> and <code>IrisPrediction</code> to define the features as well as the predicted attribute. Therefore, when our model makes predictions via our API, it needs to reference these data types as well. As a result, we need to define <code>IrisData</code> and <code>IrisPrediction</code> classes inside of our <code>api</code> project. The contents of the classes will be nearly identical to those in the <code>model</code> project with the only exception of our namespace changing from <code>model</code> to <code>api</code>.</p>\n<pre><code class=\"language-csharp\">using Microsoft.ML.Runtime.Api;\n\nnamespace api\n{\n    public class IrisData\n    {\n        [Column(&amp;quot;0&amp;quot;)]\n        public float SepalLength;\n\n        [Column(&amp;quot;1&amp;quot;)]\n        public float SepalWidth;\n\n        [Column(&amp;quot;2&amp;quot;)]\n        public float PetalLength;\n        \n        [Column(&amp;quot;3&amp;quot;)]\n        public float PetalWidth;\n\n        [Column(&amp;quot;4&amp;quot;)]\n        [ColumnName(&amp;quot;Label&amp;quot;)]\n        public string Label;\n    }    \n}\n</code></pre>\n<pre><code class=\"language-csharp\">using Microsoft.ML.Runtime.Api;\n\nnamespace api\n{\n    public class IrisPrediction\n    {\n        [ColumnName(&amp;quot;PredictedLabel&amp;quot;)]\n        public string PredictedLabels;\n    }\n}\n</code></pre>\n<h3>Building Endpoints</h3>\n<p>Now that our project is set up, it's time to add a controller that will handle prediction requests from the client. In the <code>Controllers</code> directory of our <code>api</code> project we can create a new class called <code>PredictController</code> with a single <code>POST</code> endpoint. The contents of the file should look like the code below:</p>\n<pre><code class=\"language-csharp\">using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore.Mvc;\nusing Microsoft.ML;\n\nnamespace api.Controllers\n{\n    [Route(&amp;quot;api/[controller]&amp;quot;)]\n    public class PredictController : Controller\n    {\n        // POST api/predict\n        [HttpPost]\n        public string Post([FromBody] IrisData instance)\n        {\n            var model = PredictionModel.ReadAsync&amp;lt;IrisData,IrisPrediction&amp;gt;(&amp;quot;model.zip&amp;quot;).Result;\n            var prediction = model.Predict(instance);\n            return prediction.PredictedLabels;\n        }\n    }\n}\n</code></pre>\n<h3>Testing The API</h3>\n<p>Once our <code>predict</code> endpoint is set up, it's time to test it. From the root directory of our <code>mlnetacidemo</code> solution, enter the following command.</p>\n<pre><code class=\"language-bash\">dotnet run -p api/api.csproj\n</code></pre>\n<p>In a client like POSTMAN or Insomnia, send an HHTP POST request to the endpoint <code>http://localhost:5000/api/predict</code>.</p>\n<p>The body our request should look similar to the snippet below:</p>\n<pre><code class=\"language-json\">{\n\t&amp;quot;SepalLength&amp;quot;: 3.3,\n\t&amp;quot;SepalWidth&amp;quot;: 1.6,\n\t&amp;quot;PetalLength&amp;quot;: 0.2,\n\t&amp;quot;PetalWidth&amp;quot;: 5.1,\n}\n</code></pre>\n<p>If successful, the output returned should equal <code>Iris-virginica</code> just like our console application.</p>\n\n<p>Great! Now that our application is successfully running locally, it's time to package it up into a Docker container and push it to Docker Hub.</p>\n<h3>Creating The Dockerfile</h3>\n<p>In our <code>mlnetacidemo</code> solution directory, create a <code>Dockerfile</code> with the following content:</p>\n<pre><code class=\"language-Dockerfile\">FROM microsoft/dotnet:2.0-sdk AS build\nWORKDIR /app\n\n# copy csproj and restore as distinct layers\nCOPY *.sln .\nCOPY api/*.csproj ./api/\nRUN dotnet restore\n\n# copy everything else and build app\nCOPY api/. ./api/\nWORKDIR /app/api\nRUN dotnet publish -c release -o out\n\n\nFROM microsoft/aspnetcore:2.0 AS runtime\nWORKDIR /app\nCOPY api/model.zip .\nCOPY --from=build /app/api/out ./\nENTRYPOINT [&amp;quot;dotnet&amp;quot;, &amp;quot;api.dll&amp;quot;]\n</code></pre>\n<h3>Building Our Image</h3>\n<p>To build the image, we need to enter the following command into the command prompt. This make take a while because it needs to download the .NET Core SDK and ASP.NET Core runtime Docker images.</p>\n<pre><code class=\"language-bash\">docker build -t &amp;lt;DOCKERUSERNAME&amp;gt;/&amp;lt;IMAGENAME&amp;gt;:latest .\n</code></pre>\n<h3>Test Image Locally</h3>\n<p>We need to test our image locally to make sure it can run on the cloud. To do so, we can use the <code>docker run</code> command.</p>\n<pre><code class=\"language-bash\">docker run -d -p 5000:80 &amp;lt;DOCKERUSERNAME&amp;gt;/&amp;lt;IMAGENAME&amp;gt;:latest\n</code></pre>\n<p>Although the API is exposing port 80, we bind it to the local port 5000 just to keep our prior API request intact. When sending a POST request to <code>http://localhost:5000/api/predict</code> with the appropriate body, the response should again equal <code>Iris-virginica</code>.</p>\n<p>To stop the container, use <code>Ctrl + C</code>.</p>\n<h3>Push to Docker Hub</h3>\n<p>Now that the Docker image is successfully running locally, it's time to push to Docker Hub. Again, we use the Docker CLI to do this.</p>\n<pre><code class=\"language-bash\">docker login\ndocker push &amp;lt;DOCKERUSERNAME&amp;gt;/&amp;lt;IMAGENAME&amp;gt;:latest\n</code></pre>\n\n<p>Now comes the final step which is to deploy and expose our machine learning model and API to the world. Our deployment will occur via Azure Container Instances because it requires almost no provisioning or management of servers.</p>\n<h3>Prepare Deployment Manifest</h3>\n<p>Although deployments can be performed inline in the command line, it's usually best to place all the configurations in a file for documentation and to save time not having to type in the parameters every time. With Azure, we can do that via a JSON file.</p>\n<pre><code class=\"language-json\">{\n  &amp;quot;$schema&amp;quot;:\n    &amp;quot;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#&amp;quot;,\n  &amp;quot;contentVersion&amp;quot;: &amp;quot;1.0.0.0&amp;quot;,\n  &amp;quot;parameters&amp;quot;: {\n    &amp;quot;containerGroupName&amp;quot;: {\n      &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,\n      &amp;quot;defaultValue&amp;quot;: &amp;quot;mlnetacicontainergroup&amp;quot;,\n      &amp;quot;metadata&amp;quot;: {\n        &amp;quot;description&amp;quot;: &amp;quot;Container Group name.&amp;quot;\n      }\n    }\n  },\n  &amp;quot;variables&amp;quot;: {\n    &amp;quot;containername&amp;quot;: &amp;quot;mlnetacidemo&amp;quot;,\n    &amp;quot;containerimage&amp;quot;: &amp;quot;&amp;lt;DOCKERUSERNAME&amp;gt;/&amp;lt;IMAGENAME&amp;gt;:latest&amp;quot;\n  },\n  &amp;quot;resources&amp;quot;: [\n    {\n      &amp;quot;name&amp;quot;: &amp;quot;[parameters('containerGroupName')]&amp;quot;,\n      &amp;quot;type&amp;quot;: &amp;quot;Microsoft.ContainerInstance/containerGroups&amp;quot;,\n      &amp;quot;apiVersion&amp;quot;: &amp;quot;2018-04-01&amp;quot;,\n      &amp;quot;location&amp;quot;: &amp;quot;[resourceGroup().location]&amp;quot;,\n      &amp;quot;properties&amp;quot;: {\n        &amp;quot;containers&amp;quot;: [\n          {\n            &amp;quot;name&amp;quot;: &amp;quot;[variables('containername')]&amp;quot;,\n            &amp;quot;properties&amp;quot;: {\n              &amp;quot;image&amp;quot;: &amp;quot;[variables('containerimage')]&amp;quot;,\n              &amp;quot;resources&amp;quot;: {\n                &amp;quot;requests&amp;quot;: {\n                  &amp;quot;cpu&amp;quot;: 1,\n                  &amp;quot;memoryInGb&amp;quot;: 1.5\n                }\n              },\n              &amp;quot;ports&amp;quot;: [\n                {\n                  &amp;quot;port&amp;quot;: 80\n                }\n              ]\n            }\n          }\n        ],\n        &amp;quot;osType&amp;quot;: &amp;quot;Linux&amp;quot;,\n        &amp;quot;ipAddress&amp;quot;: {\n          &amp;quot;type&amp;quot;: &amp;quot;Public&amp;quot;,\n          &amp;quot;ports&amp;quot;: [\n            {\n              &amp;quot;protocol&amp;quot;: &amp;quot;tcp&amp;quot;,\n              &amp;quot;port&amp;quot;: &amp;quot;80&amp;quot;\n            }\n          ]\n        }\n      }\n    }\n  ],\n  &amp;quot;outputs&amp;quot;: {\n    &amp;quot;containerIPv4Address&amp;quot;: {\n      &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,\n      &amp;quot;value&amp;quot;:\n        &amp;quot;[reference(resourceId('Microsoft.ContainerInstance/containerGroups/', parameters('containerGroupName'))).ipAddress.ip]&amp;quot;\n    }\n  }\n}\n</code></pre>\n<p>It's a lot to look at but for now we can use this template and save it to the file <code>azuredeploy.json</code> in the root directory of our <code>mlnetacidemo</code> solution. The only thing that needs to be changed is the value of the <code>containerimage</code> property. Replace it with your Docker Hub username and the name of the image you just pushed to Docker Hub.</p>\n<h3>Deploy</h3>\n<p>In order to deploy our application we need to make sure to log into our Azure account. To do so via the Azure CLI, type into the command prompt:</p>\n<pre><code class=\"language-bash\">az login\n</code></pre>\n<p>Follow the prompts to log in. Once logged in, it's time to create a resource group for our container.</p>\n<pre><code class=\"language-bash\">az group create --name mlnetacidemogroup --location eastus\n</code></pre>\n<p>After the group has been successfully created it's time to deploy our application.</p>\n<pre><code class=\"language-bash\">az group deployment create --resource-group mlnetacidemogroup --template-file azuredeploy.json\n</code></pre>\n<p>Give it a few minutes for your deployment to initialize. If the deployment was successful, you should see some output on the command line. Look for the <code>ContainerIPv4Address</code> property. This is the IP Address where your container is accessible. In POSTMAN or Insomnia, replace the URL to which you previously made a POST request to with <code>http://&amp;lt;ContainerIPv4Address&amp;gt;/api/predict</code> where <code>ContainerIPv4Address</code> is the value that was returned to the command line after the deployment. If successful, the response should be just like previous requests <code>Iris-virginica</code>.</p>\n<p>Once you're finished, you can clean up resources with the following command:</p>\n<pre><code class=\"language-bash\">az group delete --name mlnetacidemogroup\n</code></pre>\n\n<p>In this writeup, we built a classification machine learning model using <code>ML.NET</code> that predicts the class of an iris plant given four measurement features, exposed it via an ASP.NET Core REST API, packaged it into a container and deployed it to the cloud using Azure Container Instances. As the model changes and becomes more complex, the process is standardized enough that extending this example would require minimal changes to our existing application. Happy Coding!</p>\n","tags":["azure","devops","dotnet","ml","ai","microsoft","programming","development","csharp","aci","docker","mlnet","webapi","aspnetcore","aspnet","machinelearning","artificialintelligence"]},{"title":"E2E Dockerizing a MEAN Stack Application","contentType":"posts","date":"2018-04-29 20:10:09","url":"/posts/e2e-mean-docker-build/","content":"\n<p>Lately I've been getting familiar with <a href=\"https://docker.com\">Docker</a>. I have built single container applications using a <code>Dockerfile</code> and run them locally. This works fine, especially for deployment purposes to a production VM but it's no different than setting up the VM with all the required dependencies and pushing updates via FTP or source control. However, one of the features that I have found extremely useful is multi-container building and deployment via <code>docker-compose</code>. With <code>docker-compose</code>, not only can I build and run my application, but also dependent services like databases, caches, proxies, etc. Best of all, the builds are standardized and initialized at once without having to individually install the dependencies and components. This writeup explores how to containerize a MEAN stack application and set up a <code>docker-compose.yml</code> file for it to build and start the server and database services defined within it.</p>\n\n<p>This writeup assumes that <code>Docker</code>, <code>Docker-Compose</code> and <code>Node</code> are installed on your PC.</p>\n\n<p>The application is a CRUD todo MEAN stack application. The repo for this application can be found <a href=\"https://github.com/lqdev/todomeandockerdemo\">here</a>.</p>\n<h3>Project Structure</h3>\n<pre><code class=\"language-text\">|_models (Mongoose models)\n| |_todo.model.js\n|_public\n| |_scripts\n| | |_controllers\n| | | |_main.controller.js\n| | |_services\n| |   |_todo.service.js\n| |_views\n| | |_main.html\n| |_app.js (front-end application)\n| |_index.html\n|_Dockerfile (server service)\n|_docker-compose.yml\n|_api.js (todo api routes)\n|_config.js\n|_server.js (back-end application)\n</code></pre>\n<p>The front-end is built with <code>AngularJS</code> and the back-end is built with <code>NodeJS</code> using the <code>Express</code> web framework and <code>MongoDB</code> database. <code>MongoDB</code> models are defined with the <code>Mongoose</code> package. In the application, users can create, view, update and delete todo tasks. The <code>Dockerfile</code> is used to define the container for the web application and the <code>docker-compose.yml</code> defines both the <code>MongoDB</code> database container as well as the web application container defined in the <code>Dockerfile</code>.</p>\n\n<pre><code class=\"language-docker\">#Define base image\nFROM node:8\n\n#Set Working Directory\nWORKDIR /app\n\n#Copy pakage.json file from current directory to working directory\nADD package.json /app\n\n#Install npm packages\nRUN npm install\n\n#Copy all application files from local directory to working directory\nADD . /app\n\n#Open port where app will be listening\nEXPOSE 3000\n\n#Start application\nCMD ['npm','start']\n</code></pre>\n<h3>Define Docker Image and Application Directory</h3>\n<p>The <code>Dockerfile</code> has no extension and the syntax is like a standard text file. <code>#</code> characters denote comments. <code>Docker</code> works based off images which are basically pre-built packages that are stored in one of many registries such as <code>DockerHub</code>. <code>DockerHub</code> can be thought of as a package repository/manager like <code>npm</code> or <code>dpkg</code>. In the first two lines of the file we define which base image we want to create our container with. Since this is a MEAN stack application built entirely in <code>JavaScript</code>, we'll be using the <code>node</code> version 8 image. Then we want set the directory in which our application will reside. We do this by using the <code>WORKDIR</code> command and setting <code>/app</code> as our application directory, but any directory of your choosing is valid.</p>\n<h3>Install Dependecies</h3>\n<p>All of our dependencies should be defined in our <code>package.json</code> file. In order to install these dependencies in our container, we need to copy our local <code>package.json</code> file into our container application directory. This can be done with the <code>ADD</code> command by passing the <code>package.json</code> and application directory <code>/app</code> as arguments. Once that file has been copied, it's time to install the dependencies. To run commands through the build process of the application we use the <code>RUN</code> command. The command is no different than the one you'd use on your local machine. Therefore, to install the dependencies defined in the <code>package.json</code> file we use the command <code>RUN npm install</code>.</p>\n<h3>Copy Application Files</h3>\n<p>Once our dependencies are installed, we need to copy the rest of the files in our local project directory to our container application directory. Like with the <code>package.json</code> file we use the <code>ADD</code> command and pass <code>.</code> and <code>/app</code> as our source and destination arguments respectively.</p>\n<h4>.dockerignore</h4>\n<p>Something to keep in mind is that locally we have a <code>node_modules</code> directory containing our installed dependencies. In the previous step, we ran the <code>npm install</code> command which will create the <code>node_modules</code> directory inside our container. Therefore, there is no need to copy all these files over. Like <code>git</code>, we can set up a <code>.dockerignore</code> file which will contain the files and directory to be ignored by <code>Docker</code> when packaging and building the container. The <code>.dockerignore</code> file looks like the following.</p>\n<pre><code class=\"language-text\">node_modules/*\n</code></pre>\n<h3>Opening Ports</h3>\n<p>Our application will be listening for connections on a port. This particular application will use port 3000. We need to define the port to listen on in the <code>Dockerfile</code> as well. To do so, we'll use the <code>EXPOSE</code> command and pass the port(s) that the application will listen on. (MongoDB listens on 27017, but since the <code>Dockerfile</code> only deals with the web application and not the database we only need to specify the web application's port).</p>\n<h3>Starting the Application</h3>\n<p>After our container is set up, dependencies are installed and port is defined, it's time to start our application. Unlike the process of running commands while building the container using the <code>RUN</code> command, we'll use the <code>CMD</code> command to start our application. The arguments accepted by this are an array of strings. In this case, we start our application like we would locally by typing in <code>npm start</code>. The <code>Dockerfile</code> command to start our application is the following <code>CMD ['npm','start']</code>.</p>\n\n<pre><code class=\"language-yaml\">version: '2'\nservices:\n  db:\n    image: mongo\n    ports: \n      - 27017:27017\n  web:\n    build: .\n    ports:\n      - 3000:3000\n    links:\n      - &amp;quot;db&amp;quot;\n</code></pre>\n<p>The <code>docker-compose.yml</code> file is a way of defining, building and starting multi-container applications using <code>docker-compose</code>. In our case we have a two container application, one of the containers is the web application we defined and built in the <code>Dockerfile</code> and the other is a <code>MongoDB</code> database. The <code>docker-compose.yml</code> file can take many options, but the only ones we'll be using are the <code>version</code> and <code>services</code> option. The <code>version</code> option defines which syntax version of the <code>docker-compose.yml</code> file we'll be using. In our case we'll be using version 2. The <code>services</code> option defines the individual containers to be packaged and initialized.</p>\n<h3>Services</h3>\n<p>As mentioned, we have two containers. The names of our containers are <code>web</code> and <code>db</code>. These names can be anything you want, as long as they're descriptive and make sense to you. The <code>web</code> container will be our web application and the <code>db</code> container will be our <code>MongoDB</code> database. Notice that we have listed our <code>db</code> service first and then our <code>web</code> service. The reason for this is we want to build and initialize our database prior to our application so that by the time that the web application is initialized, it's able to successfully connect to the database. If done the other way around, an error will be thrown because the database will not be listening for connections and the web application won't be able to connect. Another way to ensure that our database is initialized prior to our web application is to use the <code>links</code> option in our <code>web</code> service and add the name of the database service <code>db</code> to the list of dependent services. The <code>ports</code> option like in our <code>Dockerfile</code> defines which ports that container will need to operate. In this case, our <code>web</code> app listens on port 3000 and the <code>db</code> service will listen on port 27017.</p>\n<h4>Container Images</h4>\n<p>The <code>docker-compose.yml</code> can build containers based on images hosted in a registry as well as those defined by a <code>Dockerfile</code>. To use images from a registry, we use the <code>image</code> option inside of our service. Take the <code>db</code> service for example. <code>MongoDB</code> already has an image in the <code>DockerHub</code> registry which we will use to build the container. Our <code>web</code> container does not have an image that is listed in a registry. However, we can still build an image based off a <code>Dockerfile</code>. To do this, we use the <code>build</code> option inside our service and pass the directory of the respective <code>Dockerfile</code> containing the build instructions for the container.</p>\n\n<p>Now that our container definitions and files are set up, we're ready to build and run our application. This can be done by typing <code>docker-compose up -d</code> in the terminal from inside our local project directory. The <code>-d</code> option runs the command detached allowing us to continue using the terminal. This command will both build and start our containers simultaneously. Once the <code>web</code> and <code>db</code> containers are up and running, we can visit <code>http://localhost:3000</code> from our browser to view and test our application. To stop the application, inside the local project directory, we can type <code>docker-compose stop</code> in our terminal to stop both containers.</p>\n\n<p>This writeup uses a pre-configured MEAN stack CRUD todo application and explores how to define a single container <code>Dockerfile</code> as well as a multi-container application using <code>docker-compose</code>. Docker streamlines how applications are built and deployed while <code>docker-compose</code> allows more complex multi-container applications to be orchestrated, linked, deployed and managed simultaneously allowing developers to spend more time  developing solutions and less time managing infrastructure and dependencies.</p>\n<h6>Links/Resources</h6>\n<p><a href=\"https://www.docker.com/community-edition#/download\">Docker Community Edition</a><br />\n<a href=\"https://docs.docker.com/get-started/\">Docker: Getting Started</a><br />\n<a href=\"https://nodejs.org/en/\">NodeJS</a></p>\n","tags":["devops","MEAN","mongodb","docker","angular","angularjs","nodejs","javascript","programming","development","docker-compose"]},{"title":"Type Driven Development - Scaling Safely with Python","contentType":"posts","date":"2018-03-24 18:16:31","url":"/posts/type-driven-development-scaling-safely-with-python/","content":"\n<p>Python is a great language. The syntax is readable and allows pseudocode to be converted into code nearly verbatim. While this is great for prototyping and moving fast, scaling can become an issue. One of the issues is with regards to documentation. In statically typed languages, even if there's no documentation, types help provide some sort of documentation that allow new contributors as well current developers to remember where they left off and what their code does. There are ways around this using docstrings as well as unit tests. However, this often involves performing tasks outside of writing code which can be time consuming. In Python 3.5, type hints or optional static typing are allowed and tools like <code>mypy</code> help write safer, more scalable code. The best part is, if the code already has docstrings and unit tests, optional static typing adds an additional layer of safety and documentation to existing projects. This writeup explores practices for documenting and developing scalable Python code as well as illustrating how to use optional static types and type checkers.</p>\n\n<p>This writeup assumes that Python 3.5 or greater is being used and both <code>mypy</code> and <code>pytest</code> packages are installed. To install them using <code>pip</code> we can type the following command in the shell:</p>\n<pre><code class=\"language-bash\">pip install pytest mypy\n</code></pre>\n\n<p><a href=\"https://www.python.org/dev/peps/pep-0257/\">PEP 257</a> provides a great overview of what docstrings are and how to use them. The summarized version of it is a string literal in classes and functions that allows developers to document logic, inputs and outputs of those particular sections of code. Below are examples of code with and without docstrings:</p>\n<h3>No Docstring</h3>\n<pre><code class=\"language-python\">def combine(a,b):\n    return a + b\n</code></pre>\n<h3>With Docstring</h3>\n<pre><code class=\"language-python\">def combine(a,b):\n    &amp;quot;&amp;quot;&amp;quot;\n    Returns the sum of two numbers\n\n    Keyword arguments:\n    a -- the first number\n    b -- the second number\n\n    Returns:\n    Sum of a and b\n    &amp;quot;&amp;quot;&amp;quot;\n    return a + b\n</code></pre>\n<p>As we can see, the string literal or docstring allows individuals who are looking at the code for the first time as well as someone who worked on it and has forgotten the logic of a program to easily decipher what the code does.</p>\n<p>Something else to notice is the function name. In this case, the logic is relatively simple and the name may make some sense at the time of writing the code. However, this simple logic is dangerous. Without knowing what is expected as input and output, there's not a clear way of knowing what this code should do. For example, someone might try to run the undocumented version of this function with parameter <code>a</code> having the value 'Hello' and <code>b</code> with the value 'World'. The function would not throw any errors and return 'HelloWorld' as a result. However, if we look at the intented logic as well as expected input and output provided by the docstring, we'd know that <code>a</code> and <code>b</code> are both supposed to be numerical values, not strings or any other type.</p>\n<p>It's clear that writing a docstring can become tedious and take up a substantial amount of time as a project grows. However, the benefits are reaped when extending the code and using it in production because more time is spent being productive rather than figuring out what the code should do and whether it's being done correctly. Docstrings however are not a panacea since there is no way to enforce what is documented in the code and serves as more of an FYI for developers using and extending the code.</p>\n\n<p>One way to prevent code from being misused is by writing tests. By writing unit tests and making sure that they pass, developers can test edge cases such as passing a string and immediately getting feedback through failing tests. Here's an example of what a unit test would look like for the <code>combine</code> function written above.</p>\n<p>In the file <code>main.py</code>, we can write the logic for our <code>combine</code> function. However, keeping in mind the docstring, we might want to add some exception handling.</p>\n<pre><code class=\"language-python\">&amp;quot;&amp;quot;&amp;quot;\nModule Containing Logic\n&amp;quot;&amp;quot;&amp;quot;\n\ndef combine(a,b):\n    &amp;quot;&amp;quot;&amp;quot;                    \n    Returns the sum of two numbers                                          \n    Keyword arguments:                 \n    a -- the first number  \n    b -- the second number                                                      \n    Returns:             \n    Sum of a and b       \n    &amp;quot;&amp;quot;&amp;quot;\n    if(type(a) == str or type(b) == str)\n        return a + b\n    else:\n        raise TypeError\n</code></pre>\n<p>In another file called <code>test_main.py</code>, we can write our tests. Our test file will look like the code below:</p>\n<pre><code class=\"language-python\">import pytest\nfrom main import combine\n\ntestparams = [(1,2,3),(2,4,6)]\n@pytest.mark.parametrize(&amp;quot;a,b,expected&amp;quot;,testparams)\ndef test_combine(a,b,expected): \n    assert combine(a,b) == expected\n\ntestparams = [('a','b'),('a','b')]\n@pytest.mark.parametrize(&amp;quot;a,b&amp;quot;,testparams)\ndef test_combine_exception(a,b):\n    with pytest.raises(TypeError):\n        combine(a,b)\n</code></pre>\n<p>In our shell we can enter the <code>pytest</code> command inside of our project directory and get the following output.</p>\n<pre><code class=\"language-bash\">pytest\n</code></pre>\n<p><img src=\"/files/images/typedrivendevelopment1.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>The results from <code>pytest</code> ensure that passing the expected parameters returns the expected output which is the sum of two numbers and passing in the wrong parameters such as those of type string return a <code>TypeError</code>. This gets us closer to where we want to be where we're able to test whether the functionality of our application does what it's supposed to. Like docstrings, there is additional work and time that needs to be accounted for when writing tests. However, this is a practice that should be taking place already and in the case of Python which does not provide the type checking or compilation is a way to if not ensure that our logic is sound, at least it provides us with an additional form of documentation and peace of mind that the code is being used accurately.</p>\n\n<p>Good practice would have us write docstrings to document our code and unit tests to ensure the soundness of our logic and code. However, what if that seems like too much work or there's not much time to perform those tasks. Is there a shorthand way that we can both document our code for posterity as well as ensure that we can only use the code as intended. That's where type hints comes in and starting with Python 3.5 have been accepted by the Python community per <a href=\"https://www.python.org/dev/peps/pep-0484/\">PEP 484</a>. With type hints our code would not change much and with a few extra characters, we can write safer code. Our <code>combine</code> function from previous examples would look as follows with type hints:</p>\n<pre><code class=\"language-python\">def combine(a:float,b:float) -&amp;gt; float:\n    return a + b    \n</code></pre>\n<p>If we run this, it should run as expected given the appropriate parameters. That being said, as with the undocumented example, if we pass in parameters 'Hello' and 'World', it should work as well and we get the result 'HelloWorld'. If we still don't get the result we want and our code is still unsafe, then what's the point? One of the benefits is the documentation piece. In the event that we had no docstring, we could still tell that <code>a</code> and <code>b</code> are both of type <code>float</code> and return a <code>float</code>. The second benefit comes from the use of <code>mypy</code>, a type checker for Python. To see it in action, we can create a script called <code>mainmypy.py</code> and add the following code:</p>\n<pre><code class=\"language-python\">def combine(a:float,b:float) -&amp;gt; float:\n    return a + b\n\ncombine(1,2)\ncombine('Hello','World')\n</code></pre>\n<p>In the shell, we can then use the <code>mypy</code> command on our script to check types.</p>\n<pre><code class=\"language-bash\">mypy mainmypy.py\n</code></pre>\n<p>The output is the following:</p>\n<p><img src=\"/files/images/typedrivendevelopment2.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>As we can see, without having to run our code, <code>mypy</code> checks the types and throws errors that we would not have found unless we ran our code. Therefore, we get both documentation by defining the types of parameters and outputs we expect which make it easier for individuals using or writing code to safely do so without having to write long, descriptive docstrings. With <code>mypy</code>, we enforce the good use of code by checking that the correct parameters are being passed in and the correct results are being returned prior to runtime making it safe to scale and write correct code most of the time.</p>\n\n<p>Python is a very expressive language that allows applications to be prototyped in no time. However, the tradeoff is that writing new code or returning to it at a later time without documenting it, particularly the types needed by functions or classes to produce an accurate result can be unsafe. Some existing practices such as docstrings and unit tests can help with documenting and writing safe code. However, tools like <code>mypy</code> and the recently introduced type hints achieve what both docstrings and unit tests do in less time and code. This is not to say that these tools are perfect and ideally, unit tests, docstrings and type hints are all integrated to make developers more productive and create safe, scalable code. Happy coding!</p>\n","tags":["python","static typing","productivity","programming","development"]},{"title":"Scaffolding A Web Page In One Line","contentType":"posts","date":"2018-03-10 16:07:00","url":"/posts/scaffold-a-webpage-in-one-line/","content":"\n<p>A few months ago, I saw VS Code released an update that had <a href=\"https://www.emmet.io/\">Emmet</a> abbreviations built in. At the time, I saw a demo and it looked like an interesting tool but I knew very little about it. This past week, while practicing <code>css</code> and scaffolding a blog-like web page for practice I decided to give Emmet a try. After reading some of the documentation and trying it out, within five minutes I was able to replicate what I had done manually with one line of &amp;quot;code&amp;quot;. The entire process is described below.</p>\n\n<p>Displayed below is the scaffold of the webpage I built. As it can be seen, in the <code>body</code> element there is a <code>nav</code> element and four <code>div</code> elements that will contain our articles and their metadata.</p>\n<pre><code class=\"language-html\">&amp;lt;html&amp;gt;\n&amp;lt;head&amp;gt;\n    &amp;lt;title&amp;gt;&amp;lt;/title&amp;gt;\n    &amp;lt;link rel=&amp;quot;stylesheet&amp;quot; href=&amp;quot;&amp;quot;&amp;gt;\n&amp;lt;/head&amp;gt;\n&amp;lt;body&amp;gt;\n    &amp;lt;nav&amp;gt;\n        &amp;lt;ul&amp;gt;\n            &amp;lt;li class=&amp;quot;nav-item&amp;quot;&amp;gt;&amp;lt;/li&amp;gt;\n            &amp;lt;li class=&amp;quot;nav-item&amp;quot;&amp;gt;&amp;lt;/li&amp;gt;\n            &amp;lt;li class=&amp;quot;nav-item&amp;quot;&amp;gt;&amp;lt;/li&amp;gt;\n        &amp;lt;/ul&amp;gt;\n    &amp;lt;/nav&amp;gt;\n    &amp;lt;div&amp;gt;\n        &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;\n            &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;&amp;lt;/h2&amp;gt;\n            &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;&amp;lt;/h6&amp;gt;\n            &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;&amp;lt;/p&amp;gt;\n        &amp;lt;/div&amp;gt;\n    &amp;lt;/div&amp;gt;\n    &amp;lt;div&amp;gt;\n        &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;\n            &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;&amp;lt;/h2&amp;gt;\n            &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;&amp;lt;/h6&amp;gt;\n            &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;&amp;lt;/p&amp;gt;\n        &amp;lt;/div&amp;gt;\n    &amp;lt;/div&amp;gt;\n    &amp;lt;div&amp;gt;\n        &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;\n            &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;&amp;lt;/h2&amp;gt;\n            &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;&amp;lt;/h6&amp;gt;\n            &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;&amp;lt;/p&amp;gt;\n        &amp;lt;/div&amp;gt;\n    &amp;lt;/div&amp;gt;\n    &amp;lt;div&amp;gt;\n        &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;\n            &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;&amp;lt;/h2&amp;gt;\n            &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;&amp;lt;/h6&amp;gt;\n            &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;&amp;lt;/p&amp;gt;\n        &amp;lt;/div&amp;gt;\n    &amp;lt;/div&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n</code></pre>\n\n<p>The <code>hmtl</code> above can be programatically built using the abbreviation below.</p>\n<pre><code class=\"language-text\">html&amp;gt;(head&amp;gt;title+link)+(body&amp;gt;(nav&amp;gt;ul&amp;gt;li.nav-item*3)+(div&amp;gt;div.article&amp;gt;h2.article-title+h6.article-date+p.article-description)*4\n</code></pre>\n<p>To break down some of the things that are going on here, we can start with the syntax. The <code>&amp;gt;</code> character denotes that the item on the left is the parent of the item(s) on the right. Similarly, the <code>+</code> character is used to describe siblings. Similar to math operations, parentheses evaluate what's inside them. Elements themselves can contain additional data such as class names and ids. This data can be appended to the element name just like <code>css</code> with <code>.</code> for classes and <code>#</code> for ids.</p>\n<p>With that knowledge, we can see that we have an <code>html</code> root element with <code>head</code> and <code>body</code> children. The <code>head</code> element has a <code>title</code> and <code>link</code> child elements. The <code>body</code> element has a <code>nav</code> element with a <code>ul</code> child which contains three <code>li</code> elements. The <code>nav</code> element has a <code>div</code> sibling which contains four <code>div</code> children that serve as the containers for our articles and their metadata.</p>\n\n<p>To help visualize what the page looks like we can add some <code>css</code>.</p>\n<pre><code class=\"language-css\">* {\n    margin: 0;\n}\n\nnav {\n    padding: 0;\n    background-color: black;\n    color: white;\n}\n\n.nav-item {\n    margin: 20px;\n    display: inline-block;\n    font-weight: bold;\n    font-size: 20px;\n}\n\n.nav-item:hover {\n    opacity: 0.7;\n}\n\n.article {\n    display: block;\n    margin: 20px;\n    padding-left: 5px;\n    background-color: lightgray;\n    border: black solid 2px;\n}\n\n.article-title {\n    display: block;\n    font-family: serif;\n    font-size: 30px;\n    text-decoration: underline;\n    display: inline;\n}\n\n.article-date {\n    font-size: 20px;\n    font-family: fantasy;\n    float: right;\n    margin-right: 5px;\n}\n\n.article-description {\n    display: block;\n    font-size: 18px;\n    font-family: sans-serif;\n    margin-top: 5px;\n}\n</code></pre>\n\n<p>After linking our <code>css</code> with the webpage as well as adding some content to the page, the result will look as follows.</p>\n<p><img src=\"/files/images/scaffoldawebpageoneline1.png\" class=\"img-fluid\" alt=\"\" /></p>\n<pre><code class=\"language-html\">&amp;lt;html&amp;gt;\n    &amp;lt;head&amp;gt;\n        &amp;lt;link rel=&amp;quot;stylesheet&amp;quot; type=&amp;quot;text/css&amp;quot; href=&amp;quot;main.css&amp;quot;/&amp;gt;\n        &amp;lt;title&amp;gt;HTML CSS Practice&amp;lt;/title&amp;gt;\n    &amp;lt;/head&amp;gt;\n    &amp;lt;body&amp;gt;\n        &amp;lt;nav&amp;gt;\n            &amp;lt;ul&amp;gt;\n                &amp;lt;li class=&amp;quot;nav-item&amp;quot;&amp;gt;Home&amp;lt;/li&amp;gt;\n                &amp;lt;li class=&amp;quot;nav-item&amp;quot;&amp;gt;About&amp;lt;/li&amp;gt;\n                &amp;lt;li class=&amp;quot;nav-item&amp;quot;&amp;gt;Contact&amp;lt;/li&amp;gt;\n            &amp;lt;/ul&amp;gt;\n        &amp;lt;/nav&amp;gt;\n        &amp;lt;div&amp;gt;\n            &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;\n                &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;Hello World&amp;lt;/h2&amp;gt;\n                &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;January 8, 2017&amp;lt;/h6&amp;gt;\n                &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;This is a short description of my program. There are more things than this.&amp;lt;/p&amp;gt;\n            &amp;lt;/div&amp;gt;\n            &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;\n                &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;Second Post&amp;lt;/h2&amp;gt;\n                &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;February 8, 2017&amp;lt;/h6&amp;gt;\n                &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;I think that I got my feet wet a little bit. Maybe there are other things that I can write about&amp;lt;/p&amp;gt;\n            &amp;lt;/div&amp;gt;\n            &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;\n                &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;Third Post&amp;lt;/h2&amp;gt;\n                &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;March 8, 2017&amp;lt;/h6&amp;gt;\n                &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;This is the third go around. I think I got pretty good at this design stuff.&amp;lt;/p&amp;gt;\n            &amp;lt;/div&amp;gt;\n            &amp;lt;div class=&amp;quot;article&amp;quot;&amp;gt;\n                &amp;lt;h2 class=&amp;quot;article-title&amp;quot;&amp;gt;Fourth Post&amp;lt;/h2&amp;gt;\n                &amp;lt;h6 class=&amp;quot;article-date&amp;quot;&amp;gt;June 8, 2017&amp;lt;/h6&amp;gt;\n                &amp;lt;p class=&amp;quot;article-description&amp;quot;&amp;gt;I really like my description of the things that are happening here and there is another post on the way very soon.&amp;lt;/p&amp;gt;\n            &amp;lt;/div&amp;gt;                        \n        &amp;lt;/div&amp;gt;\n    &amp;lt;/body&amp;gt;    \n&amp;lt;/html&amp;gt;\n</code></pre>\n<h6>Sources</h6>\n<p><a href=\"https://www.emmet.io/\">Emmet</a></p>\n","tags":["html","web development","development","tools","emmet"]},{"title":"Testing and Deploying Python Projects with Travis CI","contentType":"posts","date":"2018-02-18 20:51:32","url":"/posts/testing-deploying-python-projects-travisci/","content":"\n<p>When working on projects, especially those that others or yourself may depend upon, it is important to test to make sure that everything is working as expected. Furthermore, being able to deploy your code/packages to a central repository with a package manager makes distribution easier. Although this can all be done manually, it can also be automated. Both testing and deployment can be automated using Travis CI which makes the entire process as easy as pushing your most recent changes to GitHub. In this writeup, I will go over how to create a Python project with unit tests and deploy to PyPI. A sample project can be found at this <a href=\"https://github.com/lqdev/TravisTest\">link</a></p>\n\n<ul>\n<li><a href=\"https://github.com/\">GitHub Login</a></li>\n<li><a href=\"https://pypi.python.org/pypi\">PyPI Login</a></li>\n<li><a href=\"https://virtualenv.pypa.io/en/stable/\">virtualenv</a></li>\n</ul>\n<h3>Install virtualenv</h3>\n<pre><code class=\"language-bash\">sudo pip install virtualenv\n</code></pre>\n\n<p>For the test project, I will create a module that performs adding, subtracting, increment and decrement operations.</p>\n\n<p>We start out by creating a directory for our project.</p>\n<pre><code class=\"language-bash\">mkdir travistest\n</code></pre>\n<p>Inside that directory, we want to have have a directory for our module as well as for our tests. Therefore, we need to create a directory for both of them.</p>\n<pre><code class=\"language-bash\">mkdir travistest\nmkdir test\n</code></pre>\n<p>Finally, we want to initialize the virtual environment for our project. To do so, we can use the <code>virtualenv</code> package. For the project <code>python 3.5</code> is the version that will be used.</p>\n<pre><code class=\"language-bash\">virtualenv -p python3.5 ENV\n</code></pre>\n<p>After installation, a folder with the name <code>ENV</code> should appear in the root directory of the project. The final directory structure should look like so:</p>\n<pre><code class=\"language-text\">travistest\n|_ENV\n|_travistest\n|_test\n</code></pre>\n\n<p>For this project, I'll be using <code>pytest</code> for unit testing. Before installing anything however, I'll need to activate the virtual environment.</p>\n<pre><code class=\"language-bash\">source ENV/bin/activate\n</code></pre>\n<p>Once our virtual environment is activated, we can install <code>pytest</code>.</p>\n<pre><code class=\"language-bash\">pip install pytest\n</code></pre>\n<p>After installation, we can persist installed packages inside a <code>requirements.txt</code> file with the <code>freeze</code> command.</p>\n<pre><code class=\"language-bash\">pip freeze &amp;gt; requirements.txt\n</code></pre>\n\n<p>Inside the <code>travistest</code> module directory, the easiest way to create a module is to include an <code>__init__.py</code> file inside the directory. It's okay if it's empty.</p>\n<p>Therefore, we can start by creating the <code>__init__</code> file in that directory.</p>\n<pre><code class=\"language-bash\">touch __init__.py\n</code></pre>\n<p>Once that's created, we can begin writing the main functionality of our module. Inside a file called <code>Operations.py</code>, we can put the following code in.</p>\n<pre><code class=\"language-python\">class Operations:\n    def __init__(self):\n        pass\n    \n    def add(self,x,y):\n        return x + y\n\n    def subtract(self,x,y):\n        return x - y\n\n    def increment(self,x):\n        return x + 1\n\n    def decrement(self,x):\n    \treturn x - 1\n</code></pre>\n\n<p>Once we have our code, we need to write tests for it. Navigating to the <code>test</code> directory, we can add the following code to the <code>test_operations.py</code> file.</p>\n<pre><code class=\"language-python\">from pytest import fixture\n\n@fixture\ndef op():\n    from travistest.Operations import Operations\n    return Operations()\n\ndef test_add(op):\n    assert op.add(1,2) == 3\n\ndef test_subtract(op):\n    assert op.subtract(2,1) == 1\n\ndef test_increment(op):\n    assert op.increment(1) == 2\n\ndef test_decrement(op):\nassert op.decrement(2) == 1\n</code></pre>\n<p>To make sure everything is working correctly, from the project's root directory, we can run the <code>pytest</code> command. If all goes well, an output similar to the one below should appear.</p>\n<pre><code class=\"language-bash\">============================= test session starts ==============================\nplatform linux -- Python 3.5.2, pytest-3.4.0, py-1.5.2, pluggy-0.6.0\n\ncollected 4 items                                                              \n\ntest/test_operations.py ....                                             [100%]\n\n=========================== 4 passed in 0.04 seconds ===========================\n</code></pre>\n\n<p>To prepare for deployment and uploading to PyPI, we need to add a <code>setup.py</code> file to the root directory of our project. The contents of this file for our purposes are mostly metadata that will populate information in PyPI.</p>\n<pre><code class=\"language-python\">from distutils.core import setup\n\nsetup(\n    name='travistest',\n    packages=['travistest'],\n    version='0.0.7',\n    description='Test project to get acquainted with TravisCI',\n    url='https://github.com/lqdev/TravisTest',    \n)\n</code></pre>\n\n<h3>Enable Repository</h3>\n<p>Assuming that you have a <code>GitHub</code> login and a repository has been created for your project:</p>\n<ol>\n<li>Log into <a href=\"https://travis-ci.org/\">travis-ci.org</a> with your GitHub credentials.</li>\n<li>Once all of your repositores are synced, toggle the switch next to the repository containing your project.</li>\n</ol>\n<h3>Configure .travis.yml</h3>\n<p>Once the project has been enabled, we need fo configure Travis. This is all done using the <code>.travis.yml</code> file.</p>\n<p>In this file we'll tell Travis that the language of our project is Python version 3.5 and that we'll be using a virtual environment. Additionally we'll require sudo priviliges and target the Ubuntu Xenial 16.04 distribution. All of these configurations can be done as follows.</p>\n<pre><code class=\"language-yaml\">sudo: required\ndist: xenial\nlanguage: python\nvirtualenv:\n  system_site_packages: true\npython:\n- '3.5'\n</code></pre>\n<p>Once that is set up, we can tell it to install all of the dependencies stored in our <code>requirements.txt</code> file.</p>\n<pre><code class=\"language-yaml\">install:\n- pip install -r requirements.txt\n</code></pre>\n<p>After this, we need to tell Travis to run our tests just like we would on our local machine.</p>\n<pre><code class=\"language-yaml\">script: pytest\n</code></pre>\n<p>Once our tests have run, we need to make sure we are back in the root directory of our project for deployment.</p>\n<pre><code class=\"language-yaml\">after_script: cd ~\n</code></pre>\n<p>We're done with the automated testing script portion of our project. Now we need to setup deployment options. This section will mainly contain the credentials of your PyPI account.</p>\n<pre><code class=\"language-yaml\">deploy:\n  provider: pypi\n  user: &amp;quot;YOURUSERNAME&amp;quot;\n</code></pre>\n<p>After setting the provider and user, we need to set the password. Because this will be a public repository DO NOT enter your password on this file. Instead, we can set an encrypted version that only Travis can decrypt. To do so, while in the root directory of our project, we can enter the following command into the terminal.</p>\n<pre><code class=\"language-bash\">travis encrypt --add deployment.password\n</code></pre>\n<p>Type your password into the terminal and press <code>Ctrl + D</code>.</p>\n<p>If you take a look at your <code>.travis.yml</code> file you should see something like the following in your <code>deploy</code> options</p>\n<pre><code class=\"language-bash\">deploy:\n  provider: pypi\n  user: &amp;quot;YOURUSERNAME&amp;quot;\n  password:\n    secure: &amp;quot;YOURPASSWORD&amp;quot;\n</code></pre>\n<p>The final <code>.travis.yml</code> file should like like so</p>\n<pre><code class=\"language-yaml\">sudo: required\ndist: xenial\nlanguage: python\nvirtualenv:\n  system_site_packages: true\npython:\n- '3.5'\ninstall:\n- pip install -r requirements.txt\nscript: pytest\nafter_script: cd ~\ndeploy:\n  provider: pypi\n  user: &amp;quot;YOURUSERNAME&amp;quot;\n  password:\n    secure: &amp;quot;YOURPASSWORD&amp;quot;\n</code></pre>\n\n<p>Now that we have everything set up, deployment should be relatively easy. A build is triggered when changes are pushed to the repository on GitHub. Therefore, pushing your local changes to the remote GitHub repository should initialize the build. To track progress, visit the project's page on Travis CI.</p>\n<p><strong>NOTE: WHEN PUSHING NEW CHANGES, INCREMENT VERSION NUMBER IN THE <code>SETUP.PY</code> FILE SO THAT EXISTING FILE ERRORS DO NOT CRASH BUILDS</strong></p>\n\n<p>In this writeup, we created a Python package that performs basic operations and set up automated testing and deployment with Travis CI. Configurations can be further customized and refined to adapt more complex build processes.</p>\n","tags":["devops","travisci","python","ci/cd","programming","development"]},{"title":"Transcribing Podcasts with Microsoft Speech API","contentType":"posts","date":"2018-02-11 20:34:37","url":"/posts/transcribing-podcasts-microsoft-speech-api/","content":"\n<p>I enjoy listening to podcasts in a wide range of topics that include but are not limited to politics, software development, history, comedy and true crime. While some of the more entertainment related podcasts are best enjoyed through audio, those that are related to software development or other type of hands-on topics would benefit greatly from having a transcript. Having a transcript allows me to go back after having listened to an interesting discussion and look directly at the content I am interested in without having to listen to the podcast again. This however is not always feasible given that it costs both time and money to produce a transcript. Fortunately, there are tools out there such as Google Cloud Speech and Microsoft Speech API which allow users to convert speech to text. For this writeup, I will be focusing on the Microsoft Speech API. Because podcasts tend to be long-form, I will be using the C# client library because it allows for long audio (greater than 15 seconds) to be transcribed. The purpose of this exercise is to create a console application that takes audio segments, converts them to text and stores the results in a text file with the goal of evaluating how well the Microsoft Speech API works. The source code for the console application can be found on <a href=\"https://github.com/lqdev/PodcastsBingSpeechAPIDemo\">GitHub</a></p>\n\n<ul>\n<li><a href=\"https://www.visualstudio.com/vs/community/\">Visual Studio Community</a></li>\n<li><a href=\"https://www.microsoft.com/store/productId/9NBLGGH4MSV6\">Windows Subsystem for Linux (Ubuntu)</a></li>\n<li><a href=\"https://ffmpeg.org/\">ffmpeg</a></li>\n</ul>\n\n<ol>\n<li>Open Powershell as Administrator and input</li>\n</ol>\n<pre><code class=\"language-powershell\">Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\n</code></pre>\n<ol start=\"2\">\n<li>Restart your computer</li>\n<li>Open the Microsoft Store and install <a href=\"https://www.microsoft.com/store/productId/9NBLGGH4MSV6\">Ubuntu</a> Linux distribution</li>\n<li>Once installed, click <code>Launch</code></li>\n<li>Create your LINUX user account (Keep in mind that this is not the same as your Windows account therefore it can be different).</li>\n</ol>\n\n<p>Once Ubuntu is installed on your computer and you have created your LINUX user account, it's time to install <code>ffmpeg</code>. This will allow us to convert our file from <code>mp3</code> which is usually the format podcasts are in to <code>wav</code> which is the format accepted by the Microsoft Speech API</p>\n<pre><code class=\"language-bash\">sudo apt install -y ffmpeg\n</code></pre>\n\n<p>In order to use the Microsoft Speech API, an API key is required. This can be obtained using the following steps.</p>\n<ol>\n<li>Navigate to the <a href=\"https://azure.microsoft.com/en-us/try/cognitive-services/\">Azure Cognitive Services</a> page.</li>\n<li>Select the <code>Speech</code> tab</li>\n<li>Click <code>Get API Key</code> and follow the instructions</li>\n<li>Once you have an API key, make sure to store it somewhere like a text file in your computer for future use.</li>\n</ol>\n\n<p>Before transcribing the files, they need to be converted to <code>wav</code> format if they are not already in it. In order to ease processing, they should be split into segments as opposed to having the API process an entire multi megabyte file. To do all this, inside the Ubuntu shell, first navigate to your <code>Documents</code> folder and create a new directory for the project.</p>\n<pre><code class=\"language-bash\">cd /mnt/c/Users/&amp;lt;USERNAME&amp;gt;/Documents\nmkdir testspeechapi\n</code></pre>\n<p><code>USERNAME</code> is your Windows user name. This can be found by typing the following command into the Windows <code>CMD</code> prompt</p>\n<pre><code class=\"language-bash\">echo %USERNAME%\n</code></pre>\n\n<p>One of the podcasts I listen to is <a href=\"https://talkpython.fm/\">Talk Python To Me</a> by <a href=\"https://twitter.com/mkennedy\">Michael Kennedy</a>. For the purpose of this exercise, aside from having great content, all of the episodes are transcribed and a link is provided to the <code>mp3</code> file.</p>\n<p>The file I will be using is from episode <a href=\"https://talkpython.fm/episodes/show/149/4-python-web-frameworks-compared\">#149</a>, but it can easily be any of the episodes.</p>\n<p>In the Ubuntu shell, download the file into the <code>testspeechapi</code> directory that was recently created using <code>wget</code>.</p>\n<pre><code class=\"language-bash\">wget https://talkpython.fm/episodes/download/149/4-python-web-frameworks-compared.mp3 -O originalinput.mp3\n</code></pre>\n\n<p>The transcripts can be found in GitHub at this <a href=\"https://github.com/mikeckennedy/talk-python-transcripts/tree/master/transcripts\">link</a>. The original transcript will allow me to compare it to the output of the Microsoft Speech API and evaluate the accuracy.</p>\n<p>We can download this file into the <code>testspeechapi</code> directory just like the audio file</p>\n<pre><code class=\"language-bash\">wget https://raw.githubusercontent.com/mikeckennedy/talk-python-transcripts/master/transcripts/149.txt -O originaltranscript.txt\n</code></pre>\n\n<p>Now that we have both the original audio file and transcript, it's time to convert the format from <code>mp3</code> to <code>wav</code>. To do this, we can use <code>ffmpeg</code>. In the Ubuntu shell, enter the following command.</p>\n<pre><code class=\"language-bash\">ffmpeg -i originalinput.mp3 -f wav originalinput.wav\n</code></pre>\n\n<p>Once we have the proper format, it's time to make the files more manageable for processing. This can be done by splitting them up into equal segments using <code>ffmpeg</code>. In this case I'll be splitting it up into sixty second segments and storing them into a directory called <code>input</code> with the name <code>filexxxx.wav</code>. The <code>%04d</code> indicates that there will be 4 digits in the file name. Inside the Ubuntu shell and <code>testspeechapi</code> directory enter the following commands.</p>\n<pre><code class=\"language-bash\">mkdir input\nffmpeg -i originalinput.wav -f segment -segment_time 60 -c copy input/file%04d.wav\n</code></pre>\n\n<p>To get started building the console application, we can leverage the <a href=\"https://github.com/Azure-Samples/Cognitive-Speech-STT-ServiceLibrary\">Cognitive-Speech-STT-ServiceLibrary</a> sample program.</p>\n\n<p>The first step will be to clone the sample program from GitHub onto your computer. In the Windows <code>CMD</code> prompt navigate to the <code>testspeechapi</code> folder and enter the following command.</p>\n<pre><code class=\"language-bash\">git clone https://github.com/Azure-Samples/Cognitive-Speech-STT-ServiceLibrary\n</code></pre>\n<p>Once the project is on your computer, open the directory and launch the <code>SpeechClient.sln</code> solution in the <code>sample</code> directory</p>\n<p>When the solution launches, open the <code>Program.cs</code> file and begin making modifications.</p>\n\n<p>To read files, we can create a function that will return the list of files in the specified directory.</p>\n<pre><code class=\"language-csharp\">private static string[] GetFiles(string directory)\n{\n    string[] files = Directory.GetFiles(directory);\n    return files;\n}\n</code></pre>\n\n<p>Once we have the list of files, we can then process each file individually using the <code>Run</code> method. To do so, we need to make a slight modification to our <code>Main</code> method so that it iterates over each file and calls the <code>Run</code> method on it. To store the responses from the API, we'll also need a <code>StringBuilder</code> object which is declared at the top of our <code>Program.cs</code> file.</p>\n<pre><code class=\"language-csharp\">finalResponse = new StringBuilder();\nstring[] files = GetFiles(args[0]);\nforeach(var file in files)\n{\n    p.Run(file,&amp;quot;en-us&amp;quot;,LongDictationUrl,args[1]).Wait();\n    Console.WriteLine(&amp;quot;File {0} processed&amp;quot;,file);\n}\n</code></pre>\n\n<p>The <code>Run</code> method can be left intact. However, the <code>Run</code> method uses the <code>OnRecognitionResult</code> method to handle the result of API responses. In the <code>OnRecognitionResult</code> method, we can remove almost everything that is originally there and replace it. The response from the API returns various results of potential phrases as well as a confidence value. Generally, most of the phrases are alike and the first value is good enough for our purposes. The code for this part will take the response from the API, append it to a <code>StringBuilder</code> object and return when completed.</p>\n<pre><code class=\"language-csharp\">public Task OnRecognitionResult(RecognitionResult args)\n{\n\n    var response = args;\n\n    if(response.Phrases != null)\n    {\n\tfinalResponse.Append(response.Phrases[0].DisplayText);\n\tfinalResponse.Append(&amp;quot;\\n&amp;quot;);\n    }\n\n    return CompletedTask;\n}\n</code></pre>\n\n<p>When all the audio files have been processed, we can save the final output to a text file in the <code>testspeechapi</code> directory. This can be done with the <code>SaveOutput</code> function to which we pass in a file name and the <code>StringBuilder</code> object that captured responses from the API.</p>\n<pre><code class=\"language-csharp\">private static void SaveOutput(string filename,StringBuilder content)\n{\n    StreamWriter writer = new StreamWriter(filename);\n    writer.Write(content.ToString());\n    writer.Close();\n}\n</code></pre>\n<p><code>SaveOutput</code> can then be called from our <code>Main</code> method like so.</p>\n<pre><code class=\"language-csharp\">string username = Environment.GetEnvironmentVariable(&amp;quot;USERNAME&amp;quot;, EnvironmentVariableTarget.Process);\nSaveOutput(String.Format(@&amp;quot;C:\\\\Users\\\\{0}\\\\Documents\\\\testspeechapi\\\\apitranscript.txt&amp;quot;,username), finalResponse);\n</code></pre>\n<p>The final <code>Main</code> method should look similar to the code below</p>\n<pre><code class=\"language-csharp\">public static void Main(string[] args)\n{\n    // Send a speech recognition request for the audio.\n    finalResponse = new StringBuilder();\n\n    string[] files = GetFiles(args[0]);\n\n    var p = new Program();\n\n    foreach (var file in files)\n    {\n\tp.Run(file, &amp;quot;en-us&amp;quot;, LongDictationUrl, args[1]).Wait();\n\tConsole.WriteLine(&amp;quot;File {0} processed&amp;quot;, file);\n    }\n\n    string username = Environment.GetEnvironmentVariable(&amp;quot;USERNAME&amp;quot;, EnvironmentVariableTarget.Process);\n\n    SaveOutput(String.Format(@&amp;quot;C:\\\\Users\\\\{0}\\\\Documents\\\\testspeechapi\\\\apitranscript.txt&amp;quot;,username), finalResponse);\n}\n</code></pre>\n\n<p>The program will save the output of the <code>StringBuilder</code> object <code>finalResponse</code> to the file <code>apitranscript.txt</code>.</p>\n<p>Prior to running the program it needs to be built. In Visual Studio change the Solutions Configurations option from <code>Debug</code> to <code>Release</code> and build the application.</p>\n<p>To run the program, navigate to the <code>C:\\Users\\%USERNAME%\\Documents\\testspeechapi\\Cognitive-Speech-STT-ServiceLibrary\\sample\\SpeechClientSample\\bin\\Release</code> directory in the Windows <code>CMD</code> prompt and enter the following command and pass in the <code>input</code> directory where all the audio segments are stored and your API Key.</p>\n<pre><code class=\"language-bash\">SpeechClientSample.exe C:\\\\Users\\\\%USERNAME%\\\\Documents\\\\testspeechapi\\\\input &amp;lt;YOUR-API-KEY&amp;gt;\n</code></pre>\n<p>This process may take a while due to the number of files being processed, so be patient.</p>\n\n<p>Original</p>\n<blockquote class=\"blockquote\">\n<p>Are you considering getting into web programming? Choosing a web framework like Pyramid, Flask, or Django can be daunting. It would be great to see them all build out the same application and compare the results side-by-side. That's why when I heard what Nicholas Hunt-Walker was up to, I had to have him on the podcast. He and I chat about four web frameworks compared. He built a data-driven web app with Flask, Tornado, Pyramid, and Django and then put it all together in a presentation. We're going to dive into that right now. This is Talk Python To Me, Episode 149, recorded January 30th, 2018. Welcome to Talk Python to Me, a weekly podcast on Python, the language, the libraries, the ecosystem, and the personalities. This is your host, Michael Kennedy, follow me on Twitter where I'm @mkennedy. Keep up with the show and listen to past episodes at talkpython.fm, and follow the show on Twitter via @talkpython. Nick, welcome to Talk Python.</p>\n</blockquote>\n<p>API Response</p>\n<blockquote class=\"blockquote\">\n<p>Are you considering getting into web programming cheese in the web framework like plaster. Django can be daunting. It would be great to see them. Although that the same application and compare the results side by side. That's Why? When I heard. But Nicholas Hunt. Walker was up to, I had him on the podcast. He night chat about 4 web frameworks, compared he built a data driven web app with last tornado. Peermade Angangueo and then put it all together in a presentation. We have dive into that right now. This is talk by the to me, I was 149 recorded January 30th 2018. Welcome to talk python to me a weekly podcast on Python Language the library ecosystem in the personalities. Did you host Michael Kennedy? Follow me on Twitter where I'm at in Kennedy keep up with the show in the past episodes at talk python. Damn info, the show on Twitter via at Popeyes on. Nick Welcome to talk by phone.</p>\n</blockquote>\n\n<p>In this writeup, I converted an <code>mp3</code> podcast file to <code>wav</code> format and split it into sixty second segments using <code>ffmpeg</code>. The segments were then processed by a console application which uses the Microsoft Speech API to convert speech to text and the results were saved to a text file. When compared to the original transcript, the result produced by the API is inconsistent and at times incomprehensible. This does not mean that overall the API is unusable or inaccurate as many things could have contributed to the inaccuracy. However, the results were not as good as I would have hoped for.</p>\n<h6>Sources</h6>\n<ul>\n<li><a href=\"https://docs.microsoft.com/en-us/windows/wsl/install-win10\">Windows Subsystem for Linux Install Instructions</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/cognitive-services/speech/home\">Microsoft Speech API</a></li>\n</ul>\n","tags":["azure","csharp","apis","nlp","machine-learning","artificial-intelligence","microsoft"]},{"title":"Real-Time Sentiment Analysis with C#","contentType":"posts","date":"2018-01-18 03:41:47","url":"/posts/real-time-sentiment-analysis-csharp/","content":"<h6>This is strictly for use with the .NET Framework. With Mono it might be able to work on other platforms. <code>SimpleNetNlp</code> does not currently work with .NET Core/Standard</h6>\n<p>In this project, I will demonstate how to perform sentiment analysis on tweets using various C# libraries.</p>\n\n<ul>\n<li>TweetInviAPI</li>\n<li>SimpleNetNlp</li>\n<li>SimpleNetNlp.Models.LexParser</li>\n<li>SimpleNetNlP.Models.Sentiment</li>\n</ul>\n\n<p>In Visual Studio, Click File &amp;gt; New &amp;gt; New Project &amp;gt; Console Application</p>\n<p>All of the code below will be placed in the <code>Program</code> class.</p>\n\n<h3>Authenticate</h3>\n<p>Thanks to the <code>Tweetinvi</code> library, the authentication with the Twitter API is a breeze. Assuming that an application has been registered at <a href=\"http://apps.twitter.com\">http://apps.twitter.com</a>, the <code>SetUserCredentials</code> method can be used and the <code>Consumer Key</code>, <code>Consumer Secret</code>,<code>Access Token</code> and <code>Access Token Secret</code> can be passed into it. This type of global authentication makes it easy to perform authenticated calls throughout the entire application.</p>\n<pre><code class=\"language-csharp\">Auth.SetUserCredentials(&amp;quot;consumer-key&amp;quot;,&amp;quot;consumer-secret&amp;quot;,&amp;quot;access-token&amp;quot;,&amp;quot;access-token-secret&amp;quot;);\n</code></pre>\n<h3>Build Stream</h3>\n<p>Like the authentication, creating a stream is seamless.</p>\n<p>We can create a stream by calling the <code>CreateFilteredStream</code> method.</p>\n<pre><code class=\"language-csharp\">var stream = Stream.CreateFilteredStream();\n</code></pre>\n<p>We can then add conditions to filter on using the <code>AddTrack</code> method. In this case, I will be filtering for cryptocurrencies, ether, bitcoin, and litecoin.</p>\n<pre><code class=\"language-csharp\">stream.AddTrack(&amp;quot;cryptocurrencies&amp;quot;);\nstream.AddTrack(&amp;quot;bitcoin&amp;quot;);\nstream.AddTrack(&amp;quot;ether&amp;quot;);\nstream.AddTrack(&amp;quot;Litecoin&amp;quot;);\n</code></pre>\n<p>Additionally, we can filter by language. In my case, I will only be filtering on English. This can be done by using the <code>AddTweetLanguageFilter</code> method.</p>\n<pre><code class=\"language-csharp\">stream.AddTweetLanguageFilter(&amp;quot;en&amp;quot;);\n</code></pre>\n<p>Once we have all the filters set up, we need to handle what will happen when a matching tweet is detected. This will be handled by an <code>EventHandler</code> called <code>MatchingTweetReceived</code>.</p>\n<pre><code class=\"language-csharp\">stream.MatchingTweetReceived += OnMatchedTweet;\n</code></pre>\n<p><code>MatchingTweetReceived</code> will be bound to the <code>OnMatchedTweet</code> method which I created.</p>\n<pre><code class=\"language-csharp\">private static void OnMatchedTweet(object sender, MatchedTweetReceivedEventArgs args)\n{\n  //Do Stuff\n}\n</code></pre>\n<p>The logic inside of this method will perform sentiment analysis and output the sentiment as well as the full text of the tweet.</p>\n\n<p>Tweets can contain many non-ascii characters. Therefore, we need to sanitize it as best as possible so that it can be processed by the sentiment analyzer. To help with that, I used regular expresions to replace non-ascii characters inside of the <code>sanitize</code> method.</p>\n<pre><code class=\"language-csharp\">private static string sanitize(string raw)\n{\n  return Regex.Replace(raw, @&amp;quot;(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)&amp;quot;, &amp;quot; &amp;quot;).ToString();\n}\n</code></pre>\n\n<p>In order to perform sentiment analysis, we will be using the <code>SimpleNetNlp</code> library. This library is built on top of the Stanford CoreNLP library. In order to get the sentiment of a piece of text, we need to create a <code>Sentence</code> object which takes a string as a parameter and then get the <code>Sentiment</code> property. In our case, the parameter that will be used to instantiate a new <code>Sentence</code> object will be the sanitized text of a tweet.</p>\n<pre><code class=\"language-csharp\">var sanitized = sanitize(args.Tweet.FullText);\nstring sentence = new Sentence(sanitized);\n</code></pre>\n<p>The code above will be placed inside of the <code>OnMatchedTweet</code> method.</p>\n\n<p>Now that we have everything set up, we can just output to the console the sentiment and raw text of the tweet. To do that, we can place the code below inside the <code>OnMatchedTweet</code> method.</p>\n<pre><code class=\"language-csharp\">Console.WriteLine(sentence.Sentiment + &amp;quot;|&amp;quot; + args.Tweet);\n</code></pre>\n<p>The final <code>OnMatchedTweet</code> method looks as follows:</p>\n<pre><code class=\"language-csharp\">private static void OnMatchedTweet(object sender, MatchedTweetReceivedEventArgs args)\n{\n    var sanitized = sanitize(args.Tweet.FullText); //Sanitize Tweet\n    var sentence = new Sentence(sanitized); //Get Sentiment\n\n    //Output Tweet and Sentiment\n    Console.WriteLine(sentence.Sentiment + &amp;quot;|&amp;quot; + args.Tweet);\n}\n</code></pre>\n\n<p>Once we run the application, our console application should look something like this:</p>\n<p><img src=\"/files/images/sentiment-analysis-1.png\" class=\"img-fluid\" alt=\"\" /></p>\n\n<p>C# is not always the first language that comes to mind when doing analytics and machine learning. However, tasks such as sentiment analysis can be trivially performed thanks to libraries such as <code>Tweetinvi</code> and <code>SimpleNetNlp</code>. In its current state, this application is not very useful because it just outputs to the console sentiments and the respective tweets. In order to make it more useful, we can collect and aggregate the data someplace for more robust analysis.</p>\n","tags":["data analysis","sentiment analysis","nlp","machine learning","csharp","c#","twitter","api",".net","dotnet"]},{"title":"Authorization Code Authentication Flow in Python","contentType":"posts","date":"2017-12-29 18:45:19","url":"/posts/authorization-code-flow-python/","content":"<p>For the longest time, authentication had been my worst enemy. I took small steps towards becoming comfortable with it. At first I learned Implicit Authorization with AngularJS, then I learned Client Credentials Authorization with Python and C# and finally, I learned Authorization Code authentication with C#. The one that always gave me trouble was Authorization Code authentication because it requires user credentials.</p>\n<p>To help with that, I started with the <code>WebAuthenticationBroker</code> in C#. Although, most of it was done for me in terms of creating a <code>WebView</code> for users to authenticate with their credentials, I wanted to know whether I could do it myself without having to rely on a service to do it for me.</p>\n<p>First, I had to find an API to test this out with and in an attempt to reduce complexity, I decided to prototype it in Python. As an avid user of <a href=\"http://getpocket.com\">Pocket</a>, I chose its API to learn Authorization Flow authentication.</p>\n\n\n<p>In order to get started, we need to register an application with Pocket. This can be done by visiting the following <a href=\"https://getpocket.com/developer/apps/new\">website</a>.</p>\n\n<p>In order to create HTTP requests, we need to install the <code>requests</code> module</p>\n<pre><code>pip install requests\n</code></pre>\n\n\n<pre><code class=\"language-python\">import requests\nimport json\n</code></pre>\n\n<pre><code class=\"language-python\">consumer_key = &amp;quot;YOUR_CONSUMER_KEY&amp;quot;\nrequest_url = &amp;quot;https://getpocket.com/v3/oauth/request&amp;quot;\nheaders = {&amp;quot;Content-Type&amp;quot;:&amp;quot;application/json&amp;quot;,&amp;quot;X-Accept&amp;quot;:&amp;quot;application/json&amp;quot;}\nredirect_uri = &amp;quot;http://www.google.com&amp;quot;\npayload = {&amp;quot;consumer_key&amp;quot;:consumer_key,&amp;quot;redirect_uri&amp;quot;:redirect_uri}\n</code></pre>\n<p>Notice how the <code>redirect_uri</code> is set to <code>http://www.google.com</code>. This, for the most part is irrelevant, especially for console/desktop applications. Since this is a console application and we are not hosting a server for it, the value assigned to this field is arbitrary.</p>\n\n<pre><code class=\"language-python\">request = requests.post(request_url,headers=headers,data=json.dumps(payload))\ncode = request.json()['code']\ncode\n</code></pre>\n\n\n<pre><code class=\"language-python\">import webbrowser\n</code></pre>\n<pre><code class=\"language-python\">webbrowser.open('https://getpocket.com/auth/authorize?request_token='+ code + '&amp;amp;redirect_uri=' + redirect_uri)\n</code></pre>\n<p>This will open your default browser and redirect you to an authorization page. After logging in, should you accept giving the application access to your account, it should redirect you to <code>http://www.google.com</code>. Once on the Google page, you can close out of it.</p>\n\n<p>After authorizing the application to access your account, you need to exchange the code you received for an access token.</p>\n\n<pre><code class=\"language-python\">access_token_url = &amp;quot;https://getpocket.com/v3/oauth/authorize&amp;quot;\npayload = {&amp;quot;consumer_key&amp;quot;:consumer_key,&amp;quot;code&amp;quot;:code}\n</code></pre>\n\n<pre><code class=\"language-python\">access_token_request = requests.post(access_token_url,headers=headers,data=json.dumps(payload))\naccess_token = access_token_request.json()['access_token']\naccess_token\n</code></pre>\n\n<p>In order to test whether the authentication was successful, try making an authenticated request.</p>\n\n<pre><code class=\"language-python\">get_url = &amp;quot;https://getpocket.com/v3/get&amp;quot;\nget_payload = {&amp;quot;consumer_key&amp;quot;:consumer_key,&amp;quot;access_token&amp;quot;:access_token,&amp;quot;contentType&amp;quot;:&amp;quot;article&amp;quot;,&amp;quot;count&amp;quot;:10}\nget_request = requests.get(get_url,data=json.dumps(get_payload),headers=headers)\n</code></pre>\n\n<pre><code class=\"language-python\">response = get_request.json()\n</code></pre>\n\n<pre><code class=\"language-json\">{'complete': 1,\n 'error': None,\n 'list': {'1519049132': {'excerpt': 'Heads up! As part of our efforts to improve security and standards-based interoperability, we have implemented several new features in our authentication flows and made changes to existing ones.',\n   'favorite': '0',\n   'given_title': '',\n   'given_url': 'https://auth0.com/docs/api-auth/grant/authorization-code-pkce',\n   'has_image': '0',\n   'has_video': '0',\n   'is_article': '1',\n   'is_index': '0',\n   'item_id': '1519049132',\n   'resolved_id': '1519049132',\n   'resolved_title': 'Calling APIs from Mobile Apps',\n   'resolved_url': 'https://auth0.com/docs/api-auth/grant/authorization-code-pkce',\n   'sort_id': 5,\n   'status': '0',\n   'time_added': '1514493110',\n   'time_favorited': '0',\n   'time_read': '0',\n   'time_updated': '1514493110',\n   'word_count': '445'},\n  '1607678551': {'excerpt': 'What is the smallest number of Democrats that could have changed the outcome of the 2016 United States presidential election by relocating to another state? And where should they have moved?  It turns out this question is a variant of the knapsack problem, an NP-hard computer science classic.',\n   'favorite': '0',\n   'given_title': '',\n   'given_url': 'https://mike.place/2017/ecknapsack/',\n   'has_image': '0',\n   'has_video': '0',\n   'is_article': '1',\n   'is_index': '0',\n   'item_id': '1607678551',\n   'resolved_id': '1607678551',\n   'resolved_title': 'The Electoral College and the knapsack problem',\n   'resolved_url': 'http://mike.place/2017/ecknapsack/',\n   'sort_id': 7,\n   'status': '0',\n   'time_added': '1514478701',\n   'time_favorited': '0',\n   'time_read': '0',\n   'time_updated': '1514478701',\n   'word_count': '1564'},\n  '1616406703': {'excerpt': 'OAuth 2.0 is a protocol that allows a user to grant limited access to their resources on one site, to another site. This is done without the users having to expose their credentials. According to OAuth‘s website the protocol is not unlike a valet key.  Many luxury cars today come with a valet key.',\n   'favorite': '0',\n   'given_title': '',\n   'given_url': 'https://auth0.com/docs/protocols/oauth2',\n   'has_image': '1',\n   'has_video': '0',\n   'is_article': '1',\n   'is_index': '0',\n   'item_id': '1616406703',\n   'resolved_id': '1616406703',\n   'resolved_title': 'OAuth 2.0',\n   'resolved_url': 'https://auth0.com/docs/protocols/oauth2',\n   'sort_id': 4,\n   'status': '0',\n   'time_added': '1514493120',\n   'time_favorited': '0',\n   'time_read': '0',\n   'time_updated': '1514493120',\n   'word_count': '823'},\n  '1859670338': {'excerpt': 'Last Tuesday saw the official launch of Will Robots Take My Job? and 5 days later we have passed 500K visitors and 4M page views.  To say this surpassed any expectations we had would be a major understatement. This is a recap of how the project got started and the first 5 days after launch.',\n   'favorite': '0',\n   'given_title': '',\n   'given_url': 'https://hackernoon.com/from-idea-to-4m-page-views-in-4-weeks-622aa194787d?ref=quuu',\n   'has_image': '1',\n   'has_video': '1',\n   'is_article': '1',\n   'is_index': '0',\n   'item_id': '1859670338',\n   'resolved_id': '1773505341',\n   'resolved_title': 'From Idea to 4M Page Views in 4\\xa0Weeks',\n   'resolved_url': 'https://hackernoon.com/from-idea-to-4m-page-views-in-4-weeks-622aa194787d',\n   'sort_id': 0,\n   'status': '0',\n   'time_added': '1514494606',\n   'time_favorited': '0',\n   'time_read': '0',\n   'time_updated': '1514494606',\n   'word_count': '1837'},\n  '2001727306': {'amp_url': 'https://venturebeat.com/2017/12/22/how-ibm-builds-an-effective-data-science-team/amp/',\n   'excerpt': 'Data science is a team sport. This sentiment rings true not only with our experiences within IBM, but with our enterprise customers, who often ask us for advice on how to structure data science teams within their own organizations.',\n   'favorite': '0',\n   'given_title': '',\n   'given_url': 'https://venturebeat.com/2017/12/22/how-ibm-builds-an-effective-data-science-team/',\n   'has_image': '1',\n   'has_video': '0',\n   'is_article': '1',\n   'is_index': '0',\n   'item_id': '2001727306',\n   'resolved_id': '2001727306',\n   'resolved_title': 'How IBM builds an effective data science team',\n   'resolved_url': 'https://venturebeat.com/2017/12/22/how-ibm-builds-an-effective-data-science-team/',\n   'sort_id': 1,\n   'status': '0',\n   'time_added': '1514493979',\n   'time_favorited': '0',\n   'time_read': '0',\n   'time_updated': '1514493979',\n   'word_count': '773'},\n  '2006596547': {'excerpt': 'I ran, a lot, this year— 2017 miles over 280 hours, or just short of 12 full days. For many of you, that seems like a ridiculous amount of distance and time, but to put it in perspective, elite marathoners and ultrarunners easily run two or three times that distance in a year.',\n   'favorite': '0',\n   'given_title': '',\n   'given_url': 'https://medium.com/run-ruminate/17-thoughts-on-running-life-and-startups-5d305223669b',\n   'has_image': '1',\n   'has_video': '0',\n   'is_article': '1',\n   'is_index': '0',\n   'item_id': '2006596547',\n   'resolved_id': '2006596547',\n   'resolved_title': '17 thoughts on running, startups, and life while running 2017 miles in\\xa02017',\n   'resolved_url': 'https://medium.com/run-ruminate/17-thoughts-on-running-life-and-startups-5d305223669b',\n   'sort_id': 6,\n   'status': '0',\n   'time_added': '1514479812',\n   'time_favorited': '0',\n   'time_read': '0',\n   'time_updated': '1514479812',\n   'word_count': '1561'},\n  '2006920392': {'excerpt': 'What the heck is “work” anyway?  In the Information Age, the dictionary definition of the word just doesn’t cut it anymore. Skill sets, jobs, and entire companies are forming daily based on new technology, market demands, and trends that didn’t exist even just a few years ago.',\n   'favorite': '0',\n   'given_title': '',\n   'given_url': 'https://www.hellosign.com/blog/the-future-of-work',\n   'has_image': '1',\n   'has_video': '0',\n   'is_article': '1',\n   'is_index': '0',\n   'item_id': '2006920392',\n   'resolved_id': '2006920392',\n   'resolved_title': '4 Pillars Of The Future of Work',\n   'resolved_url': 'https://www.hellosign.com/blog/the-future-of-work',\n   'sort_id': 2,\n   'status': '0',\n   'time_added': '1514493449',\n   'time_favorited': '0',\n   'time_read': '0',\n   'time_updated': '1514493449',\n   'word_count': '1805'},\n  '2007117419': {'excerpt': 'Finding a job is not an easy task. There are multiple ways to look for jobs. Multiple steps has to be completed before you are called for an interview. After completing all these the most annoying part is when your interview doesn’t even last for 10 minutes.',\n   'favorite': '0',\n   'given_title': '',\n   'given_url': 'https://www.jobsstudios.com/blog/interview-tips',\n   'has_image': '0',\n   'has_video': '0',\n   'is_article': '1',\n   'is_index': '0',\n   'item_id': '2007117419',\n   'resolved_id': '2007117419',\n   'resolved_title': '7 Tips to prepare for an Interview',\n   'resolved_url': 'https://www.jobsstudios.com/blog/interview-tips',\n   'sort_id': 8,\n   'status': '0',\n   'time_added': '1514477292',\n   'time_favorited': '0',\n   'time_read': '0',\n   'time_updated': '1514477292',\n   'word_count': '468'},\n  '2007245672': {'excerpt': 'RESTful API Versioning, though a simple and elegant concept, is a LOT harder to enforce than it sounds. It’s hard to not break backward compatibility on a continually evolving API, and though API versioning is a great concept, it’s rarely followed without flaw.',\n   'favorite': '0',\n   'given_title': '',\n   'given_url': 'https://shift.infinite.red/snapshot-testing-api-calls-the-right-way-58ef59b7f71b',\n   'has_image': '1',\n   'has_video': '1',\n   'is_article': '1',\n   'is_index': '0',\n   'item_id': '2007245672',\n   'resolved_id': '2007245672',\n   'resolved_title': 'Snapshot Testing API Calls The Right\\xa0Way',\n   'resolved_url': 'https://shift.infinite.red/snapshot-testing-api-calls-the-right-way-58ef59b7f71b',\n   'sort_id': 3,\n   'status': '0',\n   'time_added': '1514493189',\n   'time_favorited': '0',\n   'time_read': '0',\n   'time_updated': '1514493189',\n   'word_count': '1230'},\n  '234097661': {'excerpt': 'Sanchez examines the question, concluding that intellectuals support government intervention because it makes their work have greater importance.',\n   'favorite': '0',\n   'given_title': '',\n   'given_url': 'https://www.libertarianism.org/blog/why-do-intellecuals-support-government-solutions',\n   'has_image': '0',\n   'has_video': '0',\n   'is_article': '1',\n   'is_index': '0',\n   'item_id': '234097661',\n   'resolved_id': '234097661',\n   'resolved_title': 'Why Do Intellectuals Support Government Solutions?',\n   'resolved_url': 'https://www.libertarianism.org/blog/why-do-intellecuals-support-government-solutions',\n   'sort_id': 9,\n   'status': '0',\n   'time_added': '1514477287',\n   'time_favorited': '0',\n   'time_read': '0',\n   'time_updated': '1514477287',\n   'word_count': '1423'}},\n 'search_meta': {'search_type': 'normal'},\n 'since': 1514514495,\n 'status': 1}\n</code></pre>\n<p>Hope this helps!</p>\n<h6>Sources</h6>\n<p><a href=\"https://getpocket.com/developer/docs/authentication\">Pocket API</a><br />\n<a href=\"http://docs.python-requests.org/en/master/\">Requests</a></p>\n","tags":["python","api","programming","security","authentication"]},{"title":"Client Credentials Authorization in C#","contentType":"posts","date":"2017-12-25 19:28:48","url":"/posts/client-credentials-authentication-csharp/","content":"\n<p>For this writeup, I'm going to use the Spotify API. Spotify API supports different authorization flows. In this writeup, I will be using the client credentials authorization flow. Generally this works for server-to-server authentication. Because this does not allow users the ability to provide their own credentials, there is no access to endpoints that contain user data.</p>\n<p><img src=\"/files/images/clientcredentialsflowdiagram.png\" class=\"img-fluid\" alt=\"\" /></p>\n\n<pre><code class=\"language-bash\">dotnet new console -o authtest\n</code></pre>\n\n<pre><code class=\"language-bash\">dotnet add package Newtonsoft.Json --version 10.0.3\n</code></pre>\n\n<p>When a request is made, it needs to be parsed. To better capture the data into a Plain Old CLR Object (POCO), a model can be created.</p>\n<pre><code class=\"language-csharp\">using System;\n\nnamespace authtest\n{\n    class AccessToken\n    {\n        public string access_token { get; set; }\n        public string token_type { get; set; }\n        public long expires_in { get; set; }\n    }\n}\n</code></pre>\n\n<p>In order to extract our token, an HTTP Request needs to be made to the Spotify API in order to get an access token. To do so, we can leverage the <code>HTTPClient</code> functionalities.</p>\n<pre><code class=\"language-csharp\">private static async Task&amp;lt;string&amp;gt; GetToken()\n{\n    string clientId = &amp;quot;YOUR CLIENT ID&amp;quot;;\n    string clientSecret = &amp;quot;YOUR CLIENT SECRET&amp;quot;;\n    string credentials = String.Format(&amp;quot;{0}:{1}&amp;quot;,clientId,clientSecret);\n\n    using(var client = new HttpClient())\n    {\n        //Define Headers\n        client.DefaultRequestHeaders.Accept.Clear();\n        client.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(&amp;quot;application/json&amp;quot;));\n\n        client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(&amp;quot;Basic&amp;quot;,Convert.ToBase64String(Encoding.UTF8.GetBytes(credentials)));\n\n        //Prepare Request Body\n        List&amp;lt;KeyValuePair&amp;lt;string,string&amp;gt;&amp;gt; requestData = new List&amp;lt;KeyValuePair&amp;lt;string,string&amp;gt;&amp;gt;();\n        requestData.Add(new KeyValuePair&amp;lt;string,string&amp;gt;(&amp;quot;grant_type&amp;quot;,&amp;quot;client_credentials&amp;quot;));\n\n        FormUrlEncodedContent requestBody = new FormUrlEncodedContent(requestData);\n\n        //Request Token\n        var request = await client.PostAsync(&amp;quot;https://accounts.spotify.com/api/token&amp;quot;,requestBody);\n        var response = await request.Content.ReadAsStringAsync();\n        return JsonConvert.DeserializeObject&amp;lt;AccessToken&amp;gt;(response);   \n    }\n}\n</code></pre>\n<p>We can then use this function in our main method to request the token.</p>\n<pre><code class=\"language-csharp\">static void Main(string[] args)\n{\n    Console.WriteLine(&amp;quot;Spotify API&amp;quot;);\n    AccessToken token = GetToken().Result;\n    Console.WriteLine(String.Format(&amp;quot;Access Token: {0}&amp;quot;,token.access_token));\n}\n</code></pre>\n<p>This should produce the following output</p>\n<pre><code class=\"language-bash\">Spotify API\nGetting Token\nAccess Token: &amp;quot;YOUR ACCESS TOKEN&amp;quot;\n</code></pre>\n<p>The token can now be used to make requests to the Spotify API.</p>\n<p>Source Code can be found here <a href=\"https://gist.github.com/lqdev/5e82a5c856fcf0818e0b5e002deb0c28\">Source Code</a></p>\n<h6>Sources</h6>\n<p><a href=\"https://developer.spotify.com/web-api/authorization-guide/#client_credentials_flow\">Spotify Authorization Guide</a></p>\n","tags":["csharp","authentication","web applications",".net","api",".net core"]},{"title":"Read A Text File From a URL in .NET Core","contentType":"posts","date":"2017-12-18 21:09:15","url":"/posts/read-file-from-url-dotnet-core/","content":"\n<pre><code class=\"language-bash\">dotnet new console -o urlreader\n</code></pre>\n\n<pre><code class=\"language-bash\">cd urlreader\n</code></pre>\n\n<pre><code class=\"language-csharp\">using System.IO;\nusing System.Net.Http;\nusing System.Threading.Tasks;\n</code></pre>\n\n<p>Create a function in the <code>Program.cs</code> file that will return a <code>StreamReader</code> from the specified URL. Because in this case it will be used inside the <code>Main</code> method which is <code>static</code>, the new function will also have to be <code>static</code>.</p>\n<pre><code class=\"language-csharp\">static StreamReader URLStream(String fileurl){\n    return new StreamReader(new HttpClient().GetStreamAsync(fileurl).Result);\n}\n</code></pre>\n<p>Inside the function, we return a <code>StreamReader</code> which is instantiated by creating an <code>HTTPClient</code> and asynchronously executing a <code>GET</code> request which returns a <code>Task&amp;lt;StreamReader&amp;gt;</code>. In order to resolve the object, the <code>Result</code> property needs to be requested.</p>\n\n<p>Replace the contents of the <code>Main</code> method with the following</p>\n<pre><code class=\"language-csharp\">static void Main(string[] args) {\n    string line;\n    StreamReader s = URLStream(@&amp;quot;https://algs4.cs.princeton.edu/15uf/tinyUF.txt&amp;quot;);\n    String myline = s.ReadLine(); //First Line\n    while((line = s.ReadLine()) != null) //Subsequent Lines\n    {\n        Console.WriteLine(line);\n    }\n}\n</code></pre>\n<p>In this case, the format of the first line is different from the others. However, reading the contents of that line is no different than the other lines.</p>\n\n<pre><code class=\"language-bash\">dotnet run\n\n#Expected Output\n#4 3\n#3 8\n#6 5\n#9 4\n#2 1\n#8 9\n#5 0\n#7 2\n#6 1\n#1 0\n#6 7\n</code></pre>\n","tags":[".net",".net core","microsoft","programming"]},{"title":"Design and Build Your Next Home in Minecraft","contentType":"posts","date":"2017-12-15 22:34:12","url":"/posts/build-your-house-minecraft/","content":"<h4>TLDR</h4>\n<p>Using Minecraft and Mods, virtual representations of the real world can be built to train autonomous agents to navigate and perform tasks in this virtual environment. These virtual models can then be used by physical agents such as Boston Dynamics bots to implement the tasks and instructions learned using the virtual models. Ultimately this makes it reasonable to think of a time where we can design and have our house built in a short time span by autonomous artificial agents.</p>\n\n<p>Minecraft is so hard to describe. Is it a game or is it an educational tool? At first I didn’t get it. I watched younger siblings play this game endlessly always finding ways to improve their worlds. Eventually it was bought by Microsoft and since then it has left the core experience relatively untouched. Instead of shelving it away, Microsoft has expanded across more platforms and loaded it with tons of new features such as an education edition and cross-platform play. In Minecraft I have seen individuals build houses, cities, and even rudimentary models of circuits using Redstone. Furthermore, because the project is programmable, individuals can build modifications also known as Mods to extend the default capabilities.</p>\n<p><img src=\"/files/images/buildyourhouseminecraft1.png\" class=\"img-fluid\" alt=\"\" /></p>\n\n<p>A virtual world is great, but what about the real world? Can we model the real world? A few months ago I came across a project by the New York Public Library which generated topographically accurate Minecraft maps using historical map scans and Python scripts. Although the accuracy of historical maps may be questionable, we can only hope that mapping practices have advanced significantly since then and can provide more details and data. Regardless, this project demonstrates that it is possible to model the real world in a virtual environment given the appropriate data.</p>\n<p><img src=\"/files/images/buildyourhouseminecraft2.png\" class=\"img-fluid\" alt=\"\" /></p>\n\n<p>Aside from having the obvious benefit of having a relatively accurate virtual model of the real world, there is the additional benefit of having a starting point for training autonomous agents to navigate this environment. To aid in this endeavor, Project Malmo is an adequate tool. Released to the public in 2015 by Microsoft Research, Project Malmo is a platform for artificial intelligence experimentation and research built on top of Minecraft. The project is cross-platform and agents can be built in Python, Lua, C#, C++ or Java.</p>\n<p>While the benefits of Project Malmo aside from the pedagogical exercise may not be clearly evident, this project is extremely valuable. The cost not only in terms of price but also time to build physical autonomous bots can be high. If for some reason they were to break or malfunction during field tests it can be costly. Therefore a more effective solution might be to simulate the navigation and mobility of these physical autonomous agents in a real world scenario using virtual environments.</p>\n<p><a href=\"http://www.youtube.com/watch?v=KkVj_ddseO8\"><img src=\"/files/images/buildyourhouseminecraft3.png\" class=\"img-fluid\" alt=\"Project Malmo\" /></a></p>\n\n<p>Once trained, we just need to find or build the physical agents that will carry out the work and instructions based on training performed in the virtual environments. There have been recent demonstrations of agents that may be able of handling such tasks. One of those is Boston Dynamics. For some time, Boston Dynamics has been creating anthropomorphic and zoomorphic robots that perform amazing feats. It can only be assumed that given a model that has been trained in a real-world environment, these physical agents can perform a task like building a home.</p>\n<p>The thought of a bot building a house is not too abstract. On March of 2017 several reports suggested that a home in Russia was built in under 24 hours using 3-D printing technologies. While the house had a predefined layout and the robotic arm that built it was limited in its mobility, such capabilities could be expanded with the aid of a trained model being executed by sophisticated physical autonomous agents such as those built by Boston Dynamics.</p>\n<p><img src=\"/files/images/buildyourhouseminecraft4.png\" class=\"img-fluid\" alt=\"\" /></p>\n<h6>Sources</h6>\n<p><a href=\"https://www.engadget.com/2017/06/14/minecraft-4k-slider-xbox-one-x-e3-microsoft-mojang/\">4K Minecraft</a>\n<a href=\"https://github.com/NYPL/historical-minecraft/\">Historical Minecraft</a><br />\n<a href=\"https://www.microsoft.com/en-us/research/project/project-malmo/\">Project Malmo</a><br />\n<a href=\"https://www.bostondynamics.com\">Boston Dynamics</a><br />\n<a href=\"http://mentalfloss.com/article/92757/3d-printed-house-built-24-hours-russia\">House Built in 24 Hours</a></p>\n","tags":["microsoft","artificial intelligence","thoughts","robots"]},{"title":"The Case for .NET in the Classroom","contentType":"posts","date":"2017-12-14 16:57:39","url":"/posts/dotnet-in-the-classroom/","content":"\n<p>I recently finished by M.S. Computer Science degree and throughout the course, I used a variety of programming languages to complete assignments such as Matlab (Octave), Python, R, Java, Scala, Scheme (Lisp), and C. While all powerful in their own way, in my personal time (if and when I had any), I tried to learn about .NET tools and the ecosystem as a whole mainly because I have been a long time user of Microsoft products that include but are not limited to Zune, Kin, Surface and Windows Phone. Coincidentally, around the time I started my program, Microsoft was in the process of making an aggressive push to be more open and return to its roots of making tools and services that developers love. That being said, this is not exactly something that many people are aware of and the company's image in the eyes of consumers and developers using open source technologies has only recently started to improve. With recent acquisitions of companies like Skype and Xamarin as well as products like HoloLens, the opportunities for developers to build transformative solutions for consumers and businesses across a wide range of devices is limitless. Equally exciting and advantageous is the fact that the underlying ecosystem many of these technologies rely on is .NET. Keeping all that in mind, I strongly believe that there is no better time to introduce .NET into the classroom, especially at the university level in order to equip the next generation of developers with the necessary tools to achieve more in a wide range of environments.</p>\n\n<h3>Enterprise and Licensing</h3>\n<p>For the longest time, Microsoft has been synonymous with enterprise and rightfully so as most businesses run Windows and Office. What this also meant was licensing. Many of the products and services that Microsoft provided for businesses and developers were licensed products. Even though there were in some instances free versions of the products, they had their limitations. As a result, this naturally created a barrier to entry to try and experiment with their products.</p>\n<h3>Overhead</h3>\n<p>Even when trying to do the simplest thing with .NET Framework (particularly C#, ASP.NET or F#), you needed to run Visual Studio. Like the enterprise and licensing, this provided a barrier to entry for someone trying to learn the language. Conversely, in the most closely related language Java, you could've gotten started with a text editor and command line (among other things, this might be one reason Java has been widely used in the classroom).</p>\n<h3>Platform Specific</h3>\n<p>Although most of the world ran Windows, developer tools and products like the .NET Framework targeted Windows only. While this was acceptable a few years ago, the world is much different today than it was then. The current state of computing, while not entirely a statement on Microsoft and their products is one that Microft has astutely recognized. People want their services and experiences to travel with them regardless of the platform or products they use.</p>\n<p>Along these lines is the type of devices individuals performed their computing on. A few years ago, the Windows PC was the main device for productivity. Therefore, there was little to no need to focus on mobile, let alone cross-platform capabilities. Now, individuals can perform elaborate and deeply complex workflows from the palm of their hand. Therefore, the focus has shifted from a single device category to multi-device and multi-platform one.</p>\n\n<p>Microsoft has significantly transformed their business practices and refocused on what at one time made them one of the top companies in the world by making experiences and products consumers and more importantly developers love.</p>\n<h3>Mobile-First, Cloud-First</h3>\n<p>This was the strategy Satya Nadella tried to get the entire team to rally around when he became CEO of the company. At the time, Microsoft had recently purchased Nokia, yet its mobile phone efforts were non-existent aside from the three percent. Therefore, this strategy seemed ambiguous and unattainable at least from a consumer perspective.</p>\n<p>Upon further inspection, this strategy is not limited to mobile phones. Instead it refers to the ability of individuals' computing experiences, with the aid of the cloud, to go wherever they can be most productive. The hardware and to some extent, the software in terms of the platform no longer bind individuals as the services are available nearly everywhere.</p>\n<p>A shining example of this is Xamarin. Xamarin is a mobile app development platform built on the .NET ecosystem. With this purchase, Microsoft allowed developers who were native .NET developers to work in the same environment they were used to, while targeting a new family of devices running Android or iOS. Aside from learning the some of the inner workings the Xamarin framework, at the core, individuals can leverage the .NET tools they're used to using.</p>\n<h3>Emerging Technologies</h3>\n<h4>Artificial Intelligence</h4>\n<p>Artificial intelligence, data science and machine learning have been at the forefront of discussions involving technology both in the enterprise as well as academia. Microsoft has made strides on many fronts to position themselves and provide the means for individuals to use the Microsoft set of tools to create solutions.\nOne of the ways Microsoft has worked on providing developers and individuals with tools for deep learning is the Microsoft Cognitive Toolkit (CNTK). This framework is very similar to Tensorflow. Although models can be built using BrainScript or Python, the models themselves can be easily plugged into .NET solutions seamlessly.\nAnother way in which Microsoft is providing tools for artificial intelligence is through its Bot Framework. The Bot Framework has different parts. One of them is the creation of chat bots using either NodeJS or C#. These bots can target different chat applications that include but are not limited to Skype, Facebook Messenger, Slack and many others. These bots can be made even more intelligent with Microsoft's Language Understanding Service (LUIS) which takes care of the natural language processing the bot might need to process requests adequately.</p>\n<p>The second part of the Bot Framework is Cortana Skills. Using LUIS and the Bot Framework, individuals can create speech and text-enabled solutions for individuals that interact with Cortana enabled devices which include but are not limited to smart speakers, PCs and mobile devices. This new kind of experiences again is entirely backed by the .NET ecosystem many developers already use today.</p>\n<h4>Virtual and Mixed Reality</h4>\n<p>While still in its infancy, virtual and mixed reality have received tons of support from technology companies like Facebook with Oculus, Google with Google VR, Apple with AR Kit. Likewise, Microsoft's solution is HoloLens. Applications for HoloLens can be developed with .NET tools and Unity. Yet again, developers can leverage their .NET skills to build applications for yet another family of devices to build new immersive experiences that not too long ago were closer to science-fiction than reality.</p>\n<h3>.NET Core and .NET Standard</h3>\n<p>A lot of computing functions overall are fairly standard. For example, reading from the console is a relatively universal procedure regardless of the platform. Microsoft has found a way to take advantage of the power of .NET for these scenarios and released .NET Standard and .NET Core which are cross-platform environments to develop C#, ASP.NET and F# solutions. These applications can run on the web as well as any OS platform further expanding the capabilities of the ecosystem as well as extending the number of solutions developers can create.</p>\n<p>Furthermore, Visual Studio, while helpful is not a requirement to start creating applications. Most things if not all can be done via the command line and a text editor. Therefore, the barrier to entry both in terms of cost and start time has been greatly reduced and individuals get started building applications in little to no time for free.</p>\n\n<p>While this is not the entire gamut of tools and services Microsoft provides, it is a good demonstration of how learning a single set of skills like C# and F# in conjunction with the standard curriculum can greatly empower the next generation of computer scientists and developers by expanding the number of devices and platforms they can target across a wide variety of industries and disciplines making the number of applications nearly limitless.</p>\n","tags":[".net","microsoft","thoughts","programming","development"]},{"title":"Working with URLs in Racket","contentType":"posts","date":"2017-12-13 16:01:57","url":"/posts/working-with-urls-racket/","content":"<p>Often times when working with web applications, parameters and values are passed in the path as well as the query string of a URL. In order to do something useful, parsing of the url is required. Noted below are examples of how this can be done in an elegant manner using Racket.</p>\n<p>The first step is to import the appropriate packages</p>\n<pre><code class=\"language-lisp\">(require net/url)\n(require net/url-structs)\n</code></pre>\n<p>Assuming we are working with the url string &amp;quot;https://hacker-news.firebaseio.com/v0/item/192327.json?print=pretty&amp;quot;, we can convert the string to a URL struct using the <code>string-&amp;gt;url</code> procedure.</p>\n<pre><code class=\"language-lisp\">(define myurl (string-&amp;gt;url &amp;quot;https://hacker-news.firebaseio.com/v0/item/192327.json?print=pretty&amp;quot;))\n</code></pre>\n<p>The result is the following output</p>\n<pre><code class=\"language-lisp\">myurl\n;;(url\n;; &amp;quot;https&amp;quot;\n;; #f\n;; &amp;quot;hacker-news.firebaseio.com&amp;quot;\n;; #f\n;; #t\n;; (list\n;;  (path/param &amp;quot;v0&amp;quot; '())\n;;  (path/param &amp;quot;item&amp;quot; '())\n;;  (path/param &amp;quot;192327.json&amp;quot; '()))\n;; '((print . &amp;quot;pretty&amp;quot;))\n;; #f)\n</code></pre>\n<p>The structure of the URL struct is as follows and directly maps to the values displayed above.</p>\n<pre><code class=\"language-lisp\">(struct url ( scheme\n \t      user\n \t      host\n \t      port\n \t      path-absolute?\n\t      path\n \t      query\n \t      fragment)\n    #:extra-constructor-name make-url)\n  scheme : (or/c false/c string?)\n  user : (or/c false/c string?)\n  host : (or/c false/c string?)\n  port : (or/c false/c exact-nonnegative-integer?)\n  path-absolute? : boolean?\n  path : (listof path/param?)\n  query : (listof (cons/c symbol? (or/c false/c string?)))\n  fragment : (or/c false/c string?)\n</code></pre>\n<p>To access the individual data members of the struct, the syntax <code>&amp;lt;type&amp;gt;-&amp;lt;member&amp;gt; &amp;lt;instance&amp;gt;</code>. What is returned can be further manipulated based on its type (string, list, boolean)</p>\n<pre><code class=\"language-lisp\">(url-host myurl)\n;;&amp;quot;hacker-news.firebaseio.com&amp;quot;\n</code></pre>\n<h6>Source</h6>\n<p><a href=\"https://docs.racket-lang.org/net/url.html\">URLS and HTTP</a></p>\n","tags":["lisp","racket"]},{"title":"Reading Local JSON Files in R","contentType":"posts","date":"2017-12-11 14:21:21","url":"/posts/reading-json-in-r/","content":"<p>The following example is a way of reading local JSON files into R scripts. This can be useful for config files, although it's still also acceptable for data files as well.</p>\n\n<pre><code class=\"language-r\">require(rlist)\nrequire(rjson)\n</code></pre>\n\n<pre><code class=\"language-r\">configPath &amp;lt;- '.'\nfileName &amp;lt;- 'config.json'\njsonFile &amp;lt;- fromJSON(file=paste(configPath,fileName,sep=&amp;quot;/&amp;quot;),method=&amp;quot;C&amp;quot;)\n</code></pre>\n<p>The data gets loaded in as a list. It may be tedious to access nested contents via indices. To aid with this, a helper function can be created to extract the individual nested data elements of the file.</p>\n<pre><code class=\"language-r\">#Extract values form nested list\n#top_level = list.select(obj$topkey,obj$subkey)\nextractData &amp;lt;- function(top_level) {\n    output &amp;lt;- unlist(lapply(top_level,'[[',1));\n    return(output);\n}\n</code></pre>\n\n<p>Below is a default sample json file that includes both nested and non-nested data.</p>\n<pre><code class=\"language-json\">{\n    &amp;quot;port&amp;quot;: 2100,\n    &amp;quot;connectionStrings&amp;quot;: {\n        &amp;quot;default&amp;quot;: &amp;quot;123.567.890&amp;quot;\n    }\n}\n</code></pre>\n<p>The data in that file can be accessed as shown below.</p>\n<pre><code class=\"language-r\">#Not Nested\nport &amp;lt;- config$port # Should store 2100\n\n#Nested\nconnectionString &amp;lt;- extractData(list.select(config$connectionStrings,default)) #Should store 123.567.890\n</code></pre>\n","tags":["R","programming"]},{"title":"Windows 10 Intel Display Driver Blank Screen Fix","contentType":"posts","date":"2017-12-09 14:27:31","url":"/posts/windows-10-intel-display-driver-blank-screen/","content":"<p>Recently my Dell Inspiron P57G was experiencing issues when entering sleep mode. When trying to wake the PC, a blank screen was coming on. After some research, I concluded it was due to bugs with the most recent version of the display driver. These were the steps I took to remediate the issue.</p>\n<ol>\n<li>Go to <a href=\"http://www.dell.com/support/home/us/en/04?app=drivers&amp;amp;c=us&amp;amp;l=en&amp;amp;%7Eck=mn\">Dell Support</a> page and find the Intel HD Graphics 510 515 520 530 540 Driver.</li>\n<li>Click 'Other Versions'</li>\n<li>Download the most recent driver prior to 08/13/2017.</li>\n<li>Follow the installation instructions and restart the PC.\n<ul>\n<li>If prompted to overwrite newer drivers, select 'Yes'</li>\n</ul>\n</li>\n</ol>\n<p>After restart, either force the PC to sleep or let it sleep on its own. The problem should've been fixed.</p>\n","tags":["sysadmin","drivers","pc issues"]},{"title":"Creating A Login Screen With React Native","contentType":"posts","date":"2017-12-09 13:15:57","url":"/posts/create-login-screen-react-native/","content":"<p>This assumes that you have Node and NPM installed on your PC. If not, you can go to the official website and install them. Then, come back to this page.</p>\n\n<pre><code class=\"language-bash\">npm install -g create-react-native-app\n</code></pre>\n\n<pre><code class=\"language-bash\">create-react-native-app myfirstreactnativeapp\ncd myfirstreactnativeapp\nnpm start\n</code></pre>\n\n<p>In the <code>App.js</code> file that is created, we’re going to want to edit a few details.</p>\n<p>At the top of the file, add the following line of code to import the visual components we will utilize to create the page.</p>\n<pre><code class=\"language-javascript\">Import { TextInput, StyleSheet, Button } from 'react-native';\n</code></pre>\n<p>Inside of your <code>App</code> class, you want to add a few things. The first would be to add a constructor for the component. This will allow us to define and keep track of the text input being provided by the user.</p>\n<pre><code class=\"language-javascript\">constructor(props) {\n  super(props);\n  this.state = {\n    name: &amp;quot;&amp;quot;,\n    email: &amp;quot;&amp;quot;,\n    password: &amp;quot;&amp;quot;\n  }\n}\n</code></pre>\n<p>Then, we want to update our render method to show the text inputs and submit button.</p>\n<pre><code class=\"language-javascript\">render(){\n  return (\n    &amp;lt;View&amp;gt;\n      &amp;lt;TextInput\n        style={styles.input}\n        onChangeText={(text) =&amp;gt; this.setState({name: text})}\n        placeholder=&amp;quot;Name&amp;quot;\n      /&amp;gt;\n      &amp;lt;TextInput\n        style={styles.input}\n        onChangeText={(text) =&amp;gt; this.setState({email: text})}\n        placeholder=&amp;quot;E-Mail&amp;quot;\n      /&amp;gt;\n      &amp;lt;TextInput\n        style={styles.input}\n        onChangeText={(text) =&amp;gt; this.setState({password:text})}\n        secureTextEntry={true}\n        placeholder=&amp;quot;Password&amp;quot;\n      /&amp;gt;\n      &amp;lt;Button\n        onPress={this.submit}\n        title=&amp;quot;Submit&amp;quot;\n        color=&amp;quot;#841584&amp;quot;\n      /&amp;gt;\n    &amp;lt;/View&amp;gt;\n  )\n}\n</code></pre>\n<p>Using the <code>StyleSheet</code> component, we’ll also want to add some styling to our inputs. We can do so by adding the following lines of code outside of our App class definition. These styles are applied as properties of the <code>TextInput</code> components inside the App class.</p>\n<pre><code class=\"language-javascript\">const styles = StyleSheet.create({\n  input: {\n    width: 250,\n    margin: 5\n  }\n});\n</code></pre>\n<p>Finally, we need to create a submit method to handle the press event on the submit button. I have left it blank, since how you pass the e-mail and password is entirely up to you (although hopefully it is in the body of an HTTP Request as opposed to url parameters).</p>\n<pre><code class=\"language-javascript\">submit() {\n//Do Something\n}\n</code></pre>\n","tags":["react","react native","javascript"]},{"title":"Back To School: Emacs Edition","contentType":"posts","date":"2017-12-09 13:01:19","url":"/posts/back-to-school-emacs-edition/","content":"<p>As the summer comes to a close and I prepare to enter the fall semester, I am getting back into school mode. When programming in the terminal, I usually used vim. When compared to Emacs, the learning curve was relatively flat and it handled file editing just fine. I’ve read many articles that rave about Emacs and how powerful it can be. Therefore, a couple of months ago I took the plunge and decided to give it a try. Using dired mode to browse through the various directories in my system or project was a neat feature but for my purposes of basic file editing it did not offer anything that would warrant a switch. Furthermore, the default key bindings seemed a bit too much for me which caused me to  revert back to vim.</p>\n<p>As part of the articles that I have read in praise of Emacs, one particular feature seemed to be a recurring theme – Org-Mode. Although I had heard about it, I never really understood what it was. Out of curiosity, I decided to try it out for myself and see what it had to offer. While taking notes in it seemed normal enough, it was the export functionality that truly rocked my world and made me realize how amazing this could be for note-taking. Traditionally a pen-and-paper note taker, one of the main reasons I dislike taking notes on my PC is the lack of continuity and organization. Although tools like OneNote of Evernote are extremely powerful, they were not exactly the best when it came to a semester’s worth of notes and although they could be made to have all of the notes for a semester on one page, this defeated the purpose of having different pages and notebooks. Enter Org-Mode. Again, while the process of taking notes is nothing special, the export capabilities are amazing. Furthermore, by default a table of contents is created with links so you can always find what you want when you want. Here are a few examples of what I mean.</p>\n<p><img src=\"/files/images/backtoschoolemacsedition1.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>As it can be seen, other than * or + characters in front of titles and lists, there is nothing special in the terminal. This is how it looks like in HTML format</p>\n<p><img src=\"/files/images/backtoschoolemacsedition2.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Amazing! Not only do you get an organized set of notes, but you can see how this can easily contain a semester’s worth of notes in a single HTML file with main ideas being easily accessible via links. Now, maybe you don’t have an internet connection or prefer to put these notes on your tablet or phone. No problem. Just export it to a PDF.</p>\n<p><img src=\"/files/images/backtoschoolemacsedition3.png\" class=\"img-fluid\" alt=\"\" /></p>\n<p>Magic!</p>\n<p>This is definitely going to take my note-taking to the next level by allowing me to have these documents synced across all my devices. Furthermore, it will keep notes organized and in one place. If you’re interested in getting started there may be just a few things that you need to do.</p>\n\n<p>Org Mode is built in to the latest version of Emacs. However, in the event that it is not installed you can simply try the following commands in the Emacs mini buffer.</p>\n<p><code>M-x package-install RET org RET</code></p>\n\n<p>To access the export menu, simply type the following in the mini buffer:</p>\n<p><code>C-c C-e</code></p>\n<p>HTML export is included by default. To do that, we can add on to the command used to access the export menu as follows:</p>\n<p><code>C-c C-e h h</code></p>\n<p>PDF export is a bit mode complicated. The file is first exported to LaTeX format and then converted to PDF. However, in order to do this you need to install a package on your system. Assuming you are on a Unix machine, the following command should download everything necessary to convert LaTeX files to PDF. Keep in mind, this is relatively large and should take up roughly 1-2 GB in your system. In return, it limits the number of headaches due to installing necessary packages piecemeal that have multiple dependencies. To download the package, type in the following into your terminal:</p>\n\n<p>In the terminal, enter:</p>\n<pre><code class=\"language-bash\">sudo apt-get install texlive-full\n</code></pre>\n<p>Once installed, you can use a similar command to that of HTML export.</p>\n\n<p><code>C-c C-e l p</code></p>\n<p>This should be all you need to get started. For more information on how to use Org Mode, here’s a guide with live examples. Happy note-taking!</p>\n<p><a href=\"http://home.fnal.gov/%7Eneilsen/notebook/orgExamples/org-examples.html\">Emacs Examples and Cookbook</a></p>\n","tags":["emacs","org-mode"]},{"title":"HP ProBook 430 G1 Ubuntu BIOS Upgrade","contentType":"posts","date":"2017-12-09 12:57:12","url":"/posts/hp-probook-430g1-ubuntu-bios-upgrade/","content":"<p>I recently got an HP ProBook 430 G1 which came preinstalled with Windows 10. I have several Windows 10 devices, but needed a Linux system while on the go. I had given my HP Chromebook a try using Crouton. This worked okay. However, I was limited by the specs of the Chromebook and wanted something with more power for development purposes. Therefore, I erased Windows 10 on the ProBook and installed Ubuntu. The PC worked well. My only problem with it was that I found my battery meter was inaccurate and would reach a percentage in the 60’s before suddenly dropping to zero and shutting down. This was not something I could have, especially during the middle of a work session where I might lose some/all of my work.</p>\n<p>Immediately, I assumed it had something to do with drivers/BIOS being out of date. To my surprise, BIOS updates were only available for Windows. Fortunately, through some research I was able to find a solution that allowed me to update my BIOS and fix my battery problems.</p>\n\n<ol>\n<li>Make sure PC is connected to the charger</li>\n<li>4GB USB Drive</li>\n</ol>\n\n<ol>\n<li>Format USB drive. (FAT32 required)</li>\n<li>Create folder inside USB drive called Hewlett-Packard\\BIOS\\New</li>\n<li>Download latest BIOS .exe file from HP Support website</li>\n<li>Unzip .exe file. While 7Zip is not required, I found this to work well.</li>\n<li>Copy .bin file to Hewlett-Packard\\BIOS\\New folder</li>\n<li>Copy entire BIOSUpdate folder to Hewlett-Packard folder.</li>\n<li>Restart PC</li>\n<li>Continuously press F10 to enter BIOS Setup</li>\n<li>Select Update System BIOS menu option</li>\n<li>Select Update BIOS Using Local Media option</li>\n<li>Select Accept</li>\n</ol>\n<p>At this point, the installation process will take effect and the PC will restart itself at which point the your BIOS should be up to date.</p>\n","tags":["sysadmin","ubuntu","bios"]},{"title":"Setting Up A New Ubuntu PC","contentType":"posts","date":"2017-12-09 12:49:19","url":"/posts/setting-up-new-ubuntu-pc/","content":"\n<p>This assumes that you are currently the root user.</p>\n<p>Enter the adduser to command to create a new user.</p>\n<pre><code class=\"language-bash\">adduser &amp;lt;username&amp;gt;\n</code></pre>\n<p>Immediately, you will be prompted for a password. Enter the password and continue the process.</p>\n\n<p>This is only applicable when granting root priviliges to the newly created user.</p>\n<p>Enter the usermod command to add the new user to the sudo (root) group</p>\n<pre><code class=\"language-bash\">usermod -aG sudo &amp;lt;username&amp;gt;\n</code></pre>\n<p>The user should now have root priviliges.</p>\n<p>To test that it worked log in as that user and try to run a command with sudo</p>\n<pre><code class=\"language-bash\">su - &amp;lt;username&amp;gt;\nsudo ls -la /root\n</code></pre>\n\n<p>By default, Ubuntu usually comes preloaded or has the default 24.x version of Emacs. I often like to use Emacs along with org-mode and to take full advantage, especially as it pertains to working with source code I have found version 25.x is much better suited for the task. While there are many ways to go about it, installing it via package manager appears to be the easiest.</p>\n\n<pre><code class=\"language-bash\">sudo add-apt-repository ppa:kelleyk/emacs\n</code></pre>\n\n<pre><code class=\"language-bash\">sudo apt update\n</code></pre>\n\n<h3>Text-Only Interface</h3>\n<pre><code class=\"language-bash\">sudo apt-get install emacs25-nox\n</code></pre>\n<h3>GUI Interface</h3>\n<pre><code class=\"language-bash\">sudo apt-get install emacs25\n</code></pre>\n\n<p>Sometimes Emacs keeps a backup file by default. This can cause a lot of clutter in the current directory where work is being performed. There is a way to disable this.</p>\n<p>Navigate to <code>~/.emacs.d/</code> and edit the <code>init.el</code> file and add the following line.</p>\n<pre><code class=\"language-lisp\">(setq make-backup-files nil)\n</code></pre>\n\n<p>Make sure that your packages and repositories are up to date</p>\n<pre><code class=\"language-bash\">sudo apt-get update\n</code></pre>\n\n<pre><code class=\"language-bash\">sudo apt-get install default-jdk\n</code></pre>\n\n<h3>Add PPA Repository</h3>\n<pre><code class=\"language-bash\">sudo add-apt-repository ppa:webupd8team/java\nsudo apt-get update\n</code></pre>\n<h3>Install the installer</h3>\n<pre><code class=\"language-bash\">sudo apt-get install oracle-java8-installer\n</code></pre>\n<h4>Sources</h4>\n<p><a href=\"https://www.digitalocean.com/community/tutorials/how-to-create-a-sudo-user-on-ubuntu-quickstart\">Users</a><br />\n<a href=\"http://ubuntuhandbook.org/index.php/2017/04/install-emacs-25-ppa-ubuntu-16-04-14-04/\">Emacs</a><br />\n<a href=\"https://www.digitalocean.com/community/tutorials/how-to-install-java-with-apt-get-on-ubuntu-16-04\">Java</a></p>\n","tags":["linux","ubuntu","java","sysadmin","emacs"]},{"title":"Octave Development in the Terminal","contentType":"posts","date":"2017-12-09 12:46:54","url":"/posts/octave-development-in-the-terminal/","content":"<p>MatLab is a powerful tool that is used througout various disciplines for data processing and analysis. However, in order to get access to it and all its features, there is a fee associated with it. Depending on the needs of the user, this price can at times be somewhat large. Luckily, an open source alternative known as Octave exists which works just as good in many cases at zero monetary cost. Octave's syntax is for the most part identical to MatLab making the transition a relatively simple one.</p>\n\n<ul>\n<li>Octave</li>\n<li>Tmux</li>\n<li>Text Editor (Emacs)</li>\n</ul>\n\n\n<pre><code class=\"language-bash\">sudo apt-get install tmux\n</code></pre>\n\n<pre><code>sudo apt-add-repository ppa:octave/stable\nsudo apt-get update\nsudo apt-get install octave\n</code></pre>\n\n\n<pre><code class=\"language-bash\">tmux new -s octave-ide\n</code></pre>\n\n<pre><code class=\"language-ascii\">=========================\n|           |           |\n| Console   |           |\n|           |           |\n|-----------|  Script   |\n|           |           |\n| Terminal  |           |\n|           |           |\n=========================\n</code></pre>\n<p>In order to get the layout shown above, the following steps need to be taken</p>\n<h3>Split panes horizontally</h3>\n<p><code>Ctrl+b %</code></p>\n<h3>Split panes vertically</h3>\n<h4>Switch to pane 0</h4>\n<p><code>Ctrl+b q 0</code></p>\n<h4>Do vertical split</h4>\n<p><code>Ctrl+b &amp;quot;</code></p>\n\n<p>Now that the layout is setup, work can finally be done. To start off, initialize Octave in pane 0.</p>\n\n<p>Enter the following snippet in the console window</p>\n<pre><code class=\"language-octave\">function res = square(x)\n    res = x * x\nendfunction\n</code></pre>\n<p>Then call it to make sure it does what is expected</p>\n<pre><code class=\"language-octave\">square(2) #Should equal 4\n</code></pre>\n\n<p>Once satisfied that the code works:</p>\n<ol>\n<li>Switch to pane 2 using <code>Ctrl+b q 2</code></li>\n<li>Create a script file <code>myscript.m</code></li>\n<li>Edit <code>myscript.m</code> with desired code</li>\n</ol>\n<pre><code class=\"language-octave\">1; # Prevents octave from processing file as function file.\n\nfunction res = square(x)\n    res = x * x\nendfunction\n\nsquare(2);\n</code></pre>\n<ol start=\"4\">\n<li>Save the script</li>\n</ol>\n\n<p>Switch to pane 1 <code>Ctrl+b q 1</code> and try to run the newly created script</p>\n<pre><code class=\"language-bash\">octave -W myscript.m\n</code></pre>\n<p>The output should look the same as that in the console window.</p>\n<h6>Sources</h6>\n<p><a href=\"http://wiki.octave.org/Octave_for_Debian_systems\">Octave</a></p>\n","tags":["octave","tmux","developer tools"]},{"title":"Hello World","contentType":"posts","date":"2017-12-09 12:00:00","url":"/posts/hello-world/","content":"<p>Welcome to my personal blog. This will be a blog post for documenting my learning process as well as sharing projects and ideas I come across. I often find myself doing things, researching how to do them and completing a task. A few weeks/months later I might have to do something similar and have to go through the process again. Using this blog will help me keep me reference that knowledge.</p>\n","tags":[]}]</script><script type="application/json" id="remainingContentData-notes">[{"title":"Website Post Statistics - June 2025","contentType":"notes","date":"2025-07-01 20:00","url":"/notes/post-statistics-june-2025/","content":"<p>I haven't published much this past month. Notes and responses are significantly down from last year.</p>\n<p>That said, I'm happy that so far I've published seven long-form blog posts, which is how many I published all of last year.</p>\n<p>I haven't spent as much time bookmarking and resharing content. Partially due to the fact that I'm still working on the redesign. So far <a href=\"/posts/vibe-specing-prompt-to-spec/\">vibe-specing</a> the redesign hasn't yielded good enough results. Maybe I need to break down my problem further. That experience on its own might make a good blog post.</p>\n<p>Part of the challenge with the redesign is that I'm trying to find a balance between standardizing my front-matter YAML schemas so that I can simplify my code and just have a single function handle the parsing of the front-matter, while at the same time enabling custom rendering depending on the post type. I think in the end what I'll end up doing is having different schemas per post type and maybe refactoring some of my code to remove redundancies.</p>\n","tags":["githubactions","cicd","blogging","stats","github"]},{"title":"Crate Finds - May 2025","contentType":"notes","date":"2025-06-29 18:00","url":"/notes/crate-finds-may-2025/","content":"<p>Better late than never. Here's the May 2025 drop.</p>\n<p>Some of my favorites from last month:</p>\n<ul>\n<li>Prayer to the Stars by Vianney Lopez, Sam Bottner</li>\n<li>Diarabi by Viex Farka Toure, Khruangbin</li>\n<li>Everything &amp;amp; Nothing, OHMA</li>\n<li>Bloom, Deya Dova</li>\n<li>Dance of the Seven Sisters, Deya Dova</li>\n<li>Wisdom Eye by Alice Coltrane</li>\n<li>Black Meditation by Shabaka</li>\n</ul>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/0ETG3ZcVXCPqgHN4xFQ8qN\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"Week of June 29, 2025 - Post Summary","contentType":"notes","date":"2025-06-29 14:00 -05:00","url":"/notes/2025-06-29-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/crate-finds-may-2025\">Crate Finds - May 2025</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/do-not-privatize-federal-land\">Do not privatize federal land</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Thee Sacred Souls 2025","contentType":"notes","date":"2025-06-14 12:30","url":"/notes/thee-sacred-souls-2025/","content":"<p>Some pictures from the Thee Sacred Souls concert I attended a few weeks ago.</p>\n<p>I caught them in New York when the tour started, so it was nice to catch them at the end of the tour as well.</p>\n<p>In both cases, they were great performances.</p>\n<p><img src=\"/files/images/thee-sacred-souls-2025-1.jpg\" class=\"img-fluid\" alt=\"A stage with Thee Sacred Souls band performing\" /></p>\n<p><img src=\"/files/images/thee-sacred-souls-2025-2.jpg\" class=\"img-fluid\" alt=\"Thee Sacred Souls members on stage embracing\" /></p>\n","tags":["music","concert","theesacredsouls"]},{"title":"Local Artist Websites","contentType":"notes","date":"2025-06-14 10:57","url":"/notes/local-mural-artist-websites/","content":"<p>I saw these really cool murals around the neighborhood. The artists signed their work either with their name or Instagram handle. I don't use Instagram, so it was nice to see that some of them also had websites where I could follow their work.</p>\n<ul>\n<li><a href=\"https://www.shawnimals.com/\">Shawnimals</a></li>\n<li><a href=\"https://cydsmillie.com/\">Cyd Smillie</a></li>\n</ul>\n","tags":["art","website","openweb"]},{"title":"Week of May 26, 2025 - Post Summary","contentType":"notes","date":"2025-05-26 13:06 -05:00","url":"/notes/2025-05-26-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/attitude-copilot-agent\">Copilot Agent is giving me attitude</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/pocket-shutting-down\">Pocket shutting down</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Copilot Agent is giving me attitude","contentType":"notes","date":"2025-05-22 21:14","url":"/notes/attitude-copilot-agent/","content":"<blockquote class=\"blockquote\">\n<p>(see your rules and project plan)</p>\n</blockquote>\n<p>Is that Copilot's way of saying, &amp;quot;Per my previous e-mail...&amp;quot;?</p>\n<p><img src=\"/files/images/sassy-copilot-agent.png\" class=\"img-fluid\" alt=\"A conversation with Github Copilot Agent\" /></p>\n<p>And then it rage quit.</p>\n<p><img src=\"/files/images/copilot-agent-quit.png\" class=\"img-fluid\" alt=\"Error message saying GitHub Copilot Agent exhausted token limit\" /></p>\n","tags":["github","copilot","agent","funny"]},{"title":"See you at FediForum June 2025","contentType":"notes","date":"2025-05-16 19:44","url":"/notes/attending-fediforum-2025/","content":"<p>Super excited to be attending <a href=\"https://fediforum.org/\">FediForum</a> in a few weeks!</p>\n<p><img src=\"/files/images/fediforum-ticket.png\" class=\"img-fluid\" alt=\"Screenshot of Fediforum ticket showing name and event title\" /></p>\n","tags":["fediforum","openweb","conference","community"]},{"title":"Week of May 12, 2025 - Post Summary","contentType":"notes","date":"2025-05-11 19:11 -05:00","url":"/notes/2025-05-12-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/posts/vibe-specing-prompt-to-spec\">Vibe-Specing - From concepts to specification</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/notes/too-many-browser-tabs-shellsharks\">Extreme web surfing</a></li>\n<li><a href=\"/notes/chicano-batman-album\">Diving deeper into the Chicano Batman catalog</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/evolution-of-blogging-history-web\">The Evolution of Blogging</a></li>\n<li><a href=\"/responses/zero-search-alibaba/\">ZeroSearch - Incentivize the Search Capability of LLMs without Searching</a></li>\n<li><a href=\"/responses/intel-autoround\">Introducing AutoRound</a></li>\n<li><a href=\"/responses/discourse-fediverse\">Discourse and the Fediverse</a></li>\n<li><a href=\"/responses/socialweb-network\">socialweb.network</a></li>\n<li><a href=\"/responses/telograf\">TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching</a></li>\n<li><a href=\"/responses/bukmark-club\">Bukmark Club</a></li>\n<li><a href=\"/responses/nvidia-parakeet-v2\">Parakeet TDT 0.6B V2 (En)</a></li>\n<li><a href=\"/responses/corg-context-organizer-framework\">CORG: Generating Answers from Complex, Interrelated Contexts</a></li>\n<li><a href=\"/responses/meta-locate3d\">Introducing Locate 3D</a></li>\n<li><a href=\"/responses/in-praise-of-links-osteophage\">In Praise of Links</a></li>\n<li><a href=\"/responses/announcing-distributed-press-social-inbox\">Announcing Distributed.Press Social Inbox 1.0</a></li>\n<li><a href=\"/responses/resisting-urge-rewrite-website-santala\">Resisting the urge to rewrite the website</a></li>\n<li><a href=\"/responses/just-post-on-your-blog-shellsharks\">Just Put It On Your Blog</a></li>\n<li><a href=\"/responses/fediverse-house-2025-roundup\">Fediverse House 2025 Roundup</a></li>\n<li><a href=\"/responses/self-generate-incontext-examples-llm-agents-improve-sequential-decision-making\">Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks</a></li>\n<li><a href=\"/responses/claude-artifacts-reece\">Claude Artifacts</a></li>\n<li><a href=\"/responses/survey-ai-agent-protocols\">A Survey of AI Agent Protocols</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Extreme web surfing","contentType":"notes","date":"2025-05-10 18:31","url":"/notes/too-many-browser-tabs-shellsharks/","content":"<p>This is what my browser looks like every time <a href=\"https://shellsharks.com/\">shellsharks</a> drops one of their <a href=\"https://shellsharks.com/scrolls/\">scrolls</a>.</p>\n<p><img src=\"/files/images/too-many-browser-tabs.png\" class=\"img-fluid\" alt=\"A lot of open browser tabs\" /></p>\n","tags":["indieweb","web","community","surfing","openweb","blog"]},{"title":"Diving deeper into the Chicano Batman catalog","contentType":"notes","date":"2025-05-05 20:21","url":"/notes/chicano-batman-album/","content":"<p>I find it amazing how you could've heard a song before but on rare occasions, you actually get to really listen to the song. That's been happening to me with this album from Chicano Batman. I've written before about how much I enjoy A Hundred Dead and Loving Souls. Lately though, I've been really into Itotiani, La Samoana, and Soniatl. The instrumentals are amazing. Wild to think this album is now 15 years old.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/album/6tN9jametp8Y1xtraYzYNe?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["music","chicanobatman","indie"]},{"title":"Crate Finds - April 2025","contentType":"notes","date":"2025-05-02 13:35 -05:00","url":"/notes/crate-finds-april-2025/","content":"<p>April 2025 drop.</p>\n<p>SAULT, Cleo Sol (SAULT singer), and Little Simz were on repeat.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/4xaMGKfJVa5DMpp6S3i7N8\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"Ollama Adds Mistral 3.1 Support","contentType":"notes","date":"2025-04-08 08:19","url":"/notes/ollama-adds-support-mistral-small-3-1/","content":"<p>In the latest <a href=\"https://github.com/ollama/ollama/releases/tag/v0.6.5\">Ollama 0.6.5 release</a> support was added for <a href=\"https://ollama.com/library/mistral-small3.1\">Mistral Small 3.1</a>.</p>\n<p>In an earlier post, I <a href=\"/responses/mistral-small-3-1\">highlighted the announcement and summarized the key features</a>.</p>\n<p>I've been using it with <a href=\"https://github.com/marketplace/models/azureml-mistral/mistral-small-2503\">GitHub Models</a> and based on vibes I found it was more reliable at structured output compared to Gemma 3, especially when considering the model and context window size.</p>\n<p>Now that it's on Ollama thought, I can use it offline as well. However, I'm not sure how well that'll work on my ARM64 device, which only has 16GB of RAM.</p>\n","tags":["mistral","ollama","ai","llm"]},{"title":"Install Element Desktop on Windows ARM64 Devices","contentType":"notes","date":"2025-04-07 08:38","url":"/notes/install-element-desktop-windows-arm64/","content":"<p>In a previous post, I talked about my <a href=\"/notes/running-linux-gui-apps-windows-wsl-amazing\">workaround for running Element Desktop on my ARM64 Windows device</a>.</p>\n<p>I'm happy that I no longer need that workaround.</p>\n<p><a href=\"https://github.com/element-hq/element-desktop\">Element Desktop</a> enabled support for <a href=\"https://github.com/element-hq/element-desktop/pull/624\">ARM64 on Windows</a> and shipped it as part of the <a href=\"https://github.com/element-hq/element-desktop/releases/tag/v1.11.95\">1.11.95 release</a>.</p>\n<p>While you could go to the <a href=\"https://element.io/download\">downloads website</a> and get the installer from there, you can now also get it via <a href=\"https://learn.microsoft.com/windows/package-manager/winget/\">WinGet</a>.</p>\n<p>I built on the work of the <a href=\"https://github.com/microsoft/winget-pkgs/tree/master/manifests/e/Element/Element\">existing community manifest</a> in the winget-pkgs repo and via a <a href=\"https://github.com/microsoft/winget-pkgs/pull/244768\">pull request</a> updated the latest 1.11.96 version to include the ARM64 installer.</p>\n<p>While I didn't create the manifest from scratch, I thought it was really easy to contribute a new installer. I'll leave the details of my process for another post though.</p>\n<p>Installing Element Desktop via WinGet is relativaly straightforward.</p>\n<p>Assuming you already have the WinGet tool installed, all you have to do is run the following commmand in the terminal and you're all set!</p>\n<pre><code class=\"language-powershell\">winget install Element.Element -a arm64\n</code></pre>\n","tags":["matrix","arm64","windows","element","winget","opensource","decentralization"]},{"title":"Week of March 23, 2025 - Post Summary","contentType":"notes","date":"2025-03-23 12:25 -05:00","url":"/notes/2025-03-23-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/responses/bringing-static-sites-fediverse-enhancements-implementations-maho\">Bringing Static Sites to the Fediverse - Enhancements and Implementations</a></li>\n<li><a href=\"/responses/introducing-command-a-cohere\">Introducing Command A</a></li>\n<li><a href=\"/responses/introducing-gemma-3\">Introducing Gemma 3</a></li>\n<li><a href=\"/responses/mistral-small-3-1\">Mistral Small 3.1</a></li>\n<li><a href=\"/responses/preordered-pebbleos-watch\">Just pre-ordered by PebbleOS Core Time 2 smartwatch</a></li>\n<li><a href=\"/responses/willison-tools-colophon/\">Simon Willison's AI-Generated Tools Colophon</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Crate Finds - February 2025","contentType":"notes","date":"2025-03-02 13:35 -05:00","url":"/notes/crate-finds-february-2025/","content":"<p>February's drop.</p>\n<p>I've been listening to a lot of Andre 3000 and stumbled upon Carlos Nino and Friends.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/4zpzvhZWVOvmq0ZdbrKh8H\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"Week of March 2, 2025 - Post Summary","contentType":"notes","date":"2025-03-02 05:53 -05:00","url":"/notes/2025-03-02-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/posts/fractures-unfamiliarity-circumvention-pursuit-good-time-robert-hunter\">Fractures of Unfamiliarity &amp;amp; Circumvention in Pursuit of a Nice Time and the Meaning of Franklin's Tower</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/anthropic-claude-3-7-sonnet-code\">Claude 3.7 Sonnet and Claude Code</a></li>\n<li><a href=\"/responses/inevitable-jcole-blog\">The Algorithm - J. Cole has a blog?</a></li>\n<li><a href=\"/responses/mercury-first-commercial-scale-llm\">Introducing Mercury, the first commercial-scale diffusion large language model</a></li>\n<li><a href=\"/responses/multistream-azure-ffmpeg-obs-studio-maho\">How to Multistream Using Azure, Azure VM, MonaServer 2, and FFmpeg with OBS Studio</a></li>\n<li><a href=\"/responses/nomic-embed-text-v2\">Nomic Embed Text V2: An Open Source, Multilingual, Mixture-of-Experts Embedding Model</a></li>\n<li><a href=\"/responses/s1-simple-test-time-scaling\">s1: Simple test-time scaling</a></li>\n<li><a href=\"/responses/unplatform-indieweb\">Unplatform</a></li>\n<li><a href=\"/responses/your-site-is-a-home-hamid/\">Your Site Is a Home</a></li>\n</ul>\n","tags":["weeklysummary","blogging","indieweb","website"]},{"title":"Eggpocalypse 2025","contentType":"notes","date":"2025-02-17 15:57","url":"/notes/eggpocalypse-feb-2025/","content":"<p>It's rough out there.</p>\n<p><img src=\"/files/images/eggs-limit.jpg\" class=\"img-fluid\" alt=\"Image of sign saying Limit 3 Eggs\" /></p>\n<p><img src=\"/files/images/empty-eggs.jpg\" class=\"img-fluid\" alt=\"Image of empty shelves in warehouse\" /></p>\n","tags":["costco","eggs"]},{"title":"Week of February 9, 2025 - Post Summary","contentType":"notes","date":"2025-02-09 13:15 -05:00","url":"/notes/2025-02-09-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/cdn-issue-fixed-redirects-working\">Redirect URLs working again</a></li>\n<li><a href=\"/notes/post-statistics-jan-2025\">Website Post Statistics - January 2025</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/bringing-pebble-back\">Bringing Pebble Back</a></li>\n<li><a href=\"/responses/build-a-link-blog-xuanwo\">Build a link blog</a></li>\n<li><a href=\"/responses/guide-implementing-activitypub-static-site-pt-8\">A Guide to Implementing ActivityPub in a Static Site (or Any Website) - Part 8</a></li>\n<li><a href=\"/responses/illustrated-deepseek-r1\">The Illustrated DeepSeek-R1</a></li>\n<li><a href=\"/responses/master-art-of-product-manager\">Master the Art of the Product Manager 'No'</a></li>\n<li><a href=\"/responses/openai-deep-research\">Introducing deep research</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Redirect URLs working again","contentType":"notes","date":"2025-02-08 14:38 -05:00","url":"/notes/cdn-issue-fixed-redirects-working/","content":"<p>Thanks to the kind e-mail from <a href=\"https://www.benji.dog/\">benji.dog</a>, I realized people couldn't subscribe to my RSS feeds using the links provided in the <a href=\"/feed\">/feed</a> page.</p>\n<p>I'd noticed something weird had been going on for the past few days but since the rest of my site was working, I didn't pay attention to it.</p>\n<p>When I went in earlier today to fix the issue, I realized because of changes to <a href=\"https://learn.microsoft.com/azure/cdn/edgio-retirement-faq\">Azure CDN</a>, none of my redirects were working.</p>\n<p>My site kept working fine, but that's because the migration was autmatically done by Azure.</p>\n<p>What wasn't automatically migrated were my CDN rules which I'd set up to <a href=\"/notes/new-rss-feed-links\">simplify URLs</a> and <a href=\"/responses/own-your-rss-links\">own my links</a>.</p>\n","tags":["cdn","azure","rss","ownyourlinks","cdn","frontdoor","community"]},{"title":"Website Post Statistics - January 2025","contentType":"notes","date":"2025-02-02 18:06","url":"/notes/post-statistics-jan-2025/","content":"<p>Off to a good start.</p>\n<p>In the month of January, I wrote two long-form posts:</p>\n<ul>\n<li><a href=\"/posts/listen-audiobooks-librivox-antennapod/\">Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod</a></li>\n<li><a href=\"/posts/deploy-owncast-azure/\">Deploy your own Owncast server</a></li>\n</ul>\n<p>Last year, I published about seven of them.</p>\n<p>As I tinker some more <a href=\"/notes/first-owncast-stream/\">live on stream</a> and use blog posts as a more structured way to orgqanize my explorations, I expect this year to have a healthy number of longer-form blog posts.</p>\n<p>Notes and responses are not too far behind either. Both of them are at 14 in January. That's about 10% and 5% the number of posts in both of those categories last year. I haven't been reposting things I'm reading as often, but I'd like to start doing that some more.</p>\n<p>Similar to long-form posts, as I <a href=\"/notes/tinkering-deepseek-r1-dotnet-github-models\">reflect on each stream and publish the recordings</a>, I expect at least one note every one or two weeks just for the stream.</p>\n<p>No surprise, tags on my posts continue to be related to AI.</p>\n<p>This month I didn't really do the monthly <a href=\"/tags/cratefinds\">music playlist</a> kind of posts. If I were to quickly summarize the music on repeat this month, it'd be the <a href=\"/responses/andre-3000-listening-to-the-sun\">Andre 3000 album, New Blue Sun</a>.</p>\n","tags":["githubactions","cicd","blogging","stats","github"]},{"title":"Week of February 2, 2025 - Post Summary","contentType":"notes","date":"2025-02-02 13:05 -05:00","url":"/notes/2025-02-02-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/owncast-simulcast-youtube\">Simulcasting to Owncast and YouTube</a></li>\n<li><a href=\"/notes/tinkering-deepseek-r1-dotnet-github-models\">Tinkering with DeepSeek R1, GitHub Models, and .NET on stream</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/bucees-wisconsin\">Buc-ee's plans first Wisconsin travel center</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Simulcasting to Owncast and YouTube","contentType":"notes","date":"2025-01-30 20:25","url":"/notes/owncast-simulcast-youtube/","content":"<p>About a week ago, I was telling <a href=\"https://maho.dev/\">maho.dev</a> about Owncast and my <a href=\"/posts/deploy-owncast-azure\">self-hosted setup</a>.</p>\n<p>During our chat, the question of simulcasting came up.</p>\n<p>Using the <a href=\"https://obsproject.com/forum/resources/obs-studio-stream-to-multiple-platforms-or-channels-at-once.932/\">guidance from the OBS community</a> as a starting point, I decided to try it out for myself.</p>\n<p>It didn't work as-is but after using an AI assistant to help me tweak the ffmpeg command, I was able to get it working.</p>\n<p>I posted the command to my <a href=\"/resources/wiki/owncast\">Wiki</a> and plan on writing a more detailed post at a later time.</p>\n","tags":["owncast","obs","youtube","livestream","fediverse","opensource"]},{"title":"Tinkering with DeepSeek R1, GitHub Models, and .NET on stream","contentType":"notes","date":"2025-01-29 19:59","url":"/notes/tinkering-deepseek-r1-dotnet-github-models/","content":"<p>I saw that the <a href=\"https://azure.microsoft.com/en-us/blog/deepseek-r1-is-now-available-on-azure-ai-foundry-and-github/\">DeepSeek R1 model is now on GitHub Models and Azure AI Foundry</a>, so I decided to start a stream and play around with it in the GitHub Models playground as well as a .NET application.</p>\n<p>Here's the recording.</p>\n<p><a href=\"https://www.youtube.com/watch?v=8Z6iFALi8kM\" title=\"YouTube Video Thumbnail Tinkering on Stream with DeepSeek R1, GitHub Models, and .NET\"><img src=\"http://img.youtube.com/vi/8Z6iFALi8kM/0.jpg\" class=\"img-fluid\" alt=\"YouTube Video Thumbnail Tinkering on Stream with DeepSeek R1, GitHub Models, .NET\" /></a></p>\n<p>I rambled on stream since it's my stream of consciousness as I'm tinkering with these technologies.</p>\n<p>If you're mainly interested in the code, here's the GitHub repo: <a href=\"https://github.com/lqdev/DeepSeekDotnetSample\">lqdev/DeepSeekDotnetSample</a></p>\n","tags":["deepseek","ai","dotnet","github","azure","livestream"]},{"title":"Do not feed or aggravate the gators","contentType":"notes","date":"2025-01-25 15:13","url":"/notes/do-not-feed-aggravate-gators/","content":"<p>Mama says that alligators are ornery because they got all them teeth and no toothbrush.</p>\n<p><img src=\"/files/images/do-not-feed-aggravate-gators.png\" class=\"img-fluid\" alt=\"Sign saying do not feed or aggravate the gators\" /></p>\n","tags":["gators","bayou","swamp"]},{"title":"First live stream using Owncast done","contentType":"notes","date":"2025-01-20 18:33","url":"/notes/first-owncast-stream/","content":"<p>Earlier today I tested out my <a href=\"/posts/deploy-owncast-azure/\">self-hosted Owncast setup</a>.</p>\n<p><a href=\"https://www.youtube.com/watch?v=3opjC7fEAJs\" title=\"Video Thumbnail from live stream\"><img src=\"http://img.youtube.com/vi/3opjC7fEAJs/0.jpg\" class=\"img-fluid\" alt=\"Video Thumbnail from live stream\" /></a></p>\n<p>Overall, I think it went well considering I was using the built-in camera and mic on my laptop. Despite sharing my screen, using the browser, broadcasting and recording the stream, the laptop seemed to handle it just fine. I didn't get a chance to test writing and running code which is going to make up a bulk of the content on stream.</p>\n<p>The flow on OBS is straightforward and simple. I might automate it either using my Stream Deck or just mapping hotkeys. It's simple enough that I only need to transition between a few scenes.</p>\n<p>As far as hosting the recordings, the process of uploading to YouTube was straightforward. It took a little over 10 minutes, but that's something that I can just do in the background so not really worried about that. For the time being, I think I'll just use YouTube to host the videos and mark them as unlisted. No particular reason behind that other than I only want YouTube to serve as my video host. The main place I plan on showcasing and organizing videos is on my website.</p>\n<p>For the time being, I'll just create notes like these. Eventually though, I'd like to have a video post type which shows up on my feed. The post card will only display the video but the detailed view, I'd like for it to be like my <a href=\"/resources/presentations/hello-world\">presentation pages</a>. Except, instead of a presentation, it'd contain the video, show notes, maybe the transcript, and links mentioned during the stream. Also, I'd like video posts to have their own RSS feed just like other posts on this page so folks who are interested in following along can via their RSS reader.</p>\n<p>A few other things I want to figure out are:</p>\n<ul>\n<li>Setting up a custom domain that points to my Owncast server rather than the default one provided by Azure Container Apps.</li>\n<li>Displaying and managing chat outside of the built-in Owncast frontend.</li>\n</ul>\n","tags":["owncast","livestream","selfhost","opensource","fediverse","video","azure"]},{"title":"Week of January 19, 2025 - Post Summary","contentType":"notes","date":"2025-01-19 13:59 -05:00","url":"/notes/2025-01-19-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/2025-01-12-weekly-post-summary/\">Week of January 12, 2025 - Post Summary</a></li>\n<li><a href=\"/notes/created-owncast-livestream-page\">Embedded Owncast live stream into my website</a></li>\n<li><a href=\"/notes/how-do-you-watch-blueray-on-pc\">How do you watch Blu-Ray on PC?</a></li>\n<li><a href=\"/notes/spotify-address-missing-info\">Why does Spotify need my full address?</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/ai-subtitles-coming-vlc\">AI Subtitles Are Coming to VLC</a></li>\n<li><a href=\"/responses/calm-tech-interview\">The Future of Calm Technology with Amber Case</a></li>\n<li><a href=\"/responses/landlord-of-your-notes\">Stop renting space for your notes and start being your own landlord</a></li>\n<li><a href=\"/responses/making-space-handmade-web\">Making space for a handmade web</a></li>\n<li><a href=\"/responses/rip-david-lynch\">R.I.P David Lynch</a></li>\n<li><a href=\"/responses/swarm-cyborg-insects-agents\">Swarm navigation of cyborg-insects in unknown obstructed soft terrain</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Embedded Owncast live stream into my website","contentType":"notes","date":"2025-01-18 19:20","url":"/notes/created-owncast-livestream-page/","content":"<p>The live stream page on my site is up.</p>\n<p>If you go to <a href=\"/live\">/live</a>, you'll see the stream.</p>\n<p>Still not sure what stream schedule will look like, but I'm thinking maybe I'll do a test one on Monday.</p>\n","tags":["owncast","indieweb","livestream","fediverse","opensource","website"]},{"title":"Why does Spotify need my full address?","contentType":"notes","date":"2025-01-14 19:25","url":"/notes/spotify-address-missing-info/","content":"<p><img src=\"/files/images/spotify-missing-information.png\" class=\"img-fluid\" alt=\"Pop-up requesting address information in Spotify Android app\" /></p>\n<p>Why?</p>\n","tags":["spotify","privacy"]},{"title":"How do you watch Blu-Ray on PC?","contentType":"notes","date":"2025-01-14 18:55","url":"/notes/how-do-you-watch-blueray-on-pc/","content":"<p>I have a preference for physical media. There's many reasons, some related to the sentiment of <a href=\"/responses/landlord-of-your-notes\">owning vs. renting content</a>. Along the same lines, streaming services and service providers can change the terms of service and you have little to no control over it. <a href=\"https://en.wikipedia.org/wiki/Westworld_(TV_series)\">Westworld</a> is an example of that. One day, HBO decided to pull it from its streaming service and physical copies were the only way you could watch it.</p>\n<p>Anyway, on to my most recent experience trying to play a Blu-Ray disc on my PC.</p>\n<p>The other day I decided to sit down to watch an episode of Westworld. Usually, I get the DVD version. In this case though, I had the Blu-Ray.</p>\n<p>I have an external Blu-Ray player so I thought, &amp;quot;Great! Let me just plug that into my laptop and start the show&amp;quot;.</p>\n<p>Wrong!</p>\n<p>What I thought was a simple task, ended up taking me about 20 minutes to figure out and even then I got to a subpar solution.</p>\n<p>I prefer playing media either with MPV or VLC.</p>\n<p>At first, it wouldn't play so I decided to go online and figure out why. Turns out, that's not supported out of the box and to get it working you need to download an additional DLL and <a href=\"https://www.easefab.com/resource/play-blu-ray-with-vlc.html\">configure a few things in the VLC settings</a>.</p>\n<p>I could've gone through that process, but I just wanted to watch a show. There's got to be an easier way.</p>\n<p>Next, I turned to the built-in Media Player in Windows. No luck there either.</p>\n<p>I ended up going with the <a href=\"https://www.leawo.com/blu-ray-player/\">Leawo Blu-Ray player</a>. Although it's free, you have to deal with pop-ups prompting you to upgrade to the paid version. It works though.</p>\n<p>I would prefer to use VLC or the built-in media player but it's unfortunate using them isn't as straightforward. Now that I have some more time, I'll set up VLC so that next time, it can &amp;quot;just work&amp;quot;.</p>\n<p>I'll close it out with a question.</p>\n<p>How do you watch Blu-Rays on your PC?</p>\n<p><a href=\"/contact\">Send me a message</a> and let me know.</p>\n","tags":["bluray","media","physicalmedia","vlc"]},{"title":"Week of January 12, 2025 - Post Summary","contentType":"notes","date":"2025-01-12 11:57 -05:00","url":"/notes/2025-01-12-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/posts/deploy-owncast-azure\">Deploy your own Owncast server</a></li>\n<li><a href=\"/posts/listen-audiobooks-librivox-antennapod\">Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/notes/2025-01-05-weekly-post-summary/\">Week of January 5, 2025 - Post Summary</a></li>\n<li><a href=\"/notes/attended-first-indieweb-hwc\">Attended my first IndieWeb Homebrew Website Club</a></li>\n<li><a href=\"/notes/gh-actions-az-cli-docker-azcopy-workaround\">Workaround for Azure CLI docker image azcopy issue in GitHub Actions</a></li>\n<li><a href=\"/notes/owncast-azure-container-apps-deployment\">Got an Owncast instance deployed to Azure!</a></li>\n<li><a href=\"/notes/psu-nd-ncaaf-semifinal-2025\">So glad I didn't watch the Penn State game</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/agents-huyen\">Agents</a></li>\n<li><a href=\"/responses/kaggle-agents-whitepaper\">Agents Whitepaper</a></li>\n<li><a href=\"/responses/year-of-curiosity/\">A Year of Curiosity</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Workaround for Azure CLI docker image azcopy issue in GitHub Actions","contentType":"notes","date":"2025-01-10 21:12","url":"/notes/gh-actions-az-cli-docker-azcopy-workaround/","content":"<p>Publishing on my site was broken since yesterday. I thought it was only me but after doing some digging looks like it was an issue with Azure CLI and the link it uses to download azcopy.</p>\n<p>Looks like the team is already working on a fix and you can track progress using this issue</p>\n<p><a href=\"https://github.com/Azure/azure-cli/issues/30635\">https://github.com/Azure/azure-cli/issues/30635</a></p>\n<p>If you're using Azure CLI in GitHub Actions and running into this problem, you can unblock yourself by installing azcopy manually. Here's the snippet that worked for me.</p>\n<pre><code class=\"language-yaml\">- name: Upload to blob storage\n  uses: azure/CLI@v1\n  with:\n    azcliversion: 2.67.0\n    # azcopy workadound https://github.com/Azure/azure-cli/issues/30635\n    inlineScript: |\n        tdnf install -y azcopy;\n        # YOUR SCRIPT\n</code></pre>\n","tags":["azure","github","cicd"]},{"title":"So glad I didn't watch the Penn State game","contentType":"notes","date":"2025-01-10 20:33","url":"/notes/psu-nd-ncaaf-semifinal-2025/","content":"<p>I'm so glad I didn't bother tuning in. That ending was rough.</p>\n","tags":["football","pennstate","notredame","psu","ncaaf"]},{"title":"Attended my first IndieWeb Homebrew Website Club","contentType":"notes","date":"2025-01-09 20:12","url":"/notes/attended-first-indieweb-hwc/","content":"<p>Last night I attended my <a href=\"https://events.indieweb.org/2025/01/homebrew-website-club-pacific-V49lZfOizy1E\">first IndieWeb Homebrew Website Club meeting</a>. Although it was my first, everyone was very welcoming and it felt like I'd been a longtime participant. It was also great to put faces to the many websites I already follow as well as discover new people in the community. Looking forward to the next one.</p>\n","tags":["indieweb","community"]},{"title":"Got an Owncast instance deployed to Azure!","contentType":"notes","date":"2025-01-07 19:52","url":"/notes/owncast-azure-container-apps-deployment/","content":"<p>A few months ago, I started tinkering with Owncast. In that case, I was figuring out how I could use .NET Aspire and Blazor to <a href=\"/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire/\">configure an Owncast server and embed the stream on a website</a>.</p>\n<p>Although I didn't go all the way through creating the deployment, doing so with <a href=\"https://learn.microsoft.com/dotnet/aspire/deployment/azure/aca-deployment\">Aspire would've been fairly straightforward</a>.</p>\n<p>Since I already have a website, all I care about deploying is the Owncast server.</p>\n<p>After spending about an hour on it today, I was able deploy and Owncast server to Azure Container Apps using the <a href=\"https://learn.microsoft.com/en-us/azure/container-apps/vnet-custom?tabs=bash&amp;amp;pivots=azure-portal\">virtual network environment setup tutorial</a>.</p>\n<p>The hardest part was figuring out that I needed to set up a virtual network in order to publicly expose multiple ports.</p>\n<p>Once the server was up and running, configuring OBS was relatively easy as well.</p>\n<p><img src=\"/files/images/owncast-obs-azure-container-apps-deployment.png\" class=\"img-fluid\" alt=\"AI Generated image of two aliens enjoying a beer in the middle of the desert by a campfire on a starry night\" /></p>\n<p>I'll try to put together a more detailed writeup so I remember what I did for next time as well as help others who may be interested in self-hosting their own livestream server.</p>\n","tags":["owncast","livestream","azure","selfhost","fediverse","obs","containers","aca"]},{"title":"Week of January 5, 2025 - Post Summary","contentType":"notes","date":"2025-01-05 21:08 -05:00","url":"/notes/2025-01-05-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/posts/listen-audiobooks-librivox-antennapod\">Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/internet-archive-public-domain-2025\">Welcome to the Public Domain in 2025</a></li>\n<li><a href=\"/responses/langchain-state-of-ai-2024\">LangChain State of AI 2024 Report</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Week of December 15, 2024 - Post Summary","contentType":"notes","date":"2024-12-15 13:26 -05:00","url":"/notes/2024-12-15-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/posts/rss-opml-starter-packs\">Starter Packs with OPML and RSS</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/notes/antennapod-echo-2024\">Antennapod Echo 2024</a></li>\n<li><a href=\"/notes/surrender-quantum-conscience\">Surrender to the quantum conscience</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/sora-released\">Sora is here</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Surrender to the quantum conscience","contentType":"notes","date":"2024-12-12 20:08","url":"/notes/surrender-quantum-conscience/","content":"<p><img src=\"/files/images/surrender-quantum-conscience.jpg\" class=\"img-fluid\" alt=\"Writing on railing &amp;quot;surrender to the quantum conscience&amp;quot;\" /></p>\n<p>I'm trying.</p>\n","tags":["quantum","conscience","message"]},{"title":"Antennapod Echo 2024","contentType":"notes","date":"2024-12-11 20:52","url":"/notes/antennapod-echo-2024/","content":"<p>That's a lot of podcasts!</p>\n<p><img src=\"/files/images/antennapod-2024-1.png\" class=\"img-fluid\" alt=\"Screenshot of Antennapod Echo Podcast Hours Listened\" /></p>\n<p><img src=\"/files/images/antennapod-2024-2.png\" class=\"img-fluid\" alt=\"Screenshot of Antennapod Echo Podcast Addict\" /></p>\n","tags":["podcasts","opensource","antennapod","stats"]},{"title":"Week of December 09, 2024 - Post Summary","contentType":"notes","date":"2024-12-08 19:45 -05:00","url":"/notes/2024-12-09-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/posts/rss-opml-starter-packs\">Starter Packs with OPML and RSS</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/notes/spotify-wrapped-2024\">Spotify Wrapped 2024</a></li>\n<li><a href=\"/notes/spotify-wrapped-2024-podcast-notebook-lm\">Spotify Wrapped 2024 AI Generated Podcast</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/how-i-turned-website-social-media-hub-newman\">How I turned my blog into a social media hub</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Spotify Wrapped 2024 AI Generated Podcast","contentType":"notes","date":"2024-12-05 09:02","url":"/notes/spotify-wrapped-2024-podcast-notebook-lm/","content":"<p>While looking at <a href=\"/notes/spotify-wrapped-2024\">Spotify Wrapped this year</a>, I completely missed the AI-generated podcast that came along with it.</p>\n<p>As cool as this is, <strong>it should've been opt-in</strong>.</p>\n<p>Although I would hope that when I clicked &amp;quot;I Agree&amp;quot; on the user agreement, the use of my data in this way is covered (for Spotify's sake), it's not clear or easy to find:</p>\n<ol>\n<li>What data was shared with Google's NotebookLM</li>\n<li>What Google will do with that data</li>\n</ol>\n<p>There's a reason I purposefully haven't linked Spotify to other accounts.</p>\n<p>I know there's probably hundreds of worse ways my data is being used I don't know about. This one though was hard to miss and a sad reminder that often you don't learn about how your data is being used until after the fact. Cases like this are not really a big deal. So what if you know I was going through a &amp;quot;Witchy Jam Band Phsychedelic Rock&amp;quot; phase in September? Where it does matter though is when it involves more personal and sensitive data. Again, because we don't realize it until after the fact, it's not until then that we learn what we're giving up when we make the simple, often uninformed, decision to select &amp;quot;I Agree&amp;quot;.</p>\n<p><img src=\"https://wrappedaipodcast.spotifycdn.com/bDEn3i-uWoItOTYMmcRcNGKBqZ18M9ygFTAY0X8YHXk.mp4\" class=\"img-fluid\" alt=\"AI Genereated Spotify Wrapped Podcast\" /></p>\n<!-- <video controls=\"\" autoplay=\"\" name=\"media\"><source src=\"https://wrappedaipodcast.spotifycdn.com/bDEn3i-uWoItOTYMmcRcNGKBqZ18M9ygFTAY0X8YHXk.mp4\" type=\"video/mp4\"></video> -->\n","tags":["spotify","ai","spotifywrapped","google","notebooklm","privacy","podcast","generativeai","genai"]},{"title":"Spotify Wrapped 2024","contentType":"notes","date":"2024-12-04 08:39","url":"/notes/spotify-wrapped-2024/","content":"<p>It's finally here!</p>\n<p>I can't say I'm too surprised by the results.</p>\n<p><img src=\"/files/images/spotify-wrapped-2024-0.png\" class=\"img-fluid\" alt=\"Spotify Wrapped Screenshot Summary of top artists, songs, and listening minutes\" /></p>\n<p>Compared to last year, I nearly doubled my listening minutes. Also, none of the artists from the <a href=\"/notes/spotify-wrapped-2023\">previous year</a> made it onto this list. Although, I'm not sure how Buffalo Springfield worked its way into the list of top artists.</p>\n<p>I love how my listening habits from September are described, &amp;quot;Witchy Jam Band Psychedelic Rock&amp;quot;.</p>\n<p><img src=\"/files/images/spotify-wrapped-2024-1.png\" class=\"img-fluid\" alt=\"Spotify Wrapped Screenshot Listening Habits\" /></p>\n","tags":["music","spotify","spotifywrapped","stats"]},{"title":"Running Linux GUI Apps on Windows (WSLg) is amazing!","contentType":"notes","date":"2024-11-27 20:48","url":"/notes/running-linux-gui-apps-windows-wsl-amazing/","content":"<p>So you might've heard of Windows Subsystem for Linux (WSL), but did you know you can also <a href=\"https://learn.microsoft.com/windows/wsl/tutorials/gui-apps\">run Linux GUI apps with WSLg</a>?</p>\n<p>While trying to install the Element Desktop client on my Spandragon X Elite Windows device, I quickly realized <a href=\"https://github.com/element-hq/element-desktop/issues/650\">you can't because of an issue with Electron</a>.</p>\n<p>WSL is supported on ARM64 Windows devices. This means, it should also support GUI apps.</p>\n<p>I decided to try and install the Element Desktop client for Linux in WSL and this was the result!</p>\n<p><img src=\"/files/images/element-linux-wsl.png\" class=\"img-fluid\" alt=\"Screenshot of Element Desktop Login Screen\" /></p>\n","tags":["wsl","linux","windows","gui","element","matrix"]},{"title":"Week of November 25, 2024 - Post Summary","contentType":"notes","date":"2024-11-24 19:41 -05:00","url":"/notes/2024-11-25-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/mastodon-working-again-v-4-3-1\">Mastodon instance working again</a></li>\n<li><a href=\"/notes/2024-11-18-weekly-post-summary/\">Week of November 18, 2024 - Post Summary</a></li>\n<li><a href=\"/notes/using-domain-mastodon-discovery\">Using my domain for discovery in Mastodon</a></li>\n<li><a href=\"/notes/mastodon-instance-supports-opengraph-creator-tag\">Mastodon instance now supports OpenGraph creator tag</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/digital-minimalism-moral-duty-deep-questions\">Digital minimalism: A moral duty?</a></li>\n<li><a href=\"/responses/jimmy-gambler-psu\">Jimmy The Gambler</a></li>\n<li><a href=\"/responses/implement-activitypub-static-site-series-maho\">Implement ActivityPub on a static site series by maho.dev</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Using my domain for discovery in Mastodon","contentType":"notes","date":"2024-11-19 09:35","url":"/notes/using-domain-mastodon-discovery/","content":"<p>For a while, I've known that I can use my domain and the WebFinger protocol to aid with discovery in the Fediverse, specifically Mastodon.</p>\n<p>I tried implementing this feature on my website a while back but was unsuccessful.</p>\n<p>Last night, I got an itch to try it again and got it working.</p>\n<p>Basically, what I was doing wrong the first time was, I was trying to use <code>lqdev.me</code> as the domain. Currently, my domain is not set up to use apex / naked domains. Therefore, it makes sense why that didn't work.</p>\n<p>Once I used <code>www.lqdev.me</code>, everything seemed to be working fine. The implementation isn't perfect, because technically you can use any username. <a href=\"https://www.hanselman.com/blog/use-your-own-user-domain-for-mastodon-discoverability-with-the-webfinger-protocol-without-hosting-a-server\">Scott Hanselman does a nice job explaining that</a>. The important part though is that <a href=\"/notes/now-on-bluesky\">my persence on the Fediverse is now directly linked to my domain, just like it is with Bluesky</a>. This goes beyond the link verification Mastodon provides. As a result, it provides more flexibility and opportunities to shape my presence on the Fediverse.</p>\n<p>If you're interested in implementing it for yourself, there's a ton of guides, but here's the one I used: <a href=\"https://guide.toot.as/guide/use-your-own-domain/\">Use your own user @ domain for Mastodon discoverability</a>.</p>\n<p>I don't like the <code>www</code> subdomain. So far I've put up with it for a while but this might be the forcing function to finally getting the apex domain setup.</p>\n<p>In the meantime, you can search my account in Mastodon using <code>@lqdev@www.lqdev.me</code>.</p>\n","tags":["mastodon","fediverse","indieweb","openweb","web","smallweb","cdn","internet"]},{"title":"Mastodon instance now supports OpenGraph creator tag","contentType":"notes","date":"2024-11-19 09:25","url":"/notes/mastodon-instance-supports-opengraph-creator-tag/","content":"<p>In version 4.3, <a href=\"https://blog.joinmastodon.org/2024/10/mastodon-4.3/#:%7E:text=Helping%20writers%20and%20journalists\">Mastodon introduced OpenGraph creator tags</a> to link to an author's Fediverse account from within a post.</p>\n<p>I had <a href=\"/notes/added-fediverse-og-creator-tag\">implemented it on my website</a> a while back. However, because my instance wasn't using a version that supported it, it didn't show up on my instance.</p>\n<p>Since upgrading to 4.3.1, I can now see my posts making use of the creator tag feature.</p>\n<p><img src=\"https://github.com/user-attachments/assets/c8be21fd-daa8-40ef-b923-fd6c302ab1f3\" class=\"img-fluid\" alt=\"Screenshot of Mastodon Post with Author Profile Link\" /></p>\n<p>As you can see from the original post, implementing it was relatively trivial. Another win for open protocols and standards.</p>\n","tags":["mastodon","fediverse","opengraph","protocols","indieweb"]},{"title":"Mastodon instance working again","contentType":"notes","date":"2024-11-18 17:34 -05:00","url":"/notes/mastodon-working-again-v-4-3-1/","content":"<p>After about a month of not being able to access my Mastodon instance, I'm so excited to have it working again!</p>\n<p>Not sure what I did wrong with my upgrade to 4.3.0. Fortunately, the issues appears to have been on the frontend and the fix wasn't too involved.</p>\n<p>Upgrading to the latest 4.3.1 fixed the issue I ran into during my last upgrade.</p>\n<p>Thanks to my <a href=\"/posts/rss-to-mastodon-posse-azure-logic-apps/\">POSSE setup</a>, I've been able to continue posting to my website and syndicating to Mastodon. Unfortunately, since the frontend wasn't working, I was unable to access my notifications. Because the mobile apps make use of the web frontend to log in, I couldn't use the mobile app either.</p>\n","tags":["mastodon","fediverse","sysadmin","selfhost","socialmedia","social","microblog","indieweb","posse"]},{"title":"Week of November 18, 2024 - Post Summary","contentType":"notes","date":"2024-11-17 19:56 -05:00","url":"/notes/2024-11-18-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/website-upgraded-dotnet-9/\">Upgraded to .NET 9</a></li>\n<li><a href=\"/notes/tyson-vs-paul\">Mike Tyson vs Jake Paul</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/thundercat-chromakopia\">Thundercat on Tyler's Chromakopia album?</a></li>\n<li><a href=\"/responses/dotnetconf-2024-day-1-success\">Day 1 of .NET Conf</a></li>\n<li><a href=\"/responses/dotnet-conf-2024-session-playlist\">.NET Conf 2024 - Session Recordings</a></li>\n<li><a href=\"/responses/why-you-should-make-a-website-luvstarkei/\">Why YOU Should Make A Website</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Mike Tyson vs Jake Paul","contentType":"notes","date":"2024-11-16 10:36 -05:00","url":"/notes/tyson-vs-paul/","content":"<p>Amazing that Tyson at his age just did that. He earned that paycheck. That said, it was tough to watch and fortunately it didn't seem like he got hurt too bad. It looked like Jake was pulling his punches because that fight could've ended in the 2nd / 3rd round. Overall, it's the best outcome possible. Jake can say he &amp;quot;beat&amp;quot; Mike Tyson, Tyson didn't get knocked out or hurt too bad, and they both earned a decent paycheck.</p>\n","tags":["boxing","tyson","ironmike"]},{"title":"Upgraded to .NET 9","contentType":"notes","date":"2024-11-14 00:38 -05:00","url":"/notes/website-upgraded-dotnet-9/","content":"<p>Obligatory upgrade post. Just upgraded the website to .NET 9. As with all the previous years, it involved no code changes, and all I had to do was updade the version I'm using in my project file and GitHub Actions file.</p>\n","tags":["dotnet","net9","githubactions","github","indieweb","upgrade"]},{"title":".NET Conf 2024 Bound","contentType":"notes","date":"2024-11-10 14:29 -05:00","url":"/notes/netconf-bound-2024/","content":"<p>It's that time of the year again. A new .NET release.</p>\n<p>Tons of great sessions at .NET Conf this year, which you can see in the <a href=\"https://www.dotnetconf.net/\">agenda</a>.</p>\n<p>I'll be taking part in two sessions:</p>\n<ul>\n<li>Building AI Applications from Scratch: A Hands-On Guide for .NET Developers</li>\n<li>Building the Foundation: AI Fundamentals in .NET</li>\n</ul>\n","tags":["dotnet","dotnetconf","conference","ai","dotnet9","net9"]},{"title":"Week of November 10, 2024 - Post Summary","contentType":"notes","date":"2024-11-10 09:33 -05:00","url":"/notes/2024-11-10-weekly-post-summary/","content":"<p>Started experimenting some more with posting photos this week.</p>\n\n<ul>\n<li><a href=\"/notes/2024-11-03-weekly-post-summary/\">Week of November 3, 2024 - Post Summary</a></li>\n<li><a href=\"/notes/nyc-sunrise-fall-2024\">NYC Sunrise</a></li>\n<li><a href=\"/notes/netconf-bound-2024\">.NET Conf 2024 Bound</a></li>\n<li><a href=\"/notes/finally-feels-like-fall\">Finally starting to feel like fall</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/microsoft-research-focus-oct-28-2024\">Microsoft Research Focus: Week of October 28, 2024</a></li>\n<li><a href=\"/responses/thinking-llms-instruction-following-thought-generation\">Thinking LLMs: General Instruction Following with Thought Generation</a></li>\n<li><a href=\"/responses/introducing-drift-search\">Microsoft Research: Introducing DRIFT Search</a></li>\n<li><a href=\"/responses/in-context-lora\">In-Context LoRA for Diffusion Transformers</a></li>\n<li><a href=\"/responses/leo-fm-moves-to-microblog\">Leo.fm move to micro.blog</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"NYC Sunrise","contentType":"notes","date":"2024-11-06 19:35","url":"/notes/nyc-sunrise-fall-2024/","content":"<p>I was lucky to snap this picture the other day.</p>\n<p><img src=\"/files/images/nyc-sunrise.jpg\" class=\"img-fluid\" alt=\"Sun rising behind NYC skyline\" /></p>\n","tags":["photo","sunrise","nyc","city"]},{"title":"Week of November 3, 2024 - Post Summary","contentType":"notes","date":"2024-11-03 20:42 -05:00","url":"/notes/2024-11-03-weekly-post-summary/","content":"<p>Fairly active week on the website.</p>\n\n<ul>\n<li><a href=\"/posts/digitize-analog-bookmarks-with-ai-dotnet\">Digitize Analog Bookmarks using AI, .NET, and GitHub Models</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/notes/post-statistics-october-2024\">Website Post Statistics - October 2024</a></li>\n<li><a href=\"/notes/crate-finds-october-2024\">Crate Finds - October 2024</a></li>\n<li><a href=\"/notes/finally-feels-like-fall\">Finally starting to feel like fall</a></li>\n<li><a href=\"/notes/spooky-season-2024-over\">Only 365 more days until next halloween</a></li>\n<li><a href=\"/notes/st-chroma-chromakopia\">St. Chroma is a great intro</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/spirit-box-flying-lotus\">New Release: Spirit Box by Flying Lotus</a></li>\n<li><a href=\"/responses/chromakopia-tyler-the-creator-released\">Chromakopia by Tyler The Creator Released</a></li>\n<li><a href=\"/responses/notebook-llama-oss-notebook-lm\">NotebookLlama: An Open Source version of NotebookLM</a></li>\n<li><a href=\"/responses/haves-choices-brown-nielsen\">Haves and Choices</a></li>\n<li><a href=\"/responses/art-clown-nasdaq-bell-halloween\">Art the Clown Will Ring the Nasdaq Closing Bell in Times Square on Halloween!</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Finally starting to feel like fall","contentType":"notes","date":"2024-11-03 20:30 -05:00","url":"/notes/finally-feels-like-fall/","content":"<p>It's been unseasonably warm around these parts but this weekend it finally started to cool down. That meant hot cider and cider donuts straight from the source.</p>\n<p><img src=\"/files/images/finally-feels-like-fall.jpg\" class=\"img-fluid\" alt=\"A man playing the piano and woman singing under a canopy with a lake in the background\" /></p>\n","tags":["fall","autumn","seasons"]},{"title":"St. Chroma is a great intro","contentType":"notes","date":"2024-11-01 22:28","url":"/notes/st-chroma-chromakopia/","content":"<p>I love the album overall but St. Chroma is a great intro that sets up the rest of the album.</p>\n<p>Some of my favorite parts from the track</p>\n<blockquote class=\"blockquote\">\n<p>You are the light. It's not on you, it's in you. Don't you ever in your motherfucking life dim your light for nobody.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>P said I could do it too, and boy, did I believe him. I built a path to freedom 'cause them words that he said. Give a fuck about tradition, stop impressin' the dead</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Don't you ever stop bein' who you are and dimmin' your light for none of these motherfuckers out here</p>\n</blockquote>\n<p><a href=\"https://www.youtube.com/watch?v=hCcwCv3G1FQ\" title=\"Album cover of Chromakopia - Tyler The Creator in a Mask\"><img src=\"http://img.youtube.com/vi/hCcwCv3G1FQ/0.jpg\" class=\"img-fluid\" alt=\"Album cover of Chromakopia - Tyler The Creator in a Mask\" /></a></p>\n","tags":["chromakopia","tylerthecreator","music"]},{"title":"Crate Finds - October 2024","contentType":"notes","date":"2024-11-01 09:26","url":"/notes/crate-finds-october-2024/","content":"<p>Lots of music released this month</p>\n<ul>\n<li><a href=\"/responses/chromakopia-tyler-the-creator-released\">Chromakopia by Tyler The Creator</a></li>\n<li><a href=\"/responses/spirit-box-flying-lotus\">Spirit Box by Flying Lotus</a></li>\n</ul>\n<p>Both amazing albums I've now listed to multiple times from beginning to end.</p>\n<p>That said, the Crate Finds this month are light and can best be summarized as my favorite songs from <a href=\"/notes/got-a-story-to-tell-thee-sacred-souls\">Got A Story to Tell by Thee Sacred Souls</a>.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/18ZVcs71usQpBjaYO7XEA0\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"Website Post Statistics - October 2024","contentType":"notes","date":"2024-11-01 09:25","url":"/notes/post-statistics-october-2024/","content":"<p>It's been a while since I've done one of these.</p>\n<p>One thing I'm changing is the titles. For the other posts, the titles don't match up with the date for the metrics. For example the post for &amp;quot;July 2024&amp;quot; was actually talking about June metrics. Kind of confusing, so fixing that this month.</p>\n<p>This is my post</p>\n<p>Post Type</p>\n<p>Count</p>\n<p>Blogs</p>\n<p>4</p>\n<p>Notes</p>\n<p>83</p>\n<p>Responses</p>\n<p>251</p>\n<p>Overall the trend is nice. I'm just under 1 response a day. Not like that's the goal, but in general I think it shows a steady stream of content I'm posting on the website.</p>\n<p>With two recent blog posts published, that brings me to 5. The script ran just before I hit publish on the last blog which is why you only see four.</p>\n<ul>\n<li><a href=\"/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire/\">Build your own self-hosted live streaming application with Owncast and .NET Aspire</a></li>\n<li><a href=\"/posts/digitize-analog-bookmarks-with-ai-dotnet/\">Digitize Analog Bookmarks using AI, .NET, and GitHub Models</a></li>\n</ul>\n<p>There's a few other ideas I'm tinkering with, which might easily get me close to or past the 8 blog posts from last year.</p>\n","tags":["githubactions","cicd","blogging","stats","github"]},{"title":"Only 365 more days until next halloween","contentType":"notes","date":"2024-10-31 08:29","url":"/notes/spooky-season-2024-over/","content":"<p>That's a wrap for spooky season. It was unseasonably warm so it hasn't really felt like fall.</p>\n<p><img src=\"/files/images/365-days-nightmare-before-christmas.jpg\" class=\"img-fluid\" alt=\"Nightmare Before Christmas Mayor Speaking Into Loudspeaker with Caption\" /></p>\n","tags":["fall","spookyszn","halloween"]},{"title":"Week of October 20, 2024 - Post Summary","contentType":"notes","date":"2024-10-20 09:27 -05:00","url":"/notes/2024-10-20-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire\">Build your own self-hosted live streaming application with Owncast and .NET Aspire</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/you-should-be-using-rss-reader-pluralistic/\">You should be using an RSS reader</a></li>\n<li><a href=\"/responses/chromakopia-tyler-the-creator\">Chromakopia - Tyler The Creator</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Penn State (33) vs. USC (30)","contentType":"notes","date":"2024-10-12 19:22","url":"/notes/penn-state-usc-oct-2024/","content":"<p>Wow! This game delivered. I haven't watched any games this season and I couldn't have picked a better one to tune into.</p>\n<p>It was an ugly game for Penn State and they got lucky on some calls but they found a way to get it done on the road.</p>\n","tags":["pennstate","usc","football","ncaaf","ncaa","psu"]},{"title":"New Release: Ingo Swann by Flying Lotus","contentType":"notes","date":"2024-10-10 21:13","url":"/notes/flylo-ingo-swann-single/","content":"<p>New drop from FlyLo. This feels like a new album coming. So far the theme is mystical.</p>\n<p>The previous single, <a href=\"/responses/new-flying-lotus-garmonbozia\">Garmonbozia</a>, was a reference to Twin Peaks.</p>\n<p>Ingo Swann's website describes him as</p>\n<blockquote class=\"blockquote\">\n<p>...a pioneer of parapsychology and remote viewing, who worked with CIA and NASA, wrote books, and created art.</p>\n</blockquote>\n<p>The website invites visitors to,</p>\n<blockquote class=\"blockquote\">\n<p>explore his research on NHEs, UFOs, UAPs, and the planets.</p>\n</blockquote>\n<p>Similarly, Wikipedia describes him as</p>\n<blockquote class=\"blockquote\">\n<p>an American psychic, artist, and author, whose claims of clairvoyance were investigated as a part of the Central Intelligence Agency’s Stargate Project. Swann is credited as the creator of the term “Remote Viewing,&amp;quot; a term which refers to the use of extrasensory perception to perceive distant persons, places, or events.</p>\n</blockquote>\n<p>I'm excited to discovery the mystery behind this new project from FlyLo.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/0MorRE2kwzea56XwkFMUYE?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["music","newrelease","flyinglotus","flylo","dance","cia","remoteviewing","ingoswann"]},{"title":"New Release: Got a Story to Tell by Thee Sacred Souls","contentType":"notes","date":"2024-10-10 21:10","url":"/notes/got-a-story-to-tell-thee-sacred-souls/","content":"<p>New release from Thee Sacred Souls. I really enjoyed the beginning and the end.</p>\n<p>My favorite songs are:</p>\n<ul>\n<li>Lucid Girl</li>\n<li>Price I'll Pay</li>\n<li>Live for You</li>\n<li>On My Mind</li>\n<li>Waiting on the Right Time</li>\n<li>I'm So Glad I Found You, Baby</li>\n</ul>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/album/6yvKcQNyd09wYCCp8O8mNB?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["music","newrelease","theesacredsouls","soul","rnb","oldies"]},{"title":"Crate Finds - September 2024","contentType":"notes","date":"2024-10-10 21:04","url":"/notes/crate-finds-september-2024/","content":"<p>This month had a mix of genres. Some of the ones I kept coming back to:</p>\n<ul>\n<li>Wharf Rat by Grateful Dead</li>\n<li>Deal by Grateful Dead</li>\n<li>God is Love by Marvin Gaye</li>\n<li>Save the Children by Marvin Gaye</li>\n</ul>\n<p>I listened to <a href=\"/responses/marvin-gaye-whats-going-on/\">the album What's Going On a few times throughout the month</a>.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/1EsdYKLwNkmwR8s6uRyjHZ?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"Week of September 15, 2024 - Post Summary","contentType":"notes","date":"2024-09-15 13:26 -05:00","url":"/notes/2024-09-15-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/crate-finds-august-2024\">Crate Finds - August 2024</a></li>\n<li><a href=\"/notes/leadership-ascension-track-omegamart\">Leadership Ascension Track (LAT)</a></li>\n<li><a href=\"/notes/post-statistics-september-2024\">Website Post Statistics - September 2024</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/andre-3000-listening-to-the-sun\">Listening to the Sun by Andre 3000</a></li>\n<li><a href=\"/responses/cosmohedron-duncan-hatch\">Cosmohedron by Duncan Hatch</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Website Post Statistics - September 2024","contentType":"notes","date":"2024-09-15 09:19","url":"/notes/post-statistics-september-2024/","content":"<p>The most significant update from last month is that I have now passed the number of responses compared to last year.</p>\n<p>The top tag continues to be AI and The Verge is the top domain I link to.</p>\n<p>I'm in the process of redesigning the website, so that's probably one of the things I'll be focusing the next few months.</p>\n","tags":["githubactions","cicd","blogging","stats","github"]},{"title":"Crate Finds - August 2024","contentType":"notes","date":"2024-09-15 09:11","url":"/notes/crate-finds-august-2024/","content":"<p>A little late with this post, but here's what I was listening to in August.</p>\n<p>Some songs that were on repeat:</p>\n<ul>\n<li>Dark Star by Grateful Dead</li>\n<li>Ripple by Grateful Dead</li>\n<li>Garmonbozia by Flying Lotus</li>\n<li>Pretty Girl Why by Buffalo Springfield</li>\n<li>Sit with the Guru by Strawberry Alarm Clock</li>\n</ul>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/4WioPgkf1uJsLQWAw4nYuS?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"Leadership Ascension Track (LAT)","contentType":"notes","date":"2024-09-15 08:59","url":"/notes/leadership-ascension-track-omegamart/","content":"<p>Excerpts from the Omega Mart Leadership Ascension Track (LAT)</p>\n\n<ul>\n<li>Make a human pyramid with your high school guidance counselor and middle school crush.</li>\n</ul>\n\n<ul>\n<li>Contribute 14% of your net worth to Drampcorp.</li>\n<li>Be nice to 77 dogs.</li>\n</ul>\n\n<ul>\n<li>Learn to suck venom out of a snake bite.</li>\n<li>Befriend a snake.</li>\n<li>Interview a rock and ghost write its memoir.</li>\n<li>Spend a week in the desert, living off the land.</li>\n</ul>\n\n<ul>\n<li>Buy two cakes. Decorate and dedicate one of the cakes to the other. Stack the cakes. Leave them both on a neighbor's doorstep in a ding-dong-ditch scenario.</li>\n<li>Swim to a whirlpool and dance within its rotating waves (hint: go with the flow).</li>\n</ul>\n\n<ul>\n<li>Approach a stranger in a coffee shop, have them show you their laptop search history, find the second-most recent country they've searched and move there for 7 months.</li>\n</ul>\n\n<ul>\n<li>Use an infinity mirror to get ready for a night on the town.</li>\n</ul>\n\n<ul>\n<li>Take a childhood photo to the top of a mountain. Bury the photo and create a new peak.</li>\n<li>One day a week, wake at dawn and meditate until sundown.</li>\n</ul>\n","tags":["meowwolf","omegamart","art","lasvegas","las","corporate","lat","leadershipascensiontrack","source","area15","leadership","fun","conspiracy"]},{"title":"Playing around with the Ivy Cliq+ 2","contentType":"notes","date":"2024-08-13 22:29 -05:00","url":"/notes/ivy-cliq-2-camera-photo-printer/","content":"<p><img src=\"https://s7d1.scene7.com/is/image/canon/4519C001_ivy-cliq-plus2-instant-camera-printer-plus-app-rose-gold_primary?fmt=webp-alpha&amp;amp;wid=800\" class=\"img-fluid\" alt=\"Image of Canon Ivy Cliq+ 2 camera and photo printer\" /></p>\n<p>I got my hands on an <a href=\"https://www.usa.canon.com/shop/p/ivy-cliq-2-instant-camera-printer-app\">Ivy Cliq+ 2</a> earlier today and am really liking it.</p>\n<p>It's really cool that it's both a point-and-shoot camera as well as a photo printer. The camera comes in handy if you want to capture something in the moment and print it out immediately. The printer is great for pictures you've taken on your phone or some other device. Even better, you can create collages, apply filters, and do all sorts of adjustments to images before printing.</p>\n<p>If there's one thing I didn't like is that in order to set it up, it needed access to my exact location. Trying approximate location wasn't enough. It connects over Bluetooth, so you'd think that should be enough to pair. Not sure what it needs my EXACT location for.</p>\n","tags":["analog","camera","photos","journal","memory"]},{"title":"Week of August 11, 2024 - Post Summary","contentType":"notes","date":"2024-08-11 13:54 -05:00","url":"/notes/2024-08-11-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/what-is-kick/\">What is Kick?</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/mike-masnick-bluesky-board-of-directors\">Bluesky Welcomes Mike Masnick to Board of Directors</a></li>\n<li><a href=\"/responses/blogroll-club-blog-directory/\">Blogroll Club - A blog directory</a></li>\n<li><a href=\"/responses/jackson-hole-instagram-filter-wildlife\">Jackson Hole's new Instagram filter warns you when you're dangerously close to wildlife</a></li>\n<li><a href=\"/responses/congratulations-overcast-ten-years\">Congratulations on ten years of Overcast</a></li>\n<li><a href=\"/responses/cnet-bought-for-just-100-million\">CNET purchased for JUST $100 million</a></li>\n<li><a href=\"/responses/gpt4o-system-card\">GPT-4o System Card</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"What is Kick?","contentType":"notes","date":"2024-08-05 21:30","url":"/notes/what-is-kick/","content":"<p>I'm oficially old. I had to look up what <a href=\"https://kick.com/\">Kick</a> is.</p>\n<p>In case you also didn't know, I'll save you some time.</p>\n<blockquote class=\"blockquote\">\n<p>Kick is a platform where you can watch and interact with live streamers from various categories, such as games, music, gambling and creative.</p>\n</blockquote>\n","tags":["livestream","video","app","web","internet"]},{"title":"Website Post Statistics - August 2024","contentType":"notes","date":"2024-08-01 20:19","url":"/notes/post-statistics-august-2024/","content":"<p>Finished July with more regular posting.</p>\n<p>Only 4 posts away from passing last year's response count.</p>\n<p>As mentioned on <a href=\"/notes/org-capture-functions-file-target\">another post</a>, I've been slowly working on transitioning authoring workflows to Emacs using org-capture templates.</p>\n<p>This month, I want to work on making capturing content from Nyxt more seamless, especially since responses are the types of posts I use most.</p>\n<p>The other thing I'd like to start doing is consolidating the structure of my posts. Right now the YAML front-matter for the various posts types is significantly different. However, it doesn't have to be.</p>\n<p>For most posts, I need:</p>\n<ul>\n<li>Title</li>\n<li>Date published</li>\n<li>Data updated</li>\n<li>Tags</li>\n</ul>\n<p>Responses are a little different because I also need to include the link to the original content I'm referencing in the post.</p>\n<p>Once that's done, at some point, I'd also like to consolidate my URLs.</p>\n<ul>\n<li><code>/posts</code> captures long-form blog posts</li>\n<li><code>/feed</code> captures microblog posts and responses</li>\n</ul>\n<p>Since everything is a post, consolidating under one of them for all my posts and providing additional urls like <code>/notes</code>, <code>/photos</code>, <code>/articles</code> for the individual types of posts might make more sense.</p>\n","tags":["githubactions","cicd","blogging","stats","github"]},{"title":"OPML for website feeds","contentType":"notes","date":"2024-08-01 20:08 -05:00","url":"/notes/website-feeds-opml/","content":"<p>While thiking about implementing <a href=\"/responses/well-known-feeds/\"><code>.well-known</code> for RSS feeds on my site</a>, I had another idea. Since that uses OPML anyways, I remembered recently <a href=\"/notes/blogroll-discovery-implemented//\">doing something similar for my blogroll</a>.</p>\n<p>The concept is the same, except instead of making my blogroll discoverable, I'm doing it for my feeds. At the end of the day, a blogroll is a collection of feeds, so it should just work for my own feeds.</p>\n<p>The implementation ended up being:</p>\n<ol>\n<li><p>Create an OPML file for<a href=\"/feed\"> each of the feeds on by website</a>.</p>\n<pre><code class=\"language-xml\"> &amp;lt;opml version=&amp;quot;2.0&amp;quot;&amp;gt;\n   &amp;lt;head&amp;gt;\n \t&amp;lt;title&amp;gt;Luis Quintanilla Feeds&amp;lt;/title&amp;gt;\n \t&amp;lt;ownerId&amp;gt;https://www.luisquintanilla.me&amp;lt;/ownerId&amp;gt;\n   &amp;lt;/head&amp;gt;\n   &amp;lt;body&amp;gt;\n \t&amp;lt;outline title=&amp;quot;Blog&amp;quot; text=&amp;quot;Blog&amp;quot; type=&amp;quot;rss&amp;quot; htmlUrl=&amp;quot;/posts/1&amp;quot; xmlUrl=&amp;quot;/blog.rss&amp;quot; /&amp;gt;\n \t&amp;lt;outline title=&amp;quot;Microblog&amp;quot; text=&amp;quot;Microblog&amp;quot; type=&amp;quot;rss&amp;quot; htmlUrl=&amp;quot;/feed&amp;quot; xmlUrl=&amp;quot;/microblog.rss&amp;quot; /&amp;gt;\n \t&amp;lt;outline title=&amp;quot;Responses&amp;quot; text=&amp;quot;Responses&amp;quot; type=&amp;quot;rss&amp;quot; htmlUrl=&amp;quot;/feed/responses&amp;quot; xmlUrl=&amp;quot;/responses.rss&amp;quot; /&amp;gt;\n \t&amp;lt;outline title=&amp;quot;Mastodon&amp;quot; text=&amp;quot;Mastodon&amp;quot; type=&amp;quot;rss&amp;quot; htmlUrl=&amp;quot;/mastodon&amp;quot; xmlUrl=&amp;quot;/mastodon.rss&amp;quot; /&amp;gt;\n \t&amp;lt;outline title=&amp;quot;Bluesky&amp;quot; text=&amp;quot;Bluesky&amp;quot; type=&amp;quot;rss&amp;quot; htmlUrl=&amp;quot;/bluesky&amp;quot; xmlUrl=&amp;quot;/bluesky.rss&amp;quot; /&amp;gt;\n \t&amp;lt;outline title=&amp;quot;YouTube&amp;quot; text=&amp;quot;YouTube&amp;quot; type=&amp;quot;rss&amp;quot; htmlUrl=&amp;quot;/youtube&amp;quot; xmlUrl=&amp;quot;/bluesky.rss&amp;quot; /&amp;gt;\n   &amp;lt;/body&amp;gt;\n &amp;lt;/opml&amp;gt;\n</code></pre>\n</li>\n<li><p>Add a <code>link</code> tag to the <code>head</code> element of my website.</p>\n<pre><code class=\"language-html\"> &amp;lt;link rel=&amp;quot;feeds&amp;quot; type=&amp;quot;text/xml&amp;quot; title=&amp;quot;Luis Quintanilla's Feeds&amp;quot; href=&amp;quot;/feed/index.opml&amp;quot;&amp;gt;\n</code></pre>\n</li>\n</ol>\n","tags":["rss","opml","indieweb","feeds","internet","protocols","webstandards","standards","web"]},{"title":"Crate Finds - July 2024","contentType":"notes","date":"2024-08-01 20:06 -05:00","url":"/notes/crate-finds-july-2024/","content":"<p>Grateful Dead on repeat this month. Songs I kept coming back to:</p>\n<ul>\n<li>Standing on the Moon</li>\n<li>Eyes of the World</li>\n<li>Fire on the Mountain</li>\n</ul>\n<p>A few Doors songs also got in there I hadn't listened to before like Blue Sunday.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/2TxI05v0WTDclWJNji0phj?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"Subscription dark patterns","contentType":"notes","date":"2024-07-30 20:37 -05:00","url":"/notes/subscription-dark-patterns/","content":"<p>I recently took inventory of subscriptions I'm no longer using. The ones that were especially tricky to spot were the annual subscriptions that don't show up on bank statements every month. These also happened to be ones that I hadn't used in years.</p>\n<p>Getting a list of them was the easy part. The fun didn't start until I actually went through the process of unsubscribing.</p>\n<p>For a few of them, cancelling was relatively straightforward.</p>\n<p>Others, it was an adventure. Best case, I was presented with several screens where the option to keep my subscription was front and center and the cancel button was in small print all the way at the bottom of the screen. In other cases, I had to send an e-mail to cancel. My favorite though was the one where I was able to log in and access member content. When I went to the account settings to try and cancel my subscription, they were conveniently having technical difficulties <span>🙃</span></p>\n<p>Super frustrating experience all around, but happy to get it done so I get to keep more of my money.</p>\n","tags":["darkpattern","subscriptions","services"]},{"title":"User-provided target file names in Org Capture","contentType":"notes","date":"2024-07-29 21:45 -05:00","url":"/notes/org-capture-functions-file-target/","content":"<p>I finally got my website note org-capture template working for new files.</p>\n<p>Originally, I only had it working on existing files using this snippet.</p>\n<pre><code class=\"language-elisp\">(&amp;quot;wne&amp;quot;\n &amp;quot;Creates a note in an existing file&amp;quot;\n plain\n (file buffer-file-name)\n (file ,(file-name-concat website-template-dir &amp;quot;note.txt&amp;quot;)))\n</code></pre>\n<p>This template uses the file name of the current buffer to select the insertion target.</p>\n<p>Getting it to work with a new file specified by the user, requires a small tweak.</p>\n<pre><code class=\"language-elisp\">(&amp;quot;wnn&amp;quot;\n &amp;quot;Creates a note in a new file&amp;quot;\n plain\n (file (lambda () (file-name-concat website-note-dir (format &amp;quot;%s.md&amp;quot; (read-string &amp;quot;Enter file name: &amp;quot;)))))\n (file ,(file-name-concat website-template-dir &amp;quot;note.txt&amp;quot;)))\n</code></pre>\n<p>At first, I thought I had to use the <code>function</code> target type since I wanted to use a function to capture the file name. That didn't work.</p>\n<p>I then realized, I could still keep the <code>file</code> target type. However, to fill in the file name, I could use a function which takes in user input.</p>\n<p>Now that I got this working, I've also done the same for my reponse template.</p>\n","tags":["emacs","orgmode","capture","org","website","templates","gnu"]},{"title":"Note to Self: Useful Emacs Commands Pt. 1","contentType":"notes","date":"2024-07-28 17:00 -05:00","url":"/notes/useful-emacs-commands-pt-1/","content":"<p>I still need to set up an org-capture template for snippets, so writing a note to myself to remember these commands I'll eventually want to come back to.</p>\n\n<p>In this custom function, <code>elfeed-show-yank</code> extracts the link element in an elfeed entry. <code>org-capture</code> then just invokes the org-capture template selection prompt. At this point, I can move forward with creating a response entry on the website and since the link to the entry I was viewing in elfeed is in the kill-ring, I can easily paste the URL while filling out the org-capture template.</p>\n<pre><code class=\"language-lisp\">(defun capture-elfeed-entry ()\n  (elfeed-show-yank)\n  (org-capture))\n</code></pre>\n\n<p>I recently found out, I can add filters to org-capture templates based on the mode I'm in Emacs. Here's an example:</p>\n<pre><code class=\"language-lisp\">(setq org-capture-templates-contexts\n      '((&amp;quot;wrn&amp;quot; ((in-mode . &amp;quot;elfeed-show-mode&amp;quot;)))))\n</code></pre>\n<p><code>org-capture-templates-contexts</code> defines a set of conditions that defines which contexts certain org-capture templates appear under.</p>\n<p>For example, the org-capture template mapped to <code>wrn</code> will only be visible when org-capture is invoked from a buffer in <code>elfeed-show-mode</code>.</p>\n<p>More information can be found in the <a href=\"https://orgmode.org/manual/Templates-in-contexts.html\">org-capture templates in context documentation</a></p>\n","tags":["emacs","lisp","elisp","programming","code","notetoself"]},{"title":"Week of July 28, 2024 - Post Summary","contentType":"notes","date":"2024-07-28 12:27 -05:00","url":"/notes/2024-07-28-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/2024-07-21-weekly-post-summary/\">Week of July 21, 2024 - Post Summary</a></li>\n<li><a href=\"/notes/crate-finds-june-2024\">Crate Finds - June 2024</a></li>\n<li><a href=\"/notes/useful-emacs-commands-pt-1\">Note to Self: Useful Emacs Commands Pt. 1</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/nyxt-emacs-hacks\">Nyxt Emacs Hacks</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Crate Finds - June 2024","contentType":"notes","date":"2024-07-21 18:29 -05:00","url":"/notes/crate-finds-june-2024/","content":"<p>A few new songs. Besides listening to them a few times though, there aren't many I came back to throughout the month.</p>\n<p>I'm excited for the new album from Thee Sacred Souls though.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/76c1KsZSq4yoa7XtqTa26U?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"Week of July 21, 2024 - Post Summary","contentType":"notes","date":"2024-07-21 13:50 -05:00","url":"/notes/2024-07-21-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/crate-finds-june-2024\">Crate Finds - June 2024</a></li>\n<li><a href=\"/notes/test-emacs-capture-note\">Testing org-capture template for notes</a></li>\n<li><a href=\"/notes/post-statistics-july-2024\">Website Post Statistics - July 2024</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/facebook-zombie-internet-willison\">Zombie Internet</a></li>\n<li><a href=\"/responses/test-emacs-capture-response-org\">Test response using org capture templates</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Website Post Statistics - July 2024","contentType":"notes","date":"2024-07-18 18:26","url":"/notes/post-statistics-july-2024/","content":"<p>Better late than never.</p>\n<p>Nothing too interesting this month. I haven't been posting as much.</p>\n<p>I think part of that is because I've been creating DayOne posts more.</p>\n<p>I've been relying on DayOne to share articles from my RSS reader. That's worked really well for bookmarking.</p>\n<p>As far as my authoring process, when working locally, I've been using Emacs more.</p>\n<p>One reason for Emacs is <a href=\"https://github.com/lqdev/luisquintanilla.me/tree/main/.templates\">capture templates</a>.</p>\n<p>Technically, I already have something similar with <a href=\"https://github.com/lqdev/luisquintanilla.me/tree/main/.vscode\">VS Code Snippets</a>.</p>\n<p>I like the richness of the Emacs experience more though.</p>\n<p>The future workflow I'm thinking of involves two components:</p>\n<ul>\n<li>Elfeed</li>\n<li>Nyxt</li>\n</ul>\n<p>When reading articles using Elfeed, I can create a function that takes the information from the entry I'm on and converts it into an entry on my site using capture templates.</p>\n<p>Similarly, in Nyxt while browsing the internet, I can create a similar function which uses capture templates to create website entries.</p>\n<p>Maybe I can get a long-form post out of that experiment.</p>\n<p>As far as numbers go, I'm at 56 notes for the year which is 15 more than last year.</p>\n<p>I'm closing in on passing last year's response count at 210.</p>\n<p>Most of the content shared continues to be mainly about AI.</p>\n<p>Top linked domain is still The Verge.</p>\n","tags":["githubactions","cicd","blogging","stats","github"]},{"title":"Testing org-capture template for notes","contentType":"notes","date":"2024-07-14 13:50 -05:00","url":"/notes/test-emacs-capture-note/","content":"<p>Testing an org-capture template generated note for the website.</p>\n","tags":["org","emacs","blogging","website","blogging","indieweb","personalweb","automation","capture","template","orgmode","capturetemplates"]},{"title":"Clock Tables - Org Mode, Plain Text, and AI","contentType":"notes","date":"2024-07-09 22:07","url":"/notes/love-plain-text-org-mode-clocktable/","content":"<p>Org-mode appreciation post.</p>\n<p>I use plain text and org-mode for most things in my life, especially when it comes to task and life management.</p>\n<p>I won't rehash all the reasons Emacs and org-mode are amazing. There are tons of blog posts and videos out there that would do it more justice than I ever could.</p>\n<p>Over the last few years, Emacs has become my go-to text editor. Throughout all that time, I've continued to find new features that delight.</p>\n<p>The most recent is <a href=\"https://orgmode.org/manual/The-clock-table.html\">clock table</a>.</p>\n<p>I already use org-mode to track my to-dos and perform some sort of time-block planning by setting deadlines and scheduling tasks.</p>\n<p>Recently though, I've been wanting a way to see all of the things I've worked on over the past [ INSERT TIME PERIOD ]. More importantly, I'd like to have time associated with them to see where my time has gone and evaluate whether I'm spending time on the things I should be.</p>\n<p>I knew you could <a href=\"https://orgmode.org/manual/Clocking-commands.html\">clock in and clock out</a> on tasks. However, I didn't know you could easily build a customized report that automatically updates. That's when I came across clock tables.</p>\n<p>Now, I have a way of visualizing all of the things I worked on during a week or month, and as I'm planning for the next week or month, I can adjust and reprioritize the things I'm working on.</p>\n<p>I know there are enterprise offerings like the Viva suite from Microsoft which provides detailed reports on how you spend your time.</p>\n<p>What excites me about org-mode though is that it's plain text. The clock table report that gets generated is a plain text table which makes it portable and easy to access using any text editor of your choice. It works best with Emacs, but that's not a requirement.</p>\n<p>On their own, clock tables are amazing.</p>\n<p>However, given how well language models work on plain text, they could be used as context for your queries. Imagine giving a language model as input an org file which contains:</p>\n<ul>\n<li>A clock table</li>\n<li>A list of TODO tasks (with notes, priorities, deadlines, tags, properties, and other annotations)</li>\n<li>A high level list of goals you want to achieve</li>\n</ul>\n<p>Then, you could enter a prompt along the lines of: &amp;quot;Using the following clock-table and list of goals I want to achieve, provide me with recommendations of tasks I should work on for the next week. Ensure that they align with my goals, are top priority, and provide the highest return on my efforts&amp;quot;.</p>\n<p>Additionally, you might also provide your org-agenda view containing appointments and use the results from the first query as context for the following prompt: &amp;quot;Given the agenda view for the next week, schedule the top 3 tasks you recommended&amp;quot;.</p>\n<p>The result would be a list of TODO items containing schedule / active timestamps annotations which now show up on your org-agenda view.</p>\n<p>Today, almost every productivity application out there is working on building these kinds of AI features into their products. The difference is, in many cases, the formats used by those applications aren't plain text. This adds complexity to the data extraction and transformation pipelines required to get the data into the right format for a language model to process. With plain text, there's little to no transformations required.</p>\n<p>What's even better, I can extend org-mode and Emacs using elisp to do the things I just talked about.</p>\n<p>I'm no elisp expert, so I asked Copilot to generate an elisp function that takes an org-mode clock table as input and generates a written text summary of it. Here are the results:</p>\n<pre><code class=\"language-lisp\">(defun org-clock-summary-to-chatgpt ()\n  &amp;quot;Extracts clock table data and requests a summary from ChatGPT.&amp;quot;\n  (interactive)\n  (let* ((clock-table-data (org-clocktable-get-clocktable))\n         (summary-text (org-clocktable-format-summary clock-table-data))\n         (api-endpoint &amp;quot;https://api.openai.com/v1/engines/davinci/completions&amp;quot;)\n         (api-key &amp;quot;YOUR_API_KEY&amp;quot;)) ; Replace with your actual API key\n<pre><code>;; Construct the HTTP request and send it to ChatGPT\n(with-current-buffer\n    (url-retrieve-synchronously api-endpoint\n                                `((&amp;amp;quot;text&amp;amp;quot; . ,summary-text)\n                                  (&amp;amp;quot;api_key&amp;amp;quot; . ,api-key)))\n  (goto-char (point-min))\n  (search-forward &amp;amp;quot;\\n\\n&amp;amp;quot;)\n  (let ((response-json (json-read)))\n    (message &amp;amp;quot;ChatGPT summary: %s&amp;amp;quot; (cdr (assoc 'choices response-json)))))))\n</code></pre>\n<p></code></pre></p>\n<p>All you elisp experts out there, let me know how it did.</p>\n<p>I know at least the API endpoint is wrong, but generally speaking, it seems to be doing the right thing. Such function could be extended to include org-agenda information, TODO items, and many other things that would use AI to augment the existing functionality of Emacs and org-mode to tailor them to my needs.</p>\n","tags":["emacs","orgmode","ai","plaintext","productivity","tools","technology","gnu","opensource","gtd","calendar","agenda","llm","openai"]},{"title":"Stevenson vs. Harutyunyan","contentType":"notes","date":"2024-07-06 23:37","url":"/notes/stevenson-vs-harutyunyan/","content":"\n<p>Rd</p>\n<p>1</p>\n<p>2</p>\n<p>3</p>\n<p>4</p>\n<p>5</p>\n<p>6</p>\n<p>7</p>\n<p>8</p>\n<p>9</p>\n<p>10</p>\n<p>11</p>\n<p>12</p>\n<p>Stevenson</p>\n<p>9</p>\n<p>9</p>\n<p>9</p>\n<p>10</p>\n<p>10</p>\n<p>10</p>\n<p>10</p>\n<p>10</p>\n<p>9</p>\n<p>9</p>\n<p>10</p>\n<p>10</p>\n<p>Harutyunyan</p>\n<p>10</p>\n<p>10</p>\n<p>10</p>\n<p>9</p>\n<p>9</p>\n<p>9</p>\n<p>9</p>\n<p>9</p>\n<p>10</p>\n<p>10</p>\n<p>9</p>\n<p>9</p>\n<p>Shakur did enough not to lose. It was a boring fight, but he should've done enough to be up on the scorecards.</p>\n","tags":["boxing","stevensonharutyunyan","toprank","stevenson","harutyunyan"]},{"title":"Just added Fediverse Open Graph meta tag to my website","contentType":"notes","date":"2024-07-02 21:41","url":"/notes/added-fediverse-og-creator-tag/","content":"<p>I learned from <a href=\"https://matthiasott.com/notes/highlighting-blogging-on-mastodon\">Matthias Ott</a> that Mastodon created a new Open Graph meta tag which displays a direct link to the website owner's Fediverse (Mastodon, Pixelfed, Threads, etc...) profile as part of the URL preview card on the Mastodon web and mobile apps.</p>\n<p>I just added support for it on my website.</p>\n<p>All I had to do was add the following tag to my site.</p>\n<pre><code class=\"language-html\">&amp;lt;meta property=&amp;quot;fediverse:creator&amp;quot; content=&amp;quot;@lqdev@toot.lqdev.tech&amp;quot;&amp;gt;\n</code></pre>\n<p>I'm not running the nightly version of Mastodon on my instance, but if anyone on an instance where this is already supported like mastodon.social can verify and let me know it's working, it's much appreciated.</p>\n","tags":["mastodon","fediverse","blogging","indieweb","blogs","news","journalism","openweb","distributedweb","smallweb","personalweb","opengraph","html"]},{"title":"Teofimo vs. Claggett - June 29 2024","contentType":"notes","date":"2024-06-29 23:48","url":"/notes/teofimo-claggett-fight/","content":"\n<p>Didn't see Claggett's but Lopez ring walk was a little cringe.</p>\n\n<p>Round</p>\n<p>1</p>\n<p>2</p>\n<p>3</p>\n<p>4</p>\n<p>5</p>\n<p>6</p>\n<p>7</p>\n<p>8</p>\n<p>9</p>\n<p>10</p>\n<p>11</p>\n<p>12</p>\n<p>Lopez</p>\n<p>10</p>\n<p>10</p>\n<p>10</p>\n<p>10</p>\n<p>10</p>\n<p>10</p>\n<p>?</p>\n<p>10</p>\n<p>9</p>\n<p>10</p>\n<p>10</p>\n<p>10</p>\n<p>Claggett</p>\n<p>9</p>\n<p>9</p>\n<p>9</p>\n<p>9</p>\n<p>9</p>\n<p>9</p>\n<p>?</p>\n<p>9</p>\n<p>10</p>\n<p>9</p>\n<p>9</p>\n<p>9</p>\n\n<p>Claggett put more pressure but Lopez got in the better shots and looked in control even boxing off his back foot.</p>\n\n<p>Claggett still putting in a lot of pressure but Teofimo still looks in control.</p>\n\n<p>Teofimo still in control. Claggett has some damage to the face but still keeps going forward.</p>\n\n<p>Some nasty shots from Teofimo but Claggett still keeps coming forward.</p>\n\n<p>Solid back and forth</p>\n\n<p>Claggett is solid. Making Teofimo work and he's got some wear on his face.</p>\n\n<p>PC battery died 😃</p>\n\n<p>Big uppercuts from Teofimo but Claggett keeps coming forward.</p>\n\n<p>Claggett keeps coming forward and he landed some good shots.</p>\n\n<p>Teofimo stole the round at the end. Not like he needed to. He got the better shots. Sad to see because Claggett is putting the pressure but not doing as much damage.</p>\n\n<p>Much of the same. Claggett reguses to go away. Sadly, he hasn't done enough that if / when it goes the distance, he'll lose. He needs a knockout and it looks unlikely.</p>\n\n<p>Teofimo still looked fresh in this one. Claggett still kept the pressure though</p>\n\n<p>I'd be surprised if Lopez didn't win. Claggett showed a lot of heart and made Teofimo uncomfortable. Sadly it wasn't enough though.</p>\n","tags":["boxing","teofimoclaggett","teofimo","clagget","toprank"]},{"title":"Nasty KO - Ramirez vs. Benitez","contentType":"notes","date":"2024-06-29 23:23","url":"/notes/ramirez-benitez-fight/","content":"<p>Ramirez dominated most of the fight but that knockout was insane. Watching it live, it wasn't easy to see but in the replay, Ramirez exploded from the botttom and nearly took Benitez' head off. He wasn't out cold but he got his bell rung and just didn't make the count. Still, I hope he's okay. He took a beating the rest of the fight.</p>\n<p><a href=\"https://www.youtube.com/shorts/GO3ogPThnIY\" title=\"Ramirez vs. Benitez Knowckout\"><img src=\"https://i.ytimg.com/vi/GO3ogPThnIY/frame0.jpg\" class=\"img-fluid\" alt=\"Ramirez vs. Benitez Knowckout\" /></a></p>\n","tags":["boxing","teofimoclaggett","featherweight","ko","fighting","teofimo","claggett"]},{"title":"Crate Finds - May 2024","contentType":"notes","date":"2024-06-06 22:37","url":"/notes/crate-finds-may-2024/","content":"<p>Better late than never. Here's May's drop.</p>\n<p>Some of my favorites this month are:</p>\n<ul>\n<li>Journey of the Sorcerer by the Eagles</li>\n<li>Any of the songs by Piero Piccioni</li>\n<li>Any of the songs by Fishmans</li>\n</ul>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/0x0XmEW2RcXxYM0KXzlVk0?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"Website Post Statistics - June 2024","contentType":"notes","date":"2024-06-06 22:26","url":"/notes/post-statistics-june-2024/","content":"<p>6 months in and these are the main takeaways.</p>\n<ul>\n<li>At 51 notes, I'm 10 over last year's total notes.</li>\n<li>Responses are healthy. I'm closing in on 200. I wasn't as active the month of May so I'm still shy of last year's mark, but given it's still only halfway through the year, it seems like good progress so far.</li>\n<li>Most of my posts continue to be related to AI. While I can believe it, I also think I could be more specific and get more creative with my tags. Given the lead AI has though in terms of tag numbers, it'll be harder for those more fine-grained tags to come through.</li>\n<li>I won't use the word struggle, but I have paid less attention to long form posts. There's a few that I have planned for June, so that should help bump those numbers.</li>\n</ul>\n<p>Overall I'm happy with the progress and use I'm getting out of the website.</p>\n<p>Some things I'd like to work on are simplifying my authoring process. Although github.dev and Codespaces can be convenient, it's a hassle. Authoring content locally reduces some of that friction. However, I'm still bound to my laptop.</p>\n<p>As a Day One subscriber, I haven't maximized my subscription. This past week I've started to use it more for bookmarking articles I'm reading. Maybe that's a way that I can bookmark articles when on the go and when I'm ready to reshare them on the website, I can just copy from there.</p>\n<p>Something else I can use Day One for are to start longer form blog or note drafts. Then, when I'm ready to do more serious work and publish them, I can continue on my laptop.</p>\n","tags":["githubactions","cicd","blogging","stats","github"]},{"title":"Week of June 03, 2024 - Post Summary","contentType":"notes","date":"2024-06-02 20:22 -05:00","url":"/notes/2024-06-03-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/responses/my-permadomain-rscottjones\">My Permadomain</a></li>\n<li><a href=\"/responses/microblog-podcast-hosting\">Micro.blog - Podcast hosting for $5</a></li>\n<li><a href=\"/responses/repro-gpt-2-llm-c-90-min-20-dollars-karpathy\">Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20</a></li>\n<li><a href=\"/responses/21-years-of-wordpress\">21 Years of WordPress</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Week of May 26, 2024 - Post Summary","contentType":"notes","date":"2024-05-26 18:34 -05:00","url":"/notes/2024-05-26-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/false-alarm-link-in-bio-x-twitter\">False alarm - Link is in X bio</a></li>\n<li><a href=\"/notes/no-link-in-bio-x-twitter\">X (Twitter) - No more link in bio?</a></li>\n<li><a href=\"/notes/blogroll-discovery-implemented/\">Blogroll discovery enabled</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/rip-kabosu-doge\">RIP Kabosu</a></li>\n<li><a href=\"/responses/indieweb-next-steps-durnell\">The IndieWeb's Next Stage</a></li>\n<li><a href=\"/responses/daylight-dc-1-tablet-computer\">The Daylight DC1 is a $729 attempt to build a calmer computer</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"False alarm - Link is in X bio","contentType":"notes","date":"2024-05-26 17:00","url":"/notes/false-alarm-link-in-bio-x-twitter/","content":"<p>I may have spoken too early in my <a href=\"/notes/no-link-in-bio-x-twitter\">post about X not displaying bio links correctly</a>.</p>\n<p>Not sure if it was a glitch or what was going on, but I now see my domain correctly displayed as the bio link. All good.</p>\n<p>I'd be interested in knowing whether others have experienced something similar.</p>\n","tags":["x","twitter","website"]},{"title":"X (Twitter) - No more link in bio?","contentType":"notes","date":"2024-05-26 15:48","url":"/notes/no-link-in-bio-x-twitter/","content":"<p>While making some updates to my X profile, I noticed the link displayed on my bio isn't the same one I configured in the form. Instead it's a t.co shortened link. I get I'm just a free user on the platform but that seems like an annoying change. I haven't found much details about the change, but I'd hope paying subscribers get the &amp;quot;perk&amp;quot; of having control over how the link on their bio displays.</p>\n<p><img src=\"http://i0.kym-cdn.com/entries/icons/original/000/019/304/old.jpg\" class=\"img-fluid\" alt=\"Simpsons Old Man Yells at Cloud Meme\" /></p>\n","tags":["x","twitter","website"]},{"title":"Blogroll discovery enabled","contentType":"notes","date":"2024-05-25 22:44","url":"/notes/blogroll-discovery-implemented/","content":"<p>Just enabled discovery for my <a href=\"/collections/blogroll/\">blogroll</a> based on the <a href=\"https://opml.org/blogroll.opml\">OPML blogroll spec</a>. While technically not part of the spec, I also have similar collections for <a href=\"/collections/podroll/\">podcasts</a> and <a href=\"/collections/youtube/\">YouTube</a>. I've added those as well.</p>\n<pre><code class=\"language-html\">&amp;lt;head&amp;gt;\n    &amp;lt;!--...--&amp;gt;\n    &amp;lt;link rel=&amp;quot;blogroll&amp;quot; type=&amp;quot;text/xml&amp;quot; title=&amp;quot;Luis Quintanilla's Blogroll&amp;quot; href=&amp;quot;/collections/blogroll//index.opml&amp;quot;&amp;gt;\n    &amp;lt;link rel=&amp;quot;youtuberoll&amp;quot; type=&amp;quot;text/xml&amp;quot; title=&amp;quot;Luis Quintanilla's YouTube Roll&amp;quot; href=&amp;quot;/collections/youtube//index.opml&amp;quot;&amp;gt;\n    &amp;lt;link rel=&amp;quot;podroll&amp;quot; type=&amp;quot;text/xml&amp;quot; title=&amp;quot;Luis Quintanilla's Podroll&amp;quot; href=&amp;quot;/collections/podroll//index.opml&amp;quot;&amp;gt;\n    &amp;lt;!--...--&amp;gt;\n&amp;lt;/head&amp;gt;\n</code></pre>\n<p>youtuberoll doesn't really roll off the tongue, but I'm hesitant to call it videoroll or vidroll. My concerns mainly come from the fact that YouTube != video.</p>\n<p>Ideas welcome!</p>\n","tags":["blogroll","opml","rss","indieweb","openweb","socialweb","personalweb"]},{"title":"Fury vs. Usyk","contentType":"notes","date":"2024-05-18 20:09","url":"/notes/fury-usyk-1/","content":"<p>What a great fight.</p>\n<p>I think the judges did a great job. It was a close fight, but the knockdown won it for Usyk.</p>\n<p>The ref did a good job not stopping it in the 9th and giving Fury a chance to come back.</p>\n<p>I think Fury clowned around too much in the beginning and didn't hit the gas in the middle rounds letting Usyk come back in the later rounds. The clowning probably was masking because Usyk is most likely the best fighter he's fought in his career.</p>\n<p>Great job by both fighters coming back, Usyk in those middle rounds and Fury from the knockdock where it looked like he was done.</p>\n<p>Also, it's business and most likely both fighters carried each other rather than press the action because this leaves the door open for the rematch.</p>\n<p>I doubt Usyk does the trilogy with Anthony Joshua given the series is 2-0. The next fight for both will probably be the rematch.</p>\n","tags":["boxing","fury","usyk","heavyweight","championship"]},{"title":"Website Post Statistics - May 2024","contentType":"notes","date":"2024-05-05 20:31","url":"/notes/post-statistics-may-2024/","content":"<p>Still not mid-year and I'm making progress on publishing to the website. Some notable observations this month:</p>\n<ul>\n<li>At 43 notes, I'm already past the total number of notes posted in 2023. Weekly summary posts like <a href=\"/tags/cratefinds\">Create Finds</a> and <a href=\"/tags/weeklysummary\">Weekly Summary</a> continue to contribute towards this number.</li>\n<li>Despite a slow last week of April with no posts, I was still able to manage adding nearly 40 new posts. At 184, I'm closing in and expect to surpass last year's response posts within the next two months.</li>\n<li>I'm still working through how to go about publishing longer-form content to the blog. I'm still only at 3 posts. Given that I posted 8 last year, it's not an elusive target. Still, I haven't figured out what some good topics / experiments I'm doing today would be interesting to expand upon in the blog. I'm in no rush though. I'm enjoying the shorter form posts both notes and responses.</li>\n</ul>\n","tags":["githubactions","cicd","blogging","stats","github"]},{"title":"Crate Finds - April 2024","contentType":"notes","date":"2024-05-05 20:20","url":"/notes/crate-finds-april-2024/","content":"<p>April's drop. This month, I kept coming back to:</p>\n<ul>\n<li>Pon Pon by Khruangbin</li>\n<li>This Feeling I Get by Doug Shorts</li>\n</ul>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/3zEdflqIMbJxAR42iZjtdy?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"Week of May 06, 2024 - Post Summary","contentType":"notes","date":"2024-05-05 19:24 -05:00","url":"/notes/2024-05-06-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/crate-finds-april-2024\">Crate Finds - April 2024</a></li>\n<li><a href=\"/notes/2024-04-29-weekly-post-summary/\">Week of April 29, 2024 - Post Summary</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Week of April 29, 2024 - Post Summary","contentType":"notes","date":"2024-04-28 19:55 -05:00","url":"/notes/2024-04-29-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/gha-publishing-workflow-fixed\">GitHub Actions Publishing Workflow Fixed</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/pbs-retro-roku-channel\">'PBS Retro' is coming to Roku as a FAST channel</a></li>\n<li><a href=\"/responses/google-jax-penzai\">Google Penzai</a></li>\n<li><a href=\"/responses/dumbphone-boom-real\">The Dumbphone Boom Is Real</a></li>\n<li><a href=\"/responses/introducing-phi-3\">Introducing Phi-3</a></li>\n<li><a href=\"/responses/remembering-90s-video-store-plummer\">Remembering the 90's Video Store</a></li>\n<li><a href=\"/responses/new-kamasi-washington-album-fearless-movement\">New Album: Kamasi Washington - Fearless Movement</a></li>\n<li><a href=\"/responses/one-year-retrospect-lightphone-2-verge\">One year with the Light Phone 2</a></li>\n<li><a href=\"/responses/sammo-prompt-optimization-framework\">SAMMO: A general-purpose framework for prompt optimization</a></li>\n<li><a href=\"/responses/internet-used-to-be-fun-kwon\">The Internet Used To Be Fun</a></li>\n<li><a href=\"/responses/calculus-made-easy\">Calculus Made Easy</a></li>\n<li><a href=\"/responses/ghost-activitypub-integration-announcement\">Ghost is federating over ActivityPub</a></li>\n<li><a href=\"/responses/snowflake-arctic-enterprise-ai-llm\">Introducing Snowflake Arctic</a></li>\n<li><a href=\"/responses/grow-your-own-services\">Grow Your Own Services</a></li>\n<li><a href=\"/responses/rewild-the-internet-farrell-berjon\">We Need To Rewild The Internet</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"GitHub Actions Publishing Workflow Fixed","contentType":"notes","date":"2024-04-23 22:51","url":"/notes/gha-publishing-workflow-fixed/","content":"<p>The website had been out of comission for a few days because my credentials had expired. I could still add posts and make updates. However, none of them were published to Azure.</p>\n<p>Figuring out the right incantations wasn't straightforward, so <a href=\"/resources/wiki/github-actions-azure/\">saving the fix</a> for future reference.</p>\n","tags":["github","blogging","azure","githubactions","cicd"]},{"title":"Week of April 14, 2024 - Post Summary","contentType":"notes","date":"2024-04-14 14:23 -05:00","url":"/notes/2024-04-14-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/slm-too-big\">These models are too damn big!</a></li>\n<li><a href=\"/notes/enjoying-kexp-el-sonido\">Enjoying El Sonido from KEXP</a></li>\n<li><a href=\"/notes/better-offline-podroll\">New podcast (Better Offline) added to blogroll</a></li>\n<li><a href=\"/notes/2024-04-07-weekly-post-summary/\">Week of April 07, 2024 - Post Summary</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/llm-c-karpathy\">LLM training in simple, raw C/CUDA </a></li>\n<li><a href=\"/responses/google-deepmind-recurrent-gemma\">RecurrentGemma - Open weights language model from Google DeepMind, based on Griffin.</a></li>\n<li><a href=\"/responses/beeper-acquired-automattic\">Beeper was just acquired by Automattic</a></li>\n<li><a href=\"/responses/allen-ai-olmo-llm\">Hello OLMo: A truly open LLM</a></li>\n<li><a href=\"/responses/blogroll-molly-white/\">Molly White's Blogroll</a></li>\n<li><a href=\"/responses/aragog-advanced-rag-output-grading\">ARAGOG: Advanced RAG Output Grading</a></li>\n<li><a href=\"/responses/radio-bilingue-rock-alterlatino\">Rock AlterLatino from Radio Bilingue</a></li>\n<li><a href=\"/responses/far-out-guides-hiking-app\">Far Out Guides</a></li>\n<li><a href=\"/responses/proton-acquires-standard-notes\">Proton and Standard Notes are joining forces </a></li>\n<li><a href=\"/responses/decop-detecting-copyright-llm-training-data\">DE-COP: Detecting Copyrighted Content in Language Models Training Data</a></li>\n<li><a href=\"/responses/llm-text-to-sql-right-candidates\">Using LLM to select the right SQL Query from candidates</a></li>\n<li><a href=\"/responses/griffin-mix-linear-recurrence-local-attention-language-models\">Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"These models are too damn big!","contentType":"notes","date":"2024-04-08 23:43","url":"/notes/slm-too-big/","content":"<p>While the size of these smaller language models is significantly less than the trillion parameter models like GPT, they still take up a lot of storage space. Playing around with <a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\">Mistral 7B Instruct v0.2</a>, the safetensor files containing the weights take up roughly 15GB of space. I'm thinking of playing around with <a href=\"https://learn.microsoft.com/azure/storage/blobs/blobfuse2-what-is\">blobfuse</a> to mount a storage container to my local file system. That way, I'm only caching and accessing the models I'm working with at any given time.</p>\n","tags":["ai","slm","huggingface","llm","opensource"]},{"title":"Enjoying El Sonido from KEXP","contentType":"notes","date":"2024-04-08 23:36","url":"/notes/enjoying-kexp-el-sonido/","content":"<p>I just found <a href=\"https://www.kexp.org/shows/El-Sonido/\">El Sonido from KEXP</a>. Albina Cabrera is a great host. I've already found a few new songs and artists to listen to I otherwise wouldn't have been aware of or exposed to.</p>\n","tags":["radio","music","kexp","shows"]},{"title":"New podcast (Better Offline) added to blogroll","contentType":"notes","date":"2024-04-07 16:50","url":"/notes/better-offline-podroll/","content":"<p>While doing chores today, I put on one of the episodes from <a href=\"https://www.wheresyoured.at/about/\">Ed Zitron's</a> <a href=\"https://www.betteroffline.com/\">Better Offline podcast</a>. This resulted me in me basically binging the rest of the episodes on the backlog. Also, it's now made its way into the regular rotation with other podcasts and the <a href=\"/collections/podroll/\">podroll</a>.</p>\n","tags":["podcast","podroll","betteroffline"]},{"title":"Week of April 07, 2024 - Post Summary","contentType":"notes","date":"2024-04-07 16:01 -05:00","url":"/notes/2024-04-07-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/notebook-fantasy-chicano-batman-released\">New Album - Notebook Fantasy from Chicano Batman</a></li>\n<li><a href=\"/notes/better-offline-podroll\">New podcast added to blogroll</a></li>\n<li><a href=\"/notes/post-statistics-april-2024\">Website Post Statistics - April 2024</a></li>\n<li><a href=\"/notes/2024-04-01-weekly-post-summary/\">Week of April 1, 2024 - Post Summary</a></li>\n<li><a href=\"/notes/world-wide-whack-tierra-whack-review/\">Thoughts on World Wide Whack</a></li>\n<li><a href=\"/notes/crate-finds-march-2024\">Crate Finds - March 2024</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/llm-time-series-forecasting\">Large Language Models Are Zero-Shot Time Series Forecasters</a></li>\n<li><a href=\"/responses/google-podcasts-gone-youtube-cant-replace-it-verge\">Google Podcasts is gone — and so is my faith in Google</a></li>\n<li><a href=\"/responses/intel-ipex-llm\">IPEX-LLM</a></li>\n<li><a href=\"/responses/introducing-stable-audio-2-0\">Introducing Stable Audio 2.0</a></li>\n<li><a href=\"/responses/start-using-chatgpt-seamlessly-no-signup\">Start using ChatGPT instantly</a></li>\n<li><a href=\"/responses/introducing-cohere-comandr-plus\">Introducing Command R+: A Scalable LLM Built for Business</a></li>\n<li><a href=\"/responses/openai-introducing-fine-tuning-improvements\">OpenAI - Introducing improvements to the fine-tuning API and expanding our custom models program</a></li>\n<li><a href=\"/responses/announcing-dbrx-llm\">Announcing DBRX: A new standard for efficient open source LLMs</a></li>\n<li><a href=\"/responses/apple-realm-research-paper\">ReALM: Reference Resolution As Language Modeling</a></li>\n<li><a href=\"/responses/tmobile-sidekick-jump-button-mobile-productivity\">T-Mobile Sidekick’s Jump button made mobile multitasking easy</a></li>\n<li><a href=\"/responses/matrix-five-announced\">The Matrix is coming back for a fifth movie</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Thoughts on World Wide Whack","contentType":"notes","date":"2024-04-02 22:43","url":"/notes/world-wide-whack-tierra-whack-review/","content":"<p>I saw this come across my feed a few days ago but didn't get around to listening to it until today. I really enjoyed it.</p>\n<p>Some of my favorite tracks are:</p>\n<ul>\n<li>Chanel Pit</li>\n<li>Imaginary Friends</li>\n<li>Shower Song</li>\n</ul>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/album/128kgBRYqmfY4EjTOhiDWq?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["music","newrelease","tierrawhack","worldwidewhack"]},{"title":"Website Post Statistics - April 2024","contentType":"notes","date":"2024-04-01 23:20","url":"/notes/post-statistics-april-2024/","content":"<p>My <a href=\"/posts/website-metrics-github-actions/\">website stats script</a> seems to be running as expected.</p>\n<p>Some highlights from last month:</p>\n<ul>\n<li>With 33 notes posted in the microblog, I'm about 80% of the way to the number of total notes posted last year. These monthly summary posts, as well as my <a href=\"/notes/crate-finds-march-2024\">Crate Finds</a> and <a href=\"/notes/2024-04-01-weekly-post-summary/\">Weekly Summary</a> posts have somewhat contributed to this number but I like the pace I'm setting. Not even halfway through the year and it looks promising.</li>\n<li>While most of my responses are reshares (93), there's still a broader distribution across bookmarks and other type of reponses posts.</li>\n<li>AI continue to be the main topic I share posts around.</li>\n<li>The Verge is still one of the main sources I reshare. Personally, the redesign has contributed to the way I consume content from The Verge but also, the topics covered align with many of the things I'm interested in.</li>\n</ul>\n<p>I have yet to make progress on the long-form posts, but there are some experiments I've been tinkering with that will eventually make it on there.</p>\n","tags":["githubactions","cicd","blogging","stats","github"]},{"title":"Crate Finds - March 2024","contentType":"notes","date":"2024-04-01 23:17","url":"/notes/crate-finds-march-2024/","content":"<p>Similar to February, I didn't spend too much time listening this month so the list is somewhat short. I got a chance to listen to a few songs off Chicano Batman's new album, <a href=\"/notes/notebook-fantasy-chicano-batman-released\">Noteboook Fantasy</a> though. Here's the drop for March 2024:</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/2XvkXTxdUtKm5pRwp8dRlv?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"New Album - Notebook Fantasy from Chicano Batman","contentType":"notes","date":"2024-03-31 20:19 -05:00","url":"/notes/notebook-fantasy-chicano-batman-released/","content":"<p>I have to admit, I wasn't the biggest fan of the singles released before the album. Though I have to say, there's been a few times Era Primavera has been stuck in my head. Very catchy.</p>\n<p>I haven't listened to the whole thing, but so far I really like the tracks Notebook Fantasy, Parallels, and Beautiful Daughter.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/album/11oAb5VEhTqhiAyfs5AOQX?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["music","chicanobatman","newrelease"]},{"title":"Week of April 1, 2024 - Post Summary","contentType":"notes","date":"2024-03-31 19:30 -05:00","url":"/notes/2024-04-01-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/notebook-fantasy-chicano-batman-released\">New Album - Notebook Fantasy from Chicano Batman</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/announcing-dbrx-llm\">Announcing DBRX: A new standard for efficient open source LLMs</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Introducing the 100 Sways Project","contentType":"notes","date":"2024-03-24 16:33 -05:00","url":"/notes/introducing-100-sways-project/","content":"<p>Introducing the 100 Sways project.</p>\n<p>Over the next few months, I plan on creating artifacts like blog posts, newsletters, and websites using <a href=\"https://sway.cloud.microsoft/\">Microsoft Sway</a>.</p>\n<p>Bookmark the site <a href=\"http://100sways.lqdev.tech\">100sways.lqdev.tech</a> and follow along!</p>\n<p>Note: If you can't access the website, you can use the <a href=\"https://sway.cloud.microsoft/aC19McbR3xcNo6s8\">direct link (https://sway.cloud.microsoft/aC19McbR3xcNo6s8)</a>.</p>\n\n<p>This project is an attempt to show how you might use Sway as a publishing platform for content like newsletters, blog posts, and websites.</p>\n<p>This project was inspired by <a href=\"https://jamesg.blog/\">James'</a> <a href=\"https://jamesg.blog/2024/02/19/personal-website-ideas/\">100 things you can do on your personal website post</a>.</p>\n\n<ul>\n<li>Learn more about Microsoft Sway</li>\n<li>Show how you can make the most use out of apps and services already included with your Microsoft Account</li>\n<li>Show how easy it is to create and consume web content using Sway</li>\n</ul>\n","tags":["microsoft","sway","indieweb","blog","website","microblog","blogging","content","m365","office","internet","web","microsoftoffice","microsoft365"]},{"title":"Week of March 24, 2024 - Post Summary","contentType":"notes","date":"2024-03-24 13:05 -05:00","url":"/notes/2024-03-24-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/new-forums-page\">New Forums Page</a></li>\n<li><a href=\"/notes/2024-03-18-weekly-post-summary/\">Week of March 18, 2024 - Post Summary</a></li>\n<li><a href=\"/notes/surface-event-mar-21-24-event-blog\">New Era of Work - Windows / Surface Event Blog (March 21, 2024)</a></li>\n<li><a href=\"/notes/looking-for-discord-communities\">Looking for Discord server / community recommendations</a></li>\n<li><a href=\"/notes/ai-generated-blogroll-opml\">Use AI to generate a blogroll others can subscribe to</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/building-meta-genai-infrastructure\">Building Meta’s GenAI Infrastructure</a></li>\n<li><a href=\"/responses/fast-inner-product-dnn-accelerators\">Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators</a></li>\n<li><a href=\"/responses/diffusion-models-from-scratch-yuan\">Diffusion models from scratch, from a new theoretical perspective</a></li>\n<li><a href=\"/responses/spreadsheets-are-all-you-need\">Spreadsheets are all you need</a></li>\n<li><a href=\"/responses/introducing-stable-video-3d\">Introducing Stable Video 3D: Quality Novel View Synthesis and 3D Generation from Single Images</a></li>\n<li><a href=\"/responses/mamba-linear-time-sequence-selective-state-spaces\">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a></li>\n<li><a href=\"/responses/nvidia-blackwell-b200-gpu\">Nvidia reveals Blackwell B200 GPU</a></li>\n<li><a href=\"/responses/demistifying-embedding-spaces-llms\">Demystifying Embedding Spaces using Large Language Models</a></li>\n<li><a href=\"/responses/quanto-pytorch-quantization-toolkit\">Quanto: a PyTorch quantization toolkit </a></li>\n<li><a href=\"/responses/lavague-large-action-model\">LaVague - Large Action Model framework</a></li>\n<li><a href=\"/responses/enhace-rag-application-accuracy-knowledge-graphs\">Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs</a></li>\n<li><a href=\"/responses/algorithms-modern-hardware-book\">Algorithms for Modern Hardware</a></li>\n<li><a href=\"/responses/ollama-supports-amd-gpus\">Ollama now supports AMD graphics cards</a></li>\n<li><a href=\"/responses/diffusion-models-from-scratch-duan\">Diffusion Models From Scratch</a></li>\n<li><a href=\"/responses/machine-learning-for-games-course-huggingface\">Machine Learning for Games Course - HuggingFace</a></li>\n<li><a href=\"/responses/stability-ai-ceo-resigns\">Stability CEO Resigns</a></li>\n<li><a href=\"/responses/tokenizer-playground\">The Tokenizer Playground</a></li>\n<li><a href=\"/responses/common-corpus-llm-training-dataset\">Releasing Common Corpus: the largest public domain dataset for training LLMs</a></li>\n<li><a href=\"/responses/grok-1-open-release\">Grok-1</a></li>\n<li><a href=\"/responses/openai-transformer-debugger\">OpenAI Transformer Debugger</a></li>\n<li><a href=\"/responses/online-local-chronicle-searls\">The Online Local Chronicle</a></li>\n<li><a href=\"/responses/mm1-multimodal-llm-pretraining\">MM1: Methods, Analysis &amp;amp; Insights from Multimodal LLM Pre-training</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"New Forums Page","contentType":"notes","date":"2024-03-23 12:38","url":"/notes/new-forums-page/","content":"<p>Following the patterns of a blogroll and podroll, I decided to post a <a href=\"/collections/forums/\">forums page</a> to list various forums and communities I find interesting.</p>\n<p>I've also created an <a href=\"/collections/forums//index.opml\">OPML file</a> for it so you can subscribe to them in your RSS reader of choice.</p>\n<p>Discord is good for communities. For example on servers like Shudder there's watch parties and HuggingFace reading club gets together to discuss some of the latest papers. For events, this works really well because everyone can participate in real-time. However, I find that outside of events or general chat, it's difficult to follow the message stream. I like the forum format with topics / threads much better.</p>\n<p>If you have any forum recommendations, feel free to <a href=\"/contact\">send them my way</a>.</p>\n","tags":["forums","community","rss","web","openweb","internet","indieweb"]},{"title":"Use AI to generate a blogroll others can subscribe to","contentType":"notes","date":"2024-03-21 22:01","url":"/notes/ai-generated-blogroll-opml/","content":"<p>I've seen several posts about blogrolls recently across my feed. My own needs some updating, but you can find it <a href=\"/collections/blogroll/\">here</a>. I also <a href=\"/collections/podroll/\">created one for podcasts</a> which I call a podroll.</p>\n<p>I've also seen posts about making the indie web easier.</p>\n<p>With both of those in mind, that's how this post came to be.</p>\n<p>Let's say that you have a blogroll on your website which might just be a list of links to the respective RSS feeds.</p>\n<p>If someone wanted to subscribe to the feeds on your blogroll, they could just copy the links and add them to their RSS reader. However, as your blogroll grows, doing this in bulk can be tedious.</p>\n<p>One way you can make it easier is using OPML files. Many RSS readers support importing feeds using OPML files.</p>\n<p>You could go through the process of figuring out what OPML is, how you need to format the file, and then do the work of creating and populating the file. Or...you could use AI.</p>\n<p><img src=\"https://github.com/lqdev/luisquintanilla.me/assets/11130940/2a24fd44-0ae4-46a1-bd55-68988927a33e\" class=\"img-fluid\" alt=\"Copilot chat being asked to generate OPML file\" /></p>\n<p>In this example, I opened up Copilot, provided it a list of links to RSS feeds and asked it to generate an OPML file. You can take the outputs generated using AI, save them to a file, and add it to your website.</p>\n<p>Now people who want to subscribe to the feeds on your blogroll can just load the OPML file into their RSS reader and all the feeds will be automatically added.</p>\n","tags":["blogroll","indieweb","ai","copilot","rss","openweb"]},{"title":"New Era of Work - Windows / Surface Event Blog (March 21, 2024)","contentType":"notes","date":"2024-03-21 12:02","url":"/notes/surface-event-mar-21-24-event-blog/","content":"<p><strong>12:03</strong></p>\n<ul>\n<li>Copilot is making people more productive.</li>\n</ul>\n<p><strong>12:06</strong></p>\n<ul>\n<li>Copilot for Microsoft 365 integrated into Windows.</li>\n<li>Demo: After coming back from vacation, you want to catch up.\n<ul>\n<li>Ask copilot about latest project updates</li>\n<li>Ask copilot for summary of meeting you missed which generates a summary of the discussion and action items.</li>\n<li>Copilot then can help you tackle the action items like drafting and sending e-mails.</li>\n</ul>\n</li>\n</ul>\n<p><strong>12:09</strong></p>\n<ul>\n<li>Demo: Using Copilot to configure Windows settings</li>\n</ul>\n<p><strong>12:13</strong></p>\n<ul>\n<li>Demo: Using Windows App to manage Windows 365 cloud PCs</li>\n</ul>\n<p><strong>12:21</strong></p>\n<p>Surface Pro 10 and Surface Laptop 6 announced for business. Comes with latest Intel processors, Neural Processing Units (NPUs), and Copilot.</p>\n<p><strong>12:39</strong></p>\n<p>The demo showing a .NET development workflow on the Surface Laptop 6 was great. Usually that same workflow longer on my Lenovo P4.</p>\n","tags":["windows","microsoft","surface","copilot","ai","pc","hardware"]},{"title":"Looking for Discord server / community recommendations","contentType":"notes","date":"2024-03-19 22:53","url":"/notes/looking-for-discord-communities/","content":"<p>I finally decided to dust off my Discord. So far, I've joined several communities like:</p>\n<ul>\n<li>HuggingFace</li>\n<li>LangChain</li>\n<li>LlamaIndex</li>\n<li>Racket</li>\n<li>Virtual ML.NET</li>\n<li>Shudder</li>\n<li>ClubTwit</li>\n<li>...and many others</li>\n</ul>\n<p>I've noticed that server / community discovery needs improvement, so I'm asking for your recommendations!</p>\n<p>General topics I'm interested in:</p>\n<ul>\n<li>AI</li>\n<li>Machine Learning</li>\n<li>Open Source</li>\n<li>Software Development</li>\n<li>.NET</li>\n<li>F#</li>\n<li>Open Web</li>\n<li>IndieWeb</li>\n<li>Fediverse</li>\n<li>Lisp</li>\n<li>Tech</li>\n<li>Personal Finance</li>\n<li>Podcasts</li>\n<li>Books</li>\n<li>Travel</li>\n<li>Retro Tech</li>\n<li>Retro Gaming</li>\n</ul>\n<p>E-mail me your recommendations at contact@lqdev.me or send me a Webmention!</p>\n<p>Thanks in advance.</p>\n","tags":["discord","community","recommendations","lazyweb"]},{"title":"Week of March 18, 2024 - Post Summary","contentType":"notes","date":"2024-03-17 20:43 -05:00","url":"/notes/2024-03-18-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/notes/checking-in-strangers-luggage\">When should I check in stranger's luggage?</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/building-meta-genai-infrastructure\">Building Meta’s GenAI Infrastructure</a></li>\n<li><a href=\"/responses/fast-inner-product-dnn-accelerators\">Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators</a></li>\n<li><a href=\"/responses/diffusion-models-from-scratch-yuan\">Diffusion models from scratch, from a new theoretical perspective</a></li>\n<li><a href=\"/responses/spreadsheets-are-all-you-need\">Spreadsheets are all you need</a></li>\n<li><a href=\"/responses/introducing-protonmail-desktop-app\">Introducing the Proton Mail desktop app </a></li>\n<li><a href=\"/responses/learned-from-looking-900-oss-ai-tools-huyen\">What I learned from looking at 900 most popular open source AI tools</a></li>\n<li><a href=\"/responses/lavague-large-action-model\">LaVague - Large Action Model framework</a></li>\n<li><a href=\"/responses/enhace-rag-application-accuracy-knowledge-graphs\">Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs</a></li>\n<li><a href=\"/responses/algorithms-modern-hardware-book\">Algorithms for Modern Hardware</a></li>\n<li><a href=\"/responses/ollama-supports-amd-gpus\">Ollama now supports AMD graphics cards</a></li>\n<li><a href=\"/responses/diffusion-models-from-scratch-duan\">Diffusion Models From Scratch</a></li>\n<li><a href=\"/responses/apple-vision-pro-barra\">Apple Vision Pro Perspectives - Hugo Barra</a></li>\n<li><a href=\"/responses/grok-1-open-release\">Grok-1</a></li>\n<li><a href=\"/responses/openai-transformer-debugger\">OpenAI Transformer Debugger</a></li>\n<li><a href=\"/responses/bluesky-stackable-approach-moderation\">Bluesky’s Stackable Approach to Moderation</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"When should I check in stranger's luggage?","contentType":"notes","date":"2024-03-11 17:57","url":"/notes/checking-in-strangers-luggage/","content":"<p>I was sitting at the airport waiting to board my flight and over the PA system, the usual &amp;quot;see something, say something&amp;quot; security announcements were going off.</p>\n<p>Although I've heard these many times before, this time for some reason there's one that caught my attention and got me thinking. I don't remember the exact words, but basically the gist of it said something like, &amp;quot;If anyone asks you to check in baggage other than your own, do not accept it&amp;quot;.</p>\n<p>I barely even want to deal with my own bags. Why would I want to deal with someone else's bag?</p>\n<p>I've been trying to visualize scenarios where:</p>\n<ol>\n<li>Someone would come up to you and ask you to check in THEIR luggage</li>\n<li>You would accept</li>\n</ol>\n<p>Maybe in scenarios where you're traveling with friends or family? In the case of family, maybe someone doesn't have a carry-on and if there's an extra bag that's small enough to be considered carry-on they'll &amp;quot;claim&amp;quot; it as their carry-on to not pay the extra fees. Still though, that's not checking it in. I'm just having a hard time seeing it.</p>\n<p>Putting this out there ahead of time - no, stranger, I will not check in your bag.</p>\n","tags":["question","airport","security","fun"]},{"title":"Week of March 10, 2024 - Post Summary","contentType":"notes","date":"2024-03-10 14:26 -05:00","url":"/notes/2024-03-10-weekly-post-summary/","content":"\n<ul>\n<li><a href=\"/posts/install-ollama-vscode-devcontainer\">Configure Ollama on Dev Containers and VS Code</a></li>\n<li><a href=\"/posts/getting-started-ollama-windows\">Getting started with Ollama on Windows</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/notes/crate-finds-february-2024\">Crate Finds - February 2024</a></li>\n</ul>\n\n<ul>\n<li><a href=\"/responses/windows-subsystem-for-android-end-of-support-2025/\">Microsoft to end its Android apps on Windows 11 subsystem in 2025</a></li>\n<li><a href=\"/responses/surprising-connection-after-hours-work-productivity-slack\">The surprising connection between after-hours work and decreased productivity</a></li>\n<li><a href=\"/responses/stable-diffusion-3-research-paper\">Stable Diffusion 3: Research Paper</a></li>\n<li><a href=\"/responses/wix-website-builder-ai-chatbot/\">Wix’s new AI chatbot builds websites in seconds based on prompts</a></li>\n<li><a href=\"/responses/apple-podcasts-transcript-17-4\">Apple Podcasts now includes transcripts</a></li>\n<li><a href=\"/responses/year-of-linux-desktop-4-percent/\">5 reasons why desktop Linux is finally growing in popularity</a></li>\n<li><a href=\"/responses/levels-of-complexity-rag-applications\">Levels of Complexity: RAG Applications</a></li>\n<li><a href=\"/responses/training-llms-ground-up-wilderness-startup\">Training great LLMs entirely from ground up in the wilderness as a startup</a></li>\n<li><a href=\"/responses/gemma-pytorch\">Gemma PyTorch</a></li>\n<li><a href=\"/responses/decoder-nilay-patel-why-websites-are-the-future/\">Guest host Hank Green makes Nilay Patel explain why websites have a future</a></li>\n<li><a href=\"/responses/inflection-2-5\">Inflection-2.5: meet the world's best personal AI</a></li>\n<li><a href=\"/responses/datalog-the-missing-graph-data-type-already-exists\">The missing graph datatype already exists. It was invented in the '70s</a></li>\n<li><a href=\"/responses/todays-smart-homes-hopes-realities-verge\">Today’s smart homes: the hopes and the realities</a></li>\n<li><a href=\"/responses/announcing-claude-3-model-family\">Introducing the next generation of Claude</a></li>\n<li><a href=\"/responses/answer-ai-train-70b-llm-at-home\">You can now train a 70b language model at home</a></li>\n</ul>\n","tags":["weeklysummary","blogging","website","indieweb"]},{"title":"Crate Finds - February 2024","contentType":"notes","date":"2024-03-03 00:09","url":"/notes/crate-finds-february-2024/","content":"<p>Short month and not as much time spent listening to music. As a result, this month's list is a short one. Here's the drop for February 2024:</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/1yzC774efZbULAnDyU1GFe\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"Book Review - Agency","contentType":"notes","date":"2024-03-02 23:44","url":"/notes/agency-gibson-review/","content":"<p>Just finished reading Agency by William Gibson. I was not a big fan. I posted my review with more details.</p>\n<p><a href=\"/reviews/agency-gibson\">Agency Review</a></p>\n","tags":["book","review","agency","scifi","future","nanotech","ai","gibson"]},{"title":"Website Post Statistics - March 2024","contentType":"notes","date":"2024-03-02 15:57","url":"/notes/post-statistics-march-2024/","content":"<p>Another successful run of my <a href=\"/posts/website-metrics-github-actions/\">monthly script to track post statistics</a>.</p>\n<p>Some highlights:</p>\n<ul>\n<li>Over 100 responses through the month of February. That's basically halfway through the total number of responses I posted in 2023.</li>\n<li>Almost 20 notes. Similar to responses, I'm halfway through the total number of notes posted last year.</li>\n</ul>\n<p>I've never done <a href=\"https://100daystooffload.com/\">100 days to offload</a>. Unintentionally though, between notes and responses, I'm well past it.</p>\n<p>When it comes to more long-form posts, I haven't posted as many. Still only at 1 for the year. However, there's a few projects I'm looking to undertake that involve some of my publishing practices and AI. That should result in a healthy number of longer blog posts.</p>\n","tags":["githubactions","cicd","blogging","stats","github"]},{"title":"New Single - Era Primavera (Chicano Batman)","contentType":"notes","date":"2024-02-28 22:23","url":"/notes/era-primavera-chicano-batman/","content":"<p>New song from Chicano Batman's latest album, <a href=\"https://chicanobatman.bandcamp.com/album/notebook-fantasy\">Notebook Fantasy</a>.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/78uQdQG1eaqC78z8g5Lecj\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["music","newmusic","chicanobatman"]},{"title":"New Single - May Ninth (Khruangbin)","contentType":"notes","date":"2024-02-25 16:36","url":"/notes/may-ninth-khruangbin/","content":"<p>Another banger ahead of the new album, <a href=\"https://khruangbin.bandcamp.com/album/a-la-sala\">A La Sala</a>.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/2145px2VY14o2Pgr0OIttX\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["music","newmusic","khruangbin"]},{"title":"Now where will I get my RSS fix?","contentType":"notes","date":"2024-02-25 14:18","url":"/notes/newsblur-maintenace-mode-022024/","content":"<p><img src=\"https://github.com/lqdev/luisquintanilla.me/assets/11130940/b50ebeaa-c38c-4c58-b70f-fb9b9a26a017\" class=\"img-fluid\" alt=\"Screenshot of message informing users NewsBlur is down for maintenance\" /></p>\n<p>Good thing I have elfeed as my backup so I can use that in the meantime.</p>\n","tags":["newsblur","rss","maintenance","emacs","elfeed","news"]},{"title":"Vampire's Kiss (1988)","contentType":"notes","date":"2024-02-24 22:04","url":"/notes/vampires-kiss-movie/","content":"<p><a href=\"https://www.youtube.com/watch?v=PnoSxO_2ghQ\" title=\"Vampire's Kiss Movie Trailer\"><img src=\"http://img.youtube.com/vi/PnoSxO_2ghQ/0.jpg\" class=\"img-fluid\" alt=\"Vampire's Kiss Movie Trailer\" /></a></p>\n<p>I have no idea what I just watched, but I liked it. I thought Nicolas Cage making weird movies was a relatively recent thing, but I guess it goes way back to the 80s. I kept getting thrown off by his accent, which I only noticed about a quarter into the movie and even later into the movie sometimes it wasn't there. His descent and transformation into &amp;quot;Nosferatu&amp;quot; was interesting to watch. Mostly funny, but sometimes just weird and disturbing.</p>\n<p>If you're interested, it's currently <a href=\"https://tubitv.com/movies/674418/vampire-s-kiss\">streaming for free on Tubi</a></p>\n","tags":["movie","80s","horror","vampire"]},{"title":"Website Post Statistics - February 2024","contentType":"notes","date":"2024-02-03 12:29","url":"/notes/post-statistics-february-2024/","content":"<p>Success!</p>\n<p>At the end of last year, I <a href=\"/posts/website-metrics-github-actions/\">created an automated script to tally some basic stats about my posts</a> on this site.</p>\n<p>Last month, due to user error, it <a href=\"/notes/post-statistics-gh-actions-working-almost\">triggered but failed to run susccessfully</a>.</p>\n<p>This month though, it ran as expected!</p>\n<p><img src=\"https://github.com/lqdev/luisquintanilla.me/assets/11130940/888121b7-5e0d-45ca-a688-b46039bdb45e\" class=\"img-fluid\" alt=\"Image of Successful GitHub Actions Workflow Run\" /></p>\n<p>Some highlights:</p>\n<ul>\n<li>I'm posting more notes (short micro-blog style content). Through January, I'm already over 30% of total notes posted last year which is what I'm working towards.</li>\n<li>I've already posted 50 response posts through January. This is already over 20% of total responses published last year (223)!</li>\n<li>I'm resharing more than bookmarking. Still figuring that one out but I think this is a good start.</li>\n</ul>\n","tags":["githubactions","cicd","blogging","stats"]},{"title":"Down the weird web","contentType":"notes","date":"2024-02-03 12:15","url":"/notes/down-the-weird-web/","content":"<p>Not sure how these videos ended up showing up on my feed but I'm glad they did.</p>\n<p>They're weird, creepy, and fun all rolled into one.</p>\n<p>I'm surprised that they have such low views / subscribers.</p>\n<p>Check them out on YouTube.</p>\n<ul>\n<li><a href=\"https://www.youtube.com/@thearchiveinbetween\">The Archive in Between</a></li>\n<li><a href=\"https://www.youtube.com/@alternatebazaarart\">Alternate Bazaar Art</a></li>\n</ul>\n\n<p><a href=\"https://www.youtube.com/watch?v=BYkMjaxxl_o\" title=\"Welcome to Archives In Between YouTube Video\"><img src=\"http://img.youtube.com/vi/BYkMjaxxl_o/0.jpg\" class=\"img-fluid\" alt=\"Welcome to Archives In Between YouTube Video\" /></a></p>\n<p><a href=\"https://www.youtube.com/watch?v=iHZWbDgw0Vk\" title=\"Fictional promo YouTube video for Seeing Stones National Monument from Archives in Between\"><img src=\"http://img.youtube.com/vi/iHZWbDgw0Vk/0.jpg\" class=\"img-fluid\" alt=\"Fictional promo YouTube video for Seeing Stones National Monument from Archives in Between\" /></a></p>\n\n<p><a href=\"https://www.youtube.com/watch?v=yx9Gdjrm6WA\" title=\"YouTube Short video Blood in the Pizza made with digital art\"><img src=\"http://img.youtube.com/vi/yx9Gdjrm6WA/0.jpg\" class=\"img-fluid\" alt=\"YouTube Short video Blood in the Pizza made with digital art\" /></a></p>\n","tags":["alternatebazaarart","archiveinbetween","art","ai","digital","web","internet","video","fiction"]},{"title":"Crate Finds - January 2024","contentType":"notes","date":"2024-01-31 19:53","url":"/notes/crate-finds-january-2024/","content":"<p>January 2024 drop of Crate Finds. Ones I kept coming back to:</p>\n<ul>\n<li>Where Are You by 54 Ultra</li>\n<li>Language of Love by Jason Joshua</li>\n<li>A Love International by Khruangbin</li>\n<li>Dreameater by Healing Gems</li>\n<li>La Isla En El Sol, Pt. 2 by Real Cumbia Activa RCA</li>\n<li>Portrait of a Time by Peter Cat Recording Co.</li>\n</ul>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/1gENP8zxQPp1p7GFrRUG7Y\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"Notebook Fantasy - New Chicano Batman out March 29, 2024","contentType":"notes","date":"2024-01-24 20:23","url":"/notes/notebook-fantasy-chicano-batman/","content":"<p>New Chicano Batman album!</p>\n<p><a href=\"https://chicanobatman.bandcamp.com/album/notebook-fantasy\">Notebook Fantasy</a>.</p>\n<p>It's unfortunate Gabriel isn't part of this project but looking forward to the new sound tour.</p>\n","tags":["music","chicanobatman","newmusic","album","newalbum"]},{"title":"Private social media?","contentType":"notes","date":"2024-01-22 21:40","url":"/notes/private-social-media/","content":"<p>Brain dumping a bunch of half-formed thoughts.</p>\n<p>Over the past few days, I've been thinking about different forms of sharing content. With the <a href=\"https://www.nytimes.com/2023/04/19/technology/personaltech/tiktok-twitter-facebook-social.html\">shifts in social media towards smaller, more private communities</a>, is it possible to get the best of feeds for sharing updates and information but in a smaller more intimate way?</p>\n<p>Here's a few of the options I came up with:</p>\n\n<p>Examples: <a href=\"https://dayoneapp.com/guides/tips-and-tutorials/shared-journals/\">Day One Shared Journals</a></p>\n<ul>\n<li>Pros\n<ul>\n<li>Made for an audience of one, but other can collaborate on it.</li>\n<li>Depending on the interface, looking back at specific days and events that took place is easy to look back on.</li>\n<li>May be able to include different forms of media (i.e. audio, video, photos)</li>\n</ul>\n</li>\n<li>Cons\n<ul>\n<li>No individual profiles</li>\n<li>May require everyone to use the same app / service</li>\n</ul>\n</li>\n</ul>\n\n<p>Examples: Newsletters, Mailing lists</p>\n<ul>\n<li>Pros\n<ul>\n<li>Almost everyone has an e-mail address.</li>\n<li>Built on standard protocols.</li>\n<li>Producers can use any app they'd like to compose e-mails.</li>\n<li>Consumers can use any app they'd like to read e-mails.</li>\n<li>Could use any app / client / service-provider they'd like. (Federated by default)</li>\n</ul>\n</li>\n<li>Cons\n<ul>\n<li>Content rendering may differ based on app.</li>\n<li>Could add to the noise already present in your e-mail inbox.</li>\n<li>No individual accounts / feeds.</li>\n</ul>\n</li>\n</ul>\n\n<p>Examples: Discord, Matrix, IRC, Slack, Signal</p>\n<ul>\n<li>Pros\n<ul>\n<li>Sharing could be done via SMS (though this would limit the content you share).</li>\n<li>You could form various sub-groups to sub</li>\n</ul>\n</li>\n<li>Cons\n<ul>\n<li>May require everyone to use the same app / service.</li>\n<li>May not have a way to view individual profiles or feeds.</li>\n<li>Because conversations are not often grouped by topics, it might be difficult to find conversations, especially older conversations.</li>\n</ul>\n</li>\n</ul>\n\n<p>Examples: <a href=\"https://wordpress.com/go/tutorials/private-blog-wordpress-com/\">Private WordPress Blog</a>, <a href=\"https://havenweb.org/\">Haven</a></p>\n<ul>\n<li>Pros\n<ul>\n<li>Produces can use any tool / service to author and publish posts.</li>\n<li>Consumers can access via any browser of their choice.</li>\n</ul>\n</li>\n<li>Cons\n<ul>\n<li>May have to host your own service.</li>\n<li>Consumers may have to create accounts and passwords for each of the private blogs they follow.</li>\n</ul>\n</li>\n</ul>\n\n<p>Examples: Mastodon, PixelFed, PeerTube</p>\n<ul>\n<li>Pros\n<ul>\n<li>Familar social media interface.</li>\n<li>More availability of apps and services that form part of the Fediverse make it easier for producers and consumers to use what makes the most sense for them.</li>\n</ul>\n</li>\n<li>Cons\n<ul>\n<li>Limited to the format supported by the app / service (i.e. Pixelfed is for images / video, PeerTube for video, etc.).</li>\n<li>May require you to host your own service.</li>\n</ul>\n</li>\n</ul>\n\n<p>Examples: <a href=\"https://scuttlebutt.nz/\">ScuttleButt</a></p>\n<ul>\n<li>Pros\n<ul>\n<li>Could be made for an audience of one but shared with other peers.</li>\n<li>Decentralized by default.</li>\n<li>Access individual feeds.</li>\n</ul>\n</li>\n<li>Cons\n<ul>\n<li>Similar to the Fediverse, being exposed to new protocols and technical terms may make onboarding and adoption challenging.</li>\n<li>May require you to host your own service.</li>\n<li>Protocols haven't been widely adopted so users are limited in the apps / services they can use for this.</li>\n</ul>\n</li>\n</ul>\n\n<p>Examples: <a href=\"https://discourse.nixos.org/\">NixOS Forum</a>, <a href=\"https://lemmy.world/\">Lemmy</a></p>\n<ul>\n<li>Pros\n<ul>\n<li>Could create threads and conversations on various topics</li>\n<li>Easy to search and tag content</li>\n</ul>\n</li>\n<li>Cons\n<ul>\n<li>Someone would have to host a server</li>\n<li>Might not work well for small groups</li>\n</ul>\n</li>\n</ul>\n\n<p>This is a variation on the private blog. Except instead of having a website that displays the posts, you just have an RSS feed.</p>\n<p>Examples: <a href=\"https://daverupert.com/2018/01/welcome-to-rss-club/\">RSS-Only Club</a></p>\n<ul>\n<li>Pros\n<ul>\n<li>Producers can use any software they'd like to create and publish their RSS feed</li>\n<li>Consumers can use any feed reader to access and read content</li>\n<li>Everyone could have their own feed, so effectively they'd be like profiles</li>\n</ul>\n</li>\n<li>Cons\n<ul>\n<li>You have to figure out how to produce and host the RSS feed</li>\n<li>Can't really control who sees this since the feed is public. Maybe private feeds could help here.</li>\n<li>Limited interaction. Users can't comment or reply to items in a feed.</li>\n</ul>\n</li>\n</ul>\n<p>Note that this is not an exhaustive list of pros and cons, just a few that I thought of. I persoanlly would skew towards e-mail and messaging apps. The main reason at least with e-mail is that everyone is familiar with it and most likely won't have to create separate accounts on various silos to communicate and engage with others. With apps like <a href=\"https://delta.chat/\">DeltaChat</a> you could actualy combine them and get the best of both.</p>\n<p>What are your thoughts?</p>\n","tags":["indieweb","website","community","private","privacy","blog","protocols","rss","forums","journal","e-mail","fediverse","messaging"]},{"title":"A La Sala by Khruangbin out April 5, 2024","contentType":"notes","date":"2024-01-20 21:07","url":"/notes/a-la-sala-khruangbin/","content":"<p>The first half of 2024 will be good for new music. <a href=\"/responses/chicano-batman-fly-01-2024\">Chicano Batman is releasing something</a> in the next few days. Last night I found out Khruangbin released a new single, Love International, and has a <a href=\"https://khruangbin.bandcamp.com/album/a-la-sala\">new album coming out called A La Sala on April 5th</a>. I can't wait!</p>\n","tags":["music","newmusic","khruangbin","indie","album"]},{"title":"Mindcage","contentType":"notes","date":"2024-01-18 21:42","url":"/notes/mindcage-movie/","content":"<p>I didn't know Martin Lawrence was still making movies. I knew about Bad Boys but that was almost 4 years ago. Just watched <a href=\"https://www.rottentomatoes.com/m/mindcage\">Mindcage</a>, one of his latest ones. It was alright. The story is similar to Silence of the Lambs. Detectives seeking help from a manipulative serial killer to catch another killer on the loose. The twist at the end was interesting and not what I was expecting.</p>\n","tags":["movie","thriller","mystery"]},{"title":"Subscribed to 1042 feeds on NewsBlur","contentType":"notes","date":"2024-01-17 21:06","url":"/notes/subscribed-to-1042-feeds-newsblur/","content":"<p>I use <a href=\"https://www.newsblur.com/\">NewsBlur</a> as my feed reader. In it, I subscribe to:</p>\n<ul>\n<li>newsletters</li>\n<li>personal blogs</li>\n<li>sports blogs</li>\n<li>news publications</li>\n<li>GitHub releases</li>\n<li>Mastodon feeds and tags</li>\n<li>Bluesky feeds</li>\n<li>StackOverflow tags</li>\n<li>subreddits</li>\n<li>podcasts</li>\n<li>severe weather alerts</li>\n<li>YouTube channels</li>\n<li>PeerTube instances and channels</li>\n<li>Lemmy instances and sublemmys? (I don't know the Lemmy lingo yet)</li>\n<li>Forums</li>\n</ul>\n<p>Basically, if it has an RSS feed, I subscribe to it there.</p>\n<p>Today I noticed I'm subscribed over 1000 feeds (1042 to be exact)!</p>\n<p><img src=\"https://media.giphy.com/media/YmQLj2KxaNz58g7Ofg/giphy.gif\" class=\"img-fluid\" alt=\"GIF Matthew McCounaghey on Wolf of Wall Street with a caption that says &amp;quot;You gotta pump those up, those are rookie numbers in this racket&amp;quot;\" /></p>\n<p>Over 300 of them are personal blogs. This has taken a few years to curate. I remember my reader being empty when I first started. As I've read <a href=\"https://peopleandblogs.com/\">newsletters and personal blogs</a>, I've followed things they've linked to which is where I find even more feeds to subscribe to, especially when <a href=\"https://rknight.me/blog/please-expose-your-rss/\">they're easy to find</a>.</p>\n<p>There's some pruning that needs to happen at this point whether that's because of broken links or I just don't read those publications often. However, I find myself usually <a href=\"/notes/simplifying-feed-publishing-workflow\">checking in on personal blogs and newsletters on a daily basis</a> so I think many of those will stay.</p>\n","tags":["rss","newsblur","indieweb","feeds"]},{"title":"I need to optimize my response publishing workflow","contentType":"notes","date":"2024-01-17 20:47","url":"/notes/simplifying-feed-publishing-workflow/","content":"<p>I like to use my <a href=\"/feed/responses\">response feed</a> to bookmark and reshare interesting content I find online. Typically my workflow looks like the following:</p>\n<pre><code class=\"language-mermaid\">flowchart TD\n  A[&amp;quot;Scroll through feed reader&amp;quot;] --&amp;gt; B[&amp;quot;Open articles I want to read in a new tab&amp;quot;]\n  B --&amp;gt; C[&amp;quot;Read articles&amp;quot;]\n  C --&amp;gt; D[&amp;quot;Create a response file to publish on my website&amp;quot;]\n  D --&amp;gt; E[&amp;quot;Capture interesting information from the article in the response file&amp;quot;]\n  E --&amp;gt; F[&amp;quot;Publish the response&amp;quot;]\n</code></pre>\n<p>The bottleneck I usually find is in the middle steps of creating a response file and capturing the relevant content. This usually happens as I'm reading the article. For a single file, it's not a big deal. Doing it for several can be time consuming, especially with the <a href=\"/notes/subscribed-to-1042-feeds-newsblur\">number of feeds I subscribe to</a>.</p>\n<p>There's a few things I think could help here:</p>\n<ol>\n<li>Create one response file and do a daily link dump.</li>\n<li>Create a browser extension or system where I can:\n<ol>\n<li>Highlight text or content on a page</li>\n<li>Right-click or use a keyboard shortcut to create an entry for the content I highlighted</li>\n<li>Create an entry in a database somewhere which captures the URL of the page I'm looking at.</li>\n<li>I enter a few more details like the file name I want to use and response type</li>\n<li>When I'm done with my session, I can publish. This will use the database content to compose the content for my response and create a PR</li>\n</ol>\n</li>\n</ol>\n<p>The first option is simple but I don't like it because although most of the content I come across is tech related, I prefer having individual articles I can link to with their own tags to make it easier to build relationships.</p>\n<p>The second option sounds complex, but it's basically a web highlighting tool. I know there's a few out there already so maybe worth checking out.</p>\n<p>I'd prefer to build my own to tailor it specifically to what I want just like my website generator. However, it would mean that I'd have to spend some time thinking about how to implement it.</p>\n<p>It'd be a fun project though. Maybe worthwhile experimenting with building an AI agent to help me use the content I captured to nicely format the response.</p>\n","tags":["workflow","publishing","blog","personalknowledgemanagement","pkm"]},{"title":"Building up the reading backlog","contentType":"notes","date":"2024-01-16 20:36","url":"/notes/reading-list-january-2024/","content":"<p>Lately I've been using Spotify to listen to audiobooks since it's included as part of their Premium tier. That's how I started listening to <a href=\"/reviews/making-it-in-america\">Making It In America</a>. What I didn't know was that there's a limit to the number of hours you can listen in a given month. I only got as far as chapter 2 and now I have to wait 7 days for my listening hours to be replenished for the month.</p>\n<p>As a result, I started building the reading backlog at the local library which is now at an unreasonable 164 books that I probably won't get to. It got that way because I added many of the Best of 2023 recommendations from my local library to my wishlist. I haven't added many of those to my <a href=\"/library\">library page</a> because I don't want to go through the process of manually adding them. I should look into whether there's an automated way of getting that information via APIs or batch download.</p>\n<p>Last night I started reading <a href=\"/reviews/how-to-sell-a-haunted-house\">How To Sell A Haunted House</a> and I'm enjoying it so far. The next one on deck is <a href=\"/reviews/a-city-on-mars\">A City on Mars</a>.</p>\n","tags":["books","reading","library"]},{"title":"Gravatar - Your digital profile beyond blog comments","contentType":"notes","date":"2024-01-15 11:18","url":"/notes/gravatar-digital-profile-beyond-blog-comments/","content":"<p>I had known about <a href=\"https://gravatar.com/\">Gravatar</a> since its early days. I was under the (incorrect) impression that it could only be used to comment on blogs. I thought when you created an account, you added your name, e-mail, and image as part of your profile. Then, when you wanted to comment on blogs, instead of creating individual accounts or profiles, you could just reuse your Gravatar profile. That's partially true. However, I'm happy to report I was wrong and there's so much more.</p>\n<p>While commenting on <a href=\"/responses/blog-posts-can-be-any-length-matt\">Matt Mullenweg's Birthday Gift post</a>, I didn't use a Gravatar profile. This prompted Matt to kindly respond via e-mail and suggest I check out Gravatar. I put it off for a few days. Eventually, I decided to check it out and I'm glad I did. Thanks <a href=\"https://ma.tt\">Matt</a>!</p>\n<p>The signup process was simple. All I had to do was enter my e-mail address. This immediately sent me a confirmation e-mail which I then used to proceed with onboarding.</p>\n<p>Here's where things got interesting.</p>\n<p>You can add all the things I was already aware of (name, e-mail, photo). However, you can also do much more than that! You can add your own verified accounts, links, contact information, and payments.</p>\n<p>As of the time of this writing these options include:</p>\n<ul>\n<li>Accounts\n<ul>\n<li>WordPress</li>\n<li>Instagram</li>\n<li>X / Twitter</li>\n<li>TikTok</li>\n<li>Tumblr</li>\n<li>Mastodon</li>\n<li>GitHub</li>\n<li>Twitch</li>\n<li>Fediverse</li>\n<li>Stackoverflow</li>\n<li>Calendly</li>\n<li>Vimeo</li>\n<li>TripIt</li>\n<li>Foursquare</li>\n</ul>\n</li>\n<li>Contact info\n<ul>\n<li>E-mail</li>\n<li>Phone number (home, work, mobile)</li>\n<li>Contact form</li>\n<li>Calendar</li>\n</ul>\n</li>\n<li>Payments\n<ul>\n<li>PayPal</li>\n<li>Patron</li>\n<li>Venmo</li>\n<li>Crypto\n<ul>\n<li>Bitcoin</li>\n<li>Litecoin</li>\n<li>Dogecoin</li>\n<li>Ethereum</li>\n<li>XRP</li>\n<li>Cardano</li>\n<li>Custom</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>In addition to those integrations, you can also provide as many of your own custom links as you'd like. In my case, I've chosen to link to my <a href=\"/\">website</a> and <a href=\"/feed\">RSS feeds</a>. The options are endless though.</p>\n<p>I don't have a <a href=\"https://linktr.ee/\">Linktree</a> profile so I can't objectively say how Gravatar compares to those offerings. At their core though, Gravatar and Linktree seem to address similar problems.</p>\n<p>If you're not yet committed to having your own place on the internet (website/blog) or if you're looking for a way to organize your digital identity in single place, Gravatar is an excellent option. I'd encourage you to check it out.</p>\n<p>You can find my Gravatar profile at <a href=\"/gravatar\">lqdev.me/gravatar</a>!</p>\n","tags":["profile","online","gravatar","website","web","comments","internet","automattic","wordpress","identity","linktree","indieweb","socialmedia","blog","bloggin"]},{"title":"New RSS links","contentType":"notes","date":"2024-01-07 19:47","url":"/notes/new-rss-feed-links/","content":"<p>Based on the post, <a href=\"/responses/own-your-rss-links\">own your RSS links</a>, I decided to set up redirects for all my RSS feeds which was very easy to do using Azure CDN. You can find the updated links in my <a href=\"/feed\">feed page</a>. If you're currently subscribed to any of those feeds, you can keep using the old URLs but I'd recommend updating your feed reader with the latest URLs.</p>\n","tags":["rss","indieweb","identity","social","posts"]},{"title":"Aligning mind, body, and soul","contentType":"notes","date":"2024-01-07 15:18","url":"/notes/mind-body-soul-huberman/","content":"<p>In the past two weeks I've been listening to a few episodes from the <a href=\"https://www.hubermanlab.com/podcast\">Huberman Lab podcast</a>. In many of the episodes, the host, Dr. Andrew Huberman, brings on guests to talk about topics like nutrition, fertility, motivation, finding your purpose, and many others. It's interesting though that despite the large diversity of topics and guests, at their core, many of the conversations ended up coming back to mind, body, and soul. More specifically, nurturing those various aspects of our lives. This includes but is not limited to, fueling your body with the right foods, building community, being active, learning new things, exposing yourself to new experiences, etc.</p>\n","tags":["podcast","lifestyle","wellness","health"]},{"title":"Time to try Anytype","contentType":"notes","date":"2024-01-06 11:45","url":"/notes/anytype-local-only/","content":"<p>Currently I'm using org-mode in Emacs for note-taking, journaling, habit-tracking, task management, any many other things. I've heard good things about Notion and Obsidian but haven't made the switch because I like the flexibility of Emacs. I learned about <a href=\"https://anytype.io/\">Anytype</a> a few months ago and it looked like it might be a good replacement. I'd been holding out because back then it didn't have local-only mode. In their latest release, they <a href=\"https://github.com/anyproto/roadmap/issues/34\">added local-only mode</a> so maybe it's finally time to try it. First though, I have to figure out if it works on NixOS.</p>\n","tags":["pkm","personalknowledgemanagement","anytype","knowledge"]},{"title":"GitHub Actions workflow for post metrics almost works","contentType":"notes","date":"2024-01-03 00:26","url":"/notes/post-statistics-gh-actions-working-almost/","content":"<p>A few days ago, I posted about a script I wrote. The <a href=\"/posts/website-metrics-github-actions/\">script computes computes post metrics on my website</a>. That's what I used to author my <a href=\"/notes/weblogging-rewind-2023-continued/\">(We)blogging Rewind 2023</a> posts. Because I want the script to run at periodic intervals, I automated the process using GitHub Actions.</p>\n<p>The script is scheduled to run on the first of every month and I'm happy to report that it works! Kind of.</p>\n<p><img src=\"https://github.com/lqdev/luisquintanilla.me/assets/11130940/bd413326-cc7b-44f7-a287-4580f1268cb2\" class=\"img-fluid\" alt=\"Image of an unsuccessful run of a GitHub Actions workflow\" /></p>\n<p>The GitHub Action was successfully triggered at the defined interval. Unfortunately, the script failed due to a <a href=\"https://github.com/lqdev/luisquintanilla.me/commit/1296c3f929e3ad2834a8b7963c9ed66421a4fb8a\">user error which I've since fixed</a>. Come February 1st, everything should work as expected though, so I'm happy about that.</p>\n","tags":["github","cicd","blogging","automation","githubactions","fsharp","cron"]},{"title":"Orange Bowl 2023 - the playoff game that never was","contentType":"notes","date":"2023-12-30 17:51","url":"/notes/orange-bowl-2023/","content":"<p>So basically this Orange Bowl game between Georgia and Florida State is a look at what the College Football Playoff could've been, had both teams been part of the playoff. I know teams in general were affected by the transfer portal, but it's not even close. It's barely halftime but the game is basically over unless Florida State finds a way to score 6 touchdowns in the next 30 minutes.</p>\n","tags":["football","cfb","orangebowl","fsu","uga","collegefootball"]},{"title":"2024 - The year of Gemini?","contentType":"notes","date":"2023-12-30 00:34","url":"/notes/2024-year-of-gemini/","content":"<p>If 2023 was the year of AI and the Fediverse, is 2024 the year of Gemini?</p>\n<p>I've been seeing a few posts about Gemini cross my feed reader which is more than I'm used to seeing. These include:</p>\n<ul>\n<li><a href=\"https://hackaday.com/2023/12/29/the-gopher-revival-is-upon-us/\">The Gopher Revival Is Upon Us</a></li>\n<li><a href=\"https://zserge.com/posts/small-web/\">Poor Man's Web</a></li>\n</ul>\n<p>Gemini is weird in a fun way.</p>\n<p>Since content is text-based, using terminal clients like <a href=\"https://thelambdalab.xyz/elpher/\">Elpher</a> and <a href=\"https://github.com/makew0rld/amfora\">Amfora</a> make it easy to browse pages (capsules) and blogs (gemlogs).</p>\n","tags":["gemini","protocols","smallweb","openweb","web","internet","gopher"]},{"title":"App Defaults 2023","contentType":"notes","date":"2023-12-25 22:28","url":"/notes/app-defaults-2023/","content":"<p>Late to the <a href=\"https://defaults.rknight.me/\">App Defaults</a> train but here's what I use as of December 2023. For the latest list and Every Day Carry (EDC) list, see the <a href=\"/uses\">Uses page</a>.</p>\n<ul>\n<li><strong>Operating System</strong>: <a href=\"https://nixos.org/\">NixOS</a> (Desktop) / <a href=\"https://www.android.com/\">Android</a> (Mobile)</li>\n<li><strong>E-Mail</strong>: <a href=\"https://www.thunderbird.net/\">Thunderbird</a> (Desktop) / <a href=\"https://www.microsoft.com/en-us/microsoft-365/outlook/email-and-calendar-software-microsoft-outlook\">Outlook</a> (Mobile)</li>\n<li><strong>Feed Reader</strong>: <a href=\"https://www.newsblur.com/\">NewsBlur</a></li>\n<li><strong>Podcasts</strong>: <a href=\"https://antennapod.org/\">AntennaPod</a></li>\n<li><strong>Music</strong>: <a href=\"https://open.spotify.com/\">Spotify</a></li>\n<li><strong>Notes</strong>: <a href=\"https://www.gnu.org/software/emacs/\">Emacs</a> <a href=\"https://orgmode.org/\">(Org-Mode)</a></li>\n<li><strong>Tasks</strong>: <a href=\"https://www.gnu.org/software/emacs/\">Emacs</a> <a href=\"https://orgmode.org/\">(Org-Mode)</a> / <a href=\"https://to-do.office.com/tasks/\">Microsoft To-Do</a></li>\n<li><strong>Office</strong>: <a href=\"https://www.libreoffice.org/\">LibreOffice</a></li>\n<li><strong>Browser</strong>: <a href=\"https://www.mozilla.org/en-US/firefox/new/\">Firefox</a></li>\n<li><strong>Messaging</strong>: <a href=\"https://element.io/\">Element</a> / Signal <a href=\"https://molly.im/\">(Molly)</a></li>\n<li><strong>Maps</strong>: <a href=\"https://osmand.net/\">OSMAnd~</a></li>\n<li><strong>Text Editor</strong>: <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a></li>\n<li><strong>E-Book Management</strong>: <a href=\"https://calibre-ebook.com/\">Calibre</a></li>\n<li><strong>Reading</strong>: <a href=\"https://libbyapp.com/\">Libby</a></li>\n<li><strong>Video</strong>: <a href=\"https://mpv.io/\">mpv</a> / <a href=\"https://www.videolan.org/vlc/\">VLC</a></li>\n<li><strong>Authenticator</strong>: <a href=\"https://getaegis.app/\">Aegis</a></li>\n</ul>\n","tags":["apps","appdefaults","edc"]},{"title":"One more blog post for 2023?","contentType":"notes","date":"2023-12-24 14:31","url":"/notes/one-more-post-2023/","content":"<p>Update from <a href=\"/notes/weblogging-rewind-2023/\">yesterday</a>. 2023 is still not over. I think I can squeeze out at least one more post for the year 🙂.</p>\n\n<p>Year</p>\n<p>Count</p>\n<p>2023</p>\n<p>7</p>\n<p>2022</p>\n<p>7</p>\n\n<p>Year</p>\n<p>Count</p>\n<p>2023</p>\n<p>37</p>\n<p>2022</p>\n<p>36</p>\n","tags":["weblogging","quantifiedself","indieweb","stats","analytics","blog","blogging","internet","opensource","community","selfhost","fsharp","spotify","antennapod","microblog","analytics","pkm","personalknowledgemanagement"]},{"title":"(We)blogging Rewind 2023 Continued","contentType":"notes","date":"2023-12-24 09:39","url":"/notes/weblogging-rewind-2023-continued/","content":"<p>Yesterday I posted aggregate statistics about this website in the post <a href=\"/notes/weblogging-rewind-2023/\">(We)blogging Rewind 2023</a>.</p>\n<p>I thought it'd be cool to see which domains I referenced most in my responses. Here's the breakdown of the top 5.</p>\n<p>Domain</p>\n<p>Count</p>\n<p>github.com</p>\n<p>15</p>\n<p>huggingface.co</p>\n<p>11</p>\n<p>arxiv.org</p>\n<p>10</p>\n<p>openai.com</p>\n<p>6</p>\n<p>www.theverge.com</p>\n<p>4</p>\n<p>No surprise here given that AI was one of the top tags in my responses.</p>\n<p>Updated script is here <a href=\"/resources/snippets/lqdev-me-website-post-metrics\">stats.fsx</a>.</p>\n","tags":["weblogging","quantifiedself","indieweb","stats","analytics","blog","blogging","internet","opensource","community","selfhost","fsharp","spotify","antennapod","microblog","analytics","pkm","personalknowledgemanagement"]},{"title":"Utah vs Northwestern SRS Distribution Las Vegas Bowl Thread","contentType":"notes","date":"2023-12-23 21:39","url":"/notes/srs-distribution-las-vegas-bowl-2023/","content":"<p><strong>21:40</strong></p>\n<p>This Utah defense are the real MVPs. Utah hasn't been able to put together a drive all game. So far I think I've seen 3-4 turnovers. The fact that Northwestern is ONLY up by 7 is shocking.</p>\n<p><strong>21:47</strong></p>\n<p>Make it make sense. Something must've happened. Maybe injuries or something. This season they:</p>\n<ul>\n<li>Beat Florida</li>\n<li>Beat USC</li>\n<li>Had a close game against Washington which is now in the playoffs</li>\n</ul>\n<p>Yet they can't put together a drive. Something doesn't add up.</p>\n<p><strong>21:54</strong></p>\n<p>Okay. I think I got it. Utah like many of the Pac-12 teams is a passing team. Northwestern is in the <a href=\"https://www.foxsports.com/articles/college-football/2023-college-football-defense-rankings-team-pass-and-rush-stats\">top 25 passing defenses</a>. So I guess this is the toughest defense they've faced?</p>\n<p><strong>22:01</strong></p>\n<p>7-7!</p>\n<p><img src=\"https://media.giphy.com/media/j6uK36y32LxQs/giphy.gif\" class=\"img-fluid\" alt=\"Jim Carrey GIF Dumb and Dumber with caption &amp;quot;So you're telling me there's a chance&amp;quot;\" /></p>\n<p><strong>22:16</strong></p>\n<p>Armchair quarterbacking, but at 44% 4th down conversions, I don't think you go for it on 4th and 2 in the middle of the field. No matter how well your defense is playing and going against a backup quarterback.</p>\n<p><strong>22:25</strong></p>\n<p>And there it is. Starting QB came back into the game and had a short field to work with against a tired defense. Offense is going to have to dig deep for Utah.</p>\n<p><strong>22:33</strong></p>\n<p>That's the game. Bad execution on 4th down again.</p>\n","tags":["football","utah","northwestern","cfb","collegefootball"]},{"title":"What about instrumentals? AI Generated Spotify Clips Addendum","contentType":"notes","date":"2023-12-23 19:32","url":"/notes/genai-spotify-clips-instrumental/","content":"<p>In my recent post, <a href=\"/posts/generative-ai-spotify-clips\">Using Generative AI to produce Spotify Clips</a> I listed a bunch of challenges but realized I missed a big one. While listening to <a href=\"https://www.youtube.com/watch?v=G0CqHxDG74k\">IJ by Sam Gendel</a>, I realized, what do you do about instrumental songs, solos, or songs with few lyrics?</p>\n<p>Some sort of audio-to-video technique?</p>\n<p>Maybe using metadata like title, genre, beats-per-minute (BPM), spectrogram, and musical notes would work?</p>\n<p>Or generating an image and animating it or its background to add depth similar to how in the <a href=\"https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html\">VideoPoet blog post</a>, the image of the Mona Lisa is animated.</p>\n<p>It would be a fun experiment.</p>\n","tags":["ai","genai","music","instrumental","spotify","generativeai","llm"]},{"title":"(We)blogging Rewind 2023","contentType":"notes","date":"2023-12-23 02:28","url":"/notes/weblogging-rewind-2023/","content":"<p>Inspired by <a href=\"/notes/spotify-wrapped-2023\">Spotify Wrapped</a> and <a href=\"/notes/antennapod-echo-2023\">Antennapod Echo</a>, I decided to look back at my activity on this website for 2023. Here is the <a href=\"/resources/snippets/lqdev-me-website-post-metrics\">snippet</a> I used to generate these metrics.</p>\n\n<h3>Blog</h3>\n<p>Blog posts are my more formal, long-form content. This year, I published less blog posts than last year.</p>\n<p>Year</p>\n<p>Count</p>\n<p>2023</p>\n<p>5</p>\n<p>2022</p>\n<p>7</p>\n<p>Part of that includes me getting in my head about what these posts &amp;quot;should be&amp;quot;. Historically, they have been detailed technical posts related to a project I've worked on like <a href=\"/posts/receive-webmentions-fsharp-az-functions-fsadvent\">Accept Webmentions using F#, Azure Functions, and RSS</a>. I haven't stopped experimenting and tinkering, but because they aren't end-to-end solutions, I don't think of them as &amp;quot;blog post worthy&amp;quot;. That doesn't mean though that learnings and growth haven't come from them. In more recent blog posts like <a href=\"/posts/ai-1999-1899\">AI like it's 1999 or 1899</a> and <a href=\"/posts/quick-thoughts-snapdragon-summit-2023\">Quick thoughts about Snapdragon Summit 2023</a>, I've started experimenting more with just posting. In 2024, I want to be more flexible in my posting and not worry too much about whether the blog posts are &amp;quot;good enough&amp;quot;.</p>\n<h3>Microblog</h3>\n<p>This site also contains a microblog feed, one for <a href=\"/feed\">notes</a> and another for <a href=\"/feed/responses\">responses</a>.</p>\n<h4>Notes</h4>\n<p>Notes tend to be original content that although can include links to external content, they are short-form and informal versions of blog posts. This post is an example of a note. This year, I posted less notes than last year.</p>\n<p>Year</p>\n<p>Count</p>\n<p>2023</p>\n<p>33</p>\n<p>2022</p>\n<p>36</p>\n<p>Usually, when these notes start to get long, I turn them into blog posts. <a href=\"/posts/quick-thoughts-snapdragon-summit-2023\">Quick thoughts about Snapdragon Summit 2023</a> is an example of that. However, I haven't set a criteria for what makes a note &amp;quot;long&amp;quot;. This post for example I think is getting to the &amp;quot;blog post&amp;quot; length. I can't tell you why though, it's just &amp;quot;vibes&amp;quot;. One of the benefits of starting with notes types of posts is that the frame of mind is more informal which lets me just write and post without thinking too much about what the post &amp;quot;should be&amp;quot;. In 2024, I want to continue using notes as the starting point for these types of posts. Maybe later in the future, I just merge them both.</p>\n<h4>Responses</h4>\n<p>Responses are reactions to content created / published by someone else. These can include bookmarks, replies, likes (star), and reposts (reshare). This year, I posted significantly more responses than last year.</p>\n<p>Year</p>\n<p>Count</p>\n<p>2023</p>\n<p>216</p>\n<p>2022</p>\n<p>146</p>\n<p>I'm very happy with this, especially my bookmark use explained later. Not only am I happy because I'm actually making use of my website and the work I put in previous years that enabled me to publish all types of content but also because these responses create a log of my interactions with the internet over the years I can look back at.</p>\n\n<p>Looking deeper into my responses and breaking them down by type, bookmarks take up the top spot with more than half of my responses being bookmarks. That makes sense since I'm usually bookmarking articles, papers, and other content I come across on the internet.</p>\n<p>Type</p>\n<p>Count</p>\n<p>bookmark</p>\n<p>151</p>\n<p>reshare</p>\n<p>48</p>\n<p>reply</p>\n<p>10</p>\n<p>star</p>\n<p>7</p>\n<p>Let me start off with bookmarks. Bookmark style reponses on my website have slowly become my default capture system. Now, not everything I read and come across makes it onto my bookmarks. I'm experimenting with the first pass which I might just turn into another post. Generally, the things that are interesting or thought provoking, I post. Bookmark posts generally include:</p>\n<ul>\n<li>Post title - Gives me a readable way to see what the post is about</li>\n<li>Source link - Provides a direct link to the source material</li>\n<li>Tags - Enable me to index and organize related topics</li>\n<li>Quotes / Notes - Provide highlights and things I thought were interesting about the post</li>\n</ul>\n<p>When all these pieces come together, they provide me with distilled bits of information that are organized in a way that enable quick retrieval and action when needed.</p>\n<p>Moving on to other posts. Since the post type is effectively just metadata, the line between reshare, star, and reply is often blurry. It's not as blurry with replies because I use that to draft replies / comments to other posts and content on the internet. My <a href=\"/responses/tracy-durnell-20-blogging-anniversary\">Tracy Durnell 20th anniversary of blogging</a> post is an example of a reply. In these examples, the replies were directed at someone. However, for star and reshare, I'm still trying to understand where they fit in my flow. Today other than visually expressing the post type, the role these post types serve is webmentions. However, I'm not sure how many websites I create these post types for actually use webmentions. Maybe some analysis worth doing for 2025.</p>\n\n<p>AI and LLMs took the top spots which I think in general aligns with what was top of mind for everyone in 2023.</p>\n<p>Tag</p>\n<p>Count</p>\n<p>ai</p>\n<p>104</p>\n<p>llm</p>\n<p>42</p>\n<p>untagged</p>\n<p>41</p>\n<p>opensource</p>\n<p>31</p>\n<p>internet</p>\n<p>17</p>\n<p>Not much commentary here. Tags have helped me organize related content on my website. I do think that I get too high level with my tags and could do a better job of being more specific. Ultimately, there's no limit to the number of tags I can use so why not be liberal with it. The only self-imposed limitation I've placed on myself is the first 3 tags should be the most important because they're the ones that show up when my articles are automatically cross-posted to Mastodon.</p>\n\n<p>Overall, I'm happy with my activity and heavy use of my own website this year. I plan to make even more use of the bookmark type of posts in 2024 and plan on posting more whether that's as a note or blog post, just posting is something I want to work towards.</p>\n","tags":["weblogging","quantifiedself","indieweb","stats","analytics","blog","blogging","internet","opensource","community","selfhost","fsharp","spotify","antennapod","microblog","analytics","pkm","personalknowledgemanagement"]},{"title":"Growing ActivityPub adoption","contentType":"notes","date":"2023-12-18 20:28","url":"/notes/growing-activitypub-adoption-december-2023/","content":"<p>Exciting developments in the ActivityPub space. A few months ago, the <a href=\"/responses/wordpress-activitypub-plugin/\">WordPress plugin</a> was released. In the last few days, Threads recently started testing their <a href=\"https://www.theverge.com/2023/12/13/24000120/threads-meta-activitypub-test-mastodon\">ActivityPub integration</a>. Not so new but new to me, I saw that <a href=\"https://meta.discourse.org/t/activitypub-plugin/266794\">Discourse, the forum software, also has an ActivityPub plugin</a>. <a href=\"https://www.theverge.com/2023/12/18/24006062/flipboard-fediverse-mastodon-activitypub-profiles-social\">Flipboard</a> is one of the latest names to express plans for ActivityPub integration. Not sure what the end result will look like when all these platforms are all interoping with each other but I'm excited to see platforms building on open protocols.</p>\n","tags":["activitypub","fediverse","openweb","protocols","internet","web"]},{"title":"Crate Finds - Fall 2023","contentType":"notes","date":"2023-12-17 14:59","url":"/notes/crate-finds-fall-2023/","content":"<p>Trying something new out I'm calling Crate Finds. I'm using it as a means to look back at either new music I came across or music I'm listening to at various points in time. Kind of like Spotify Wrapped, but less stats and aggregates and more point-in-time snapshots without the granuliarity of scrobbler services like Last.fm.</p>\n<p>This is the first drop from Fall 2023.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/23TDgK6Duz53KFMo02Vh25?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n","tags":["cratefinds","music","playlist","spotify"]},{"title":"Have you tried turning it off and on again? - Mastodon edition","contentType":"notes","date":"2023-12-15 15:53","url":"/notes/turning-off-on-mastodon-edition/","content":"<p>My Mastodon instance crashed this morning due to a combination of the usual running out of space issue and me being too lazy to remember to remove federated media regularly from my instance BEFORE it crashes. Now I know you can schedule this with CRON jobs but for some reason I've never been able to configure that correctly nor have I been motivated to get to the bottom of it.</p>\n<p>Anyway, I went through the usual dance of starting up my services and <a href=\"/resources/wiki/mastodon-server-cleanup\">using the CLI to clean up my instance</a>.</p>\n<p>However, none of my commands worked because it couldn't connect to the database. A quick look at the status showed the database and all my services were running which led me to go down a troubleshooting rabbit hole with no end in sight and little guidance as to what might be the problem.</p>\n<p>Eventually, before gave up I thought, &amp;quot;my instance is down anyway, couldn't I just turn it off and on again?&amp;quot; I did exactly that and when it restarted I was able to continue with my cleanup tasks as normal.</p>\n<p>So as a reminder, if all else fails, try turning it off and on again.</p>\n","tags":["mastodon","sysadmin","selfhost","fediverse"]},{"title":"Today I Learned about Firefish","contentType":"notes","date":"2023-12-13 19:36","url":"/notes/til-firefish/","content":"<p>TIL about <a href=\"https://joinfirefish.org/\">Firefish</a>.</p>\n<p><img src=\"/files/images/firefish-admin-user-page.png\" class=\"img-fluid\" alt=\"firefish.social admin page\" /></p>\n<p>I like the look of it. It reminds me of Tumblr in many ways.</p>\n","tags":["firefish","fediverse","socialmedia"]},{"title":"Antennapod Echo (Wrapped) 2023","contentType":"notes","date":"2023-12-12 21:03","url":"/notes/antennapod-echo-2023/","content":"<p>This was a nice surprise to see when I opened up Antennapod the other day. Using my listening statistics, it was able to compile a yearly summary for me (locally).</p>\n<p><img src=\"/files/images/antennapod-wrapped-1.png\" class=\"img-fluid\" alt=\"Antennapod Echo\" /></p>\n<p><img src=\"/files/images/antennapod-wrapped-2.png\" class=\"img-fluid\" alt=\"Antennapod Echo\" /></p>\n<p>Looks like I have a lot of catching up to do on the backlog.</p>\n<p><img src=\"/files/images/antennapod-wrapped-3.png\" class=\"img-fluid\" alt=\"Antennapod Echo\" /></p>\n<p><img src=\"/files/images/antennapod-wrapped-4.png\" class=\"img-fluid\" alt=\"Antennapod Echo\" /></p>\n<p><img src=\"/files/images/antennapod-wrapped-5.png\" class=\"img-fluid\" alt=\"Antennapod Echo\" /></p>\n","tags":["podcasts","opensource","antennapod","stats"]},{"title":"Spotify Wrapped 2023","contentType":"notes","date":"2023-12-12 21:03","url":"/notes/spotify-wrapped-2023/","content":"<p>Kind of late, but here's my Spotify Wrapped for the year. No idea how Eslabon Armado snuck in there at the end. It'd be cool if I was also able to get data from my MP3 player which is where I mainly listen to music. Though the artists wouldn't change much.</p>\n<p><img src=\"/files/images/spotify-wrapped-1.png\" class=\"img-fluid\" alt=\"Spotify Wrapped 2023 Summary\" /></p>\n","tags":["music","stats","spotifywrapped"]},{"title":"RSS Only Blog","contentType":"notes","date":"2023-12-06 21:20","url":"/notes/rss-only-blog/","content":"<p>I stumbled upon <a href=\"https://theunderground.blog/\">theunderground blog</a> by <a href=\"https://chrismcleod.dev/\">Chris McLeod</a> which is an interesting twist on <a href=\"https://daverupert.com/rss-club/\">RSS Club</a>. Currently theunderground has a page that serves as the homepage and the RSS feed itself. I wonder whether it's possible to take it even further by using XSL(T) to style the RSS feed as shown in this <a href=\"https://darekkay.com/blog/rss-styling/\">Style Your RSS Feed</a> post by <a href=\"https://darekkay.com/blog/rss-styling/\">Darek Kay</a>. By styling the RSS feed, it can serve as the homepage as well. Using redirection, when people land on your domain, they would instead be presented with the styled RSS feed.</p>\n","tags":["rss","blogs","social"]},{"title":"Ayaneo Slide","contentType":"notes","date":"2023-11-20 17:34","url":"/notes/ayaneo-slide/","content":"<p>I NEED ONE!</p>\n<p><a href=\"https://www.youtube.com/watch?v=G6Mhrb9heiI\" title=\"AYANEO Slide Promo Video\"><img src=\"http://img.youtube.com/vi/G6Mhrb9heiI/0.jpg\" class=\"img-fluid\" alt=\"Ayaneo Slide Promo Video\" /></a></p>\n<p>This gives me <a href=\"https://en.wikipedia.org/wiki/Danger_Hiptop\">Sidekick</a> / <a href=\"https://en.wikipedia.org/wiki/Mylo_(Sony)\">Sony Mylo</a> vibes. The only reason I wouldn't get it is battery life. I don't care for the gaming capabilities.</p>\n<p>Throw in an ARM-based processor like the Snapdragon X Elite for all-day battery life, give me USB-C ports to attach a dongle to, and I'm sold.</p>\n","tags":["pc","handheld"]},{"title":"Website upgraded to .NET 8","contentType":"notes","date":"2023-11-16 01:27","url":"/notes/net8-website-update/","content":"<p>Obligatory yearly website update post. When I built this website, I started on .NET 5. 3 years later and although my code has changed to add new features, I haven't had to make any code changes to account for a new version of .NET. I love that I can get all the benefits of the new version without having to do any rewrites.</p>\n<p><img src=\"/files/images/upgrade-dotnet-8.png\" class=\"img-fluid\" alt=\"GitHub PR showing upgrade to .NET 8\" /></p>\n","tags":["dotnet","indieweb"]},{"title":"Timeless habits","contentType":"notes","date":"2023-11-12 14:05","url":"/notes/covey-timeless-habits/","content":"<p>Following up on the theme of cycles and looking back at things from yesteryear explored in my post <a href=\"/notes/ai-abundance-scarcity-cycle-repeats-rss-enclosure-use-case\">AI abundance after scarcity cycles</a>, I came across another example.</p>\n<p>While looking through some of my old books, I came across Stephen Covey's &amp;quot;7 Habits of Highly Effective People&amp;quot;. I remember reading it over ten years ago. At the time, the habits and themes explored in the book had an impact on me. However, since that time I haven't looked at the book again. Today, I cracked it open and skimmed through the foreword. In it, Stephen talks about common human challenges such as:</p>\n<ul>\n<li>Fear and insecurity - This could mean a fear of the future, like losing your job.</li>\n<li>Lack of life balance - Put in the context of being always online, our lives continue to get more complex and demanding.</li>\n</ul>\n<p>Given recent talking points of AI coming for our jobs and the accelerated rate of technological innovation, you could pluck these challenges from the time they were written and they'd still be relevant today. You could also place them 500 or 1000 years ago and they'd also be relevant which made me think of the <a href=\"https://en.wikipedia.org/wiki/Lindy_effect\">Lindy Effect</a>. In short, it states that the longer a technology or idea continues to live, the higher the likelihood it will continue to exist into the future.</p>\n<p>I found it funny though that while flipping through the book and reading about these timeless challenges and principles, I came across the opposite of that. One of my bookmarks was a Borders membership card. If you don't know or remember what Borders was, in a gist, it was one of the large bookstores which succumbed to disruption from e-commerce (i.e. Amazon)</p>\n<p><img src=\"/files/images/borders-membership-card.jpg\" class=\"img-fluid\" alt=\"An image of a Borders member card\" /></p>\n","tags":["books","reading","sevenhabits"]},{"title":"AI abundance after scarcity cycles","contentType":"notes","date":"2023-11-12 10:37","url":"/notes/ai-abundance-scarcity-cycle-repeats-rss-enclosure-use-case/","content":"<p>While writing the post <a href=\"/notes/rss-community-calendars-new-events\">RSS for new event notifications</a> I mentioned how the flow of getting new event notifications through RSS might work:</p>\n<blockquote class=\"blockquote\">\n<p>When a new event is added to the calendar, the RSS feed can be updated. The <code>channel</code>property has the link to the shared calendar you can subscribe to and the individual events have <em>.ics</em> files embedded in the <code>enclosure</code>.</p>\n</blockquote>\n<p>I actually didn't know if that was possible until I looked at the <a href=\"https://en.wikipedia.org/wiki/RSS_enclosure\">RSS enclosure syntax</a>. Based on that page, as long as you provide the URL to the <em>.ics</em> file, length of the file, and the MIME type (text/calendar), you can link to your events inside enclosures. So it is possible.</p>\n<p>That's not the point of this post though. When I was looking at the references, I came across this post talking about the <a href=\"https://www.rssboard.org/rss-enclosures-use-case\">use case for RSS enclosures</a>. I don't know when this post was published but given the problems mentioned with accessing high quality video, I assume it was in the days when the internet was on dial-up or DSL speeds. At that time, downloading webpages or e-mail was slow. Forget about downloading multimedia. As that post mentions, with enclosures, you could schedule your RSS client to download content like videos during off-peak hours so any content you'd want to access would download overnight and be available to you in the morning. About 30 years later, in many places, even with a connection that has 1MB download speeds, downloading a podcast that's 100MB or a video that's 2GB is relatively fast. It might take a while, but compared to 30 years ago, it's &amp;quot;instant&amp;quot;. So at this point, it's not really technical challenge as much as it is an access challenge.</p>\n<p>Low bandwith speeds forced people to get creative in how they utilized their resources. The internet was not the only time this happened. About 30 years ago or so when cell phones were just becoming mainstream they too suffered from scarcity. Cell phones and plans were very expensive. You had limited minutes and when SMS became a thing, those too were limited. If you wanted to have hour long conversations with friends and loved ones, you'd have to wait until off-peak hours so you could use your &amp;quot;free&amp;quot; minutes. Fast-forward 30 years later, cell phones and cell phone plans are relatively affordable. When you purchase these plans, you get access to unlimited minutes, messages, and data. So again, the technical limitations aren't as prevalent. It's mainly a matter of access and whether your cell phone provider offers good coverage in your area.</p>\n<p>This brings me to my point. We're seeing this cycle again with AI. Large Language Models today are expensive to run and I'm not overindexing on the financial perspective. You pay per call, access to these models can be throttled at times, there's only so many GPUs you can get access to in a data center to run them, you pay the latency penalty by going through the network as well as the cost of inferencing, and you also have token limits. About a year into the release of ChatGPT, access to these models has been expanded, the models have gotten more capable (<a href=\"https://openai.com/research/gpt-4v-system-card\">GPT-4V</a> and GPT4 with 128k tokens), it's become <a href=\"https://openai.com/blog/new-models-and-developer-products-announced-at-devday\">cheaper to run these models</a>. At the same time, <a href=\"https://www.qualcomm.com/news/onq/2023/10/rethink-whats-possible-with-new-snapdragon-x-elite-platform\">computing is becoming more efficient</a> and <a href=\"https://ai.meta.com/llama/#inside-the-model\">smaller &amp;quot;open-source&amp;quot; models</a> are becoming more capable. Today, we have to be creative with how we utilize our resources, but we're only a year in. If we extrapolate how these cycles play out though, in 30 years we'll forget how challenging it was to use and access early AI technologies and AI will become more abundant like the internet, cell phones, and other technologies before it.</p>\n","tags":["ai","internet","history","rss","openai","llm","gpt","opensource"]},{"title":"RSS for new event notifications","contentType":"notes","date":"2023-11-12 09:52","url":"/notes/rss-community-calendars-new-events/","content":"<p>After my post on <a href=\"/notes/rss-community-calendars\">Shared calendars and RSS</a> I kept thinking about what else you can get out of sharing a calendar using RSS. What I didn't see immediately is the main use case of RSS, though I did mention it as a drawback.</p>\n<blockquote class=\"blockquote\">\n<p>The main drawbacks I see are that I can't get notifications or see it in my calendar.</p>\n</blockquote>\n<p>Actually, with RSS, I <em>can</em> get notifications when a new event has been added to the calendar. With iCal for example, when a new event is added to the calendar, it'll automatically be populated in my calendar client. However, I won't get a notification that a new event has been added. Conversely, with RSS, I won't see it in my calendar client but I will get a notification in my RSS reader.</p>\n<p>When paired together though you could use RSS to get notified when a new event is added to the calendar and also be able to view it in your calendar client since it'll automatically be populated (assuming you're already subscribed to the shared calendar). That means managing two feeds, but the more I think about it, the RSS format makes some sense. The shared calendar could be the source of truth. When a new event is added to the calendar, the RSS feed can be updated. The <code>channel</code>property has the link to the shared calendar you can subscribe to and the individual events have <em>.ics</em> files embedded in the <code>enclosure</code>. As a consumer, the new event notification will then appear in my RSS reader.</p>\n","tags":["rss","calendars","protocols","community"]},{"title":"Shared calendars and RSS","contentType":"notes","date":"2023-11-11 13:34","url":"/notes/rss-community-calendars/","content":"<p>I just came across a new use for RSS - shared calendars. Well, not exactly sharing calendars in RSS format. While I have seen calendars in RSS format as I'll mention later in this post, I think there's enough downsides that make this impractical. However, for discoverability, there's some RSS patterns that shared calendars could benefit from.</p>\n<p>While reading a <a href=\"https://doc.searls.com/2023/11/09/datepress/\">recent post on Doc Searls' blog</a> it got me thinking about using public calendars for sharing community events. By leveraging iCal or CalDav, this could be a good alternative to current solutions like Facebook Events or Meetup. It could also be a great way to foster interoperability. There's still the discoverability problem, but platforms like Meetup and Facebook Events could enable subscribing to shared public calendars.</p>\n<p>This sent me down a search to see whether others were using shared calendars, similar to <a href=\"https://bsquarebulletin.com/test-calendar/?r34icsym=202312\">The Big Calendar</a>. It was nice to see that Penn State has shared calendars for their various sports. That shouldn't be surprising. As a university, they already share their academic calendars. The part that I found surprising though was that RSS was one of the options.</p>\n<p><img src=\"/files/images/rss-community-calendars.png\" class=\"img-fluid\" alt=\"Penn State Subscription Calendar Subscriptions\" /></p>\n<p>Downloading the RSS feed enabled me to import it into my feed reader and view it there. I'd prefer if instead of downloading the RSS feed as a file, they just provided the link so I could copy and paste it into my feed reader. However, it's still nice that just like podcasts and YouTube feeds, I can now also see Penn State athletic events in my feed.</p>\n<p>I'm not sure what the benefit of subscribing to this feed via RSS as opposed to the calendar itself has. The main drawbacks I see are that I can't get notifications or see it in my calendar. I also don't know if changes and updates to events also propagate to the RSS feed. That's why I'd prefer to just subscribe to the calendar directly.</p>\n<p>Going back to the broader topic of shared calendars and discoverability mentioned in the <a href=\"https://bsquarebulletin.com/test-calendar/?r34icsym=202312\">DataPress proposal</a> for community calendars, you could imagine a solution similar to how websites can make their <a href=\"https://blog.jim-nielsen.com/2021/automatically-discoverable-rss-feeds/\">RSS feeds discoverable</a>. As a calendar and community maintainer, you could post the link to your shared calendars on your website. As a community member, you'd just need to point your calendar client to the respective website's domain (i.e. gopsusports.com) and your client would automatically import the shared calendars. This is how shared calendars work today. The difference is, you don't need the exact link to the calendar, you just need the website domain. The calendar client is what handles the discovery and import process.</p>\n<p>One important thing to highlight, doing something like this isn't exclusive to communities. You could imagine businesses being able to implement similar practices to communicate their business hours and availability. Platforms like Yelp or Google could then just import the public calendar for that business and display the business hours on their platform. To schedule your next oil change, imagine looking at your auto service shop's shared calendar and sending a calendar invite for one of their open slots. This is no different to solutions like <a href=\"https://www.microsoft.com/microsoft-365/business/scheduling-and-booking-app\">Microsoft Bookings</a>. The difference is, instead of using these platforms, you would do it from your preferred calendar client of choice.</p>\n<p>You could also see this working for individuals as well. If you need to coordinate something with other people, instead of the constant back and forth to find availability, you can just share your public calendar and they can find a time that works for them.</p>\n<p>Anyway, the point of this longish post is that by building on simple components like calendars, iCal, CalDAV, and RSS, communities, businesses, and individuals could communicate and collaborate more effectively without being constrained to any single platform.</p>\n","tags":["rss","calendars","protocols","community"]},{"title":"Apple M3 Announced","contentType":"notes","date":"2023-10-31 09:00","url":"/notes/apple-m3-scary-fast/","content":"<p>Apple unveiled their <a href=\"https://www.apple.com/newsroom/2023/10/apple-unveils-m3-m3-pro-and-m3-max-the-most-advanced-chips-for-a-personal-computer/\">M3 family of chips</a> and new devices.</p>\n<p>First off, The Verge always does a great job covering and condensing these events into a short video.</p>\n<p><a href=\"https://www.youtube.com/watch?v=gN8oCWGeRtI\" title=\"Apple Scary Fast Event - The Verge\"><img src=\"http://img.youtube.com/vi/gN8oCWGeRtI/0.jpg\" class=\"img-fluid\" alt=\"Apple Scary Fast Event - The Verge\" /></a></p>\n<p>My initial impressions of the anouncement, this is why Apple wins.</p>\n<p>I'm excited by the Snapdragon announcement last week. However, as I mentioned in my <a href=\"/posts/quick-thoughts-snapdragon-summit-2023\">post</a>, &amp;quot;the biggest miss from Qualcomm and Windows partners today was not having a lineup of devices ready for purchase&amp;quot;.</p>\n<p>That wasn't the case for Apple. Not only did they announce their new chips but they also announced devices which will be available as late as the end of November.</p>\n<p>I understand Apple owns the entire process so they can align their chip and device announcements. However, Qualcomm brought out many partners on stage. Unless I missed it, I don't remember seeing any PC announcements that would be ready for purchase within the next 2-3 months.</p>\n","tags":["arm","apple","m3"]},{"title":"Quick Thoughts Snapdragon Summit 2023 Addendum","contentType":"notes","date":"2023-10-25 19:26","url":"/notes/snapdragon-summit-2023-addendum/","content":"<p>Yesterday I wrote a post with some quick thoughts on <a href=\"/posts/quick-thoughts-snapdragon-summit-2023\">why I'm excited for the new Snapdragon X Elite</a> set of chips.</p>\n<p>There's something I missed which came to mind this morning.</p>\n<p>If your Snapdragon X Elite powered computer is running Windows, and it also has calling and messaging capabilities like a phone, does that mean that you're now the proud owner of a &amp;quot;Windows Phone&amp;quot;? As a fan of Windows Phone, that sounds interesting.</p>\n<p>Now you might say, &amp;quot;Windows doesn't have all my apps&amp;quot;. Fair point but with Windows Subsystem for Android (WSA), maybe the app gap isn't much of a concern. More importantly, as I mentioned in the post, if the apps no longer become the main mode of interaction and instead are relegated to serve as background services to the AI interface, does it even matter?</p>\n<p>In the post, I also mentioned how I'd prefer my mobile computer runs Linux (and no, I don't mean AOSP). Even that is not a strict limitation, since I can run Windows Subsystem for Linux (WSL) inside Windows. So technically I can still have Linux on my mobile computer even if it's running Windows.</p>\n<p>Let's see if 2024 becomes the Year of the Windows (or Linux) Phone.</p>\n","tags":["windows","qualcomm","windowsphone","mobilecomputing","arm","hardware","5g","ai"]},{"title":"Upgraded to Mastodon 4.2.0","contentType":"notes","date":"2023-09-24 20:31","url":"/notes/upgraded-mastodon-4-2-0/","content":"<p>Good thing I'd upgraded to 4.1.7 just a few weeks ago, so getting to 4.2.0 was relatively easy. The <a href=\"/resources/wiki/mastodon-server-upgrades/\">wiki</a> where I'm tracking updates makes it easy to quickly remember the general process. I haven't tried all of the new features, but it's nice to see there's a new onboarding flow to the Web UI which should make it easier to get started for new folks.</p>\n","tags":["mastodon","fediverse","sysadmin","socialmedia","decentralization","internet","web"]},{"title":"Mastodon 4.1.7 feels snappy","contentType":"notes","date":"2023-09-09 16:44","url":"/notes/mastodon-upgrade-4-1-7/","content":"<p>I didn't inspect the changelog between 4.1.0 and 4.1.7 in great detail, but the latest stable version feels snappy.</p>\n","tags":["mastodon","fediverse"]},{"title":"College Football Week 1 Notes","contentType":"notes","date":"2023-09-06 00:17","url":"/notes/cfb-week-1-2023-notes/","content":"<p>College football is back!</p>\n<p>Penn State had a great win. Looking back at a <a href=\"/notes/psu-future-allar\">post from last year</a>, some of my thoughts around the upcoming class were right. Looking at the upcoming schedule it looks well spaced out and if they can stay healthy, they'll have a great season.</p>\n<p>Tough losses for LSU and Clemson.</p>\n<p>This year I'm trying a new way to experience games: blogs + audio streams.</p>\n<p>This past week, although I tuned into the games, I didn't watch any of them on TV. Instead, I tuned into the live audio streams on <a href=\"https://thevarsitynetwork.com/feed\">The Varsity Network</a> and for live discusson, I tuned into the various team blogs / communities provided by <a href=\"https://www.sbnation.com/\">SBNation</a> (fun fact: I learned SB stands for Sports Blogs). For example, <a href=\"https://www.blackshoediaries.com/\">Black Shoe Diaries</a>, the Penn State blog, had a <a href=\"https://www.blackshoediaries.com/2023/9/2/23855867/penn-state-nittany-lions-football-west-virginia-fourth-quarter-open-thread-bsd\">live thread</a> for each of the quarters where site writers and fans got to interact in real-time. Other team blogs had similar threads.</p>\n<p>Overall, I thought it was a fun experience, and I'm most likely going to do it again this week.</p>\n<p>Games I'm most looking forward to Week 2:</p>\n<ul>\n<li>Stanford vs USC</li>\n<li>Temple vs Rutgers</li>\n<li>Texas vs Alabama</li>\n</ul>\n","tags":["sports","football"]},{"title":"Need to automate seasonal 'blue ckecks'","contentType":"notes","date":"2023-07-23 16:23","url":"/notes/automate-website-updates/","content":"<p><img src=\"/files/images/july-checkmark.png\" class=\"img-fluid\" alt=\"Profile header image with alien emoji\" /></p>\n<p>I really need to automate my website updates. I had forgotten to update the &amp;quot;checkmark&amp;quot; (for lack of better words) for the month of July. Usually, I try to have an icon show up as my &amp;quot;blue check&amp;quot; (Twitter inspired) that is relevant for each month. July is <a href=\"https://www.britannica.com/event/Roswell-incident\">Roswell</a> month mostly because I think I already used the sun emoji for one of the other summer months. I guess being in the US, the US flag would've made sense here, but somehow aliens seemed more fun. Anyway, I need to find a good way to automate this so it's not one of those things that happen when I remember. Since I just changed to the alien emoji, it means I probably will only have it up for a week. August is Tux month because of Linux (again, sun and aliens were already taken). Recommendations are appreciated. Though I'm excited for spooky season in September &amp;amp; October.</p>\n","tags":["github","roswell","truthisoutthere","xfiles","automation"]},{"title":"Thinking of switching to NixOS","contentType":"notes","date":"2023-07-21 17:15","url":"/notes/maybe-switching-nixos/","content":"<p>Time to distro hop. Maybe. For the past two years I've been happily using Manjaro as my main distribution. During that time I've also kept an eye on the immutable distributions. One of the main ones I've been paying attention to is <a href=\"https://nixos.org/\">NixOS</a>. Like Arch, it's taken me some time to wrap my head around it. Now that it's gotten easier to set up through a graphical installer, I'll start out with a live install to see how I like it.</p>\n","tags":["linux","nixos"]},{"title":"Blog instead of e-mail?","contentType":"notes","date":"2023-06-18 21:20","url":"/notes/metablogging-life-after-email/","content":"<p>I've been thinking about shifting at least some of my communications from e-mails to blogs especially in the context of business communications.</p>\n<p>Blogs are pull rather than push, so I'm not innundating other's inboxes. People who are interested in the projects or topics I'm working on can subscribe and consume the content as they wish. Also it's a way to be more expressive since it's a medium where ideas can be connected to provide more relevant context. That doesn't mean chats and even e-mail couldn't be used for urgent communication. For async non-urgent communciation though, blogs might be a good alternative to e-mail.</p>\n<p>These two articles provide a good introduction into how individuals and companies have implemented similar practices:</p>\n<ul>\n<li><a href=\"https://ideaspace.ystrickler.com/p/metablogging\">Metablogging</a></li>\n<li><a href=\"https://www.fastcompany.com/3017396/is-there-life-after-email-yes-and-its-amazing\">Is There Life After E-mail? Yes, and It's Amazing</a></li>\n</ul>\n","tags":["blogging","email","communication"]},{"title":"Matrix Synapse REALLY needs an Admin UI","contentType":"notes","date":"2023-06-11 21:32","url":"/notes/matrix-synapse-needs-admin-ui/","content":"<p>Spent the day playing sysadmin and removed about 50GB worth of data from my Matrix Synapse instance. Like Mastodon, most of that data wasn't even mine. Most of it comes is remote data from other users in large rooms. I really like Matrix and the ability to host my own messaging server that provides end-to-end encryption as well as federation with other instances. However, not having a straight-forward way of managing my server and cleaning up resources is painful.</p>\n<p>Anyway, if anyone is interested, here are some resources that have helped me in this process.</p>\n<p><a href=\"/resources/wiki/matrix-synapse-admin\">Matrix Synapse Server Administration</a></p>\n","tags":["matrix","selfhost","synapse","opensource","sysadmin","messaging"]},{"title":"New AI generated phone wallpaper","contentType":"notes","date":"2023-06-07 23:02","url":"/notes/bing-ai-phone-wallpaper/","content":"<p>I'm really liking my new phone wallpaper. Courtesy of DALL-E.</p>\n<p><img src=\"/files/images/bing-ai-pug-wallpaper.jpg\" class=\"img-fluid\" alt=\"pug in the utah desert on mushrooms opening his third eye and being beamed up by aliens\" /></p>\n<p>In case anyone is interested in the prompt - &amp;quot;pug in the utah desert on mushrooms opening his third eye and being beamed up by aliens&amp;quot;</p>\n","tags":["ai","dalle","dumbphone"]},{"title":"I'm on Bluesky","contentType":"notes","date":"2023-06-02 15:50","url":"/notes/now-on-bluesky/","content":"<p>I'm now on Bluesky. Follow me at <a href=\"https://bsky.app/profile/lqdev.me\">@lqdev.me</a></p>\n<p><img src=\"/files/images/bsky-profile.png\" class=\"img-fluid\" alt=\"Bluesky web app showing lqdev.me profile\" /></p>\n","tags":["web","socialmedia","bluesky"]},{"title":"Newspapers, CDs, and MP3 players - Am I in 2006?","contentType":"notes","date":"2023-05-29 19:50","url":"/notes/am-i-in-2006-newspapers-cds-mp3s/","content":"<p>While flipping through a local (physical) newspaper, I came across an ad for a sale happening at a nearby record store.</p>\n<p><img src=\"/files/images/newspaper_ad.JPG\" class=\"img-fluid\" alt=\"Newpaper ad for a memorial day CD and vinyl sale\" /></p>\n<p>Having nothing planned, I decided to check it out and ended up with a nice haul of about 15 CDs and 2 DVDs.</p>\n<p><img src=\"/files/images/cds_mp3.JPG\" class=\"img-fluid\" alt=\"3 CDs on a wooden table with an portable MP3 player\" /></p>\n<p>I fired up my <a href=\"https://www.bestbuy.com/site/lg-8x-external-usb-double-layer-dvdrw-cd-rw-drive-black/5852919.p?skuId=5852919\">portable CD/DVD drive</a> to transfer my new purchases to my <a href=\"https://www.fiio.com/m6\">portable MP3 player</a>.</p>\n<p>Once finished, I popped one of the DVDs into the drive, made popcorn, and settled in. If I didn't know any better, this could easily be a scene from 2006.</p>\n","tags":["tech","media","retro","cds","dvds"]},{"title":"Downsides of untitled RSS items","contentType":"notes","date":"2023-02-23 21:11","url":"/notes/mastodon-untitled-rss-items/","content":"<p>Although titles are optional for RSS feed items and it's a common practice for microblog platforms like Mastodon to exclude them, it unfortunately means some clients like Elfeed don't know how to render those posts.</p>\n<p><img src=\"/files/images/elfeed-mastodon-rss.png\" class=\"img-fluid\" alt=\"Screenshot of elfeed displaying RSS items from Mastodon\" /></p>\n","tags":["mastodon","rss","emacs"]},{"title":"Mastodon 4.1.0 released","contentType":"notes","date":"2023-02-11 10:09","url":"/notes/mastodon-4-1-0-release/","content":"<p>Mastodon <a href=\"https://github.com/mastodon/mastodon/releases/tag/v4.1.0\">v4.1.0</a> was released yesterday. This was a highly anticipated release for me since it introduced the option to <a href=\"https://github.com/mastodon/mastodon/pull/22149\">remove old cached profile media</a> which was taking up a significant portion of my disk space.</p>\n<p>After upgrading, I was able to cut my used disk space in half.</p>\n<p>If you're running your own instance or have the ability to contact your instance admin, I can't recommend upgrading to this version enough.</p>\n","tags":["mastodon","fediverse"]},{"title":"Testing untagged notes","contentType":"notes","date":"2023-02-03 13:05","url":"/notes/test-untagged-notes/","content":"<p>Testing untagged notes which is the same as notes with less than three tags</p>\n","tags":[]},{"title":"Testing titled and tagged notes","contentType":"notes","date":"2023-02-03 12:58","url":"/notes/test-titled-notes/","content":"<p>Testing whether notes appear with title</p>\n","tags":["microblog","posse","web"]},{"title":"Skype Feature Request - Default Encrypted Conversations","contentType":"notes","date":"2023-01-30 17:25","url":"/notes/skype-feature-request-default-e2ee/","content":"<p>As the feedback post suggests, Skype offers the option to have <a href=\"https://support.skype.com/en/faq/FA34824/what-are-skype-private-conversations\">encrypted conversations (chat &amp;amp; calls) using the Signal protocol</a>. However, it's not the detault. If you're using Skype and this is something you'd like added, feel free to upvote the feedback request.</p>\n<p><a href=\"https://feedbackportal.microsoft.com/feedback/idea/0b770e9e-bca0-ed11-a81b-002248519701\">https://feedbackportal.microsoft.com/feedback/idea/0b770e9e-bca0-ed11-a81b-002248519701</a></p>\n","tags":["communication","messsaging","privacy","skype"]},{"title":"Microsoft Forms Test Embedding","contentType":"notes","date":"2023-01-30 11:46","url":"/notes/ms-forms-embed-test/","content":"<p>Testing whether my site can handle embedding <a href=\"https://forms.office.com/\">Microsoft Forms</a>. Useful for polls across a variety of channels.</p>\n<iframe width=\"640px\" height=\"480px\" src=\"https://forms.office.com/Pages/ResponsePage.aspx?id=DQSIkWdsW0yxEjajBLZtrQAAAAAAAAAAAAMAANHhN2pUQTZMR1VCUUxDUDNGSFUwWTZLWkdDQklVSC4u&amp;embed=true\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" style=\"border: none; max-width:100%; max-height:100vh\" allowfullscreen webkitallowfullscreen mozallowfullscreen msallowfullscreen> </iframe>\n","tags":["forms","microsoft","polls","web","indieweb"]},{"title":"i3-gaps Released","contentType":"notes","date":"2023-01-26 10:46","url":"/notes/i3-gaps-released/","content":"<p><img src=\"https://i3wm.org/docs/gaps1920.png\" class=\"img-fluid\" alt=\"Screenshot of i3-gaps\" /></p>\n<p>Nice! <a href=\"https://i3wm.org/docs/userguide.html#gaps\">i3-gaps</a> was recently merged into the main i3 codebase some  time ago and with the <a href=\"https://github.com/i3/i3/blob/next/RELEASE-NOTES-4.22\">4.22 release</a> a few weeks ago, you can now use it with your existing i3 configuration. I've started tinkering with it and I like the way it makes my desktop look.</p>\n","tags":["i3","windowmanager","linux","desktop"]},{"title":"New IndieWeb Wiki .NET Page","contentType":"notes","date":"2023-01-25 21:14","url":"/notes/indieweb-wiki-dotnet-page/","content":"<p>Just created a new <a href=\"https://indieweb.org/.NET\">.NET page on the IndieWeb Wiki</a>. If your personal website or blog is using .NET, feel free to add it there. Also, if you're building any services or libraries for the IndieWeb, add them as well.</p>\n<p>Looking to learn more about IndieWeb? <a href=\"https://indiewebify.me/\">https://indiewebify.me/</a> or the <a href=\"https://indieweb.org/Getting_Started\">IndieWeb Wiki</a> are great places to get started.</p>\n","tags":["indieweb","wiki","dotnet","community"]},{"title":"Removing avatars and headers in Mastodon","contentType":"notes","date":"2022-12-26 17:00","url":"/notes/mastodon-prune-profiles-media-vnext/","content":"<p>The next version of Mastodon can't come soon enough.</p>\n<p>About 20GB of disk space on my instance are being taken up by media from remote users. And this is for a single user instance! I can't imagine how it must be on other instances with multiple accounts.</p>\n<p><img src=\"/files/images/mastodon-media-usage.png\" class=\"img-fluid\" alt=\"Command line displaying disk space used by Mastodon for media\" /></p>\n<p>A recent <a href=\"https://github.com/mastodon/mastodon/pull/22149\">PR</a> should make things much better though by allowing you to remove these files.</p>\n<p>The only thing you need to do is pass the <code>--prune-profiles</code> flag to the <code>media remove</code> CLI command.</p>\n<p>The PR is already merged so hopefully that means it's coming the next release.</p>\n","tags":["mastodon","fediverse","selfhost","sysadmin"]},{"title":"Upgraded to Mastodon v4.0.2","contentType":"notes","date":"2022-12-26 16:03","url":"/notes/upgraded-mastodon-v4-0-2/","content":"<p>Upgrading my server to 4.0.2 wasn't bad, especially considering I was coming from a 3.x version. Also posting this to make sure that there have been no changes to the API and my website can still post to Mastodon on my behalf.</p>\n","tags":["mastodon","selfhost","fediverse","upgade","sysadmin"]},{"title":"Fitch: CLI system information utility built with F#","contentType":"notes","date":"2022-12-25 20:10","url":"/notes/fitch-fsharp-cli-v0/","content":"<p>I wasn't really planning on this, but here we are. I've been using <a href=\"https://github.com/dylanaraps/neofetch\">Neofetch</a> on my PC to display system information whenever the terminal opens. However, in some cases, that would hang and it would take a while before I was able to use my terminal.</p>\n<p>I then ran into <a href=\"https://github.com/unxsh/nitch\">Nitch</a> which is simple and fast. However, it was missing the local and public IP information Neofetch provides out of the box. Nitch is written using the <a href=\"https://nim-lang.org/\">Nim</a> programming language. Originally, I had planned on extending Nitch and I figured out the code I needed to write to support IP information. However, my program wouldn't build and I was unable to find any information online to unblock myself.</p>\n<p>That's when I looked at the existing components in Nitch and found that it was basically just reading files from the <code>/proc</code> and <code>/etc</code> system directories. At that point, I decided to see if I could build my own utility using F#. A couple of hours later and the result was <a href=\"/github/fitch\">Fitch</a>. This works well enough for my needs and in the future, I might publish it as a dotnet global tool to make it easy for others to use.</p>\n<p><img src=\"/files/images/fitch-display.png\" class=\"img-fluid\" alt=\"Screeenshot of command line running Fitch F# System Utility\" /></p>\n","tags":["dotnet","fsharp","linux","system","utilities","cli","fitch"]},{"title":"Use Azure CDN from Verizon Premium for Regex URL redirects","contentType":"notes","date":"2022-12-21 15:09","url":"/notes/azure-cdn-regex-url-redirect/","content":"<p>Well, that's time that I'm never going to get back.</p>\n<p>Imagine that you wanted to redirect a URL,</p>\n<p>From: <code>https://mydomain/github/repo-name</code><br />\nTo: <code>https://github.com/username/repo-name</code></p>\n<p>You could use regular expressions to parse out the <code>repo-name</code> part of the URL path from the source URL to the destination URL. This is what it might look like using something like NGINX.</p>\n<pre><code class=\"language-bash\">location ~ ^/github/(.*) {\n     return 307 https://github.com/yourGHusername/$1;\n}\n</code></pre>\n<p>Unless I wasn't looking at the right documentation, it's not obvious that the rules engine for most Azure CDN plans don't support this feature, except for the <a href=\"https://learn.microsoft.com/azure/cdn/cdn-verizon-premium-rules-engine-reference-features\">Verizon Premium</a>. While the standard rules engine supports wildcards for source URLs, lack of regular expresssions support means there's no way to parameterize the values of the destination URL. If this is something  you need, make sure to use the Verizon Premium plan.</p>\n","tags":["azure","azurecdn","redirect","indieweb","selfhosting"]},{"title":"Now accepting webmentions","contentType":"notes","date":"2022-12-18 22:41","url":"/notes/now-accepting-webmentions/","content":"<p>It took me a while to get all the pieces working but I'm excited that my website now accepts <a href=\"https://www.w3.org/TR/webmention/\">webmentions</a>!</p>\n<p>A few months ago I implemented <a href=\"/notes/webmentions-partially-implemented/\">sending webmentions</a>.</p>\n<p>Accepting them was a bit tricky because there were a few design decisions I was optimizing for.</p>\n<p>In the meantime, you can check out <a href=\"https://github.com/lqdev/WebmentionFs\">WebmentionFs</a>, an F# library I built for validating and receiving webmentions along with the <a href=\"https://github.com/lqdev/WebmentionService\">WebmentionService</a> Azure Functions backend that I'm using to process webmentions for my website.</p>\n<p>Also, feel free to send me a webmention. Here's my endpoint for now <a href=\"https://lqdevwebmentions.azurewebsites.net/api/inbox\">https://lqdevwebmentions.azurewebsites.net/api/inbox</a>. Check out this <a href=\"/posts/sending-webmentions-fsharp-fsadvent/\">blog post</a> where I show how you can send webmentions using F#. You can use any HTTP Client of your choosing to send an HTTP POST request to that endpoint with the body containing a link to one of my articles (target) and the link to your post (source).</p>\n<p>Stay tuned for a more detailed post coming on December 21st!</p>\n","tags":["indieweb","webmentions","community","fsharp","azurefunctions","azure"]},{"title":"TIL: Firefox Task Manager","contentType":"notes","date":"2022-12-01 11:24","url":"/notes/til-firefox-task-manager/","content":"<p>Today I learned about <a href=\"https://support.mozilla.org/en-US/kb/task-manager-tabs-or-extensions-are-slowing-firefox\">Firefox Task Manager</a>. Typing <code>about:performance</code> into the address bar brings up the task manager which lets you inspect the tabs and extensions consuming the most energy and resources.</p>\n<p><img src=\"/files/images/firefox-task-manager.png\" class=\"img-fluid\" alt=\"Firefox Task Manager\" /></p>\n","tags":["firefox","browser","tools","web","internet","opensource"]},{"title":"Use your domain for verification everywhere","contentType":"notes","date":"2022-11-22 10:19","url":"/notes/own-your-links-domain-verification/","content":"<p>I've never had a blue checkmark on Twitter so I can't say what that process is like. Looking at it from the outside though it seems the process isn't simple or straightforward. My experience with verification has been with other applications such as <a href=\"https://docs.joinmastodon.org/user/profile/#verification\">Mastodon</a> and <a href=\"https://indieauth.com/\">IndieAuth</a>. In both of these cases, the main verification techniques use your domain and/or the <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Link_types/me\"><code>rel=me</code> link type</a>. Although with Mastodon you can add multiple verified links to your profile, verification on those external domains pose challenges which include but are not limited to:</p>\n<ul>\n<li>You can't edit the HTML directly so you can't add a <code>rel=me</code> link.</li>\n<li>You can only add one link, so you're forced to choose between your website and Mastodon.</li>\n<li>In scenarios where multiple links (i.e. e-mail, webpage, Twitter) are supported, Mastodon isn't one of the options</li>\n</ul>\n<p>So how do you overcome these challenges? By using your domain.</p>\n<p>A few months ago I wrote about <a href=\"/posts/static-website-redirects\">owning your links</a>. The general concept of owning your links is, using your domain to redirect to other properties and profiles you own. For example, instead of telling people to visit my Twitter profile at <em>twitter.com/ljquintanilla</em>, I can instead point them to <em>lqdev.me/twitter</em>. Not only does my domain become the source of truth, but in general it's easier to remember compared to usernames and proprietary domains. Since my site is statically generated, I own my links by using <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element/meta#attr-http-equiv\">http-equiv meta tags</a>. Although the only job for these pages is to redirect to the specified site, in the end they are still HTML pages. This means, I can embed any other HTML I want, including <code>rel=me</code> links. By including the <code>rel=me</code> link in the redirect page, you satisfy the requirements of applications like Mastodon because that's the page they'll land on before being redirected to the target site. As a result, I can verify my GitHub, Twitter, or any other online property I want with Mastodon using my own domain without having to make any changes or overcome any of the challenges imposed by those other platforms. What you end up with is something like this:</p>\n<p><img src=\"/files/images/mastodon-profile-verified-links.png\" class=\"img-fluid\" alt=\"Screenshot of lqdev profile verified links\" /></p>\n<p>Before signing off, I'll note that this isn't a perfect solution and technically anyone could do the same thing on their own domain in an attempt to impersonate you, so just be aware of that.</p>\n","tags":["indieweb","ownyourlinks","internet","domain","identity","technology","ownership"]},{"title":"Configured GitHub Codespaces Dev Container with .NET 7","contentType":"notes","date":"2022-11-21 11:28","url":"/notes/net7-devcontainer-gh-codespaces/","content":"<p>Soon after .NET 7 was released, I <a href=\"/notes/net7-website-update\">upgraded</a> the static site generator I use as well as the GitHub Actions that build and publish my website. Having upgraded last year from .NET 6, the process was as smooth as I had expected with no code refactoring required.</p>\n<p>When it comes to development environments, for quick status updates like the ones on my <a href=\"/feed\">feed</a> or minor edits, I've been using <a href=\"/notes/surface-duo-blogging-github-dev\">github.dev</a>. However, there's been times where I've needed to run and debug code to confirm that my changes work. This is where I hit some of the limitation of github.dev which means unless I set up a Codespace, I have to save my work and move offline to my PC. Codespaces are great, but given that Codespaces are nice to haves not a requirement for my workflow at this time, it didn't make sense for me to pay for them. That's why I was excited to learn that GitHub is providing up to <a href=\"https://github.blog/2022-11-10-whats-new-with-codespaces-from-github-universe-2022/\">60 hours per month of free Codespace usage</a> to all developers. That's more than enough for me.</p>\n<p><img src=\"/files/images/net7-gh-codespaces.png\" class=\"img-fluid\" alt=\"Blog post authored in GitHub Codespaces with integrated terminal open\" /></p>\n<p>By default, Codespace images come preinstalled with the .NET 6 SDK which makes sense considering it's the latest LTS. However, since my static site generator targets .NET 7, I had to configure my Codespace to use .NET 7. This was just as easy as upgrading to .NET 7. All I had to do was provide the .NET 7 SDK Docker image as part of my <a href=\"https://github.com/lqdev/luisquintanilla.me/blob/main/.devcontainer.json\">devcontainer.json</a> configuration file. From there, Codespaces takes care of the rest. As a result, I can now run and debug my code all in one place without interrupting my workflow.</p>\n<p>PS: This post was authored in GitHub Codespaces 🙂</p>\n","tags":["dotnet","github","codespaces","development","devcontainer","website","internet"]},{"title":"The Fediverse beyond Mastodon","contentType":"notes","date":"2022-11-20 19:28","url":"/notes/fediverse-beyond-mastodon/","content":"<p>It's been great to see many old and new friends joining Mastodon and considering alternatives to online communities over the past few weeks. Selfishly my feed had been quiet since I started using it and self-hosting back in 2018 so it's nice to see more activity. As folks have started to settle in and get a better understanding of Mastodon, I'd like to point out that Mastodon is only one of many apps and services on the <a href=\"https://fediverse.party/en/fediverse/\">Fediverse</a>. Now I won't try to explain the Fediverse but in general, it's the collection of applications and services that interoperate in a federated manner, Mastodon being one of them. If in general you like Mastodon and are looking for alternatives to services like Instagram, YouTube, SoundCloud, the Fediverse has a few options:</p>\n<ul>\n<li>Instagram ➡️ <a href=\"https://pixelfed.org/\">Pixelfed</a></li>\n<li>YouTube ➡️ <a href=\"https://joinpeertube.org/\">PeerTube</a></li>\n<li>SoundCloud ➡️ <a href=\"https://funkwhale.audio/\">Funkwhale</a></li>\n<li>Twitch ➡️ <a href=\"https://owncast.online/\">Owncast</a></li>\n</ul>\n<p>You can create an account for each of these services using different instances, just like Mastodon. You also have the option of self-hosting which I find amazing. However, because for the most part these services implement the ActivityPub protocol, it means federation extends beyond Mastodon. You can follow people on PixelFed, PeerTube, and many of these other services using your Mastodon account which means you don't have to create accounts for those services just to follow people. While I don't have experience with all of these services, I recommend checking them out. I especially like what Pixelfed, PeerTube and Owncast are doing but those are the main ones I have experience with.</p>\n","tags":["fediverse","mastodon","decentralization","owncast","peertube","pixelfed","funkwhale","alternatives"]},{"title":"Upgraded website to .NET 7","contentType":"notes","date":"2022-11-11 09:59","url":"/notes/net7-website-update/","content":"<p>Last year, I posted about <a href=\"/notes/net6-website-update\">upgrading this website to .NET 6</a> and how easy that process was. I just had a chance to upgrade to .NET 7 and the experience was just as smooth. All I had to do was change the version in two places:</p>\n<p><strong>GitHub Actions *.yml file</strong></p>\n<pre><code class=\"language-yml\">dotnet-version: '7.0.x'\n</code></pre>\n<p><strong>*.fsproj  file</strong></p>\n<pre><code class=\"language-xml\">&amp;lt;TargetFramework&amp;gt;net7.0&amp;lt;/TargetFramework&amp;gt;\n</code></pre>\n<p>I'm happy that with such a simple change, I can get all the benefits of .NET 7 without doing a lot of work.</p>\n","tags":["dotnet","website","net7","upgrade"]},{"title":"Penn State future class looks promising","contentType":"notes","date":"2022-11-05 19:08","url":"/notes/psu-future-allar/","content":"<p>Slow start but good bounce back by Penn State after two straight losses. This game also gave a good glimpse of the future class and it looks promising especially at the QB and RB spots. If they can stay healthy, they may be playoff contenders in a couple of years.</p>\n","tags":["psu","football","college","ncaa","ncaafb","pennstate","collegefootball"]},{"title":"Penn State vs Ohio State","contentType":"notes","date":"2022-10-29 11:58","url":"/notes/psu-osu-2022/","content":"<p><strong>11:58</strong></p>\n<p>Hoping for a good game.</p>\n<p><img src=\"https://media.giphy.com/media/GCSIwtwqAMBTq/giphy.gif\" class=\"img-fluid\" alt=\"Michael Scott ready to get hurt again GIF\" /></p>\n<hr />\n<p><strong>12:22</strong></p>\n<p>Defense is clutch. As always in big games, they'll have to work overtime just to keep the offense in the game.</p>\n<hr />\n<p><strong>12:42</strong></p>\n<p>Progress...at least there was no turnover on the last series.</p>\n<hr />\n<p><strong>12:59</strong></p>\n<p><img src=\"https://media.giphy.com/media/b6iVj3IM54Abm/giphy.gif\" class=\"img-fluid\" alt=\"Undetaker rising from dead GIF\" /></p>\n<hr />\n<p><strong>13:16</strong></p>\n<p><img src=\"https://media.giphy.com/media/y2i2oqWgzh5ioRp4Qa/giphy.gif\" class=\"img-fluid\" alt=\"Football player they had us in first half GIF\" /></p>\n<hr />\n<p><strong>14:53</strong></p>\n<p><img src=\"https://media.giphy.com/media/4bWWKmUnn5E4/giphy.gif\" class=\"img-fluid\" alt=\"Sweating GIF\" /></p>\n<hr />\n<p><strong>15:09</strong></p>\n<p>Going for it on 4th &amp;amp; 2 feels like that 4th &amp;amp; 5 from a few years ago.</p>\n<hr />\n<p><strong>15:30</strong></p>\n<p>Defense showed up today. Can't expect to win a game with 3 turnovers.</p>\n<p><img src=\"https://media.giphy.com/media/WoQmhbWcr7LoODs8a4/giphy.gif\" class=\"img-fluid\" alt=\"Spongebob head out GIF\" /></p>\n<hr />\n<p><strong>15:33</strong></p>\n<p>This is hard to watch.</p>\n<p><img src=\"https://media.giphy.com/media/qgri3D9sTwCUGMcT8L/giphy.gif\" class=\"img-fluid\" alt=\"DJ Khaled Another One GIF\" /></p>\n","tags":["psu","osu","pennstate","ohiostate","ncaafb","football","ncaa","collegefootball"]},{"title":"Impressed by Azure Cloud Shell","contentType":"notes","date":"2022-10-13 22:45","url":"/notes/impressed-az-cloud-shell/","content":"<p>I can probably count the number of times I've used the Azure Cloud Shell inside the Azure Portal with one hand. However, I recently had a need to use it and was impressed by the experience.</p>\n<p>I wanted to create an Azure Function app but didn't want to install all the tools required. Currently I'm running Manjaro on an ASUS L210MA. That's my daily driver and although the specs are incredibly low I like the portability. The Azure set of tools run on Linux but are easier to install on Debian distributions. The low specs plus the Arch distro didn't make me want to go through the process of installing the Azure tools locally for a throwaway app.</p>\n<p>That's when I thought about using Azure Cloud Shell. Setup was relatively quick and easy. It was also great to see the latest version of .NET, Azure Functions Core Tools, and Azure CLI were already installed. Even Git was already there. After creating the function app, I was able to make some light edits using the built-in Monaco editor. When I was ready to test the app, I was able to open up a port, create a proxy, and test the function locally. It's not a high-end machine so I won't be training machine learning models on it anytime soon. However, for experimentation and working with my Azure account it's more than I need.</p>\n","tags":["azure","cloud","tools","development"]},{"title":"TIL: Automaticc owns Tumblr","contentType":"notes","date":"2022-10-09 20:49","url":"/notes/til-automaticc-tumblr/","content":"<p>Last I heard of Tumblr, they were owned by Verizon. While readying the book <a href=\"https://book.micro.blog/\">Indie Microblogging</a> by Manton Reece, I learned (and later confirmed) they were owned by Automaticc, the company behind WordPress. Out of the the different homes it's had, I can't think of a better place for it.</p>\n","tags":["tumblr","automattic","til","socialmedia","blogging"]},{"title":"Terrifier 2 - 100/10","contentType":"notes","date":"10/09/2022 14:21","url":"/notes/terrifier-2-100-10/","content":"<p><a href=\"https://www.terrifier2themovie.com/home/\">Terrifier 2</a> did not disappoint. 100/10. Can't wait for Terrifier 3.</p>\n<p><img src=\"https://media.tenor.com/029LgfMnsisAAAAM/applause-leonardo-dicaprio.gif\" class=\"img-fluid\" alt=\"Leo DiCaprio clapping\" /></p>\n","tags":["terrifier","horror","movie","art","terrifier2"]},{"title":"ALMOST ready to receive Webmentions","contentType":"notes","date":"2022-10-06 22:29","url":"/notes/almost-ready-to-receive-webmentions/","content":"<p>Finally got a working prototype. A few days ago I added the <a href=\"/resources/snippets/webmentions-request-verification\">request</a> and <a href=\"/resources/snippets/webmentions-verification\">webmention</a> verification components.</p>\n<p>I just finished creating the web API and Azure Table Storage components. Now I just need to run some additional tests and deploy it 🙂</p>\n<p><img src=\"https://media.tenor.com/cWNpE2earJkAAAAd/chris-pratt-rubbing-hands.gif\" class=\"img-fluid\" alt=\"Chriss Pratt Rubbing Hands GIF\" /></p>\n","tags":["webmentions","indieweb","comments"]},{"title":"Personal Feed - One Year In","contentType":"notes","date":"09/29/2022 10:55","url":"/notes/feed-one-year/","content":"<p>It's been a year since I created this feed and <a href=\"/notes/hello-world\">started posting</a> mainly on my website. Since then, I've:</p>\n<ul>\n<li>Used a <a href=\"/notes/lqdevme-redirect\">shorter domain</a> to redirect to my website domain.</li>\n<li>Created <a href=\"/posts/automate-yaml-front-matter-vs-code-snippets/\">VS Code snippets</a> to simplify metadata tagging.</li>\n<li>Used <a href=\"/notes/surface-duo-blogging-github-dev\">github.dev</a> as the main interface for authoring and editing posts. For longer posts, I use VS Code locally but for these smaller microblog-style posts, github.dev makes it really easy.</li>\n<li>Syndicated posts to Twitter and <a href=\"/notes/mastodon-posse-enabled\">Mastodon</a>.</li>\n<li>Created a <a href=\"/feed/responses\">response feed</a> for interactions like replies, reshares (repost), and likes (favorites) with support for sending <a href=\"/notes/webmentions-partially-implemented/\">Webmentions</a>.</li>\n<li>Created <a href=\"/feed\">RSS feeds</a> for each of my feeds.</li>\n</ul>\n<p>It hasn't happened overnight. Instead, it's been the result of small incremental efforts over time.</p>\n<p>Overall, I don't think I've posted any more or less than I do on other platforms.</p>\n<p>What I have enjoyed the most though has been:</p>\n<ul>\n<li>Learning.</li>\n<li>Building the tools and processes to author and share content.</li>\n<li>Owning my content. My website acts as the single source of truth.</li>\n<li>Not requiring others to have or create accounts on individual platforms to access content.</li>\n<li>Choosing how I author content. I'm sure this post is way over the 280 character limit and if there's a typo, I can just edit the post and republish the website 🙂</li>\n</ul>\n<p>Going forward, I plan to:</p>\n<ul>\n<li>Update RSS feeds to include post content, not just link to the post.</li>\n<li>Accept Webmentions. I'm about halfway done with the main parts of my <a href=\"https://github.com/lqdev/luisquintanilla.me/blob/main/_scratch/receive-web-mentions.md\">implementation</a>.</li>\n<li>Implement tags for easier discoverability / search.</li>\n<li>Consolidate metadata for posts (articles, microblogs, responses).</li>\n</ul>\n","tags":["microblog","blog","feed","rss","indieweb","openweb","internet"]},{"title":"Webmentions (partially) implemented","contentType":"notes","date":"09/17/2022 19:59","url":"/notes/webmentions-partially-implemented/","content":"<p>Success! I just partially implemented <a href=\"https://www.w3.org/TR/webmention/\">Webmentions</a> for my website. Although I haven't figured out a good way to receive Webmentions yet, I'm able to send them. Fortunately most of the work was done, as detailed in the post <a href=\"/posts/sending-webmentions-fsharp-fsadvent/\">Sending Webmentions with F#</a>. The rest was mainly a matter of adapting it to my static site generator.</p>\n<p>Below is an example of a <a href=\"/responses/webmention-test-1/\">post on my website</a> being displayed in the <a href=\"https://webmention.rocks/test/1\">Webmention test suite website</a>.</p>\n<p><strong>Source</strong></p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/190879250-4554750f-b435-4627-bad9-ecc3d96f9ed0.png\" class=\"img-fluid\" alt=\"Webmention on lqdev.me\" /></p>\n<p><strong>Target</strong></p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/190879274-f6566225-2173-4213-a3d0-eeb9fdc67df9.png\" class=\"img-fluid\" alt=\"Webmention displayed in webmention test suite\" /></p>\n<p>What does this mean? It means I can comment on any website I want, regardless of whether they allow comments or not. As if that weren't enough, I have full ownership of my content as my website is the single source of truth. As a bonus, if the website I comment on supports receiving Webmentions, my post will be displayed on their website / articles as a comment. The next step is to handle deleted comments, but so far I'm happy with the progress.</p>\n","tags":["indieweb","webmentions","fsharp","website"]},{"title":"New (LTE) Rotary Phone","contentType":"notes","date":"09/06/2022 19:37","url":"/notes/lte-rotary-phone/","content":"<p>Very cool!</p>\n<p><a href=\"https://yewtu.be/embed/uV1C-41tq64\" title=\"New (LTE) Rotary Phone\"><img src=\"https://yewtu.be/vi/uV1C-41tq64/maxres.jpg\" class=\"img-fluid\" alt=\"New (LTE) Rotary Phone\" /></a></p>\n","tags":["project","phone","dumbphone"]},{"title":"Praying for OSU downfall","contentType":"notes","date":"09/03/2022 20:09","url":"/notes/pray-ohio-state-football-downfall-nd-tweet/","content":"<p>Same... 🙏</p>\n<blockquote class=\"blockquote\">\n<p>Praying on Ohio State's downfall</p>— Barstool Penn State (<a href=\"https://twitter.com/PSUBarstool\">@PSUBarstool</a>) September 3, 2022</p>\n</blockquote>\n<p>Source: <a href=\"https://twitter.com/PSUBarstool/status/1566230451378950144\">https://twitter.com/PSUBarstool/status/1566230451378950144</a></p>\n","tags":["psu","osu","pennstate","ohiostate","ncaafb","football","ncaa","collegefootball"]},{"title":"TIL: VS Code Live Preview Extension","contentType":"notes","date":"09/03/2022 18:47","url":"/notes/til-vs-code-live-preview-extension/","content":"<p>Today I Learned about the <a href=\"https://marketplace.visualstudio.com/items?itemName=ms-vscode.live-server\">VS Code Live Preview extension</a>. While reading the <a href=\"https://code.visualstudio.com/updates/v1_71\">release notes</a> for the latest version of VS Code, I noticed a <a href=\"https://code.visualstudio.com/updates/v1_71#_live-preview\">note</a> on the Live Preview extension. Usually to preview static content, I use Python's HTTP server with the <code>python -m http.server</code> command. This extension simplifies the process because it's built into Visual Studio Code. If you do any kind of web development, it's definitely worth a look.</p>\n","tags":["til","vscode","vs","extension","development","web"]},{"title":"1-0 Penn State survives Purdue","contentType":"notes","date":"09/01/2022 23:18","url":"/notes/psu-survives-purdue/","content":"<p>College football is definitely back. I don't miss those 4th quarter heart attacks, but a win is a win.</p>\n","tags":["pennstate","purdue","football","college","ncaafb","ncaa","collegefootball"]},{"title":"QRZ New Ham Jumpstart Program","contentType":"notes","date":"08/31/2022 21:20","url":"/notes/qrz-new-ham-jumstart-program/","content":"<p>Very cool! QRZ just announced the <a href=\"https://forums.qrz.com/index.php?threads/announcing-the-qrz-new-ham-jumpstart-program.823607/\">New Ham Jumpstart Program</a>, a new program to make it easier for new ham radio operators to quickly get on the air.</p>\n","tags":["radio","qrz","hamradio","fcc"]},{"title":"Dall-E Outpainting and generative models","contentType":"notes","date":"08/31/2022 17:49","url":"/notes/open-ai-dall-e-outpainting-generative-models/","content":"<p>Things are heating up in the AI generative model space. OpenAI just announced <a href=\"https://openai.com/blog/dall-e-introducing-outpainting/\">Outpainting</a>. A few days ago, Stability.ai released <a href=\"https://stability.ai/blog/stable-diffusion-public-release\">Stable Diffusion</a> and recently a painting generated using <a href=\"https://www.midjourney.com/home/\">Midjourney</a> won <a href=\"https://arstechnica.com/information-technology/2022/08/ai-wins-state-fair-art-contest-annoys-humans/\">first place</a> in the digital category of the Colorado State Fair fine art competition.</p>\n","tags":["dalle","openai","genai","ai"]},{"title":"Terrifier 2","contentType":"notes","date":"08/23/2022 19:57","url":"/notes/terrifier-2-trailer/","content":"<p>I know what I'm doing October 6th.</p>\n<p><a href=\"https://yewtu.be/embed/6KkONLf_ZKU\" title=\"Terrifier 2 Trailer\"><img src=\"https://yewtu.be/vi/6KkONLf_ZKU/maxres.jpg\" class=\"img-fluid\" alt=\"Terrifier 2\" /></a></p>\n","tags":["terrifier","horror","movie","art","terrifier2"]},{"title":"World's Largest Smartphone Camera","contentType":"notes","date":"07/07/2022 21:17","url":"/notes/worlds-largest-smartphone-camera/","content":"<p>Cries in Lumia 1020 <span>😢</span></p>\n<p><img src=\"https://yewtu.be/vi/lYPe4MsALk4/maxres.jpg\" class=\"img-fluid\" alt=\"Thumbnail Marques Brownlee World's Largest Smartphone Camera\" /></p>\n<p>Source: <a href=\"https://yewtu.be/embed/lYPe4MsALk4\">World's Largest Smartphone Camera | Marques Brownlee</a></p>\n","tags":["camera","tech","smartphone"]},{"title":"Terry Carnation","contentType":"notes","date":"07/03/2022 22:04","url":"/notes/terry-carnation/","content":"<p>Can't believe I hadn't heard about these podcasts before:</p>\n<ul>\n<li><a href=\"https://radiorentalusa.com/\">Radio Rental</a></li>\n<li><a href=\"https://audioboom.com/channels/5046661\">Dark Air</a></li>\n</ul>\n<p>Love the horror anthology vibes like Tales From The Crypt and Freakshow for Radio Rental. For Dark Air, I really like the Space Ghost Coast to Coast and actual Coast to Coast feel. Best of all, the host Terry Carnation is Rainn Wilson. Even <a href=\"https://www.terrycarnation.com/\">Terry's website</a> has a great 90's web feel to it. Looks like Dark Air has been off the air for some time, but hopefully Radio Rental keeps publishing content.</p>\n","tags":["podcast","terrycarnation","radiorental","darkair"]},{"title":"iPod Touch discontinued","contentType":"notes","date":"05/10/2022 12:27","url":"/notes/ipod-touch-discontinued/","content":"<p>Apple just announced they're discontinuing the <a href=\"https://www.apple.com/newsroom/2022/05/the-music-lives-on/\">iPod Touch</a>. I'm surprised it hadn't happened earlier! Still, it's sad to see the end of an era for these devices which are effectively smartphones without the phone / cellular data connectivity. I still have my iPod Touch from 2014 that I use occasionally for FaceTime which aside from poor battery life and being stuck on an older version of iOS, it still works smoothly.</p>\n<p>For the past two years, my audio solution has been a dedicated MP3 device. Since most of the content I consume I download and listen to offline, a dedicated MP3 device doesn't drain my phone's battery or take up storage space.</p>\n<p>I started with the <a href=\"https://www.fiio.com/m5\">FiiO M5</a>, which is like an iPod Shuffle. The device was great, but all it took was one drop for the screen to crack at which point I upgraded to the <a href=\"https://www.fiio.com/m6\">FiiO M6</a>. The nice thing about the M6 is that the OS is a stripped down version of Android meaning you can <a href=\"https://www.fiio.com/newsinfo/139014.html\">side-load applications</a> such as Spotify. The bad thing, the hardware can't handle (even heavily optimized versions) of these applications making the user experience frustrating. The touchscreen interface is great for navigation especially when you have large collections. However, since I only use the device for podcasts and audiobooks and don't get the benefits of installing streaming music apps, I'd be better off with a more affordable device like those from <a href=\"https://www.westerndigital.com/c/mp3-players\">SanDisk</a>.</p>\n","tags":["apple","ipod","music","ipodtouch"]},{"title":"A year's worth of Azure hosting costs","contentType":"notes","date":"2022-05-04 20:36","url":"/notes/azure-costs-website-hosting-2021/","content":"<p>It's been a little over a year since I started hosting my website on Azure. The resources I use include:</p>\n<ul>\n<li>Azure Storage: Where all my site assets are stored.</li>\n<li>Azure CDN: Content delivery</li>\n<li>Azure Logic Apps: Event-based jobs to syndicate new posts on Twitter and Mastodon.</li>\n</ul>\n<p>I haven't tracked traffic for the entire year but since October of last year the site has gotten roughly 5,000 visitors.</p>\n<p>Taking all that into account, I've only had to pay $2.06 most of it being the storage I use for the site. In comparison, I was previously hosting my website through my domain provider in a shared server where it cost about $60!</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/166848883-8d6cfe2c-5ff3-468c-b95f-b0ecab69595a.png\" class=\"img-fluid\" alt=\"Azure portal billing cost\" /></p>\n<p>If you're interested in how this site is built, check out the <a href=\"/colophon\">colophon</a>.</p>\n","tags":["azure","website","cost","hosting","selfhosting","cloud","indieweb"]},{"title":"Chip-making Chip Shortage","contentType":"notes","date":"05/03/2022 18:50","url":"/notes/chipmaking-chip-shortage/","content":"<p><a href=\"https://9to5mac.com/2022/05/03/chips-for-chipmaking-machines/\">Chips for chipmaking machines are affected by chip shortage, says TSMC and Intel - via 9to5Mac</a></p>\n<p><img src=\"https://res-5.cloudinary.com/hdwvnfa8d/image/upload/q_auto/v1/ghost-blog-images/Spider-Man-Pointing-Meme.jpg\" class=\"img-fluid\" alt=\"Spiderman meme pointing at each other\" /></p>\n","tags":["chip","pc","hardware","news"]},{"title":"Twitter Circles: Top 150 friends","contentType":"notes","date":"05/03/2022 14:31","url":"/notes/twitter-circles-myspace-top-friends/","content":"<p>Twitter is testing a new feature called <a href=\"https://www.theverge.com/2022/5/3/23055515/twitter-circle-close-friends-private-tweets-150-people\">Twitter Circles</a> where you can restrict the reach of your tweets to up to 150 people. I remember the days of MySpace where many relationships were made / broken as a result of being in someone's Top 8 Friends list <span>😆</span></p>\n","tags":["twitter","myspace","socialmedia"]},{"title":"Mastodon POSSE enabled","contentType":"notes","date":"04/27/2022 21:29","url":"/notes/mastodon-posse-enabled/","content":"<p>Hello Mastodon from lqdev.me. Got <a href=\"https://indieweb.org/POSSE\">POSSE (Publish (on your) Own Site, Syndicate Elsewhere)</a> working for Mastodon! I already had an Azure Logic App which took the latest post from my RSS feed and published to Twitter. Now it's also enabled for Mastodon. Might do a writeup later in the blog on how to do it.</p>\n","tags":["mastodon","fediverse","posse","indieweb"]},{"title":"ATOM Weather Alerts","contentType":"notes","date":"04/27/2022 20:19","url":"/notes/atom-rss-nws-weather-alerts/","content":"<p>Yesterday I ran into more feed goodness. When going to a website to get the weather forecast, weather alerts are displayed (if there are any).</p>\n<p>I thought, wouldn't it be great to be able to subscribe to those alerts without having to go to the website?</p>\n<p>A quick search revealed the National Weather Service provides these alerts in ATOM format for each of the states in the U.S.\nYou have the option of tracking updates at the national, state, zone, and county level.</p>\n<p>Subscribing is as easy as adding the link for the respective feed to your favorite feed reader. If you're interested, you can find the list of feeds at <a href=\"https://alerts.weather.gov/\">https://alerts.weather.gov/</a></p>\n","tags":["rss","atom","weather","alerts"]},{"title":"Welcome to the Fediverse","contentType":"notes","date":"04/27/2022 13:04","url":"/notes/welcome-to-fediverse-tips/","content":"<p>In light of recent developments on Twitter, I see more people looking for alternative platforms. Personally, I don't think anything beats the level of customization and control you get from using your own website and RSS for sharing ideas and publishing content. However, it's great to see so many new friends in the Fediverse on platforms like <a href=\"https://joinmastodon.org/\">Mastodon</a>. I won't go into what the Fediverse is but you can check out the <a href=\"https://joinfediverse.wiki/What_is_the_Fediverse%3F\">Fediverse wiki</a> for more info. In many ways the Fediverse will feel familiar, yet foreign. Many people have shared tips on getting started, but I'll share some of the ones I've personally used to organically curate my feed.</p>\n<ul>\n<li>Use hashtags. Similar to Twitter, hashtags are a good way of finding people and content related to the topic of interest.</li>\n<li>Follow <a href=\"https://mstdn.social/@feditips\">feditips</a> and <a href=\"https://mastodon.online/@FediFollows\">FediFollows</a>. As the names suggest, they provide good tips and suggest interesting accounts to follow in the Fediverse.</li>\n</ul>\n<p>As an alternative if you're not ready to create an account, you can use RSS. I'm a big <a href=\"/posts/rediscovering-rss-user-freedom/\">fan of RSS</a>. Appending <code>.rss</code> to any account on platforms like Mastodon (for example <a href=\"https://mastodon.online/@FediFollows.rss\">https://mastodon.online/@FediFollows.rss</a>) provides you with the RSS feed of an account's posts letting you subscribe to their content through your favorite RSS feed reader.</p>\n<p>Feel free to say hi at <a href=\"https://toot.lqdev.tech/@lqdev\">toot.lqdev.tech/@lqdev</a>. See you in the Fediverse!</p>\n","tags":["fediverse","mastodon","twitter","socialmedia","decentralization"]},{"title":"Assorted links - Ubuntu 22.04 LTS, Bitwarden, DALLE-2, Emacs","contentType":"notes","date":"04/21/2022 19:44","url":"/notes/2022-04-21-assorted-links/","content":"<p>Here's a few links I found in the interwebs today that I thought were worth sharing.</p>\n\n<p>For my personal computing needs, Manjaro has been my default Linux distro. However, for hosting and cloud VMs I go back and forth between Debian and Ubuntu depending on which better supports the software I'm running. Canonical announced the release of the latest Ubuntu LTS version 22.04. Check out their <a href=\"https://ubuntu.com/blog/ubuntu-22-04-lts-released\">blog post</a> for more details on what's new.</p>\n\n<p>When it comes to password managers, other than KeePass, my favorite it <a href=\"https://bitwarden.com/\">Bitwarden</a>. It's open-source, built on .NET, has excellent features that get even better with the low-cost paid version, but most importantly provides you the option of self-hosting. If you're looking to self-host Bitwarden on DigitalOcean, make sure to check out the <a href=\"https://bitwarden.com/blog/digitalocean-marketplace/\">guide</a> they just published.</p>\n\n<p>A few weeks ago, OpenAI announced the release of <a href=\"https://openai.com/dall-e-2/\">DALLE-2</a>. Open AI describes DALLE-2 as, &amp;quot;...a new AI system that can create realistic images and art from a description in natural language.&amp;quot; If you want to understand the details of how it works, you could read the 27 page research paper <a href=\"https://arxiv.org/pdf/2204.06125.pdf\">Hierarchical Text-Conditional\nImage Generation with CLIP Latents</a>. AssemblyAI just published an approachable guide into the inner workings of DALLE-2. If you're interested in that, check out their <a href=\"https://www.assemblyai.com/blog/how-dall-e-2-actually-works/\">blog post</a> on the topic.</p>\n\n<p>In my early days of using Emacs, I often found it difficult to customize it to my needs. Not only did I not know what was possible, but it also caused some anxiety because I didn't feel confident enough making major tweaks. If you're getting started with Emacs and aren't sure how to configure it, this neat <a href=\"https://emacs.amodernist.com/\">utility</a> helps you generate a config file for Emacs.</p>\n\n<p>If you're building machine learning applications, one of the main things you'll need is data. This data can reside in databases such as MySQL, SQL Server, and Postgres. Algorithms are applied to this data downstream using libraries written in languages like Python, R, and .NET. This not only means potentially moving the data from the database to another process but also using another library to train models. According to the project description on GitHub, &amp;quot;PostgresML is an end-to-end machine learning system. It enables you to train models and make online predictions using only SQL, without your data ever leaving your favorite database.&amp;quot; If that's something you're interested in, check out the project on <a href=\"https://github.com/postgresml/postgresml\">GitHub</a>.</p>\n","tags":["links","assortedlinks","ubuntu","bitwarden","dalle","openai","emacs","linkblog"]},{"title":"Archive 81 - so far so good!","contentType":"notes","date":"01/15/2022 21:01","url":"/notes/archive-81-black-tapes/","content":"<p>About 2 episodes into the show <a href=\"https://www.netflix.com/title/80222802\">Archive 81</a> on Netflix and so far so good. I listened to a few episodes of the <a href=\"https://www.archive81.com/\">podcast</a> (by the same name) it's based on a few years ago. Now it makes me want to go back and listen to it again. It also makes me think about what the <a href=\"https://theblacktapespodcast.com/\">Black Tapes</a> could've been had it gotten a TV or movie adaptation. If you're into the found footage, cult horror type of stories (think Blair Witch + Midsommar + Rosemary's Baby), definitely check it out.</p>\n","tags":["podcast","tv","archive81","netflix","horror","suspense"]},{"title":"Vertigo AI","contentType":"notes","date":"12/28/2021 15:23","url":"/notes/vertigo-ai/","content":"<p><a href=\"https://en.wikipedia.org/wiki/Vertigo_(film)\">Vertigo</a> is one of my all time favorite movies, so I was excited to learn about <a href=\"http://www.chrispeters.com/vertigo-ai-experiment/\">Vertigo AI</a>. Vertigo AI is an experiment that trained a model on footage from the film Vertigo to produce its own original short film. There aren't many details on the AI aspects of it, but I suspect there's a Generative Adversarial Network (GAN) similar to <a href=\"http://norman-ai.mit.edu/\">Norman</a> behind it. Check out the generated movie.</p>\n<p><a href=\"https://yewtu.be/embed/zeCDqFCGN8Y\" title=\"Vertigo AI\"><img src=\"https://yewtu.be/vi/zeCDqFCGN8Y/maxres.jpg\" class=\"img-fluid\" alt=\"Vertigo AI\" /></a></p>\n","tags":["ai","vertigo","hitchcock","genai"]},{"title":"Site updated to .NET 6","contentType":"notes","date":"12/22/2021 20:27","url":"/notes/net6-website-update/","content":"<p>That was easy. Site's been updated to .NET 6 and all it took was a one-line change.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/147173815-9703691c-26cd-481a-98e6-fa23a9ce9674.png\" class=\"img-fluid\" alt=\"GitHub PR diff .NET 6 update\" /></p>\n","tags":["dotnet","website","net6","upgrade","selfhost"]},{"title":"New Jami username","contentType":"notes","date":"11/19/2021 10:37","url":"/notes/new-jami-username/","content":"<p>I got a reminder on the importance of backups last night. New Jami username is lqdev1. Fortunately I wasn't as active on there so not a big deal to lose my original username.</p>\n","tags":["jami","messaging","opensource","gnu"]},{"title":"TOSDR: Terms of Service CliffNotes","contentType":"notes","date":"11/16/2021 13:10","url":"/notes/terms-of-service-cliffnotes-tosdr/","content":"<p>I recently came accross the <a href=\"https://tosdr.org/\">Terms of Service, Didn't Read (TOSDR)</a> website. Like most people, I'm guilty of clicking &amp;quot;I have read and agree to the terms of service&amp;quot; without actually doing so. TOSDR simplifies terms of service documents by highlighting the main points for many popular services. I'm not sure how maintenance and the frequency of that maintenance work. There's a rating systems which I also am not sure how it works. However, you are provided with the documents that were used to come up with the ratings and summaries, so you can look into it yourself. Overall, I think it's an incredibly useful tool to make sense of how your data is managed across these platforms.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/142041533-e79dc8f4-4ba2-42b3-bbfd-e2becab8f357.png\" class=\"img-fluid\" alt=\"Duck Duck Go and YouTube TOSDR service comparisons\" /></p>\n","tags":["tos","tools","tosdr"]},{"title":"Reminiscing about Windows Phone","contentType":"notes","date":"11/13/2021 21:00","url":"/notes/windows-phone-lumia-reminiscing/","content":"<p>Found these while cleaning out some drawers 😢. Good times.</p>\n<p><img src=\"/files/images/windows-phone-lumias.png\" class=\"img-fluid\" alt=\"Two Nokia Lumia Windows Phones\" /></p>\n","tags":["windowsphone","lumia","nokia","windows","smartphone","phone"]},{"title":"Podcast management with RSS and VLC","contentType":"notes","date":"11/05/2021 10:27","url":"/notes/rss-vlc-podcast-management/","content":"<p>I've written before about how <a href=\"/posts/rediscovering-rss-user-freedom/\">versatile the RSS protocol is for publishing and consuming content</a>. Podcasts are one of the many media types that largely rely on RSS, making it easy to subscribe and manage using an RSS client of your choice. Like RSS, <a href=\"https://www.videolan.org/vlc/\">VLC</a> is versatile in the type of media it can play. When you combine the two, you're able to stay updated on the latest episodes from your favorite podcasts using the RSS reader of your choice and use the episode's URL to stream the audio in VLC.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/140527726-6635cbf3-8082-45d1-9b51-a4e136168abf.png\" class=\"img-fluid\" alt=\".NET Rocks podcast RSS feed and VLC playing\" /></p>\n","tags":["rss","vlc","podcast","media","audio"]},{"title":"Blogging on the Surface Duo with github.dev","contentType":"notes","date":"10/31/2021 13:51","url":"/notes/surface-duo-blogging-github-dev/","content":"<p>Lately I've been using github.dev to author posts and make light edits on this site. One use case where it's come extremely handy has been authoring content, specifically these microblog-like posts from my Surface Duo. Instead of opening the laptop, I can open up my Surface Duo and visit my site's repo through github.dev. When I orient the device in landscape mode I have one screen display the VS Code-like web environment while the other screen displays the keyboard. Snippets, version control, extensions, and themes all carry over making the experience consistent. Committing changes through the version control utilities immediately kicks off the GitHub Action to build and publish the site with the latest changes. Pairing the GitHub online editor environment with the Duo's dual-screen form factor showcases the productivity and convenience both of these products provide.</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/139595618-a2f905dd-e3a0-4f34-8702-2350f548d8c7.png\" class=\"img-fluid\" alt=\"Microblogging on Surface Duo in github.dev\" /></p>\n","tags":["surface","duo","blogging","github","surfaceduo","microsoft"]},{"title":"Personal Site Stats - GoatCounter","contentType":"notes","date":"10/31/2021 11:57","url":"/notes/personal-site-stats-goatcounter/","content":"<p>I've been looking at different alternatives to Google Analytics for website stats. I don't need anything fancy, just basic page counts. Eventually I settled on <a href=\"https://www.goatcounter.com/\">GoatCounter</a> because it's lightweight, open-source, and privacy-friendly. You can either use their managed service or self-host. Deployment with Docker felt a bit much so I did a standalone deployment to a VM. The Linode StackScript is probably the easiest way to do it.</p>\n","tags":["analytics","web","goatcounter","indieweb","internet"]},{"title":"Early Radio History (U.S)","contentType":"notes","date":"10/29/2021 02:05","url":"/notes/early-radio-history-us/","content":"<p>While browsing the interwebs, I ran into the site <a href=\"https://earlyradiohistory.us/\">United States Early Radio History</a> which contains events spanning different time periods. As an operator it's fascinating to learn about the history and see how radio continues to play a large role in our day-to-day.</p>\n","tags":["radio","communication","hamradion","history","unitedstates"]},{"title":"Web Neural Network API - Working Draft","contentType":"notes","date":"10/24/2021 20:36","url":"/notes/web-neural-network-api-working-draft/","content":"<p>While browsing the interwebs I came across the <a href=\"https://www.w3.org/TR/webnn/\">Web Neural Network API</a> spec from the <a href=\"https://www.w3.org/groups/wg/webmachinelearning\">W3C Web Machine Learning Working Group</a>. The abstract defines it as &amp;quot;a dedicated low-level API for neural network inference hardware acceleration.&amp;quot;. Although there are already a few frameworks like ONNX &amp;amp; TensorFlow.js that allow you to inference in the browser, this spec looks interesting because it provides an abstraction that allows you to take advantage of hardware acceleration using the framework of your choice. As the <a href=\"https://github.com/webmachinelearning/webnn/blob/main/explainer.md\">explainer document</a> mentions, &amp;quot;this architecture allows JavaScript frameworks to tap into cutting-edge machine learning innovations in the operating system and the hardware platform underneath it without being tied to platform-specific capabilities, bridging the gap between software and hardware through a hardware-agnostic abstraction layer.&amp;quot;. It's just a working draft for now but I'm looking forward to how this develops.</p>\n","tags":["webnn","ai","neuralnetwork","w3c","standards","api","protocol"]},{"title":"Top 10 listicles","contentType":"notes","date":"10/24/2021 19:33","url":"/notes/top-10-listicles/","content":"<p>The other day I was going down a Wikipedia rabbit hole and learned those click-baity &amp;quot;Top 10 ...&amp;quot; or &amp;quot;Worst 10 ...&amp;quot; articles apparently have a name, <a href=\"https://en.wikipedia.org/wiki/Listicle\">listicles</a>. The more you know...</p>\n","tags":["til","internet","listicles","wikipedia"]},{"title":"lqdev.me redirect","contentType":"notes","date":"10/22/2021 13:57","url":"/notes/lqdevme-redirect/","content":"<p>That was easier than I thought. Visiting <a href=\"http://lqdev.me\">lqdev.me</a> now redirects to this site. Redirect isn't limited to the main page though. You can also access any other resources as well. For example, visiting <a href=\"http://lqdev.me/feed\">lqdev.me/feed</a> would redirect you to the main feed on this site.</p>\n","tags":["website","domain","redirect","ownyourlinks"]},{"title":"TIL: Canonical HTML tags","contentType":"notes","date":"10/21/2021 21:31","url":"/notes/rel-canonical-tags/","content":"<p>Today I learned about canonical URLs. While looking into how I can syndicate content from this site, specifically longer form blog posts on other sites like dev.to, I came across canonical tags.</p>\n<p>Basically they're a way of telling the internet, specifically search engines, what version of your content is the main copy or single-source of truth.</p>\n<p>Currently I've configured my site so it's not indexed or crawled. However, for sites I don't own I don't have the same level of control. Check out the <a href=\"https://moz.com/learn/seo/canonicalization\">canonicalization</a> and <a href=\"https://moz.com/blog/rel-canonical\">SEO best practices for canonical URLs</a> articles from Moz if you're interested in learning more.</p>\n","tags":["html","til","canonical","web","w3c","standards"]},{"title":"Next gen stick figures using AI and NVIDIA Canvas","contentType":"notes","date":"10/19/2021 10:51 PM","url":"/notes/nvidia-canvas-next-gen-stick-figures-ai/","content":"<p>I have almost zero artistic skills, so when I came across <a href=\"https://www.nvidia.com/en-us/studio/canvas/\">NVIDIA Canvas</a> I was extremely excited!</p>\n<p>Essentially it's an application that takes simple brushstrokes as input and uses a Generative Adversarial Network (GAN) to turn them into realistic landscape photos.</p>\n<p>I had known about GANs being used to generate faces (see <a href=\"https://thispersondoesnotexist.com/\">This Person Does Not Exist</a>) but it's one of the first times I've seen them used to enhance drawings.</p>\n<p>Time to take my stick figures to the next level!</p>\n","tags":["nvidia","ai","genai","art"]},{"title":"Coffee with a Codex","contentType":"notes","date":"10/19/2021 10:21 AM","url":"/notes/coffee-with-codex/","content":"<p>Came accross this event series via <a href=\"https://boffosocko.com/type/status/\">Chris Aldrich's microblog</a>.</p>\n<p>Here's Chris' original post</p>\n<blockquote class=\"blockquote\">\n<p>Every Monday, I sit down with a cup of coffee and learn something interesting about history through old manuscripts. Join our friends Dot Porter (<a href=\"https://twitter.com/leoba\">@leoba</a>) and the Schoenberg Institute for Manuscript Studies (<a href=\"https://twitter.com/sims_mss\">@sims_mss</a>) at <a href=\"https://schoenberginstitute.org/coffee-with-a-codex/\">Coffee with a Codex</a>.</p>\n</blockquote>\n<p>Basically it's a virtual meeting where experts go over manuscripts on a variety of topics. I'll probably check out next week's event.</p>\n","tags":["event","learning","history"]},{"title":"Success! Matrix migration complete","contentType":"notes","date":"10/09/2021 12:23 AM","url":"/notes/matrix-migration-complete/","content":"<p>It only took 6 hours but I was able to get it done with a little help.</p>\n<p><img src=\"https://media.giphy.com/media/mIZ9rPeMKefm0/giphy.gif\" class=\"img-fluid\" alt=\"Bender from Futurama happy dance\" /></p>\n","tags":["selfhost","matrix","messaging","opensource","element","synapse"]},{"title":"Windows 11 upgrade","contentType":"notes","date":"10/05/2021 08:55 AM","url":"/notes/windows-11-upgrade/","content":"<p>No Windows 11 upgrade for me. 😥</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/135946651-d4d9acee-5618-4846-85aa-8387077a4785.png\" class=\"img-fluid\" alt=\"PC doesn't meet windows 11 upgrade requirements message\" /></p>\n<p>Just because I can't doesn't mean you can't though. Go and check if you're eligible at Windows Update</p>\n<p><strong>Start &amp;gt; Settings &amp;gt; Update &amp;amp; Security &amp;gt; Windows Update</strong>.</p>\n<p>See <a href=\"https://support.microsoft.com/windows/manage-updates-in-windows-643e9ea7-3cf6-7da6-a25c-95d4f7f099fe#WindowsVersion=Windows_10\">Manage updates in Windows</a> for more details.</p>\n<p>For alternative install options, check out the <a href=\"https://www.microsoft.com/software-download/windows11\">Download Windows 11</a> page.</p>\n","tags":["windows10","windows","windows11","microsoft"]},{"title":"PSA: Personal site not down","contentType":"notes","date":"10/04/2021 12:55 PM","url":"/notes/fb-ig-personal-feed-up/","content":"<p>Just jumping on the bandwagon. A few online services are down at the moment 😬</p>\n<p><img src=\"https://user-images.githubusercontent.com/11130940/135891253-c3857919-b6fa-4eca-a483-a03b22abe4de.png\" class=\"img-fluid\" alt=\"Web service outages on downdetector.com\" /></p>\n<p>Not to worry...the personal site is still chugging along 😌</p>\n","tags":["indieweb","website","socialmedia","feed","personalwebsite"]},{"title":"Cultivating digital gardens","contentType":"notes","date":"10/03/2021 09:57 PM","url":"/notes/digital-gardens-technology-review-article/","content":"<p>Just read an interesting article publised in Technology Review called <a href=\"https://www.technologyreview.com/2020/09/03/1007716/digital-gardens-let-you-cultivate-your-own-little-bit-of-the-internet/\">Digital gardens let you cultivate your own little bit of the internet</a>.</p>\n<p>In a sense, some of the updates to this site like the addition of the different feeds can be thought of as &amp;quot;cultivating&amp;quot; my own digital garden.</p>\n","tags":["indieweb","digitalgarden","website","internet"]},{"title":"The Many Saints of Newark","contentType":"notes","date":"10/02/2021 08:19 PM","url":"/notes/many-saints-of-newark/","content":"<p>I would've been okay with <a href=\"https://www.rottentomatoes.com/m/the_many_saints_of_newark\">The Many Saints of Newark</a> being a 4 hour movie. I didn't want it to end. Now I have to rewatch The Sopranos.</p>\n","tags":["sopranos","movie","newjersey","newark","nj"]},{"title":"The future of Skype","contentType":"notes","date":"10/01/2021 4:20 PM","url":"/notes/skype-future-annoucement-09-2021/","content":"<p>For personal use, I tend to default to Skype for my video / audio call needs.</p>\n<p>Great to see there are some long-term plans for the app and it'll be around for a while more.</p>\n<p><a href=\"https://www.skype.com/en/blogs/2021-08-announcement-modern-skype/\">The future of Skype: Fast, playful, delightful and buttery smooth​</a></p>\n","tags":["skype","messaging","microsoft","roadmap"]},{"title":"When Planets Mate","contentType":"notes","date":"09/29/2021 07:56 PM","url":"/notes/when-planets-mate/","content":"<p>Found this interesting video by Chackram &amp;amp; Riley Thompson. My favorite part starts around the 7:15 mark.</p>\n<p><a href=\"https://vimeo.com/340043225\"><img src=\"https://i.vimeocdn.com/video/788261540-f6e1a0bb577b65081153fc4b2af03b6addffa07a0551187165bc9e71003063d7-d?mw=1500&amp;amp;mh=844&amp;amp;q=70\" class=\"img-fluid\" alt=\"When Planets Mate Video by Chakram &amp;amp; Riley Thompson\" /></a></p>\n","tags":["indie","art","video","music"]},{"title":"Hello World","contentType":"notes","date":"09/29/2021 07:16 PM","url":"/notes/hello-world/","content":"<p>Hello World! First post in the personal feed.</p>\n","tags":["blog","microblog","indieweb","internet","feed","rss"]}]</script><script type="application/json" id="remainingContentData-responses">[{"title":"I Deleted My Second Brain","contentType":"star","date":"2025-08-13 08:03","url":"/star/i-deleted-my-second-brain/","content":"<div class=\"response-target\" >→ <a href=\"https://notes.jim-nielsen.com/#2025-08-12T2249\" >https://notes.jim-nielsen.com/#2025-08-12T2249</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>I don’t build a second brain. I inhabit the first.</p>\n</blockquote>\n</div>\n","tags":["pkm","notes","secondbrain"]},{"title":"Engineering for Slow Internet","contentType":"reply","date":"2025-08-12 08:41","url":"/reply/engineering-for-slow-internet/","content":"<div class=\"response-target\" >→ <a href=\"https://brr.fyi/posts/engineering-for-slow-internet\" >https://brr.fyi/posts/engineering-for-slow-internet</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Does this webapp really need to be 20 MB? What all is being loaded that could be deferred until it is needed, or included in an “optional” add-on bundle? Is there a possibility of a “lite” version, for bandwidth-constrained users?</p>\n</blockquote>\n<p>While Antarctica is an edge case, this article illustrates some of the motivations behind my <a href=\"/text\">text-first website</a>.</p>\n<p>By trimming the excess not only do you get to the core of the app or website, but it also loads faster.</p>\n</div>\n","tags":["slowweb","performance","internet","computing"]},{"title":"Stop saving everything","contentType":"reshare","date":"2025-08-11 23:00","url":"/reshare/stop-saving-everything/","content":"<div class=\"response-target\" >→ <a href=\"https://ellanew.com/2025/08/11/ptpl-168-stop-saving-everything-start-reflecting\" >https://ellanew.com/2025/08/11/ptpl-168-stop-saving-everything-start-reflecting</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>If your read-it-later list isn’t getting cleared weekly, perhaps it’s time to delete the lot.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Save from a mindset of abundance, rather than scarcity, and process the things you’ve saved each week (or month, at the most). If you are worried that something you deleted truly would have changed your life, just stop.</p>\n<p>STOP.</p>\n<p>You can’t read it all, do it all, be it all. Trust that those potentially life-changing ideas will come around again, when you are ready for them</p>\n</blockquote>\n<p>Good reminder. Lately I've been sending a lot of notes to myself with stuff to read but just haven't had the time to get to it. That said, the act of sending myself those notes is low friction that I don't feel FOMO when I don't get to read the articles and consume the media.</p>\n</div>\n","tags":["pkm","productivity","notetaking"]},{"title":"BLM Lands Marked for Potential Sale (OnX)","contentType":"reshare","date":"2025-08-10 21:13","url":"/reshare/blm-lands-marked-for-potential-sale-onx/","content":"<div class=\"response-target\" >→ <a href=\"https://www.onxmaps.com/onx-access-initiatives/public-land-sales-blm-rmp\" >https://www.onxmaps.com/onx-access-initiatives/public-land-sales-blm-rmp</a></div><div class=\"response-content\" ><p>Sad to see what's happening with our public lands. Great to see folks staying vigilant and providing information of what's happening on the ground.</p>\n</div>\n","tags":["onx","bureaulandmanagement","nature"]},{"title":"Tumblr’s move to WordPress and fediverse integration is ‘on hold’","contentType":"reply","date":"2025-06-30 20:21","url":"/reply/tumblr-wordpress-fediverse-integration-pause/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/news/695124/tumblr-wordpress-automattic-fediverse-integration-on-hold-decoder\" >https://www.theverge.com/news/695124/tumblr-wordpress-automattic-fediverse-integration-on-hold-decoder</a></div><div class=\"response-content\" ><p>That's unfortunate. Having Tumblr as your frontend for posting and curating feeds, with WordPress as the backend and integrated with decentralized networks like the Fediverse for distribution would've been amazing. It sounds like Tumblr is still strategic to Automattic and as more people move away from the centralized social platforms and towards their own corners of the internet, having these differente building blocks in place will be even more important.</p>\n</div>\n","tags":["tumblr","wordpress","fediverse","automattic"]},{"title":"Do not privatize federal land","contentType":"reply","date":"2025-06-29 17:26","url":"/reply/do-not-privatize-federal-land/","content":"<div class=\"response-target\" >→ <a href=\"https://marginalrevolution.com/marginalrevolution/2025/06/privatize-federal-land.html\" >https://marginalrevolution.com/marginalrevolution/2025/06/privatize-federal-land.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>I’ve long advocated selling off some federal land...Most of this “public land” is never used by the public. Selling some of it would actually make it more accessible and useful to real people.</p>\n</blockquote>\n<p>How about no. If privatization means, John and Jane Doe can purchase some land for their homestead is one thing. We've tried that before by the way - <a href=\"https://en.wikipedia.org/wiki/Homestead_Acts\">Homestead Act</a>. In reality, it means some private equity firm or hedge fund buying the land, building expensive and shoddy cookit-cutter condos, and <a href=\"https://www.nar.realtor/magazine/real-estate-news/commercial/build-to-rent-homes\">selling you a subscription for a roof over your head</a>. I say condos because it's hot and there's not a lot of water in Vegas, so keeping those brand new AI data centers cool might be a problem.</p>\n<p>Also, if you haven't seen that area that Alex highlights in his post, it seems like a deserted landscape. And yes, the terrain can be rough, but actually, it's a beautiful landscape that is best left undisturbed.</p>\n</div>\n","tags":["bureaulandmanagement","uspol","land"]},{"title":"Self-Adapting Language Models (SEAL)","contentType":"reshare","date":"2025-06-16 20:33","url":"/reshare/self-adapting-language-models-seal/","content":"<div class=\"response-target\" >→ <a href=\"https://jyopari.github.io/posts/seal\" >https://jyopari.github.io/posts/seal</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. <strong>We introduce Self-Adapting LLMs (SEAL) 🦭, a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives.</strong> Given a new input, the model produces a self-edit — a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop, using the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's generation to parameterize and control its own adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation in response to new data.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We demonstrate SEAL in two domains: (1) <strong>Knowledge Incorporation</strong>, where the model integrates new factual information by generating logical implications as synthetic data, and (2) <strong>Few-Shot Learning</strong>, where the model autonomously selects data augmentations and training hyperparameters to adapt to new abstract reasoning tasks.</p>\n</blockquote>\n<ul>\n<li><a href=\"https://github.com/Continual-Intelligence\">Code</a></li>\n<li><a href=\"https://arxiv.org/abs/2506.10943\">Paper</a></li>\n</ul>\n</div>\n","tags":["agent","ai","llm","mit","research"]},{"title":"How we built our multi-agent research system - Anthropic","contentType":"reshare","date":"2025-06-16 17:39","url":"/reshare/anthropic-how-we-built-multi-agent-system/","content":"<div class=\"response-target\" >→ <a href=\"https://www.anthropic.com/engineering/built-multi-agent-research-system\" >https://www.anthropic.com/engineering/built-multi-agent-research-system</a></div><div class=\"response-content\" ><p>Great article.</p>\n<p>TLDR: These are largely engineering problems.</p>\n<blockquote class=\"blockquote\">\n<p>The journey of this multi-agent system from prototype to production taught us critical lessons about system architecture, tool design, and prompt engineering. A multi-agent system consists of multiple agents (<strong>LLMs autonomously using tools in a loop</strong>) working together. Our Research feature involves an agent that plans a research process based on user queries, and then uses tools to create parallel agents that search for information simultaneously. Systems with multiple agents introduce new challenges in agent coordination, evaluation, and reliability.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>The essence of search is compression: distilling insights from a vast corpus. <strong>Subagents facilitate compression by operating in parallel</strong> with their own context windows, exploring different aspects of the question simultaneously before condensing the most important tokens for the lead research agent. Each subagent also provides <strong>separation of concerns</strong>—distinct tools, prompts, and exploration trajectories—which reduces path dependency and enables thorough, independent investigations.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Once intelligence reaches a threshold, multi-agent systems become a vital way to scale performance. For instance, although individual humans have become more intelligent in the last 100,000 years, human societies have become exponentially more capable in the information age because of our collective intelligence and ability to coordinate. Even generally-intelligent agents face limits when operating as individuals; <strong>groups of agents can accomplish far more</strong>.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Our internal evaluations show that multi-agent research systems excel especially for breadth-first queries that involve pursuing multiple independent directions simultaneously.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Multi-agent systems work mainly because they help spend enough tokens to solve the problem...Multi-agent architectures effectively scale token usage for tasks that exceed the limits of single agents.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We’ve found that multi-agent systems excel at valuable tasks that involve heavy parallelization, information that exceeds single context windows, and interfacing with numerous complex tools.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>Our Research system uses a multi-agent architecture with an orchestrator-worker pattern, where a lead agent coordinates the process while delegating to specialized subagents that operate in parallel.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Traditional approaches using Retrieval Augmented Generation (RAG) use static retrieval. That is, they fetch some set of chunks that are most similar to an input query and use these chunks to generate a response. In contrast, our architecture uses a <strong>multi-step search that dynamically finds relevant information, adapts to new findings, and analyzes results to formulate high-quality answers</strong>.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p><strong>Think like your agents.</strong> To iterate on prompts, you must understand their effects. To help us do this, we built simulations using our Console with the exact prompts and tools from our system, then watched agents work step-by-step.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Teach the orchestrator how to delegate.</strong> In our system, the lead agent decomposes queries into subtasks and describes them to subagents. Each subagent needs an <strong>objective</strong>, an <strong>output format</strong>, <strong>guidance on the tools and sources to use</strong>, and <strong>clear task boundaries</strong>. Without detailed task descriptions, agents duplicate work, leave gaps, or fail to find necessary information.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Scale effort to query complexity.</strong> Agents struggle to judge appropriate effort for different tasks, so we embedded scaling rules in the prompts...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Tool design and selection are critical.</strong> Agent-tool interfaces are as critical as human-computer interfaces. Using the right tool is efficient—often, it’s strictly necessary...Bad tool descriptions can send agents down completely wrong paths, so each tool needs a distinct purpose and a clear description.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Let agents improve themselves.</strong>...When given a prompt and a failure mode, they are able to diagnose why the agent is failing and suggest improvements.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Start wide, then narrow down.</strong> Search strategy should mirror expert human research: explore the landscape before drilling into specifics.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Guide the thinking process.</strong>...Our testing showed that extended thinking improved instruction-following, reasoning, and efficiency. Subagents also plan, then use interleaved thinking after tool results to evaluate quality, identify gaps, and refine their next query. This makes subagents more effective in adapting to any task.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Parallel tool calling transforms speed and performance.</strong>...For speed, we introduced two kinds of parallelization: (1) the lead agent spins up 3-5 subagents in parallel rather than serially; (2) the subagents use 3+ tools in parallel.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Our prompting strategy focuses on instilling <strong>good heuristics rather than rigid rules</strong>. We studied how skilled humans approach research tasks and encoded these strategies in our prompts—strategies like <strong>decomposing difficult questions into smaller tasks</strong>, <strong>carefully evaluating the quality of sources</strong>, <strong>adjusting search approaches based on new information</strong>, and <strong>recognizing when to focus on depth (investigating one topic in detail) vs. breadth (exploring many topics in parallel)</strong>.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>...evaluating multi-agent systems presents unique challenges...Because we don’t always know what the right steps are, we usually can't just check if agents followed the “correct” steps we prescribed in advance. Instead, we need flexible evaluation methods that judge whether agents achieved the right outcomes while also following a reasonable process.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Start evaluating immediately with small samples.</strong>...it’s best to start with small-scale testing right away with a few examples, rather than delaying until you can build more thorough evals.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>LLM-as-judge evaluation scales when done well.</strong>...We used an LLM judge that evaluated each output against criteria in a rubric: <strong>factual accuracy</strong> (do claims match sources?), <strong>citation accuracy</strong> (do the cited sources match the claims?), <strong>completeness</strong> (are all requested aspects covered?), <strong>source quality</strong> (did it use primary sources over lower-quality secondary sources?), and <strong>tool efficiency</strong> (did it use the right tools a reasonable number of times?)...[we] found that a single LLM call with a single prompt outputting scores from 0.0-1.0 and a pass-fail grade was the most consistent and aligned with human judgements.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Human evaluation catches what automation misses.</strong>...Adding source quality heuristics to our prompts helped resolve this issue. Even in a world of automated evaluations, manual testing remains essential.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>the best prompts for these agents are not just strict instructions, but <strong>frameworks for collaboration</strong> that define the <strong>division of labor</strong>, <strong>problem-solving approaches</strong>, and <strong>effort budgets</strong>. Getting this right relies on <strong>careful prompting and tool design</strong>, <strong>solid heuristics</strong>, <strong>observability</strong>, and <strong>tight feedback loops</strong>.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p><strong>Agents are stateful and errors compound.</strong>...Agents can run for long periods of time, maintaining state across many tool calls. This means we need to <strong>durably execute code and handle errors along the way</strong>. Without effective mitigations, minor system failures can be catastrophic for agents. When errors occur, we can't just restart from the beginning...We combine the adaptability of AI agents built on Claude with deterministic safeguards like retry logic and regular checkpoints.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Debugging benefits from new approaches</strong>...Adding full production tracing let us diagnose why agents failed and fix issues systematically. Beyond standard observability, we monitor agent decision patterns and interaction structures—all without monitoring the contents of individual conversations, to maintain user privacy.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Deployment needs careful coordination</strong>...we use rainbow deployments to avoid disrupting running agents, by gradually shifting traffic from old to new versions while keeping both running simultaneously.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Synchronous execution creates bottlenecks.</strong> Currently, our lead agents execute subagents synchronously, waiting for each set of subagents to complete before proceeding. This simplifies coordination, but creates bottlenecks in the information flow between agents. Asynchronous execution would enable additional parallelism: agents working concurrently and creating new subagents when needed. But this asynchronicity adds challenges in result coordination, state consistency, and error propagation across the subagents. As models can handle longer and more complex research tasks, we expect the performance gains will justify the complexity.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>For all the reasons described in this post, the gap between prototype and production is often wider than anticipated.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Multi-agent research systems can operate reliably at scale with <strong>careful engineering</strong>, <strong>comprehensive testing</strong>, <strong>detail-oriented prompt and tool design</strong>, <strong>robust operational practices</strong>, and <strong>tight collaboration between research, product, and engineering teams</strong> who have a strong understanding of current agent capabilities.</p>\n</blockquote>\n</div>\n","tags":["agent","ai","anthropic","engineering"]},{"title":"MLS over ActivityPub Draft","contentType":"reshare","date":"2025-06-14 10:47","url":"/reshare/mls-over-activitypub-draft/","content":"<div class=\"response-target\" >→ <a href=\"https://swicg.github.io/activitypub-e2ee/mls\" >https://swicg.github.io/activitypub-e2ee/mls</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p><a href=\"https://messaginglayersecurity.rocks/\">Messaging Layer Security (MLS)</a> is an IETF standard for end-to-end encrypted (E2EE) messaging. It lets people on laptops and phones communicate with each other in a secure way that no one in between can see.<br />\n<br>\nMLS is designed to use pluggable lower-level protocols. This specification defines an envelope format for distributing MLS messages through the network, and an Activity Streams 2.0 profile for the packets of application data stored inside the messages.<br />\n<br>\nThis specification is ready for review from both ActivityPub developers and security analysts. It’s time to start making proof-of-concept implementations and testing interoperability.</p>\n</blockquote>\n<p>Source: https://socialwebfoundation.org/2025/06/13/mls-over-activitypub-draft/</p>\n</div>\n","tags":["protocol","socialweb","activitypub","w3c"]},{"title":"Faircamp - A static site generator for audio producers","contentType":"star","date":"2025-06-05 20:05","url":"/star/faircamp-static-site-audio-producers/","content":"<div class=\"response-target\" >→ <a href=\"https://simonrepp.com/faircamp/\" >https://simonrepp.com/faircamp/</a></div><div class=\"response-content\" ><p>This is such a cool project. Looks like it's super simple to get started with and it's packed with a ton of features so creators get to stay in their flow.</p>\n<p>I also like how creators are showcased.</p>\n</div>\n","tags":["faircamp","music","socialweb","indieweb","staticsite","technology","tools","openweb","opensource"]},{"title":"State-Of-The-Art Prompting For AI Agents","contentType":"reshare","date":"2025-06-03 20:23","url":"/reshare/state-of-the-art-prompting-ai-agents-y-combinator/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=DL82mGde6wo\" >https://www.youtube.com/watch?v=DL82mGde6wo</a></div><div class=\"response-content\" ><p><a href=\"https://www.youtube.com/watch?v=DL82mGde6wo\" title=\"YouTube Thumbnail for State-of-the-art prompting for ai agents\"><img src=\"http://img.youtube.com/vi/DL82mGde6wo/0.jpg\" class=\"img-fluid\" alt=\"YouTube Thumbnail for State-of-the-art prompting for ai agents\" /></a></p>\n</div>\n","tags":["ai","agents","prompting"]},{"title":"Introducing ElevenLabs Conversational AI 2.0","contentType":"reshare","date":"2025-06-03 20:19","url":"/reshare/elevenlabs-conversational-ai-2-0/","content":"<div class=\"response-target\" >→ <a href=\"https://elevenlabs.io/blog/conversational-ai-2-0\" >https://elevenlabs.io/blog/conversational-ai-2-0</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Conversational AI 2.0 launches with advanced features and enterprise readiness.</p>\n</blockquote>\n<ul>\n<li>Natural turn-taking to understand the flow of conversation.</li>\n<li>Multilingual communication with integrated language detection</li>\n<li>Integrated RAG: knowledgeable agents, minimum latency, maximum privacy</li>\n<li>Multimodality</li>\n<li>Batch calls</li>\n</ul>\n</div>\n","tags":["ai","speech","elevenlabs"]},{"title":"The Darwin Gödel Machine - AI that improves itself by rewriting its own code","contentType":"reshare","date":"2025-06-03 20:16","url":"/reshare/sakana-ai-gwm/","content":"<div class=\"response-target\" >→ <a href=\"https://sakana.ai/dgm/\" >https://sakana.ai/dgm/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A longstanding goal of AI research has been the creation of AI that can learn indefinitely. One tantalizing path toward that goal is an AI that improves itself by rewriting its own code, including any code responsible for learning. That idea, known as a Gödel Machine, proposed by Jürgen Schmidhuber decades ago, is a hypothetical self-improving AI. It optimally solves problems by recursively rewriting its own code when it can mathematically prove a better strategy, making it a key concept in meta-learning or “learning to learn.”<br />\n<br>\nWhile the theoretical Gödel Machine promised provably beneficial self-modifications, its realization relied on an impractical assumption: that the AI could mathematically prove that a proposed change in its own code would yield a net improvement before adopting it. We, in collaboration with Jeff Clune’s lab at UBC, propose something more feasible: a system that harnesses the principles of open-ended algorithms like Darwinian evolution to search for improvements that empirically improve performance.</p>\n<p>We call the result the Darwin Gödel Machine (full technical report). DGMs leverage foundation models to propose code improvements, and use recent innovations in open-ended algorithms to search for a growing library of diverse, high-quality AI agents. Our experiments show that DGMs improve themselves the more compute they are provided. In line with the clear trend that AI systems that rely on learning ultimately outperform those designed by hand, there is a potential that DGMs could soon outperform hand-designed AI systems.</p>\n</blockquote>\n<p><a href=\"https://arxiv.org/abs/2505.22954\">Technical Report</a></p>\n</div>\n","tags":["ai","agent","sakana"]},{"title":"The stage has been set for the growth of small social media","contentType":"reshare","date":"2025-05-26 18:14","url":"/reshare/stage-set-growth-small-social-media-cheapskate/","content":"<div class=\"response-target\" >→ <a href=\"https://cheapskatesguide.org/articles/small-social-growth.html\" >https://cheapskatesguide.org/articles/small-social-growth.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Last month during Meta's antitrust trial, Mark Zuckerberg admitted that his company's focus has changed radically from where it began...[his] testimony implies that Facebook (Meta) may no longer be as much of an option if we actually want to talk to each other on line. But, neither are many of the other large social media sites.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...if companies allowed us to discover great new small blogs and small social media sites, many of us would decide to spend more time there. Then, companies would be less able to advertise to us on their own sites.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...we have been told by the mainstream media for well over a decade that small social media is dead. They say the same about small blogs, yet small blogs are actually more plentiful than ever, with over <a href=\"https://firstsiteguide.com/blogging-stat\">600 million blogs</a> on the Internet and about 7 million new blog articles published each day.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>If you decide to create your own forum, do yourself a favor. The way to protect yourself and your users is by <strong>running your social media site at a domain that you own and on a server that you control</strong>, and don't agree to hand over any of your rights to anyone for any reason.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I believe large corporate-run social media sites are vulnerable in a way they have not been for twenty years. Their users have reached a state of extreme dissatisfaction with their &amp;quot;enshittified&amp;quot; platforms, and they are looking for something, anything, better.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>But each of you who are reading these words can help end this situation by creating your own small social media site.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>All you need to start your own small site is a domain name, a server that you can buy, rent, or set up on an old computer, perhaps social media software (see above), patience, a willingness to work, and ideas for ways of notifying potential users of the existence of your site.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Run your new site as you and your users see fit, and make it far superior to Facebook, Twitter (X), Reddit, or any of the other major sites whose owners think you have no other options.</p>\n</blockquote>\n</div>\n","tags":["social","openweb","personalweb"]},{"title":"RSS is (not) dead (yet)","contentType":"star","date":"2025-05-26 18:11","url":"/star/rss-not-dead-yet-mcnamee/","content":"<div class=\"response-target\" >→ <a href=\"https://audmcname.com/comics/rss-is-not-dead-yet/\" >https://audmcname.com/comics/rss-is-not-dead-yet/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Illustrating the history of RSS. Created in collaboration with <a href=\"https://audmcname.com/alliaservice.com\">A. Service</a>, and debuted at VanCAF 2023</p>\n</blockquote>\n<p><img src=\"https://audmcname.com/wp-content/uploads/rss/rss0.jpg\" class=\"img-fluid\" alt=\"Cover of Rss is not dead yet comic\" /></p>\n</div>\n","tags":["rss","comic","art"]},{"title":"Agent Network Protocol - The HTTP of the Agentic Web Era","contentType":"reshare","date":"2025-05-17 10:56","url":"/reshare/agent-network-protocol/","content":"<div class=\"response-target\" >→ <a href=\"https://agent-network-protocol.com/\" >https://agent-network-protocol.com/</a></div><div class=\"response-content\" ><p><a href=\"https://agent-network-protocol.com/blogs/posts/mcp-anp-comparison.html\">Comparison of MCP and ANP: What Kind of Communication Protocol Do Agents Need?</a></p>\n<blockquote class=\"blockquote\">\n<p>MCP and ANP differ significantly in protocol architecture, identity authentication, and information organization.<br />\n<br></p>\n<ul>\n<li>MCP is a typical Client-Server (CS) architecture, while <strong>ANP is a typical Peer-to-Peer (P2P) architecture</strong>.</li>\n<li>MCP's identity authentication is based on the OAuth standard, facilitating client access to current internet resources. <strong>ANP's identity authentication is based on the W3C DID standard</strong>, focusing on cross-platform interoperability among agents, enabling seamless connectivity.</li>\n<li>MCP organizes information using JSON-RPC technology, essentially API calls. <strong>ANP uses semantic web Linked-Data technology</strong> to build a data network that is easily accessible and understandable by AI.</li>\n</ul>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>ANP is agent-centric, where each agent has equal status, forming a decentralized agent collaboration network.</p>\n</blockquote>\n</div>\n","tags":["agent","ai","protocol"]},{"title":"Chicago Dust Storm","contentType":"reshare","date":"2025-05-16 19:49","url":"/reshare/chicago-dust-storm/","content":"<div class=\"response-target\" >→ <a href=\"https://www.nbcchicago.com/news/local/satellite-imagery-shows-large-dust-storm-over-chicago-area-from-space/3748009/\" >https://www.nbcchicago.com/news/local/satellite-imagery-shows-large-dust-storm-over-chicago-area-from-space/3748009/</a></div><div class=\"response-content\" ><p>Never change Chicago.</p>\n<p>What felt like the first week of good weather since forever ends with a dust storm.</p>\n</div>\n","tags":["chicago","weather","duststorm"]},{"title":"Claude Artifacts","contentType":"reply","date":"2025-05-11 08:42","url":"/reply/claude-artifacts-reece/","content":"<div class=\"response-target\" >→ <a href=\"https://www.manton.org/2025/05/10/openai-rolls-out-new-things.html\" >https://www.manton.org/2025/05/10/openai-rolls-out-new-things.html</a></div><div class=\"response-content\" ><p>💯💯💯. Artifacts are great. In ChatGPT when I find myself trying to format outputs as markdown, the UI gets confused and some of the content is rendered in markdown while the rest gets rendered as part of the UI which makes it unusable and hard to copy. The best way I've found to get around it is to ask it to format in org-mode or asciidoc. Claude Artifacts make this a non-issue. I also love that in addition to copying and downloading the artifact, you can also publish it as a URL.</p>\n</div>\n","tags":["claude","chatgpt","plaintext","ai","org","emacs","asciidoc","markdown"]},{"title":"Announcing Distributed.Press Social Inbox 1.0","contentType":"reshare","date":"2025-05-10 20:47","url":"/reshare/announcing-distributed-press-social-inbox/","content":"<div class=\"response-target\" >→ <a href=\"https://hypha.coop/dripline/announcing-dp-social-inbox/\" >https://hypha.coop/dripline/announcing-dp-social-inbox/</a></div><div class=\"response-content\" ><p>I hadn't heard of <a href=\"https://distributed.press/\">Distributed Press</a> and this is an older post (from 2023), but the idea is interesting.</p>\n<blockquote class=\"blockquote\">\n<p>Hypha and Sutty are thrilled to announce the release of the Social Inbox, a new feature of Distributed.Press that integrates a website’s comment section with federated social media platforms like Mastodon. With the Social Inbox enabled, websites obtain their own account on the Fediverse, allowing it to automatically send out new posts to followers at the time of publication. When other users reply to posts, you can approve them to be published to the site as comments. The Social Inbox allows readers to directly engage with your posts where they already are, and gives publishers the ability to incorporate public dialogue into their websites.</p>\n</blockquote>\n</div>\n","tags":["decentralization","indieweb","social","fediverse","dweb"]},{"title":"Discourse and the Fediverse","contentType":"star","date":"2025-05-09 22:20","url":"/star/discourse-fediverse/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.discourse.org/2025/04/discourse-and-the-fediverse/\" >https://blog.discourse.org/2025/04/discourse-and-the-fediverse/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Two years ago, we started working on a plugin that brings Discourse and the Fediverse closer together. Discourse communities are online spaces that facilitate open collaboration and communication. The Fediverse offers ways to expand the reach of Discourse communities and help them build bridges with people active in other spaces, all while keeping the conversation civil, meaningful and focused. This post will describe how the ActivityPub plugin works and how you can enable your Discourse community to connect with other communities or Fediverse users.</p>\n</blockquote>\n</div>\n","tags":["discourse","fediverse","social","openweb","opensource","forum"]},{"title":"In Praise of Links","contentType":"star","date":"2025-05-09 21:05","url":"/star/in-praise-of-links-osteophage/","content":"<div class=\"response-target\" >→ <a href=\"https://osteophage.neocities.org/writing/in-praise-of-links\" >https://osteophage.neocities.org/writing/in-praise-of-links</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Hyperlinks deserve more recognition in light of all the ways their value has been sidelined and denied.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>To that end, I present a link compilation in praise of links. It includes things I agree with entirely and things I don't, spanning from the 2020s to the early 2000s, to supply a tapestry of perspectives, context, and examples on the value and importance of links. Links may have their downsides, challenges, and vulnerabilities, but my hope is that this compilation will (re)invigorate your appreciation for linking as a technology and a social practice, all the better to understand what's at stake when links are discarded and devalued.</p>\n</blockquote>\n</div>\n","tags":["openweb","internet","indieweb","decentralization","web"]},{"title":"The Evolution of Blogging","contentType":"star","date":"2025-05-09 20:54","url":"/star/evolution-of-blogging-history-web/","content":"<div class=\"response-target\" >→ <a href=\"https://thehistoryoftheweb.com/the-evolution-of-blogging/\" >https://thehistoryoftheweb.com/the-evolution-of-blogging/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...around 1997, when Jorn Barger launched his website, titling it “The Robot Wisdom Weblog.” Barger’s plan was to curate his favorite links from around the web, and add bits of commentary to each one. In other words, he meant to keep a log of his web experience. Hence weblog.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In their earliest days, webloggers stood as gatekeepers to the web’s ever-growing well of content. Each day, these URL pioneers would post a few new links and sprinkle in their own commentary. Blogs acted as a signpost for web users, and following a few key blogs was enough to keep track of just about everything new on the web. Many began to look at the blogging community as a brand new type of media, one that often stood far closer to an impartial truth than traditional mediums would allow for.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>1999, Peter Merholz threw up his own weblog, and in the sidebar added the tagline: &amp;quot;I’ve decided to pronounce the word “weblog” as wee’- blog. Or “blog” for short.&amp;quot;</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In the beginning, most bloggers hand coded their sites, adding a new HTML page to their server each time they had a new entry, or just updating the homepage to include the newest links. But soon enough, tools showed up to help bloggers with the process.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...on the fringes, a new type of blog was emerging. The personal blog. These sites ditched the curated links and focused exclusively on commentary. Bloggers used their site to chronicle their personal journey, from the almost boring and banal to the weird and wonderful. This new type of blog was less an alternative media source and more akin to an online journal or diary. And these writers saw themselves not as gatekeepers to the web, but as sharers of their own identity.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>New tools soon caught up with this shifting perspective. The first of the bunch to do so was LiveJournal...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Blogger came next. And it would prove to be the most popular of the early blogging tools. The platform first launched in August of 1999, but picked up steam the following year when it introduced the concept of a permalink.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>These platforms made blogging open, in every sense of the word. Visitors were greeted with a single, open textarea on a free platform that was open to all to participate.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Blogger became the platform choice for a lot of writers out there. It certainly was for Mena Trott</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Trott had the feeling that platforms like Blogger did not go nearly far enough.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Trott wanted her own site to stand out from the crowd. To be quintessentially hers. Blogger wouldn’t let her do that. So together with her husband, a Perl programmer, she built Movable Type...And with that move, Trott ushered in a third generation of blogging.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Movable Type allowed users to set up their blog on their own server, and have complete control over its look and feel.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Blog (the noun and verb) became a word familiar not just to web geeks, but to everyone. Lots of imitators and innovators followed in the wake of Moveable Type. Open source platforms like WordPress rose to meet the rising demand. But for a long time, Moveable Type was the standard. It fed its community with a constant stream of new features that promoted openness and freedom.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Blogging, as a community and a practice, continues to be refined to this day thanks to the dedication of web users everywhere. But its sharpest point was made the day Mena Trott decided that <strong>a blog could be more than just some text on a page. It could be one of a kind.</strong></p>\n</blockquote>\n</div>\n","tags":["blogging","history","web","internet"]},{"title":"socialweb.network","contentType":"reshare","date":"2025-05-09 20:49","url":"/reshare/socialweb-network/","content":"<div class=\"response-target\" >→ <a href=\"https://socialweb.network/\" >https://socialweb.network/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Your gateway to decentralized social networking protocols, resources, and community</p>\n</blockquote>\n</div>\n","tags":["fediverse","social","openweb","decentralized","community","indieweb"]},{"title":"Fediverse House 2025 Roundup","contentType":"star","date":"2025-05-09 20:36","url":"/star/fediverse-house-2025-roundup/","content":"<div class=\"response-target\" >→ <a href=\"https://about.flipboard.com/fediverse/fediverse-house-2025-roundup/\" >https://about.flipboard.com/fediverse/fediverse-house-2025-roundup/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>No Walls, Just Vibes at SXSW’s Fediverse House</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Fediverse House was SXSW’s first physical gathering dedicated entirely to the constellation of interoperable social platforms powered by protocols like ActivityPub and AT Protocol. (We know “Fediverse” isn’t quite accurate to describe the whole ecosystem; in this case, it was just a convenient, fun way to name the space.)</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>It didn’t matter which platform you preferred or which protocol you were building on; everyone was focused on the singular goal of building a better internet.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...the real power move for creators is ownership and control of their work and livelihoods.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...identity portability is a radical shift, and...decentralization could lead to more humane social spaces.</p>\n</blockquote>\n<ul>\n<li><a href=\"https://dot-social.simplecast.com/\">DotSocial Podcast</a></li>\n<li><a href=\"https://flipboard.video/c/fediverse.house/videos\">Fediverse House PeerTube Channel</a></li>\n</ul>\n</div>\n","tags":["fediverse","conference","indieweb","openweb"]},{"title":"Resisting the urge to rewrite the website","contentType":"reshare","date":"2025-05-05 20:06","url":"/reshare/resisting-urge-rewrite-website-santala/","content":"<div class=\"response-target\" >→ <a href=\"https://hamatti.org/posts/resisting-the-urge-to-rewrite-the-website/\" >https://hamatti.org/posts/resisting-the-urge-to-rewrite-the-website/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>I have half-jokingly been entertaining an idea of building a new blog: for every blog post published, you get to add one new feature. The first blog post needs to be written in a plain txt file and deployed as-is to a server. With the second post, you can choose to add something you want: maybe a bit of HTML navigation to be able to have links between the blog posts. The third post could come with Markdown support so your posts are not plain text anymore but rendered into HTML. And so on.<br />\n<br>\nThe idea would is that you get to “earn” feature development by writing blog posts. If you want a fancy site with bells and whistles and good tooling, you have to write and publish a lot. That way, you cannot get lost into the rabbit hole of new shiny feature development at the expense of writing.</p>\n</blockquote>\n<p>I like this suggestion. My rewrite has been ongoing but slow. I tried vibe-coding my way to success but didn't have much luck there, so it's taking more time to do it manually. If I was doing it from scratch it wouldn't be a challenge. The challenge is making all the posts and current structure fit into what I want to website to eventually look like.</p>\n<blockquote class=\"blockquote\">\n<p>I use different tools for writing and coding. I can write my blog posts on my iPad when I’m in the library or local pub and at that time, there’s no way for me to tinker with the tools or the website itself.</p>\n</blockquote>\n<p>Another great suggestion. A lot of my desire to rewrite stems from frictions in my current workflow as well as being able to support new and different post types. Back when I had my <a href=\"/notes/surface-duo-blogging-github-dev\">Duo</a>, I didn't mind the mobile VS Code authoring experience, but now that I'm back to a single screen device, not being able to publish from my phone creates a lot of friction.</p>\n</div>\n","tags":["website","indieweb","blogging"]},{"title":"Just Put It On Your Blog","contentType":"star","date":"2025-05-05 19:57","url":"/star/just-post-on-your-blog-shellsharks/","content":"<div class=\"response-target\" >→ <a href=\"https://shellsharks.com/just-put-it-on-your-blog\" >https://shellsharks.com/just-put-it-on-your-blog</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>It’s great to have a place to share your thoughts. A place you can go back to when you want to remember something you had written or thought about before. A place you can refer people to when they have questions you’ve answered in the past. A place to be you. So, get a blog, and put all the things there.</p>\n</blockquote>\n<p>It is great to carve out your own space in the interwebs. If you run out of ideas of what to do with your website, here's <a href=\"https://jamesg.blog/2024/02/19/personal-website-ideas\">James' blog with 100+ ideas</a>.</p>\n</div>\n","tags":["blogging","indieweb","website"]},{"title":"Running RAG with ONNX Runtime GenAI for On-Prem Windows","contentType":"reshare","date":"2025-05-02 15:19","url":"/reshare/rag-onnx-runtime-genai-windows/","content":"<div class=\"response-target\" >→ <a href=\"https://devblogs.microsoft.com/ise/running-rag-onnxruntime-genai/\" >https://devblogs.microsoft.com/ise/running-rag-onnxruntime-genai/</a></div><div class=\"response-content\" ><p>Really great to see these case studies and comparisons.</p>\n<p>A while back we published a blog post showcasing how experiences like the <a href=\"https://devblogs.microsoft.com/dotnet/introducing-ai-dev-gallery-gateway-to-local-ai-development/\">AI Dev Gallery</a> make use of ONNX Runtime and the various AI building blocks in .NET to enable a diverse set of scenarios.</p>\n<p>For more ONNX Runtime GenAI focused C# content, you can also reference the post <a href=\"https://devblogs.microsoft.com/dotnet/using-phi3-csharp-with-onnx-for-text-and-vision-samples-md/\">Using Phi-3 &amp;amp; C# with ONNX for text and vision samples</a></p>\n</div>\n","tags":["onnxruntime","windows","dotnet"]},{"title":"GPT Image 1 - Image Generation API","contentType":"reshare","date":"2025-04-24 19:50","url":"/reshare/gpt-image-1-api/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/index/image-generation-api/\" >https://openai.com/index/image-generation-api/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, we’re bringing the natively multimodal model that powers this experience in ChatGPT to the API via <code>gpt-image-1</code>, enabling developers and businesses to easily integrate high-quality, professional-grade image generation directly into their own tools and platforms.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The <code>gpt-image-1</code> model is now available globally via the Images API, with support in the Responses API coming soon.</p>\n</blockquote>\n</div>\n","tags":["openai","ai","gpt"]},{"title":"The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation","contentType":"reshare","date":"2025-04-08 08:30","url":"/reshare/llama-4-herd/","content":"<div class=\"response-target\" >→ <a href=\"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\" >https://ai.meta.com/blog/llama-4-multimodal-intelligence/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We’re introducing Llama 4 Scout and Llama 4 Maverick, the first open-weight natively multimodal models with unprecedented context length support and our first built using a mixture-of-experts (MoE) architecture. We’re also previewing Llama 4 Behemoth, one of the smartest LLMs in the world and our most powerful yet to serve as a teacher for our new models.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks</p>\n</blockquote>\n<p>After tinkering with Mistral Small 3.1, I'm excited to try Llama 4 Scout once it lands in GitHub Models and Ollama.</p>\n<p>I don't think that Mistral Small 3.1 uses MoE architecture which should help a ton considering the 10 million token context size.</p>\n</div>\n","tags":["llama","meta","ai","llm"]},{"title":"Gen Z Leading Tumblr Revival","contentType":"reshare","date":"2025-04-08 06:05","url":"/reshare/gen-z-tumblr-revival/","content":"<div class=\"response-target\" >→ <a href=\"https://www.fastcompany.com/91310237/the-tumblr-revival-is-real-and-gen-z-is-leading-the-charge\" >https://www.fastcompany.com/91310237/the-tumblr-revival-is-real-and-gen-z-is-leading-the-charge</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Thanks to Gen Z, the site has found new life. As of 2025, Gen Z makes up 50% of Tumblr’s active monthly users and accounts for 60% of new sign-ups</p>\n</blockquote>\n<p>Don't call it a comeback.</p>\n<blockquote class=\"blockquote\">\n<p>User numbers spiked in January during the near-ban of TikTok and jumped again last year when Brazil temporarily banned X.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>To keep up with the momentum, Tumblr introduced Reddit-style Communities in December, letting users connect over shared interests like photography and video games. In January, it debuted Tumblr TV—a TikTok-like feature that serves as both a GIF search engine and a short-form video platform.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>But perhaps Tumblr’s greatest strength is that it isn’t TikTok or Facebook.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>For its users...that’s part of the appeal.</p>\n</blockquote>\n<p>Tumblr's been through a rough ride, but at the core there's a lot of nice things to like about it. In previous posts, I talked about some opportunities for Tumblr as a result of <a href=\"/responses/shipping-wordpress-tumblr/\">replatforming on top of WordPress</a> and its expressed intent for <a href=\"/responses/tumblr-still-working-fediverse-integration/\">ActivityPub integration</a>. In doing so, not only would it enable an intuitive front-end for publishing different kinds of posts, but it would also provide opportunities for self-hosting and a more federated and decentralized social platform.</p>\n<p>I'm excited to see where this goes.</p>\n<p><a href=\"https://www.businessinsider.com/gen-z-flocking-tumblr-millennials-musk-zuckerberg-safe-space-2025-4\">Related post from Business Insider's Amanda Hoover</a> on the topic.</p>\n</div>\n","tags":["tumblr","automattic","socialmedia","genz","activitypub","fediverse"]},{"title":"Bringing Static Sites to the Fediverse - Enhancements and Implementations","contentType":"reshare","date":"2025-03-18 21:08","url":"/reshare/bringing-static-sites-fediverse-enhancements-implementations-maho/","content":"<div class=\"response-target\" >→ <a href=\"https://maho.dev/2025/03/a-guide-to-implementing-activitypub-in-a-static-site-or-any-website-q1-2025-updates/\" >https://maho.dev/2025/03/a-guide-to-implementing-activitypub-in-a-static-site-or-any-website-q1-2025-updates/</a></div><div class=\"response-content\" ><p>Awesome post from my friend <a href=\"https://maho.dev/\">maho.dev</a></p>\n<blockquote class=\"blockquote\">\n<p>I’ve introduced customizable templates for note generation. This enhancement allows the full content of a blog post to be included directly in ActivityPub notes, moving beyond mere link sharing—a practice often associated with bots—and leveraging the full potential of the Fediverse.</p>\n</blockquote>\n<p>This is cool. I'd like something better for my POSSE setup. Might give this a try.</p>\n</div>\n","tags":["fediverse","indieweb","activitypub"]},{"title":"Introducing Gemma 3","contentType":"reshare","date":"2025-03-18 20:55","url":"/reshare/introducing-gemma-3/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.google/technology/developers/gemma-3/\" >https://blog.google/technology/developers/gemma-3/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, we're introducing Gemma 3, a collection of lightweight, state-of-the-art open models built from the same research and technology that powers our Gemini 2.0 models. These are our most advanced, portable and responsibly developed open models yet. They are designed to run fast, directly on devices — from phones and laptops to workstations — helping developers create AI applications, wherever people need them. Gemma 3 comes in a range of sizes (1B, 4B, 12B and 27B), allowing you to choose the best model for your specific hardware and performance needs.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Build with the world's best single-accelerator model: Gemma 3 delivers state-of-the-art performance for its size, outperforming Llama3-405B, DeepSeek-V3 and o3-mini in preliminary human preference evaluations on LMArena’s leaderboard. This helps you to <strong>create engaging user experiences that can fit on a single GPU or TPU host.</strong></p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Create AI with <strong>advanced text and visual reasoning capabilities</strong>: Easily build applications that analyze images, text, and short videos, opening up new possibilities for interactive and intelligent applications.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Handle complex tasks with an expanded context window: Gemma 3 offers a <strong>128k-token context window</strong> to let your applications process and understand vast amounts of information.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Create AI-driven workflows using function calling: <strong>Gemma 3 supports function calling and structured output</strong> to help you automate tasks and build agentic experiences.</p>\n</blockquote>\n</div>\n","tags":["gemma","google","ai"]},{"title":"Introducing Command A","contentType":"reshare","date":"2025-03-18 20:45","url":"/reshare/introducing-command-a-cohere/","content":"<div class=\"response-target\" >→ <a href=\"https://cohere.com/blog/command-a\" >https://cohere.com/blog/command-a</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, we’re introducing Command A, a new state-of-the-art generative model optimized for demanding enterprises that require fast, secure, and high-quality AI. Command A delivers maximum performance with minimal hardware costs when compared to leading proprietary and open-weights models, such as GPT-4o and DeepSeek-V3. For private deployments, Command A excels on business-critical agentic and multilingual tasks, while being deployable on just two GPUs, compared to other models that typically require as many as 32.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Its 256k context length (2x most leading models) can handle much longer enterprise documents. Other key features include Cohere’s advanced retrieval-augmented generation (RAG) with verifiable citations, agentic tool use, enterprise-grade security, and strong multilingual performance.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The next generation of Cohere models will help power a range of AI applications for customers across industries like finance, healthcare, manufacturing, energy, and the public sector. In particular, they will seamlessly integrate with North, our secure AI agents platform to unlock the full potential of your company data and people with AI agents.</p>\n</blockquote>\n</div>\n","tags":["cohere","ai","llm"]},{"title":"Simon Willison's AI-Generated Tools Colophon","contentType":"reshare","date":"2025-03-18 20:35","url":"/reshare/willison-tools-colophon/","content":"<div class=\"response-target\" >→ <a href=\"https://tools.simonwillison.net/colophon\" >https://tools.simonwillison.net/colophon</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The tools on tools.simonwillison.net were mostly built using AI-assisted programming.<br />\n<br>\nThis page lists the commit messages for each tool, many of which link to the LLM transcript used to produce the code.<br />\n<br>\nThe descriptions for each of the tools were generated using Claude 3.7 Sonnet.</p>\n</blockquote>\n<p>This is such a cool use of a website. Using AI, Simon has generated a set of small utilities that are useful to him. If they're useful to others though, he's made them avaiable on his website.</p>\n</div>\n","tags":["colophon","indieweb","ai","llm"]},{"title":"Mistral Small 3.1","contentType":"reshare","date":"2025-03-18 20:31","url":"/reshare/mistral-small-3-1/","content":"<div class=\"response-target\" >→ <a href=\"https://mistral.ai/fr/news/mistral-small-3-1\" >https://mistral.ai/fr/news/mistral-small-3-1</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Building on Mistral Small 3, this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini, while delivering inference speeds of 150 tokens per second.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Lightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases.</p>\n</blockquote>\n</div>\n","tags":["mistral","ai","opensource"]},{"title":"Just pre-ordered by PebbleOS Core Time 2 smartwatch","contentType":"reshare","date":"2025-03-18 20:01","url":"/reshare/preordered-pebbleos-watch/","content":"<div class=\"response-target\" >→ <a href=\"https://ericmigi.com/blog/introducing-two-new-pebbleos-watches\" >https://ericmigi.com/blog/introducing-two-new-pebbleos-watches</a></div><div class=\"response-content\" ><p>Just pre-ordered my Pebble Core Time 2 Duo! Sadly it won't be here until December.</p>\n<p>Go to <a href=\"https://store.repebble.com/\">https://store.repebble.com/</a> to get yours.</p>\n</div>\n","tags":["pebbleos","smartwatch","opensource","pebble"]},{"title":"Introducing AX: Why Agent Experience Matters","contentType":"reshare","date":"2025-03-05 21:49","url":"/reshare/ax-why-agent-experience-matters/","content":"<div class=\"response-target\" >→ <a href=\"https://biilmann.blog/articles/introducing-ax/\" >https://biilmann.blog/articles/introducing-ax/</a></div><div class=\"response-content\" ><p>Interesting post from <a href=\"https://biilmann.blog/\">Matt Biilman</a>.</p>\n<blockquote class=\"blockquote\">\n<p>...the largest disruption from the current evolution of AI will come from bringing agency to computers.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Computers will perceive their environment and take actions autonomously in order to achieve goals.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Companies discovered that improving the DX of their products would empower and incentivize developers to extend their product with new capabilities and lead to huge competitive advantages. For developer tool companies, DX became a key competitive differentiator.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We need to start focusing on AX or “agent experience” — the holistic experience AI agents will have as the user of a product or platform.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Too many companies are focusing on adding shallow AI features all over their products or building yet another AI agent. The real breakthrough will be thinking about how your customers’ favorite agents can help them derive more value from your product. This requires thinking deeply about agents as a persona your team is building and developing for.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>As AI agents start becoming useful and commonplace, we’re broadly going to see two approaches enabling agents to interact with the software we depend on:<br />\n<br>\nA closed vertical approach, where companies tightly integrate their own agents into their own software. An open approach, where companies focus on making their software accessible to external agents.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Leaning into AX as a strategy, means embracing a vision of an open agent world. This vision aligns with the original ethos of the open web: a place where many diverse competing agents (built by different people or companies) can seamlessly interact with software on behalf of their users. Prioritizing AX makes it as simple as possible for any agent a user prefers, to deliver outcomes on their behalf.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Software designed for AI agents has the potential to deliver exponential value. As an industry we must collectively focus on building an open agent ecosystem and designing thoughtful AX to create a better, more open, and connected digital world.</p>\n</blockquote>\n</div>\n","tags":["ai","agents","ax"]},{"title":"Introducing Mercury, the first commercial-scale diffusion large language model","contentType":"reshare","date":"2025-02-26 20:10","url":"/reshare/mercury-first-commercial-scale-llm/","content":"<div class=\"response-target\" >→ <a href=\"https://www.inceptionlabs.ai/news\" >https://www.inceptionlabs.ai/news</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We are announcing the Mercury family of diffusion large language models (dLLMs), a new generation of LLMs that push the frontier of fast, high-quality text generation.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Mercury is up to 10x faster than frontier speed-optimized LLMs. Our models run at over 1000 tokens/sec on NVIDIA H100s, a speed previously possible only using custom chips.</p>\n</blockquote>\n</div>\n","tags":["diffusion","llm","ai"]},{"title":"The Algorithm - J. Cole has a blog?","contentType":"reshare","date":"2025-02-25 21:34","url":"/reshare/inevitable-jcole-blog/","content":"<div class=\"response-target\" >→ <a href=\"https://www.inevitable.live/algorithm/first-post\" >https://www.inevitable.live/algorithm/first-post</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This is just a place for me to share. I been wanting a lil blog for years. Somewhere to post random shit I fuck with where the audience is way smaller than it is on the social media platforms. Finally pulled the trigger, bare with us as we still developing this page and the layout.</p>\n</blockquote>\n<p>This is cool!</p>\n<p>I ran into this while reading the post <a href=\"https://herbertlui.net/blogging-for-smaller-audiences-and-deeper-connections/\">Blogging for smaller audiences and deeper connections</a> by <a href=\"https://herbertlui.net/\">Herbert Lui</a>.</p>\n<p>Looking at <a href=\"https://www.inevitable.live/algorithm\">other posts</a>, it looks like he or at least someone is actively posting.</p>\n</div>\n","tags":["music","blog","indieweb","creativity","art"]},{"title":"Your Site Is a Home","contentType":"star","date":"2025-02-25 20:49","url":"/star/your-site-is-a-home-hamid/","content":"<div class=\"response-target\" >→ <a href=\"https://nazhamid.com/journal/your-site-is-a-home/\" >https://nazhamid.com/journal/your-site-is-a-home/</a></div><div class=\"response-content\" ><p>While going through a backlog of personal blogs I haven't read in a few weeks, I'm noticing a trend in posts that express similar sentiments to those in Naz's post.</p>\n<p>I'm happy to see more people choosing to have a place that gives them more autonomy and freedom to express themselves in whichever form makes the most sense for them.</p>\n<blockquote class=\"blockquote\">\n<p>Your site is a home.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Eventually, social networks were created...novelty and the promise of interconnectedness by gathering in a common town square to blast out whatever was going on in our lives eventually won out.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>But you might have still had your website, your home to return to...the place you built.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Things started to change and instead of going home, you, and everybody else started to live in the town square.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>There is a shape to your physical home. Arranged and organized in the ways that make sense for you, the reward is a space that works for you.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>A platform or network doesn’t allow for much configuration. The town square isn’t owned by you.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>People became absolutely reliant on the same gathering place...the convenience of seeing friends (sometimes) outweighed your other neighbors spouting garbage and hate. You came to rely on this place for everything.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>You can still have a home.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I have very little at the town square, because it’s not a public one. It’s a walled-off town square, whose rules and borders change at the whims of those who created it.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Thanks for visiting my home. I’m glad you dropped by.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I’d love to see yours sometime.</p>\n</blockquote>\n</div>\n","tags":["personalweb","indieweb","socialmedia"]},{"title":"How to Multistream Using Azure, Azure VM, MonaServer 2, and FFmpeg with OBS Studio","contentType":"reshare","date":"2025-02-25 20:33","url":"/reshare/multistream-azure-ffmpeg-obs-studio-maho/","content":"<div class=\"response-target\" >→ <a href=\"https://maho.dev/2025/02/how-to-multistream-using-azure-azure-vm-monaserver-2-and-ffmpeg-with-obs-studio/\" >https://maho.dev/2025/02/how-to-multistream-using-azure-azure-vm-monaserver-2-and-ffmpeg-with-obs-studio/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In this guide, we will set up a multistreaming workflow using Azure, an Azure VM, MonaServer 2, and FFmpeg to broadcast to multiple platforms like LinkedIn Live and YouTube Live. We’ll use OBS Studio as the main streaming software. Let’s get started.</p>\n</blockquote>\n<p>Love this detailed writeup from <a href=\"https://maho.dev/\">Maho</a>.</p>\n<p>When I originally experimented with something similar using my Owncast instance, I was running ffmpeg locally but I like his solution better.</p>\n</div>\n","tags":["livestream","azure","obs"]},{"title":"Unplatform","contentType":"reshare","date":"2025-02-25 20:20","url":"/reshare/unplatform-indieweb/","content":"<div class=\"response-target\" >→ <a href=\"https://unplatform.fromthesuperhighway.com/\" >https://unplatform.fromthesuperhighway.com/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Unplatform is an interactive guidebook, online library, and recommendations database intended to help you escape social media and join the indie web.</p>\n</blockquote>\n<p>Love this guide.</p>\n</div>\n","tags":["indieweb","socialmedia","personalweb"]},{"title":"Nomic Embed Text V2: An Open Source, Multilingual, Mixture-of-Experts Embedding Model","contentType":"reshare","date":"2025-02-24 20:47","url":"/reshare/nomic-embed-text-v2/","content":"<div class=\"response-target\" >→ <a href=\"https://www.nomic.ai/blog/posts/nomic-embed-text-v2\" >https://www.nomic.ai/blog/posts/nomic-embed-text-v2</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today we're excited to announce Nomic Embed Text V2, our next-generation embedding model that brings the Mixture of Experts (MoE) architecture to text embeddings on a new expanded multilingual training dataset.</p>\n</blockquote>\n<p>Personally, I found the part on MoE to be the most interesting about this release.</p>\n<blockquote class=\"blockquote\">\n<p>Rather than a dense model which uses all parameters on an input, the MoE architecture dynamically routes to different &amp;quot;experts&amp;quot; - sparse subsets of parameters at each layer - activating, ideally, only the parameters especially needed to process the input. This approach allows for more efficient use of compute when generating embeddings.<br />\n<br>\nIn our experiments, we found that alternating MoE layers with 8 experts and top-2 routing provides the optimal balance between performance and efficiency. This results in 475M total parameters in the model, but only 305M active during training and inference.<br />\n<br>\nResearch into embedding model architecture has significant practical implications for working with text embeddings in production:<br />\n<br></p>\n<ul>\n<li>Lower latency for high-volume applications of embeddings like retrieval<br />\n<br></li>\n<li>Reduced deployment costs through more efficient parameter usage<br />\n<br></li>\n<li>More accessibility to embeddings in settings with constrained compute</li>\n</ul>\n</blockquote>\n</div>\n","tags":["nomic","ai","embeddings"]},{"title":"Claude 3.7 Sonnet and Claude Code","contentType":"reshare","date":"2025-02-24 20:43","url":"/reshare/anthropic-claude-3-7-sonnet-code/","content":"<div class=\"response-target\" >→ <a href=\"https://www.anthropic.com/news/claude-3-7-sonnet\" >https://www.anthropic.com/news/claude-3-7-sonnet</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, we’re announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model can think for.\n<br>\nClaude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development. Along with the model, we’re also introducing a command line tool for agentic coding, Claude Code. Claude Code is available as a limited research preview, and enables developers to delegate substantial engineering tasks to Claude directly from their terminal.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Claude 3.7 Sonnet and Claude Code mark an important step towards AI systems that can truly augment human capabilities. With their ability to reason deeply, work autonomously, and collaborate effectively, they bring us closer to a future where AI enriches and expands what humans can achieve.</p>\n</blockquote>\n</div>\n","tags":["ai","anthropic","claude"]},{"title":"The Ultra-Scale Playbook: Training LLMs on GPU Clusters","contentType":"reshare","date":"2025-02-19 22:03","url":"/reshare/ultrascale-playbook-training-llms-gpu-clusters/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/spaces/nanotron/ultrascale-playbook\" >https://huggingface.co/spaces/nanotron/ultrascale-playbook</a></div><div class=\"response-content\" ><p>Gold.</p>\n<blockquote class=\"blockquote\">\n<p>All the techniques we'll cover in this book tackle one or several of the following three key challenges, which we'll keep bumping into throughout the book:</p>\n<hr>\n  1. **Memory Usage**: it's a hard limitation - if a training step doesn't fit in memory, training cannot proceed  \n  2. **Compute Efficiency**: we want our hardware to spend most time computing, so we need to reduce time spent on data transfers or waiting for other GPUs to perform work.  \n  3. **Communication overhead**: we want to minimize communication overhead as it keeps GPUs idle. To archieve this we will try to make best use of intra-node (fast) and inter-node (slower) bandwidths as well as overlap communication with compute as much as possible.  \n<hr>\nIn many places we'll see that we can trade one of these (computation, communication, memory) for another (e.g. recomputation or Tensor Parallelism). Finding the right balance is key to scaling training.\n</blockquote>\n<p><img src=\"https://nanotron-ultrascale-playbook.static.hf.space/dist/files/images/ultra-cheatsheet.svg\" class=\"img-fluid\" alt=\"Ultra-Scale Playbook Cheatsheet\" /></p>\n</div>\n","tags":["ai","llm","training"]},{"title":"Dream Job - Google Super Bowl 2025 Ad","contentType":"star","date":"2025-02-17 16:00","url":"/star/google-dream-job-2025-sb-ad/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=-7e6g11BJc0\" >https://www.youtube.com/watch?v=-7e6g11BJc0</a></div><div class=\"response-content\" ><p>I didn't watch the Super Bowl this year so I didn't get a chance to watch the ads until a few days later.</p>\n<p>From the ones I saw, I think Google's was my favorite.</p>\n<p>Apart from the touching story, what stood out to me the most was its focus on humanity, with technology playing a supporting role rather than being the main character.</p>\n<p><a href=\"https://www.youtube.com/watch?v=-7e6g11BJc0\" title=\"Thumbnail Google Dream Job Super Bowl 2025 Ad\"><img src=\"http://img.youtube.com/vi/-7e6g11BJc0/0.jpg\" class=\"img-fluid\" alt=\"Thumbnail Google Dream Job Super Bowl 2025 Ad\" /></a></p>\n</div>\n","tags":["google","ad","superbowl","ai"]},{"title":"Languages & Runtime Community Standup - Tensors in .NET","contentType":"reshare","date":"2025-02-15 15:12","url":"/reshare/dotnet-language-runtime-standup-tensors/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=VOEeNffChSg\" >https://www.youtube.com/watch?v=VOEeNffChSg</a></div><div class=\"response-content\" ><p>Great session on Tensors in .NET by <a href=\"https://bsky.app/profile/tannergooding.bsky.social\">Tanner</a></p>\n<p>Tensors are the foundational data structure powering man AI workloads today.</p>\n<p>Introduced in .NET 9, <a href=\"https://learn.microsoft.com/en-us/dotnet/core/whats-new/dotnet-9/overview#tensort\">Tensors</a> builds on top of earlier work like <a href=\"https://devblogs.microsoft.com/dotnet/announcing-ml-net-3-0/#tensor-primitives-integration\">TensorPrimitives</a>, <a href=\"https://devblogs.microsoft.com/dotnet/hardware-intrinsics-in-net-core/\">hardware intrinsics</a>, and <a href=\"https://learn.microsoft.com/en-us/dotnet/standard/generics/math\">generic math</a>.</p>\n<p>You can check out the recording below to learn more.</p>\n<p><a href=\"https://www.youtube.com/watch?v=VOEeNffChSg\" title=\"Language Runtime Community Standup Tensors in .NET Thumbnail\"><img src=\"http://img.youtube.com/vi/VOEeNffChSg/0.jpg\" class=\"img-fluid\" alt=\"Language Runtime Community Standup Tensors in .NET Thumbnail\" /></a></p>\n</div>\n","tags":["ai","tensor","dotnet","runtime"]},{"title":"Generative AI for Beginners (.NET) is now available","contentType":"reshare","date":"2025-02-15 12:14","url":"/reshare/generative-ai-for-beginners-dotnet-course/","content":"<div class=\"response-target\" >→ <a href=\"https://devblogs.microsoft.com/dotnet/announcing-generative-ai-for-beginners-dotnet/\" >https://devblogs.microsoft.com/dotnet/announcing-generative-ai-for-beginners-dotnet/</a></div><div class=\"response-content\" ><p>If you're a .NET developer looking to get started with Generative AI, there's <a href=\"https://devblogs.microsoft.com/dotnet/announcing-generative-ai-for-beginners-dotnet/\">a new course for beginners</a> that just launched.</p>\n<p>Through a series of self-paced lessons, you'll learn the foundations to help you start using AI to augment the capabilities of your .NET applications.</p>\n<p>Check out the <a href=\"https://aka.ms/genainet\">course</a> and give us feedback!</p>\n</div>\n","tags":["ai","dotnet","course","generativeai"]},{"title":"AI Dev Gallery now in the Microsoft Store","contentType":"reshare","date":"2025-02-15 12:02","url":"/reshare/ai-dev-gallery-windows-preview-ms-store/","content":"<div class=\"response-target\" >→ <a href=\"https://apps.microsoft.com/detail/9n9pn1mm3bd5\" >https://apps.microsoft.com/detail/9n9pn1mm3bd5</a></div><div class=\"response-content\" ><p>So excited to see the <a href=\"https://apps.microsoft.com/detail/9n9pn1mm3bd5\">AI Dev Gallery is now in the Microsoft Store</a>.</p>\n<p>Through various interactive samples, developers get to see how they might use AI for different tasks like text classification, object detection, and many others.</p>\n<p>Best of all, the models are all running locally and you get to see and export the C# code powering the samples to Visual Studio so you can continue tinkering on your own and integrate into your own applications.</p>\n<p>A few months ago, the team came on the .NET AI Community Standup to showcase the app. Since then, it's only kept improving and introducing new scenarios.</p>\n<p>You can check out the recording from that stream here.</p>\n<p><a href=\"https://www.youtube.com/watch?v=5H9TxzCQfNo\" title=\"Thumbnail of AI Community Standup AI Dev Gallery\"><img src=\"http://img.youtube.com/vi/5H9TxzCQfNo/0.jpg\" class=\"img-fluid\" alt=\"Thumbnail of AI Community Standup AI Dev Gallery\" /></a></p>\n</div>\n","tags":["ai","windows","dotnet","microsoft","onnx"]},{"title":"Why Blog If Nobody Reads It?","contentType":"reshare","date":"2025-02-10 17:02","url":"/reshare/why-blog-nobody-reads-andy/","content":"<div class=\"response-target\" >→ <a href=\"https://andysblog.uk/why-blog-if-nobody-reads-it/\" >https://andysblog.uk/why-blog-if-nobody-reads-it/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>There is a hidden value in blogging... You do it not for the applause but because it needs doing.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>You write because you think, because you observe, because you need to put it somewhere.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>If someone reads it? Bonus. If not? The work still got done.</p>\n</blockquote>\n<p>💯💯💯</p>\n<p>I agree with many of the points in Andy's post.</p>\n<p>I don't have analytics on my website, so I couldn't tell you who or how many people are visiting it.</p>\n<p>And that's okay.</p>\n<p>When someone <a href=\"https://malici.ous.computer/@shellsharks/statuses/01JKGDJCE0WZYAWRB816J8JFG2\">links to content they found on my website and tags me</a> or <a href=\"/notes/cdn-issue-fixed-redirects-working\">sends me an e-mail letting me know something on my website is broken</a> is when I know there's at least one person.</p>\n<p>Many of the content on here serves as a reminder of how I've solved problems for myself. The microposts and responses are observations that capture a moment in time. If the messages reach someone on the other end and either solve their problem or resonate with them, I'm happy that happens, but I see that as a bonus.</p>\n</div>\n","tags":["blogging","expression","creativity","writing"]},{"title":"Bringing Pebble Back","contentType":"reshare","date":"2025-02-06 19:42","url":"/reshare/bringing-pebble-back/","content":"<div class=\"response-target\" >→ <a href=\"https://ericmigi.com/blog/why-were-bringing-pebble-back\" >https://ericmigi.com/blog/why-were-bringing-pebble-back</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p><strong>TL;DR</strong>\nWe’re making a new Pebble-style smartwatch.</p>\n</blockquote>\n<p>This is amazing news! By the time I decided to get a Pebble, they'd already been sold to Google.</p>\n<p>I agree with Eric's points here:</p>\n<blockquote class=\"blockquote\">\n<p>I've tried every single smart watch out there, but none do it for me. No one makes a smartwatch with the core set of features I want:<br />\n<br></p>\n<ul>\n<li><strong>Always-on e-paper screen</strong> (it’s reflective rather than emissive. Sunlight readable. Glanceable. Not distracting to others like a bright wrist)</li>\n<li><strong>Long battery life</strong> (one less thing to charge. It’s annoying to need extra cables when traveling)\nSimple and beautiful user experience around a core set of features I use regularly (telling time, notifications, music control, alarms, weather, calendar, sleep/step tracking)</li>\n<li><strong>Buttons!</strong> (to play/pause/skip music on my phone without looking at the screen)</li>\n<li><strong>Hackable</strong> (apparently you can’t even write your own watchfaces for Apple Watch? That is wild. There were &amp;gt;16k watchfaces on the Pebble appstore!)</li>\n</ul>\n</blockquote>\n<p>Garmin is the closest I've come to it at least on the battery life and buttons front. Pebble was one-of-a-kind.</p>\n<p>I already signed up to stay up to date with the project and plan on being a day one customer. You can too at <a href=\"https://repebble.com/\">repebble.com</a>.</p>\n<p>P.S. While not surprised, I didn't know Eric was behind another amazing project I'm a fan of, <a href=\"https://www.beeper.com/\">Beeper</a>.</p>\n</div>\n","tags":["eink","pebble","smartwatch","google","opensource"]},{"title":"Build a link blog","contentType":"reshare","date":"2025-02-04 22:29","url":"/reshare/build-a-link-blog-xuanwo/","content":"<div class=\"response-target\" >→ <a href=\"https://xuanwo.io/links/2025/01/link-blog/\" >https://xuanwo.io/links/2025/01/link-blog/</a></div><div class=\"response-content\" ><p>Great to see more people sharing knowledge and linking to others from their own websites / platforms.</p>\n<blockquote class=\"blockquote\">\n<p>I decided to follow simon's approach to creating a link blog, where I can share interesting links I find on the internet along with my own comments and thoughts about them.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...this is an excellent way to share knowledge while also keeping a personal record. It’s much better than simply saving something to Readwise to read later and leaving a few highlights or dull comments like &amp;quot;interesting.&amp;quot;</p>\n</blockquote>\n</div>\n","tags":["blogging","indieweb","linkblog","personalweb","openweb"]},{"title":"Master the Art of the Product Manager 'No'","contentType":"star","date":"2025-02-03 20:26","url":"/star/master-art-of-product-manager/","content":"<div class=\"response-target\" >→ <a href=\"https://letsnotdothat.com/\" >https://letsnotdothat.com/</a></div><div class=\"response-content\" ><p>😂😂😂</p>\n<p>This website is amazing.</p>\n<blockquote class=\"blockquote\">\n<p>We already finalized our OKRs for H1 but if you throw it in the backlog maybe we can get it prioritized in H2 planning</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This could be part of a larger initiative, but let’s hold off for now</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We should validate that with users first</p>\n</blockquote>\n</div>\n","tags":["fun","business","productmanager"]},{"title":"Introducing deep research","contentType":"reshare","date":"2025-02-03 20:17","url":"/reshare/openai-deep-research/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/index/introducing-deep-research/\" >https://openai.com/index/introducing-deep-research/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>An agent that uses reasoning to synthesize large amounts of online information and complete multi-step research tasks for you.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Powered by a version of the upcoming OpenAI o3 model that’s optimized for web browsing and data analysis, it leverages reasoning to search, interpret, and analyze massive amounts of text, images, and PDFs on the internet, pivoting as needed in reaction to information it encounters.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>Deep research was trained using end-to-end reinforcement learning on hard browsing and reasoning tasks across a range of domains. Through that training, it learned to plan and execute a multi-step trajectory to find the data it needs, backtracking and reacting to real-time information where necessary. The model is also able to browse over user uploaded files, plot and iterate on graphs using the python tool, embed both generated graphs and images from websites in its responses, and cite specific sentences or passages from its sources. As a result of this training, it reaches new highs on a number of public evaluations focused on real-world problems.</p>\n</blockquote>\n</div>\n","tags":["ai","openai","agent","research"]},{"title":"A Guide to Implementing ActivityPub in a Static Site (or Any Website) - Part 8","contentType":"reshare","date":"2025-02-02 18:23","url":"/reshare/guide-implementing-activitypub-static-site-pt-8/","content":"<div class=\"response-target\" >→ <a href=\"https://www.maho.dev/2025/01/a-guide-to-implementing-activitypub-in-a-static-site-or-any-website-part-8/\" >https://www.maho.dev/2025/01/a-guide-to-implementing-activitypub-in-a-static-site-or-any-website-part-8/</a></div><div class=\"response-content\" ><p>Part 8 of <a href=\"https://www.maho.dev/2024/02/a-guide-to-implement-activitypub-in-a-static-site-or-any-website/\">&amp;quot;A guide to implement ActivityPub in a static site (or any website)&amp;quot;</a> by <a href=\"https://www.maho.dev/\">maho.dev</a> recently dropped.</p>\n<p>This part goes over how to add replies and comments to your static site.</p>\n</div>\n","tags":["fediverse","indieweb","activitypub","personalweb","internet","opensource"]},{"title":"Buc-ee's plans first Wisconsin travel center","contentType":"star","date":"2025-01-28 20:25","url":"/star/bucees-wisconsin/","content":"<div class=\"response-target\" >→ <a href=\"https://www.wisn.com/article/bucees-plans-first-wisconsin-travel-center-in-oak-creek/63527062\" >https://www.wisn.com/article/bucees-plans-first-wisconsin-travel-center-in-oak-creek/63527062</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Buc-ee's, the popular Texas-based travel center chain, is set to open its first Wisconsin location in Oak Creek, city officials announced.</p>\n</blockquote>\n<p>Time to get my banana pudding fix.</p>\n</div>\n","tags":["bucees","wisconsin","travel"]},{"title":"Short Long Form","contentType":"reshare","date":"2025-01-20 21:21","url":"/reshare/short-long-form-moreale/","content":"<div class=\"response-target\" >→ <a href=\"https://manuelmoreale.com/short-long-form\" >https://manuelmoreale.com/short-long-form</a></div><div class=\"response-content\" ><p>This is something I've been dealing with on my site. I separate my <a href=\"/posts/1\">long-form</a> from <a href=\"/feed\">short-form</a> posts. However, I don't have a hard limit that clearly defines which is which. An initial reason for the separation was, I wanted to have a feed that was easy to scroll through. That was harder to do if I was rendering an entire long-form post as part of the feed. In the end though, most of my posts are all markdown files and their main distinction is how they are rendered.</p>\n<p>As I work on my website redesign, I want to change how I think about posts. I want to separate the post content from the rendering. A post is a post regardless of content and length. Regardless of whether it's a response, image, video, note, or anything in between, I don't want to constrain the content of my post. I can choose to render posts differently depending on whether it's being viewed in a feed or individual post page. The content driving those different views though shouldn't change.</p>\n</div>\n","tags":["instagram","posts","attention"]},{"title":"Semantic Search and On-Device ML in Emacs","contentType":"reshare","date":"2025-01-20 21:09","url":"/reshare/semantic-search-on-device-ml-emacs/","content":"<div class=\"response-target\" >→ <a href=\"https://lepisma.xyz/2025/01/17/emacs-on-device-ml/index.html\" >https://lepisma.xyz/2025/01/17/emacs-on-device-ml/index.html</a></div><div class=\"response-content\" ><p>This is a cool walk-through of how to add semantic search to Emacs using local ML models.</p>\n<blockquote class=\"blockquote\">\n<p>With ONNX.el you can run any ML model inside Emacs, including ones for images, audios, etc. In case you are running embedding models, combine this with sem.el to perform semantic searches. Depending on your input modality, you might have to figure out preprocessing. For text, tokenizers.el should cover all of what you need in modern NLP for preprocessing, though you will have to do something on your own for anything non-text or multimodal.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>By default sem runs a local all-MiniLM-L6-v2 model for text embeddings. This model is small and runs fast on CPU. Additionally, we load an O2 optimized variant which helps further. The vectors are stored in a lancedb database on file system which runs without a separate process. I was originally writing the vector db core myself to learn more of Rust, but then stopped doing that since lancedb gives all that I needed, out of the box.</p>\n</blockquote>\n</div>\n","tags":["onnx","emacs","ai","ml"]},{"title":"Digital soverignity is more important than ever","contentType":"star","date":"2025-01-20 20:46","url":"/star/digital-sovereignty-white/","content":"<div class=\"response-target\" >→ <a href=\"https://www.mollywhite.net/micro/entry/202501190959\" >https://www.mollywhite.net/micro/entry/202501190959</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...you needed to move anything you care about online to a space you control. Digital sovereignty is more important than ever.</p>\n</blockquote>\n<p>💯💯💯💯💯</p>\n<p>Best time was yesterday. Next best time is now.</p>\n</div>\n","tags":["personalweb","digitalsoverignty","indieweb"]},{"title":"Making space for a handmade web","contentType":"reshare","date":"2025-01-18 19:25","url":"/reshare/making-space-handmade-web/","content":"<div class=\"response-target\" >→ <a href=\"https://www.figma.com/blog/making-space-for-a-handmade-web/\" >https://www.figma.com/blog/making-space-for-a-handmade-web/</a></div><div class=\"response-content\" ><p>Love this article.</p>\n<blockquote class=\"blockquote\">\n<p>Before the web was more commercialized, it was personal...A website was a way to represent yourself and connect to others, a space to tend and call your very own.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The imperfect, handmade character of websites has been sanded away in favor of efficiency, and social media platforms have risen in the stead of amateur sites. Many of us conform to these containers and retreat from expressing our authentic selves publicly...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>If the idea of a more handcrafted internet resonates with you, the best way to be part of the movement is simply to make your own website. This may seem intimidating if we expect webpages to be a holistic reflection of ourselves, like a resume, portfolio, or blog. We limit the web as a medium when we expect that a website must fulfill a host of needs, or serve any function at all. <strong>A website doesn’t need to be anything but your own</strong>.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>A more personal web becomes possible once we turn away from our preconceived notions of what a website must be. Let’s use our tools to continuously push at the boundaries of the web. Doing so will create a more creative, expansive environment that allows us to reclaim our agency and reinvigorate our collective understanding of software as an artisanal craft. Instead of concentrating on a few, monolithic sites, what if we visited many, more personalized sites? We should explore the far reaches of the web and invite our friends to join in: After all, <strong>we make the internet, and only we can build the web we want</strong>.</p>\n</blockquote>\n</div>\n","tags":["indieweb","personalweb","internet","websites","handmadeweb"]},{"title":"R.I.P David Lynch","contentType":"reshare","date":"2025-01-16 19:11","url":"/reshare/rip-david-lynch/","content":"<div class=\"response-target\" >→ <a href=\"https://apnews.com/article/david-lynch-dies-9107f3ce0b4dd49dbe3dc2ae3c09ed59\" >https://apnews.com/article/david-lynch-dies-9107f3ce0b4dd49dbe3dc2ae3c09ed59</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>David Lynch, the filmmaker celebrated for his uniquely dark and dreamlike vision in such movies as “Blue Velvet” and “Mulholland Drive” and the TV series “Twin Peaks,” has died just days before his 79th birthday.</p>\n</blockquote>\n<p>I didn't understand all of his work, but I appreciate his originality and overall aesthetic.</p>\n</div>\n","tags":["davidlynch","twinpeaks"]},{"title":"Swarm navigation of cyborg-insects in unknown obstructed soft terrain","contentType":"reshare","date":"2025-01-15 21:02","url":"/reshare/swarm-cyborg-insects-agents/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2403.17392\" >https://arxiv.org/abs/2403.17392</a></div><div class=\"response-content\" ><p>I'm going to tell future generations, this is how we built multi-agent systems back in my day.</p>\n<blockquote class=\"blockquote\">\n<p>Cyborg insects refer to hybrid robots that integrate living insects with miniature electronic controllers to enable robotic-like programmable control. These creatures exhibit advantages over conventional robots in adaption to complex terrain and sustained energy efficiency. Nevertheless, there is a lack of literature on the control of multi-cyborg systems. This research gap is due to the difficulty in coordinating the movements of a cyborg system under the presence of insects' inherent individual variability in their reactions to control input. Regarding this issue, we propose a swarm navigation algorithm and verify it under experiments. This research advances swarm robotics by integrating biological organisms with control theory to develop intelligent autonomous systems for real-world applications.</p>\n</blockquote>\n</div>\n","tags":["robotics","ai","insects","agents"]},{"title":"AI Subtitles Are Coming to VLC","contentType":"reshare","date":"2025-01-15 20:49","url":"/reshare/ai-subtitles-coming-vlc/","content":"<div class=\"response-target\" >→ <a href=\"https://news.itsfoss.com/ai-subtitles-vlc/\" >https://news.itsfoss.com/ai-subtitles-vlc/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Shown off at CES 2025 is VLC's upcoming automatic subtitling feature, which is local in nature, running entirely on a user's machine without needing to connect to some distant server.<br />\n<br>\nAccording to Jean-Baptiste Kempf, the creator and lead developer of VLC, this new feature is powered by open source AI models, which carry out subtitle generation tasks in two stages.<br />\n<br>\nOne aspect is automatic generation of subtitles from the video, and the other is translation of those subtitles into over 100 languages.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Unfortunately, there's no word on when this feature will be arriving on VLC.</p>\n</blockquote>\n</div>\n","tags":["vlc","ai","opensource","local"]},{"title":"The Future of Calm Technology with Amber Case","contentType":"reshare","date":"2025-01-14 21:13","url":"/reshare/calm-tech-interview/","content":"<div class=\"response-target\" >→ <a href=\"https://josebriones.substack.com/p/the-future-of-calm-technology-with\" >https://josebriones.substack.com/p/the-future-of-calm-technology-with</a></div><div class=\"response-content\" ><p>Interesting interview and introduction to <a href=\"https://calmtech.com/\">Calm Technology</a> and the <a href=\"https://www.calmtech.institute/\">Calm Tech Institute</a>.</p>\n</div>\n","tags":["digitalminimalism","technology"]},{"title":"Stop renting space for your notes and start being your own landlord","contentType":"reshare","date":"2025-01-13 21:54","url":"/reshare/landlord-of-your-notes/","content":"<div class=\"response-target\" >→ <a href=\"https://ellanew.com/ptpl/139-2025-01-13-be-the-landlord-of-your-notes\" >https://ellanew.com/ptpl/139-2025-01-13-be-the-landlord-of-your-notes</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>When you choose to keep your notes in locally stored plain text files, you’re choosing to be your own landlord. Keeping notes in a proprietary app is the same as renting: the land you’re building on belongs to someone else, with no guarantee of continuing access.</p>\n</blockquote>\n<p>This is so true for many other areas of our digital lives as well.</p>\n</div>\n","tags":["localfirst","notetaking","pkm"]},{"title":"Welcome to the Public Domain in 2025","contentType":"reshare","date":"2025-01-03 15:53","url":"/reshare/internet-archive-public-domain-2025/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.archive.org/2025/01/01/welcome-to-the-public-domain-in-2025/\" >https://blog.archive.org/2025/01/01/welcome-to-the-public-domain-in-2025/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>On January 1, 2025, we celebrate published works from 1929 and published sound recordings from 1924 entering the public domain! The passage of these works into the public domain celebrates our shared cultural heritage. The ability to breathe new life into long forgotten works, remix the most popular and enduring works of the time, and to better circulate the oddities we find in thrift stores, attics, and on random pockets of the internet are now freely available for us all.<br />\n<br>\nWhile not at the same blockbuster level as 2024 with Steamboat Willie’s passage into the public domain, works from 1929 still inhabit strong cultural significance today. The works of 1929 continue to capture the Lost Generation’s voice, the rise of sound film, and the emerging modern moment of the 1920s.</p>\n</blockquote>\n</div>\n","tags":["internetarchive","publicdomain"]},{"title":"LangChain State of AI 2024 Report","contentType":"reshare","date":"2024-12-29 12:12","url":"/reshare/langchain-state-of-ai-2024/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.langchain.dev/langchain-state-of-ai-2024/\" >https://blog.langchain.dev/langchain-state-of-ai-2024/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In 2024, developers leaned into complexity with multi-step agents, sharpened efficiency by doing more with fewer LLM calls, and added quality checks to their apps using methods of feedback and evaluation.</p>\n</blockquote>\n<p><img src=\"https://blog.langchain.dev/content/images/size/w1600/2024/12/Top-10-LLM-Providers-bar-chart.png\" class=\"img-fluid\" alt=\"Bar Chart Showing Top 10 LLMs Used 2024\" /></p>\n<p><img src=\"https://blog.langchain.dev/content/images/size/w1600/2024/12/Top-10-Retrievers--1-.png\" class=\"img-fluid\" alt=\"Graphic showing Top 10 Retrievers / Vector Stores 2024\" /></p>\n<p><img src=\"https://blog.langchain.dev/content/images/size/w1600/2024/12/Top-evaluation-metrics--1-.png\" class=\"img-fluid\" alt=\"Graphic showing Top LLM Evaluation Metrics 2024\" /></p>\n</div>\n","tags":["ai","2024","report","langchain","llm"]},{"title":"MarkItDown - Convert files to Markdown","contentType":"reshare","date":"2024-12-18 21:43","url":"/reshare/microsoft-markitdown/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/microsoft/markitdown\" >https://github.com/microsoft/markitdown</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>MarkItDown is a utility for converting various files to Markdown (e.g., for indexing, text analysis, etc). It supports:<br />\n<br></p>\n<ul>\n<li>PDF</li>\n<li>PowerPoint</li>\n<li>Word</li>\n<li>Excel</li>\n<li>Images (EXIF metadata and OCR)</li>\n<li>Audio (EXIF metadata and speech transcription)</li>\n<li>HTML</li>\n<li>Text-based formats (CSV, JSON, XML)</li>\n<li>ZIP files (iterates over contents)</li>\n</ul>\n</blockquote>\n</div>\n","tags":["markdown","etl","data","python","ai","microsoft"]},{"title":"Agentic Fediverse","contentType":"reshare","date":"2024-12-17 19:44","url":"/reshare/agentic-fediverse/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/muni-town/agentic-fediverse\" >https://github.com/muni-town/agentic-fediverse</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The &amp;quot;Agentic Fediverse&amp;quot; is the idea of a new kind of network federation, a complementary iteration on the concept of federation currently established with ActivityPub. It's an experiment and something that we in Muni Town are still trying to define more concretely.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>We are still working on defining the core tenets of the agentic fediverse, but here is what we have so far.<br />\n<br>\nAn agentic fediverse is...<br />\n<br></p>\n<ol>\n<li>A fediverse of agents; agent-centric, as opposed to server-centric.</li>\n<li>Local-first; solve local problems for local people.</li>\n<li>Accessible by default; inaccessibility is a bug.</li>\n<li>Systematically consensual; architected on the basis of informed consent.</li>\n</ol>\n</blockquote>\n</div>\n","tags":["fediverse","munitown","agents","agentic","activitypub","protocols","socialweb"]},{"title":"A New Social Web - Hello, Social Web","contentType":"reshare","date":"2024-12-17 19:39","url":"/reshare/hello-social-web/","content":"<div class=\"response-target\" >→ <a href=\"https://www.anew.social/hello-social-web/\" >https://www.anew.social/hello-social-web/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We're A New Social, a new non-profit organization focused on building cross-protocol tools and services for the open social web.<br />\n<br>\nOur mission is to liberate people's networks from their platforms, enabling <a href=\"https://www.augment.ink/bridges-the-last-network-effect/?ref=anew.social\">The Last Network Effect</a> and leveling the playing field across the open social web.<br />\n<br>\nThe first project we'll take on to accomplish this mission is <a href=\"https://fed.brid.gy/?ref=anew.social\">Bridgy Fed</a>, a service that enables users of ActivityPub-based platforms like Mastodon, ATProto-based platforms like Bluesky, and websites to interact and engage across ecosystems.</p>\n</blockquote>\n</div>\n","tags":["anewsocial","fediverse","socialmedia","protocols","atproto","activitypub","socialweb","personalweb"]},{"title":"StreamBuilder: our open-source framework for powering your dashboard","contentType":"reshare","date":"2024-12-17 19:03","url":"/reshare/streambuilder-tumblr-framework-building-feed/","content":"<div class=\"response-target\" >→ <a href=\"https://engineering.tumblr.com/post/722102563011493888/streambuilder-our-open-source-framework-for\" >https://engineering.tumblr.com/post/722102563011493888/streambuilder-our-open-source-framework-for</a></div><div class=\"response-content\" ><p>Interesting post. Whether building your feeds or plugging into existing platforms like <a href=\"https://bsky.social/about/blog/3-30-2023-algorithmic-choice\">Bluesky</a>, this framework could serve as a good starting point.</p>\n<blockquote class=\"blockquote\">\n<p>Today, we’re abnormally jazzed to announce that we’re open-sourcing the custom framework we built to power your dashboard on Tumblr. We call it StreamBuilder, and we’ve been using it for many years.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>StreamBuilder has a lot going on. The primary architecture centers around “streams” of content: whether posts from a blog, a list of blogs you’re following, posts using a specific tag, or posts relating to a search. These are separate kinds of streams, which can be mixed together, filtered based on certain criteria, ranked for relevancy or engagement likelihood, and more.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>So, what’s included in the box?<br />\n<br></p>\n<ul>\n<li>The full framework library of code that we use today, on Tumblr, to power almost every feed of content you see on the platform.</li>\n<li>A YAML syntax for composing streams of content, and how to filter, inject, and rank them.</li>\n<li>Abstractions for programmatically composing, filtering, ranking, injecting, and debugging streams.</li>\n<li>Abstractions for composing streams together—such as with carousels, for streams-within-streams.</li>\n<li>An abstraction for cursor-based pagination for complex stream templates.</li>\n<li>Unit tests covering the public interface for the library and most of the underlying code.</li>\n</ul>\n</blockquote>\n<p><a href=\"https://github.com/Automattic/stream-builder\">GitHub Repo</a></p>\n</div>\n","tags":["tumblr","algorithms","feed","socialmedia"]},{"title":"State of the Word 2024","contentType":"reshare","date":"2024-12-17 16:50","url":"/reshare/state-of-the-word-2024-wordpress/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=KLybH5YvIPQ\" >https://www.youtube.com/watch?v=KLybH5YvIPQ</a></div><div class=\"response-content\" ><p>Cool retrospective and announcements from this WordPress' State of the Word 2024.</p>\n<p>Focused mode and templates look interesting.</p>\n<p><a href=\"https://www.youtube.com/watch?v=KLybH5YvIPQ\" title=\"State of the Word 2024 YouTube Thumbnail\"><img src=\"http://img.youtube.com/vi/KLybH5YvIPQ/0.jpg\" class=\"img-fluid\" alt=\"State of the Word 2024 YouTube Thumbnail\" /></a></p>\n</div>\n","tags":["wordpress","conference","blogging","website","opensource"]},{"title":"Tumblr adds Reddit-like ‘communities’","contentType":"reshare","date":"2024-12-16 22:00","url":"/reshare/tumblr-adding-reddit-like-communities/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/12/13/24320336/tumblr-communities-reddit-topics-groups\" >https://www.theverge.com/2024/12/13/24320336/tumblr-communities-reddit-topics-groups</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Tumblr is introducing a new Community feature — in-app groups organized by topic or interest.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Communities are similar to subreddits or Facebook groups and had previously been in beta.</p>\n</blockquote>\n<p>Given recent announcements like <a href=\"/responses/shipping-wordpress-tumblr/\">Tumblr replatforming on top of WordPress</a> and <a href=\"/responses/wordpress-activitypub-plugin/\">WordPress adding support for ActivityPub</a>, I could see communities becoming not only an alternative to Reddit, but also Lemmy in the Fediverse.</p>\n</div>\n","tags":["tumblr","reddit","communities","automattic","wordpress","activitypub","fediverse","lemmy"]},{"title":"Build a YouTube chat app with .NET","contentType":"reshare","date":"2024-12-16 21:22","url":"/reshare/youtube-chat-app-ai-dotnet/","content":"<div class=\"response-target\" >→ <a href=\"https://jordanmatthiesen.me/programming/build-youtube-chatbot-dotnet/\" >https://jordanmatthiesen.me/programming/build-youtube-chatbot-dotnet/</a></div><div class=\"response-content\" ><p>Great post from <a href=\"https://jordanmatthiesen.me/\">jordanmatthiesen.me</a>.</p>\n<blockquote class=\"blockquote\">\n<p>Recently on a trip for a tech conference I created a YouTube chat app using .NET and AI. This is part of my exploration into creating a larger app for chatting about .NET AI development (leveraging docs, presentations, and sample code my team has been working on at Microsoft).</p>\n</blockquote>\n</div>\n","tags":["dotnet","ai","youtube","chat","microsoft.extensions.ai","openai"]},{"title":"Sora is here","contentType":"reshare","date":"2024-12-09 21:34","url":"/reshare/sora-released/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/index/sora-is-here/\" >https://openai.com/index/sora-is-here/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Our video generation model is rolling out at <a href=\"https://sora.com\">sora.com⁠</a>.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Users can generate videos up to 1080p resolution, up to 20 sec long, and in widescreen, vertical or square aspect ratios. You can bring your own assets to extend, remix, and blend, or generate entirely new content from text.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The version of Sora we are deploying has many limitations. It often generates unrealistic physics and struggles with complex actions over long durations.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We’re introducing our video generation technology now to give society time to explore its possibilities and co-develop norms and safeguards that ensure it’s used responsibly as the field advances.</p>\n</blockquote>\n</div>\n","tags":["sora","openai","ai"]},{"title":"How I turned my blog into a social media hub","contentType":"reshare","date":"2024-12-05 19:22","url":"/reshare/how-i-turned-website-social-media-hub-newman/","content":"<div class=\"response-target\" >→ <a href=\"https://www.fastcompany.com/91235471/how-i-turned-my-blog-into-a-social-media-hub\" >https://www.fastcompany.com/91235471/how-i-turned-my-blog-into-a-social-media-hub</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...my blog has become a central hub for almost everything I write online, including nearly all my social media posts, links to my published work on other sites, and the occasional freestanding blog post. While this approach has a few trade-offs, I feel better about posting in the first place knowing that I’m doing it on my own terms.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>There’s also just something deeply satisfying about seeing a running feed of my own writing, on my own website, with my own design and presentation. A social network alone can’t replicate that, but the POSSE approach offers the best of both worlds.</p>\n</blockquote>\n<p>I've enjoyed seeing over the past few weeks, as people seek new online platforms, the personal website is being rediscovered.</p>\n</div>\n","tags":["indieweb","social","microblog","blogging","internet","web","socialmedia"]},{"title":"Jimmy The Gambler","contentType":"reshare","date":"2024-11-24 19:36","url":"/reshare/jimmy-gambler-psu/","content":"<div class=\"response-target\" >→ <a href=\"https://www.yardbarker.com/college_football/articles/jimmy_the_gambler_made_all_the_difference_for_penn_state/s1_17286_41290206\" >https://www.yardbarker.com/college_football/articles/jimmy_the_gambler_made_all_the_difference_for_penn_state/s1_17286_41290206</a></div><div class=\"response-content\" ><p>😂 😂 😂</p>\n<p>I don't think I've ever heard James Franklin called a gambler.</p>\n<p>The game against Minnesota was too close for comfort, but a win is a win. In many ways, thanks to Jimmy, The Gambler.</p>\n</div>\n","tags":["pennstate","ncaaf","collegefootball","psu"]},{"title":"Implement ActivityPub on a static site series by maho.dev","contentType":"reshare","date":"2024-11-19 10:03","url":"/reshare/implement-activitypub-static-site-series-maho/","content":"<div class=\"response-target\" >→ <a href=\"https://maho.dev/2024/02/a-guide-to-implement-activitypub-in-a-static-site-or-any-website/\" >https://maho.dev/2024/02/a-guide-to-implement-activitypub-in-a-static-site-or-any-website/</a></div><div class=\"response-content\" ><p>I've been following along with this amazing series from <a href=\"https://maho.dev/\">Maho</a> on implementing ActivityPub on static websites.</p>\n<p>As I think about what I want out of my website and the way I engage in the Fediverse, there's a lot of overlap.</p>\n<p>Today, what I mainly use my Mastodon instance for is:</p>\n<ol>\n<li>Having a Fediverse presence on a self-hosted Mastodon instance.</li>\n<li><a href=\"/posts/rss-to-mastodon-posse-azure-logic-apps/\">Cross-posting posts on by website using the POSSE pattern</a>.</li>\n</ol>\n<p>Although I have learned a ton from self-hosting my own Mastodon instance, neither of the points listed above require me self-hosting or even having an account on someone else's instance.</p>\n<p>Yesterday, <a href=\"/notes/using-domain-mastodon-discovery\">I took the first step in linking my Fediverse presence to my domain</a>. That fulfills my first requirement.</p>\n<p>I rarely post original content on Mastodon and for consumption, <a href=\"/notes/subscribed-to-1042-feeds-newsblur\">I already subscribe to accounts and tags via RSS</a>. Therefore, the second requirement is one that can naturally occur without having to use a Mastodon instance as an intermediary. I can just post on my website and because my posts can show up in my outbox, there's no need to POSSE. I don't want my presence to be limited to Mastodon though, but since I plan on supporting media, reviews, and other types of posts, I expect my posts to be accessible across other platforms on the Fediverse like Pixelfed, Bookwyrm, and many others.</p>\n<p>Since my website and website features are built using .NET, Maho's guide simplifies my implementation. Because <a href=\"/colophon\">I'm not using any of the existing static site generators and rolled my own</a>, that means that there's going to be some effort and customizations required on my end. It would no different from my <a href=\"/posts/receive-webmentions-fsharp-az-functions-fsadvent/\">custom Webmentions implementation</a> though. My hope is that I will save time I often spend maintaining the server as well as money since I no longer need to rent a server to host my instance. At the same time, I get to learn and contribute to building a more open, decentralized, and personal web.</p>\n</div>\n","tags":["acvititypub","fediverse","staticsite","blogging","indieweb","openweb","personalweb","dotnet","internet"]},{"title":"Digital minimalism: A moral duty?","contentType":"reshare","date":"2024-11-19 09:56","url":"/reshare/digital-minimalism-moral-duty-deep-questions/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=KK9ktzbBDPE\" >https://www.youtube.com/watch?v=KK9ktzbBDPE</a></div><div class=\"response-content\" ><p>I saw a reference to this paper yesterday and added it to my Read-It-Later list.</p>\n<p>Here's a link to the original paper: <a href=\"https://philpapers.org/archive/AYLITA.pdf\">https://philpapers.org/archive/AYLITA.pdf</a></p>\n<p>Chances are, I wasnt going to get to it until way later with the backlog of other things I want to read later.</p>\n<p>Lucky for me, that was the subject of the latest Deep Questions episode. Cal does a nice job breaking down the main arguments of the paper.</p>\n<p><a href=\"https://www.youtube.com/watch?v=KK9ktzbBDPE\" title=\"A thumbnail of Deep Questions Podcast Episode on Digital Minimalism\"><img src=\"http://img.youtube.com/vi/KK9ktzbBDPE/0.jpg\" class=\"img-fluid\" alt=\"A thumbnail of Deep Questions Podcast Episode on Digital Minimalism\" /></a></p>\n<p>The main takeaway for me was, technology is a great tool. However, when it robs you of your rational decision making abilities and pushes you to compulsive behaviors that move you away from the habits and practices that fuel your growth, it's a problem. For example, lets say you want to build a habit of reading or working out. Unfortunately, when you <INSERT TIME SUCKING DISTRACTION OF CHOICE>, time, attention, and energy that would've otherwise been spent on those habits is now diverted towards the distraction. That is irrational and goes against your desire to grow, yet making the rational choice is where most of us struggle. Although the paper focuses on our digital lives, I can see other ways similar arguments can be applied.</p>\n<p>I've also included the audio version as well: <a href=\"https://www.buzzsprout.com/1121972/episodes/16124495-ep-327-would-kant-use-tiktok\">https://www.buzzsprout.com/1121972/episodes/16124495-ep-327-would-kant-use-tiktok</a></p>\n</div>\n","tags":["digitalminimalism","postcast","deepquestions","philosophy","calnewport"]},{"title":"Why YOU Should Make A Website","contentType":"reshare","date":"2024-11-16 18:08","url":"/reshare/why-you-should-make-a-website-luvstarkei/","content":"<div class=\"response-target\" >→ <a href=\"https://youtu.be/uNlZ50b6wSs\" >https://youtu.be/uNlZ50b6wSs</a></div><div class=\"response-content\" ><p>This video showed up in my feed yesterday. <a href=\"https://luvstarkei.com/\">LuvstarKei</a> makes some great points for building your own space on the web.</p>\n<p><a href=\"https://www.youtube.com/watch?v=uNlZ50b6wSs\" title=\"LuvstarKei Thumbnail Why You Should Make A Website YouTube Video\"><img src=\"http://img.youtube.com/vi/uNlZ50b6wSs/0.jpg\" class=\"img-fluid\" alt=\"LuvstarKei Thumbnail Why You Should Make A Website YouTube Video\" /></a></p>\n<p>With the most recent Twitter / X exodus, it's great to see folks moving to places built on more open protocols like AT (Bluesky) or ActivityPub (Fediverse).</p>\n<p>That said, those platforms are not your own, even in cases where you self-host. If tomorrow Threads, Bluesky, or Mastodon stop hosting your instance or stop development altogether, what happens to the content and your community? In the case of the Fediverse, many projects are being built in the open and there are other providers you could switch to. The switching costs though may not make it worth bothering to export and keep your content as is usually the case. That's not to say with a personal website you might not have similar issues (i.e. switching from WordPress to Ghost). However, HTML is HTML and you can rehost that anywhere on the internet.</p>\n<p>Everyone is looking for different things from the social platforms, and that's not to say they don't have their value. However, from an ownership, permanence, and creative freedom perspective, nothing beats having your own website.</p>\n</div>\n","tags":["website","indieweb","socialmedia","personalweb"]},{"title":".NET Conf 2024 - Session Recordings","contentType":"reshare","date":"2024-11-15 11:52 -05:00","url":"/reshare/dotnet-conf-2024-session-playlist/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/playlist?list=PLdo4fOcmZ0oXeSG8BgCVru3zQtw_K4ANY\" >https://www.youtube.com/playlist?list=PLdo4fOcmZ0oXeSG8BgCVru3zQtw_K4ANY</a></div><div class=\"response-content\" ><p>The recordings are out for .NET Conf 2024 in case you missed any of the sessions.</p>\n<p>Here are the recordings from the sessions I participated in.</p>\n<ul>\n<li><a href=\"https://youtu.be/7Rw_ciSh2Wk\">Building AI Applications From Scratch: A Hands-On Guide for .NET Developers</a></li>\n<li><a href=\"https://youtu.be/MdM8G2e7jOg\">Building the Foundation: AI Fundamentals in .NET</a></li>\n</ul>\n<p>Also, there's a <a href=\"https://www.youtube.com/playlist?list=PLdo4fOcmZ0oXU8g5XIKiOPX4tgIqG2Ddc\">Premier Bonus playlist</a> which has a ton of amazing bonus content.</p>\n</div>\n","tags":["dotnet","dotnetconf","net9","dotnet9","programming","dev"]},{"title":"Thundercat on Tyler's Chromakopia album?","contentType":"star","date":"2024-11-13 02:11","url":"/star/thundercat-chromakopia/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=KvR_QlXaAfU\" >https://www.youtube.com/watch?v=KvR_QlXaAfU</a></div><div class=\"response-content\" ><p>Love this new &amp;quot;making of&amp;quot; video. I was surprised to learn Thundercat is on the new album. I didn't catch which track it's on, but will be listening for it.</p>\n<p><a href=\"https://www.youtube.com/watch?v=KvR_QlXaAfU\" title=\"Mask Is Off Video Thumbnail\"><img src=\"http://img.youtube.com/vi/KvR_QlXaAfU/0.jpg\" class=\"img-fluid\" alt=\"Mask Is Off Video Thumbnail\" /></a></p>\n</div>\n","tags":["tylerthecreator","chromakopia","thundercat","music"]},{"title":"Day 1 of .NET Conf","contentType":"reshare","date":"2024-11-13 01:47","url":"/reshare/dotnetconf-2024-day-1-success/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=hM4ifrqF_lQ\" >https://www.youtube.com/watch?v=hM4ifrqF_lQ</a></div><div class=\"response-content\" ><p>Day 1 of .NET Conf was a lot of fun. Tons of great sessions. Here's the recording of all the sessions.</p>\n<p><a href=\"https://www.youtube.com/watch?v=hM4ifrqF_lQ\" title=\".NET Conf Day 1 LiveStream\"><img src=\"http://img.youtube.com/vi/hM4ifrqF_lQ/0.jpg\" class=\"img-fluid\" alt=\".NET Conf Day 1 LiveStream\" /></a></p>\n<p>If you're interested in checking out my session with Jeremy, Building AI Applications From Scratch, here's a link to it.</p>\n<p><a href=\"https://www.youtube.com/live/hM4ifrqF_lQt=23407\" title=\".NET Conf Day 1 LiveStream Countdown\"><img src=\"http://img.youtube.com/vi/hM4ifrqF_lQ/1.jpg\" class=\"img-fluid\" alt=\".NET Conf Day 1 LiveStream Countdown\" /></a></p>\n<p>Tomorrow, I'll also get a chance to share the stage with Tarek and Tanner to talk <a href=\"https://www.dotnetconf.net/agenda#:%7E:text=Building%20the%20Foundation:%20AI%20Fundamentals%20in%20.NET\">AI Fundamentals</a>.</p>\n</div>\n","tags":["dotnet","dotnetconf","dotnet9","net9","conference","ai","aspire","microsoft"]},{"title":"Leo.fm move to micro.blog","contentType":"reply","date":"2024-11-10 14:27 -05:00","url":"/reply/leo-fm-moves-to-microblog/","content":"<div class=\"response-target\" >→ <a href=\"https://leo.fm/2024/11/04/you-might-notice.html\" >https://leo.fm/2024/11/04/you-might-notice.html</a></div><div class=\"response-content\" ><p>So cool to see you posting more on your own platform.</p>\n</div>\n","tags":["indieweb","microblog","twit"]},{"title":"Microsoft Research: Introducing DRIFT Search","contentType":"reshare","date":"2024-11-04 21:59 -05:00","url":"/reshare/introducing-drift-search/","content":"<div class=\"response-target\" >→ <a href=\"https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/\" >https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/</a></div><div class=\"response-content\" >\n<blockquote class=\"blockquote\">\n<p>DRIFT search (Dynamic Reasoning and Inference with Flexible Traversal)...builds upon Microsoft’s GraphRAG technique, combining characteristics of both global and local search to generate detailed responses in a method that balances computational costs with quality outcomes.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<ol>\n<li><strong>Primer</strong>: When a user submits a query, DRIFT compares it to the top K most semantically relevant community reports. This generates an initial answer along with several follow-up questions, which act as a lighter version of global search. To do this, we expand the query using Hypothetical Document Embeddings (HyDE), to increase sensitivity (recall), embed the query, look up the query against all community reports, select the top K and then use the top K to try to answer the query. The aim is to leverage high-level abstractions to guide further exploration.</li>\n<li><strong>Follow-Up</strong>: With the primer in place, DRIFT executes each follow-up using a local search variant. This yields additional intermediate answers and follow-up questions, creating a loop of refinement that continues until the search engine meets its termination criteria, which is currently configured for two iterations (further research will investigate reward functions to guide terminations). This phase represents a globally informed query refinement. Using global data structures, DRIFT navigates toward specific, relevant information within the knowledge graph even when the initial query diverges from the indexing persona. This follow-up process enables DRIFT to adjust its approach based on emerging information.</li>\n<li><strong>Output Hierarchy</strong>: The final output is a hierarchy of questions and answers ranked on their relevance to the original query. This hierarchical structure can be customized to fit specific user needs. During benchmark testing, a naive map-reduce approach aggregated all intermediate answers, with each answer weighted equally.</li>\n</ol>\n</blockquote>\n</div>\n","tags":["ai","search","RAG","llm","microsoft","research","msr","graphrag"]},{"title":"Microsoft Research Focus: Week of October 28, 2024","contentType":"reshare","date":"2024-11-04 21:39 -05:00","url":"/reshare/microsoft-research-focus-oct-28-2024/","content":"<div class=\"response-target\" >→ <a href=\"https://www.microsoft.com/en-us/research/blog/research-focus-week-of-october-28-2024/\" >https://www.microsoft.com/en-us/research/blog/research-focus-week-of-october-28-2024/</a></div><div class=\"response-content\" >\n<blockquote class=\"blockquote\">\n<p>Language agents are AI systems that can understand, reason and respond in natural language to complete various tasks. While the latest LLMs are capable enough to power reasonably good language agents, the closed-API model makes it hard to improve them when they perform sub-optimally. Recent studies have explored using techniques like self-reflection and prompt optimization to improve performance. Unfortunately, self-reflection can be used only during the agent’s current run, while contemporary prompt optimization techniques are designed and tested to work on simple single-step agents.</p>\n<p>In a recent paper: METAREFLECTION: Learning Instructions for Language Agents using Past Reflections, researchers from Microsoft introduce a novel offline reinforcement learning technique that enhances the performance of language agents by augmenting a semantic memory based on experiential learnings from past trials. They demonstrate the efficacy of METAREFLECTION across multiple domains, including complex logical reasoning, biomedical semantic similarity, open world question answering, and vulnerability threat detection, in Infrastructure-as-Code, spanning different agent designs. METAREFLECTION boosts language agents’ performance by 4% to 16.82% over the baseline agent implementations and performs on par with existing state-of-the-art prompt optimization techniques while requiring fewer LLM calls.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>Generative AI applications rely on large, foundation models, particularly LLMs. LLMs often have tens to hundreds of billions of parameters, making them too large for a single graphics processing unit (GPU) to handle in terms of both memory and computation. Because of their size, training these models requires distributing the workload across hundreds or even thousands of GPUs. This can lead to significant communication overhead, a challenge that arises when data needs to be shared between different GPUs.</p>\n<p>In a recent paper: Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping, researchers from Microsoft introduce a system designed to enhance the efficiency of LLM training by reducing the time lost to communication between GPUs.</p>\n<p>Domino breaks down data dependencies in a single batch of training into smaller, independent pieces. These smaller pieces are processed in parallel, and communication between GPUs happens simultaneously with computation, minimizing delays.</p>\n<p>Test results comparing Domino to Megatron-LM show that Domino speeds up the training process by up to 1.3x on Nvidia DGX-H100 GPUs.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>Large vision-language models (VLMs) such as GPT-4V and GPT-4o show promise in driving intelligent agent systems that operate within user interfaces (UI). However, VLMs’ full potential remains underexplored in real-world applications, particularly when it comes to acting as general agents across diverse operating systems and applications with only vision input. One limiting factor is the absence of a robust technique for screen parsing which is capable of 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associating the intended action with the corresponding region on the screen.</p>\n<p>In a recent article: OmniParser for pure vision-based GUI agent, researchers from Microsoft present a compact screen parsing module that can convert UI screenshots into structured elements. OmniParser can be used with a variety of models to create agents capable of taking actions on UIs. When used with GPT-4V, OmniParser significantly improves the agent capability to generate precisely grounded actions for interface regions.</p>\n<p>OmniParser with GPT-4V agent achieved the best performance on the recently released  WindowsAgentArena (opens in new tab) benchmark.</p>\n</blockquote>\n</div>\n","tags":["ai","microsoft","research","msr","microsoftresearch"]},{"title":"In-Context LoRA for Diffusion Transformers","contentType":"reshare","date":"2024-11-04 21:28 -05:00","url":"/reshare/in-context-lora/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2410.23775\" >https://arxiv.org/abs/2410.23775</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20∼100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems.</p>\n</blockquote>\n<p>Repo: https://github.com/ali-vilab/In-Context-LoRA</p>\n</div>\n","tags":["ai","transformers","diffusers","imagegeneration","lora","finetuning"]},{"title":"Thinking LLMs: General Instruction Following with Thought Generation","contentType":"reshare","date":"2024-11-04 21:24 -05:00","url":"/reshare/thinking-llms-instruction-following-thought-generation/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2410.10630\" >https://arxiv.org/abs/2410.10630</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. We propose a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. We show that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning &amp;amp; problem-solving tasks.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","research","meta","chainofthought","training","finetuning"]},{"title":"Haves and Choices","contentType":"reshare","date":"2024-11-01 09:37","url":"/reshare/haves-choices-brown-nielsen/","content":"<div class=\"response-target\" >→ <a href=\"https://notes.jim-nielsen.com/#2024-10-30T2141\" >https://notes.jim-nielsen.com/#2024-10-30T2141</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The “have to” narrative positions us as repositories of instructions made elsewhere, as if we were just programs following the code we’ve been given...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The “choose to” narrative has no illusions about our power and recognizes that we are small players in a bigger, and certainly unjust, world. But we are not machines. And maybe we don’t like the choices available to us, maybe we wish there were others within reach. But once we accept that there are choices to make, we may notice where we have some room to maneuver, some space to play with, some opportunity or avenue or loophole we can exploit.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Refusing your own agency time and again is like disconnecting from a power source—the energy is still there, latent and ready, but the plug dangles inches from the outlet.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...accepting that you have some agency might hurt: you bump up against the systems that constrain your choices; you see more clearly how other people’s choices limit (or expand) your own. But it keeps you connected to that source, that font of energy that is yours and no one else’s. It keeps you hooked up to who you are, and to what you want.</p>\n</blockquote>\n<p>Interesting excerpts from <a href=\"https://everythingchanges.us/blog/haves-and-choices/\">Mandy Brown's &amp;quot;Haves and Choices&amp;quot; article</a> to think about.</p>\n</div>\n","tags":["life","thoughts"]},{"title":"Art the Clown Will Ring the Nasdaq Closing Bell in Times Square on Halloween!","contentType":"reshare","date":"2024-10-31 09:05","url":"/reshare/art-clown-nasdaq-bell-halloween/","content":"<div class=\"response-target\" >→ <a href=\"https://bloody-disgusting.com/the-further/3837684/terrifier-takeover-art-the-clown-will-ring-the-nasdaq-closing-bell-in-times-square-on-halloween/\" >https://bloody-disgusting.com/the-further/3837684/terrifier-takeover-art-the-clown-will-ring-the-nasdaq-closing-bell-in-times-square-on-halloween/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>With Terrifier 3 now playing in theaters and just passing $50 million worldwide, Art the Clown’s domination of the Halloween season continues with a takeover of the Nasdaq MarketSite in Times Square!</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...Art the Clown and star Lauren LaVera will ring the Nasdaq closing bell!</p>\n</blockquote>\n<p>This is so cool! Hopefully that's all he does.</p>\n</div>\n","tags":["art","terrifier","halloween","movie","horror","newyork","ny","nasdaq"]},{"title":"New Release: Spirit Box by Flying Lotus","contentType":"star","date":"2024-10-30 08:45","url":"/star/spirit-box-flying-lotus/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=E-BbWldXfJk&amp;amp;amp;list=OLAK5uy_kQDB4vsLJy0bmjuYsCXKa-eMO-W33na2w\" >https://www.youtube.com/watch?v=E-BbWldXfJk&amp;amp;list=OLAK5uy_kQDB4vsLJy0bmjuYsCXKa-eMO-W33na2w</a></div><div class=\"response-content\" ><p>He's back! A day after <a href=\"/responses/chromakopia-tyler-the-creator-released\">Tyler</a> too.</p>\n<p>I already loved Garmonbozia but Ajhussi is my favorite track.</p>\n<p>Although on this album you hear some of the early FlyLo sound, it still feels new and fresh.</p>\n<p><a href=\"https://www.youtube.com/playlist?list=OLAK5uy_kQDB4vsLJy0bmjuYsCXKa-eMO-W33na2w\" title=\"Flying Lotus Spirit Box Album Playlist Thumbnail\"><img src=\"http://img.youtube.com/vi/E-BbWldXfJk/0.jpg\" class=\"img-fluid\" alt=\"Flying Lotus Spirit Box Album Playlist Thumbnail\" /></a></p>\n</div>\n","tags":["music","flyinglotus","spiritbox","newmusic","album"]},{"title":"Chromakopia by Tyler The Creator Released","contentType":"star","date":"2024-10-28 19:59","url":"/star/chromakopia-tyler-the-creator-released/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=hCcwCv3G1FQ&amp;amp;amp;list=OLAK5uy_nt1Nw4wT6I7VlzNknxTiIz3hfED0ttO8Q\" >https://www.youtube.com/watch?v=hCcwCv3G1FQ&amp;amp;list=OLAK5uy_nt1Nw4wT6I7VlzNknxTiIz3hfED0ttO8Q</a></div><div class=\"response-content\" ><p>I did a first pass earlier today and I generally enjoyed this new album from Tyler.</p>\n<p>Like many previous albums, it's highly introspective. I loved the instrumentals as well.</p>\n<p>Some first impressions:</p>\n<ul>\n<li>Track 10 is not a double track as it's generally been the case of his previous albums.</li>\n<li>&amp;quot;Like Him&amp;quot; adds a new contrasting dimension to Tyler's relationship with his father.</li>\n</ul>\n<p>I'll need some more time to digest it and read through the lyrics but overall, great album.</p>\n<p><a href=\"https://www.youtube.com/watch?v=hCcwCv3G1FQ&amp;amp;list=OLAK5uy_nt1Nw4wT6I7VlzNknxTiIz3hfED0ttO8Q\" title=\"Tyler The Creator Chromakopia Playlist Thumbnail\"><img src=\"http://img.youtube.com/vi/hCcwCv3G1FQ/0.jpg\" class=\"img-fluid\" alt=\"Tyler The Creator Chromakopia Playlist Thumbnail\" /></a></p>\n</div>\n","tags":["chromakopia","music","tylerthecreator","newmusic","album","rap","hiphop"]},{"title":"R.I.P 399 - Queen of the Tetons","contentType":"reshare","date":"2024-10-25 21:59","url":"/reshare/rip-399-queen-tetons/","content":"<div class=\"response-target\" >→ <a href=\"https://apnews.com/article/famous-grizzly-399-killed-grand-teton-wyoming-3e13c4b5234926cbd799dbb3db1ffac8\" >https://apnews.com/article/famous-grizzly-399-killed-grand-teton-wyoming-3e13c4b5234926cbd799dbb3db1ffac8</a></div><div class=\"response-content\" ><p>Sad news from the Tetons this week. Grizzly 399 passed away after being struck by a vehicle.</p>\n<p><a href=\"https://www.youtube.com/watch?v=9gXa-bs_9i0\" title=\"PBS 399 Queen of the Tetons Documentary\"><img src=\"http://img.youtube.com/vi/9gXa-bs_9i0/0.jpg\" class=\"img-fluid\" alt=\"PBS 399 Queen of the Tetons Documentary\" /></a></p>\n<blockquote class=\"blockquote\">\n<p>Grizzly No. 399 died Tuesday night on a highway in Snake River Canyon south of Jackson...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>At 28 years old, No. 399 was the oldest known reproducing female grizzly in the Yellowstone ecosystem. Each spring, wildlife enthusiasts eagerly awaited her emergence from her den to see how many cubs she had birthed over the winter — then quickly shared the news online.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The grizzly lived through a time of strife over her species in the region...</p>\n</blockquote>\n</div>\n","tags":["399","grandteton","grizzly","teton"]},{"title":"Tiny Desk Concerts - Kamasi Washington","contentType":"reshare","date":"2024-10-22 09:30","url":"/reshare/kamasi-washington-tiny-desk-concert-npr/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=x8WTPgeVPjg\" >https://www.youtube.com/watch?v=x8WTPgeVPjg</a></div><div class=\"response-content\" ><p>I think this is Kamasi's first time on Tiny Desk. Great performance. I didn't realize Brandon Coleman was on keys. The encore, &amp;quot;Asha The First&amp;quot;, was really good.</p>\n<p><a href=\"https://www.youtube.com/watch?v=x8WTPgeVPjg\" title=\"Thumbnail of Kamasi Washington performing on Tiny Desk Concerts\"><img src=\"http://img.youtube.com/vi/x8WTPgeVPjg/0.jpg\" class=\"img-fluid\" alt=\"Thumbnail of Kamasi Washington performing on Tiny Desk Concerts\" /></a></p>\n</div>\n","tags":["kamasiwashington","music","tinydesk","npr","jazz","brainfeeder"]},{"title":"Noid by Tyler The Creator","contentType":"reshare","date":"2024-10-21 20:15","url":"/reshare/noid-tyler-the-creator-chromakopia/","content":"<div class=\"response-target\" >→ <a href=\"https://open.spotify.com/track/1tnZxHryc2wWtjUZC1LQw5\" >https://open.spotify.com/track/1tnZxHryc2wWtjUZC1LQw5</a></div><div class=\"response-content\" ><p>I'm ready for this new album. It reminds me of Cherry Bomb in some ways. Love the instrumentals.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/1tnZxHryc2wWtjUZC1LQw5?utm_source=generator\" width=\"100%\" height=\"352\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n</div>\n","tags":["music","chromakopia","tylerthecreator","noid","newmusic"]},{"title":"Chromakopia - Tyler The Creator","contentType":"star","date":"2024-10-18 09:28","url":"/star/chromakopia-tyler-the-creator/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=gkZ4dLMH-B8\" >https://www.youtube.com/watch?v=gkZ4dLMH-B8</a></div><div class=\"response-content\" ><p>2024 just keeps delivering. Most of the artists I follow have dropped an album this year. Now I'm just waiting on FlyLo. Hopefully he releases his album by end of year.</p>\n<p>I don't know what to expect from Chromakopia, but I can't wait after watching this trailer.</p>\n<p><a href=\"https://www.youtube.com/watch?v=gkZ4dLMH-B8\" title=\"Thumbnail of St. Chroma Chromakopia album anouncement video\"><img src=\"http://img.youtube.com/vi/gkZ4dLMH-B8/0.jpg\" class=\"img-fluid\" alt=\"Thumbnail of St. Chroma Chromakopia album anouncement video\" /></a></p>\n</div>\n","tags":["music","tylerthecreator","chromakopia","newmusic"]},{"title":"You should be using an RSS reader","contentType":"reply","date":"2024-10-18 08:54","url":"/reply/you-should-be-using-rss-reader-pluralistic/","content":"<div class=\"response-target\" >→ <a href=\"https://pluralistic.net/2024/10/16/keep-it-really-simple-stupid/#read-receipts-are-you-kidding-me-seriously-fuck-that-noise\" >https://pluralistic.net/2024/10/16/keep-it-really-simple-stupid/#read-receipts-are-you-kidding-me-seriously-fuck-that-noise</a></div><div class=\"response-content\" ><p>Yes, you should!</p>\n<blockquote class=\"blockquote\">\n<p>...the conduit through which I experience Molly's excellent work is totally enshittification-proof, and the more I use it, the easier it is for everyone to be less enshittified.<br />\n</br>\nThis conduit is anti-lock-in, it works for nearly the whole internet. It is surveillance-resistant, far more accessible than the web or any mobile app interface. It is my secret super-power.<br />\n</br>\nIt's RSS.<br />\n<br>\n...But RSS isn't just good for the news! It's good for everything.<br />\n</br>\nYour RSS reader doesn't (necessarily) have an algorithm. By default, you'll get everything as it appears, in reverse-chronological order.<br />\n</br>\nNow, you sign up to so many feeds that you're feeling overwhelmed and you want an algorithm to prioritize posts – or recommend content. Lots of RSS readers have some kind of algorithm and recommendation system...<br />\n</br>\nBut you control the algorithm, you control the recommendations. And if a new RSS reader pops up with an algorithm you're dying to try, you can export all the feeds you follow with a single click, which will generate an OPML file. Then, with one click, you can import that OPML file into any other RSS reader in existence and all your feeds will be seamlessly migrated there. You can delete your old account, or you can even use different readers for different purposes.<br />\n</br>\nRSS basically works like social media should work. Using RSS is a chance to visit a utopian future in which the platforms have no power, and all power is vested in publishers, who get to decide what to publish, and in readers, who have total control over what they read and how, without leaking any personal information through the simple act of reading.<br />\n</br>\nAnd here's the best part: every time you use RSS, you bring that world closer into being!<br />\n</br>\nUnlike those largely useless, performative boycotts of widely used platforms, switching to RSS doesn't require that you give anything up. Not only does switching to RSS let you continue to follow all the newsletters, webpages and social media accounts you're following now, it makes doing so better: more private, more accessible, and less enshittified.<br />\n</br>\nUsing RSS to follow the stuff that matters to you will have an immediate, profoundly beneficial impact on your own digital life – and it will appreciably, irreversibly nudge the whole internet towards a better state.</p>\n</blockquote>\n<p>Great article by <a href=\"https://craphound.com/bio/\">Cory Doctorow</a>.</p>\n<p>I've written at length on my website as to <a href=\"/posts/rediscovering-rss-user-freedom\">why I enjoy RSS</a>. My use goes back to the days of Google Reader. Like many, once that was disbanded, my consumption pretty much shifted to social media for a few years. After growing dissatisfaction with the limitations and lack of control over my feed, I started my journey back to RSS readers eventually landing on <a href=\"/notes/subscribed-to-1042-feeds-newsblur\">NewsBlur</a> and elfeed as my readers of choice.</p>\n<p>On this website, RSS plays an important role in a few areas.</p>\n<p>RSS is how I:</p>\n<ul>\n<li><a href=\"/notes/website-feeds-opml/\">Enable discovery of my own feeds</a></li>\n<li><a href=\"/feed\">Enable visitors to subscribe and get the latest content</a></li>\n<li><a href=\"/posts/rss-to-mastodon-posse-azure-logic-apps\">Syndicate posts to Mastodon</a>.</li>\n<li><a href=\"/notes/blogroll-discovery-implemented/\">Enable feed discovery of websites I subscribe to</a> through my:\n<ul>\n<li><a href=\"/collections/blogroll/\">Blogroll</a></li>\n<li><a href=\"/collections/podroll/\">Podroll (podcasts)</a></li>\n<li><a href=\"/collections/youtube/\">YouTubeRoll?</a></li>\n<li><a href=\"/collections/forums/\">ForumRoll?</a></li>\n</ul>\n</li>\n</ul>\n<p>So how can you get started?</p>\n<ul>\n<li>Learn <a href=\"https://aboutfeeds.com/\">about feeds</a>.</li>\n<li>Check out the many <a href=\"https://matthiasott.com/notes/we-love-rss\">getting started guides</a>.</li>\n<li>Browse through <a href=\"https://blogroll.club/\">blogrolls and feed catalogs</a></li>\n<li><a href=\"https://www.theverge.com/24036427/rss-feed-reader-best\">Explore various feed readers</a> to find the one that works best for you.</li>\n</ul>\n</div>\n","tags":["rss","pluralistic","indieweb","protocols","openweb","freeweb","internet","smallweb"]},{"title":"Fighting for our web - A talk from XOXO","contentType":"star","date":"2024-10-10 20:20","url":"/star/fighting-for-our-web-white/","content":"<div class=\"response-target\" >→ <a href=\"https://www.citationneeded.news/fighting-for-our-web/\" >https://www.citationneeded.news/fighting-for-our-web/</a></div><div class=\"response-content\" ><p>I haven't watched the entire talk, but here are some excerpts from the transcript that resonated with me.</p>\n<p><a href=\"https://www.youtube.com/watch?v=MTaeVVAvk-c\" title=\"Thumbnail of Molly White Speaking at XOXO Conference\"><img src=\"http://img.youtube.com/vi/MTaeVVAvk-c/0.jpg\" class=\"img-fluid\" alt=\"Thumbnail of Molly White Speaking at XOXO Conference\" /></a></p>\n<blockquote class=\"blockquote\">\n<p>For a lot of people, the web feels worse than it used to. Much of our time is spent on a handful of giant social networks that do everything they can to keep people in their apps for as long as possible, even if it has detrimental effects on the people who are using them, or on the social networks themselves.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Websites outside of these handful of social networks are harder and harder to even find, and partially because of that, they have a harder and harder time sustaining themselves.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>What is left for those of us who saw the web as an infinite canvas, a tool to reach those who we never could have dreamed of reaching before, a medium that could stretch the limits of what was even possible in an analog world?</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...having said all this, would it surprise you to hear that now more than ever, I feel that same burning feeling of excitement around what’s possible?</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>That’s because what really sucks about the web these days, what has us feeling despair and anger, has everything to do with the industry that has formed around the web, but not the web itself. The web is still just a substrate on which anything can be built. Most importantly, the web is the people who use it, not the companies that have established themselves around it.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>And the widespread disillusionment that we’re seeing may actually be a good thing. More people than ever have realized that the utopian dreams of a web that could only bring about positive and wonderful things might have been misguided.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>With this knowledge comes power. The power to shape the web that we want to see, while fighting against the one that we don’t.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>My experience in fighting this fight has helped to convince me of just how much power we, everyday normal people, have—even when we’re staring down massive platforms and billion-dollar companies.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I ended up doing what I enjoy doing—building something cool, mostly for the sake of building it, and writing down what I saw.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...this simple act of building something interesting and somewhat different had an impact that I could not have anticipated. It turned out that people were starving for it.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>And even though I was a nobody, it had a major impact.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The lesson was clear. The tech industry, even with its billions of dollars, is not an indomitable force. Though they can and will ignore what people want from them, they cannot control what those people think. And the tide can turn against them. And it doesn’t always take a job at The New York Times or a huge pre-established platform to become one of the voices speaking up, helping to turn that tide. Sometimes you just have to make something cool. And using the very same technology that enabled crypto guys to sell their scam tokens, or the boosterist journalists to publish obsequious descriptions of companies that were really selling vaporware, or the crypto community to spam social media with promotions for their NFTs, I was able to do something a little different to push back against that very same phenomenon.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>What they don’t seem to realize is that in doing so, by reducing the web only to the types of expression that can happen within their cramped boxes—where you can’t write more than 280 characters, or you can’t publish your cool JavaScript-based art project, or you can’t say the things that you want to say without getting de-boosted by the engagement maximization machine, or you can’t read what your friends are posting without the platform interjecting offensive troll posts or soulless AI-generated meme images—they’re creating a thirst for everything outside of those boxes.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>A thirst for what the web really is—a medium, a conduit, a tool that is used by readers and artists and creators and explorers, not a gatekeeper that seems to be in an ever more adversarial relationship with everyone who uses it.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The platforms I talk about are deeply entrenched to the point where many people barely use the web outside of them.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>But the platforms do not exist without the people, and there are a lot more of us than there are of them.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...it has never been easier to do cool things on the web. What used to be expensive and require a great deal of technical expertise is now becoming more and more accessible, both financially and technologically, by the day. More people than ever have access to the web, both in terms of access to devices, but also in terms of access to internet connectivity and software. What used to be the realm of the nerds and those with the financial wherewithal to purchase expensive home computers, connectivity, and software packages, is now home to people from all walks of life, who bring new ideas, perspectives, and experiences that we often forget were in short supply during what some of us think of as the “good old days” of the internet.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>And so, we—all of us—can build the projects we want to see, write the software that the big platforms won’t, and create the services that people need. We can wire everything together, whether the platforms like it or not, to tear down the walls that they put up. We can modify the software, reverse engineer their systems, and wrestle back control of how we experience the web, even through these very platforms. We can share information and help people understand what is happening around them. We can teach others to build the things they want to build, and share with one another the important and meaningful work that is being done.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We can build the web that we want to see, and we can return to that place where the web is a place of wonder, where all of us feel that same burning feeling of excitement as we push the web back towards the wonderful, beautiful, joyful place it ought to be.</p>\n</blockquote>\n</div>\n","tags":["internet","indieweb","protocolsnotplatforms","humanweb","smallweb","web3.0"]},{"title":"Marvin Gaye - What's Going On","contentType":"star","date":"2024-09-25 20:19","url":"/star/marvin-gaye-whats-going-on/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=o5TmORitlKk&amp;amp;amp;list=OLAK5uy_krxU85sRSpvmZqiwHyVmYNhKBDVgvj-CE\" >https://www.youtube.com/watch?v=o5TmORitlKk&amp;amp;list=OLAK5uy_krxU85sRSpvmZqiwHyVmYNhKBDVgvj-CE</a></div><div class=\"response-content\" ><p>This is such a beautiful album. It seems I'm not the only one that thinks so since it's rated as the <a href=\"https://www.rollingstone.com/music/music-lists/best-albums-of-all-time-1062063/marvin-gaye-whats-going-on-4-1063232/\">#1 album of all time by Rolling Stone</a>. The powerful messages in the lyrics still resonate to this day and the composition is such a perfect match for Marvin Gaye's voice. 50 years later it feels like this album has transcended space and time.</p>\n<p><a href=\"https://www.youtube.com/watch?v=o5TmORitlKk\" title=\"Marvin Gaye What's Going On Album Playlist Thumbnail\"><img src=\"http://img.youtube.com/vi/o5TmORitlKk/0.jpg\" class=\"img-fluid\" alt=\"Marvin Gaye What's Going On Album Playlist Thumbnail\" /></a></p>\n<p>Over the past few days, I've listened to it from beginning to end at least 3 times.</p>\n<p>Of course there's the classics like <em>What's Going On</em> and <em>Mercy Mercy Me</em> that many people know. However, the ones that have quickly become my favorites are:</p>\n\n<p><a href=\"https://www.youtube.com/watch?v=E1KqO8YtXlY\" title=\"Save The Children Marvin Gaye Thumbnail\"><img src=\"http://img.youtube.com/vi/E1KqO8YtXlY/0.jpg\" class=\"img-fluid\" alt=\"Save The Children Marvin Gaye Thumbnail\" /></a></p>\n\n<p><a href=\"https://www.youtube.com/watch?v=2OIOoTJwEVg\" title=\"God Is Love Marvin Gaye Thumbnail\"><img src=\"http://img.youtube.com/vi/2OIOoTJwEVg/0.jpg\" class=\"img-fluid\" alt=\"God Is Love Marvin Gaye Thumbnail\" /></a></p>\n</div>\n","tags":["music","marvingaye","whatsgoingon","soul","rnb","album","war","nature","environment","conservation","love"]},{"title":"Listening to the Sun by Andre 3000","contentType":"star","date":"2024-09-14 22:08 -05:00","url":"/star/andre-3000-listening-to-the-sun/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=2nOxSeBhQAA\" >https://www.youtube.com/watch?v=2nOxSeBhQAA</a></div><div class=\"response-content\" ><p>Simply amazing.</p>\n<p><a href=\"https://www.youtube.com/watch?v=2nOxSeBhQAA\" title=\"Andre 3000 Listening to the Sun Thumbnail\"><img src=\"http://img.youtube.com/vi/2nOxSeBhQAA/0.jpg\" class=\"img-fluid\" alt=\"Andre 3000 Listening to the Sun Thumbnail\" /></a></p>\n</div>\n","tags":["andre3000","music","instrumental","experimental","ambient","jazz","creative","instrumental"]},{"title":"Cosmohedron by Duncan Hatch","contentType":"star","date":"2024-09-14 22:06 -05:00","url":"/star/cosmohedron-duncan-hatch/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=3pxrECZYEAA\" >https://www.youtube.com/watch?v=3pxrECZYEAA</a></div><div class=\"response-content\" ><p>I love the visuals on this video.</p>\n<p><a href=\"https://www.youtube.com/watch?v=3pxrECZYEAA\" title=\"Cosmohedron Animated Short Thumbnail\"><img src=\"http://img.youtube.com/vi/3pxrECZYEAA/0.jpg\" class=\"img-fluid\" alt=\"Cosmohedron Animated Short Thumbnail\" /></a></p>\n</div>\n","tags":["short","animation","psychidelic","duncanhatch","video","youtube"]},{"title":"Shipping Tumblr and WordPress","contentType":"star","date":"2024-09-05 22:15","url":"/star/shipping-wordpress-tumblr/","content":"<div class=\"response-target\" >→ <a href=\"https://automattic.com/2024/08/27/shipping-tumblr-and-wordpress/\" >https://automattic.com/2024/08/27/shipping-tumblr-and-wordpress/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Since Automattic acquired Tumblr we’ve made it more efficient, grown its revenue, and worked to improve the platform. But there’s one part of the plan that we haven’t yet started, which is to run Tumblr on WordPress. I’m pleased to say we’re kicking off that project now!</p>\n</blockquote>\n<p>A little late, but I can't express how much I love this.</p>\n<blockquote class=\"blockquote\">\n<p>We love Tumblr’s streamlined posting experience and its current product direction. We’re not changing that. We’re talking about running Tumblr’s backend on WordPress. You won’t even notice a difference from the outside.</p>\n</blockquote>\n<p>Although it sounds outside the scope of the original plan, it would be amazing if the shared backend as well as the Tumblr publishing front-end made their way into the open-source version of WordPress.</p>\n<p>That would open the door to self-hosted personal websites that also make it significantly easier to post content in smaller chunks like a microblog.</p>\n<p>Pair that with ActivityPub integrations and you now have a connected web of individuals where the first place they post content to is their own website and for discovery / broader reach can also federate and post to other platforms that also support protocols like ActivityPub.</p>\n<p>Can't wait to see how this project develops.</p>\n<blockquote class=\"blockquote\">\n<p>Do you yearn for the days when people owned their corner of the internet and expressed themselves in wild and wacky ways? Do you want to see an internet focused on creativity, art, and ideas instead of debating and dividing? Do you think content and data should be owned by authors and artists, instead of getting locked behind the closed platform of a mega-corporation? Do you want to build an internet where anyone with a story can tell it, and anyone with a product can sell it, regardless of income, gender, politics, language, or where they live in the world?</p>\n</blockquote>\n<p>The answer to all of those is YES.</p>\n</div>\n","tags":["wordpress","tumblr","personalweb","blogging","indieweb","microblog","posse","smallweb","automattic"]},{"title":"Join me at DEVintersection in Las Vegas - September 10-12","contentType":"reshare","date":"2024-09-04 20:10 -05:00","url":"/reshare/devintersection-vegas-sept-2024/","content":"<div class=\"response-target\" >→ <a href=\"https://devblogs.microsoft.com/dotnet/discover-dotnet-at-dev-intersection-las-vegas-2024/\" >https://devblogs.microsoft.com/dotnet/discover-dotnet-at-dev-intersection-las-vegas-2024/</a></div><div class=\"response-content\" ><p>I'm excited to be at DEVintersection again this year where I'll get a chance to meet old and new friends.</p>\n<p>I have a few sessions where I'll be talking about some of my favorite things, .NET and AI.</p>\n<ul>\n<li><a href=\"https://www.devintersection.com/#!/session/An%20Introduction%20to%20AI%20in%20.NET/6788\">Introduction to AI in .NET</a></li>\n<li><a href=\"https://www.devintersection.com/#!/session/Building%20AI%20Applications%20from%20Scratch:%20A%20Hands-On%20Guide%20for%20.NET%20Developers/6789\">Building AI Applications from Scratch: A Hands-On Guide for .NET Developers</a></li>\n<li><a href=\"https://www.devintersection.com/#!/session/Ask%20The%20Experts%20-%20Starting%20out%20on%20your%20.NET%20&amp;amp;%20AI%20explorations/7023\">Ask The Experts - Starting out on your .NET &amp;amp; AI explorations</a></li>\n</ul>\n<p>Other sessions I recommend as well:</p>\n<ul>\n<li><a href=\"https://www.devintersection.com/#!/session/Navigating%20the%20World%20of%20AI%20Models%20in%20.NET:%20From%20Local%20Development%20to%20the%20Cloud/7022\">Navigating the World of AI Models in .NET: From Local Development to the Cloud</a></li>\n<li><a href=\"https://www.devintersection.com/#!/session/KEYNOTE%20-%20Practical%20Real-World%20AI%20for%20Developers/6986\">KEYNOTE - Practical Real-World AI for Developers</a></li>\n<li><a href=\"https://www.devintersection.com/#!/session/Making%20Web%20Applications%20Intelligent%20and%20Performant%20with%20Redis/6982\">Making Web Applications Intelligent and Performant with Redis</a></li>\n<li><a href=\"https://www.devintersection.com/#!/session/Building%20AI%20Copilots:%20Integrate%20Semantic%20Kernel,%20Azure%20OpenAI,%20and%20Azure%20Cosmos%20DB%20with%20.NET%20Aspire/6960\">Building AI Copilots: Integrate Semantic Kernel, Azure OpenAI, and Azure Cosmos DB with .NET Aspire</a></li>\n<li><a href=\"https://www.devintersection.com/#!/session/Introduction%20to%20.NET%20Aspire/6964\">Introduction to .NET Aspire</a></li>\n</ul>\n<p>There's so many more, but in the interest of not listing them all out, check out the <a href=\"https://www.devintersection.com/#!/sessions\">schedule</a>.</p>\n<p>See you there!</p>\n</div>\n","tags":["devintersection","conference","dotnet","ai","aspnet","LAS","lasvegas","desert"]},{"title":"NPR Tiny Desk Concerts: Bob Weir, Wolf Bros, Mikaela Davis","contentType":"reshare","date":"2024-08-26 19:27 -05:00","url":"/reshare/npr-tiny-desk-bob-weir-wolf-bros-mikaela-davis/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=4l_gUwdPrNY\" >https://www.youtube.com/watch?v=4l_gUwdPrNY</a></div><div class=\"response-content\" ><p>I came across this gem over the weekend. <a href=\"https://www.mikaeladavis.com/\">Mikaela Davis</a> on the harp just takes Bird Song and Ripple to the next level.</p>\n<p><a href=\"https://www.youtube.com/watch?v=4l_gUwdPrNY\" title=\"NPR Tiny Desk Concert Bob Weir Wolf Bros video thumbnail\"><img src=\"http://img.youtube.com/vi/4l_gUwdPrNY/0.jpg\" class=\"img-fluid\" alt=\"NPR Tiny Desk Concert Bob Weir Wolf Bros video thumbnail\" /></a></p>\n</div>\n","tags":["music","tinydesk","gratefuldead","bobweir","npr","wolfbros","mikaeladavis","concert","deadhead"]},{"title":"Bringing Llama 3 to life","contentType":"reshare","date":"2024-08-22 16:21 -05:00","url":"/reshare/meta-bringing-llama-3-to-life/","content":"<div class=\"response-target\" >→ <a href=\"https://engineering.fb.com/2024/08/21/production-engineering/bringing-llama-3-to-life/\" >https://engineering.fb.com/2024/08/21/production-engineering/bringing-llama-3-to-life/</a></div><div class=\"response-content\" ><p>At AI Infra @ Scale 2024, Meta engineers discussed every step of how we built and brought Llama 3 to life, from data and training to inference.</p>\n<p><a href=\"https://www.youtube.com/watch?v=ELIcy6flgQI\" title=\"Bringing Llama 3 to life talk video thumbnail\"><img src=\"http://img.youtube.com/vi/ELIcy6flgQI/0.jpg\" class=\"img-fluid\" alt=\"Bringing Llama 3 to life talk video thumbnail\" /></a></p>\n</div>\n","tags":["ai","meta","llama3","opensource","llm"]},{"title":"Transformers in music recommendation","contentType":"reshare","date":"2024-08-21 20:16 -05:00","url":"/reshare/transformers-music-recommendation-google/","content":"<div class=\"response-target\" >→ <a href=\"https://research.google/blog/transformers-in-music-recommendation/\" >https://research.google/blog/transformers-in-music-recommendation/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We present a music recommendation ranking system that uses Transformer models to better understand the sequential nature of user actions based on the current user context.</p>\n</blockquote>\n</div>\n","tags":["google","ai","transformers","deeplearning","neuralneworks","recommendation","research","music"]},{"title":"New Chicano Batman: Tanto Arriba Como Abajo","contentType":"reshare","date":"2024-08-20 20:24 -05:00","url":"/reshare/new-chicano-batman-tanto-arriba-como-abajo/","content":"<div class=\"response-target\" >→ <a href=\"https://open.spotify.com/track/518ruJoGWraifuVpTBKr5a?si=6e7c5ac2ff7c42f3\" >https://open.spotify.com/track/518ruJoGWraifuVpTBKr5a?si=6e7c5ac2ff7c42f3</a></div><div class=\"response-content\" ><p>As above, so below.</p>\n<p>Some new Chicano Batman was a pleasant surprise this morning. Even better, an instrumental track.</p>\n<p>You can hear some of their older sound in this track, which I really enjoyed.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/518ruJoGWraifuVpTBKr5a\" width=\"100%\" height=\"152\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n</div>\n","tags":["music","chicanobatman","instrumental","newmusic"]},{"title":"Pain and sorrow on repeat","contentType":"reshare","date":"2024-08-19 21:47 -05:00","url":"/reshare/pain-and-sorrow-on-repeat/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=CfB0Uqd2aRE\" >https://www.youtube.com/watch?v=CfB0Uqd2aRE</a></div><div class=\"response-content\" ><p>I've had this song on repeat since<a href=\"/responses/new-flying-lotus-garmonbozia\">Friday</a>. Also, I didn't know the name of the song is a <a href=\"https://twinpeaks.fandom.com/wiki/Garmonbozia\">Twin Peaks</a> reference. Given his recent focus as a composer, I hope this means there's a bigger Twin Peaks project or collaboration in the works.</p>\n<p><a href=\"https://www.youtube.com/watch?v=CfB0Uqd2aRE\" title=\"Garmonbozia by Flying Lotus visualizer video thumbnail\"><img src=\"http://img.youtube.com/vi/CfB0Uqd2aRE/0.jpg\" class=\"img-fluid\" alt=\"Garmonbozia by Flying Lotus visualizer video thumbnail\" /></a></p>\n</div>\n","tags":["music","flyinglotus","twinpeaks","davidlynch","flylo","tv"]},{"title":"Thundercat + Yo Gabba Gabba","contentType":"reshare","date":"2024-08-17 11:06 -05:00","url":"/reshare/thundercat-yo-gabba-gabba/","content":"<div class=\"response-target\" >→ <a href=\"https://x.com/DatDaDatty/status/1824251654813704281\" >https://x.com/DatDaDatty/status/1824251654813704281</a></div><div class=\"response-content\" ><p><a href=\"https://x.com/DatDaDatty/status/1824251654813704281\"><img src=\"https://pbs.twimg.com/ext_tw_video_thumb/1824251568817844225/pu/img/25k7XExQAqNaL1kB.jpg\" class=\"img-fluid\" alt=\"Image of Thundercat playing the bass on Yo Gabba Gabba Show\" /></a></p>\n<p>Here's something I didn't know I needed in my life. Perfect way to start a Saturday morning.</p>\n</div>\n","tags":["music","thundercat","yogabbagabba","fun"]},{"title":"Organic Maps removed from Google Play Store","contentType":"reply","date":"2024-08-17 11:00 -05:00","url":"/reply/organic-maps-removed-play-store/","content":"<div class=\"response-target\" >→ <a href=\"https://twitter.com/organicmapsapp/status/1824727403580596260\" >https://twitter.com/organicmapsapp/status/1824727403580596260</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Last night Organic Maps was removed from the Play Store without any warnings or additional details due to &amp;quot;not meeting the requirements for the Family Program&amp;quot;. Compared to Google Maps and other maps apps rated for 3+ age, there are no ads or in-app purchases in Organic Maps. We have asked for an appeal.</p>\n</blockquote>\n<p>The app is still available on <a href=\"https://f-droid.org/en/packages/app.organicmaps/\">F-Droid</a>. Much better place for getting apps in my opinion.</p>\n</div>\n","tags":["organicmaps","google","play","fdroid","opensource","osm","openstreetmaps","maps","app","store"]},{"title":"New Flying Lotus: Garmonbozia","contentType":"reshare","date":"2024-08-16 09:52 -05:00","url":"/reshare/new-flying-lotus-garmonbozia/","content":"<div class=\"response-target\" >→ <a href=\"https://flyinglotus.bandcamp.com/track/garmonbozia\" >https://flyinglotus.bandcamp.com/track/garmonbozia</a></div><div class=\"response-content\" ><p>Good way to start a Friday. New drop from FlyLo. I like it.</p>\n<iframe style=\"border: 0; width: 100%; height: 120px;\" src=\"https://bandcamp.com/EmbeddedPlayer/track=1672253486/size=large/bgcol=ffffff/linkcol=0687f5/tracklist=false/artwork=small/transparent=true/\" seamless><a href=\"https://flyinglotus.bandcamp.com/track/garmonbozia\">Garmonbozia by Flying Lotus</a></iframe>\n</div>\n","tags":["music","flyinglotus","garmonbozia","newmusic","flylo","bandcamp"]},{"title":"The US government wants to make it easier for you to click the 'unsubscribe' button","contentType":"reshare","date":"2024-08-12 21:31 -05:00","url":"/reshare/us-govt-fcc-easier-unsubscribe/","content":"<div class=\"response-target\" >→ <a href=\"https://apnews.com/article/consumer-protection-ftc-fcc-biden-250f6eece6e2665535019128e8fa38da\" >https://apnews.com/article/consumer-protection-ftc-fcc-biden-250f6eece6e2665535019128e8fa38da</a></div><div class=\"response-content\" ><p>Given my <a href=\"/notes/subscription-dark-patterns\">recent experience unsubscribing from content</a>, there's definitely room for improvement.</p>\n</div>\n","tags":["regulation","fcc","subscriptions","uspol","government","us","darkpatterns"]},{"title":"Bluesky Welcomes Mike Masnick to Board of Directors","contentType":"reshare","date":"2024-08-07 22:14 -05:00","url":"/reshare/mike-masnick-bluesky-board-of-directors/","content":"<div class=\"response-target\" >→ <a href=\"https://bsky.social/about/blog/08-06-2024-board\" >https://bsky.social/about/blog/08-06-2024-board</a></div><div class=\"response-content\" ><p>Nice! I remember reading <a href=\"https://knightcolumbia.org/content/protocols-not-platforms-a-technological-approach-to-free-speech\">Protocols, Not Platforms</a> many years ago and felt inspired to seek out a better web.</p>\n</div>\n","tags":["bluesky","protocols","platforms","socialmedia","decentralization","distributedweb","indieweb","smallweb","humanweb","web"]},{"title":"Jackson Hole's new Instagram filter warns you when you're dangerously close to wildlife","contentType":"reshare","date":"2024-08-07 21:59 -05:00","url":"/reshare/jackson-hole-instagram-filter-wildlife/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/8/7/24215490/jackson-hole-travel-tourism-board-instagram-filter-animals-national-park-safety\" >https://www.theverge.com/2024/8/7/24215490/jackson-hole-travel-tourism-board-instagram-filter-animals-national-park-safety</a></div><div class=\"response-content\" ><p><img src=\"https://i.ytimg.com/vi/F8Knnumz6vQ/hqdefault.jpg\" class=\"img-fluid\" alt=\"Chapelle Show Meme Modern Problems Require Modern Solutions\" /></p>\n</div>\n","tags":["wildlife","wyoming","tourons","instagram","socialmedia","fun","jacksonhole","teton","nationalpark"]},{"title":"CNET purchased for JUST $100 million","contentType":"reply","date":"2024-08-06 09:06 -05:00","url":"/reply/cnet-bought-for-just-100-million/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/8/6/24214374/cnet-zeff-davis-acquisition-digital-media-100-million\" >https://www.theverge.com/2024/8/6/24214374/cnet-zeff-davis-acquisition-digital-media-100-million</a></div><div class=\"response-content\" ><p>JUST $100 million. That's pocket change 🙃</p>\n<blockquote class=\"blockquote\">\n<p>Red Ventures paid $500 million for the tech property once valued at $1.8 billion.</p>\n</blockquote>\n<p>Given previous valuations though, I see why the headline was phrased that way.</p>\n</div>\n","tags":["cnet","tech","acquisition","news"]},{"title":"Blogroll Club - A blog directory","contentType":"reshare","date":"2024-08-05 20:53 -05:00","url":"/reshare/blogroll-club-blog-directory/","content":"<div class=\"response-target\" >→ <a href=\"https://notes.jeddacp.com/a-blog-directory/\" >https://notes.jeddacp.com/a-blog-directory/</a></div><div class=\"response-content\" ><p>A Blog Directory is a cool project from <a href=\"https://notes.jeddacp.com/\">JC (Probably)</a> and <a href=\"https://louplummer.lol/\">Lou Plummer</a>. <a href=\"https://blogroll.club/\">Check it out</a>!</p>\n</div>\n","tags":["blog","rss","indieweb","blogroll","community","web","smallweb","personalweb"]},{"title":"Congratulations on ten years of Overcast","contentType":"reshare","date":"2024-08-04 17:07 -05:00","url":"/reshare/congratulations-overcast-ten-years/","content":"<div class=\"response-target\" >→ <a href=\"https://marco.org/2024/07/16/overcast-rewrite\" >https://marco.org/2024/07/16/overcast-rewrite</a></div><div class=\"response-content\" ><p>I no longer have an iPhone, but whenever anyone asks for a podcast app recommendation, Overcast is the first one I mention. Congrats on 10 years. Here's to many more.</p>\n</div>\n","tags":["podcast","app","overcast","anniversary","milestone","apple","ios","rss"]},{"title":"Reddit says companies must pay for data access","contentType":"reply","date":"2024-07-31 16:23 -05:00","url":"/reply/reddit-companies-pay-data-access/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/7/31/24210565/reddit-microsoft-anthropic-perplexity-pay-ai-search\" >https://www.theverge.com/2024/7/31/24210565/reddit-microsoft-anthropic-perplexity-pay-ai-search</a></div><div class=\"response-content\" ><p><img src=\"https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExejNzaDF5aHZ6dHRlenl6bnh4c2h6YWoxbnRuOXBqZnlmOGVlbWlwOCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/xT9DPzhNGA8MKjxwFG/giphy.gif\" class=\"img-fluid\" alt=\"GIF of Snoopy laughing\" /></p>\n<p>I'm all for compensation. Remind me again, how much are the communities that create the content and make Reddit what it is getting out of these licensing deals?</p>\n</div>\n","tags":["ai","reddit","data","licensing","microsoft","anthropic","perplexity"]},{"title":".well-known feeds","contentType":"reply","date":"2024-07-31 09:01 -05:00","url":"/reply/well-known-feeds/","content":"<div class=\"response-target\" >→ <a href=\"https://kiko.io/post/My-well-known-feeds-and-thoughts-beyond/\" >https://kiko.io/post/My-well-known-feeds-and-thoughts-beyond/</a></div><div class=\"response-content\" ><p>This is a clever use of <em>.well-known</em> and OPML.</p>\n<p>Definitely something I want to experiment with and implement on my site even if it's not widely adopted.</p>\n</div>\n","tags":["standards","feeds","rss","web","indieweb","smallweb"]},{"title":"Tensors from scratch series","contentType":"star","date":"2024-07-29 22:26 -05:00","url":"/star/tensors-from-scratch-blog-series/","content":"<div class=\"response-target\" >→ <a href=\"https://maharshi.bearblog.dev/tensors-from-scratch-part-2/\" >https://maharshi.bearblog.dev/tensors-from-scratch-part-2/</a></div><div class=\"response-content\" ><p>I've been enjoying reading through this Tensors series.</p>\n<p>If you're interested, here's also the link to <a href=\"https://maharshi.bearblog.dev/tensors-from-scratch-part-1/\">part 1</a>.</p>\n</div>\n","tags":["tensors","math","ai","c","machinelearning","ml","artificialintelligence"]},{"title":"Meta AI's Segment Anything Model (SAM) 2","contentType":"reshare","date":"2024-07-29 22:22 -05:00","url":"/reshare/meta-ai-segment-anything-model-2/","content":"<div class=\"response-target\" >→ <a href=\"https://ai.meta.com/blog/segment-anything-2/\" >https://ai.meta.com/blog/segment-anything-2/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n\n<ul>\n<li>Following up on the success of the Meta Segment Anything Model (SAM) for images, we're releasing SAM 2, a unified model for real-time promptable object segmentation in images and videos that achieves state-of-the-art performance.</li>\n<li>In keeping with our approach to open science, we're sharing the code and model weights with a permissive Apache 2.0 license.</li>\n<li>We're also sharing the SA-V dataset, which includes approximately 51,000 real-world videos and more than 600,000 masklets (spatio-temporal masks).</li>\n<li>SAM 2 can segment any object in any video or image - even for objects and visual domains it has not seen previously, enabling a diverse range of use cases without custom adaptation.</li>\n<li>SAM 2 has many potential real-world applications. For example, the outputs of SAM 2 can be used with a generative video model to create new video effects and unlock new creative applications. SAM 2 could also aid in faster annotation tools for visual data to build better computer vision systems.</li>\n</ul>\n</blockquote>\n</div>\n","tags":["meta","ai","computervision","sam2","segmentanythingmodel","aimodel","cv"]},{"title":"Zombie Internet","contentType":"reply","date":"2024-07-16 11:48 -05:00","url":"/reply/facebook-zombie-internet-willison/","content":"<div class=\"response-target\" >→ <a href=\"https://simonwillison.net/2024/Jul/15/facebook-is-the-zombie-internet/#atom-everything\" >https://simonwillison.net/2024/Jul/15/facebook-is-the-zombie-internet/#atom-everything</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In my experience, the supermajority of engagement on viral AI Facebook pages is just as artificially-generated as the content they publish.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Whether it's a child transforming into a water bottle cyborg, a three-armed flight attendant rescuing Tiger Jesus from a muddy plane crash, or a hybrid human-monkey baby being stung to death by giant hornets, all tend to have copy+pasted captions, reactions &amp;amp; comments which usually make no sense in the observed context.</p>\n</blockquote>\n<p>I've noticed similar patterns on YouTube. Sometime the comments include timestamp links which makes them seem more credible, but upon further inspection, it's all bot activity.</p>\n</div>\n","tags":["bots","web","comments","ai","facebook","internet","personalweb"]},{"title":"Test response using org capture templates","contentType":"reshare","date":"2024-07-14 13:47 -05:00","url":"/reshare/test-emacs-capture-response-org/","content":"<div class=\"response-target\" >→ <a href=\"https://orgmode.org/manual/Capture-templates.html\" >https://orgmode.org/manual/Capture-templates.html</a></div><div class=\"response-content\" ><p>Testing org-capture template generated response file</p>\n</div>\n","tags":["org","emacs","blogging","automation","orgmode","templates","capturetemplates","website","personalweb","capture"]},{"title":"Home-Cooked Software and Barefoot Developers","contentType":"star","date":"2024-07-06 22:02","url":"/star/home-cooked-software-barefoot-developers-appleton/","content":"<div class=\"response-target\" >→ <a href=\"https://maggieappleton.com/home-cooked-software\" >https://maggieappleton.com/home-cooked-software</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The emerging golden age of home-cooked software, barefoot developers, and why the local-first community should help build it</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This is a talk I presented Local-first Conference in Berlin, May 2024. It's specifically directed at the local-first\ncommunity, but its relevant to anyone involved in building software.<br />\n</br>\nFor the last ~year I've been keeping a close eye on how language models capabilities meaningfully change the speed, ease, and accessibility of software development. The slightly bold theory I put forward in this talk is that we're on a verge of a golden age of local, home-cooked software and a new kind of developer – what I've called the barefoot developer.</p>\n</blockquote>\n</div>\n","tags":["localfirst","smallweb","sofware","llm","programming","talk","indieweb"]},{"title":"Meta Large Language Model Compiler: Foundation Models of Compiler Optimization","contentType":"reshare","date":"2024-06-27 22:39","url":"/reshare/meta-llm-compiler/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/collections/facebook/llm-compiler-667c5b05557fe99a9edd25cb\" >https://huggingface.co/collections/facebook/llm-compiler-667c5b05557fe99a9edd25cb</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.</p>\n</blockquote>\n<ul>\n<li><a href=\"https://huggingface.co/collections/facebook/llm-compiler-667c5b05557fe99a9edd25cb\">HuggingFace Collection</a></li>\n<li><a href=\"https://scontent-lga3-1.xx.fbcdn.net/v/t39.2365-6/448997590_1496256481254967_2304975057370160015_n.pdf?_nc_cat=106&amp;amp;ccb=1-7&amp;amp;_nc_sid=3c67a6&amp;amp;_nc_ohc=4Yn8V9DFdbsQ7kNvgHtvfLI&amp;amp;_nc_ht=scontent-lga3-1.xx&amp;amp;oh=00_AYDYp657HzWrcs2CZ6ZBjStwB03bo760w9voXwJorfXA_w&amp;amp;oe=6683F28D\">Paper</a></li>\n</ul>\n</div>\n","tags":["ai","compiler","meta","llm"]},{"title":"Deep Questions - Debunking AI Model Capabilities / Distributed Webs of Trust","contentType":"reply","date":"2024-06-24 20:40","url":"/reply/deep-questions-debunking-genai-rss-blogroll/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=OvlfCW3Ec1g\" >https://www.youtube.com/watch?v=OvlfCW3Ec1g</a></div><div class=\"response-content\" ><p><a href=\"https://www.youtube.com/watch?v=OvlfCW3Ec1g\" title=\"Deep Questions Podcast Debunking AI Episode Thumbnail\"><img src=\"http://img.youtube.com/vi/OvlfCW3Ec1g/0.jpg\" class=\"img-fluid\" alt=\"Deep Questions Podcast Debunking AI Episode Thumbnail\" /></a></p>\n<p>Nice job by Cal debunking misconceptions about AI model capabilities. The segment highlights a few points I cover in my unpublished <a href=\"https://github.com/lqdev/luisquintanilla.me/blob/main/_scratch/nolm-not-only-language-models.md\">NoLM - Not Only Language Models</a> blog post. Specifically the fact that Language Models on their own can't do much and need to be connected to data sources and other systems. Complex AI systems will be built with more specialized roles and leverage various components for their planning and execution. In the end though, models will require integration into existing systems. Those integrations need to be done by people, meaning humans are still in control of the AI-assisted system capabilities.</p>\n<p><a href=\"https://www.youtube.com/watch?v=OvlfCW3Ec1g&amp;amp;t=4082\" title=\"Deep Questions Podcast Debunking AI Episode Thumbnail\"><img src=\"http://img.youtube.com/vi/OvlfCW3Ec1g/0.jpg\" class=\"img-fluid\" alt=\"Same Deep Questions Podcast Debunking AI Episode Thumbnail\" /></a></p>\n<p>Later in the podcast, Cal takes a question about distributed webs of trust. I agree with Cal's point of using existing open standards like RSS for content consumption. It's the reason you often hear the phrase, &amp;quot;or wherever you get your podcasts&amp;quot;. Assuming you have a program that can read an RSS feed, you can follow all types of content. On the topic of discovery, Cal makes the suggestion of using distributed webs of trust. Using domain names and linking as ways of discovering content. While blogrolls were not directly called out, it's one of the benefits a curated set of links provides.</p>\n</div>\n","tags":["ai","deepquestions","rss","calnewport","podcast","blogroll","opml","social","socialmedia","distributedweb"]},{"title":"Affirmations for bloggers","contentType":"star","date":"2024-06-11 21:42","url":"/star/ntietz-blogging-affirmations/","content":"<div class=\"response-target\" >→ <a href=\"https://ntietz.com/blog/blogging-affirmations/\" >https://ntietz.com/blog/blogging-affirmations/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n\n<p>Here are the things I've seen and learned. Each of these will be expanded in its own section.<br />\n<br></p>\n<ul>\n<li>You have things to write about.</li>\n<li>Your perspective matters.</li>\n<li>You are good enough.</li>\n<li>Posts don't have to be novel.</li>\n<li>People will read it.</li>\n<li>Mistakes are okay!</li>\n<li>It's okay to ask for things.</li>\n<li>You can get started quickly.</li>\n<li>You can write on a schedule.</li>\n</ul>\n</blockquote>\n<p><img src=\"https://i.imgflip.com/1otri4.jpg\" class=\"img-fluid\" alt=\"Meme of man pointing at himself in the mirror\" /></p>\n</div>\n","tags":["blogging","writing","personalweb","socialweb","indieweb"]},{"title":"The cassette tape is making a comeback","contentType":"reply","date":"2024-06-11 21:31","url":"/reply/npr-cassette-tapes-making-comeback/","content":"<div class=\"response-target\" >→ <a href=\"https://www.npr.org/2024/06/07/nx-s1-4976071/the-cassette-tape-is-making-a-comeback-thanks-to-a-family-run-company-in-missouri\" >https://www.npr.org/2024/06/07/nx-s1-4976071/the-cassette-tape-is-making-a-comeback-thanks-to-a-family-run-company-in-missouri</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Despite the odds, cassette tapes are making a comeback. And one family-owned company in Springfield, Missouri is a leader in the revival.</p>\n</blockquote>\n<p>Hopefully this also means that players are also making a comeback because you need something to play them on. Having visited a museum recently full of old radio recordings on cassette tapes, I was sad to find out I couldn't listen to them because the tape player was broken. It's hard enough finding a decent MP3 player, I'm sure it must be just as hard if not harder finding a tape player.</p>\n</div>\n","tags":["cassette","tapes","retro","music","comeback"]},{"title":"Deep Dive into Ownership in Mojo","contentType":"reshare","date":"2024-06-11 21:20","url":"/reshare/deep-dive-ownership-mojo/","content":"<div class=\"response-target\" >→ <a href=\"https://www.modular.com/blog/deep-dive-into-ownership-in-mojo\" >https://www.modular.com/blog/deep-dive-into-ownership-in-mojo</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In the second part of the ownership series in Mojo, we built on the mental model developed in the <a href=\"http://www.modular.com/blog/what-ownership-is-really-about-a-mental-model-approach\">first part</a> and provided practical examples to illustrate how ownership works in Mojo. We covered the different kinds of values (BValue, LValue, and RValue) and how they propagate through expressions. We also explained the function argument conventions (borrowed, inout, owned) and demonstrated how these conventions help manage memory safely and efficiently. We concluded with three fundamental rules:<br />\n<br></p>\n<ul>\n<li><strong>Rule 1</strong>: Owned arguments take RValue on the caller side but are LValue on the callee side.</li>\n<li><strong>Rule 2</strong>: Owned arguments own the type if the transfer operator ^ is used; otherwise, they copy the type if it is Copyable.</li>\n<li><strong>Rule 3</strong>: Copy operations are optimized to move operations if the type is Copyable and Movable and isn’t used anymore, reducing unnecessary overhead.<br />\n<br>\nLastly, we emphasized that the main goals of ownership in Mojo are:<br />\n<br></li>\n<li><strong>Memory Safety</strong>: Enforcing exclusive ownership and proper lifetimes to prevent memory errors such as use-after-free and double-free.</li>\n<li><strong>Performance Optimization</strong>: Converting unnecessary copy operations into move operations to reduce overhead and enhance performance.</li>\n<li><strong>Ease of Use</strong>: Automating memory management through ownership rules and the transfer operator, simplifying development.</li>\n<li><strong>Compile-Time Guarantees</strong>: Providing strong compile-time guarantees through type-checking and dataflow lifetime analysis, catching errors early in the development process.</li>\n</ul>\n</blockquote>\n</div>\n","tags":["mojo","python","c","cpp","c++","pl","programming","programminglanguage"]},{"title":"EFF Presents - The Encryptids - Bigfoot","contentType":"star","date":"2024-06-11 21:12","url":"/star/eff-the-encryptids-bigfoot/","content":"<div class=\"response-target\" >→ <a href=\"https://www.eff.org/deeplinks/2024/05/bigfoot\" >https://www.eff.org/deeplinks/2024/05/bigfoot</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>I’m proud to share the first post in a series from our friends, <a href=\"https://www.eff.org/Encryptids\">The Encryptids</a>—the rarely-seen enigmas who inspire campfire lore. But this time, they’re spilling secrets about how they survive this ever-digital world. We begin by checking in with the legendary Bigfoot de la Sasquatch...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>People say I'm the most famous of The Encryptids, but sometimes I don't want the spotlight. They all want a piece of me: exes, ad trackers, scammers, even the government. A picture may be worth a thousand words, but my digital profile is worth cash (to skeezy data brokers). I can’t hit a city block without being captured by doorbell cameras, CCTV, license plate readers, and a maze of street-level surveillance. It can make you want to give up on privacy altogether. Honey, no. Why should you have to hole up in some dank, busted forest for freedom and respect? You don’t.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Privacy isn't about hiding. It's about revealing what you want to who you want on your terms. It's your basic right to dignity.</strong></p>\n</blockquote>\n</div>\n","tags":["eff","privacy","digitalrights","campaign"]},{"title":"Mapping the Mind of a Large Language Model","contentType":"reshare","date":"2024-06-11 21:03","url":"/reshare/mapping-mind-large-language-model-anthropic/","content":"<div class=\"response-target\" >→ <a href=\"https://www.anthropic.com/research/mapping-mind-language-model\" >https://www.anthropic.com/research/mapping-mind-language-model</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today we report a significant advance in understanding the inner workings of AI models. We have identified how millions of concepts are represented inside Claude Sonnet, one of our deployed large language models. This is the first ever detailed look inside a modern, production-grade large language model. This interpretability discovery could, in future, help us make AI models safer.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Previously, we made some progress matching patterns of neuron activations, called features, to human-interpretable concepts. We used a technique called &amp;quot;dictionary learning&amp;quot;, borrowed from classical machine learning, which isolates patterns of neuron activations that recur across many different contexts. In turn, any internal state of the model can be represented in terms of a few active features instead of many active neurons. Just as every English word in a dictionary is made by combining letters, and every sentence is made by combining words, every feature in an AI model is made by combining neurons, and every internal state is made by combining features.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In October 2023, we reported success applying dictionary learning to a very small &amp;quot;toy&amp;quot; language model and found coherent features corresponding to concepts like uppercase text, DNA sequences, surnames in citations, nouns in mathematics, or function arguments in Python code.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We used the same scaling law philosophy that predicts the performance of larger models from smaller ones to tune our methods at an affordable scale before launching on Sonnet.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We successfully extracted millions of features from the middle layer of Claude 3.0 Sonnet, (a member of our current, state-of-the-art model family, currently available on claude.ai), providing a rough conceptual map of its internal states halfway through its computation. This is the first ever detailed look inside a modern, production-grade large language model.</p>\n</blockquote>\n</div>\n","tags":["ai","interpretability","anthropic","llm"]},{"title":"Claude's Character","contentType":"reshare","date":"2024-06-11 20:56","url":"/reshare/claudes-character-anthropic/","content":"<div class=\"response-target\" >→ <a href=\"https://www.anthropic.com/research/claude-character\" >https://www.anthropic.com/research/claude-character</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Companies developing AI models generally train them to avoid saying harmful things and to avoid assisting with harmful tasks. The goal of this is to train models to behave in ways that are &amp;quot;harmless&amp;quot;. But when we think of the character of those we find genuinely admirable, we don’t just think of harm avoidance. We think about those who are curious about the world, who strive to tell the truth without being unkind, and who are able to see many sides of an issue without becoming overconfident or overly cautious in their views. We think of those who are patient listeners, careful thinkers, witty conversationalists, and many other traits we associate with being a wise and well-rounded person.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>AI models are not, of course, people. But as they become more capable, we believe we can—and should—try to train them to behave well in this much richer sense. Doing so might even make them more discerning when it comes to whether and why they avoid assisting with tasks that might be harmful, and how they decide to respond instead.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Claude 3 was the first model where we added &amp;quot;character training&amp;quot; to our alignment finetuning process: the part of training that occurs after initial model training, and the part that turns it from a predictive text model into an AI assistant. The goal of character training is to make Claude begin to have more nuanced, richer traits like curiosity, open-mindedness, and thoughtfulness.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Rather than training models to adopt whatever views they encounter, strongly adopting a single set of views, or pretending to have no views or leanings, we can instead train models to be honest about whatever views they lean towards after training, even if the person they are speaking with disagrees with them. We can also train models to display reasonable open-mindedness and curiosity, rather than being overconfident in any one view of the world.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In order to steer Claude’s character and personality, we made a list of many character traits we wanted to encourage the model to have...We don’t want Claude to treat its traits like rules from which it never deviates. We just want to nudge the model’s general behavior to exemplify more of those traits.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Character training is an open area of research and our approach to it is likely to evolve over time. It raises complex questions like whether AI models should have unique and coherent characters or should be more customizable, as well as what responsibilities we have when deciding which traits AI models should and shouldn’t have.</p>\n</blockquote>\n</div>\n","tags":["anthropic","claude","alignment","ai"]},{"title":"Ultravox - An open, fast, and extensible multimodal LLM","contentType":"reshare","date":"2024-06-11 20:51","url":"/reshare/ultravox-multimodal-llm/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/fixie-ai/ultravox\" >https://github.com/fixie-ai/ultravox</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Ultravox is a new kind of multimodal LLM that can understand text as well as human speech, without the need for a separate Audio Speech Recognition (ASR) stage. Building on research like AudioLM, SeamlessM4T, Gazelle, SpeechGPT, and others, we've extended Meta's Llama 3 model with a multimodal projector that converts audio directly into the high-dimensional space used by Llama 3. This direct coupling allows Ultravox to respond much more quickly than systems that combine separate ASR and LLM components. In the future this will also allow Ultravox to natively understand the paralinguistic cues of timing and emotion that are omnipresent in human speech.<br />\n<br>\nThe current version of Ultravox (v0.1), when invoked with audio content, has a time-to-first-token (TTFT) of approximately 200ms, and a tokens-per-second rate of ~100, all using a Llama 3 8B backbone. While quite fast, we believe there is considerable room for improvement in these numbers. We look forward to working with LLM hosting providers to deliver state-of-the-art performance for Ultravox.<br />\n<br>\nUltravox currently takes in audio and emits streaming text. As we evolve the model, we'll train it to be able to emit a stream of speech tokens that can then be converted directly into raw audio by an appropriate unit vocoder. We're interested in working with interested parties to build this functionality!</p>\n</blockquote>\n</div>\n","tags":["ai","multimodal","llm"]},{"title":"Apple - Private Cloud Compute","contentType":"reshare","date":"2024-06-11 20:42","url":"/reshare/apple-private-cloud-compute/","content":"<div class=\"response-target\" >→ <a href=\"https://security.apple.com/blog/private-cloud-compute/\" >https://security.apple.com/blog/private-cloud-compute/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We set out to build Private Cloud Compute with a set of core requirements:<br />\n<br></p>\n<ol>\n<li><strong>Stateless computation on personal user data.</strong> ...we want a strong form of stateless data processing where personal data leaves no trace in the PCC system.</li>\n<li><strong>Enforceable guarantee.</strong> Security and privacy guarantees are strongest when they are entirely technically enforceable, which means it must be possible to constrain and analyze all the components that critically contribute to the guarantees of the overall Private Cloud Compute system.</li>\n<li><strong>No privileged runtime access.</strong> Private Cloud Compute must not contain privileged interfaces that would enable Apple’s site reliability staff to bypass PCC privacy guarantees, even when working to resolve an outage or other severe incident.</li>\n<li><strong>Non-targetability.</strong> An attacker should not be able to attempt to compromise personal data that belongs to specific, targeted Private Cloud Compute users without attempting a broad compromise of the entire PCC system.</li>\n<li><strong>Verifiable transparency.</strong> Security researchers need to be able to verify, with a high degree of confidence, that our privacy and security guarantees for Private Cloud Compute match our public promises.</li>\n</ol>\n</blockquote>\n</div>\n","tags":["ai","pcc","privacy","cloud","privatecloudcompute","wwdc"]},{"title":"Introducing Apple’s On-Device and Server Foundation Models","contentType":"reshare","date":"2024-06-11 20:26","url":"/reshare/introducing-apple-on-device-server-foundational-models/","content":"<div class=\"response-target\" >→ <a href=\"https://machinelearning.apple.com/research/introducing-apple-foundation-models\" >https://machinelearning.apple.com/research/introducing-apple-foundation-models</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Apple Intelligence is comprised of multiple highly-capable generative models that are specialized for our users’ everyday tasks, and can adapt on the fly for their current activity. The foundation models built into Apple Intelligence have been fine-tuned for user experiences such as writing and refining text, prioritizing and summarizing notifications, creating playful images for conversations with family and friends, and taking in-app actions to simplify interactions across apps.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In the following overview, we will detail how two of these models — a ~3 billion parameter on-device language model, and a larger server-based language model available with Private Cloud Compute and running on Apple silicon servers — have been built and adapted to perform specialized tasks efficiently, accurately, and responsibly. These two foundation models are part of a larger family of generative models created by Apple to support users and developers; this includes a coding model to build intelligence into Xcode, as well as a diffusion model to help users express themselves visually, for example, in the Messages app.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>Our foundation models are trained on Apple's AXLearn framework, an open-source project we released in 2023. It builds on top of JAX and XLA, and allows us to train the models with high efficiency and scalability on various training hardware and cloud platforms, including TPUs and both cloud and on-premise GPUs. We used a combination of data parallelism, tensor parallelism, sequence parallelism, and Fully Sharded Data Parallel (FSDP) to scale training along multiple dimensions such as data, model, and sequence length.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>We find that data quality is essential to model success, so we utilize a hybrid data strategy in our training pipeline, incorporating both human-annotated and synthetic data, and conduct thorough data curation and filtering procedures. We have developed two novel algorithms in post-training: (1) a rejection sampling fine-tuning algorithm with teacher committee, and (2) a reinforcement learning from human feedback (RLHF) algorithm with mirror descent policy optimization and a leave-one-out advantage estimator. We find that these two algorithms lead to significant improvement in the model’s instruction-following quality.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>Both the on-device and server models use grouped-query-attention. We use shared input and output vocab embedding tables to reduce memory requirements and inference cost. These shared embedding tensors are mapped without duplications. The on-device model uses a vocab size of 49K, while the server model uses a vocab size of 100K, which includes additional language and technical tokens.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>For on-device inference, we use low-bit palletization, a critical optimization technique that achieves the necessary memory, power, and performance requirements. To maintain model quality, we developed a new framework using LoRA adapters that incorporates a mixed 2-bit and 4-bit configuration strategy — averaging 3.5 bits-per-weight — to achieve the same accuracy as the uncompressed models.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Additionally, we use an interactive model latency and power analysis tool, Talaria, to better guide the bit rate selection for each operation. We also utilize activation quantization and embedding quantization, and have developed an approach to enable efficient Key-Value (KV) cache update on our neural engines.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>Our foundation models are fine-tuned for users’ everyday activities, and can dynamically specialize themselves on-the-fly for the task at hand...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We represent the values of the adapter parameters using 16 bits, and for the ~3 billion parameter on-device model, the parameters for a rank 16 adapter typically require 10s of megabytes. The adapter models can be dynamically loaded, temporarily cached in memory, and swapped — giving our foundation model the ability to specialize itself on the fly for the task at hand while efficiently managing memory and guaranteeing the operating system's responsiveness.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>We compare our models with both open-source models (Phi-3, Gemma, Mistral, DBRX) and commercial models of comparable size (GPT-3.5-Turbo, GPT-4-Turbo)1. We find that our models are preferred by human graders over most comparable competitor models. On this benchmark, our on-device model, with ~3B parameters, outperforms larger models including Phi-3-mini, Mistral-7B, and Gemma-7B. Our server model compares favorably to DBRX-Instruct, Mixtral-8x22B, and GPT-3.5-Turbo while being highly efficient.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>To further evaluate our models, we use the Instruction-Following Eval (IFEval) benchmark to compare their instruction-following capabilities with models of comparable size. The results suggest that both our on-device and server model follow detailed instructions better than the open-source and commercial models of comparable size.</p>\n</blockquote>\n</div>\n","tags":["ai","apple","slm","edge","wwdc"]},{"title":"Introducing Apple Intelligence","contentType":"reshare","date":"2024-06-11 20:16","url":"/reshare/introducing-apple-intelligence/","content":"<div class=\"response-target\" >→ <a href=\"https://www.apple.com/newsroom/2024/06/introducing-apple-intelligence-for-iphone-ipad-and-mac/\" >https://www.apple.com/newsroom/2024/06/introducing-apple-intelligence-for-iphone-ipad-and-mac/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Apple today introduced Apple Intelligence, the personal intelligence system for iPhone, iPad, and Mac that combines the power of generative models with personal context to deliver intelligence that’s incredibly useful and relevant.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Apple Intelligence unlocks new ways for users to enhance their writing and communicate more effectively. With brand-new systemwide Writing Tools built into iOS 18, iPadOS 18, and macOS Sequoia, users can rewrite, proofread, and summarize text nearly everywhere they write...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Apple Intelligence powers exciting image creation capabilities to help users communicate and express themselves in new ways. With Image Playground, users can create fun images in seconds, choosing from three styles: Animation, Illustration, or Sketch...All images are created on device, giving users the freedom to experiment with as many images as they want.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Powered by Apple Intelligence, Siri becomes more deeply integrated into the system experience. With richer language-understanding capabilities, Siri is more natural, more contextually relevant, and more personal, with the ability to simplify and accelerate everyday tasks.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>A cornerstone of Apple Intelligence is on-device processing, and many of the models that power it run entirely on device. To run more complex requests that require more processing power, Private Cloud Compute extends the privacy and security of Apple devices into the cloud to unlock even more intelligence.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Apple is integrating ChatGPT access into experiences within iOS 18, iPadOS 18, and macOS Sequoia, allowing users to access its expertise — as well as its image- and document-understanding capabilities — without needing to jump between tools.</p>\n</blockquote>\n</div>\n","tags":["ai","apple","appleintelligence"]},{"title":"The Verge - Apple WWDC 2024 keynote in 18 minutes","contentType":"reshare","date":"2024-06-11 20:05","url":"/reshare/verge-wwdc-2024-18-minute-recap-video/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=sBXdyUA6A88\" >https://www.youtube.com/watch?v=sBXdyUA6A88</a></div><div class=\"response-content\" ><p>I always look forward to watching these recap videos from the Verge.</p>\n<p><a href=\"https://www.youtube.com/watch?v=sBXdyUA6A88\" title=\"Thumbnail of Verge WWDC 2024 Keynote recap video\"><img src=\"http://img.youtube.com/vi/sBXdyUA6A88/0.jpg\" class=\"img-fluid\" alt=\"Thumbnail of Verge WWDC 2024 Keynote recap video\" /></a></p>\n</div>\n","tags":["apple","wwdc","keynote","wwdc2024","ai","ios","ipad","iphone","watchos","ipados","macos","visionos"]},{"title":"Andrej Karpathy - Let's reproduce GPT-2 (124M)","contentType":"reshare","date":"2024-06-11 19:59","url":"/reshare/karpathy-reproduce-gpt-2-124m-tutorial/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=l8pRSuU81PU\" >https://www.youtube.com/watch?v=l8pRSuU81PU</a></div><div class=\"response-content\" ><p><a href=\"https://www.youtube.com/watch?v=l8pRSuU81PU\" title=\"YouTube thumbnail of Karpathy Let's Preproduce GPT video tutorial\"><img src=\"http://img.youtube.com/vi/l8pRSuU81PU/0.jpg\" class=\"img-fluid\" alt=\"YouTube thumbnail of Karpathy Let's Preproduce GPT video tutorial\" /></a></p>\n</div>\n","tags":["ai","gpt","transformer","tutorial"]},{"title":"TinyAgent: Function Calling at the Edge","contentType":"reshare","date":"2024-06-11 19:43","url":"/reshare/TinyAgent-function-calling-edge/","content":"<div class=\"response-target\" >→ <a href=\"https://bair.berkeley.edu/blog/2024/05/29/tiny-agent/\" >https://bair.berkeley.edu/blog/2024/05/29/tiny-agent/</a></div><div class=\"response-content\" ><p><img src=\"https://bair.berkeley.edu/static/blog/tiny-agent/Figure2.png\" class=\"img-fluid\" alt=\"High level conceptual diagram of TinyAgent system\" /></p>\n<blockquote class=\"blockquote\">\n<p>The ability of LLMs to execute commands through plain language (e.g. English) has enabled agentic systems that can complete a user query by orchestrating the right set of tools...recent multi-modal efforts such as the GPT-4o or Gemini-1.5 model, has expanded the realm of possibilities with AI agents. While this is quite exciting, the large model size and computational requirements of these models often requires their inference to be performed on the cloud. This can create several challenges for their widespread adoption. First and foremost, uploading data such as video, audio, or text documents to a third party vendor on the cloud, can result in privacy issues. Second, this requires cloud/Wi-Fi connectivity which is not always possible...latency could also be an issue as uploading large amounts of data to the cloud and waiting for the response could slow down response time, resulting in unacceptable time-to-solution. These challenges could be solved if we deploy the LLM models locally at the edge.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...current LLMs like GPT-4o or Gemini-1.5 are too large for local deployment. One contributing factor is that a lot of the model size ends up memorizing general information about the world into its parametric memory which may not be necessary for a specialized downstream application.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...this leads to an intriguing research question:<br />\n<br>\nCan a smaller language model with significantly less parametric memory emulate such emergent ability of these larger language models?</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Achieving this would significantly reduce the computational footprint of agentic systems and thus enable efficient and privacy-preserving edge deployment. Our study demonstrates that this is feasible for small language models through training with specialized, high-quality data that does not require recalling generic world knowledge.<br />\n<br>\nSuch a system could particularly be useful for semantic systems where the AI agent’s role is to understand the user query in natural language and, instead of responding with a ChatGPT-type question answer response, orchestrate the right set of tools and APIs to accomplish the user’s command. For example, in a Siri-like application, a user may ask a language model to create a calendar invite with particular attendees. If a predefined script for creating calendar items already exists, the LLM simply needs to learn how to invoke this script with the correct input arguments (such as attendees’ email addresses, event title, and time). This process does not require recalling/memorization of world knowledge from sources like Wikipedia, but rather requires reasoning and learning to call the right functions and to correctly orchestrate them.<br />\n<br>\nOur goal is to develop Small Language Models (SLM) that are capable of complex reasoning that could be deployed securely and privately at the edge. Here we will discuss the research directions that we are pursuing to that end. First, we discuss how we can enable small open-source models to perform accurate function calling, which is a key component of agentic systems. It turns out that off-the-shelf small models have very low function calling capabilities. We discuss how we address this by systematically curating high-quality data for function calling, using a specialized Mac assistant agent as our driving application. We then show that fine-tuning the model on this high quality curated dataset, can enable SLMs to even exceed GPT-4-Turbo’s function calling performance. We then show that this could be further improved and made efficient through a new Tool RAG method. Finally, we show how the final models could be deployed efficiently at the edge with real time responses.</p>\n</blockquote>\n</div>\n","tags":["agent","ai","slm","function","edge","local","languagemodel","smalllanguagemodel","research"]},{"title":"Tierra Whack - NPR Tiny Desk Concert","contentType":"reshare","date":"2024-06-08 11:55","url":"/reshare/tierra-whack-npr-tiny-desk-2024/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=uRG6IWYVazM\" >https://www.youtube.com/watch?v=uRG6IWYVazM</a></div><div class=\"response-content\" ><p>Great performance.</p>\n<p><a href=\"https://www.youtube.com/watch?v=uRG6IWYVazM\" title=\"Thumbnail of Tierra Whack NPR Tiny Desk Concert\"><img src=\"http://img.youtube.com/vi/uRG6IWYVazM/0.jpg\" class=\"img-fluid\" alt=\"Thumbnail of Tierra Whack NPR Tiny Desk Concert\" /></a></p>\n</div>\n","tags":["music","tinydesk","tierrawhack","hiphop","rap","livemusic","performance","npr"]},{"title":"Omakub - An Omakase Developer Setup for Ubuntu 24.04","contentType":"reshare","date":"2024-06-06 22:14","url":"/reshare/omakub-ubuntu-developer-desktop/","content":"<div class=\"response-target\" >→ <a href=\"https://omakub.org/\" >https://omakub.org/</a></div><div class=\"response-content\" ><p>Cool project.</p>\n<blockquote class=\"blockquote\">\n<p>Turn a fresh Ubuntu installation into a fully-configured, beautiful, and modern web development system by running a single command.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Omakub is an opinionated take on what Linux can be at its best.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Omakub includes a curated set of applications and tools that one might discover through hours of watching YouTube, reading blogs, or just stumbling around Linux internet. All so someone coming straight from a platform like Windows or the Mac can immediately start enjoying a ready-made system, without having to do any configuration and curation legwork at all.</p>\n</blockquote>\n</div>\n","tags":["linux","developer","ubuntu","desktop","37signals"]},{"title":"My Permadomain","contentType":"reply","date":"2024-05-30 21:41","url":"/reply/my-permadomain-rscottjones/","content":"<div class=\"response-target\" >→ <a href=\"https://rscottjones.com/my-permadomain/\" >https://rscottjones.com/my-permadomain/</a></div><div class=\"response-content\" ><p>I'm guilty of owning way too many domains but this short-hand (lqdev.me) and other permadomain (luisquintanilla.me) are the ones I use mostly.</p>\n<p>A potential happy medium I've seen is using subdomains (i.e. project.lqdev.me).</p>\n<p><a href=\"https://blog.jim-nielsen.com/2018/pathnames-to-subdomains/\">Jim Nielsen has great examples</a> of how he's doing that today. Though that's an older post, he also addressed it most recently in the post <a href=\"https://blog.jim-nielsen.com/2023/domain-sins-of-my-youth/\">Domain Sins of My Youth</a>.</p>\n<p>That somehow feels better than the URL you get when you use free hosting like GitHub Pages. As a bonus, it's already tied to your identity (if using your personal domain). Also, you save some money.</p>\n<p>There's a few ways I practice this today. In addition to the previously mentioned domains, I also own lqdev.tech. Services and projects I host live there.</p>\n<p>For example:</p>\n<ul>\n<li>Mastodon Instance (toot.lqdev.tech)</li>\n<li>Matrix server (matrix.lqdev.tech)</li>\n<li>Webmentions service (webmentions.lqdev.tech)</li>\n</ul>\n<p>So far it's been working well.</p>\n</div>\n","tags":["domains","personalweb","permadomain","indieweb","projects","selfhosting","hosting","self-hosting"]},{"title":"Micro.blog - Podcast hosting for $5","contentType":"star","date":"2024-05-30 21:27","url":"/star/microblog-podcast-hosting/","content":"<div class=\"response-target\" >→ <a href=\"https://www.manton.org/2024/05/29/podcast-hosting-for.html\" >https://www.manton.org/2024/05/29/podcast-hosting-for.html</a></div><div class=\"response-content\" ><p>This is cool! The Micro.blog premium plan has tons of great features as well.</p>\n<blockquote class=\"blockquote\">\n<p>Six years ago, we launched our $10/month plan with podcast hosting. Since then we’ve added several big features to the plan...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Today, I want to bring the podcast feature to more people, so we’re moving it down to the standard $5/month plan.</p>\n</blockquote>\n</div>\n","tags":["microblog","podcast","indieweb","hosting","blog","blogging"]},{"title":"Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20","contentType":"reshare","date":"2024-05-28 20:33","url":"/reshare/repro-gpt-2-llm-c-90-min-20-dollars-karpathy/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/karpathy/llm.c/discussions/481\" >https://github.com/karpathy/llm.c/discussions/481</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...the TLDR is that we're training a 12-layer GPT-2 (124M), from scratch, on 10B tokens of FineWeb, with max sequence length of 1024 tokens.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The 124M model is the smallest model in the GPT-2 series released by OpenAI in 2019, and is actually quite accessible today, even for the GPU poor. With llm.c, which is quite efficient at up to ~60% model flops utilization, reproducing this model on one 8X A100 80GB SXM node takes ~90 minutes. For example, on Lambda this node goes for ~$14/hr, so the total cost of reproducing this model today is about $20. You can train the model with a single GPU too, it would just take proportionally longer (e.g. ~4-24 hours depending on the GPU).</p>\n</blockquote>\n</div>\n","tags":["ai","llm","gpt","gpt2","llmc","c","slm"]},{"title":"21 Years of WordPress","contentType":"reshare","date":"2024-05-27 21:05","url":"/reshare/21-years-of-wordpress/","content":"<div class=\"response-target\" >→ <a href=\"https://ma.tt/2024/05/wp21/\" >https://ma.tt/2024/05/wp21/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>21 years since Mike and I did the first release of WordPress, forking Michel’s work on b2/cafélog.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I’ve been thinking a lot about elements that made WordPress successful in its early years that we should keep in mind as we build this year and beyond. Here’s 11 opinions:<br />\n<br></p>\n<ol>\n<li>Simple things should be easy and intuitive, and complex things possible.<br />\n<br>...</li>\n<li>Wikis are amazing, and our documentation should be wiki-easy to edit.<br />\n<br>...</li>\n<li>It’s important that we all do support, go to meetups and events, anything we can to stay close to regular end-users of what we make.</li>\n</ol>\n</blockquote>\n<p>Congrats to WordPress and the team on 21 years. I think the following are some good elements to keep building on.</p>\n</div>\n","tags":["wordpress","blogging","web","anniversary","opensource","openweb"]},{"title":"The Daylight DC1 is a $729 attempt to build a calmer computer","contentType":"reshare","date":"2024-05-26 16:48","url":"/reshare/daylight-dc-1-tablet-computer/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/5/23/24163225/daylight-dc1-tablet-livepaper\" >https://www.theverge.com/2024/5/23/24163225/daylight-dc1-tablet-livepaper</a></div><div class=\"response-content\" ><p>Like many in the comments, I like the idea of this device and what it's aiming to do.</p>\n<p>The &amp;quot;LivePaper&amp;quot; display technology seems interesting.</p>\n<p>At $729 though, it seems a little steep. I paid a fraction of that for my <a href=\"/uses\">Onyx Boox Nova Air 2</a> and I have most of the functionality of this device. As an e-reader, note-taking device, and tablet that can install any Android app, it works perfectly fine for my use cases and I'm very happy with it.</p>\n<p>Still, I'm interested in seeing whether this takes off or whether it ends up being another Rabbit R1 or Humane Ai Pin.</p>\n</div>\n","tags":["eink","digitalminimalism","daylight","tablet","technology","gadgets"]},{"title":"The IndieWeb's Next Stage","contentType":"reply","date":"2024-05-25 21:54","url":"/reply/indieweb-next-steps-durnell/","content":"<div class=\"response-target\" >→ <a href=\"https://tracydurnell.com/2024/05/17/indieweb-next-stage/\" >https://tracydurnell.com/2024/05/17/indieweb-next-stage/</a></div><div class=\"response-content\" ><p><span>🙋</span></p>\n<p>Great post.</p>\n<p>One of the challenges I've found, even when using appliances like those from hosting providers like <a href=\"https://www.linode.com/marketplace/apps/linode/wordpress/\">Linode</a> or tools like <a href=\"https://yunohost.org/\">YunoHost</a> is connecting them to your domain.</p>\n<p>Usually, that's specific to your domain name provider and usually a manual process.</p>\n<p>In cases where that's easy, you're often overcharged for the convenience. Compared to &amp;quot;free&amp;quot; social media and publishing websites, it makes using your own domain a less desirable option.</p>\n<p>Maybe advocating for a multi-staged approach like <a href=\"https://indiewebify.me/\">IndieWebify.Me</a> could make the journey more approachable. More importantly, making it a journey, rather than a destination could help with meeting folks where they are and guide them closer towards their goals.</p>\n<p>In any case, the proposed list of goals seem like a great start towards helping people create their own place on the web.</p>\n</div>\n","tags":["indieweb","community","personalweb"]},{"title":"RIP Kabosu","contentType":"reshare","date":"2024-05-25 19:55","url":"/reshare/rip-kabosu-doge/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/5/24/24163865/doge-meme-shiba-inu-kabosu-dead-crypto\" >https://www.theverge.com/2024/5/24/24163865/doge-meme-shiba-inu-kabosu-dead-crypto</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The face of one of the defining memes of the 2010s, the doge meme, died on Friday. Kabosu, the shiba inu with the knowing face that launched a million internet jokes, was 18 when she died</p>\n</blockquote>\n</div>\n","tags":["shiba","doge","meme","dog"]},{"title":"Erykah Badu + Reggie Watts + Marc Rebillet","contentType":"star","date":"2024-05-18 23:50","url":"/star/badu-watts-rebillet/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=AtR1yVmCCvw\" >https://www.youtube.com/watch?v=AtR1yVmCCvw</a></div><div class=\"response-content\" ><p>I never knew I needed this in my life.</p>\n<p><a href=\"https://www.youtube.com/watch?v=AtR1yVmCCvw\" title=\"Erykah Badu, Reggie Watts, Marc Rebillet Jamming\"><img src=\"http://img.youtube.com/vi/AtR1yVmCCvw/0.jpg\" class=\"img-fluid\" alt=\"Erykah Badu, Reggie Watts, Marc Rebillet Jamming\" /></a></p>\n</div>\n","tags":["badu","watts","rebillet","music"]},{"title":"One year with the Light Phone 2","contentType":"reply","date":"2024-04-28 20:43","url":"/reply/one-year-retrospect-lightphone-2-verge/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/24140675/light-phone-2-one-year-retrospect\" >https://www.theverge.com/24140675/light-phone-2-one-year-retrospect</a></div><div class=\"response-content\" ><p><img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExZXVrN2kxamptbnZtdTVwZnM1ZndnOWFydnBlOTdja2FpMTI1cjdnZSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3o6ZtfZp8ID54YUne0/giphy.gif\" class=\"img-fluid\" alt=\"GIF image with man signing into microphone with caption don't call it a comeback\" /></p>\n<p>Between this article and <a href=\"/responses/dumbphone-boom-real\">The Dumbphone Boom is Real</a>, I'm seeing more publications on the subject. Far from a comeback, but still cool to see.</p>\n</div>\n","tags":["dumbphone","retro","technology","lightphone","minimalism","digitalminimalism"]},{"title":"New Album: Kamasi Washington - Fearless Movement","contentType":"reshare","date":"2024-04-28 20:34","url":"/reshare/new-kamasi-washington-album-fearless-movement/","content":"<div class=\"response-target\" >→ <a href=\"https://kamasiwashington.ffm.to/fearlessmovement\" >https://kamasiwashington.ffm.to/fearlessmovement</a></div><div class=\"response-content\" ><p>2024 just keeps getting better in terms of new music releases.</p>\n<p>Kamasi Washington has a new album coming out this week called Fearless Movement.</p>\n<p>The most recent single, Dream State, with Andre 3000 is great.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/track/1nLU0aUyHaza8hZm1Jp6lu?utm_source=generator\" width=\"100%\" height=\"152\" frameBorder=\"0\" allowfullscreen=\"\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\" loading=\"lazy\"></iframe>\n</div>\n","tags":["newmusic","kamasiwashington","jazz","album","music"]},{"title":"The Dumbphone Boom Is Real","contentType":"reshare","date":"2024-04-25 23:38","url":"/reshare/dumbphone-boom-real/","content":"<div class=\"response-target\" >→ <a href=\"https://www.newyorker.com/culture/infinite-scroll/the-dumbphone-boom-is-real\" >https://www.newyorker.com/culture/infinite-scroll/the-dumbphone-boom-is-real</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Will Stults spent too much time on his iPhone, doom-scrolling the site formerly known as Twitter and tweeting angrily at Elon Musk as if the billionaire would actually notice. Stults’s partner, Daisy Krigbaum, was addicted to Pinterest and YouTube, bingeing videos on her iPhone before going to sleep. Two years ago, they both tried Apple’s Screen Time restriction tool and found it too easy to disable, so the pair decided to trade out their iPhones for more low-tech devices. They’d heard about so-called dumbphones, which lacked the kinds of bells and whistles—a high-resolution screen, an app store, a video camera—that made smartphones so addictive. But they found the process of acquiring one hard to navigate. “The information on it was kind of disparate and hard to get to. A lot of people who know the most about dumbphones spend the least time online,” Krigbaum said. A certain irony presented itself: figuring out a way to be less online required aggressive online digging.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The growing dumbphone fervor may be motivated, in part, by the discourse around child safety online. Parents are increasingly confronted with evidence that sites like Instagram and TikTok intentionally try to hook their children. Using those sites can increase teens’ anxiety and lower their self-esteem, according to some studies, and smartphones make it so that kids are logged on constantly. Why should this situation be any healthier for adults? After almost two decades with iPhones, the public seems to be experiencing a collective ennui with digital life. So many hours of each day are lived through our portable, glowing screens, but the Internet isn’t even fun anymore. We lack the self-control to wean ourselves off, so we crave devices that actively prevent us from getting sucked into them. That means opting out of the prevailing technology and into what Cal Newport, a contributing writer for The New Yorker, has called a more considered “digital minimalism.”</p>\n</blockquote>\n<p>While dumbphones aren't a cure-all for unhealthy technology habits, as a dumbphone user, I can relate to the frustrations that come from the lack of device availability and support. Even when new devices hit the market, they tend to be targeted towards non-US markets.</p>\n</div>\n","tags":["dumbphone","technology","culture","digitalminimalism","minimalism"]},{"title":"Introducing Snowflake Arctic","contentType":"reshare","date":"2024-04-25 23:34","url":"/reshare/snowflake-arctic-enterprise-ai-llm/","content":"<div class=\"response-target\" >→ <a href=\"https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/\" >https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, the Snowflake AI Research Team is thrilled to introduce Snowflake Arctic, a top-tier enterprise-focused LLM that pushes the frontiers of cost-effective training and openness. Arctic is efficiently intelligent and truly open.<br />\n<br></p>\n<ul>\n<li>Efficiently Intelligent: Arctic excels at enterprise tasks such as SQL generation, coding and instruction following benchmarks even when compared to open source models trained with significantly higher compute budgets. In fact, it sets a new baseline for cost-effective training to enable Snowflake customers to create high-quality custom models for their enterprise needs at a low cost.<br />\n<br></li>\n<li>Truly Open: Apache 2.0 license provides ungated access to weights and code. In addition, we are also open sourcing all of our data recipes and research insights.<br />\n<br>\nSnowflake Arctic is available from Hugging Face, NVIDIA API catalog and Replicate today or via your model garden or catalog of choice, including Snowflake Cortex, Amazon Web Services (AWS), Microsoft Azure, Lamini, Perplexity and Together over the coming days.</li>\n</ul>\n</blockquote>\n</div>\n","tags":["snowflake","ai","llm","enterprise"]},{"title":"‘PBS Retro’ is coming to Roku as a FAST channel","contentType":"star","date":"2024-04-25 23:31","url":"/star/pbs-retro-roku-channel/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/4/24/24139057/pbs-retro-free-roku-channel-fast-streaming\" >https://www.theverge.com/2024/4/24/24139057/pbs-retro-free-roku-channel-fast-streaming</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>PBS is making child edutainment classics like Zoboomafoo, Mister Rogers’ Neighborhood, and Reading Rainbow available for free on a new ‘PBS Retro’ channel on Roku.</p>\n</blockquote>\n<p>This is cool! Although I'm kind of bummed the stuff I grew up watching is now considered &amp;quot;retro&amp;quot;.</p>\n</div>\n","tags":["pbs","roku","tv","fast","retro","education"]},{"title":"We Need To Rewild The Internet","contentType":"reshare","date":"2024-04-25 23:11","url":"/reshare/rewild-the-internet-farrell-berjon/","content":"<div class=\"response-target\" >→ <a href=\"https://www.noemamag.com/we-need-to-rewild-the-internet/\" >https://www.noemamag.com/we-need-to-rewild-the-internet/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The internet has become an extractive and fragile monoculture. But we can revitalize it using lessons learned by ecologists.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Our online spaces are not ecosystems, though tech firms love that word. They’re plantations; highly concentrated and controlled environments...<br />\n<br>\nWe all know this. We see it each time we reach for our phones. But what most people have missed is how this concentration reaches deep into the internet’s infrastructure — the pipes and protocols, cables and networks, search engines and browsers. These structures determine how we build and use the internet, now and in the future.<br />\n<br>\nThey’ve concentrated into a series of near-planetary duopolies.<br />\n<br>\nTwo kinds of everything may be enough to fill a fictional ark and repopulate a ruined world, but can’t run an open, global “network of networks” where everyone has the same chance to innovate and compete.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The internet made the tech giants possible. Their services have scaled globally, via its open, interoperable core. But for the past decade, they’ve also worked to enclose the varied, competing and often open-source or collectively provided services the internet is built on into their proprietary domains. Although this improves their operational efficiency, it also ensures that the flourishing conditions of their own emergence aren’t repeated by potential competitors. For tech giants, the long period of open internet evolution is over. Their internet is not an ecosystem. It’s a zoo.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Up close, internet concentration seems too intricate to untangle; from far away, it seems too difficult to deal with. But what if we thought of the internet not as a doomsday “hyperobject,” but as a damaged and struggling ecosystem facing destruction? What if we looked at it not with helpless horror at the eldritch encroachment of its current controllers, but with compassion, constructiveness and hope?</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Rewilding “aims to restore healthy ecosystems by creating wild, biodiverse spaces,” according to the International Union for Conservation of Nature. More ambitious and risk-tolerant than traditional conservation, it targets entire ecosystems to make space for complex food webs and the emergence of unexpected interspecies relations. It’s less interested in saving specific endangered species. Individual species are just ecosystem components, and focusing on components loses sight of the whole. Ecosystems flourish through multiple points of contact between their many elements, just like computer networks. And like in computer networks, ecosystem interactions are multifaceted and generative.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Whatever we do, the internet isn’t returning to old-school then-common interfaces like FTP and Gopher, or organizations operating their own mail servers again instead of off-the-shelf solutions like G-Suite. But some of what we need is already here, especially on the web. Look at the resurgence of RSS feeds, email newsletters and blogs, as we discover (yet again) that relying on one app to host global conversations creates a single point of failure and control. New systems are growing, like the Fediverse with its federated islands, or Bluesky with algorithmic choice and composable moderation.<br />\n<br>\nWe don’t know what the future holds. Our job is to keep open as much opportunity as we can, trusting that those who come later will use it. Instead of setting purity tests for which kind of internet is most like the original, we can test changes against the values of the original design. Do new standards protect the network’s “generality,” i.e. its ability to support multiple uses, or is functionality limited to optimize efficiency for the biggest tech firms?</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...our internet took off because it was designed as a general-purpose network, built to connect anyone.<br />\n<br>\nOur internet was built to be complex and unbiddable, to do things we cannot yet imagine.<br />\n<br>\nInternet infrastructure is a degraded ecosystem, but it’s also a built environment, like a city. Its unpredictability makes it generative, worthwhile and deeply human.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We need to stop thinking of internet infrastructure as too hard to fix. It’s the underlying system we use for nearly everything we do.<br />\n<br>\nRewilding the internet connects and grows what people are doing across regulation, standards-setting and new ways of organizing and building infrastructure, to tell a shared story of where we want to go. It’s a shared vision with many strategies. The instruments we need to shift away from extractive technological monocultures are at hand or ready to be built.</p>\n</blockquote>\n</div>\n","tags":["internet","technology","openweb","personalweb","online"]},{"title":"SAMMO: A general-purpose framework for prompt optimization","contentType":"reshare","date":"2024-04-25 22:48","url":"/reshare/sammo-prompt-optimization-framework/","content":"<div class=\"response-target\" >→ <a href=\"https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/\" >https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Large language models (LLMs) have revolutionized a wide range of tasks and applications that were previously reliant on manually crafted machine learning (ML) solutions, streamlining through automation. However, despite these advances, a notable challenge persists: the need for extensive prompt engineering to adapt these models to new tasks. New generations of language models like GPT-4 and Mixtral 8x7B advance the capability to process long input texts. This progress enables the use of longer inputs, providing richer context and detailed instructions to language models. A common technique that uses this enhanced capacity is the Retrieval Augmented Generation (RAG) approach. RAG dynamically incorporates information into the prompt based on the specific input example.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>To address these challenges, we developed the Structure-Aware Multi-objective Metaprompt Optimization (SAMMO) framework. SAMMO is a new open-source tool that streamlines the optimization of prompts, particularly those that combine different types of structural information like in the RAG example above. It can make structural changes, such as removing entire components or replacing them with different ones. These features enable AI practitioners and researchers to efficiently refine their prompts with little manual effort.<br />\n<br>\nCentral to SAMMO’s innovation is its approach to treating prompts not just as static text inputs but as dynamic, programmable entities—metaprompts. SAMMO represents these metaprompts as function graphs, where individual components and substructures can be modified to optimize performance, similar to the optimization process that occurs during traditional program compilation.<br />\n<br>\nThe following key features contribute to SAMMO’s effectiveness:<br />\n<br></p>\n<ul>\n<li>Structured optimization: Unlike current methods that focus on text-level changes, SAMMO focuses on optimizing the structure of metaprompts. This granular approach facilitates precise modifications and enables the straightforward integration of domain knowledge, for instance, through rewrite operations targeting specific stylistic objectives.<br />\n<br></li>\n<li>Multi-objective search: SAMMO’s flexibility enables it to simultaneously address multiple objectives, such as improving accuracy and computational efficiency. Our paper illustrates how SAMMO can be used to compress prompts without compromising their accuracy.<br />\n<br></li>\n<li>General purpose application: SAMMO has proven to deliver significant performance improvements across a variety of tasks, including instruction tuning, RAG, and prompt compression.</li>\n</ul>\n</blockquote>\n</div>\n","tags":["ai","microsoft","research","prompt","optimization","sammo"]},{"title":"Remembering the 90's Video Store","contentType":"reply","date":"2024-04-25 22:31","url":"/reply/remembering-90s-video-store-plummer/","content":"<div class=\"response-target\" >→ <a href=\"https://amerpie.lol/2024/04/21/in-the-prestreaming.html\" >https://amerpie.lol/2024/04/21/in-the-prestreaming.html</a></div><div class=\"response-content\" ><p>Reading this post and attached image brings back so many memories.</p>\n<p>I remember in the last few years of Blockbuster and early days of Netflix, for about $20/month you could rent unlimited movies (I think it was up to three at a time).</p>\n<p>If you binge watched them or weren't happy with the choices you made, you could just drive down to your local store, return them, and grab a new set of movies.</p>\n<p>Initially you had a return period, but towards the end, there were none so you basically got to keep some movies as long as you liked.</p>\n<p>Good times.</p>\n</div>\n","tags":["nostalgia","90s","videos","blockbuster"]},{"title":"Ghost is federating over ActivityPub","contentType":"star","date":"2024-04-25 22:11","url":"/star/ghost-activitypub-integration-announcement/","content":"<div class=\"response-target\" >→ <a href=\"https://activitypub.ghost.org/\" >https://activitypub.ghost.org/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In 2024, Ghost is adopting ActivityPub and connecting with other federated platforms across the web.<br />\n<br>\nThis means that, soon, Ghost publishers will be able to follow, like and interact with one another in the same way that you would normally do on a social network — but on your own website.<br />\n<br>\nThe difference, of course, is that you’ll also be able to follow, like, and interact with users on Mastodon, Threads, Flipboard, Buttondown, WriteFreely, Tumblr, WordPress, PeerTube, Pixelfed... or any other platform that has adopted ActivityPub, too. You don’t need to limit yourself to following people who happen to use the same platform as you.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>For the past few years the choice has been difficult. Either participate in closed networks at the mercy of algorithms, or set up an independent website at the expense of your growth.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Email gave us private messaging technology that isn’t owned by a single company.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>ActivityPub is doing the same for social technology.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The open web is coming back, and with it returns diversity. You can both publish independently and grow faster than ever before with followers from all over the world &amp;amp; the web.</p>\n</blockquote>\n<p>I can't express how much I love this. Personally I don't use Ghost, but given platforms like WordPress and now Ghost are adding support for ActivityPub, it empowers people to build their own platforms.</p>\n<p>That said, this still doesn't address the challenges of building your own website, which as mentioned in the post are one of the appealing aspects of current closed networks.</p>\n<p>Still though, there is a vast number of creators, businesses, and company websites or blogs that can benefit from this today. When paired with RSS, it gives people choice and autonomy in how they create and consume content as I mentioned in a previous post, <a href=\"/posts/rediscovering-rss-user-freedom\">Rediscovering the RSS protocol</a></p>\n</div>\n","tags":["ghost","openweb","activitypub","fediverse","blogging","websites","indieweb"]},{"title":"Google Penzai","contentType":"reshare","date":"2024-04-25 22:06","url":"/reshare/google-jax-penzai/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/google-deepmind/penzai\" >https://github.com/google-deepmind/penzai</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A JAX research toolkit for building, editing, and visualizing neural networks.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Penzai is a JAX library for writing models as legible, functional pytree data structures, along with tools for visualizing, modifying, and analyzing them. Penzai focuses on making it easy to do stuff with models after they have been trained, making it a great choice for research involving reverse-engineering or ablating model components, inspecting and probing internal activations, performing model surgery, debugging architectures, and more. (But if you just want to build and train a model, you can do that too!)</p>\n</blockquote>\n<p><img src=\"https://github.com/google-deepmind/penzai/raw/main/docs/_static/readme_teaser.png\" class=\"img-fluid\" alt=\"Screenshot of Google Penzai Neural Network Visualization\" /></p>\n</div>\n","tags":["google","jax","ai","visualization"]},{"title":"Introducing Phi-3","contentType":"reshare","date":"2024-04-23 22:47","url":"/reshare/introducing-phi-3/","content":"<div class=\"response-target\" >→ <a href=\"https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/\" >https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Starting today, Phi-3-mini, a 3.8B language model is available on Microsoft Azure AI Studio, Hugging Face, and Ollama.<br />\n<br></p>\n<ul>\n<li>Phi-3-mini is available in two context-length variants—4K and 128K tokens. It is the first model in its class to support a context window of up to 128K tokens, with little impact on quality.</li>\n<li>It is instruction-tuned, meaning that it’s trained to follow different types of instructions reflecting how people normally communicate. This ensures the model is ready to use out-of-the-box.</li>\n<li>It is available on Azure AI to take advantage of the deploy-eval-finetune toolchain, and is available on Ollama for developers to run locally on their laptops.</li>\n<li>It has been optimized for ONNX Runtime with support for Windows DirectML along with cross-platform support across graphics processing unit (GPU), CPU, and even mobile hardware.</li>\n<li>It is also available as an NVIDIA NIM microservice with a standard API interface that can be deployed anywhere. And has been optimized for NVIDIA GPUs.<br />\n<br>\nIn the coming weeks, additional models will be added to Phi-3 family to offer customers even more flexibility across the quality-cost curve. Phi-3-small (7B) and Phi-3-medium (14B) will be available in the Azure AI model catalog and other model gardens shortly.</li>\n</ul>\n</blockquote>\n</div>\n","tags":["microsoft","phi3","ai","slm","llm","genai"]},{"title":"Introducing Llama 3","contentType":"reshare","date":"2024-04-18 21:01","url":"/reshare/introducing-llama-3/","content":"<div class=\"response-target\" >→ <a href=\"https://llama.meta.com/llama3/\" >https://llama.meta.com/llama3/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Build the future of AI with Meta Llama 3</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Now available with both 8B and 70B pretrained and instruction-tuned versions to support a wide range of applications</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Llama 3 models take data and scale to new heights. It’s been trained on our two recently announced custom-built 24K GPU clusters on over 15T token of data – a training dataset 7x larger than that used for Llama 2, including 4x more code. This results in the most capable Llama model yet, which supports a 8K context length that doubles the capacity of Llama 2.</p>\n</blockquote>\n</div>\n","tags":["meta","ai","llama3","llama","llm"]},{"title":"Rock AlterLatino from Radio Bilingue","contentType":"reshare","date":"2024-04-14 15:18","url":"/reshare/radio-bilingue-rock-alterlatino/","content":"<div class=\"response-target\" >→ <a href=\"http://https://https://radiobilingue.org/rb-programas/alterlatino/\" >http://https://https://radiobilingue.org/rb-programas/alterlatino/</a></div><div class=\"response-content\" ><p>While listening to <a href=\"https://891khol.org/\">KHOL</a> earlier today, they were rebroadcasting a recording of <a href=\"http://https://https://radiobilingue.org/rb-programas/alterlatino/\">A Todo Pulmon</a>, a radio show from <a href=\"https://https://radiobilingue.org/\">Radio Bilingue</a> in Fresno, CA. Good stuff.</p>\n</div>\n","tags":["radio","music","rock","alternative","fresno","radiobilingue","khol"]},{"title":"Proton and Standard Notes are joining forces ","contentType":"reshare","date":"2024-04-10 22:35","url":"/reshare/proton-acquires-standard-notes/","content":"<div class=\"response-target\" >→ <a href=\"https://proton.me/blog/proton-standard-notes-join-forces\" >https://proton.me/blog/proton-standard-notes-join-forces</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...today, we’re happy to announce that Standard Notes will also join us to advance our shared mission.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Both Proton and Standard Notes share a strong commitment to our communities, so Standard Notes will remain open source, freely available, and fully supported. Prices are not changing, and if you have a current subscription to Standard Notes, it will continue to be honored. Proton aspires to do the right thing and be a responsible home for open-source projects, and just as we did with SimpleLogin, we are committed to preserving what makes Standard Notes special and much loved.<br />\n<br>\nIn the coming months, we hope to find ways to make Standard Notes more easily accessible to the Proton community. This way, in addition to protecting your email, calendar, files, passwords, and online activity, you can also protect your notes.</p>\n</blockquote>\n<p>This is another exciting acquisition! I mainly use org-mode in Emacs for note taking. However, I love the ecosystem Proton is building with their security and privacy focused set of collaborative software offerings.</p>\n</div>\n","tags":["proton","standardnotes","notetaking","security","privacy","email","notes"]},{"title":"RecurrentGemma - Open weights language model from Google DeepMind, based on Griffin.","contentType":"reshare","date":"2024-04-10 21:57","url":"/reshare/google-deepmind-recurrent-gemma/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/google-deepmind/recurrentgemma\" >https://github.com/google-deepmind/recurrentgemma</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>RecurrentGemma is a family of open-weights Language Models by Google DeepMind, based on the novel Griffin architecture. This architecture achieves fast inference when generating long sequences by replacing global attention with a mixture of local attention and linear recurrences.<br />\n<br>\nThis repository contains the model implementation and examples for sampling and fine-tuning. We recommend most users adopt the Flax implementation, which is highly optimized. We also provide an un-optimized PyTorch implementation for reference.</p>\n</blockquote>\n</div>\n","tags":["ai","gemma","google","llm","opensource","slm","griffin","neuralnetwork"]},{"title":"Hello OLMo: A truly open LLM","contentType":"reshare","date":"2024-04-09 22:17","url":"/reshare/allen-ai-olmo-llm/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e7359222\" >https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e7359222</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, The Allen Institute for AI (AI2) has released OLMo 7B, a truly open, state-of-the-art large language model released alongside the pre-training data and training code. This empowers researchers and developers to use the best and open models to advance the science of language models collectively.</p>\n</blockquote>\n<p><a href=\"https://huggingface.co/allenai/OLMo-7B\">OLMo-7B on HuggingFace</a></p>\n</div>\n","tags":["allenai","llm","opensource"]},{"title":"LLM training in simple, raw C/CUDA ","contentType":"reshare","date":"2024-04-09 22:15","url":"/reshare/llm-c-karpathy/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/karpathy/llm.c\" >https://github.com/karpathy/llm.c</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>LLM training in simple, pure C/CUDA. There is no need for 245MB of PyTorch or 107MB of cPython. For example, training GPT-2 (CPU, fp32) is ~1,000 lines of clean code in a single file. It compiles and runs instantly, and exactly matches the PyTorch reference implementation. I chose GPT-2 as the first working example because it is the grand-daddy of LLMs, the first time the modern stack was put together.</p>\n</blockquote>\n</div>\n","tags":["llm","gpt","c","programming","learning","tutorial"]},{"title":"Beeper was just acquired by Automattic","contentType":"reshare","date":"2024-04-09 22:06","url":"/reshare/beeper-acquired-automattic/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/4/9/24124179/beeper-app-automattic-acquisition-matrix-messaging\" >https://www.theverge.com/2024/4/9/24124179/beeper-app-automattic-acquisition-matrix-messaging</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Beeper, the upstart messaging app that attempts to corral all your messaging services into one inbox, is being acquired by Automattic, the giant that runs Wordpress.com, Tumblr, and a number of other hugely popular web properties</p>\n</blockquote>\n<p>This is exciting especially given some of the recent developments in the EU. What's most interesting to me is how Beeper leverages open protocols like Matrix and for bridging capabilities where possible to provide secure messaging.</p>\n<p>With more people moving to smaller spaces to communicate with their communities, being able to do so in a single place without everyone being on the same platform like in the early days of the internet is a welcome development.</p>\n<p>Additional coverage from the <a href=\"https://blog.beeper.com/2024/04/09/beeper-is-now-available/\">Beeper blog</a>.</p>\n<blockquote class=\"blockquote\">\n<p>What we’re announcing today…<br />\n<br></p>\n<ul>\n<li>No more waitlist – Beeper is now available to everyone!</li>\n<li>Beeper has been acquired by Automattic</li>\n<li>Our new Android app is out of beta</li>\n<li>We’re renaming Beeper Cloud → Beeper (sorry for the confusion)</li>\n</ul>\n</blockquote>\n<p>and <a href=\"https://ma.tt/2024/04/beeper-texts/\">Matt Mullenweg's blog</a></p>\n<blockquote class=\"blockquote\">\n<p>Today the announcement went out that we’re combining the best technology from Beeper and Texts to create a great private, secure, and open source messaging client for people to have control of their communications. We’re going to use the Beeper brand, because it’s fun. This is not unlike how browsers have evolved, where solid tech and encryption on top of an open ecosystem has created untold value for humanity.<br />\n<br>\nA lot of people are asking about iMessage on Android… I have zero interest in fighting with Apple, I think instead it’s best to focus on messaging networks that want more engagement from power-user clients. This is an area I’m excited to work on when I return from my sabbatical next month.</p>\n</blockquote>\n</div>\n","tags":["beeper","messaging","automattic","matrix"]},{"title":"ARAGOG: Advanced RAG Output Grading","contentType":"reshare","date":"2024-04-09 00:32","url":"/reshare/aragog-advanced-rag-output-grading/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2404.01037\" >https://arxiv.org/abs/2404.01037</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary Index as a competent retrieval approach. All resources related to this research are publicly accessible for further investigation through our GitHub repository ARAGOG (this https URL). We welcome the community to further this exploratory study in RAG systems.</p>\n</blockquote>\n<p><a href=\"https://github.com/predlico/ARAGOG\">GitHub repo</a></p>\n</div>\n","tags":["rag","ai","research","llm","knowledge","retrieval","retrievalaugmentedgeneration"]},{"title":"OpenAI - Introducing improvements to the fine-tuning API and expanding our custom models program","contentType":"reshare","date":"2024-04-04 22:58","url":"/reshare/openai-introducing-fine-tuning-improvements/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/blog/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program\" >https://openai.com/blog/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program</a></div><div class=\"response-content\" >\n<blockquote class=\"blockquote\">\n<p>Today, we’re introducing new features to give developers even more control over their fine-tuning jobs, including:<br />\n<br></p>\n<ul>\n<li>Epoch-based Checkpoint Creation: Automatically produce one full fine-tuned model checkpoint during each training epoch, which reduces the need for subsequent retraining, especially in the cases of overfitting</li>\n<li>Comparative Playground: A new side-by-side Playground UI for comparing model quality and performance, allowing human evaluation of the outputs of multiple models or fine-tune snapshots against a single prompt</li>\n<li>Third-party Integration: Support for integrations with third-party platforms (starting with Weights and Biases this week) to let developers share detailed fine-tuning data to the rest of their stack</li>\n<li>Comprehensive Validation Metrics: The ability to compute metrics like loss and accuracy over the entire validation dataset instead of a sampled batch, providing better insight on model quality</li>\n<li>Hyperparameter Configuration: The ability to configure available hyperparameters from the Dashboard (rather than only through the API or SDK)</li>\n<li>Fine-Tuning Dashboard Improvements: Including the ability to configure hyperparameters, view more detailed training metrics, and rerun jobs from previous configurations</li>\n</ul>\n</blockquote>\n\n<ul>\n<li>Assisted Fine-Tuning</li>\n</ul>\n<blockquote class=\"blockquote\">\n<p>Today, we are formally announcing our assisted fine-tuning offering as part of the Custom Model program. Assisted fine-tuning is a collaborative effort with our technical teams to leverage techniques beyond the fine-tuning API, such as additional hyperparameters and various parameter efficient fine-tuning (PEFT) methods at a larger scale. It’s particularly helpful for organizations that need support setting up efficient training data pipelines, evaluation systems, and bespoke parameters and methods to maximize model performance for their use case or task.</p>\n</blockquote>\n<ul>\n<li>Custom-Trained Model</li>\n</ul>\n<blockquote class=\"blockquote\">\n<p>In some cases, organizations need to train a purpose-built model from scratch that understands their business, industry, or domain. Fully custom-trained models imbue new knowledge from a specific domain by modifying key steps of the model training process using novel mid-training and post-training techniques. Organizations that see success with a fully custom-trained model often have large quantities of proprietary data—millions of examples or billions of tokens—that they want to use to teach the model new knowledge or complex, unique behaviors for highly specific use cases.</p>\n</blockquote>\n</div>\n","tags":["openai","finetuning","llm"]},{"title":"Introducing Command R+: A Scalable LLM Built for Business","contentType":"reshare","date":"2024-04-04 22:46","url":"/reshare/introducing-cohere-comandr-plus/","content":"<div class=\"response-target\" >→ <a href=\"https://txt.cohere.com/command-r-plus-microsoft-azure/\" >https://txt.cohere.com/command-r-plus-microsoft-azure/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Command R+ is a state-of-the-art RAG-optimized model designed to tackle enterprise-grade workloads, and is available first on Microsoft Azure</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Command R+, like our recently launched Command R model, features a 128k-token context window and is designed to offer best-in-class:<br />\n<br></p>\n<ul>\n<li>Advanced Retrieval Augmented Generation (RAG) with citation to reduce hallucinations</li>\n<li>Multilingual coverage in 10 key languages to support global business operations</li>\n<li>Tool Use to automate sophisticated business processes</li>\n</ul>\n</blockquote>\n</div>\n","tags":["cohere","llm","comandr","azure","comandrplus"]},{"title":"IPEX-LLM","contentType":"reshare","date":"2024-04-04 11:05","url":"/reshare/intel-ipex-llm/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/intel-analytics/ipex-llm\" >https://github.com/intel-analytics/ipex-llm</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency</p>\n</blockquote>\n</div>\n","tags":["intel","llm","pytorch","cpu","gpu"]},{"title":"Introducing Stable Audio 2.0","contentType":"reshare","date":"2024-04-04 11:01","url":"/reshare/introducing-stable-audio-2-0/","content":"<div class=\"response-target\" >→ <a href=\"https://stability.ai/news/stable-audio-2-0\" >https://stability.ai/news/stable-audio-2-0</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Stable Audio 2.0 sets a new standard in AI-generated audio, producing high-quality, full tracks with coherent musical structure up to three minutes in length at 44.1kHz stereo.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The new model introduces audio-to-audio generation by allowing users to upload and transform samples using natural language prompts.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Stable Audio 2.0 was exclusively trained on a licensed dataset from the AudioSparx music library, honoring opt-out requests and ensuring fair compensation for creators.</p>\n</blockquote>\n</div>\n","tags":["stabilityai","ai","audio","nlp"]},{"title":"The Matrix is coming back for a fifth movie","contentType":"reply","date":"2024-04-04 10:56","url":"/reply/matrix-five-announced/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/24120122/the-matrix-sequel-drew-goddard\" >https://www.theverge.com/24120122/the-matrix-sequel-drew-goddard</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Deadline reports that The Martian writer Drew Goddard has been tapped to pen and direct another Matrix movie executive produced by Lana Wachowski. Currently, the new film has no title or projected premiere date, and there’s been no announcement as to whether franchise stars like Keanu Reeves, Carrie-Anne Moss, Laurence Fishburne, Yahya Abdul-Mateen II, or Jessica Henwick will return.</p>\n</blockquote>\n<p>Not sure how to feel about this, but I'll end up watching anyway.</p>\n</div>\n","tags":["matrix","movie","scifi","neo","theone"]},{"title":"Google Podcasts is gone — and so is my faith in Google","contentType":"reply","date":"2024-04-02 22:54","url":"/reply/google-podcasts-gone-youtube-cant-replace-it-verge/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/4/2/24118873/google-podcasts-shutdown-graveyard\" >https://www.theverge.com/2024/4/2/24118873/google-podcasts-shutdown-graveyard</a></div><div class=\"response-content\" ><p>Good article. I felt this way when Google Reader and a few other services were shut down.</p>\n<p>That being said, this is kind of a good thing.</p>\n<blockquote class=\"blockquote\">\n<p>Luckily, there are plenty of good podcast apps out there, like Pocket Casts, Overcast, Antennapod, and even Apple Podcasts.</p>\n</blockquote>\n<p>This line basically says it all. Podcasts, like blogging, continue to be an open ecosystem and where the saying,&amp;quot;wherever you get your podcasts&amp;quot;, is still going strong.</p>\n</div>\n","tags":["google","podcasts","youtube","audio","opensource","rss"]},{"title":"Start using ChatGPT instantly","contentType":"reshare","date":"2024-04-01 20:44","url":"/reshare/start-using-chatgpt-seamlessly-no-signup/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/blog/start-using-chatgpt-instantly\" >https://openai.com/blog/start-using-chatgpt-instantly</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We’re making it easier for people to experience the benefits of AI without needing to sign up.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We may use what you provide to ChatGPT to improve our models for everyone. If you’d like, you can turn this off through your Settings - whether you create an account or not.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We’ve also introduced additional content safeguards for this experience, such as blocking prompts and generations in a wider range of categories.</p>\n</blockquote>\n</div>\n","tags":["openai","chatgpt","ai","llm"]},{"title":"T-Mobile Sidekick’s Jump button made mobile multitasking easy","contentType":"reshare","date":"2024-04-01 20:27","url":"/reshare/tmobile-sidekick-jump-button-mobile-productivity/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/24115039/danger-hiptop-t-mobile-sidekick-jump-button\" >https://www.theverge.com/24115039/danger-hiptop-t-mobile-sidekick-jump-button</a></div><div class=\"response-content\" ><p>Bring back the Sidekick! <a href=\"/notes/ayaneo-slide\">Ayaneo Slide</a> is probably the closest to this today. Would love to see a <a href=\"/responses/windows-12-mobile-concept/\">smaller version of it running on Windows on ARM-based Snapdragon processors</a>.</p>\n<blockquote class=\"blockquote\">\n<p>Before the iPhone, before Android, before webOS, a revolutionary soap bar of a phone made it incredibly easy to get shit done. The Danger Hiptop, better known as the T-Mobile Sidekick, made the internet portable and affordable like no phone before.</p>\n</blockquote>\n</div>\n","tags":["tmobile","nostalgia","sidekick","tmobile","phones","smartphones","productivity","mobile","cellphones","handhelds"]},{"title":"Announcing DBRX: A new standard for efficient open source LLMs","contentType":"reshare","date":"2024-03-31 20:26","url":"/reshare/announcing-dbrx-llm/","content":"<div class=\"response-target\" >→ <a href=\"https://www.databricks.com/blog/announcing-dbrx-new-standard-efficient-open-source-customizable-llms\" >https://www.databricks.com/blog/announcing-dbrx-new-standard-efficient-open-source-customizable-llms</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, we are excited to advance our mission by open sourcing DBRX, a general purpose large language model (LLM) built by our Mosaic Research team that outperforms all established open source models on standard benchmarks. We believe that pushing the boundary of open source models enables generative AI for all enterprises that is customizable and transparent.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We are excited about DBRX for three distinct reasons. First, it handily beats open source models, such as, LLaMA2-70B, Mixtral, and Grok-1 on language understanding, programming, math, and logic...<br />\n<br>\nSecond, DBRX beats GPT-3.5 on most benchmarks...<br />\n<br>\nThird, DBRX is a Mixture-of-Experts (MoE) model built on the MegaBlocks research and open source project, making the model extremely fast in terms of tokens/second.</p>\n</blockquote>\n</div>\n","tags":["databricks","ai","llm","opensource"]},{"title":"One-step Diffusion with Distribution Matching Distillation","contentType":"reshare","date":"2024-03-26 21:44","url":"/reshare/one-step-diffusion-distribution-matching-distillation/","content":"<div class=\"response-target\" >→ <a href=\"https://tianweiy.github.io/dmd/\" >https://tianweiy.github.io/dmd/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p><strong>Our one-step generator achieves comparable image quality with StableDiffusion v1.5 while being 30x faster.</strong><br />\n<br>\nDiffusion models are known to approximate the score function of the distribution they are trained on. In other words, an unrealistic synthetic image can be directed toward higher probability density region through the denoising process (see SDS). Our core idea is training two diffusion models to estimate not only the score function of the target real distribution, but also that of the fake distribution. We construct a gradient update to our generator as the difference between the two scores, essentially nudging the generated images toward higher realism as well as lower fakeness (see VSD). Our method is similar to GANs in that a critic is jointly trained with the generator to minimize a divergence between the real and fake distributions, but differs in that our training does not play an adversarial game that may cause training instability, and our critic can fully leverage the weights of a pretrained diffusion model. Combined with a simple regression loss to match the output of the multi-step diffusion model, our method outperforms all published few-step diffusion approaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shot COCO-30k, comparable to Stable Diffusion but orders of magnitude faster. Utilizing FP16 inference, our model generates images at 20 FPS on modern hardware.</p>\n</blockquote>\n<p><a href=\"https://arxiv.org/abs/2311.18828\">Paper</a></p>\n</div>\n","tags":["ai","genai","diffusion","dmd","stablediffusion","dalle"]},{"title":"404 Media Now Has a Full Text RSS Feed","contentType":"star","date":"2024-03-26 21:16","url":"/star/404-media-full-text-rss/","content":"<div class=\"response-target\" >→ <a href=\"https://www.404media.co/404-media-now-has-a-full-text-rss-feed/\" >https://www.404media.co/404-media-now-has-a-full-text-rss-feed/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We paid for the development of full text RSS feeds for Ghost-based publishers. Now we can offer them to our paid subscribers, and other Ghost sites can use the service too.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Our friends Anil Dash and Ernie Smith have recently written passionately and persuasively about the importance of RSS to the open web, and about how a technology that turns 25 years old this month remains both subversive and quite versatile. RSS-based distribution underpins a podcasting ecosystem that has allowed for shows to be distributed not just on Apple Podcasts but on Spotify, Google Podcasts, Pocket Casts, Overcast, and whatever other podcast player you might want to listen on. “Being able to say, ‘wherever you get your podcasts’ is a radical statement,” Dash wrote. “Because what it represents is the triumph of exactly the kind of technology that's supposed to be impossible: open, empowering tech that's not owned by any one company, that can't be controlled by any one company, and that allows people to have ownership over their work and their relationship with their audience.”</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>RSS has empowered podcasters, but that it needs a “creator economy rethink” for text.</p>\n</blockquote>\n</div>\n","tags":["rss","404media","ghost","404","media","protocols","opensource","technology"]},{"title":"Stability CEO Resigns","contentType":"reshare","date":"2024-03-23 11:49","url":"/reshare/stability-ai-ceo-resigns/","content":"<div class=\"response-target\" >→ <a href=\"https://stability.ai/news/stabilityai-announcement\" >https://stability.ai/news/stabilityai-announcement</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Earlier today, Emad Mostaque resigned from his role as CEO of Stability AI and from his position on the Board of Directors of the company to pursue decentralized AI.</p>\n</blockquote>\n</div>\n","tags":["stability","ai","news"]},{"title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces","contentType":"reshare","date":"2024-03-21 20:44","url":"/reshare/mamba-linear-time-sequence-selective-state-spaces/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2312.00752\" >https://arxiv.org/abs/2312.00752</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5× higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.</p>\n</blockquote>\n</div>\n","tags":["ai","models","mamba","llm","compute"]},{"title":"The Online Local Chronicle","contentType":"reply","date":"2024-03-19 22:17","url":"/reply/online-local-chronicle-searls/","content":"<div class=\"response-target\" >→ <a href=\"https://doc.searls.com/2024/03/19/the-online-local-chronicle/\" >https://doc.searls.com/2024/03/19/the-online-local-chronicle/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In the same way that every little place in America used to have a printed newspaper, every little place in America could have an online local chronicle.<br />\n<br>\nBroadly speaking, an online local chronicle is a collection of facts organized mostly in chronological order. The “pages” of the chronicle can be thought of as subsets of a community’s universal timeline of events. These online local chronicles could become the backbone of local news operations.</p>\n</blockquote>\n<p>Nice project. Unfortunately, it's rare you get local news. I like publications / websites like <a href=\"https://www.hobokengirl.com/\">Hoboken Girl</a> and <a href=\"https://blockclubchicago.org/\">Block Club Chicago</a>. I wish there were more of them in more cities and towns. I know they're there in some forms like Facebook Groups. Even better, it'd be great to have the websites for these publications be the main source of truth that then syndicated their content to the various platforms out there.</p>\n</div>\n","tags":["community","news","local","knowledgebase","information","web","opensource"]},{"title":"Quanto: a PyTorch quantization toolkit ","contentType":"reshare","date":"2024-03-19 22:00","url":"/reshare/quanto-pytorch-quantization-toolkit/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/blog/quanto-introduction\" >https://huggingface.co/blog/quanto-introduction</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Quantization is a technique to reduce the computational and memory costs of evaluating Deep Learning Models by representing their weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Today, we are excited to introduce quanto, a versatile pytorch quantization toolkit, that provides several unique features:<br />\n<br></p>\n<ul>\n<li>available in eager mode (works with non-traceable models)</li>\n<li>quantized models can be placed on any device (including CUDA and MPS),</li>\n<li>automatically inserts quantization and dequantization stubs,</li>\n<li>automatically inserts quantized functional operations,</li>\n<li>automatically inserts quantized modules (see below the list of supported modules),</li>\n<li>provides a seamless workflow for a float model, going from a dynamic to a static quantized model,</li>\n<li>supports quantized model serialization as a state_dict,</li>\n<li>supports not only int8 weights, but also int2 and int4,</li>\n<li>supports not only int8 activations, but also float8.</li>\n</ul>\n</blockquote>\n</div>\n","tags":["huggingface","quantization","pytorch","tools","ai"]},{"title":"Demystifying Embedding Spaces using Large Language Models","contentType":"reshare","date":"2024-03-19 21:39","url":"/reshare/demistifying-embedding-spaces-llms/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2310.04475\" >https://arxiv.org/abs/2310.04475</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing Large Language Models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decoding user preferences in recommender systems. Our work couples the immense information potential of embeddings with the interpretative power of LLMs.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","embedding","interpretability"]},{"title":"The Tokenizer Playground","contentType":"reshare","date":"2024-03-18 22:35","url":"/reshare/tokenizer-playground/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/spaces/Xenova/the-tokenizer-playground\" >https://huggingface.co/spaces/Xenova/the-tokenizer-playground</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Experiment with different tokenizers (running locally in your browser).\nI really love doing this thing</p>\n</blockquote>\n</div>\n","tags":["ai","tokenizer","huggingface","playground","aiplayground"]},{"title":"Introducing Stable Video 3D: Quality Novel View Synthesis and 3D Generation from Single Images","contentType":"reshare","date":"2024-03-18 21:29","url":"/reshare/introducing-stable-video-3d/","content":"<div class=\"response-target\" >→ <a href=\"https://stability.ai/news/introducing-stable-video-3d\" >https://stability.ai/news/introducing-stable-video-3d</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today we are releasing Stable Video 3D (SV3D), a generative model based on Stable Video Diffusion, advancing the field of 3D technology and delivering greatly improved quality and view-consistency.<br />\n<br>\nThis release features two variants: SV3D_u and SV3D_p. SV3D_u generates orbital videos based on single image inputs without camera conditioning. SV3D_p extends the capability by accommodating both single images and orbital views, allowing for the creation of 3D video along specified camera paths.<br />\n<br>\nStable Video 3D can be used now for commercial purposes with a Stability AI Membership. For non-commercial use, you can download the model weights on Hugging Face and view our research paper here.</p>\n</blockquote>\n</div>\n","tags":["stabilityai","ai","video","3d","computervision"]},{"title":"Nvidia reveals Blackwell B200 GPU","contentType":"reshare","date":"2024-03-18 21:09","url":"/reshare/nvidia-blackwell-b200-gpu/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/3/18/24105157/nvidia-blackwell-gpu-b200-ai\" >https://www.theverge.com/2024/3/18/24105157/nvidia-blackwell-gpu-b200-ai</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Nvidia reveals Blackwell B200 GPU, the ‘world’s most powerful chip’ for AI</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>‘Built to democratize trillion-parameter AI.’</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Nvidia says the new B200 GPU offers up to 20 petaflops of FP4 horsepower from its 208 billion transistors. Also, it says, a GB200 that combines two of those GPUs with a single Grace CPU can offer 30 times the performance for LLM inference workloads while also potentially being substantially more efficient. It “reduces cost and energy consumption by up to 25x” over an H100, says Nvidia.</p>\n</blockquote>\n</div>\n","tags":["ai","nvidia","gpu","hardware","pchardware","pc"]},{"title":"Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs","contentType":"reshare","date":"2024-03-17 21:29","url":"/reshare/enhace-rag-application-accuracy-knowledge-graphs/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/\" >https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A practical guide to constructing and retrieving information from knowledge graphs in RAG applications with Neo4j and LangChain<br />\n<br>\nGraph retrieval augmented generation (Graph RAG) is gaining momentum and emerging as a powerful addition to traditional vector search retrieval methods. This approach leverages the structured nature of graph databases, which organize data as nodes and relationships, to enhance the depth and contextuality of retrieved information.</p>\n</blockquote>\n</div>\n","tags":["knowledgegraph","rag","genai","langchain","generativeai","retrievalaugmentedgeneration","patterns","applicationpatterns"]},{"title":"LaVague - Large Action Model framework","contentType":"reshare","date":"2024-03-17 21:22","url":"/reshare/lavague-large-action-model/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/lavague-ai/LaVague\" >https://github.com/lavague-ai/LaVague</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Redefining internet surfing by transforming natural language instructions into seamless browser interactions.</p>\n</blockquote>\n</div>\n","tags":["ai","genai","lavague","selenium","web","lam","largeactionmodel"]},{"title":"Spreadsheets are all you need","contentType":"reshare","date":"2024-03-17 21:18","url":"/reshare/spreadsheets-are-all-you-need/","content":"<div class=\"response-target\" >→ <a href=\"https://spreadsheets-are-all-you-need.ai/index.html\" >https://spreadsheets-are-all-you-need.ai/index.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A low-code way to learn AI - Learn how AI works from a real LLM implemented entirely in Excel</p>\n</blockquote>\n</div>\n","tags":["excel","tutorial","ai","genai","tokenizers","transformers"]},{"title":"Building Meta’s GenAI Infrastructure","contentType":"reshare","date":"2024-03-17 21:14","url":"/reshare/building-meta-genai-infrastructure/","content":"<div class=\"response-target\" >→ <a href=\"https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/\" >https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<ul>\n<li>Marking a major investment in Meta’s AI future, we are announcing two 24k GPU clusters. We are sharing details on the hardware, network, storage, design, performance, and software that help us extract high throughput and reliability for various AI workloads. We use this cluster design for Llama 3 training.</li>\n<li>We are strongly committed to open compute and open source. We built these clusters on top of Grand Teton, OpenRack, and PyTorch and continue to push open innovation across the industry.</li>\n<li>This announcement is one step in our ambitious infrastructure roadmap. By the end of 2024, we’re aiming to continue to grow our infrastructure build-out that will include 350,000 NVIDIA H100 GPUs as part of a portfolio that will feature compute power equivalent to nearly 600,000 H100s.</li>\n</ul>\n</blockquote>\n</div>\n","tags":["meta","ai","infrastraucture","generativeai","genai","facebook"]},{"title":"OpenAI Transformer Debugger","contentType":"reshare","date":"2024-03-17 21:05","url":"/reshare/openai-transformer-debugger/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/openai/transformer-debugger\" >https://github.com/openai/transformer-debugger</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Transformer Debugger (TDB) is a tool developed by OpenAI's Superalignment team with the goal of supporting investigations into specific behaviors of small language models. The tool combines automated interpretability techniques with sparse autoencoders.<br />\n<br>\nTDB enables rapid exploration before needing to write code, with the ability to intervene in the forward pass and see how it affects a particular behavior. It can be used to answer questions like, &amp;quot;Why does the model output token A instead of token B for this prompt?&amp;quot; or &amp;quot;Why does attention head H attend to token T for this prompt?&amp;quot; It does so by identifying specific components (neurons, attention heads, autoencoder latents) that contribute to the behavior, showing automatically generated explanations of what causes those components to activate most strongly, and tracing connections between components to help discover circuits.</p>\n</blockquote>\n</div>\n","tags":["openai","ai","transformer","debugger","tools"]},{"title":"Grok-1","contentType":"reshare","date":"2024-03-17 20:45","url":"/reshare/grok-1-open-release/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/xai-org/grok-1\" >https://github.com/xai-org/grok-1</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This repository contains JAX example code for loading and running the Grok-1 open-weights model.</p>\n</blockquote>\n</div>\n","tags":["grok","ai","x","github","opensource","moe"]},{"title":"Ollama now supports AMD graphics cards","contentType":"reshare","date":"2024-03-17 20:43","url":"/reshare/ollama-supports-amd-gpus/","content":"<div class=\"response-target\" >→ <a href=\"https://ollama.com/blog/amd-preview\" >https://ollama.com/blog/amd-preview</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated by AMD graphics cards on Ollama for Linux and Windows.</p>\n</blockquote>\n</div>\n","tags":["ollama","gpu","amd","ai","llm","opensource"]},{"title":"Apple Vision Pro Perspectives - Hugo Barra","contentType":"reshare","date":"2024-03-15 21:12","url":"/reshare/apple-vision-pro-barra/","content":"<div class=\"response-target\" >→ <a href=\"https://hugo.blog/2024/03/11/vision-pro/\" >https://hugo.blog/2024/03/11/vision-pro/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Friends and colleagues have been asking me to share my perspective on the Apple Vision Pro as a product.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This started as blog post and became an essay before too long, so I’ve structured my writing in multiple sections each with a clear lead to make it a bit easier to digest — peppered with my own ‘takes’. I’ve tried to stick to original thoughts for the most part and link to what others have said where applicable.<br />\n<br>\nSome of the topics I touch on:<br />\n<br></p>\n<ul>\n<li>Why I believe Vision Pro may be an over-engineered “devkit”</li>\n<li>The genius &amp;amp; audacity behind some of Apple’s hardware decisions</li>\n<li>Gaze &amp;amp; pinch is an incredible UI superpower and major industry ah-ha moment</li>\n<li>Why the Vision Pro software/content story is so dull and unimaginative</li>\n<li>Why most people won’t use Vision Pro for watching TV/movies</li>\n<li>Apple’s bet in immersive video is a total game-changer for Live Sports</li>\n<li>Why I returned my Vision Pro… and my Top 10 wishlist to reconsider</li>\n<li>Apple’s VR debut is the best thing that ever happened to Oculus/Meta</li>\n<li>My unsolicited product advice to Meta for Quest Pro 2 and beyond</li>\n</ul>\n</blockquote>\n</div>\n","tags":["apple","visionpro","vr","ar"]},{"title":"Bluesky’s Stackable Approach to Moderation","contentType":"reshare","date":"2024-03-15 20:53","url":"/reshare/bluesky-stackable-approach-moderation/","content":"<div class=\"response-target\" >→ <a href=\"https://bsky.social/about/blog/03-12-2024-stackable-moderation\" >https://bsky.social/about/blog/03-12-2024-stackable-moderation</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Bluesky was created to put users and communities in control of their social spaces online. The first generation of social media platforms connected the world, but ended up consolidating power in the hands of a few corporations and their leaders. Our online experience doesn’t have to depend on billionaires unilaterally making decisions over what we see. On an open social network like Bluesky, you can shape your experience for yourself.<br />\n<br>\nToday, we’re excited to announce that we’re <a href=\"https://github.com/bluesky-social/ozone\">open-sourcing Ozone</a>, our collaborative moderation tool. With Ozone, individuals and teams can work together to review and label content across the network. Later this week, we’re opening up the ability for you to run your own independent moderation services, seamlessly integrated into the Bluesky app. This means that you'll be able to create and subscribe to additional moderation services on top of what Bluesky requires, giving you unprecedented control over your social media experience.</p>\n</blockquote>\n</div>\n","tags":["moderation","community","bluesky","atprotocol","socialmedia","internet","web"]},{"title":"Introducing the Proton Mail desktop app ","contentType":"reshare","date":"2024-03-15 20:06","url":"/reshare/introducing-protonmail-desktop-app/","content":"<div class=\"response-target\" >→ <a href=\"https://proton.me/blog/proton-mail-desktop-app\" >https://proton.me/blog/proton-mail-desktop-app</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, we’re excited to broaden the horizons of secure communication by launching the Proton Mail desktop app. Anyone can now use the new Proton Mail desktop app for Windows and macOS, with a Linux version in beta.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>With the new Proton Mail desktop apps, you get a dedicated email experience, allowing you to enjoy all the productivity innovations of our web app, allowing you to go through your emails and events faster without the potential distractions that pop up anytime you open your browser. And, of course, your privacy remains protected at all times.</p>\n</blockquote>\n</div>\n","tags":["email","proton","desktop","app","privacy","security"]},{"title":"What I learned from looking at 900 most popular open source AI tools","contentType":"reshare","date":"2024-03-14 22:13","url":"/reshare/learned-from-looking-900-oss-ai-tools-huyen/","content":"<div class=\"response-target\" >→ <a href=\"https://huyenchip.com//2024/03/14/ai-oss.html\" >https://huyenchip.com//2024/03/14/ai-oss.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>So many cool ideas are being developed by the community. Here are some of my favorites.<br />\n<br></p>\n<ul>\n<li>Batch inference optimization: FlexGen, llama.cpp</li>\n<li>Faster decoder with techniques such as Medusa, LookaheadDecoding</li>\n<li>Model merging: mergekit</li>\n<li>Constrained sampling: outlines, guidance, SGLang</li>\n<li>Seemingly niche tools that solve one problem really well, such as einops and safetensors.</li>\n</ul>\n</blockquote>\n</div>\n","tags":["ai","tools","analysis","openai","github"]},{"title":"You can now train a 70b language model at home","contentType":"reshare","date":"2024-03-08 12:13","url":"/reshare/answer-ai-train-70b-llm-at-home/","content":"<div class=\"response-target\" >→ <a href=\"https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html\" >https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, we’re releasing Answer.AI’s first project: a fully open source system that, for the first time, can efficiently train a 70b large language model on a regular desktop computer with two or more standard gaming GPUs (RTX 3090 or 4090). This system, which combines FSDP and QLoRA, is the result of a collaboration between Answer.AI, Tim Dettmers (U Washington), and Hugging Face’s Titus von Koeller and Sourab Mangrulkar.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","qlora","gpu","nvidia","finetuning"]},{"title":"Levels of Complexity: RAG Applications","contentType":"reshare","date":"2024-03-07 22:08","url":"/reshare/levels-of-complexity-rag-applications/","content":"<div class=\"response-target\" >→ <a href=\"https://jxnl.github.io/blog/writing/2024/02/28/levels-of-complexity-rag-applications/\" >https://jxnl.github.io/blog/writing/2024/02/28/levels-of-complexity-rag-applications/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This post comprehensive guide to understanding and implementing RAG applications across different levels of complexity. Whether you're a beginner eager to learn the basics or an experienced developer looking to deepen your expertise, you'll find valuable insights and practical knowledge to help you on your journey. Let's embark on this exciting exploration together and unlock the full potential of RAG applications.</p>\n</blockquote>\n</div>\n","tags":["rag","ai","llm","architecture","app"]},{"title":"5 reasons why desktop Linux is finally growing in popularity","contentType":"reshare","date":"2024-03-07 21:27","url":"/reshare/year-of-linux-desktop-4-percent/","content":"<div class=\"response-target\" >→ <a href=\"https://www.zdnet.com/article/5-reasons-why-desktop-linux-is-finally-growing-in-popularity/\" >https://www.zdnet.com/article/5-reasons-why-desktop-linux-is-finally-growing-in-popularity/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>StatCounter reported that desktop Linux reached over 4% market share for the first time.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Why is Linux finally growing?<br />\n<br>\nWhile Windows is the king of the hill with 72.13% and MacOS comes in a distant second at 15.46%, it's clear that Linux is making progress.<br />\n<br></p>\n<ol>\n<li>Microsoft isn't that interested in Windows</li>\n<li>Linux gaming, thanks to Steam, is also growing</li>\n<li>Users are finally figuring out that some Linux distros are easy to use</li>\n<li>Finding and installing Linux desktop software is easier than ever</li>\n<li>The Linux desktop is growing in popularity in India</li>\n</ol>\n</blockquote>\n</div>\n","tags":["linux","pc","desktop","desktop"]},{"title":"Inflection-2.5: meet the world's best personal AI","contentType":"reshare","date":"2024-03-07 21:22","url":"/reshare/inflection-2-5/","content":"<div class=\"response-target\" >→ <a href=\"https://inflection.ai/inflection-2-5\" >https://inflection.ai/inflection-2-5</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>At Inflection, our mission is to create a personal AI for everyone. Last May, we released Pi—a personal AI, designed to be empathetic, helpful, and safe. In November we announced a new major foundation model, Inflection-2, the second best LLM in the world at the time.<br />\n<br>\nNow we are adding IQ to Pi’s exceptional EQ.<br />\n<br>\nWe are launching Inflection-2.5, our upgraded in-house model that is competitive with all the world's leading LLMs like GPT-4 and Gemini. It couples raw capability with our signature personality and unique empathetic fine-tuning. Inflection-2.5 is available to all Pi's users today, at pi.ai, on iOS, on Android, or our new desktop app.<br />\n<br>\nWe achieved this milestone with incredible efficiency: Inflection-2.5 approaches GPT-4’s performance, but used only 40% of the amount of compute for training.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","inflection"]},{"title":"The surprising connection between after-hours work and decreased productivity","contentType":"reshare","date":"2024-03-07 21:14","url":"/reshare/surprising-connection-after-hours-work-productivity-slack/","content":"<div class=\"response-target\" >→ <a href=\"https://slack.com/blog/news/the-surprising-connection-between-after-hours-work-and-decreased-productivity\" >https://slack.com/blog/news/the-surprising-connection-between-after-hours-work-and-decreased-productivity</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Quick take: How do you spend your time at work and what is it costing you? Slack’s Workforce Index, based on survey responses from more than 10,000 desk workers around the globe, uncovers new findings on how to structure the workday to maximize productivity and strengthen employee well-being and satisfaction.<br />\n<br>\nKey learnings include:<br />\n<br></p>\n<ul>\n<li>Employees who log off at the end of the workday register 20% higher productivity scores than those who feel obligated to work after hours.</li>\n<li>Making time for breaks during the workday improves employee productivity and well-being, and yet half of all desk workers say they rarely or never take breaks.</li>\n<li>On average, desk workers say that the ideal amount of focus time is around four hours a day, and more than two hours a day in meetings is the tipping point at which a majority of workers feel overburdened by meetings.</li>\n<li>Three out of every four desk workers report working in the 3 to 6pm timeframe, but of those, only one in four consider these hours highly productive.</li>\n</ul>\n</blockquote>\n</div>\n","tags":["slack","productivity","work"]},{"title":"Training great LLMs entirely from ground up in the wilderness as a startup","contentType":"reshare","date":"2024-03-07 21:10","url":"/reshare/training-llms-ground-up-wilderness-startup/","content":"<div class=\"response-target\" >→ <a href=\"https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness\" >https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Given that we’ve successfully trained pretty strong multimodal language models at Reka, many people have been particularly curious about the experiences of building infrastructure and training large language &amp;amp; multimodal models from scratch from a completely clean slate.<br />\n<br>\nI complain a lot about external (outside Google) infrastructure and code on my social media, leading people to really be curious about what are the things I miss and what I hate/love in the wilderness. So here’s a post (finally). This blogpost sheds light on the challenges and lessons learned.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Figuring out things in the wilderness was an interesting experience. It was unfortunately not painless. Compute scarcity and also unreliable compute providers made things significantly harder than expected but we’re glad we pulled through with brute technical strength.<br />\n<br>\nAll in all, this is only a small part of the story of how we started a company, raised some money, bought some chips and matched Gemini pro/GPT 3.5 and outperformed many others in less than a year having to build everything from scratch.</p>\n</blockquote>\n</div>\n","tags":["llm","ai","startup","compute","gpu","training"]},{"title":"The missing graph datatype already exists. It was invented in the '70s","contentType":"reshare","date":"2024-03-06 21:39","url":"/reshare/datalog-the-missing-graph-data-type-already-exists/","content":"<div class=\"response-target\" >→ <a href=\"https://tylerhou.com/posts/datalog-go-brrr/\" >https://tylerhou.com/posts/datalog-go-brrr/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The datatype for a graph is a relation, and graph algorithms are queries on the relation. But modern languages need better support for the relational model.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This post is a response to/inspired by <a href=\"https://www.hillelwayne.com/post/graph-types/\">The Hunt for the Missing Data Type (HN) by Hillel Wayne</a>. I suggest reading his article first.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I claim the reason why it is so difficult to support graphs in languages nowadays is because the imperative/structured programming model of modern programming languages is ill-suited for graph algorithms. As Wayne correctly points out, the core problem is that when you write a graph algorithm in an imperative language like Python or Rust, you have to choose some explicit representation for the graph. Then, your traversal algorithm is dependent on the representation you chose. If you find out later that your representation is no longer efficient, it is a lot of work to adapt your algorithms for a new representation.<br />\n<br>\nSo what if we just, like, didn’t do this?<br />\n<br>\nWe already have a declarative programming language where expressing graph algorithms is extremely natural—Datalog, whose semantics are based on* the relational algebra, which was developed in the 1970s.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Wonderful! Except for the “writing Datalog” part.<br />\n<br>\nIf Datalog is so great, why hasn’t it seen more adoption?<br />\n<br>\nThe short answer is that Datalog is relatively esoteric outside of academia and some industry applications and, as a result, is not a great language from a “software engineering” perspective. It is hard for programmers accustomed to imperative code to write Datalog programs, and large Datalog programs can be hard to write and understand.</p>\n</blockquote>\n</div>\n","tags":["datalog","programming","graphs","datatypes","programminglanguages","software","database","algorithms"]},{"title":"Apple Podcasts now includes transcripts","contentType":"reshare","date":"2024-03-05 20:52","url":"/reshare/apple-podcasts-transcript-17-4/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/3/5/24091555/apple-podcasts-transcripts-ios-17-4-update\" >https://www.theverge.com/2024/3/5/24091555/apple-podcasts-transcripts-ios-17-4-update</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Apple Podcasts will auto-generate transcripts for podcasts beginning today, thanks to the 17.4 update for iPhones and iPads. Transcripts will automatically appear for new podcast episodes shortly after their publication, while Apple will transcribe podcast back catalogs over time.\n<br>\nThe podcast transcripts are searchable, allowing users to type in a specific word or phrase and skip to that part of an episode. Users can find transcripts for individual podcast episodes on the bottom-left corner of the “Now Playing” screen.\n<br>\nPodcasters who don’t want to use Apple’s automated transcription can opt to upload their own transcripts via RSS tags or in Apple Podcasts Connect for premium episodes, or they can download and edit Apple’s transcript before reuploading.</p>\n</blockquote>\n<p>This is cool and great for accessibility.</p>\n<p>I recently chose to <a href=\"/responses/decoder-nilay-patel-why-websites-are-the-future/\">read the latest episode of Decoder</a> instead of listening to it. One of the advantages this also provided was that I could reference direct quotes in my post from the episode.</p>\n<p>I could see Apple taking this further by making it easier to generate show notes / descriptions based on the episode using AI.</p>\n</div>\n","tags":["iphone","apple","podcasts","ios","ipados"]},{"title":"Microsoft to end its Android apps on Windows 11 subsystem in 2025","contentType":"reshare","date":"2024-03-05 20:47","url":"/reshare/windows-subsystem-for-android-end-of-support-2025/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/3/5/24091370/microsoft-windows-11-android-apps-end-of-support\" >https://www.theverge.com/2024/3/5/24091370/microsoft-windows-11-android-apps-end-of-support</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Microsoft is ending support for its Android subsystem in Windows 11 next year. The software giant first announced it was bringing Android apps to Windows 11 with Amazon’s Appstore nearly three years ago, but this Windows Subsystem for Android will now be deprecated starting March 5th, 2025.</p>\n</blockquote>\n<p>That's unfortunate considering the new lineup of ARM-based PCs expected later this year. It would've been nice to have a <a href=\"/responses/windows-12-mobile-concept/\">mobile PC with 5G support</a> that could run mobile apps for scenarios where there are no web / native PC apps.</p>\n</div>\n","tags":["windows","android","wsa","pc"]},{"title":"Stable Diffusion 3: Research Paper","contentType":"reshare","date":"2024-03-05 12:55","url":"/reshare/stable-diffusion-3-research-paper/","content":"<div class=\"response-target\" >→ <a href=\"https://stability.ai/news/stable-diffusion-3-research-paper\" >https://stability.ai/news/stable-diffusion-3-research-paper</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Key Takeaways:<br />\n<br>\nToday, we’re publishing our research paper that dives into the underlying technology powering Stable Diffusion 3.<br />\n<br>\nStable Diffusion 3 outperforms state-of-the-art text-to-image generation systems such as DALL·E 3, Midjourney v6, and Ideogram v1 in typography and prompt adherence, based on human preference evaluations.<br />\n<br>\nOur new Multimodal Diffusion Transformer (MMDiT) architecture uses separate sets of weights for image and language representations, which improves text understanding and spelling capabilities compared to previous versions of SD3.</p>\n</blockquote>\n</div>\n","tags":["stabilityai","stablediffusion","ai","generativeai","genai","images"]},{"title":"Wix’s new AI chatbot builds websites in seconds based on prompts","contentType":"reshare","date":"2024-03-04 23:15","url":"/reshare/wix-website-builder-ai-chatbot/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/3/4/24090095/wix-ai-website-generator-chatbot\" >https://www.theverge.com/2024/3/4/24090095/wix-ai-website-generator-chatbot</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>You can now build a website, images and all, using only prompts in Wix’s new AI website builder. Creating a website is free, but you’ll have to upgrade to one of Wix’s premium plans if you want to do things like accept payments or don’t want to be limited to using a Wix domain name.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>You’d probably need to delve into Wix’s advanced editing features and know things about actual web development for that. But it was very easy to use the basic AI generator to create something that looks close to a legitimate site to start with, making it much easier to get to a basic starting point.</p>\n</blockquote>\n</div>\n","tags":["wix","ai","websites","internet","web"]},{"title":"Today’s smart homes: the hopes and the realities","contentType":"reshare","date":"2024-03-04 23:13","url":"/reshare/todays-smart-homes-hopes-realities-verge/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/24080426/smart-home-tech-matter-pets-kitchen-hubs-how-to\" >https://www.theverge.com/24080426/smart-home-tech-matter-pets-kitchen-hubs-how-to</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The Verge team and others share their experiences of how smart technologies affect their lives — how it can often help and sometimes frustrate.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In these articles, we’ve concentrated on how our own experiences, and the experiences of others, have affected how we regard smart home tech. We’ve got personal accounts by one reporter who decided to put together a brand-new smart home and another whose brother moved into a home haunted by the ghosts of someone else’s smart tech. Several of our staffers wax enthusiastically about their favorite devices and automations. A writer describes how smart tech makes his home more accessible. Our smart home reviewer tells how she uses technology to keep her varied pets (and she has a lot of them) happy and healthy. We talk to people who use smart devices to help them care for their parents — and more.</p>\n</blockquote>\n</div>\n","tags":["iot","verge","smarthome","technology"]},{"title":"Guest host Hank Green makes Nilay Patel explain why websites have a future","contentType":"reshare","date":"2024-03-04 22:45","url":"/reshare/decoder-nilay-patel-why-websites-are-the-future/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/24087834/hank-green-decoder-podcast-google-youtube-web-media-platforms-distribution-future\" >https://www.theverge.com/24087834/hank-green-decoder-podcast-google-youtube-web-media-platforms-distribution-future</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...the last platform on the web of any scale or influence is Google Search. And so, over time, webpages have become dramatically optimized for Google Search. And that means the kinds of things people write about, the containers that we write in, are mostly designed to be optimized for Google Search. They’re not designed for, “I need to just quickly tell you about this and move on.” Our little insight was, “Well, what if we just don’t do that? What if we only write for the people who come directly to our website instead of the people who find our articles through Search or Google Discover or whatever other Google platforms are in the world?” And so we just made these little blog posts, and the idea was, if you just come to our website one more time a day because there’s one more thing to look at that you’ll like, we will be fine.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>more and more people are starting to realize, “Oh, we should just make the websites more valuable.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...if you start writing for other people, which is the heart of what a blog post really is: it’s you trying to entertain yourself and trying to entertain just a handful of other people, you’re going to go really much farther than trying to satisfy the robot.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Why am I writing in the text box that pays money to Elon and Mark [Zuckerberg] and not my text box?</strong><br />\n<br>\nWhy do we all work for free? Look, we want to talk about the platform era and media. Why do we all work for free?<br />\n<br>\n...It’s very confusing, and there are a lot of reasons. If you just sit back and think about why, there are a million reasons why.<br />\n<br>\nOne, the software is nicer to use than most CMSes. You just pick one. Name a company that makes a CMS. They’re like, “Is this as fun to use as Twitter?” And the answer is no. Flatly no. Even the one we have now for quick posts is not as fun to use as Twitter was in its heyday. Will this immediately bring me the dopamine hit of immediate feedback? No.<br />\n<br>\n[When redesigning the website]...the first instinct was, “Let’s at least make it easier to publish. Let’s at least remove the barriers to entry to getting on the website, and then we can do comments, and then we can think about how we can distribute in different ways.” So that is working. My team is happier. We did not know that the Twitter thing would happen, but the Twitter thing happened, and our desire to publish in the boxes we controlled went up as a group. And then, on top of it, our audience saw that we were having fun. And once you are having fun anywhere on the internet, people sort of gravitate to you. So traffic has gone up.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The distribution actually just creates the work or creates the pressures that force all the work to be the same. And I think over time that’s what drives the audiences away. So there’s a real change in how these platforms work, where, over time, they just become more and more of the same thing and the creators become more and more the same. And that’s a little exhausting. And every place where you see open distribution, you see a huge variety of creators and content.<br />\n<br>\nPodcasts have basically open distribution. Like podcast or distributor RSS feeds, that means people kind of own their distribution, there’s a vast array of podcast creators. There’s a vast array of podcast formats. They don’t all sound like the beginning of YouTube videos or whatever. And I hate to keep picking on YouTube; you can pick any algorithmic platform, and it’s the same. TikTokers are more the same than different. Podcasters are more different than the same. The web is distributed largely through websites and through RSS. There’s a huge variety of websites and the way websites look. But then you see the algorithmic search pressure push web design kind of all under the same box.<br />\n<br>\nNewsletters distributed by email: open distribution. The newsletter economy is full of a huge variety of creators doing a huge variety of things. They’re more different than the same. So all I see with the fediverse is, “Oh, this is going to open social distribution up a little bit.” It’s going to allow us to control our distribution networks. It’s going to say, “I’m not on Twitter, but people on Twitter can follow my website, and I can go promote that follow anywhere I want in different ways and build an audience outside of the pressures of the algorithm.” To me, just that, that ability to try, is 1 percent better.\n<br>\nIf you’re me and you run a big website and you are thinking, “How can I redistribute this website, how can I reach people more directly?” my brain is lit up. You should be able to follow me at TheVerge.com and see all my quick posts in your Threads account when Threads federates.</p>\n</blockquote>\n</div>\n","tags":["websites","internet","podcast","decoder","rss","platforms","blogs","technology"]},{"title":"Gemma PyTorch","contentType":"reshare","date":"2024-03-04 21:29","url":"/reshare/gemma-pytorch/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/google/gemma_pytorch\" >https://github.com/google/gemma_pytorch</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Gemma is a family of lightweight, state-of-the art open models built from research and technology used to create Google Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This is the official PyTorch implementation of Gemma models. We provide model and inference implementations using both PyTorch and PyTorch/XLA, and support running inference on CPU, GPU and TPU.</p>\n</blockquote>\n</div>\n","tags":["google","gemma","pytorch","ai","llm"]},{"title":"Introducing the next generation of Claude","contentType":"reshare","date":"2024-03-04 21:25","url":"/reshare/announcing-claude-3-model-family/","content":"<div class=\"response-target\" >→ <a href=\"https://www.anthropic.com/news/claude-3-family\" >https://www.anthropic.com/news/claude-3-family</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus. Each successive model offers increasingly powerful performance, allowing users to select the optimal balance of intelligence, speed, and cost for their specific application.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Claude 3 Opus is our most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Opus shows us the outer limits of what’s possible with generative AI.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Claude 3 Haiku is our fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with unmatched speed. Users will be able to build seamless AI experiences that mimic human interactions.</p>\n</blockquote>\n</div>\n","tags":["ai","anthropic","claude","llm"]},{"title":"GGUF, the long way around","contentType":"reshare","date":"2024-02-29 21:49","url":"/reshare/gguf-long-way-around/","content":"<div class=\"response-target\" >→ <a href=\"https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/\" >https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We’ve been on a whirlwind adventure to build up our intuition of how machine learning models work, what artifacts they produce, how the machine learning artifact storage story has changed over the past couple years, and finally ended up in GGUF’s documentation to better understand the log that is presented to us when we perform local inference on artifacts in GGUF.</p>\n</blockquote>\n</div>\n","tags":["ai","genai","gguf","llm","opensource"]},{"title":"Predictive Human Preference: From Model Ranking to Model Routing","contentType":"reshare","date":"2024-02-29 10:43","url":"/reshare/predictive-human-preference-model-ranking-routing/","content":"<div class=\"response-target\" >→ <a href=\"https://huyenchip.com//2024/02/28/predictive-human-preference.html\" >https://huyenchip.com//2024/02/28/predictive-human-preference.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Human preference has emerged to be both the Northstar and a powerful tool for AI model development. Human preference guides post-training techniques including RLHF and DPO. Human preference is also used to rank AI models, as used by LMSYS’s Chatbot Arena.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Chatbot Arena aims to determine which model is generally preferred. I wanted to see if it’s possible to do predictive human preference: determine which model is preferred for each query.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This post first discusses the correctness of Chatbot Arena, which will then be used as a baseline to evaluate the correctness of preference predictions. It then discusses how to build a preference predictor and the initial results.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","evaluation","ml"]},{"title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","contentType":"reshare","date":"2024-02-28 22:03","url":"/reshare/era-1-bit-llms/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2402.17764\" >https://arxiv.org/abs/2402.17764</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.</p>\n</blockquote>\n</div>\n","tags":["llm","ai","bitnet","research"]},{"title":"Tubi’s new redesign wants to push you down the rabbit hole","contentType":"reshare","date":"2024-02-28 21:51","url":"/reshare/tubi-redesign-down-the-rabbit-hole/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/2/28/24085869/tubi-redesign-shows-movies-turple\" >https://www.theverge.com/2024/2/28/24085869/tubi-redesign-shows-movies-turple</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Tubi says its new ‘turple’-forward brand identity is all about encouraging viewers to fall down rabbit holes to find exciting shows and movies to watch.</p>\n</blockquote>\n<p>I've often found <a href=\"/responses/streaming-fatigue-collecting-dvds\">good content to watch on Tubi</a>. Sure, they're not the latest blockbusters but there are decades worth of movies out there. Just like today, not all movies that come out are good but there's still tons of great content. Sometimes, there's even <a href=\"/responses/dread-central-overlook-motel\">overlooked gems</a>.</p>\n</div>\n","tags":["tv","movies","tubi","streaming"]},{"title":"What a bunch of A-list celebs taught me about how to use my phone","contentType":"star","date":"2024-02-28 10:24","url":"/star/celebs-how-to-use-phone/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/24084772/celebrities-no-phone-bieber-sheeran-cruise-cera-ipad\" >https://www.theverge.com/24084772/celebrities-no-phone-bieber-sheeran-cruise-cera-ipad</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...[phone-free celebs are] not trying to disconnect from everyone, but they are trying to get away from that feeling of being tapped constantly on the shoulder by all the calls, texts, and emails.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>So many celebrities ditch their phone, disconnect from their social media, log off entirely.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>A few years ago, Ed Sheeran shared a strategy...He hasn’t had a phone since 2015...Being phoneless hadn’t cut his contact to the world, Sheeran said, just reduced it — and that was the point. “I have friends email and people email, and every few days I’ll sit down and open up my laptop, and I’ll answer 10 emails at a time,” he said. “I’ll send them off, and I close my laptop, and then that’ll be it. And then I’ll go back to living my life, and I don’t feel overwhelmed by it.”</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Read and watch enough celebrity interviews, and the lesson becomes obvious: that the most powerful and connected device in your life shouldn’t be within arm’s reach at all times. All that does is invite distraction and makes it too easy to disengage from your life every time you get bored or sad or curious even for a second.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>It sounds a little like I’m advocating for the return of the ’90s, when the computer was a giant box that lived in a central room of your home and the only way to use it was to go to it. And to some extent, I am! I’m increasingly convinced that my primary computer should be a device I use on purpose — that I sit down at, operate, and then extract myself from until the next time. Whether it’s a laptop on a desk or an iPad on your nightstand, your computer should be a place as much as it is a device. And when you’re not in that place, you’re somewhere else. The computer doesn’t come along.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Over the last few weeks, as an experiment, I’ve moved as many apps as possible — the obviously distracting social media stuff but also anything I can live without on a minute-to-minute basis — off my phone and onto my tablet and my laptop...<br />\n<br>\nSo far, it’s been great. I’m realizing how much of a crutch my phone really has become: I would open up TikTok just to keep me company on the walk to the kitchen or scroll through Threads while I waited for the microwave to finish. Now, I’m not sure I’m doing any less of those things in aggregate, but at least I’m doing them on purpose. I’ve turned time-wasting into a deliberate activity — I sit in my scrolling chair and scroll away, then I get up, and the scrolling stays put. And best of all, when I leave the house, there’s nothing to scroll at all.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>There has always been talk in tech about removing friction: the obsessive corporate desire to make everything easier, faster, fewer clicks, fewer chances for you to decide not to click that ad or buy that thing or like that post or upload that photo...It should be a little harder for someone to distract me while I’m eating dinner with my wife or hanging out with my kid.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>It’s not about ditching technology, just about doing technology on purpose.</p>\n</blockquote>\n</div>\n","tags":["technology","communication","phones","smartphones","dumbphones","email","celebrities"]},{"title":"Tumblr and WordPress to Sell Users’ Data to Train AI Tools","contentType":"reply","date":"2024-02-27 21:09","url":"/reply/automattic-ai-data-tumblr-wordpress/","content":"<div class=\"response-target\" >→ <a href=\"https://www.404media.co/tumblr-and-wordpress-to-sell-users-data-to-train-ai-tools/\" >https://www.404media.co/tumblr-and-wordpress-to-sell-users-data-to-train-ai-tools/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Tumblr and WordPress.com are preparing to sell user data to Midjourney and OpenAI, according to a source with internal knowledge about the deals and internal documentation referring to the deals</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The internal documentation details a messy and controversial process within Tumblr itself. One internal post made by Cyle Gage, a product manager at Tumblr, states that a query made to prepare data for OpenAI and Midjourney compiled a huge number of user posts that it wasn’t supposed to. It is not clear from Gage’s post whether this data has already been sent to OpenAI and Midjourney, or whether Gage was detailing a process for scrubbing the data before it was to be sent.</p>\n</blockquote>\n<p>I generally enjoy what Automattic does for the web as a whole. However, if these claims are true, it's unfortunate. I believe there's a way to opt-out, but I'd love to learn more before jumping to conclusions.</p>\n<p>That said, WordPress (.com not .org) and Tumblr are platforms just like Reddit, Twitter, and the Meta set of offerings. I'm sure somewhere in their Terms of Service, there's some clauses around their ownserhip of the data you published on their platforms and just like it's sold to data brokers and advertisers, they can also sell it to companies training AI models.</p>\n<p>To counter these types of moves from platforms, I wish it were as easy as saying <a href=\"https://indieweb.org/own_your_data\">&amp;quot;build your own platform&amp;quot;</a>. Doing so can be as &amp;quot;simple&amp;quot; as setting up a website using <a href=\"https://indieweb.org/own_your_links\">your own domain</a>. Unfortunately, today, it's still not as easy to do and one of the products / companies that help you do that is WordPress. I think it's important though to note the distinction there between WordPress the company and WordPress the technology. Another piece that complicates building your own site is, there's still other ways for companies training AI models to use data that's publicly available on the internet. These are the arguments that are currently being litigated in several legal cases. Maybe there are opportunities to explore <a href=\"/responses/verge-ai-robots-txt/\">a robots.txt for AI</a>.</p>\n<p>AI models need high quality data that's representative and as close as possible to the real world in order to improve. There is a role here for <a href=\"/responses/cosmopedia-ai-synthetic-dataset/\">synthetic data</a>. High quality synthetic data is behind <a href=\"/responses/phi-2-huggingface/\">groundbreaking models like Microsoft's Phi</a>. Instincts tell me that synthetic data can only go so far though and real data is still needed. In that case, as an AI consumer who makes use of these AI models, but don't want to contribute my data, do I have a responsibility to contribute my data to improve the systems that I use? Piracy aside, in some ways it reminds me of torrenting. You usually run into scenarios where there are multiple people downloading a file. However, there's only a handful of seeders who once obtained, make the file available for others to download. There are also additional considerations such as how people are compensated for contributing their data to these systems. It's important to note that this is not a new problem and people had been thinking about this though in different contexts. Maybe it's time to reconsider ideas like <a href=\"https://www.microsoft.com/research/wp-content/uploads/2016/02/Inverse11.pdf\">inverse privacy</a> and <a href=\"https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society\">data dignity</a>.</p>\n<p>There are no clear answers here and there are a lot of things to consider. However, it's comforting that as a society we're having these conversations.</p>\n</div>\n","tags":["tumblr","automattic","wordpress","ai"]},{"title":"The latest Microsoft Copilot update on Android makes me mourn the death of Cortana","contentType":"reply","date":"2024-02-27 21:04","url":"/reply/copilot-android-update-mourn-cortana-death/","content":"<div class=\"response-target\" >→ <a href=\"https://www.windowscentral.com/software-apps/the-latest-microsoft-copilot-update-on-android-makes-me-mourn-the-death-of-cortana\" >https://www.windowscentral.com/software-apps/the-latest-microsoft-copilot-update-on-android-makes-me-mourn-the-death-of-cortana</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Microsoft Copilot will soon be able to be your default assistant app on Android.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>It's a shame that Cortana never worked out for Microsoft. If things had lined up differently, we may have seen Copilot gain access to smart devices and commands like Gemini has with Google Assistant (though that setup isn't perfect). While Copilot has a place on a computer, I think an assistant on your smartphone needs to be able to do more day-to-day tasks.</p>\n</blockquote>\n<p>So many things were ahead of their time. I just want a <a href=\"/responses/windows-12-mobile-concept/\">Windows Mobile PC (Windows Phone?) with an LLM-backed Cortana</a>.</p>\n</div>\n","tags":["cortana","copilot","ai","microsoft","virtualassistant","assistant","chatbot"]},{"title":"Physical media week - Exploring the importance of discs and cartridges in an increasingly digital age","contentType":"reshare","date":"2024-02-27 12:21","url":"/reshare/verge-physical-media-week/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/entertainment/24054458/physical-media-preservation-discs-cartridges-digital\" >https://www.theverge.com/entertainment/24054458/physical-media-preservation-discs-cartridges-digital</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The bright promise of streaming and digital stores has given way to a darker reality: we rarely have ownership over the art we love, and much is getting lost in the process. Only a fraction of movies released over the last century are available on streaming services, while a staggering 90 percent of classic video games are considered “critically endangered” by archivists. As these platforms continue to dominate the media landscape, a whole lot of cultural history is being abandoned.<br />\n<br>\nIn this special issue, The Verge will explore how physical media factors into this and its importance in keeping art alive and accessible. That could mean boutique publishers releasing beautiful special editions of games and movies, foundations dedicated to preserving the physical history of video games, or musicians releasing their latest albums on floppy discs. We’ll also be looking at some cautionary tales in the shift to subscription services and offering tips on building bookshelf-worthy collections.<br />\n<br>\nCartridges and discs have been hurtling toward obsolescence — but it turns out, they may be more important than ever.</p>\n</blockquote>\n</div>\n","tags":["media","verge","disc","dvd","cd","floppy","tech","technology"]},{"title":"JRE - Now available wherever you get your podcasts","contentType":"reshare","date":"2024-02-26 18:29","url":"/reshare/jre-wherever-you-get-your-podcasts/","content":"<div class=\"response-target\" >→ <a href=\"https://joeroganexp.libsyn.com/rss\" >https://joeroganexp.libsyn.com/rss</a></div><div class=\"response-content\" ><p>Since Joe Rogan went exclusive with Spotify, I maybe listened to a handful of episodes. The main reason for it is, I don't use Spotify to listen to podcasts. Periodically, I'd scroll through the feed to see if he had any intereting guests on. As part of his new contract, the podcast is now available on other platforms. That means you can listen <a href=\"https://podbay.fm/p/the-joe-rogan-experience/about\">wherever you get your podcasts</a>.</p>\n</div>\n","tags":["rss","podcast","protocols","jre","joerogan","audio","spotify"]},{"title":"My streaming fatigue got so bad I started collecting DVDs","contentType":"reply","date":"2024-02-26 15:44","url":"/reply/streaming-fatigue-collecting-dvds/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/24044151/streaming-subscription-prices-dvd-collection\" >https://www.theverge.com/24044151/streaming-subscription-prices-dvd-collection</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>After spending years reassuring myself that I don’t need physical copies of movies because of streaming, DVDs have officially reentered my life.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Walmart...Thrift stores, flea markets, the library, and even my local mall’s FYE have also become places I frequent to get my hands on oft-ignored discs.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>It makes sense to subscribe to all these services if you’re into the exclusive content on each one and have the patience to sift through their massive libraries. However, all I’ve been watching lately is the junk on Discovery Plus, simply because I’m too tired to find anything else — especially when the extremely specific shows and movies I want to watch keep switching services or just aren’t available. One of the most devastating examples of this was when both The Office and Parks and Recreation moved from Netflix to Peacock, disrupting the casual binge-watching sessions that I would default to when I was done with work.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Within the past year, nearly every streaming service has raised its prices, including Netflix, Disney Plus, Hulu, Paramount Plus, Discovery Plus, and Apple TV Plus.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I’m not saying DVDs are flawless: there’s a reason no one wants them anymore!</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Despite this, it’s still nice to have something that you physically own and don’t even need an internet connection to use. So when Best Buy confirmed it would stop selling DVDs this year and rumors emerged that Walmart would do the same, I was pretty disappointed. I can’t imagine Walmart without its bin of DVDs, nor can I even see Best Buy without its already-shrunken selection of movies.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>It’s 2024, and I’m not ready to say goodbye to DVDs — in fact, I’m just getting started.</p>\n</blockquote>\n<p>Great article from Emma.</p>\n<p>Personally, I've been doing the same. Just a few weekends ago, I got something like 8-10 DVDs for ~$25 at my local thrift store. That haul included 3 seasons of The Sopranos.</p>\n<p>With streaming services taking back control of their content and putting it on their own platforms, I don't want to have to keep signing up for a new service just to watch the shows and movies I enjoy. Also, that's assuming you can find the content to begin with (i.e. Westworld).</p>\n<p>Parks and Recreation, The Office, and Breaking Bad were some of the first shows I started collecting and have slowly been building up my collection. To save on space, I've ditched the cases and have the DVDs organized in a CD case. I've not only limited myself to DVDs but have also started collecting CDs as well.</p>\n<p>Whenever I want variety, I just use one of the free streaming services like:</p>\n<ul>\n<li><a href=\"https://www.justwatch.com/us/provider/freevee\">Freevee</a></li>\n<li><a href=\"https://pluto.tv/\">Pluto TV</a></li>\n<li><a href=\"https://tubitv.com/\">Tubi</a></li>\n<li>YouTube</li>\n</ul>\n<p>Yes, there are ads but at least I'm not paying for it and I know that's part of the deal. There are a ton of good older (and sometimes original) TV shows and movies on those platforms to keep me entertained. The most recent ones being Stargate and <a href=\"/notes/vampires-kiss-movie\">Vampire's Kiss</a>.</p>\n</div>\n","tags":["streaming","media","dvds","movies","tv","physicalmedia"]},{"title":"Announcing Mistral Large","contentType":"reshare","date":"2024-02-26 15:39","url":"/reshare/announcing-mistral-large/","content":"<div class=\"response-target\" >→ <a href=\"https://mistral.ai/news/mistral-large/\" >https://mistral.ai/news/mistral-large/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Mistral Large is our flagship model, with top-tier reasoning capacities. It is also available on Azure.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Mistral Large comes with new capabilities and strengths:<br />\n<br></p>\n<ul>\n<li>It is natively fluent in English, French, Spanish, German, and Italian, with a nuanced understanding of grammar and cultural context.<br />\n<br></li>\n<li>Its 32K tokens context window allows precise information recall from large documents.<br />\n<br></li>\n<li>Its precise instruction-following enables developers to design their moderation policies – we used it to set up the system-level moderation of le Chat.<br />\n<br></li>\n<li>It is natively capable of function calling. This, along with constrained output mode, implemented on la Plateforme, enables application development and tech stack modernisation at scale.</li>\n</ul>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>At Mistral, our mission is to make frontier AI ubiquitous. This is why we’re announcing today that we’re bringing our open and commercial models to Azure.</p>\n</blockquote>\n</div>\n","tags":["ai","mistral","llm","opensource","azure"]},{"title":"The direct influence of Twin Peaks on Zelda","contentType":"reshare","date":"2024-02-24 12:14","url":"/reshare/direct-influence-twin-peaks-zelda/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/24078662/twin-peaks-zelda-links-awakening-influence\" >https://www.theverge.com/24078662/twin-peaks-zelda-links-awakening-influence</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In a 2010 interview, Link’s Awakening director Takashi Tezuka revealed the inspiration for this memorably bizarre cast of characters. “At the time, Twin Peaks was rather popular. The drama was all about a small number of characters in a small town,” Tezuka said. “So I wanted to make something like that, while it would be small enough in scope to easily understand, it would have deep and distinctive characteristics.”</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>... [Mark] Frost reveals in an interview with The Verge, he actually spoke with Nintendo about the Zelda franchise. “I don’t want to overstate it. It was a single conversation. But it was fun,” he tells me.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>“They were talking to me about a Twin Peaks game, and they mentioned Zelda at the time,” says Frost. “They said, ‘One of the things we love about your show is how there’s all sorts of sideways associations that can drive the story forward.’ They asked me about that as they were thinking about expanding the Zelda universe.”</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Though he’d never played a Zelda game, Frost had enough experience with fantasy storytelling that he had some suggestions. “I’d played lots of Dungeons &amp;amp; Dragons when I was young, so I was familiar with the kind of story they were thinking about,” he says. “I think I said, ‘Don’t be afraid to use dreamlike, Jungian symbolism. Things can connect thematically without having to connect concretely.’ It was things like that that I was urging them [to consider].”</p>\n</blockquote>\n</div>\n","tags":["twinpeaks","nintendo","zelda","tv","culture","gaming"]},{"title":"GPT in 500 lines of SQL","contentType":"reshare","date":"2024-02-24 12:06","url":"/reshare/gpt-500-lines-sql/","content":"<div class=\"response-target\" >→ <a href=\"https://explainextended.com/2023/12/31/happy-new-year-15/\" >https://explainextended.com/2023/12/31/happy-new-year-15/</a></div><div class=\"response-content\" ><p><a href=\"https://github.com/quassnoi/explain-extended-2024\">GitHub Repo</a></p>\n</div>\n","tags":["sql","gpt","ai","llm"]},{"title":"Ali vs. Frazier I","contentType":"star","date":"2024-02-24 00:19","url":"/star/ali-frazier-I/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=eIm2eK5uuVA\" >https://www.youtube.com/watch?v=eIm2eK5uuVA</a></div><div class=\"response-content\" ><p><a href=\"https://www.youtube.com/watch?v=eIm2eK5uuVA\" title=\"Boxing match between Joe Frazier and Muhammad Ali I\"><img src=\"http://img.youtube.com/vi/eIm2eK5uuVA/0.jpg\" class=\"img-fluid\" alt=\"Boxing match between Joe Frazier and Muhammad Ali I\" /></a></p>\n<p>Such a good fight. Frazier had no defense but just kept coming forward.</p>\n</div>\n","tags":["boxing","ali","frazier"]},{"title":"Jim Cramer says McDonald’s embracing AI at drive-thrus is good news for Nvidia","contentType":"reply","date":"2024-02-24 00:02","url":"/reply/mcdonalds-ai-drive-thru-nvidia/","content":"<div class=\"response-target\" >→ <a href=\"https://www.cnbc.com/2024/02/23/jim-cramer-mcdonalds-use-of-ai-at-drive-thrus-is-good-news-for-nvidia.html\" >https://www.cnbc.com/2024/02/23/jim-cramer-mcdonalds-use-of-ai-at-drive-thrus-is-good-news-for-nvidia.html</a></div><div class=\"response-content\" ><p>I wish they'd use AI to keep the ice cream machines from breaking instead.</p>\n</div>\n","tags":["mcdonalds","ai","nvidia"]},{"title":"Salami Rose Joe Louis Live Band at Bandcamp - May 2023","contentType":"star","date":"2024-02-22 22:38","url":"/star/salami-rose-joe-louis-live-bandcamp-2023/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=VrnEQ3TqZGE\" >https://www.youtube.com/watch?v=VrnEQ3TqZGE</a></div><div class=\"response-content\" ><p><a href=\"https://www.youtube.com/watch?v=VrnEQ3TqZGE\" title=\"Salami Rose Jose Louis and a band playing a live concert\"><img src=\"http://img.youtube.com/vi/VrnEQ3TqZGE/0.jpg\" class=\"img-fluid\" alt=\"Salami Rose Jose Louis and a band playing a live concert\" /></a></p>\n<p>Great performance. I remember the first time I listened to Salami was when she opened for Flying Lotus and she was amazing.</p>\n</div>\n","tags":["brainfeeder","salamirosejoelouis","music","bandcamp"]},{"title":"Bluesky: An Open Social Web","contentType":"reshare","date":"2024-02-22 21:03","url":"/reshare/bluesky-open-social-web/","content":"<div class=\"response-target\" >→ <a href=\"https://bsky.social/about/blog/02-22-2024-open-social-web\" >https://bsky.social/about/blog/02-22-2024-open-social-web</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, we’re excited to announce that the Bluesky network is federating and opening up in a way that allows you to host your own data. What does this mean?</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Your data, such as your posts, likes, and follows, needs to be stored somewhere. With traditional social media, your data is stored by the social media company whose services you've signed up for. If you ever want to stop using that company's services, you can do that—but you would have to leave that social network and lose your existing connections.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>It doesn't have to be this way! An alternative model is how the internet itself works. Anyone can put up a website on the internet</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We think social media should work the same way. When you register on Bluesky, by default we'll suggest that Bluesky will store your data. But if you'd like to let another company store it, or even store it yourself, you can do that. You'll also be able to change your mind at any point, moving your data to another provider without losing any of your existing posts, likes, or follows. From your followers' perspective, your profile is always available at your handle—no matter where your information is actually stored, or how many times it has been moved.</p>\n</blockquote>\n<p>I don't spend a lot of time on Bluesky, but I love what they're doing.</p>\n<p>They now:</p>\n<ul>\n<li><a href=\"https://bsky.social/about/blog/02-06-2024-join-bluesky\">Made their service generally available</a>.</li>\n<li>Have over <a href=\"https://bsky.app/profile/bsky.app/post/3klzrudt4uk2z\">5 million users</a>.</li>\n<li><a href=\"/responses/bluesky-rss-support/\">Support RSS</a></li>\n</ul>\n<p>Now you can self-host your data. I'm excited.</p>\n<p>The other piece of this that's interesting is their feature which enables you to <a href=\"https://bsky.social/about/blog/7-05-2023-namecheap\">use your domain as a custom handle</a>. Not only is your identity portable, but also your data. I'd be interested to see how this works in practice given you can already do some of this on the Fediverse on platforms like Mastodon. ... identity component to me is crucial. That's one of the challenges with Mastodon today. While you can move instances, your identity changes and while most of your data comes with you, there's tihngs that still dont transfer over. The other part that I'd be interested in seeing is whether or not they can be efficient in the storage of federated data. One of the challenges with Mastodon is, your server quickly fills up because of data from other instances (when you federate). <a href=\"/notes/mastodon-4-1-0-release\">It's gotten better</a>, but this is an area I spend most of my time when maintaining my own self-hosted instance.</p>\n<p>I'm excited to tinker and self-host my own data. Maybe I'll also <a href=\"/posts/rss-to-mastodon-posse-azure-logic-apps\">syndicate to Bluesky just like I do with Mastodon today</a>.</p>\n<p>In the meantime, you can find me on Bluesky <a href=\"/bluesky\">@lqdev.me</a>.</p>\n</div>\n","tags":["bluesky","socialmedia","web","internet","community"]},{"title":"Stable Diffusion 3 - Early Preview","contentType":"reshare","date":"2024-02-22 20:59","url":"/reshare/stable-diffusion-3-early-preview/","content":"<div class=\"response-target\" >→ <a href=\"https://stability.ai/news/stable-diffusion-3\" >https://stability.ai/news/stable-diffusion-3</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Announcing Stable Diffusion 3 in early preview, our most capable text-to-image model with greatly improved performance in multi-subject prompts, image quality, and spelling abilities.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The Stable Diffusion 3 suite of models currently range from 800M to 8B parameters.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Stable Diffusion 3 combines a diffusion transformer architecture and flow matching.</p>\n</blockquote>\n</div>\n","tags":["stabilityai","ai","stablediffusion","imagegeneration","ml","image","genai","generativeai"]},{"title":"On Blogs","contentType":"star","date":"2024-02-21 15:14","url":"/star/on-blogs-searls/","content":"<div class=\"response-target\" >→ <a href=\"https://doc.searls.com/2024/02/21/on-blogs/\" >https://doc.searls.com/2024/02/21/on-blogs/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Thoughts I jotted down on Mastodon:<br />\n<br></p>\n<ol>\n<li>Blogs are newsletters that don’t require subscriptions.<br />\n<br></li>\n<li>Blogrolls are lists of blogs.<br />\n<br></li>\n<li>Both require the lowest possible cognitive and economic overhead.<br />\n<br></li>\n<li>That’s why they are coming back.<br />\n<br>\nI know, they never left. But you get my point.</li>\n</ol>\n</blockquote>\n</div>\n","tags":["blogs","writing","newsletters","blogroll"]},{"title":"Windows 12 Mobile - Concept","contentType":"reply","date":"2024-02-21 14:59","url":"/reply/windows-12-mobile-concept/","content":"<div class=\"response-target\" >→ <a href=\"https://www.windowscentral.com/phones/windows-phone/what-would-microsofts-windows-phone-look-like-in-2024-its-like-a-micro-pc-running-windows-12-in-your-pocket\" >https://www.windowscentral.com/phones/windows-phone/what-would-microsofts-windows-phone-look-like-in-2024-its-like-a-micro-pc-running-windows-12-in-your-pocket</a></div><div class=\"response-content\" ><p>Cool concept.</p>\n<p><a href=\"https://www.youtube.com/watch?v=XhwFp6lMOcQ\" title=\"Windows 12 Mobile Concept Video\"><img src=\"http://img.youtube.com/vi/XhwFp6lMOcQ/0.jpg\" class=\"img-fluid\" alt=\"Windows 12 Mobile Concept Video\" /></a></p>\n<p>The likelihood of it happening is low, but there's a lot of really great opportunities here especially with <a href=\"/posts/quick-thoughts-snapdragon-summit-2023\">the new wave or ARM PCs coming</a>. I don't know what the device form factor looks like, but I wouldn't mind carrying around a pocket PC - a true mobile computer. Already with the Windows Store, you have access to tons of apps. For the apps that aren't in the Store, there's the browser. That seemed to be good enough for <a href=\"/responses/vision-pros-most-important-app-safari\">Apple's Vision Pro</a>. Taking it a step further would the app gap matter as much if you have Copilot as your concierge orchestrating tasks for you using the various services? Better yet, what if these services had their own assistants / GPTs Copilot could talk to and coordinate on your behalf?</p>\n<p>At some point, I might just use <a href=\"/responses/openai-sora\">OpenAI's Sora model</a> to live vicariously through an AI-generated video depicting this alternate reality where Windows Phone exists...</p>\n</div>\n","tags":["windows","mobile","windowsphone","pc"]},{"title":"The killer app of Gemini Pro 1.5 is video","contentType":"reshare","date":"2024-02-21 14:45","url":"/reshare/gemini-pro-video-willison/","content":"<div class=\"response-target\" >→ <a href=\"https://simonwillison.net/2024/Feb/21/gemini-pro-video/\" >https://simonwillison.net/2024/Feb/21/gemini-pro-video/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>I’ve been playing with Gemini Pro 1.5 for a few days, and I think the most exciting feature isn’t so much the token count... it’s the ability to use video as an input.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The ability to extract structured content from text is already one of the most exciting use-cases for LLMs. GPT-4 Video and LLaVA expanded that to images. And now Gemini Pro 1.5 expands that to video.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The ability to analyze video like this feels SO powerful. Being able to take a 20 second video of a bookshelf and get back a JSON array of those books is just the first thing I thought to try.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...as always with modern AI, there are still plenty of challenges to overcome...But this really does feel like another one of those glimpses of a future that’s suddenly far closer then I expected it to be.</p>\n</blockquote>\n</div>\n","tags":["google","ai","gemini"]},{"title":"HuggingChat","contentType":"reshare","date":"2024-02-21 14:38","url":"/reshare/huggingchat/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/chat/assistants\" >https://huggingface.co/chat/assistants</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The goal of this app is to showcase that it is now possible to build an open source alternative to ChatGPT.</p>\n</blockquote>\n<ul>\n<li><a href=\"https://github.com/huggingface/chat-ui\">GitHub repo</a></li>\n<li><a href=\"https://github.com/huggingface/text-generation-inference\">Text Generation Inference</a></li>\n</ul>\n</div>\n","tags":["huggingface","assistants","ai","chat","opensource"]},{"title":"Gemma: Introducing new state-of-the-art open models","contentType":"reshare","date":"2024-02-21 14:30","url":"/reshare/gemma-google-ai-models/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.google/technology/developers/gemma-open-models/\" >https://blog.google/technology/developers/gemma-open-models/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. Developed by Google DeepMind and other teams across Google, Gemma is inspired by Gemini, and the name reflects the Latin gemma, meaning “precious stone.” Accompanying our model weights, we’re also releasing tools to support developer innovation, foster collaboration, and guide responsible use of Gemma models.</p>\n</blockquote>\n<p><a href=\"https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b\">HuggingFace Gemma Release</a></p>\n</div>\n","tags":["ai","opensource","google","model","llm"]},{"title":"Podcast - localfirst.fm","contentType":"reshare","date":"2024-02-20 21:38","url":"/reshare/localfirstfm-podcast/","content":"<div class=\"response-target\" >→ <a href=\"https://www.localfirst.fm/\" >https://www.localfirst.fm/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A podcast about local-first software development</p>\n</blockquote>\n</div>\n","tags":["development","software","podcast","tech","podcast","localfirst","local"]},{"title":"Keep your phone number private with Signal usernames","contentType":"reshare","date":"2024-02-20 17:01","url":"/reshare/signal-usernames/","content":"<div class=\"response-target\" >→ <a href=\"https://signal.org/blog/phone-number-privacy-usernames/\" >https://signal.org/blog/phone-number-privacy-usernames/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Signal’s mission and sole focus is private communication. For years, Signal has kept your messages private, your profile information (like your name and profile photo) private, your contacts private, and your groups private – among much else. Now we’re taking that one step further, by making your phone number on Signal more private.<br />\n<br>\nHere’s how:<br />\n<br>\nNew default: Your phone number will no longer be visible to everyone in Signal...<br />\n<br>\nConnect without sharing your phone number...<br />\n<br>\nControl who can find you on Signal by phone number...<br />\n<br>\nRight now, these options are in beta, and will be rolling out to everyone in the coming weeks.</p>\n</blockquote>\n</div>\n","tags":["signal","privacy","security","osint","messaging"]},{"title":"Cosmopedia v0.1","contentType":"reshare","date":"2024-02-20 16:50","url":"/reshare/cosmopedia-ai-synthetic-dataset/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/datasets/HuggingFaceTB/cosmopedia\" >https://huggingface.co/datasets/HuggingFaceTB/cosmopedia</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Cosmopedia is a dataset of synthetic textbooks, blogposts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1.The dataset contains over 30 million files and 25 billion tokens, making it the largest open synthetic dataset to date.<br />\n<br>\nIt covers a variety of topics; we tried to map world knowledge present in Web datasets like RefinedWeb and RedPajama, and generate synthetic content that covers them. This is the v0.1 of Cosmopedia, with ample room for improvement and topics to be more comprehensively covered. We hope this dataset will help the community's research efforts in the increasingly intriguing domain of synthetic data.<br />\n<br>\nThis work is inspired by the great work of Phi1.5.</p>\n</blockquote>\n</div>\n","tags":["cosmopedia","dataset","huggingface","mixtral","ai","genai","llm"]},{"title":"MLX Swift - On-device ML research with MLX and Swift","contentType":"reshare","date":"2024-02-20 16:25","url":"/reshare/swift-mlx-ml-research/","content":"<div class=\"response-target\" >→ <a href=\"https://www.swift.org/blog/mlx-swift/\" >https://www.swift.org/blog/mlx-swift/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The Swift programming language has a lot of potential to be used for machine learning research because it combines the ease of use and high-level syntax of a language like Python with the speed of a compiled language like C++.<br />\n<br>\n<a href=\"https://github.com/ml-explore/mlx\">MLX</a> is an array framework for machine learning research on Apple silicon. MLX is intended for research and not for production deployment of models in apps.<br />\n<br>\n<a href=\"https://github.com/ml-explore/mlx-swift/\">MLX Swift</a> expands MLX to the Swift language, making experimentation on Apple silicon easier for ML researchers.</p>\n</blockquote>\n</div>\n","tags":["apple","mlx","swift","opensource","ml","ai"]},{"title":"Ollama - Windows Preview","contentType":"reshare","date":"2024-02-19 16:13","url":"/reshare/ollama-windows-preview/","content":"<div class=\"response-target\" >→ <a href=\"https://ollama.com/blog/windows-preview\" >https://ollama.com/blog/windows-preview</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Ollama is now available on Windows in preview, making it possible to pull, run and create large language models in a new native Windows experience. Ollama on Windows includes built-in GPU acceleration, access to the full model library, and the Ollama API including OpenAI compatibility.</p>\n</blockquote>\n<p><a href=\"https://ollama.com/download/windows\">Download (https://ollama.com/download/windows)</a></p>\n</div>\n","tags":["ollama","windows","llm","opensource","localmodels","ml","ai"]},{"title":"StatiCrypt: Protected Posts on a Static Site","contentType":"star","date":"2024-02-17 12:27","url":"/star/staticrypt-protected-posts-static-site-jayless/","content":"<div class=\"response-target\" >→ <a href=\"https://www.jayeless.net/2024/02/staticrypt.html\" >https://www.jayeless.net/2024/02/staticrypt.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...I was longing for a way to do friends-only blog posts on the open web, today I came across Stati­Crypt , an open-source utility that lets you encrypt static HTML pages behind a password.</p>\n</blockquote>\n</div>\n","tags":["staticsite","web","blogs","community","openweb","security","privacy"]},{"title":"Observable 2.0 - Announcing Observable Framework","contentType":"reshare","date":"2024-02-15 20:48","url":"/reshare/observable-2-0/","content":"<div class=\"response-target\" >→ <a href=\"https://observablehq.com/blog/observable-2-0\" >https://observablehq.com/blog/observable-2-0</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today we’re launching Observable 2.0 with a bold new vision: an open-source static site generator for building fast, beautiful data apps, dashboards, and reports.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Our mission is to help teams communicate more effectively with data. Effective presentation of data is critical for deep insight, nuanced understanding, and informed decisions. Observable notebooks are great for ephemeral, ad hoc data exploration. But notebooks aren’t well-suited for polished dashboards and apps.</p>\n</blockquote>\n</div>\n","tags":["data","analytics","observable"]},{"title":"V-JEPA: The next step toward Yann LeCun’s vision of advanced machine intelligence (AMI)","contentType":"reshare","date":"2024-02-15 20:20","url":"/reshare/v-jepa-meta/","content":"<div class=\"response-target\" >→ <a href=\"https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/\" >https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, we’re publicly releasing the Video Joint Embedding Predictive Architecture (V-JEPA) model, a crucial step in <a href=\"https://openreview.net/pdf?id=BZ5a1r-kVsf\">advancing machine intelligence</a> with a more grounded understanding of the world.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This early example of a physical world model excels at detecting and understanding highly detailed interactions between objects.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In the spirit of responsible open science, we’re releasing this model under a Creative Commons NonCommercial license for researchers to further explore.</p>\n</blockquote>\n</div>\n","tags":["meta","ai","ami","ml","opensource","multimodal"]},{"title":"OpenAI Sora - Creating video from text","contentType":"reshare","date":"2024-02-15 20:15","url":"/reshare/openai-sora/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/sora\" >https://openai.com/sora</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Sora is an AI model that can create realistic and imaginative scenes from text instructions.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Sora can generate videos up to a minute long while maintaining visual quality and adherence to the user’s prompt.</p>\n</blockquote>\n</div>\n","tags":["openai","multimodal","ai","texttovideo"]},{"title":"Introducing Gemini 1.5","contentType":"reshare","date":"2024-02-15 20:11","url":"/reshare/gemini-1-5-google/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/?ref=ellipsismx.com\" >https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/?ref=ellipsismx.com</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, we’re announcing our next-generation model: Gemini 1.5.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Gemini 1.5 delivers dramatically enhanced performance. It represents a step change in our approach, building upon research and engineering innovations across nearly every part of our foundation model development and infrastructure. This includes making Gemini 1.5 more efficient to train and serve, with a new Mixture-of-Experts (MoE) architecture.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Gemini 1.5 Pro comes with a standard 128,000 token context window. But starting today, a limited group of developers and enterprise customers can try it with a context window of up to 1 million tokens via AI Studio and Vertex AI in private preview.</p>\n</blockquote>\n</div>\n","tags":["ai","gemini","lm","google"]},{"title":"Club TWiT Shows Now Open to Everyone!","contentType":"reshare","date":"2024-02-14 22:37","url":"/reshare/club-twit-shows-available-to-all/","content":"<div class=\"response-target\" >→ <a href=\"https://twit.tv/posts/inside-twit/club-shows-now-open-everyone\" >https://twit.tv/posts/inside-twit/club-shows-now-open-everyone</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We are thrilled to announce that our Club TWiT shows are now available to everyone in audio form. That's right, you can now listen to your favorite shows anytime, anywhere, and it's all starting as early as the end of this week.</p>\n</blockquote>\n</div>\n","tags":["twit","podcast","tech","community"]},{"title":"We ❤️ RSS","contentType":"star","date":"2024-02-14 22:27","url":"/star/we-love-rss-ott/","content":"<div class=\"response-target\" >→ <a href=\"https://matthiasott.com/notes/we-love-rss\" >https://matthiasott.com/notes/we-love-rss</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>What makes RSS so powerful is that it is an open format. RSS is one of the reasons the blogosphere grew so rapidly and it is the reason why podcasting exploded: because this open format allowed everyone to participate by simply publishing a feed anywhere on the web, without being restricted by platform requirements, closed APIs, and paywalls. And this superpower is also why RSS is having a renaissance today: it allows everyone to subscribe to, share, syndicate, and cross-post content on the open web. And it also enables creative automations using tools like Zapier, IFTTT, Huggin, or n8n.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>There is no denying that RSS is having a moment again. Not only because it allows us all to improve the discoverability of our work and explore online content in a personalized and deliberate way, but also because it remains one of the most powerful and influential technologies of the open web. RSS already is the cornerstone of many open technology systems like podcasting, which can’t be owned and controlled by any one company. As Anil Dash notes, this alone is radical, because it is the triumph of exactly the kind of technology that's supposed to be impossible: open and empowering tech that allows people to have ownership over their work and their relationship with their audience.</p>\n</blockquote>\n</div>\n","tags":["rss","web","internet","oss","openweb","indieweb"]},{"title":"The text file that runs the internet","contentType":"reshare","date":"2024-02-14 20:47","url":"/reshare/verge-ai-robots-txt/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/24067997/robots-txt-ai-text-file-web-crawlers-spiders\" >https://www.theverge.com/24067997/robots-txt-ai-text-file-web-crawlers-spiders</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>For three decades, a tiny text file has kept the internet from chaos. This text file has no particular legal or technical authority, and it’s not even particularly complicated. It represents a handshake deal between some of the earliest pioneers of the internet to respect each other’s wishes and build the internet in a way that benefitted everybody. It’s a mini constitution for the internet, written in code.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>It’s called robots.txt and is usually located at yourwebsite.com/robots.txt. That file allows anyone who runs a website — big or small, cooking blog or multinational corporation — to tell the web who’s allowed in and who isn’t. Which search engines can index your site? What archival projects can grab a version of your page and save it? Can competitors keep tabs on your pages for their own files? You get to decide and declare that to the web.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>It’s not a perfect system, but it works. Used to, anyway. For decades, the main focus of robots.txt was on search engines; you’d let them scrape your site and in exchange they’d promise to send people back to you. Now AI has changed the equation: companies around the web are using your site and its data to build massive sets of training data, in order to build models and products that may not acknowledge your existence at all.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The robots.txt file governs a give and take; AI feels to many like all take and no give. But there’s now so much money in AI, and the technological state of the art is changing so fast that many site owners can’t keep up. And the fundamental agreement behind robots.txt, and the web as a whole — which for so long amounted to “everybody just be cool” — may not be able to keep up either.</p>\n</blockquote>\n</div>\n","tags":["ai","internet","web"]},{"title":"Introducing Sudo for Windows","contentType":"reshare","date":"2024-02-13 21:51","url":"/reshare/introducing-sudo-windows/","content":"<div class=\"response-target\" >→ <a href=\"https://devblogs.microsoft.com/commandline/introducing-sudo-for-windows/\" >https://devblogs.microsoft.com/commandline/introducing-sudo-for-windows/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Sudo for Windows is a new way for users to run elevated commands directly from an unelevated console session. It is an ergonomic and familiar solution for users who want to elevate a command without having to first open a new elevated console.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We are also excited to announce that we are open-sourcing this project here on GitHub!</p>\n</blockquote>\n<p><a href=\"https://github.com/microsoft/sudo\">GitHub Repo</a></p>\n</div>\n","tags":["windows","sudo","system"]},{"title":"NVIDIA Chat with RTX","contentType":"reshare","date":"2024-02-13 21:08","url":"/reshare/nvidia-chat-rtx/","content":"<div class=\"response-target\" >→ <a href=\"https://www.nvidia.com/ai-on-rtx/chat-with-rtx-generative-ai/\" >https://www.nvidia.com/ai-on-rtx/chat-with-rtx-generative-ai/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Chat With RTX is a demo app that lets you personalize a GPT large language model (LLM) connected to your own content—docs, notes, videos, or other data. Leveraging retrieval-augmented generation (RAG), TensorRT-LLM, and RTX acceleration, you can query a custom chatbot to quickly get contextually relevant answers. And because it all runs locally on your Windows RTX PC or workstation, you’ll get fast and secure results.</p>\n</blockquote>\n</div>\n","tags":["nvidia","chat","ai","rag","chatbot","gpu","llm"]},{"title":"Stable Cascade","contentType":"reshare","date":"2024-02-13 21:02","url":"/reshare/stability-ai-stable-cascade/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/Stability-AI/StableCascade\" >https://github.com/Stability-AI/StableCascade</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Stable Cascade consists of three models: Stage A, Stage B and Stage C, representing a cascade for generating images, hence the name &amp;quot;Stable Cascade&amp;quot;. Stage A &amp;amp; B are used to compress images, similarly to what the job of the VAE is in Stable Diffusion. However, as mentioned before, with this setup a much higher compression of images can be achieved. Furthermore, Stage C is responsible for generating the small 24 x 24 latents given a text prompt. The following picture shows this visually. Note that Stage A is a VAE and both Stage B &amp;amp; C are diffusion models.<br />\n<br>\nFor this release, we are providing two checkpoints for Stage C, two for Stage B and one for Stage A. Stage C comes with a 1 billion and 3.6 billion parameter version, but we highly recommend using the 3.6 billion version, as most work was put into its finetuning. The two versions for Stage B amount to 700 million and 1.5 billion parameters. Both achieve great results, however the 1.5 billion excels at reconstructing small and fine details. Therefore, you will achieve the best results if you use the larger variant of each. Lastly, Stage A contains 20 million parameters and is fixed due to its small size.</p>\n</blockquote>\n</div>\n","tags":["stabilityai","ai","stablediffusion"]},{"title":"Memory and new controls for ChatGPT","contentType":"reshare","date":"2024-02-13 21:00","url":"/reshare/openai-chatgpt-memory/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/blog/memory-and-new-controls-for-chatgpt\" >https://openai.com/blog/memory-and-new-controls-for-chatgpt</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We’re testing memory with ChatGPT. Remembering things you discuss across all chats saves you from having to repeat information and makes future conversations more helpful.<br />\n<br>\nYou're in control of ChatGPT's memory. You can explicitly tell it to remember something, ask it what it remembers, and tell it to forget conversationally or through settings. You can also turn it off entirely.<br />\n<br>\nWe are rolling out to a small portion of ChatGPT free and Plus users this week to learn how useful it is. We will share plans for broader roll out soon.</p>\n</blockquote>\n</div>\n","tags":["openai","chatgpt","memory","ai","llm","gpt"]},{"title":"Here’s how WhatsApp plans to interoperate with other messaging apps","contentType":"reshare","date":"2024-02-06 20:03","url":"/reshare/how-whatsapp-plans-interoperate-other-messaging-services/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/2/6/24063705/whatsapp-interoperability-plans-eu-dma\" >https://www.theverge.com/2024/2/6/24063705/whatsapp-interoperability-plans-eu-dma</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>WhatsApp, like many other major tech platforms, will have to make some significant changes to comply with the European Union’s Digital Markets Act (DMA). One of those changes is interoperability with other messaging platforms...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The shift toward interoperability will first include text messages, images, voice messages, videos, and files sent from one person to another. In theory, this would allow users to chat with people on WhatsApp through third-party apps, like iMessage, Telegram, Google Messages, and Signal, and vice versa.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>As noted by Wired, WhatsApp wants the messaging services it connects with to use the same Signal Protocol to encrypt messages. Meta is also open to apps using alternate encryption protocols so long as companies can prove “they reach the security standards that WhatsApp outlines in its guidance.” The third-party services will also have to sign a contract with Meta before they plug into WhatsApp, with more details about the agreement coming in March.</p>\n</blockquote>\n</div>\n","tags":["whatsapp","messaging","interoperability","signal"]},{"title":"Introducing Nomic Embed: A Truly Open Embedding Model","contentType":"reshare","date":"2024-02-03 20:37","url":"/reshare/nomic-embed-open-embedding-model/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.nomic.ai/posts/nomic-embed-text-v1\" >https://blog.nomic.ai/posts/nomic-embed-text-v1</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We're excited to announce the release of Nomic Embed, the first<br />\n<br></p>\n<ul>\n<li>Open source</li>\n<li>Open data</li>\n<li>Open training code</li>\n<li>Fully reproducible and auditable</li>\n</ul>\n</blockquote>\n</div>\n","tags":["embedding","ai","opensource","ml","model"]},{"title":"UNREDACTED Magazine Issue 006","contentType":"reshare","date":"2024-01-31 21:24","url":"/reshare/unredacted-magazine-issue-006/","content":"<div class=\"response-target\" >→ <a href=\"https://inteltechniques.com/blog/2024/01/27/unredacted-magazine-issue-006/\" >https://inteltechniques.com/blog/2024/01/27/unredacted-magazine-issue-006/</a></div><div class=\"response-content\" ><ul>\n<li><a href=\"https://inteltechniques.com/magazine.html\">https://inteltechniques.com/magazine.html</a></li>\n<li><a href=\"https://inteltechniques.com/issues/006.pdf\">https://inteltechniques.com/issues/006.pdf</a></li>\n</ul>\n</div>\n","tags":["osint","magazine","unredacted","privacy","security"]},{"title":"Tiny Desk Premiere: Thee Sacred Souls","contentType":"reshare","date":"2024-01-31 19:50","url":"/reshare/thee-sacred-souls-npr-tiny-desk-concerts/","content":"<div class=\"response-target\" >→ <a href=\"https://www.npr.org/2024/01/26/1226810515/tiny-desk-concert-thee-sacred-souls\" >https://www.npr.org/2024/01/26/1226810515/tiny-desk-concert-thee-sacred-souls</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>San Diego-based trio <a href=\"https://www.npr.org/artists/1106773743/thee-sacred-souls\">Thee Sacred Souls</a> made its mark at the Tiny Desk with satin vocals and vintage melodies. Paying homage to southern California Latino culture meeting American soul roots, the group's sweet fusion melodies brought history and love into the space.</p>\n</blockquote>\n</div>\n","tags":["theesacredsouls","music","concert","npr","tinydeskconcert"]},{"title":"Ollama - Python & JavaScript Libraries","contentType":"reshare","date":"2024-01-25 21:26","url":"/reshare/python-js-ollama-libraries/","content":"<div class=\"response-target\" >→ <a href=\"https://ollama.ai/blog/python-javascript-libraries\" >https://ollama.ai/blog/python-javascript-libraries</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The initial versions of the Ollama Python and JavaScript libraries are now available:<br />\n<br>\n<a href=\"https://github.com/ollama/ollama-python\">Ollama Python Library</a><br />\n<a href=\"https://github.com/ollama/ollama-js\">Ollama JavaScript Library</a><br />\n<br>\nBoth libraries make it possible to integrate new and existing apps with Ollama in a few lines of code, and share the features and feel of the Ollama REST API.</p>\n</blockquote>\n</div>\n","tags":["ollama","ai","llama","python","javascript","opensource"]},{"title":"Google’s Hugging Face deal puts ‘supercomputer’ power behind open-source AI","contentType":"reshare","date":"2024-01-25 21:23","url":"/reshare/google-cloud-huggingface-deal/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/1/25/24050445/google-cloud-hugging-face-ai-developer-access\" >https://www.theverge.com/2024/1/25/24050445/google-cloud-hugging-face-ai-developer-access</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Google Cloud’s new partnership with AI model repository Hugging Face is letting developers build, train, and deploy AI models without needing to pay for a Google Cloud subscription. Now, outside developers using Hugging Face’s platform will have “cost-effective” access to Google’s tensor processing units (TPU) and GPU supercomputers, which will include thousands of Nvidia’s in-demand and export-restricted H100s.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Google said that Hugging Face users can begin using the AI app-building platform Vertex AI and the Kubernetes engine that helps train and fine-tune models “in the first half of 2024.”</p>\n</blockquote>\n<p><a href=\"https://www.googlecloudpresscorner.com/2024-01-25-Google-Cloud-and-Hugging-Face-Announce-Strategic-Partnership-to-Accelerate-Generative-AI-and-ML-Development\">Press Release</a></p>\n</div>\n","tags":["google","huggingface","ai","cloud","llm","ml","developers"]},{"title":"New OpenAI embedding models and API updates","contentType":"reshare","date":"2024-01-25 21:20","url":"/reshare/new-openai-text-embedding-models-3/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/blog/new-embedding-models-and-api-updates\" >https://openai.com/blog/new-embedding-models-and-api-updates</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We are launching a new generation of embedding models, new GPT-4 Turbo and moderation models, new API usage management tools, and soon, lower pricing on GPT-3.5 Turbo.</p>\n</blockquote>\n</div>\n","tags":["openai","llm","embedding","openai","gpt"]},{"title":"FOSDEM 2024 Schedule","contentType":"reshare","date":"2024-01-23 22:19","url":"/reshare/fosdem-2024-schedule/","content":"<div class=\"response-target\" >→ <a href=\"https://fosdem.org/2024/schedule/events/\" >https://fosdem.org/2024/schedule/events/</a></div><div class=\"response-content\" ><p>Lots of great sessions. I'm looking forward to the sessions on the following topics:</p>\n<ul>\n<li>Matrix</li>\n<li>AI</li>\n<li>Nix / NixOS</li>\n<li>Software Defined Radio (SDR) &amp;amp; Amateur Radio</li>\n<li>Modern Email</li>\n<li>Collaboration &amp;amp; Content Management</li>\n</ul>\n</div>\n","tags":["fosdem","opensource","foss","floss","ai","matrix","nixos"]},{"title":"UNREDACTED Magazine Status","contentType":"reshare","date":"2024-01-23 21:34","url":"/reshare/unredacted-magazine-status-jan-2024/","content":"<div class=\"response-target\" >→ <a href=\"https://inteltechniques.com/blog/2024/01/05/unredacted-magazine-status/\" >https://inteltechniques.com/blog/2024/01/05/unredacted-magazine-status/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...the magazine is not 'dead'. Much like the podcast, it is simply on a hiatus. Many people falsely report online that the podcast and magazine are officially never coming back, which is contradictory to my <a href=\"https://inteltechniques.com/blog/2023/11/20/my-irish-exit/\">previous post</a>. The reason there have been no issues of the magazine is simply a lack of submissions.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The magazine is a community-driven product. Without the community driving it, it will go nowhere. If you would like to submit an article, please email it to staff@unredactedmagazine.com.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Sponsors are lined up to pay the costs and keep the content free, but there lies other problems. We received constant complaints about having sponsors. Most readers demanded free content without ads, which is unrealistic.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We have considered a small fee per issue, but the credit card fraud which comes with that is an even bigger issue. What is the solution? I do not know yet. If the articles pour in, I will figure it out.</p>\n</blockquote>\n</div>\n","tags":["magazine","unredacted","osint"]},{"title":"Microsoft is Dead (2007)","contentType":"reply","date":"2024-01-23 21:12","url":"/reply/graham-microsoft-dead/","content":"<div class=\"response-target\" >→ <a href=\"https://paulgraham.com/microsoft.html\" >https://paulgraham.com/microsoft.html</a></div><div class=\"response-content\" ><p><img src=\"https://media.giphy.com/media/b6iVj3IM54Abm/giphy.gif\" class=\"img-fluid\" alt=\"GIF of WWE Undertaker sitting up\" /></p>\n<p>Interesting points.</p>\n</div>\n","tags":["google","microsoft","web"]},{"title":"NightShade","contentType":"reshare","date":"2024-01-23 20:57","url":"/reshare/nightshade-ai-model-poisoning-tool/","content":"<div class=\"response-target\" >→ <a href=\"https://nightshade.cs.uchicago.edu/\" >https://nightshade.cs.uchicago.edu/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Nightshade, a tool that turns any image into a data sample that is unsuitable for model training. More precisely, Nightshade transforms images into &amp;quot;poison&amp;quot; samples, so that models training on them without consent will see their models learn unpredictable behaviors that deviate from expected norms, e.g. a prompt that asks for an image of a cow flying in space might instead get an image of a handbag floating in space.</p>\n</blockquote>\n<p><a href=\"https://nightshade.cs.uchicago.edu/whatis.html\">What is NightShade?</a></p>\n</div>\n","tags":["research","tools","ai","generativeai","llm","computervision","cv"]},{"title":"Introducing Shared Journals for Day One","contentType":"reshare","date":"2024-01-23 20:26","url":"/reshare/introducing-shared-journals-day-one/","content":"<div class=\"response-target\" >→ <a href=\"https://dayoneapp.com/blog/introducing-shared-journals/\" >https://dayoneapp.com/blog/introducing-shared-journals/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Shared Journals are a private space for your closest friends and family to shared life updates and memories. Shared Journals introduce a new dimension to journaling, offering a unique way to share your personal stories and experiences with up to 30 selected individuals, while keeping your individual entries private and secure.</p>\n</blockquote>\n<p>Awesome! When I was writing the post <a href=\"/notes/private-social-media\">Private Social Media</a> yesterday, I wasn't aware that these had already launched. I knew they were in beta but it's great to see they're now generally available. I'll have to give them a try.</p>\n</div>\n","tags":["dayone","automattic","journal"]},{"title":"Introducing Stable LM 2 1.6B","contentType":"reshare","date":"2024-01-23 09:36","url":"/reshare/introducing-stable-lm-2-1-6B/","content":"<div class=\"response-target\" >→ <a href=\"https://stability.ai/news/introducing-stable-lm-2\" >https://stability.ai/news/introducing-stable-lm-2</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Stable LM 2 1.6B is a state-of-the-art 1.6 billion parameter small language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This model's compact size and speed lower hardware barriers, allowing more developers to participate in the generative AI ecosystem.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In addition to the pre-trained and instruction-tuned version, we release the last checkpoint before the pre-training cooldown. We include optimizer states to facilitate developers in fine-tuning and experimentation. Data details will be provided in the upcoming technical report.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Stable LM 2 1.6B can be used now both commercially and non-commercially with a Stability AI Membership &amp;amp; you can test the model on <a href=\"https://huggingface.co/spaces/stabilityai/stablelm-2-1_6b-zephyr\">Hugging Face</a>.</p>\n</blockquote>\n</div>\n","tags":["ai","stabilityai","slm","llm","smalllanguagemodel"]},{"title":"The top five RSS readers for keeping up with your news feeds","contentType":"reshare","date":"2024-01-22 20:49","url":"/reshare/top-five-rss-feed-readers-verge/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/24036427/rss-feed-reader-best\" >https://www.theverge.com/24036427/rss-feed-reader-best</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>RSS readers allow you to collect the articles of specific sources in one app, making it a lot easier to find the content you’re interested in without crawling through a lot of noise.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Whatever RSS feed reader you choose, it’s worth it to try at least one or two. This way, you can keep up with news from your favorite sources without depending on the chaos that is your email account or the random opinions from TikTok.</p>\n</blockquote>\n<p>Great overview of the various RSS feed readers out there.</p>\n</div>\n","tags":["rss","news","feeds","indieweb","protocols"]},{"title":"The Vision Pro’s first killer app is the web, whether Apple likes it or not","contentType":"reply","date":"2024-01-20 13:29","url":"/reply/vision-pros-most-important-app-safari/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/1/20/24044343/apple-vision-pro-safari-killer-app\" >https://www.theverge.com/2024/1/20/24044343/apple-vision-pro-safari-killer-app</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>it’s increasingly clear that the early success of the Vision Pro, and much of the answer to the question of what this headset is actually for, will come from a single app: Safari.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>That’s right, friends. Web browsers are back.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...at least at first, the open web is Apple’s best chance to make its headset a winner. Because at least so far, it seems developers are not exactly jumping to build new apps for Apple’s new platform.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Some of the high-profile companies that have announced they’re not yet building apps for the Vision Pro and its visionOS platform — Netflix, Spotify, YouTube, and others — are the very same ones that have loudly taken issue with how Apple runs the App Store.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>But what if you don’t need the App Store to reach Apple users anymore? All this corporate infighting has the potential to completely change the way we use our devices, starting with the Vision Pro.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...we’ve all stopped opening websites and started tapping app icons, but the age of the URL might be coming back.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>If you believe the open web is a good thing, and that developers should spend more time on their web apps and less on their native ones, this is a big win for the future of the internet.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The problem is, it’s happening after nearly two decades of mobile platforms systematically downgrading and ignoring their browsing experience...Mobile platforms treat browsers like webpage viewers, not app platforms, and it shows.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>There are some reasons for hope, though...the company appears to be still invested in making Safari work.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Safari for visionOS will also come with some platform-specific features: you’ll be able to open multiple windows at the same time and move them all around in virtual space.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>With a good browser and powerful PWAs, many users might mostly not notice the difference between opening the Spotify app and going to Spotify.com. That’s a win for the whole web.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>here’s the real question for Apple: which is more important, getting the Vision Pro off to a good start or protecting the sanctity of its App Store control at all costs? As Apple tries to create a platform shift to face computers, I’m not sure it can have it both ways.</p>\n</blockquote>\n<p>Great article by <a href=\"https://www.theverge.com/authors/david-pierce\">David Pierce</a>. As part of my website stats, I should probably start also counting authors I reference since many of the articles from the Verge I've previously linked to are written by David.</p>\n<p>As someone who <a href=\"/responses/replacing-apps-websites-kingett\">accesses services - &amp;quot;apps&amp;quot; - primarily through the web browser on desktop</a>, this is exciting to see. While native apps have their advantages, the types of cross-platform connected experiences that can be delivered through the browser can't be ignored. First-class support for browsers in various platforms can only make these experiences even better. With more folks building their own platforms on the web on top of open standards that have been around for decades, I'm excited for the future of the web.</p>\n</div>\n","tags":["apple","safari","visionpro","web","browser","vr","xr","mr","ar","indieweb","openweb"]},{"title":"How Discord Serves 15-Million Users on One Server","contentType":"reshare","date":"2024-01-17 20:40","url":"/reshare/how-discord-serves-15-million-users-using-one-server/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.bytebytego.com/p/how-discord-serves-15-million-users\" >https://blog.bytebytego.com/p/how-discord-serves-15-million-users</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In early summer 2022, the Discord operations team noticed unusually high activity on their dashboards. They thought it was a bot attack, but it was legitimate traffic from MidJourney - a new, fast-growing community for generating AI images from text prompts.\n<br>\nTo use MidJourney, you need a Discord account. Most MidJourney users join one main Discord server. This server grew so quickly that it soon hit Discord’s old limit of around 1 million users per server.\n<br>\nThis is the story of how the Discord team creatively solved this challenge.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Discord’s real-time messaging backend is built with Elixir. Elixir runs on the BEAM virtual machine. BEAM was created for Erlang - a language optimized for large real-time systems requiring rock-solid reliability and uptime.\n<br>\nA key capability BEAM provides is extremely lightweight parallel processes. This enables a single server to efficiently run tens or hundreds of thousands of processes concurrently.\n<br>\nElixir brings friendlier, Ruby-inspired syntax to the battle-tested foundation of BEAM. Combined they make it much easier to program massively scalable, fault-tolerant systems.\n<br>\nSo by leveraging BEAM's lightweight processes, the Elixir code powering Discord can &amp;quot;fan out&amp;quot; messages to hundreds of thousands of users around the world concurrently. However, limits emerge as communities grow larger.</p>\n</blockquote>\n</div>\n","tags":["beam","erlang","elixir","discord","scale","enterprise","chat","communities"]},{"title":"Podcasting is in its YouTube era","contentType":"reshare","date":"2024-01-17 20:24","url":"/reshare/podcast-youtube-era/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/1/9/24032155/youtube-podcast-rss-spotify-apple-audacy-bankruptcy\" >https://www.theverge.com/2024/1/9/24032155/youtube-podcast-rss-spotify-apple-audacy-bankruptcy</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, YouTube at very long last debuts RSS integration.</p>\n</blockquote>\n<p>This means more hosts saying &amp;quot;...or wherever you get your podcasts.&amp;quot; 🙂</p>\n<p><a href=\"https://support.google.com/youtube/answer/13973017\">Support Post</a></p>\n</div>\n","tags":["rss","podcasts","youtube"]},{"title":"Each Facebook User is Monitored by Thousands of Companies","contentType":"reshare","date":"2024-01-17 20:09","url":"/reshare/facebook-users-monitored-thousands-companies-markup/","content":"<div class=\"response-target\" >→ <a href=\"https://themarkup.org/privacy/2024/01/17/each-facebook-user-is-monitored-by-thousands-of-companies-study-indicates\" >https://themarkup.org/privacy/2024/01/17/each-facebook-user-is-monitored-by-thousands-of-companies-study-indicates</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>By now most internet users know their online activity is constantly tracked.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>But what is the scale of this surveillance? Judging from data collected by Facebook and <a href=\"https://innovation.consumerreports.org/wp-content/uploads/2024/01/CR_Who-Shares-Your-Information-With-Facebook.pdf\">newly described in a unique study</a> by non-profit consumer watchdog Consumer Reports, it’s massive, and examining the data may leave you with more questions than answers.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Using a panel of 709 volunteers who shared archives of their Facebook data, Consumer Reports found that a total of 186,892 companies sent data about them to the social network. On average, each participant in the study had their data sent to Facebook by 2,230 companies. That number varied significantly, with some panelists’ data listing over 7,000 companies providing their data.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n\n<br>\nThe data examined by Consumer Reports in this study comes from two types of collection: events and custom audiences. Both categories include information about what people do outside of Meta’s platforms.\n<br>\nCustom audiences allow advertisers to upload customer lists to Meta, often including identifiers like email addresses and mobile advertising IDs...\n<br>\nThe other category of data collection, “events,” describes interactions that the user had with a brand, which can occur outside of Meta’s apps and in the real world. Events can include visiting a page on a company’s website, leveling up in a game, visiting a physical store, or purchasing a product...\n</blockquote>\n<blockquote class=\"blockquote\">\n\n<br>\nFacebook users can browse through the list of companies that have sent their data to Facebook by going to: [https://accountscenter.facebook.com/info_and_permissions](https://accountscenter.facebook.com/info_and_permissions)\n</blockquote>\n</div>\n","tags":["socialmedia","privacy","facebook","meta","surveillance"]},{"title":"Talking about Open Source LLMs on Oxide and Friends","contentType":"reshare","date":"2024-01-17 20:03","url":"/reshare/talking-open-source-llms-oxide-friends-willison/","content":"<div class=\"response-target\" >→ <a href=\"https://simonwillison.net/2024/Jan/17/oxide-and-friends/\" >https://simonwillison.net/2024/Jan/17/oxide-and-friends/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>I recorded <a href=\"https://oxide.computer/podcasts/oxide-and-friends/1692510\">an episode</a> of the Oxide and Friends podcast on Monday, talking with Bryan Cantrill and Adam Leventhal about Open Source LLMs.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n\n<br>\nThis technology is clearly extremely important to the future of all sorts of things that we want to do.\n<br>\nI am totally on board with it. There are people who will tell you that it’s all hype and bluster. I’m over that. This stuff’s real. It’s really useful.\n<br>\nIt is far too important for a small group of companies to completely control this technology. That would be genuinely disastrous. And I was very nervous that was going to happen, back when it was just OpenAI and Anthropic that had the only models that were any good, that was really nerve-wracking.\n<br>\nToday I’m not afraid of that at all, because there are dozens of organizations now that have managed to create one of these things...\n</blockquote>\n<blockquote class=\"blockquote\">\n\n<p>One of the most exciting things for me about this technology is that it’s a teaching assistant that is always available to you.\n<br>\nYou know that thing where you’re learning—especially in a classroom environment—and you miss one little detail and you start falling further and further behind everyone else because there was this one little thing you didn’t quite catch, and you don’t want to ask stupid questions?\n<br>\nYou can ask stupid questions of ChatGPT anytime you like and it can help guide you through to the right answer.\n<br>\nThat’s kind of a revelation.</p>\n</blockquote>\n</div>\n","tags":["ai","podcast","opensource","llm"]},{"title":"Notion’s new calendar app is designed to keep your meetings organized","contentType":"reshare","date":"2024-01-17 19:33","url":"/reshare/notion-cron-calendar-app/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2024/1/17/24041330/notion-calendar-app\" >https://www.theverge.com/2024/1/17/24041330/notion-calendar-app</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>After acquiring Cron in 2022, Notion is bringing the calendar app fully into its all-in-one workspace.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The big new feature coming with the rebranding is Notion integration. If you or your company uses Notion, you’ll be able to create or link Notion documents inside a calendar invite. If you have a database filled with due dates, you can add that as a calendar to Notion Calendar. It sounds like a much better way to handle agendas and notes than sending them around before and after a meeting or hunting for them in your Slack. Putting everything in the calendar event is a good move.</p>\n</blockquote>\n<p>This is one of the reasons I like org-mode in Emacs. Being able to annotate documents with timestamps and deadlines that show up and you can organize inside the Agenda view is so powerful. The integrations and learning curve is steeper compared to a tool like Notion but I find it simple and powerful enough for GTD-style workflows, I'd have a hard time moving. I have yet to use AnyType, so maybe <a href=\"/notes/anytype-local-only\">after trying that</a>, I choose to shift some of my workflows there.</p>\n</div>\n","tags":["calendar","notion","productivity","gtd","software","app","pkm"]},{"title":"OpenMentions","contentType":"reshare","date":"2024-01-16 20:21","url":"/reshare/openmentions/","content":"<div class=\"response-target\" >→ <a href=\"https://openmentions.com/\" >https://openmentions.com/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>OpenMentions is a project designed to use Webmentions and ActivityPub for topical content discovery. The site is organised along the lines of a hierarchy of topics going from broad to fine. This we call OpenTopic – the idea being that many sites could host the full collection of topics so that the loss of any one site is not the loss of all topics.<br />\n<br>\nThe intention is that this site should own nothing and that topic hierarchies are organic and discoverable.</p>\n</blockquote>\n</div>\n","tags":["webmentions","indieweb","activitypub","online"]},{"title":"LeftoverLocals: Listening to LLM responses through leaked GPU local memory","contentType":"reshare","date":"2024-01-16 20:16","url":"/reshare/lefoverlocals-llm-responses-leaked-gpu-memory/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/\" >https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We are disclosing LeftoverLocals: a vulnerability that allows recovery of data from GPU local memory created by another process on Apple, Qualcomm, AMD, and Imagination GPUs. LeftoverLocals impacts the security posture of GPU applications as a whole, with particular significance to LLMs and ML models run on impacted GPU platforms. By recovering local memory—an optimized GPU memory region—we were able to build a PoC where an attacker can listen into another user’s interactive LLM session (e.g., llama.cpp) across process or container boundaries</p>\n</blockquote>\n</div>\n","tags":["security","llm","ai"]},{"title":"Easy Ways to Improve Your Privacy & Security in 2024","contentType":"reshare","date":"2024-01-15 16:54","url":"/reshare/easy-ways-improve-privacy-security-2024/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.thenewoil.org/easy-ways-to-improve-your-privacy-and-security-in-2024\" >https://blog.thenewoil.org/easy-ways-to-improve-your-privacy-and-security-in-2024</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Every year, I like to remind everyone to go back to the basics. For those who are new to privacy and security and may be trying to create some new, positive habits, this serves as a great entry point. For veteran privacy enthusiasts, the basics form our foundation for more advanced techniques later, making it imperative to ensure we cover all those bases. So in that spirit, let’s all pause – wherever we are in our privacy journeys – to do a quick check and make sure we’ve got the basics covered. If you’re one of those new people I mentioned, welcome! But also know that this post is packed with information, so try not to get overwhelmed. Maybe bookmark this post and do one thing per day or something like that.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Strong Passwords...<br />\n<br>\nMulti-Factor Authentication (MFA)...<br />\n<br>\nRegular Software Updates...<br />\n<br>\nSecure Your Wi-Fi Network...<br />\n<br>\nBe Cautious with Communications...<br />\n<br><br />\nReview App Permissions...<br />\n<br>\nReview Your Account Settings...<br />\n<br>\nSecure Browsing Habits...<br />\n<br>\nDevice Security...<br />\n<br>\nReview Financial Statements...<br />\n<br>\nEducate Yourself...</p>\n</blockquote>\n</div>\n","tags":["privacy","security"]},{"title":"Hands-on - Clicks for iPhone","contentType":"reply","date":"2024-01-15 16:45","url":"/reply/hands-on-clicks-iphone/","content":"<div class=\"response-target\" >→ <a href=\"https://9to5mac.com/2024/01/12/clicks-iphone-hands-on/\" >https://9to5mac.com/2024/01/12/clicks-iphone-hands-on/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Smartphones and physical keyboards aren’t a combination we think of often, but Clicks for iPhone is trying to bring that back with a new keyboard case that’s extremely good.</p>\n</blockquote>\n<p>Wishful thinking on my end but I'd buy a Blackberry-like device. Not a &amp;quot;smartphone&amp;quot;, but an internet-connected device with all the phone capabilities that has a physical keyboard.</p>\n</div>\n","tags":["smarphone","keyboard","iphone","featurephone"]},{"title":"Goblin - Tumblr meets the Fediverse","contentType":"reshare","date":"2024-01-15 16:14","url":"/reshare/goblin-tumblr-fediverse/","content":"<div class=\"response-target\" >→ <a href=\"https://goblin.band/\" >https://goblin.band/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Goblin band is an attempt to replicate the colletive creative energy that happens on tumblr and take it to the fediverse</p>\n</blockquote>\n<p><strong>Repo</strong>: <a href=\"https://github.com/johnHackworth/goblin\">https://github.com/johnHackworth/goblin</a></p>\n</div>\n","tags":["tumblr","fediverse","activitypub","automattic","firefish","socialmedia","opensource","community","decentralization","federation"]},{"title":"Fly - New Chicano Batman out 1/23","contentType":"reshare","date":"2024-01-15 12:36","url":"/reshare/chicano-batman-fly-01-2024/","content":"<div class=\"response-target\" >→ <a href=\"https://twitter.com/ChicanoBatman/status/1746940313678299500\" >https://twitter.com/ChicanoBatman/status/1746940313678299500</a></div><div class=\"response-content\" ><p>Starting off the year right with new <a href=\"https://chicanobatman.com/\">Chicano Batman</a>!</p>\n<blockquote class=\"blockquote\">\n<p>“Fly” is yours 1/23 🕊 And this is just the beginning!</p>\n<p><img src=\"https://pbs.twimg.com/media/GDqtSgWa8AAf7by?format=jpg&amp;amp;name=small\" class=\"img-fluid\" alt=\"A closeup image of a hand wearing a ring that says FLY\" /></p>\n</blockquote>\n</div>\n","tags":["music","chicanobatman"]},{"title":"FLOSS Weekly is now on Hackaday!","contentType":"reshare","date":"2024-01-15 12:15","url":"/reshare/floss-weekly-now-on-hackaday/","content":"<div class=\"response-target\" >→ <a href=\"https://hackaday.com/2023/12/20/floss-weekly-episode-762-spilling-the-tea/\" >https://hackaday.com/2023/12/20/floss-weekly-episode-762-spilling-the-tea/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We’re excited to announce that Hackaday is the new home of FLOSS Weekly, a long-running podcast about free, libre, and open-source software! The TWiT network hosted the podcast for an incredible seventeen years, but due to some changes on their end, they recently had to wind things down. They were gracious enough to let us pick up the torch, with Jonathan Bennett now taking over hosting duties.</p>\n</blockquote>\n<p>That didn't take long. Last month <a href=\"/responses/floss-weekly-ends-twit-start-of-an-era/\">I learned FLOSS Weekly was ending on the TWiT network</a>. It's great to see it has found a new home in Hackaday! Time to update the RSS feeds and <a href=\"/collections/podroll/\">podroll</a>.</p>\n</div>\n","tags":["podcast","floss","hackaday","opensource","technology","internet"]},{"title":"Migrating from Substack to self-hosted Ghost","contentType":"reshare","date":"2024-01-14 16:11","url":"/reshare/migrating-substack-self-hosted-ghost-white/","content":"<div class=\"response-target\" >→ <a href=\"https://citationneeded.news/substack-to-self-hosted-ghost/\" >https://citationneeded.news/substack-to-self-hosted-ghost/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>I have found myself with a roughly $103/mo setup...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>However, more important to me than the exact price is the degree of control I have over my own not-a-platform...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I realize that this is...a lot. If you are a newsletter writer looking to flee the Substack ship, please don't let this discourage you.</p>\n</blockquote>\n<p>Love seeing this! Whether through self-hosted or hosted options, I hope more people get to experience the benefits of owning their own platform.</p>\n</div>\n","tags":["substack","selfhost","indieweb","ghost"]},{"title":"More than an OpenAI Wrapper: Perplexity Pivots to Open Source","contentType":"reshare","date":"2024-01-13 09:43","url":"/reshare/perplexity-ai-pivots-open-source/","content":"<div class=\"response-target\" >→ <a href=\"https://thenewstack.io/more-than-an-openai-wrapper-perplexity-pivots-to-open-source/\" >https://thenewstack.io/more-than-an-openai-wrapper-perplexity-pivots-to-open-source/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...Perplexity has become a surprisingly strong player in a market otherwise dominated by OpenAI, Microsoft, Google and Meta.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>At its core, Perplexity is a search engine.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...over the past year, Perplexity has evolved rapidly. It now has its own search index and has built its own LLMs based on open source models. They’ve also begun to combine their proprietary technology products. At the end of November, Perplexity announced two new “online LLMs” — LLMs combined with a search index — called pplx-7b-online and pplx-70b-online. They were built on top of the open source models mistral-7b and llama2-70b.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Using open source models has been critical for the growth of Perplexity.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...the default Perplexity model still relies on GPT 3.5 (and a dash of LLaMA-2). But the intention is to move away from that long-standing reliance on OpenAI for its base model.</p>\n</blockquote>\n</div>\n","tags":["ai","perplexity","opensource","search","llm"]},{"title":"Replacing apps with websites","contentType":"reply","date":"2024-01-09 09:37","url":"/reply/replacing-apps-websites-kingett/","content":"<div class=\"response-target\" >→ <a href=\"https://robertkingett.com/posts/6421/\" >https://robertkingett.com/posts/6421/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>I woke up today...and decided I was going to delete all the apps off my iPhone.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>If I didn’t need it to do a specialized function? Gone, poof. I decided to just use websites and bookmarks instead.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The websites worked just fine.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...websites are just badly designed these days, especially with using a screen reader on a mobile device. I can’t quite describe it, because mobile web browsers aren’t really well designed either, so it makes the website worse because it doesn’t even render all elements as smoothly as on desktop.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Even though there are problems, I’m honestly glad I deleted almost all my apps off my phone and started to pin websites to my home screen more. For one thing, it cuts down on notifications. it’s super freeing to not get a random notification because you didn’t open the app in a day, so the app pings you to say hey I’m still here please pay attention to me, I feel lonely, and nobody will give me animals to snuggle with.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>With pinned websites, my phone is faster and my home screen is much more organized as well.</p>\n</blockquote>\n<p>I did this a few years ago. Websites work just fine for almost everything I need to do. The main benefits I noticed:</p>\n<ul>\n<li>Less distractions</li>\n<li>More real estate when viewing websites on laptop / desktop</li>\n<li>Less apps to drain the battery when running on the background</li>\n<li>More restricted access / permissions (i.e. websites don't need unrestricted access to my contacts or other information on my phone)</li>\n</ul>\n<p>Better PWA support on mobile platforms would go a long way here to better balance between apps and websites. In the meantime though, pinning websites and websites overall work just fine for most things I need to do day-to-day.</p>\n</div>\n","tags":["apps","websites","internet","mobile"]},{"title":"A blog post can be any length","contentType":"reply","date":"2024-01-07 22:37","url":"/reply/blog-posts-can-be-any-length-matt/","content":"<div class=\"response-target\" >→ <a href=\"https://ma.tt/2024/01/birthday-gift/\" >https://ma.tt/2024/01/birthday-gift/</a></div><div class=\"response-content\" ><p><img src=\"https://github.com/lqdev/luisquintanilla.me/assets/11130940/d42cc57e-b882-4042-b2be-e68810105c3d\" class=\"img-fluid\" alt=\"A comment thread between Luis and Matt\" /></p>\n<p><img src=\"https://media.giphy.com/media/26FLgGTPUDH6UGAbm/giphy.gif\" class=\"img-fluid\" alt=\"A GIF of a man dressed in black pointing up to blinking overhead text &amp;quot;THIS&amp;quot;\" /></p>\n</div>\n","tags":["writing","blog","web","internet"]},{"title":"Own Your RSS Links","contentType":"reply","date":"2024-01-07 15:48","url":"/reply/own-your-rss-links/","content":"<div class=\"response-target\" >→ <a href=\"https://heydingus.net/blog/2024/1/please-own-your-rss-links\" >https://heydingus.net/blog/2024/1/please-own-your-rss-links</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...owning the address where your audience finds you is important. It allows you to be mobile, nimble, and without attached strings. It helps you show off all the things and places you want folks to see because you can put all these URLs on your /feeds page. It’s user-friendly in more ways than one (pretty cool how you can make all those URLs human-readable, huh?).</p>\n<p>...it means your audience never has to think about how they’re going to get your stuff.</p>\n</blockquote>\n<p>This is a great idea. I've written before about <a href=\"/notes/own-your-links-domain-verification\">owning your links</a>. Today, that's how I expose many of my links in the <a href=\"/contact\">contact page</a>. For example, to access my Mastodon profile, instead of going to the actual URL, you can just visit <a href=\"/mastodon\">lqdev.me/mastodon</a> which redirects to the actual URL. If tomorrow I choose to change where and how my Mastodon presence is hosted, the URL doesn't have to change. However, I haven't done the same for my RSS links. Recently I've been thinking about restructuring my website, specifically my microblog feed which includes notes and responses. Today, the RSS urls are coupled to the folder structure on my website which is subject to change and isn't flexible. By setting up more user-friendly and stable RSS urls through redirection, that wouldn't be an issue and readers wouldn't have to change the RSS URL they use.</p>\n</div>\n","tags":["rss","indieweb"]},{"title":"AdventureTaco - Favorite Photos 2023 Edition","contentType":"reshare","date":"2024-01-06 21:33","url":"/reshare/favorite-photos-2023-adventuretaco/","content":"<div class=\"response-target\" >→ <a href=\"https://adventuretaco.com/favorite-photos-2023-edition/\" >https://adventuretaco.com/favorite-photos-2023-edition/</a></div><div class=\"response-content\" ><p><img src=\"https://live.staticflickr.com/65535/53444961109_ca25a7de02_h.jpg\" class=\"img-fluid\" alt=\"A Toyota Tacoma Driving on a dirt road towards mountains in the background\" /></p>\n</div>\n","tags":["landscapes","photography","overlanding","adventure","photos","desert","mountain"]},{"title":"Birthday Gift - I want you to blog","contentType":"reshare","date":"2024-01-03 00:16","url":"/reshare/i-want-you-to-blog-mullenweg/","content":"<div class=\"response-target\" >→ <a href=\"https://ma.tt/2024/01/birthday-gift/\" >https://ma.tt/2024/01/birthday-gift/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>the gift I most want for my 40th is something everyone can do.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I want you to blog.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Publish a post. About anything! It can be long or short, a photo or a video, maybe a quote or a link to something you found interesting. Don’t sweat it. Just blog. Share something you created, or amplify something you enjoyed. It doesn’t take much. The act of publishing will be a gift for you and me.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>That’s it! No wrapping paper or bows. Just blogs and blogs and blogs, each unique and beautiful in its own way.</p>\n</blockquote>\n</div>\n","tags":["blogging","wordpress","writing","community"]},{"title":"The Year For Blogging To Pump Up The Volume","contentType":"reshare","date":"2024-01-02 18:36","url":"/reshare/blogging-pump-up-volume/","content":"<div class=\"response-target\" >→ <a href=\"https://bix.blog/2024/01/01/the-year-for-blogging-to-pump-up-the-volume/\" >https://bix.blog/2024/01/01/the-year-for-blogging-to-pump-up-the-volume/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>There’s been a lot of pontificating lately that the web is ripe for a blogging renaissance, wishing for it to be true. Much of it from people who don’t seem to notice that’s it’s already begun. Maybe they don’t anymore know quite where to look. Maybe the sorts of blogging they’re seeing isn’t what they mean. (To the blognoscenti, do things like “wordvomits” count?) If you haven’t seen it, either, that’s okay. All you have to do is choose to be a part of it. There’s never been a better time: those who managed to monopolize our attentions and keep too many of us chattering for a few hundred characters at a time to the benefit of advertisers are losing their relevance.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I’m not one for making personal resolutions, but let me suggest one on behalf of the blogosphere: this is the year we pump up the volume.</p>\n</blockquote>\n</div>\n","tags":["blogging","community","writing","movie","radio"]},{"title":"Blogs + RSS - best decentralized social media we ever had","contentType":"reply","date":"2023-12-30 00:30","url":"/reply/blogs-rss-decentralized-social-media-lemmer-webber/","content":"<div class=\"response-target\" >→ <a href=\"https://octodon.social/@cwebber/111647596861000656\" >https://octodon.social/@cwebber/111647596861000656</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Bring back self-hosted blogs, reinstall a feed reader, make your feed icon prominent on your blog.  Blogs + Atom/RSS is the best decentralized social media system we've ever had!<br />\n<br>\nAnd yes I am saying that as co-author of ActivityPub: self hosted blogs is the best decentralized social networking we've had</p>\n</blockquote>\n<p>💯 💯 💯 💯 💯 💯</p>\n</div>\n","tags":["rss","blog","decentralized","socialmedia","blogging","mastodon","fediverse","indieweb","atom","feed"]},{"title":"What are farm animals thinking?","contentType":"reply","date":"2023-12-27 14:57","url":"/reply/what-farm-animals-think-orwell/","content":"<div class=\"response-target\" >→ <a href=\"https://www.science.org/content/article/not-dumb-creatures-livestock-surprise-scientists-their-complex-emotional-minds\" >https://www.science.org/content/article/not-dumb-creatures-livestock-surprise-scientists-their-complex-emotional-minds</a></div><div class=\"response-content\" ><p>I think I've read about this somewhere before...</p>\n<blockquote class=\"blockquote\">\n<p>“All animals are equal, but some animals are more equal than others.”\n― George Orwell, Animal Farm</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>“Four legs good, two legs bad.”\n― George Orwell, Animal Farm</p>\n</blockquote>\n</div>\n","tags":["science","animals","farm","orwell","animallfarm","meme"]},{"title":"A Purple Life - 2023 Goals and Accomplishments","contentType":"reply","date":"2023-12-24 18:05","url":"/reply/2023-goals-accomplishments-purple-life/","content":"<div class=\"response-target\" >→ <a href=\"https://apurplelife.com/2023/12/19/2023-goals-accomplishments/\" >https://apurplelife.com/2023/12/19/2023-goals-accomplishments/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>My posts have been...including more reviews of fancy travel hacked flights, tours and slow travel locations. Possibly as a result of this shift in topic – or possibly simply because blogging seems to be on its way out according to a few of my blogging peers – my comments section has been quieter lately. I talked about this in one of my monthly recaps with the spin that I didn’t realize I had come to rely on getting at least one comment per post to know that people (and not just bots 🙂 ) were reading my words and they weren’t floating into an abyss.<br />\n<br>\nI didn’t want to be reliant on external validation when I had written this blog without it being public for years, and hadn’t realized I had come to rely on anything but the joy I get from writing it. So I was trying to grow after realizing that not receiving comments on multiple posts in a row bothered me for some reason. I’m going to do my best to not rely on that kind of feedback going forward and will continue to blog for the main reason I always have: for myself 🙂 .<br />\n<br>\nI also followed a reader suggestion to add a “Like” button at the bottom of my posts (it’s after the “Share This” section and before the “Related” articles section) because readers said they don’t necessarily have something they want to comment, but that a Like button would help show there is still a human reading. All fair 🙂 .</p>\n</blockquote>\n<p>🙋‍♂️ there's still a human reading your posts on this end 🙂</p>\n<p>Also, today I learned there's a Guineafowl Pufferfish.</p>\n<p><img src=\"https://apurplelife.com/wp-content/uploads/2023/12/GFPuffer-copy.png\" class=\"img-fluid\" alt=\"Picture of a Guineafowl Pufferfish\" /></p>\n</div>\n","tags":["blogging","review","goals"]},{"title":"Bluesky now supports RSS","contentType":"reshare","date":"2023-12-23 20:59","url":"/reshare/bluesky-rss-support/","content":"<div class=\"response-target\" >→ <a href=\"https://bsky.app/profile/bsky.app/post/3kh5rjl6bgu2i\" >https://bsky.app/profile/bsky.app/post/3kh5rjl6bgu2i</a></div><div class=\"response-content\" ><p>I'm not as active on there, but feel free to <a href=\"https://bsky.app/profile/did:plc:pme7qquljcdx6i4zyawoxypd/rss\">subscribe to my Bluesky feed</a> wherever you subscribe to feeds.</p>\n<p>Also, if you have any feed recommendations, <a href=\"/contact\">let me know</a>.</p>\n<blockquote class=\"blockquote\">\n<p>📢 1.60 is rolling out now (3/5)</p>\n<p>RSS feeds for profiles!</p>\n<p>Access your posts via RSS by pasting your profile link into your RSS feed reader and it will automatically be discovered.</p>\n</blockquote>\n</div>\n","tags":["bluesky","rss","socialmedia"]},{"title":"Conor McGregor pitching Zune and Windows Phone to Cristiano Ronaldo","contentType":"reply","date":"2023-12-23 20:23","url":"/reply/mcgregor-ronaldo-zune-windows-phone-meme/","content":"<div class=\"response-target\" >→ <a href=\"https://twitter.com/HappyPunch/status/1738661281236672833\" >https://twitter.com/HappyPunch/status/1738661281236672833</a></div><div class=\"response-content\" ><p><img src=\"https://pbs.twimg.com/amplify_video_thumb/1738661229822967808/img/nmYicHlHX6lR_pha.jpg\" class=\"img-fluid\" alt=\"Conor McGregor speaking to someone with Cristiano Ronaldo next to him\" /></p>\n<p>What was probably being said according to Copilot 😂</p>\n<blockquote class=\"blockquote\">\n<p>Cristiano, you need to listen to me, mate. You're wasting your time and money on those Apple products. They're overpriced, overrated, and outdated. They're nothing compared to the real champions of technology: Windows Phone and Zune. Trust me, I know what I'm talking about. I've used them both, and they're the best phone and MP3 player ever made. Let me tell you why. First of all, they had a sleek design, a smooth interface, and a unique tile system that was way ahead of its time. They were also compatible with Microsoft's ecosystem, which gave you access to Office, Xbox, and Bing. And don't get me started on the Zune Pass, which let you stream and download unlimited music for a low monthly fee. You can't beat that, mate. Second of all, they had no apps, no games, no support, and no market share. Wait, what? No, no, no, that's not a bad thing. That's a good thing. That means they were exclusive, elite, and original. They were not for the masses, they were for the class. They were not for the sheep, they were for the lions. They were not for the followers, they were for the leaders. They were not for the weak, they were for the strong. They were not for you, they were for me. And they can be for you too, if you have the courage, the vision, and the taste to switch to Windows Phone and Zune. I'm telling you, Cristiano, you're missing out on the best phone and MP3 player ever made. And I'm willing to prove it to you. I'll bet you a million euros that if you try them for a week, you'll never go back to your Apple products. What do you say? Are you man enough to take the challenge? Or are you too scared to face the truth? Come on, Cristiano, don't be a coward. Don't be a fool. Don't be a loser. Be a winner. Be a legend. Be a Windows Phone and Zune user.</p>\n</blockquote>\n</div>\n","tags":["windowsphone","zune","ai","copilot","meme"]},{"title":"Classic Palmtop","contentType":"reshare","date":"2023-12-22 09:28","url":"/reshare/classic-palmtop-concept/","content":"<div class=\"response-target\" >→ <a href=\"https://www.artstation.com/artwork/aoDzL0\" >https://www.artstation.com/artwork/aoDzL0</a></div><div class=\"response-content\" ><p><img src=\"https://cdna.artstation.com/p/files/images/images/070/172/228/large/michal-kalisz-classicpalmtop-xen200-mk-scene07.jpg?1701906705\" class=\"img-fluid\" alt=\"Cyberdeck radio concept art rendering with display closed front and back profile\" /></p>\n<p><img src=\"https://cdnb.artstation.com/p/files/images/images/070/172/345/large/michal-kalisz-classicpalmtop-xen200-mk-bright-03.jpg?1701907028\" class=\"img-fluid\" alt=\"Cyberdeck radio concept art rendering with display open and keyboard showing\" /></p>\n<p><img src=\"https://media.giphy.com/media/cALkoAIov3Y9a/giphy.gif\" class=\"img-fluid\" alt=\"Robert Downey Jr. as Tony Stark saying I need it\" /></p>\n</div>\n","tags":["concept","tech","cyberpunk"]},{"title":"Proton Mail versus Tuta (Tutanota) encryption","contentType":"reply","date":"2023-12-21 21:18","url":"/reply/proton-mail-vs-tutanota-encryption/","content":"<div class=\"response-target\" >→ <a href=\"https://proton.me/blog/proton-vs-tuta-encryption\" >https://proton.me/blog/proton-vs-tuta-encryption</a></div><div class=\"response-content\" ><p><img src=\"https://media.giphy.com/media/gl0mkIZOW6Nwc/giphy.gif\" class=\"img-fluid\" alt=\"GIF of comedian Bill Hader eating popcorn\" /></p>\n</div>\n","tags":["protonmail","tutanota","encryption"]},{"title":"Phi-2 now on HuggingFace","contentType":"reshare","date":"2023-12-20 18:37","url":"/reshare/phi-2-huggingface/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/microsoft/phi-2\" >https://huggingface.co/microsoft/phi-2</a></div><div class=\"response-content\" ><p>When <a href=\"/responses/microsoft-phi-2\">Phi-2 initially released</a>, it was on the Azure AI Studio Model Catalog. It's nice to see it's now in HuggingFace as well.</p>\n</div>\n","tags":["opensource","ai","phi","languagemodel","smalllanguagemodel","huggingface","transformers","microsoft"]},{"title":"Spritely and Veilid: Exciting Projects Building the Peer-to-Peer Web","contentType":"reshare","date":"2023-12-19 11:47","url":"/reshare/spritely-veilid-p2p-web/","content":"<div class=\"response-target\" >→ <a href=\"https://www.eff.org/deeplinks/2023/12/meet-spritely-and-veilid\" >https://www.eff.org/deeplinks/2023/12/meet-spritely-and-veilid</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>While there is a surge in federated social media sites, like Bluesky and Mastodon, some technologists are hoping to take things further than this model of decentralization with fully peer-to-peer applications. Two leading projects, Spritely and Veilid, hint at what this could look like.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Spritely is a framework for building distributed apps that don’t even have to know that they’re distributed. The project is spearheaded by Christine Lemmer-Webber, who was one of the co-authors of the ActivityPub spec that drives the fediverse. She is taking the lessons learned from that work, combining them with security and privacy minded object capabilities models, and mixing it all up into a model for peer to peer computation that could pave the way for a generation of new decentralized tools.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The Veilid project was released at DEFCON 31 in August and has a number of promising features that could lead to it being a fundamental tool in future decentralized systems. Described as a cross between TOR and Interplanetary File System (IPFS), Veilid is a framework and protocol that offers two complementary tools. The first is private routing, which, much like TOR, can construct an encrypted private tunnel over the public internet allowing two devices to communicate with each other without anyone else on the network knowing who is talking to whom...The second tool that Veilid offers is a Distributed Hash Table (DHT), which lets anyone look up a bit of data associated with a specific key, wherever that data lives on the network.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Public interest in decentralized tools and services is growing, as people realize that there are downsides to centralized control over the platforms that connect us all. The past year has seen interest in networks like the fediverse and Bluesky explode and there’s no reason to expect that to change. Projects like Spritely and Veilid are pushing the boundaries of how we might build apps and services in the future. The things that they are making possible may well form the foundation of social communication on the internet in the next decade, making our lives online more free, secure, and resilient.</p>\n</blockquote>\n\n<ul>\n<li><a href=\"https://spritely.institute/\">Spritely Institute Website</a></li>\n<li><a href=\"https://veilid.com/\">Veilid Website</a></li>\n</ul>\n</div>\n","tags":["p2p","veilid","spritely","openweb","web","internet","decentralization"]},{"title":"2023 in social media: the case for the fediverse","contentType":"reshare","date":"2023-12-19 09:26","url":"/reshare/2023-social-media-case-fediverse/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/23990974/social-media-2023-fediverse-mastodon-threads-activitypub\" >https://www.theverge.com/23990974/social-media-2023-fediverse-mastodon-threads-activitypub</a></div><div class=\"response-content\" ><p>Good article from David Pierce. I'd add that many of these platforms (i.e. Mastodon, Lemmy, PeerTube, WordPress) have strong RSS support which offer another degree of freedom from opting out of signing up for any of the platforms but still being able to follow the people and topics you care about. Sure, the experience may not be as rich but it's yet another way for people to participate in the ecosystem.</p>\n<blockquote class=\"blockquote\">\n<p>A new kind of social internet is currently forming. Right now it might still look like “Twitter and Reddit, only different,” but that’s only the very beginning of what’s to come. Hopefully.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I’m convinced we’ll be better off with a hundred different apps for Snapchat or Instagram or X instead of just one...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>It doesn’t make sense that we have a dozen usernames, a dozen profiles, a dozen sets of fans and friends. All that stuff should belong to me, and I should be able to access it and interact with it anywhere and everywhere.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Decentralizing social media can sound like a sort of kumbaya anti-capitalist manifesto: “It’s about openness and sharing, not capitalism, man!” In practice it’s the opposite: it’s a truly free market approach to social networking.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...in a fediverse-dominated world, the way to win is not to achieve excellent lock-in and network effects. The only way to win is to build the best product.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...so far we’re mostly in the “popular app, but federated” phase of this transition.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Almost everything in the fediverse is a one-to-one competitor to an existing platform...Some of these apps are very good! But nearly all of them are differentiated only in that they’re federated.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Let’s be super clear about this: the point of the fediverse is not that it’s federated...Making the “It’s federated!” argument is like making the “It’s better for privacy!” argument: it makes you feel good, and at best it’s a useful tiebreaker, but it doesn’t actually matter. All that matters is the product.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>2023 was the year “fediverse” became a buzzword, 2024 will be the year it becomes an industry. (Hopefully one with a better name, but I’ll get over that.) We’ve spent too long living our lives online in someone else’s spaces. What’s next will belong to all of us. All that’s left to do is start posting.</p>\n</blockquote>\n</div>\n","tags":["fediverse","socialmedia","activitypub","internet","web","openweb","smallweb"]},{"title":"\"Microblogging\" prior art","contentType":"reshare","date":"2023-12-15 11:18","url":"/reshare/microblogging-telegram-jwz/","content":"<div class=\"response-target\" >→ <a href=\"https://mastodon.social/@jwz/111583679963120813\" >https://mastodon.social/@jwz/111583679963120813</a></div><div class=\"response-content\" ><p>Just published a blog post, <a href=\"/posts/ai-1999-1899\">AI like it's 1999 or 1899</a>, inspired by this post from jwz, among other things.</p>\n<p><img src=\"https://files.mastodon.social/media_attachments/files/111/583/682/973/573/497/original/1d93c25bbdc52784.png\" class=\"img-fluid\" alt=\"A meme of a telegram saying F**k You Strong Letter to Follow\" /></p>\n</div>\n","tags":["microblogging","telegram","meme"]},{"title":"FLOSS Weekly ended its run on the TWiT network","contentType":"reply","date":"2023-12-14 21:10","url":"/reply/floss-weekly-ends-twit-start-of-an-era/","content":"<div class=\"response-target\" >→ <a href=\"https://doc.searls.com/2023/12/14/start-of-an-era/\" >https://doc.searls.com/2023/12/14/start-of-an-era/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>After 17 years and 761 episodes, FLOSS Weekly ended its run on the TWiT network yesterday.</p>\n</blockquote>\n<p>Nooo! So sad to hear that FLOSS Weekly is ending, especially after learning that <a href=\"/responses/farewell-privacy-security-osint-podcast/\">The Privacy, Security, and OSINT Show with Michael Bazzell ended as well</a>.</p>\n<p>At least there's some hope at the end of Doc's post which hints at it living on in some form.</p>\n<blockquote class=\"blockquote\">\n<p>By the way, FLOSS Weekly has not slipped below the waves. I expect it will be picked up somewhere else on the Web, and wherever you get your podcasts. (I love that expression because it means podcasting isn’t walled into some giant’s garden.) When that happens, I’ll point to it here.</p>\n</blockquote>\n<p>In any case, it was good while it lasted. Also, there's still <a href=\"https://www.reality2cast.com/\">Reality 2.0</a> where the guests and topics are just as interesting and entertaining.</p>\n</div>\n","tags":["podcast","floss","twit","opensource"]},{"title":"Farewell to The Privacy, Security, and OSINT Show","contentType":"reply","date":"2023-12-13 20:00","url":"/reply/farewell-privacy-security-osint-podcast/","content":"<div class=\"response-target\" >→ <a href=\"https://www.adamsdesk.com/posts/farewell-privacy-security-and-osint-show/\" >https://www.adamsdesk.com/posts/farewell-privacy-security-and-osint-show/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...a farewell episode was released on November 20th, 2023 entitled “My Irish Exit”. It was finally officially confirmed that [The Privacy, Security, and OSINT Show with Michael Bazzell]...has reached an end.</p>\n</blockquote>\n<p>That's unfortunate. I really enjoyed listening to this show and even had it listed in my <a href=\"/collections/podroll/\">podroll</a>. The UNREDACTED magazine had great content as well.</p>\n</div>\n","tags":["podcast","privacy","osint"]},{"title":"Introducing Journaling Suggestions for Day One","contentType":"reshare","date":"2023-12-12 20:49","url":"/reshare/journaling-suggestions-day-one/","content":"<div class=\"response-target\" >→ <a href=\"https://dayoneapp.com/blog/introducing-journaling-suggestions/\" >https://dayoneapp.com/blog/introducing-journaling-suggestions/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>See journaling recommendations inspired by your photos, locations, activities and more. Exclusively for iPhone.</p>\n</blockquote>\n<p>I've been tinkering with Day One the past few months. When paired with their templates, this is a nice addition. Too bad it's iPhone exclusive. Hopefully it makes its way to Android at some point.</p>\n</div>\n","tags":["journaling","dayone"]},{"title":"Aoudaghost, unveiling the lost economic hub of the Sahara","contentType":"reshare","date":"2023-12-12 19:45","url":"/reshare/aoudaghost-aprin/","content":"<div class=\"response-target\" >→ <a href=\"https://wildmanlife.com/aoudaghost-economic-hub-of-the-sahara/\" >https://wildmanlife.com/aoudaghost-economic-hub-of-the-sahara/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Since 2001, the site has been on the UNESCO World Heritage Tentative List.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Today, Aoudaghost is in a state of complete abandonment. The remains of the once-thriving town are concentrated in the area most protected by the wind and sand, with several walls and fortifications yet to be fully englobed by the desert. From the adjacent cliff, the current state of Aoudaghost can be seen in its entirety, but only the mind can imagine the Aoudaghost that served as an economic and cultural hub for the Sahara</p>\n</blockquote>\n</div>\n","tags":["travel","sahara"]},{"title":"Tumblr’s ‘fediverse’ integration is still being worked on","contentType":"reshare","date":"2023-12-11 20:29","url":"/reshare/tumblr-still-working-fediverse-integration/","content":"<div class=\"response-target\" >→ <a href=\"https://techcrunch.com/2023/12/11/tumblrs-fediverse-integration-is-still-being-worked-on-says-owner-and-automattic-ceo-matt-mullenweg/\" >https://techcrunch.com/2023/12/11/tumblrs-fediverse-integration-is-still-being-worked-on-says-owner-and-automattic-ceo-matt-mullenweg/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Despite delays, the plan to connect Tumblr’s blogging site to the wider world of decentralized social media, also known as the “fediverse,” is still on, it seems.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...Mullenweg explained that despite the re-org, which will see many Tumblr employees move to other projects at the end of the year, Automattic did switch someone over to Tumblr to work on the fediverse integration, which will continue in the new year.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>“I remain a huge believer in open standards and user freedom, though I don’t claim to have the truth on which particular standard is better or best, to serve our customers we will support everything we can in good faith to give users more freedom, choice, and avoid lock-in,” [Matt Mullenweg] also said in his AMA.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Mullunweg also noted that a larger effort to migrate Tumblr’s half a billion blogs to WordPress on the backend is something he’s also contemplating in the new year.</p>\n</blockquote>\n</div>\n","tags":["tumblr","socialmedia","fediverse"]},{"title":"Blogging is where it's at, again","contentType":"star","date":"2023-12-06 21:02","url":"/star/blogging-where-its-at-again-mcleod/","content":"<div class=\"response-target\" >→ <a href=\"https://chrismcleod.dev/blog/blogging-is-where-its-at-again/\" >https://chrismcleod.dev/blog/blogging-is-where-its-at-again/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>the blog is the “natural form” of posting on the web: a site of your own, that you control[1] and set your own rules on content and discussion; where you can post whatever you like without worrying about “The Algorithm”</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>For better or for worse, social media opened up the web to a lot more people for a number of reasons...But deep down I feel having your own site is better. For the web, and for you: the writer and the reader.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...stumbling into such a trove of active blogs has enthused me about blogging as a medium again. It’s sparked a thought that through a combination of increased blogging activity, declining platforms, and increasing adoption of open standards to glue everything together, that maybe — just maybe — we can swing the web back towards the blog again.</p>\n</blockquote>\n<p>Agree with many of the points. Also, TIL you could subscribe to OPML feeds.</p>\n</div>\n","tags":["blogging","socialmedia","rss","community"]},{"title":"How I Take and Publish Notes","contentType":"reshare","date":"2023-12-05 21:42","url":"/reshare/take-and-publish-notes-nielsen/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.jim-nielsen.com/2023/how-i-take-and-publish-notes/\" >https://blog.jim-nielsen.com/2023/how-i-take-and-publish-notes/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>99% of the time, this is how my note-taking process goes:</p>\n<ul>\n<li>I’m catching up on my RSS feed (on my phone in the Reeder app)</li>\n<li>I read something that strikes me as interesting, novel, or insightful.</li>\n<li>I copy/paste it as an blockquote into a new, plain-text note in iA writer.</li>\n<li>I copy/paste the link of the article into iA writer.</li>\n<li>I finish reading the article and copy/paste anything else in the article that strikes me.</li>\n<li>I add my own comments in the note as they pop into my head.</li>\n<li>I move on to the next article in my RSS feed.</li>\n<li>Repeat.</li>\n</ul>\n</blockquote>\n<p>Kind of meta but somewhat similar process for me. To publish the different content found on my <a href=\"/feed/responses/\">response feed</a>, I:</p>\n<ol>\n<li>Go through articles on my RSS feed (NewsBlur on both desktop and mobile).</li>\n<li>Copy URL and block quotes from article and paste them somewhere. When I have time like now, I create a post like this one, usually in VS Code. If I don't have time though, I've been experimenting with using a messaging app like Element and E-mail as a read-it-later service. At minimum, I create a message with the link and send it to myself for later review. Later on when I have time, I create the post with additional comments and content from the article.</li>\n<li>(Optional) Add some of my own comments.</li>\n<li>Publish the notes.</li>\n<li>Repeat.</li>\n</ol>\n</div>\n","tags":["blogging","notetaking"]},{"title":"Where this flower blooms","contentType":"reshare","date":"2023-11-12 11:53","url":"/reshare/tyler-ted-talk/","content":"<div class=\"response-target\" >→ <a href=\"https://twitter.com/studiosipu/status/1723582194432794638/video/1\" >https://twitter.com/studiosipu/status/1723582194432794638/video/1</a></div><div class=\"response-content\" ><p>Preach <span>🙌</span></p>\n<p><a href=\"https://twitter.com/studiosipu/status/1723582194432794638/video/1\" title=\"Tyler The Creator Speech at Camp FlagGnow\"><img src=\"https://pbs.twimg.com/ext_tw_video_thumb/1723582065843769344/pu/img/JSrlq95E1Hr5RzY_.jpg\" class=\"img-fluid\" alt=\"Tyler The Creator Speech at Camp FlagGnow\" /></a></p>\n</div>\n","tags":["tedtalk","festival"]},{"title":"Texts Joins Automattic","contentType":"star","date":"2023-10-24 17:13","url":"/star/texts-joins-automattic/","content":"<div class=\"response-target\" >→ <a href=\"https://ma.tt/2023/10/texts-joins-automattic/\" >https://ma.tt/2023/10/texts-joins-automattic/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Using an all-in-one messaging app is a real game-changer for productivity and keeping up with things.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This is obviously a tricky area to navigate, as in the past the networks have blocked third-party clients, but I think with the current anti-trust and regulatory environments this is actually something the big networks will appreciate: it maintains the same security as their clients, opens them up in a way consumers will love and is very user-centric, and because we’re committed to supporting all their features it can actually increase engagement and usage of their platforms.</p>\n</blockquote>\n<p>I can relate to the feeling of wanting to have one inbox expressed in the video. Coincidentally, I've been playing with <a href=\"https://delta.chat/\">Delta Chat</a> and by building on top of e-mail, some of the issues with the siloed platforms are alleviated. Also, <a href=\"https://www.emailisnotdead.com/\">e-mail isn't dead</a> and despite some of its shortcomings, it's still broadly used to sign up and sign into platforms.</p>\n</div>\n","tags":["messaging","interop","decentralization","opensource"]},{"title":"Automattic is acquiring Texts and betting big on the future of messaging","contentType":"reshare","date":"2023-10-24 09:16","url":"/reshare/automattic-acquires-texts-messaging/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2023/10/24/23928685/automattic-texts-acquisition-universal-messaging\" >https://www.theverge.com/2023/10/24/23928685/automattic-texts-acquisition-universal-messaging</a></div><div class=\"response-content\" ><p>I'm really liking the recent acquisitions from Automattic. I'm just starting to use Day One and really enjoy it. Pocket Casts is a fantastic podcast app, though I prefer to use AntennaPod. WordPress is also starting to make it easy to plug into the Fediverse using your blog. I'm excited for Texts and what that might offer in the current siloed messaging landscape.</p>\n<blockquote class=\"blockquote\">\n<p>Automattic, the company that runs WordPress.com, Tumblr, Pocket Casts, and a number of other popular web properties, just made a different kind of acquisition: it’s buying Texts, a universal messaging app, for $50 million.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Texts is an app for all your messaging apps. You can use it to log in to WhatsApp, Instagram, LinkedIn, Signal, iMessage, and more and see and respond to all your messages in one place.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...Mullenweg says he’s bullish on solutions like Matrix, which offers a decentralized and open-source messaging network, and other up-and-coming standards for messaging. He’s already thinking about how Texts might gently nudge people toward more open protocols over time.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Mullenweg and Automattic see a big future for messaging, as more online interaction shifts away from public-first social networks and toward things like group chats. Hardly anyone has figured out how to build a meaningful and sustainable business from chat, but Mullenweg thinks it’s possible. And he thinks it starts with making your messaging a little less messy.</p>\n</blockquote>\n</div>\n","tags":["messaging","interop","decentralization","opensource"]},{"title":"POSSE: A Better Way to Post on Social Networks","contentType":"reshare","date":"2023-10-23 18:55","url":"/reshare/posse-better-way-to-post-social-networks/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2023/10/23/23928550/posse-posting-activitypub-standard-twitter-tumblr-mastodon\" >https://www.theverge.com/2023/10/23/23928550/posse-posting-activitypub-standard-twitter-tumblr-mastodon</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The platform era is ending. Rather than build new Twitters and Facebooks, we can create a stuff-posting system that works better for everybody.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In a POSSE world, everybody owns a domain name, and everybody has a blog. (I’m defining “blog” pretty loosely here — just as a place on the internet where you post your stuff and others consume it.)</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>But there are some big challenges to the idea...The most immediate question...is simply how to build a POSSE system that works. POSSE’s problems start at the very beginning: it requires owning your own website, which means buying a domain and worrying about DNS records and figuring out web hosts, and by now, you’ve already lost the vast majority of people who would rather just type a username and password into some free Meta platform...Even those willing and able to do the technical work can struggle to make POSSE work.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>When I ask Doctorow why he believed in POSSE, he describes the tension every poster feels on the modern internet. “I wanted to find a way to stand up a new platform in this moment,” he says, “where, with few exceptions, everyone gets their news and does their reading through the silos that then hold you to ransom. And I wanted to use those silos to bring in readers and to attract and engage with an audience, but I didn’t want to become beholden to them.” <strong>The best of both worlds is currently a lot of work. But the poster’s paradise might not be so far away.</strong></p>\n</blockquote>\n</div>\n","tags":["indieweb","posse","socialnetworks","internet","community"]},{"title":"Engage a Wider Audience With ActivityPub on WordPress.com","contentType":"reshare","date":"2023-10-14 20:57","url":"/reshare/activitypub-plugin-available-hosted-wordpress/","content":"<div class=\"response-target\" >→ <a href=\"https://wordpress.com/blog/2023/10/11/activitypub/\" >https://wordpress.com/blog/2023/10/11/activitypub/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Exciting times are here for all WordPress.com users! The revolutionary ActivityPub feature is now available across all WordPress.com plans, unlocking a world of engagement and interaction for your blog. Your blogs can now be part of the rapidly expanding fediverse, which enables you to connect with a broader audience and attract more followers.</p>\n</blockquote>\n<p>I can't believe I missed these news but so exciting!</p>\n</div>\n","tags":["activitypub","mastodon","wordpress","blogging"]},{"title":"Radiooooo, The Musical Time Machine","contentType":"reshare","date":"2023-10-06 09:33","url":"/reshare/radiooooo/","content":"<div class=\"response-target\" >→ <a href=\"https://radiooooo.com/\" >https://radiooooo.com/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Radiooooo is a project born in 2013, dreamt up by a little family of friends, both djs and music lovers, who decided to share their record collections and the fruit of many years of research, for all to enjoy.</p>\n<p>« Sharing and discovering » , « curiosity and pleasure » these are the foundations of this musical time machine.</p>\n<p>Radiooooo is a collaborative website, whose goal is to open each and everyone’s horizons through culture and beauty.</p>\n</blockquote>\n</div>\n","tags":["music","radio"]},{"title":"BOOX Palma","contentType":"reshare","date":"2023-09-26 21:22","url":"/reshare/boox-palma-eink-phone/","content":"<div class=\"response-target\" >→ <a href=\"https://shop.boox.com/products/palma\" >https://shop.boox.com/products/palma</a></div><div class=\"response-content\" ><p>I really like my Boox e-reader but having a more pocketable device would be amazing. It's unforunate you can't also use it for handwritten notes but at this size it makes sense.</p>\n<p><a href=\"https://www.youtube.com/watch?v=K_n5tYz6DfM\" title=\"Boox Palma Promo Video\"><img src=\"http://img.youtube.com/vi/K_n5tYz6DfM/0.jpg\" class=\"img-fluid\" alt=\"Boox Palma Promo Video\" /></a></p>\n</div>\n","tags":["eink","ereader","mp3","minimalism","simplicity"]},{"title":"PSA: Fat Bear Week Oct 4-10 2023","contentType":"reshare","date":"2023-09-25 20:31","url":"/reshare/fat-bear-week-2023/","content":"<div class=\"response-target\" >→ <a href=\"https://www.nps.gov/katm/learn/fat-bear-week.htm\" >https://www.nps.gov/katm/learn/fat-bear-week.htm</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Fat Bear Week - an annual celebration of success. All bears are winners but only one true champion will emerge. Held over the course of seven days and concluding on the Fat Bear Tuesday, people chose which bear to crown in this tournament style bracket where bears are pitted against each other for your vote.</p>\n</blockquote>\n</div>\n","tags":["nature","bears","nps"]},{"title":"Tracy Durnell 20th anniversary of blogging","contentType":"reply","date":"2023-09-23 21:38","url":"/reply/tracy-durnell-20-blogging-anniversary/","content":"<div class=\"response-target\" >→ <a href=\"https://tracydurnell.com/2023/09/23/my-20th-anniversary-of-blogging/\" >https://tracydurnell.com/2023/09/23/my-20th-anniversary-of-blogging/</a></div><div class=\"response-content\" ><p>Happy 20th anniversary! Also, thanks for the generous linking. Lots of new folks to read and subscribe to.</p>\n</div>\n","tags":["blogs","blogging","indieweb"]},{"title":"The Atlantic’s Guide to Privacy","contentType":"reshare","date":"2023-09-17 13:03","url":"/reshare/atlantic-privacy-guide-2023/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theatlantic.com/technology/archive/2023/09/managing-digital-privacy-personal-information-online/675184/\" >https://www.theatlantic.com/technology/archive/2023/09/managing-digital-privacy-personal-information-online/675184/</a></div><div class=\"response-content\" ><p>Sponsored post, but it's still a good list with guidance and suggestions for common questions.</p>\n</div>\n","tags":["privacy","security","internet"]},{"title":"WordPress ActivityPub Plugin","contentType":"reshare","date":"2023-09-17 12:32","url":"/reshare/wordpress-activitypub-plugin/","content":"<div class=\"response-target\" >→ <a href=\"https://wordpress.org/plugins/activitypub/\" >https://wordpress.org/plugins/activitypub/</a></div><div class=\"response-content\" ><p>I can't believe I missed this announcement. This is great to see!</p>\n<blockquote class=\"blockquote\">\n<p>Enter the fediverse with ActivityPub, broadcasting your blog to a wider audience! Attract followers, deliver updates, and receive comments from a diverse user base of ActivityPub-compliant platforms.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>With the ActivityPub plugin installed, your WordPress blog itself function as a federated profile, along with profiles for each author. For instance, if your website is example.com, then the blog-wide profile can be found at @example.com@example.com, and authors like Jane and Bob would have their individual profiles at @jane@example.com and @bobz@example.com, respectively.</p>\n</blockquote>\n</div>\n","tags":["wordpress","activitypub","fediverse","openweb","decentralization","internet","blogging","opensource"]},{"title":"Tailscale has partnered with Mullvad","contentType":"reshare","date":"2023-09-08 21:26","url":"/reshare/tailscale-partners-mullvad/","content":"<div class=\"response-target\" >→ <a href=\"https://mullvad.net/en/blog/2023/9/7/tailscale-has-partnered-with-mullvad/\" >https://mullvad.net/en/blog/2023/9/7/tailscale-has-partnered-with-mullvad/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today we announce a partnership with Tailscale that allows you to use both in conjunction through the Tailscale app. This functionality is not available through the Mullvad VPN app.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This partnership allows customers of Tailscale to make use of our WireGuard VPN servers as “exit nodes”. This means that whilst connected to Tailscale, you can access your devices across Tailscale’s mesh network, whilst still connecting outbound through Mullvad VPN WireGuard servers in any location.</p>\n</blockquote>\n</div>\n","tags":["privacy","vpn","security"]},{"title":"Plato's Cave Regrets to Inform You It Will Be Raising Its Rent","contentType":"reshare","date":"2023-09-07 21:25","url":"/reshare/platos-cave-raising-rent/","content":"<div class=\"response-target\" >→ <a href=\"https://www.jwz.org/blog/2023/09/platos-cave-regrets-to-inform-you-it-will-be-raising-its-rent/\" >https://www.jwz.org/blog/2023/09/platos-cave-regrets-to-inform-you-it-will-be-raising-its-rent/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>If you are receiving this letter, it means you have been designated a tenant of the cave—i.e., you are chained to the wall, you are forced to watch shadows for all eternity, you are projecting said shadow puppets, and/or you are a philosopher who was able to break free and understand the true shackles of reality (PhD candidates about to argue their thesis).</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We do not undertake this lightly. As the costs of maintaining a cave meant to trap you in your ignorance increases year after year, we want you to know, from the bottom of our hearts, that we, too, are suffering. We get that times are tough, and we hope you can extend that sympathy to us, the managers of your cave.</p>\n<p>Please rest assured that cave costs are increasing everywhere. We manage many other caves like that of Polyphemus the Cyclops and the childhood home of Zeus. So, trust us: we know caves.</p>\n<p>We hope you will continue to enjoy living in our cave. We believe you are a valued part of the Plato's Cave community. Credit, cash, or Venmo all work.</p>\n</blockquote>\n<p>Original Source: <a href=\"https://www.mcsweeneys.net/articles/platos-cave-regrets-to-inform-you-it-will-be-raising-its-rent\">https://www.mcsweeneys.net/articles/platos-cave-regrets-to-inform-you-it-will-be-raising-its-rent</a></p>\n<p><img src=\"https://media.giphy.com/media/zHd8x7Pik0Ftm/giphy.gif\" class=\"img-fluid\" alt=\"Boy laughing and then crying\" /></p>\n</div>\n","tags":["simulation"]},{"title":"Generative AI and .NET - Part 2 SDK","contentType":"reshare","date":"2023-09-06 09:29","url":"/reshare/generative-ai-dotnet-pt-2-powell/","content":"<div class=\"response-target\" >→ <a href=\"https://www.aaron-powell.com/posts/2023-09-04-generative-ai-and-dotnet---part-2-sdk/\" >https://www.aaron-powell.com/posts/2023-09-04-generative-ai-and-dotnet---part-2-sdk/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>It’s time to have a look at how we can build the basics of an application using Azure OpenAI Services and the .NET SDK.</p>\n</blockquote>\n</div>\n","tags":["ai","dotnet","generativeai","llm","azure"]},{"title":"Doing Laundry on Campus Without a Phone","contentType":"reply","date":"2023-09-02 18:45","url":"/reply/doing-laundry-without-phone/","content":"<div class=\"response-target\" >→ <a href=\"https://naveenarun.wordpress.com/2023/08/31/doing-laundry-on-campus-without-a-phone/\" >https://naveenarun.wordpress.com/2023/08/31/doing-laundry-on-campus-without-a-phone/</a></div><div class=\"response-content\" ><p>This article reminds me of this gem 😂</p>\n<p><a href=\"https://www.youtube.com/watch?v=mzb355qT8RI\" title=\"Dennis Orders a Boba Tea - Scene | It's Always Sunny in Philadelphia | FX\"><img src=\"http://img.youtube.com/vi/mzb355qT8RI/0.jpg\" class=\"img-fluid\" alt=\"Dennis Orders a Boba Tea - Scene | It's Always Sunny in Philadelphia | FX\" /></a></p>\n</div>\n","tags":["technology","tv"]},{"title":"Announcement - Solo by Pixelfed","contentType":"reply","date":"2023-09-01 10:55","url":"/reply/pixelfed-solo-announcement/","content":"<div class=\"response-target\" >→ <a href=\"https://mastodon.social/@pixelfed/110989967683252426\" >https://mastodon.social/@pixelfed/110989967683252426</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Solo by Pixelfed</p>\n<p>We've been secretly building a single user federated photo sharing server (based on Pixelfed), with minimal setup, and built-in import</p>\n<p>Solo is simple, download the code, drag your photos to the media directory and open your browser</p>\n<p>So simple, yet super smart, Solo won't get in your way, but it will impress</p>\n<p>Launching Oct 2023</p>\n</blockquote>\n<p>Very excited about this!</p>\n</div>\n","tags":["fediverse","pixelfed","photos","socialmedia"]},{"title":"Generative AI and .NET - Part 1 Intro","contentType":"reshare","date":"2023-09-01 10:46","url":"/reshare/generative-ai-dotnet-pt-1-powell/","content":"<div class=\"response-target\" >→ <a href=\"https://www.aaron-powell.com/posts/2023-09-01-generative-ai-and-dotnet---part-1-intro/\" >https://www.aaron-powell.com/posts/2023-09-01-generative-ai-and-dotnet---part-1-intro/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>over this series I’m going to share my learnings on the APIs, SDKs, and the like. The goal here isn’t to “build something” but rather to share what I’ve learnt, the mistakes I’ve made, the things I’ve found confusing, and the code I’ve had to rewrite umpteen times because “oh, that’s a better way to do it”.</p>\n</blockquote>\n</div>\n","tags":["ai","dotnet","generativeai","llm"]},{"title":"Supporting the Open Source AI Community","contentType":"reshare","date":"2023-08-31 18:48","url":"/reshare/supporting-open-source-ai-community-az/","content":"<div class=\"response-target\" >→ <a href=\"https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/\" >https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We believe artificial intelligence has the power to save the world—and that a thriving open source ecosystem is essential to building this future.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>To help close this resource gap, we’re announcing today the a16z Open Source AI Grant program. We’ll support a small group of open source developers through grant funding (not an investment or SAFE note), giving them the opportunity to continue their work without the pressure to generate financial returns.</p>\n</blockquote>\n</div>\n","tags":["ai","opensource","machinelearning","llm"]},{"title":"WordPress at 25 and the Indie Web","contentType":"reshare","date":"2023-08-30 21:02","url":"/reshare/podcast-wordpress-indieweb-reality2/","content":"<div class=\"response-target\" >→ <a href=\"https://www.reality2cast.com/151\" >https://www.reality2cast.com/151</a></div><div class=\"response-content\" ><p>Good discussion on the open web, self-hosting, radio, and the indie web.</p>\n</div>\n","tags":["podcast","indieweb","wordpress","selfhost","radio","openweb","fediverse"]},{"title":"We Don’t Need a New Twitter","contentType":"reshare","date":"2023-08-28 20:46","url":"/reshare/no-new-twitter-newport/","content":"<div class=\"response-target\" >→ <a href=\"https://www.newyorker.com/culture/cultural-comment/we-dont-need-a-new-twitter\" >https://www.newyorker.com/culture/cultural-comment/we-dont-need-a-new-twitter</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>If Meta can succeed in capturing some of this peak-Twitter magic, while avoiding late-stage Twitter’s struggles, the company will perhaps even reclaim some of the cultural gravity that it squandered a decade ago when Facebook took its turn toward crazy-uncle irrelevance. But can Meta possibly succeed in building a saner, nicer Twitter?</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Breaking news can spread quickly, as can clips that are funny in an original or strange way—but these innocuous trends feel serendipitous, like a rainbow spanning storm clouds. To reach the Twitter masses, conspiracy, demagoguery, and cancellation are much more likely to succeed. The result is a Faustian bargain for our networked era: trusting the wisdom of crowds to identify what’s interesting can create an intensely compelling stream of shared content, but this content is likely to arrive drenched in rancor.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The obvious way Meta can attempt to escape this bargain is by moving Threads away from retransmission-based curation and toward algorithmic ranking. This will give the company more control over which discussions are amplified, but, in doing so, they will also lose the human-powered selectivity that makes Twitter so engaging.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>If we look past this narrow discussion of Threads’ challenges, however, a broader question arises: Why is it so important to create a better version of Twitter in the first place? Ignored amid the hand-wringing about the toxic turn taken by large-scale conversation platforms are the many smaller, less flashy sites and services that have long been supporting a more civilized form of digital interaction.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>“The Internet has become the ultimate narrowcasting vehicle: everyone from UFO buffs to New York Yankee fans has a Website (or dozen) to call his own,” the journalist Richard Zoglin wrote in 1996. “A dot-com in every pot.”</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We’ve gone from Zoglin’s dot-com in every pot to the social-media age’s vision of every pot being filled with slop <a href=\"https://www.newyorker.com/culture/cultural-comment/tiktok-and-the-fall-of-the-social-media-giants\">from the same platforms</a>.</p>\n</blockquote>\n</div>\n","tags":["twitter","socialmedia","indieweb","internet","culture"]},{"title":"Retrostream TV","contentType":"reshare","date":"2023-07-23 23:51","url":"/reshare/restrostrange-tv/","content":"<div class=\"response-target\" >→ <a href=\"https://retrostrange.com/\" >https://retrostrange.com/</a></div><div class=\"response-content\" ><p>Interesting project.</p>\n<blockquote class=\"blockquote\">\n<p>RetroStrange is part independent media experiment, part handmade exaltation of vintage weirdness and the public domain. It is produced by Noah Maher and Phil Nelson.</p>\n</blockquote>\n</div>\n","tags":["livestream"]},{"title":"WordPress Playground","contentType":"reshare","date":"2023-07-15 00:27","url":"/reshare/wordpress-playground/","content":"<div class=\"response-target\" >→ <a href=\"https://developer.wordpress.org/playground/\" >https://developer.wordpress.org/playground/</a></div><div class=\"response-content\" ><p>Not a WordPress user but this is cool, especially this - <em>&amp;quot;Build an entire site, save it, host it&amp;quot;</em></p>\n<blockquote class=\"blockquote\">\n<p>WordPress Playground makes WordPress instantly accessible for users, learners, extenders, and contributors. You can:</p>\n<ul>\n<li>Try a block, a theme, or a plugin</li>\n<li>Build an entire site, save it, host it</li>\n<li>Test your plugin with many specific WordPress and PHP versions</li>\n<li>Embed a real, interactive WordPress site in your tutorial or course</li>\n<li>Showcase a plugin or theme on your website</li>\n<li>Preview pull requests from your repository</li>\n<li>…or even run WordPress locally using the VisualStudio Code plugin or a CLI tool called wp-now</li>\n</ul>\n</blockquote>\n</div>\n","tags":["indieweb","wordpress","openweb"]},{"title":"A slack clone in 5 lines of bash","contentType":"reshare","date":"2023-07-12 22:07","url":"/reshare/dam-slack-clone-bash/","content":"<div class=\"response-target\" >→ <a href=\"https://the-dam.org/docs/explanations/suc.html\" >https://the-dam.org/docs/explanations/suc.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>suc provides Slack, Mattermost, etc.’s core features:</p>\n<p>Real-time, rich-text chat,<br />\nFile sharing,<br />\nFine-grained access control,<br />\nStraightforward automation and integration with other tools,<br />\nData encryption in transit<br />\nand optionally at rest,<br />\nstate-of-the-art user authentication.</p>\n<p>This paper shows how suc implements those features. suc stays small by leveraging the consistent and composable primitives offered by modern UNIX implementations</p>\n</blockquote>\n</div>\n","tags":["linux","bash","communication"]},{"title":"GPT-4 API general availability","contentType":"reshare","date":"2023-07-06 22:08","url":"/reshare/gpt-4-ga/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/blog/gpt-4-api-general-availability\" >https://openai.com/blog/gpt-4-api-general-availability</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>GPT-4 API general availability and deprecation of older models in the Completions API. GPT-3.5 Turbo, DALL·E and Whisper APIs are also generally available, and we are releasing a deprecation plan for older models of the Completions API, which will retire at the beginning of 2024.</p>\n</blockquote>\n</div>\n","tags":["openai","gpt4","llm","ai"]},{"title":"Introducing Proton Pass","contentType":"reshare","date":"2023-06-29 10:32","url":"/reshare/protonpass-release/","content":"<div class=\"response-target\" >→ <a href=\"https://proton.me/blog/proton-pass-launch\" >https://proton.me/blog/proton-pass-launch</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We’re happy to announce the global launch of Proton Pass...a password manager, one of the most highly demanded services from the Proton community in our annual surveys since we first launched Proton Mail...</p>\n</blockquote>\n</div>\n","tags":["security"]},{"title":"fsharpConf 2023","contentType":"reshare","date":"2023-06-26 13:02","url":"/reshare/fsharp-conf-2023/","content":"<div class=\"response-target\" >→ <a href=\"http://fsharpconf.com/\" >http://fsharpconf.com/</a></div><div class=\"response-content\" ><p>I'm really enjoying the sessions. Kudos to the team who put it together and presenters delivering great content.</p>\n<p>If you're interested, check out the stream at <a href=\"http://fsharpconf.com/\">http://fsharpconf.com/</a>.</p>\n</div>\n","tags":["fsharp","fsharpConf","dotnet","programming","community","opensource"]},{"title":"LGR review of the Book 8088 and Hand 386 Retro DOS PCs","contentType":"reshare","date":"2023-06-20 23:29","url":"/reshare/book-8088-hand-386-dos-pcs/","content":"<div class=\"response-target\" >→ <a href=\"https://yewtu.be/watch?v=6bODiZ5bP84\" >https://yewtu.be/watch?v=6bODiZ5bP84</a></div><div class=\"response-content\" ><p><a href=\"https://yewtu.be/embed/6bODiZ5bP84\" title=\"Book 8088 and Hand 386 Retro DOS PCs\"><img src=\"https://yewtu.be/vi/6bODiZ5bP84/maxres.jpg\" class=\"img-fluid\" alt=\"Book 8088 and Hand 386 Retro DOS PCs\" /></a></p>\n<p><img src=\"https://media.giphy.com/media/l0HFkA6omUyjVYqw8/giphy.gif\" class=\"img-fluid\" alt=\"GIF of baby throwing cash out window\" /></p>\n</div>\n","tags":["retrocomputing","review","dos","pcs"]},{"title":"WordPress - Newsletter paid subscriptions","contentType":"star","date":"2023-06-01 21:00","url":"/star/wordpress-paid-newsletter/","content":"<div class=\"response-target\" >→ <a href=\"https://wordpress.com/blog/2023/06/01/newsletters-paid-subscriptions/\" >https://wordpress.com/blog/2023/06/01/newsletters-paid-subscriptions/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...we’re introducing a big update — the ability to add paid subscriptions and premium content, whatever plan you’re on. Including the Free plan.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Paid subscriptions let your fans support your art, writing, or project directly.</p>\n</blockquote>\n</div>\n","tags":["web","blogging","wordpress","newsletters"]},{"title":"Implement DNS in a weekends","contentType":"reshare","date":"2023-05-14 00:39","url":"/reshare/implement-dns-weekend/","content":"<div class=\"response-target\" >→ <a href=\"https://implement-dns.wizardzines.com/\" >https://implement-dns.wizardzines.com/</a></div><div class=\"response-content\" ></div>\n","tags":["internet","dns","www","zine"]},{"title":"How generative AI is changing the way developers work","contentType":"reshare","date":"2023-04-20 22:11","url":"/reshare/generative-ai-developer-work/","content":"<div class=\"response-target\" >→ <a href=\"https://github.blog/2023-04-14-how-generative-ai-is-changing-the-way-developers-work/\" >https://github.blog/2023-04-14-how-generative-ai-is-changing-the-way-developers-work/</a></div><div class=\"response-content\" ></div>\n","tags":["ai","engineering","software"]},{"title":"Offline Is Just Online With Extreme Latency","contentType":"reshare","date":"2023-04-20 11:00","url":"/reshare/nielsen-offline-online-latency/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.jim-nielsen.com/2023/offline-is-online-with-extreme-latency/\" >https://blog.jim-nielsen.com/2023/offline-is-online-with-extreme-latency/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>you can think of online/offline as part of the same continuum just different measurements of latency. There are gradations of latency when you’re “online”, and “offline” is merely at the slowest end of that spectrum.</p>\n</blockquote>\n</div>\n","tags":["web","offline"]},{"title":"Testing tagged responses","contentType":"reshare","date":"2023-04-04 23:30","url":"/reshare/test-tagged-response/","content":"<div class=\"response-target\" >→ <a href=\"http://lqdev.me/tags\" >http://lqdev.me/tags</a></div><div class=\"response-content\" ><p>My response</p>\n</div>\n","tags":["webdev"]},{"title":"Darklang is going all-in on AI","contentType":"reshare","date":"2023-03-28 22:24","url":"/reshare/darklang-gpt/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.darklang.com/gpt/\" >https://blog.darklang.com/gpt/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>On February 1, we stopped working on what we're now calling &amp;quot;darklang-classic&amp;quot;, and are fully heads down on building &amp;quot;darklang-gpt&amp;quot;, which is the same core Darklang but redesigned to have AI as the primary (or possibly only) way of writing code.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"GPT-4","contentType":"reshare","date":"2023-03-14 13:59","url":"/reshare/openai-gpt-4/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/product/gpt-4\" >https://openai.com/product/gpt-4</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>GPT-4 can accept images as inputs and generate captions, classifications, and analyses.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>GPT-4 is capable of handling over 25,000 words of text, allowing for use cases like long form content creation, extended conversations, and document search and analysis.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Access Your Bitwarden Vault Without a Password","contentType":"reshare","date":"2023-02-23 11:20","url":"/reshare/access-bitwarden-vault-passwordless/","content":"<div class=\"response-target\" >→ <a href=\"https://bitwarden.com/blog/access-your-bitwarden-vault-without-a-password/\" >https://bitwarden.com/blog/access-your-bitwarden-vault-without-a-password/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Bitwarden expands the Log in with device option that lets you use a second device to authenticate your Bitwarden vault login instead of using your Bitwarden password.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Attacking Machine Learning Systems","contentType":"reshare","date":"2023-02-07 10:54","url":"/reshare/attacking-machine-learning-systems/","content":"<div class=\"response-target\" >→ <a href=\"https://www.schneier.com/blog/archives/2023/02/attacking-machine-learning-systems.html\" >https://www.schneier.com/blog/archives/2023/02/attacking-machine-learning-systems.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>At their core, modern ML systems have complex mathematical models that use training data to become competent at a task. And while there are new risks inherent in the ML model, all of that complexity still runs in software. Training data are still stored in memory somewhere. And all of that is on a computer, on a network, and attached to the Internet. Like everything else, these systems will be hacked through vulnerabilities in those more conventional parts of the system.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Skype 8.93 Released","contentType":"reshare","date":"2023-01-30 11:07","url":"/reshare/skype-8-93-release/","content":"<div class=\"response-target\" >→ <a href=\"https://answers.microsoft.com/skype/forum/all/release-notes-for-skype-893/6407f3c4-4037-4e6b-b93f-1953408cf9f3\" >https://answers.microsoft.com/skype/forum/all/release-notes-for-skype-893/6407f3c4-4037-4e6b-b93f-1953408cf9f3</a></div><div class=\"response-content\" ><p>A few updates I found interesting.</p>\n<blockquote class=\"blockquote\">\n<p>The Beauty of languages: You can now use your own voice during a translated call in Skype.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Universal translator: During a Skype call, if a participant speaks different languages, Skype Translator will automatically detect the languages and translate it for you.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Extra! Extra! Read all about it: Stay up-to-date with the latest news and trends.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Hit me up sometime: You can easily add contacts in Skype on mobile using a unique QR code to get connected.</p>\n</blockquote>\n<p>I have a few thoughts on the <a href=\"https://support.skype.com/faq/FA34951/what-is-the-today-tab-in-skype-and-how-do-i-use-it\">Today</a> feature and overall updates Skype has been receiving over the past few months but I'll leave those for another post.</p>\n</div>\n","tags":[]},{"title":"WebmentionFs Send Test","contentType":"reply","date":"2023-01-28 11:24","url":"/reply/webmentionfs-send-test/","content":"<div class=\"response-target\" >→ <a href=\"https://webmention.rocks/test/1\" >https://webmention.rocks/test/1</a></div><div class=\"response-content\" ><p>Testing WebmentionFs send integration. From Website.</p>\n</div>\n","tags":[]},{"title":"Microsoft Edge 'Phoenix' is an internal reimagining of the Edge web browser with a new UI and more features","contentType":"reshare","date":"2023-01-26 11:26","url":"/reshare/microsoft-edge-phoenix-browser/","content":"<div class=\"response-target\" >→ <a href=\"https://www.windowscentral.com/software-apps/windows-11/microsoft-edge-phoenix-is-an-internal-reimagining-of-the-edge-web-browser-with-a-new-ui-and-more-features\" >https://www.windowscentral.com/software-apps/windows-11/microsoft-edge-phoenix-is-an-internal-reimagining-of-the-edge-web-browser-with-a-new-ui-and-more-features</a></div><div class=\"response-content\" ><p>As someone who spends a significant amount of time in the browser, making Edge, more specifically the browser the platform for applications, this looks appealing. Especially when you consider Progressive Web App (PWA) support and integrations with the Windows Store.</p>\n</div>\n","tags":[]},{"title":"Eternal Tumblr","contentType":"reply","date":"2023-01-25 19:26","url":"/reply/hon-eternal-tumblr/","content":"<div class=\"response-target\" >→ <a href=\"https://newsletter.danhon.com/archive/s14e09-building-and-documentation-making-new/\" >https://newsletter.danhon.com/archive/s14e09-building-and-documentation-making-new/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Automattic, the current owner of tumblr, one of the biggest meme mines in the world, has said they’re going to implement ActivityPub, which is the underlying protocol on which Mastodon operates. There are boring ways this could go, and then interesting ways this could go.</p>\n</blockquote>\n<p>Interesting points. I'm looking forward to how this shakes out.</p>\n</div>\n","tags":[]},{"title":"The Best Time to Own a Domain Was 20 Years Ago; The Second Best Time Is Today","contentType":"star","date":"2023-01-25 10:29","url":"/star/nielsen-best-time-to-own-domain/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.jim-nielsen.com/2023/best-time-to-own-a-domain/\" >https://blog.jim-nielsen.com/2023/best-time-to-own-a-domain/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>if own your domain, create value there, and drive people to it, you’re paying ~$10 a year to build unbounded value over the years — value you control.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>That is why owning a domain (and publishing your content there) is like planting a tree: it’s value that starts small and grows. The best time to own a domain and publish your content there was 20 years ago. The second best time is today.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I love a technology like <code>rel=me</code> which pushes the idea of domain ownership into broader and broader spheres of society — “How do I get that nice little green checkmark on my profile?” It reinforces the value of owning your own domain (which you can verify elsewhere) and encourages citizens of the internet people to build value in their own little corners of the world wide web.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Bitwarden Open Source Security Explained","contentType":"reshare","date":"2023-01-25 10:26","url":"/reshare/bitwarden-open-source-security-explained/","content":"<div class=\"response-target\" >→ <a href=\"https://bitwarden.com/blog/bitwarden-open-source-security-explained/\" >https://bitwarden.com/blog/bitwarden-open-source-security-explained/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This article answers three common questions about how being open source strengthens Bitwarden security, transparency, and privacy.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"BioProviders - .NET library for accessing and manipulating bioinformatic datasets","contentType":"star","date":"2023-01-24 10:08","url":"/star/github-fsharp-bioproviders/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/fsprojects/BioProviders\" >https://github.com/fsprojects/BioProviders</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Karpathy - Let's build GPT: from scratch, in code, spelled out","contentType":"reshare","date":"2023-01-18 10:45","url":"/reshare/karpathy-lets-build-gpt-from-scratch-video/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=kCc8FmEb1nY\" >https://www.youtube.com/watch?v=kCc8FmEb1nY</a></div><div class=\"response-content\" ><p><a href=\"https://www.youtube.com/watch?v=kCc8FmEb1nY\" title=\"Let's build GPT\"><img src=\"https://yewtu.be/vi/kCc8FmEb1nY/maxres.jpg\" class=\"img-fluid\" alt=\"Let's build GPT\" /></a></p>\n</div>\n","tags":[]},{"title":"Subscribe Wherever You Get Your Content","contentType":"star","date":"2023-01-11 10:43","url":"/star/subscribe-wherever-you-get-your-content-nielsen/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.jim-nielsen.com/2023/subscribe-wherever-you-get-your-content/\" >https://blog.jim-nielsen.com/2023/subscribe-wherever-you-get-your-content/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Wouldn’t it be amazing if a similar phrase could enter the larger public consciousness for blogs or even people who you follow online?</p>\n<p>Instead of:</p>\n<pre><code>“Follow me on Twitter.”  \n“I’m @realChuckieCheese on Instagram.”  \n“Subscribe to my videos on YouTube.”  \n</code></pre>\n<p>You’d hear some popular influencer saying:</p>\n<pre><code>“Follow me wherever you follow people online.”  \n“Find me and subscribe wherever you get your content.”\n</code></pre>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In a world like this, perhaps <a href=\"https://twitter.com/jimniels/status/1597654715873267713\">apex domains could become the currency of online handles</a>: follow me <code>@jim-nielsen.com</code>.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Building a website like it's 1999... in 2022","contentType":"reply","date":"2023-01-11 10:18","url":"/reply/building-website-like-1999-in-2022-localghost/","content":"<div class=\"response-target\" >→ <a href=\"https://localghost.dev/blog/building-a-website-like-it-s-1999-in-2022/\" >https://localghost.dev/blog/building-a-website-like-it-s-1999-in-2022/</a></div><div class=\"response-content\" ><p>Now the only challenges for me are:</p>\n<ol>\n<li>Should the song on my website match my ringtone?</li>\n<li>Where do I place the flames GIF?</li>\n</ol>\n<p><img src=\"/files/images/flames.gif\" class=\"img-fluid\" alt=\"Flames GIF\" /></p>\n</div>\n","tags":[]},{"title":"Bring Back Blogging Directory","contentType":"reshare","date":"2023-01-04 21:03","url":"/reshare/bring-back-blogs-directory/","content":"<div class=\"response-target\" >→ <a href=\"https://bringback.blog/\" >https://bringback.blog/</a></div><div class=\"response-content\" ><p>If you're looking for new folks follow, here's a list of blogs on a variety of topics.</p>\n</div>\n","tags":[]},{"title":"Intel launches new entry-level Core i3 N-series mobile CPUs","contentType":"reshare","date":"2023-01-04 00:28","url":"/reshare/ces-intel-entry-level-n-series-processor/","content":"<div class=\"response-target\" >→ <a href=\"https://www.windowscentral.com/hardware/computers-desktops/intel-core-i3-n-series-launch-ces-2023\" >https://www.windowscentral.com/hardware/computers-desktops/intel-core-i3-n-series-launch-ces-2023</a></div><div class=\"response-content\" ><p><img src=\"https://cdn.mos.cms.futurecdn.net/kkWgG7UZh3ySjrniGmYmDk-1200-80.jpg\" class=\"img-fluid\" alt=\"Overview of Intel N-Series Processor Features\" /></p>\n<p>The article suggests these processors are for the education market. Hopefully a few of the devices with these processors in a Surface Go form-factor are made generally available as well. I think the N200 series is the sweet spot in terms of performance and battery life with only 6W of max turbo power. The i3 which draws a max of 15W may be too much. At that point, some of the newly announced <a href=\"https://www.windowscentral.com/hardware/computers-desktops/13th-gen-intel-core-mobile-cpus-announced-ces-2023\">U-series processors</a> might make more sense.</p>\n</div>\n","tags":[]},{"title":"Personal Websites and Online Identities","contentType":"reply","date":"2023-01-03 23:14","url":"/reply/personal-websites-and-online-identifies-hd/","content":"<div class=\"response-target\" >→ <a href=\"https://hacdias.com/2022/12/26/personal-websites-and-online-identities\" >https://hacdias.com/2022/12/26/personal-websites-and-online-identities</a></div><div class=\"response-content\" ><p>Great post.</p>\n<p>For me, I've found POSSE to Twitter and Mastodon using RSS and IFTTT-like services has been a relatively low-effort thing to do. The downside though is any follow-up conversations take place on those platforms. That hasn't been an issue for me yet though so until it is, I'll continue to use my relatively low-effort solution.</p>\n<p>I agree with you on likes / reposts. My current implementation of those posts is focused on Webmentions, which means it's virtually meaningless since few people know about let alone integrate Webmentions into their website. I would argue replies tend to fall into that same space as well. I have however found great use in bookmark posts to which I try and add some content from the source document to help me quickly identfy why I thought that post / website was interesting or relevant to a specific topic at that time.</p>\n</div>\n","tags":[]},{"title":"Petals - Easy way to run 100B+ language models at home","contentType":"reshare","date":"2023-01-03 22:34","url":"/reshare/petals-ml-model-inference/","content":"<div class=\"response-target\" >→ <a href=\"https://petals.ml/\" >https://petals.ml/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Run 100B+ language models at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Bring Back Personal Blogging","contentType":"star","date":"2023-01-03 22:20","url":"/star/bring-back-personal-blogging/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/23513418/bring-back-personal-blogging\" >https://www.theverge.com/23513418/bring-back-personal-blogging</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In the beginning, there were blogs, and they were the original social web. We built community. We found our people. We wrote personally. We wrote frequently. We self-policed, and we linked to each other so that newbies could discover new and good blogs.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The biggest reason personal blogs need to make a comeback is a simple one: we should all be in control of our own platforms.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>People built entire communities around their favorite blogs, and it was a good thing. You could find your people, build your tribe, and discuss the things your collective found important.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Buy that domain name. Carve your space out on the web. Tell your stories, build your community, and talk to your people. It doesn’t have to be big. It doesn’t have to be fancy. You don’t have to reinvent the wheel. It doesn’t need to duplicate any space that already exists on the web — in fact, it shouldn’t. This is your creation. It’s your expression. It should reflect you.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"OpenAI Text Embedding Ada 002 Model","contentType":"reshare","date":"2023-01-02 13:48","url":"/reshare/openai-text-embedding-ada-002/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/blog/new-and-improved-embedding-model/\" >https://openai.com/blog/new-and-improved-embedding-model/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The new model, <code>text-embedding-ada-002</code>, replaces five separate models for text search, text similarity, and code search, and outperforms our previous most capable model, Davinci, at most tasks, while being priced 99.8% lower.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"It’s Time to Get Back Into RSS","contentType":"star","date":"2022-12-28 10:50","url":"/star/miessler-time-get-back-rss/","content":"<div class=\"response-target\" >→ <a href=\"https://danielmiessler.com/blog/its-time-to-get-back-into-rss/\" >https://danielmiessler.com/blog/its-time-to-get-back-into-rss/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We all mourned when Reader died and took RSS with it, but it's time to return to what made it great</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Umbrel - Personal Server OS","contentType":"reshare","date":"2022-12-28 10:27","url":"/reshare/umbrel-personal-server-os-selfhosting/","content":"<div class=\"response-target\" >→ <a href=\"https://umbrel.com/\" >https://umbrel.com/</a></div><div class=\"response-content\" ><p>Really cool project. Don't really care for the crypto stuff but the rest looks very interesting.</p>\n</div>\n","tags":[]},{"title":"Matrix Holiday Update","contentType":"star","date":"2022-12-27 21:34","url":"/star/matrix-holiday-edition-2022/","content":"<div class=\"response-target\" >→ <a href=\"https://matrix.org/blog/2022/12/25/the-matrix-holiday-update-2022\" >https://matrix.org/blog/2022/12/25/the-matrix-holiday-update-2022</a></div><div class=\"response-content\" ><p>These updates are exciting!</p>\n<blockquote class=\"blockquote\">\n<p><a href=\"https://macaw.social/@wongmjane/109529583352532543\">Reddit appears to be building out new Chat functionality</a> using Matrix</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><a href=\"https://meta.discourse.org/t/matrix-protocol-for-chat/210780\">Discourse</a> is working on adding Matrix support</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><a href=\"https://www.theregister.com/2022/06/30/thunderbird_102\">Thunderbird</a> launched Matrix support.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Automattic is busy building <a href=\"https://github.com/Automattic/chatrix\">Matrix plugins for Wordpress</a></p>\n</blockquote>\n<p>Not as exciting</p>\n<blockquote class=\"blockquote\">\n<p>...only a handful of these initiatives have resulted in funding reaching the core Matrix team. <strong>This is directly putting core Matrix development at risk.</strong></p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In short: folks love the amazing decentralised encrypted comms utopia of Matrix. But organisations also love that they can use it without having to pay anyone to develop or maintain it. <strong>This is completely unsustainable</strong>, and Element is now literally unable to fund the entirety of the Matrix Foundation on behalf of everyone else - and has had to lay off some of the folks working on the core team as a result.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In the interim, if you are an organisation who’s building on Matrix and you want the project to continue to flourish, please mail funding@matrix.org to discuss how you can support the foundations that you are depending on.</p>\n</blockquote>\n<p>I'm looking forward to the future of the Matrix protocol, especially the <a href=\"https://arewep2pyet.com/\">P2P components</a>.</p>\n</div>\n","tags":[]},{"title":"Jon Udell: Personal RSS aggregators","contentType":"reshare","date":"2022-12-25 16:29","url":"/reshare/personal-rss-aggregators-udell/","content":"<div class=\"response-target\" >→ <a href=\"https://jonudell.info/bytecols/2002-05-15.html\" >https://jonudell.info/bytecols/2002-05-15.html</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"PINE64 Announces the PineTab2 Linux Tablet","contentType":"star","date":"2022-12-21 01:20","url":"/star/pine64-announces-pinetab-2-linux/","content":"<div class=\"response-target\" >→ <a href=\"https://9to5linux.com/pine64-announces-the-pinetab2-linux-tablet-with-up-to-8gb-ram-and-rk3566-soc\" >https://9to5linux.com/pine64-announces-the-pinetab2-linux-tablet-with-up-to-8gb-ram-and-rk3566-soc</a></div><div class=\"response-content\" ><p><img src=\"https://c.tenor.com/R0d3sZ4fq6EAAAAC/money-dollars.gif\" class=\"img-fluid\" alt=\"Futurama Fry Take My Money GIF\" /></p>\n</div>\n","tags":[]},{"title":"Mastodon Hashtag RSS feeds","contentType":"reply","date":"2022-12-15 22:33","url":"/reply/mastodon-hashtag-rss-boffosocko/","content":"<div class=\"response-target\" >→ <a href=\"https://boffosocko.com/2022/12/15/55812758/\" >https://boffosocko.com/2022/12/15/55812758/</a></div><div class=\"response-content\" ><p>Worked well for me. Thanks for sharing!</p>\n</div>\n","tags":[]},{"title":"Exploring Large Language Models with ChatGPT","contentType":"star","date":"2022-12-12 08:22","url":"/star/twiml-exploring-llm-chatgpt/","content":"<div class=\"response-target\" >→ <a href=\"https://podbay.fm/p/the-twiml-ai-podcast-formerly-this-week-in-machine-learning-and-artificial-intelligence/e/1670516880\" >https://podbay.fm/p/the-twiml-ai-podcast-formerly-this-week-in-machine-learning-and-artificial-intelligence/e/1670516880</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today we're joined by ChatGPT, the latest and coolest large language model developed by OpenAl. In our conversation with ChatGPT, we discuss the background and capabilities of large language models, the potential applications of these models, and some of the technical challenges and open questions in the field. We also explore the role of supervised learning in creating ChatGPT, and the use of PPO in training the model. Finally, we discuss the risks of misuse of large language models, and the best resources for learning more about these models and their applications.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"GitHub Copilot is generally available for businesses","contentType":"reshare","date":"2022-12-07 17:59","url":"/reshare/gh-copilot-business-ga/","content":"<div class=\"response-target\" >→ <a href=\"https://github.blog/2022-12-07-github-copilot-is-generally-available-for-businesses/\" >https://github.blog/2022-12-07-github-copilot-is-generally-available-for-businesses/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>GitHub Copilot for Business gives organizations:</p>\n<ul>\n<li><strong>The power of AI</strong>. Millions of developers have already used GitHub Copilot to build software faster, stay in the flow longer, and solve problems in new ways—all right from their editor of choice.</li>\n<li><strong>Simple license management</strong>. Administrators can enable GitHub Copilot for their teams and select which organizations, teams, and developers receive licenses.</li>\n<li><strong>Organization-wide policy management</strong>. You can easily set policy controls to enforce user settings for public code matching on behalf of your organization.</li>\n<li><strong>Your code is safe with us</strong>. With Copilot for Business, we won’t retain code snippets, store or share your code regardless if the data is from public repositories, private repositories, non-GitHub repositories, or local files.</li>\n</ul>\n</blockquote>\n</div>\n","tags":[]},{"title":"Simulating the Wrapinator 5000","contentType":"star","date":"2022-12-07 16:19","url":"/star/fsadvent-wrapinator-5000/","content":"<div class=\"response-target\" >→ <a href=\"https://brandewinder.com/2022/12/04/simulating-wrapinator-5000/\" >https://brandewinder.com/2022/12/04/simulating-wrapinator-5000/</a></div><div class=\"response-content\" ><p>Cool use of <a href=\"https://diffsharp.github.io/\">DiffSharp</a>.</p>\n</div>\n","tags":[]},{"title":"Microsoft Teams adds free communities feature","contentType":"reply","date":"2022-12-07 15:50","url":"/reply/teams-communities-feature/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2022/12/7/23497938/microsoft-teams-communities-feature\" >https://www.theverge.com/2022/12/7/23497938/microsoft-teams-communities-feature</a></div><div class=\"response-content\" ><p><img src=\"http://blogs.windows.com/wp-content/uploads/sites/2/2012/11/Family_5F00_Hub_5F00_16x9_5F00_753FF398.jpg\" class=\"img-fluid\" alt=\"Windows Phone Rooms Feature Screenshot\" /></p>\n<p>This reminds me so much of the <a href=\"https://blogs.windows.com/windowsexperience/2012/11/29/how-i-use-rooms-on-windows-phone-8/\">Rooms</a> feature in Windows Phone. 😢</p>\n</div>\n","tags":[]},{"title":"Geospatial is a function of your life ","contentType":"star","date":"2022-12-06 10:43","url":"/star/florence-fsharp-geospatial/","content":"<div class=\"response-target\" >→ <a href=\"https://dev.to/cognipla/geospatial-is-a-function-of-your-life-1924\" >https://dev.to/cognipla/geospatial-is-a-function-of-your-life-1924</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Florence, a low-code geospatial library for everyone that ranks city places with (life-inspired) functions.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Florence heavily laverages .NET Interactive/Polyglot Notebooks runtime to hide and execute code behind the scene (including breaking some language constraints).</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Microsoft Open-Sources Agricultural AI Toolkit FarmVibes.AI ","contentType":"reshare","date":"2022-12-06 09:55","url":"/reshare/msft-open-sources-farmvibes-ai/","content":"<div class=\"response-target\" >→ <a href=\"https://www.infoq.com/news/2022/12/microsoft-farmvibes/\" >https://www.infoq.com/news/2022/12/microsoft-farmvibes/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Microsoft Research recently open-sourced <a href=\"https://www.microsoft.com/en-us/research/project/project-farmvibes/articles/farmvibes-ai/\">FarmVibes.AI</a>, a suite of ML models and tools for sustainable agriculture. FarmVibes.AI includes data processing workflows for fusing multiple sets of spatiotemporal and geospatial data, such as weather data and satellite and drone imagery.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"How open source is revitalizing the payphone","contentType":"reshare","date":"2022-11-30 10:04","url":"/reshare/philtel-open-source-payphone/","content":"<div class=\"response-target\" >→ <a href=\"https://opensource.com/article/22/11/open-source-payphone-philtel\" >https://opensource.com/article/22/11/open-source-payphone-philtel</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>PhilTel is a telephone collective based in Philadelphia, Pennsylvania, focusing on making communications accessible to everyone by installing free-to-use payphones. While you'll be able to make standard telephone calls through our phones, we're also focusing on offering interesting services or experiences. We don't want to only facilitate human-to-human interaction but also human-to-machine interaction and give people an environment where they can explore the telephone network and learn from it.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"The Perks of a High-Documentation, Low-Meeting Work Culture","contentType":"star","date":"2022-11-23 22:12","url":"/star/perks-high-documentation-culture/","content":"<div class=\"response-target\" >→ <a href=\"https://www.tremendous.com/blog/the-perks-of-a-high-documentation-low-meeting-work-culture\" >https://www.tremendous.com/blog/the-perks-of-a-high-documentation-low-meeting-work-culture</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>the intangible, overarching benefit of practicing meeting mindfulness is this: you spend less of your day sort-of-listening and more of your day really thinking.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Database of nearly 500 million WhatsApp users’ mobile phones is up for sale","contentType":"reshare","date":"2022-11-23 15:00","url":"/reshare/whatsapp-data-leak-alternatives/","content":"<div class=\"response-target\" >→ <a href=\"https://cybernews.com/news/whatsapp-data-leak/\" >https://cybernews.com/news/whatsapp-data-leak/</a></div><div class=\"response-content\" ><p>Just a friendly reminder, there are <a href=\"/posts/alternatives-to-whatsapp\">alternatives</a>.</p>\n</div>\n","tags":[]},{"title":"Tumblr to add support for ActivityPub","contentType":"reshare","date":"2022-11-23 14:49","url":"/reshare/tumblr-activitypub/","content":"<div class=\"response-target\" >→ <a href=\"https://techcrunch.com/2022/11/21/tumblr-to-add-support-for-activitypub-the-social-protocol-powering-mastodon-and-other-apps/\" >https://techcrunch.com/2022/11/21/tumblr-to-add-support-for-activitypub-the-social-protocol-powering-mastodon-and-other-apps/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Automattic CEO Matt Mullenweg — whose company acquired Tumblr from Verizon in 2019 — suggested...Tumblr... would soon add ...activitypub.</p>\n</blockquote>\n<p>I'm perfectly happy using my site as the main place to post content but considering it's under new management and adopting open standards, it might be time to checkout out Tumblr again.</p>\n</div>\n","tags":[]},{"title":"Digital Library of Amateur Radio & Communications Surpasses 25,000 Items","contentType":"star","date":"2022-11-21 23:06","url":"/star/digital-library-amateur-radio-25k/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.archive.org/2022/11/16/digital-library-of-amateur-radio-communications-surpasses-25000-items/\" >https://blog.archive.org/2022/11/16/digital-library-of-amateur-radio-communications-surpasses-25000-items/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In the six weeks since announcing that Internet Archive has begun gathering content for the Digital Library of Amateur Radio and Communications (DLARC), the project has quickly grown to more than 25,000 items, including ham radio newsletters, podcasts, videos, books, and catalogs.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Telepathic technical writing","contentType":"star","date":"2022-11-21 10:29","url":"/star/telepathic-tech-writing/","content":"<div class=\"response-target\" >→ <a href=\"https://derw.substack.com/p/telepathic-technical-writing\" >https://derw.substack.com/p/telepathic-technical-writing</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...blog posts are always async, but they can lead to conversations and debates, once the reader is done reading. There's also the nature of blog posts being one-to-many, whereas chat is many-to-many, if done in a public channel. One-to-many forms of communication should generally be more formal, but can spread ideas and thoughts more coherently than many-to-many.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Digital Books wear out faster than Physical Books","contentType":"reshare","date":"2022-11-20 23:14","url":"/reshare/digital-physical-books-wear/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.archive.org/2022/11/15/digital-books-wear-out-faster-than-physical-books/\" >https://blog.archive.org/2022/11/15/digital-books-wear-out-faster-than-physical-books/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Our paper books have lasted hundreds of years on our shelves and are still readable. Without active maintenance, we will be lucky if our digital books last a decade.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Feta - Matrix Server Distribution","contentType":"star","date":"2022-11-16 22:29","url":"/star/feta-matrix-raspi/","content":"<div class=\"response-target\" >→ <a href=\"https://www.feta.bz/\" >https://www.feta.bz/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Feta is a Matrix server distribution for the Raspberry Pi 3 and 4.</p>\n</blockquote>\n<p>This looks like a great project to get started with self-hosting and join the Matrix network. Having something similar for the fediverse would be great as well. I've hosted Matrix and Mastodon servers on a Raspberry Pi before so I know it's up to the task.</p>\n</div>\n","tags":[]},{"title":"Use RSS for privacy and efficiency","contentType":"star","date":"2022-11-16 00:08","url":"/star/rss-privacy-efficiency/","content":"<div class=\"response-target\" >→ <a href=\"https://rsapkf.org/weblog/q2z/\" >https://rsapkf.org/weblog/q2z/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Two Decades of Blogging, The Free As A Bird Edition","contentType":"star","date":"2022-11-05 16:52","url":"/star/two-decades-blogging-free-bird-edition/","content":"<div class=\"response-target\" >→ <a href=\"https://www.zylstra.org/blog/2022/11/two-decades-of-blogging-the-free-as-a-bird-edition/\" >https://www.zylstra.org/blog/2022/11/two-decades-of-blogging-the-free-as-a-bird-edition/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"It looks like I’m moving to Mastodon","contentType":"reshare","date":"2022-11-05 15:42","url":"/reshare/willison-mastodon/","content":"<div class=\"response-target\" >→ <a href=\"https://simonwillison.net/2022/Nov/5/mastodon/\" >https://simonwillison.net/2022/Nov/5/mastodon/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"How to Leave Dying Social Media Platforms","contentType":"reshare","date":"2022-10-30 12:18","url":"/reshare/how-to-leave-dying-social-platforms/","content":"<div class=\"response-target\" >→ <a href=\"https://doctorow.medium.com/how-to-leave-dying-social-media-platforms-9fc550fe5abf\" >https://doctorow.medium.com/how-to-leave-dying-social-media-platforms-9fc550fe5abf</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"A Proposal for News Organization Mastodon Servers and More","contentType":"star","date":"2022-10-28 21:07","url":"/star/proposal-news-mastodon-servers/","content":"<div class=\"response-target\" >→ <a href=\"https://newsletter.danhon.com/archive/4230/\" >https://newsletter.danhon.com/archive/4230/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Here’s how it works in practice:</p>\n<ol>\n<li>A news organization (or any organization, let’s just start with news) already asserts ownership of its domain e.g. via its certificate, so we piggyback trust off its domain.</li>\n<li>It stands up a Mastodon or other social server at a standard address. I’d propose follow.washingtonpost.com but there’s a bunch of reasons why you might do something else, see below, and uses the agreed well-known autodiscovery protocol to return the address for its Mastodon server (but I don’t see an entry for activitypub or Mastodon yet).</li>\n<li>It creates accounts for its staff on its Mastodon server. Nobody else gets an account; the public can’t sign up.</li>\n</ol>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>What you get:</p>\n<ol>\n<li>Verified accounts. Instead of relying on a third party to “verify” your account by sticking a blue check against your display name, account provenance and “verification” is inherited as a property of the Mastodon server....</li>\n<li>Ease of discovery...all a user would have to do, to find Washington Post accounts to follow, would be to know the washingtonpost.com domain. Autodiscovery would let your Mastodon client point itself to the appropriate server.</li>\n</ol>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Not just news organizations...anyone can set up a Mastodon server...the federation means that “official” accounts become “more official” when their server home is hung off the domain of the actual organization.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>You wouldn’t need Twitter (or anyone else, really) to verify that the UK Prime Minister’s account is official, because you’d have following.gov.uk as the Mastodon server, which means you can trust that server as much as you trust .gov.uk domains.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Your university or college wants you to have a social media account? Sure, you can have it hosted at following.ucla.edu.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>And yes, brands can get in on it. Sure. That way there’s a tiny chance you’re following the Proper Brand Account rather than a Parody Brand Account, which… is probably for the best. Or it’s easier to see that a Parody Account is a Parody Account because you can look at the parent server.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Why I use Jellyfin for my home media library","contentType":"reply","date":"2022-10-27 20:13","url":"/reply/jellyfin-home-media-library/","content":"<div class=\"response-target\" >→ <a href=\"https://www.jeffgeerling.com/blog/2022/why-i-use-jellyfin-my-home-media-library\" >https://www.jeffgeerling.com/blog/2022/why-i-use-jellyfin-my-home-media-library</a></div><div class=\"response-content\" ><p>It's also built with .NET 🙂</p>\n</div>\n","tags":[]},{"title":"What work looks like","contentType":"star","date":"2022-10-25 23:01","url":"/star/what-work-looks-like/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.jim-nielsen.com/2022/what-work-looks-like/\" >https://blog.jim-nielsen.com/2022/what-work-looks-like/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Suspension","contentType":"star","date":"2022-10-25 22:44","url":"/star/ott-suspension/","content":"<div class=\"response-target\" >→ <a href=\"https://matthiasott.com/notes/suspension\" >https://matthiasott.com/notes/suspension</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>write your most important thoughts on your own site. You can share the link on as many platforms as you like and have conversations with anyone who wants to connect with you and your work. But nobody can take it from you. You are in control. Forever.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Android 12L for Surface Duo is now available","contentType":"star","date":"2022-10-24 21:58","url":"/star/surface-duo-android-12L/","content":"<div class=\"response-target\" >→ <a href=\"https://www.windowscentral.com/hardware/laptops/android-12l-for-surface-duo-is-now-available-with-new-ui-and-more\" >https://www.windowscentral.com/hardware/laptops/android-12l-for-surface-duo-is-now-available-with-new-ui-and-more</a></div><div class=\"response-content\" ><p>Downloading now.</p>\n</div>\n","tags":[]},{"title":"PyTorch Joins Linux Foundation as Top Level Project","contentType":"reshare","date":"2022-10-20 01:07","url":"/reshare/pytorch-lf-top-level-project/","content":"<div class=\"response-target\" >→ <a href=\"https://pytorch.org/blog/PyTorchfoundation/\" >https://pytorch.org/blog/PyTorchfoundation/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>PyTorch is moving to the Linux Foundation (LF) as a top-level project under the name PyTorch Foundation.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The PyTorch Technical Governance now supports a hierarchical maintainer structure and clear outlining of processes around day to day work and escalations. This doesn’t change how we run things, but it does add discipline and openness that at our scale feels essential and timely.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Pocket Casts Mobile Apps Are Now Open Source","contentType":"star","date":"2022-10-20 00:23","url":"/star/pocket-casts-mobile-open-source/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.pocketcasts.com/2022/10/19/pocket-casts-mobile-apps-are-now-open-source/\" >https://blog.pocketcasts.com/2022/10/19/pocket-casts-mobile-apps-are-now-open-source/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We’ve been eager to take this step since we joined Automattic last year...</p>\n</blockquote>\n<p>Wow. Yet another company I didn't know was owned by Automaticc (parent company of WordPress). So not only do they have a stake in regular blogs and websites, but they also are in the microblogging space with Tumblr. With Pocket Casts they're now into podcasts too. Good for them.</p>\n<blockquote class=\"blockquote\">\n<p>We believe that podcasting can not and should not be controlled by Apple and Spotify, and instead support a diverse ecosystem of third-party clients.</p>\n</blockquote>\n<p>I couldn't agree more. Though to be fair to Apple, in all the years since podcasts have been a thing, despite them being one of the main indices, they didn't make any overt attempts to lock down the ecosystem. It's not until companies like Amazon and Spotify tried to make certain content platform exclusives that the ecosystem has started to feel more closed.</p>\n</div>\n","tags":[]},{"title":"Thoughts on Bluesky AT Protocol","contentType":"star","date":"2022-10-19 09:56","url":"/star/staltz-thoughts-bluesky-at-protocol/","content":"<div class=\"response-target\" >→ <a href=\"https://twitter.com/andrestaltz/status/1582448952057958401\" >https://twitter.com/andrestaltz/status/1582448952057958401</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>My experience working on SSB (i.e. non-crypto fully decentralized protocol) is suitable for small world communication, and definitely not suitable for big world. Email proves that federation can do big world.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Bluesky - The Authenticated Transport (AT) Protocol","contentType":"reshare","date":"2022-10-18 17:23","url":"/reshare/bluesky-at-protocol/","content":"<div class=\"response-target\" >→ <a href=\"https://blueskyweb.org/blog/10-18-2022-the-at-protocol\" >https://blueskyweb.org/blog/10-18-2022-the-at-protocol</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Bluesky was created to build a social protocol. In the spring, we released “ADX,” the very first iteration of the protocol...ADX is now the “Authenticated Transport Protocol&amp;quot;...more simply, the “AT Protocol.”</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The “AT Protocol” is a new federated social network</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>What makes AT Protocol unique:</p>\n<ul>\n<li>Account portability</li>\n<li>Algorithmic choice</li>\n<li>Interoperation</li>\n<li>Performance</li>\n</ul>\n</blockquote>\n<p><a href=\"https://atproto.com/\">AT Procol website</a></p>\n</div>\n","tags":[]},{"title":"Overdue insights on daily blogging","contentType":"star","date":"2022-10-18 17:20","url":"/star/overdue-insights-daily-blogging/","content":"<div class=\"response-target\" >→ <a href=\"https://herbertlui.net/overdue-insights-on-daily-blogging/\" >https://herbertlui.net/overdue-insights-on-daily-blogging/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Here are a bunch of not-so-obvious lessons I’ve internalized through writing each day:</p>\n<ul>\n<li>Writing can be a starting point, not an ending one</li>\n<li>Write to think</li>\n<li>The power of DIFY: Do it for yourself, don’t think too much about what you want other people to get out of it.</li>\n<li>Think small</li>\n<li>Gain energy</li>\n<li>A lot of books are collections of blog posts</li>\n<li>Small is an unblocker</li>\n</ul>\n</blockquote>\n</div>\n","tags":[]},{"title":"Patterns for collaborative blogging","contentType":"reshare","date":"2022-10-18 17:19","url":"/reshare/patterns-collaborative-blogging/","content":"<div class=\"response-target\" >→ <a href=\"https://visitmy.website/2022/10/04/patterns-for-collaborative-blogging/\" >https://visitmy.website/2022/10/04/patterns-for-collaborative-blogging/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"HTTP Archive State of the Web 2022 Report","contentType":"reshare","date":"2022-10-18 09:20","url":"/reshare/http-archive-state-of-web-2022-almanac/","content":"<div class=\"response-target\" >→ <a href=\"https://almanac.httparchive.org/en/2022/\" >https://almanac.httparchive.org/en/2022/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The Web Almanac is a comprehensive report on the state of the web, backed by real data and trusted web experts. The 2022 edition is comprised of 23 chapters spanning aspects of page content, user experience, publishing, and distribution.</p>\n</blockquote>\n<ul>\n<li><a href=\"https://almanac.httparchive.org/en/2022/seo\">Chapter 10: SEO</a></li>\n<li><a href=\"https://almanac.httparchive.org/en/2022/privacy\">Chapter 13: Privacy</a></li>\n<li><a href=\"https://almanac.httparchive.org/en/2022/pwa\">Chapter 17: PWA</a></li>\n<li><a href=\"https://almanac.httparchive.org/en/2022/jamstack\">Chapter 19: Jamstack</a></li>\n<li><a href=\"https://almanac.httparchive.org/en/2022/page-weight\">Chapter 21: Page Weight</a></li>\n</ul>\n</div>\n","tags":[]},{"title":"Readium LCP - Digital Rights Management Technology","contentType":"reshare","date":"2022-10-16 11:14","url":"/reshare/readium-lcp-drm-technology/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.archive.org/2022/10/10/new-ebook-protection-software-gaining-popularity-among-publishers-and-libraries/\" >https://blog.archive.org/2022/10/10/new-ebook-protection-software-gaining-popularity-among-publishers-and-libraries/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Readium LCP was developed five years ago to protect digital files from unauthorized distribution. Unlike proprietary platforms, the technology is open to anyone who wants to look inside the codebase and make improvements. It is a promising alternative for libraries and users wanting to avoid the limitations of traditional DRM.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Low Earth Orbit Visualization","contentType":"star","date":"2022-10-14 21:57","url":"/star/leo-visualization/","content":"<div class=\"response-target\" >→ <a href=\"https://platform.leolabs.space/visualization\" >https://platform.leolabs.space/visualization</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Discu.eu - Blog comments on a static site via social networks","contentType":"reshare","date":"2022-10-14 15:41","url":"/reshare/discueu-comments-static-page/","content":"<div class=\"response-target\" >→ <a href=\"https://berk.es/2022/10/12/blog-comments-on-a-static-site-via-social-networks/\" >https://berk.es/2022/10/12/blog-comments-on-a-static-site-via-social-networks/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>discu.eu, a fantastic service that you provide with an URL and then gives back results from various social networks. Places where that URL is discussed. Hackernews, Reddit, and/or Lobsters. It has an API, that I can call with some JavaScript and then insert in the page.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Matrix protocol added to ejabberd","contentType":"reshare","date":"2022-10-14 15:39","url":"/reshare/ejabberd-matrix-protocol/","content":"<div class=\"response-target\" >→ <a href=\"https://www.process-one.net/blog/matrix-protocol-added-to-ejabberd/\" >https://www.process-one.net/blog/matrix-protocol-added-to-ejabberd/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Identity - The Hellō Cooperative","contentType":"reshare","date":"2022-10-14 15:36","url":"/reshare/hello-decentralized-identity-coop/","content":"<div class=\"response-target\" >→ <a href=\"https://www.hello.coop/pages/approach.html\" >https://www.hello.coop/pages/approach.html</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Swiss Dwellings: A large dataset of apartment models","contentType":"reshare","date":"2022-10-14 15:33","url":"/reshare/swiss-dwelling-apartment-dataset/","content":"<div class=\"response-target\" >→ <a href=\"https://zenodo.org/record/7070952#.Y0m5eX7MK00\" >https://zenodo.org/record/7070952#.Y0m5eX7MK00</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This dataset contains detailed data on 42,207 apartments (242,257 rooms) in 3,093 buildings including their geometries, room typology as well as their visual, acoustical, topological and daylight characteristics.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Streaming services lost the plot","contentType":"reply","date":"2022-10-13 15:06","url":"/reply/streaming-companies-lost-plot/","content":"<div class=\"response-target\" >→ <a href=\"https://www.jeffgeerling.com/blog/2022/streaming-services-lost-plot\" >https://www.jeffgeerling.com/blog/2022/streaming-services-lost-plot</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>I got serious about consolidating the media my family consumes; I decided to buy blu-ray and DVD copies of all the movies and TV shows we actually cared about, and rip them onto a NAS.</p>\n<p>I'm glad I've gone down this path, and become more intentional about my media consumption, especially as companies are deleting already-purchased content from users' media libraries! It's sickening (and, IMHO, should be illegal) they can show a 'buy' button for a DRM'ed digital downloads that the user never actually 'owns'.</p>\n</blockquote>\n<p>Couldn't agree more. Although the focus of this post is on video, you can say the same for music and books.</p>\n</div>\n","tags":[]},{"title":"Element - New app layout for Android","contentType":"reshare","date":"2022-10-13 10:32","url":"/reshare/matrix-element-android-redesign-2022/","content":"<div class=\"response-target\" >→ <a href=\"https://twitter.com/element_hq/status/1580555805803261952\" >https://twitter.com/element_hq/status/1580555805803261952</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Doc Quixote","contentType":"star","date":"2022-10-12 23:30","url":"/star/doc-quixote/","content":"<div class=\"response-target\" >→ <a href=\"http://scripting.com/2022/10/01/133834.html?title=docQuixote\" >http://scripting.com/2022/10/01/133834.html?title=docQuixote</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>I spent great time, energy and money, over many years to create the writing and programming environment I wanted to use and I wanted my peers to use, so we could work together to create species-saving communication tools, and just beauty...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...I read the story of David Bowie's last days, he did something amazing when he knew he had a short time to live. He stepped back and got out of the way. He understood this is no longer his world.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>When you're young, you think expansively, and as you get old reality sinks in and your imagination contracts. The horizon gets closer and closer. We don't get to mold the world, we are not gods, no matter how good or generous, smart of ruthless you may be, we all start out young and if we're lucky we get old and then we're gone.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"How Wine works 101","contentType":"star","date":"2022-10-12 18:25","url":"/star/how-wine-works/","content":"<div class=\"response-target\" >→ <a href=\"https://werat.dev/blog/how-wine-works-101/\" >https://werat.dev/blog/how-wine-works-101/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Sane - Infopunk's Digital Garden","contentType":"star","date":"2022-10-11 22:42","url":"/star/sane-app/","content":"<div class=\"response-target\" >→ <a href=\"https://www.sane.fyi/\" >https://www.sane.fyi/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Adding fragmention links to my website","contentType":"star","date":"2022-10-11 21:40","url":"/star/jamesg-fragmentation-links/","content":"<div class=\"response-target\" >→ <a href=\"https://jamesg.blog/2022/10/11/fragmention-links/\" >https://jamesg.blog/2022/10/11/fragmention-links/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Researchers Publish Results of NLP Community Metasurvey","contentType":"reshare","date":"2022-10-11 20:38","url":"/reshare/nlp-metasurvey-results/","content":"<div class=\"response-target\" >→ <a href=\"https://www.infoq.com/news/2022/10/nlp-community-metasurvey/\" >https://www.infoq.com/news/2022/10/nlp-community-metasurvey/</a></div><div class=\"response-content\" ><p>Relevant links:</p>\n<ul>\n<li><a href=\"https://nlpsurvey.net/\">Results</a></li>\n<li><a href=\"https://nlpsurvey.net/nlp-metasurvey-results.pdf\">Report</a></li>\n</ul>\n</div>\n","tags":[]},{"title":"Rediscover the Joy in Coding: learn F#!","contentType":"star","date":"2022-10-10 18:28","url":"/star/manning-rediscover-joy-coding-fsharp/","content":"<div class=\"response-target\" >→ <a href=\"https://freecontent.manning.com/rediscover-the-joy-in-coding-learn-f\" >https://freecontent.manning.com/rediscover-the-joy-in-coding-learn-f</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Can Indie Social Media Save Us?","contentType":"star","date":"2022-10-09 22:09","url":"/star/indie-social-media-save-us/","content":"<div class=\"response-target\" >→ <a href=\"https://www.newyorker.com/tech/annals-of-technology/can-indie-social-media-save-us\" >https://www.newyorker.com/tech/annals-of-technology/can-indie-social-media-save-us</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Handwritten Blog","contentType":"star","date":"2022-10-09 14:25","url":"/star/handwrittern-blog/","content":"<div class=\"response-target\" >→ <a href=\"https://handwritten.blog/\" >https://handwritten.blog/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Things Learned Blogging","contentType":"reply","date":"2022-10-06 15:50","url":"/reply/things-learned-blogging/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.jim-nielsen.com/2021/things-learned-blogging/\" >https://blog.jim-nielsen.com/2021/things-learned-blogging/</a></div><div class=\"response-content\" ><p>Both this post as well as <a href=\"https://macwright.com/2019/02/06/how-to-blog.html\">Tom MacWright's &amp;quot;How to Blog&amp;quot;</a> resonate. I've been posting more consistently to my microblog feeds. These posts are more informal, but just the aspect of producing something even if it's just a short snippet is gratifying. The informality of it makes the posts significantly shorter but the pace at which I publish content and share ideas is faster.</p>\n</div>\n","tags":[]},{"title":"Jim Nielsen: My Favorite RSS Feeds","contentType":"reshare","date":"2022-10-06 15:35","url":"/reshare/nielsen-favorite-rss-feeds/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.jim-nielsen.com/2019/i-love-rss/\" >https://blog.jim-nielsen.com/2019/i-love-rss/</a></div><div class=\"response-content\" ><p>It's always fun to stumble upon these lists and finding more interesting people and websites to follow.</p>\n</div>\n","tags":[]},{"title":"Numerical F#nalysis 01:Introduction","contentType":"reshare","date":"2022-10-06 09:24","url":"/reshare/numerical-fnalysis-intro/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=QMSmcpn8nLM&amp;amp;amp;ab_channel=ResairchiaVideos\" >https://www.youtube.com/watch?v=QMSmcpn8nLM&amp;amp;ab_channel=ResairchiaVideos</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"F# with Python - TensorFlow Binding","contentType":"reshare","date":"2022-10-06 09:21","url":"/reshare/fsharp-python-tf-binding/","content":"<div class=\"response-target\" >→ <a href=\"https://www.compositional-it.com/news-blog/f-with-python-pt-2-tensorflow-binding/\" >https://www.compositional-it.com/news-blog/f-with-python-pt-2-tensorflow-binding/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Unredacted Magazine - Issue 004","contentType":"reshare","date":"2022-10-05 20:13","url":"/reshare/unredacted-mag-issue-4/","content":"<div class=\"response-target\" >→ <a href=\"https://unredactedmagazine.com/issues/004.pdf\" >https://unredactedmagazine.com/issues/004.pdf</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Locations/Venues - IndieWebCamp Berlin 2022","contentType":"star","date":"2022-10-05 15:52","url":"/star/indiewebcamp-berlin-2022-location-venues/","content":"<div class=\"response-target\" >→ <a href=\"https://archive.org/details/locations-venues-indie-web-camp-berlin-2022\" >https://archive.org/details/locations-venues-indie-web-camp-berlin-2022</a></div><div class=\"response-content\" ><p>Interesting discussion at about 21:30 on a federated wiki / review aggregator.</p>\n</div>\n","tags":[]},{"title":"The OG Social Network: Other People's Websites","contentType":"reshare","date":"2022-10-05 15:25","url":"/reshare/og-social-network-other-peoples-websites/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.jim-nielsen.com/2022/other-peoples-websites/\" >https://blog.jim-nielsen.com/2022/other-peoples-websites/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Google+ was Google trying to mimic the walled garden of Facebook — their “how” of extracting value from the people of the internet. But they already had an answer for Facebook’s News Feed in front of them: Blogger / Google Reader, the read / write of the internet.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>They provide the tools – Reader, Blogger, Search — we provide the personal websites. The open, accessible, indexable web as The Next Great Social Network.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Patching the open web","contentType":"reply","date":"2022-10-05 10:46","url":"/reply/patching-open-web/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.jim-nielsen.com/2022/patching-open-web/#readlists\" >https://blog.jim-nielsen.com/2022/patching-open-web/#readlists</a></div><div class=\"response-content\" ><p>I've been doing something similar to readlists, except with RSS feeds. I create a <a href=\"https://manual.calibre-ebook.com/news.html\">custom recipe in Calibre</a> which pulls and aggregates the 50 latest articles for each feed in my recipe. I limit it to only look at articles published in the past day since I do this every evening. Think of it like the evening paper. The result is an EPUB file which I then import into my e-book reader.</p>\n<p>A few advantages I've found to doing this are:</p>\n<ul>\n<li>I take a break from the computer.</li>\n<li>Since the e-book reader is offline, I focus on reading the article and don't have the option to click on links and get distracted browsing the internet.</li>\n<li>Since I'm already using the e-book reader, it's easy to transition to reading a book.</li>\n</ul>\n</div>\n","tags":[]},{"title":"Minimalism as Narcissism","contentType":"star","date":"2022-10-03 20:56","url":"/star/minimalism-narcissism/","content":"<div class=\"response-target\" >→ <a href=\"https://www.thisdaysportion.com/posts/minimalism-as-narcissim/\" >https://www.thisdaysportion.com/posts/minimalism-as-narcissim/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Stellarium 1.0 Released","contentType":"star","date":"2022-10-03 20:28","url":"/star/stellarium-1-0/","content":"<div class=\"response-target\" >→ <a href=\"https://stellarium.org/release/2022/10/01/stellarium-1.0.html\" >https://stellarium.org/release/2022/10/01/stellarium-1.0.html</a></div><div class=\"response-content\" ><p>Can't believe this is the first time I'd heard of this desktop app. Usually using mobile apps like Sky Guide is convenient when on the go, but Stellarium not only seems to have lots of information, but also cross-platform and web-based.</p>\n</div>\n","tags":[]},{"title":"Note-taking Apps: Bear and Joplin","contentType":"star","date":"2022-10-02 18:16","url":"/star/bear-joplin/","content":"<div class=\"response-target\" >→ <a href=\"https://wilw.dev/blog/2022/10/02/bear-joplin/\" >https://wilw.dev/blog/2022/10/02/bear-joplin/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"FlyLo - Terrifier 2","contentType":"reply","date":"2022-09-29 12:32","url":"/reply/flylo-terrifier-2/","content":"<div class=\"response-target\" >→ <a href=\"https://twitter.com/flyinglotus/status/1575517121122996225\" >https://twitter.com/flyinglotus/status/1575517121122996225</a></div><div class=\"response-content\" ><p>This ☝️. If you know, you know. 🔥</p>\n</div>\n","tags":[]},{"title":"Stadia Shuts Down","contentType":"reshare","date":"2022-09-29 12:24","url":"/reshare/google-stadia-shutdown/","content":"<div class=\"response-target\" >→ <a href=\"https://www.theverge.com/2022/9/29/23378713/google-stadia-shutting-down-game-streaming-january-2023\" >https://www.theverge.com/2022/9/29/23378713/google-stadia-shutting-down-game-streaming-january-2023</a></div><div class=\"response-content\" ><p><img src=\"https://c.tenor.com/pGORhlFQBx0AAAAC/dj-khaled-another-one.gif\" class=\"img-fluid\" alt=\"DJ Khaled Another One GIF\" /></p>\n</div>\n","tags":[]},{"title":"How to - Build a personal webpage from scratch","contentType":"star","date":"2022-09-29 11:42","url":"/star/how-to-build-personal-webpage-scratch/","content":"<div class=\"response-target\" >→ <a href=\"https://rutar.org/writing/how-to-build-a-personal-webpage-from-scratch/\" >https://rutar.org/writing/how-to-build-a-personal-webpage-from-scratch/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Appeal of Small Computers","contentType":"star","date":"2022-09-28 21:42","url":"/star/appeal-small-pc/","content":"<div class=\"response-target\" >→ <a href=\"https://andregarzia.com/2022/09/The-appeal-of-small-computers.html\" >https://andregarzia.com/2022/09/The-appeal-of-small-computers.html</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"MNT Pocket Reform - Open Source Pocket PC","contentType":"reshare","date":"2022-09-28 21:38","url":"/reshare/mnt-pocket-reform/","content":"<div class=\"response-target\" >→ <a href=\"https://spectrum.ieee.org/meet-an-open-source-pc-that-can-fit-in-your-pocket\" >https://spectrum.ieee.org/meet-an-open-source-pc-that-can-fit-in-your-pocket</a></div><div class=\"response-content\" ><p><img src=\"https://c.tenor.com/R0d3sZ4fq6EAAAAC/money-dollars.gif\" class=\"img-fluid\" alt=\"Take My Money GIF\" /></p>\n</div>\n","tags":[]},{"title":"DALL-E Available Without Waitlist","contentType":"reshare","date":"2022-09-28 11:36","url":"/reshare/dalle-no-waitlist/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/blog/dall-e-now-available-without-waitlist/\" >https://openai.com/blog/dall-e-now-available-without-waitlist/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Federated Wiki","contentType":"star","date":"2022-09-28 10:44","url":"/star/fed-wiki-udell/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.jonudell.net/tag/fedwiki/\" >https://blog.jonudell.net/tag/fedwiki/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"GitHub for English Teachers","contentType":"star","date":"2022-09-28 10:42","url":"/star/github-english-teachers/","content":"<div class=\"response-target\" >→ <a href=\"https://www.infoworld.com/article/3674309/github-for-english-teachers.html\" >https://www.infoworld.com/article/3674309/github-for-english-teachers.html</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"FOSDEM 2023 Dates (Feb 4 & 5 2023)","contentType":"reshare","date":"2022-09-26 21:48","url":"/reshare/fosdem-2023-dates/","content":"<div class=\"response-target\" >→ <a href=\"https://fosdem.org/2023/news/2022-09-14-fosdem-2023-dates/\" >https://fosdem.org/2023/news/2022-09-14-fosdem-2023-dates/</a></div><div class=\"response-content\" ><p>It feels like the 2022 conference was just yesterday. In any case, save the date February 4-5,2023.</p>\n</div>\n","tags":[]},{"title":"The story behind Joplin","contentType":"reshare","date":"2022-09-26 20:44","url":"/reshare/joplin-interview/","content":"<div class=\"response-target\" >→ <a href=\"https://opensource.com/article/22/9/joplin-interview\" >https://opensource.com/article/22/9/joplin-interview</a></div><div class=\"response-content\" ><p>Good read. Other than Emacs, <a href=\"https://joplinapp.org/\">Joplin</a> is my go-to notetaking application.</p>\n</div>\n","tags":[]},{"title":"Launch of Proton Drive - Encrypted Cloud Storage","contentType":"reshare","date":"2022-09-26 20:15","url":"/reshare/proton-drive-launch/","content":"<div class=\"response-target\" >→ <a href=\"https://proton.me/blog/proton-drive-launch\" >https://proton.me/blog/proton-drive-launch</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Web-Based Matrix Rain","contentType":"star","date":"2022-09-25 22:36","url":"/star/web-matrix-rain/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/Rezmason/matrix\" >https://github.com/Rezmason/matrix</a></div><div class=\"response-content\" ><p>Matrix Rain sample with different variations.</p>\n</div>\n","tags":[]},{"title":"MLPerf - Modular ML for Benchmarks Working Group","contentType":"star","date":"2022-09-23 10:04","url":"/star/mlperf-modular-ml-workgroup/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/mlcommons/ck/blob/master/docs/mlperf-education-workgroup.md\" >https://github.com/mlcommons/ck/blob/master/docs/mlperf-education-workgroup.md</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Narrative-Commitment-Task (NCT) - Goal Setting","contentType":"star","date":"2022-09-23 09:57","url":"/star/nct-goal-setting/","content":"<div class=\"response-target\" >→ <a href=\"https://mariachec.medium.com/on-objectives-okrs-ncts-lessons-from-reinventing-the-company-goal-setting-10eb969e4ca5\" >https://mariachec.medium.com/on-objectives-okrs-ncts-lessons-from-reinventing-the-company-goal-setting-10eb969e4ca5</a></div><div class=\"response-content\" ><p>Alternative to OKRs</p>\n</div>\n","tags":[]},{"title":"Moving with Prototypes","contentType":"star","date":"2022-09-22 21:48","url":"/star/moving-prototypes/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.jim-nielsen.com/2022/moving-with-prototypes/\" >https://blog.jim-nielsen.com/2022/moving-with-prototypes/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Mutant Standard Emojis","contentType":"reshare","date":"2022-09-22 20:33","url":"/reshare/mutant-standard-emojis/","content":"<div class=\"response-target\" >→ <a href=\"https://mutant.tech/\" >https://mutant.tech/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>an alternative...emoji set.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"ARM Support for VS Extensions","contentType":"star","date":"2022-09-22 20:02","url":"/star/vs-extension-arm-support/","content":"<div class=\"response-target\" >→ <a href=\"https://devblogs.microsoft.com/visualstudio/now-introducing-arm64-support-for-vs-extensions/\" >https://devblogs.microsoft.com/visualstudio/now-introducing-arm64-support-for-vs-extensions/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"TensorStore","contentType":"reshare","date":"2022-09-22 19:56","url":"/reshare/tensorstore/","content":"<div class=\"response-target\" >→ <a href=\"https://ai.googleblog.com/2022/09/tensorstore-for-high-performance.html\" >https://ai.googleblog.com/2022/09/tensorstore-for-high-performance.html</a></div><div class=\"response-content\" ><p>High-perf, scalable array storage that can be used for scenarios like language models.</p>\n</div>\n","tags":[]},{"title":"5 Git configurations I make on Linux","contentType":"star","date":"2022-09-22 19:44","url":"/star/five-git-configurations-linux/","content":"<div class=\"response-target\" >→ <a href=\"https://opensource.com/article/22/9/git-configuration-linux\" >https://opensource.com/article/22/9/git-configuration-linux</a></div><div class=\"response-content\" ><ol>\n<li>Create global configuration</li>\n<li>Set default name</li>\n<li>Set default email address</li>\n<li>Set default branch name</li>\n<li>Set default editor</li>\n</ol>\n</div>\n","tags":[]},{"title":"Open AI Whisper","contentType":"star","date":"2022-09-22 19:44","url":"/star/open-ai-whisper/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/blog/whisper/\" >https://openai.com/blog/whisper/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"An app can be a home-cooked meal","contentType":"reshare","date":"2022-09-21 22:28","url":"/reshare/app-home-cooked/","content":"<div class=\"response-target\" >→ <a href=\"https://www.robinsloan.com/notes/home-cooked-app/\" >https://www.robinsloan.com/notes/home-cooked-app/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>When you liberate programming from the requirement to be general and professional and scalable, it becomes a different activity altogether, just as cooking at home is really nothing like cooking in a commercial kitchen.</p>\n</blockquote>\n<p>Same goes for websites and self-hosting.</p>\n</div>\n","tags":[]},{"title":"Welcome to La Terminal","contentType":"reshare","date":"2022-09-21 19:48","url":"/reshare/welcome-to-la-terminal/","content":"<div class=\"response-target\" >→ <a href=\"https://xibbon.com/terminal/2022/09/21/welcome-to-la-terminal.html\" >https://xibbon.com/terminal/2022/09/21/welcome-to-la-terminal.html</a></div><div class=\"response-content\" ><p>I'm not an Apple user, but this is cool.</p>\n</div>\n","tags":[]},{"title":"RSS at 20 still going string","contentType":"reply","date":"2022-09-18 10:20","url":"/reply/rss-20-at-20/","content":"<div class=\"response-target\" >→ <a href=\"https://www.zylstra.org/blog/2022/09/rss2-0-at-20/\" >https://www.zylstra.org/blog/2022/09/rss2-0-at-20/</a></div><div class=\"response-content\" ><p>20 years and still going strong.</p>\n</div>\n","tags":[]},{"title":"Test Response","contentType":"reply","date":"2022-09-16 14:37:00","url":"/reply/webmention-test-1/","content":"<div class=\"response-target\" >→ <a href=\"https://webmention.rocks/test/1\" >https://webmention.rocks/test/1</a></div><div class=\"response-content\" ><p>This is my comment 2</p>\n</div>\n","tags":[]},{"title":"Every User a Neo","contentType":"reshare","date":"2022-09-04 18:12","url":"/reshare/every-user-neo/","content":"<div class=\"response-target\" >→ <a href=\"https://www.linuxjournal.com/content/every-user-neo\" >https://www.linuxjournal.com/content/every-user-neo</a></div><div class=\"response-content\" ></div>\n","tags":[]}]</script><script type="application/json" id="remainingContentData-bookmarks">[{"title":"Resources List for the Personal Web","contentType":"bookmarks","date":"2025-05-30 13:32","url":"/responses/resource-list-personal-web/","content":"<div class=\"response-target\" >→ <a href=\"https://discourse.32bit.cafe/t/resources-list-for-the-personal-web/49\" >https://discourse.32bit.cafe/t/resources-list-for-the-personal-web/49</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This list is to help guide and help those who are in every stage of their web-building journey on the personal web. This list is not meant to overwhelm you, but rather give you options and find tools, graphics, utilities, codes, and everything in between to help get you creating more on the independent web.</p>\n</blockquote>\n</div>\n","tags":["32bitcafe","personalweb","indieweb","openweb","community","retro"]},{"title":"Pocket shutting down","contentType":"bookmarks","date":"2025-05-22 20:21","url":"/responses/pocket-shutting-down/","content":"<div class=\"response-target\" >→ <a href=\"https://support.mozilla.org/en-US/kb/future-of-pocket\" >https://support.mozilla.org/en-US/kb/future-of-pocket</a></div><div class=\"response-content\" ><p>I haven't used Pocket in a long time but sad to hear it's shutting down. It's great they're offering the option of letting you export your data and platforms like <a href=\"https://www.manton.org/2025/05/22/with-pocket-shutting-down-ive.html\">Micro.blog</a> are making it easy to host that content on your own site (assuming you're using Micro.blog).</p>\n<p>For bookmarking solutions, I've been using my website as well as messages-to-self on Element. What's on my website is the content that I really want to make sure I archive, whereas the messages to self I treat more as a read-it-later solution. Eventually, some of those make it to my website. I'm working on my mobile publishing flow to simplify my bookmarking process but overall, I'm happy with my current system.</p>\n<p>This is also another great reminder why owning your content is important.</p>\n</div>\n","tags":["pocket","mozilla","bookmark"]},{"title":"Bukmark Club","contentType":"bookmarks","date":"2025-05-09 21:09","url":"/responses/bukmark-club/","content":"<div class=\"response-target\" >→ <a href=\"https://bukmark.club/\" >https://bukmark.club/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The BUKMARK.CLUB is a collection of websites from across the Internet.</p>\n</blockquote>\n</div>\n","tags":["bookmark","internet","web","community"]},{"title":"Introducing AutoRound","contentType":"bookmarks","date":"2025-05-09 20:45","url":"/responses/intel-autoround/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/blog/autoround\" >https://huggingface.co/blog/autoround</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>As large language models (LLMs) and vision-language models (VLMs) continue to grow in size and complexity, deploying them efficiently becomes increasingly challenging. Quantization offers a solution by reducing model size and inference latency. Intel's AutoRound emerges as a cutting-edge quantization tool that balances accuracy, efficiency, and compatibility.<br />\n<br>\nAutoRound is a weight-only post-training quantization (PTQ) method developed by Intel. It uses signed gradient descent to jointly optimize weight rounding and clipping ranges, enabling accurate low-bit quantization (e.g., INT2 - INT8) with minimal accuracy loss in most scenarios. For example, at INT2, it outperforms popular baselines by up to 2.1x higher in relative accuracy.</p>\n</blockquote>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2309.05516\">Paper</a></li>\n<li><a href=\"https://github.com/intel/auto-round\">Repo</a></li>\n</ul>\n</div>\n","tags":["quantization","intel","ai"]},{"title":"Parakeet TDT 0.6B V2 (En)","contentType":"bookmarks","date":"2025-05-09 20:43","url":"/responses/nvidia-parakeet-v2/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2\" >https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>parakeet-tdt-0.6b-v2 is a 600-million-parameter automatic speech recognition (ASR) model designed for high-quality English transcription, featuring support for punctuation, capitalization, and accurate timestamp prediction.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This XL variant of the FastConformer [1] architecture integrates the TDT [2] decoder and is trained with full attention, enabling efficient transcription of audio segments up to 24 minutes in a single pass.</p>\n</blockquote>\n</div>\n","tags":["nvidia","ai","transcription"]},{"title":"Introducing Locate 3D","contentType":"bookmarks","date":"2025-05-09 20:33","url":"/responses/meta-locate3d/","content":"<div class=\"response-target\" >→ <a href=\"https://locate3d.atmeta.com/\" >https://locate3d.atmeta.com/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Locate 3D transforms how AI understands physical space, delivering <strong>state-of-the-art object localization in complex 3D environments</strong>.<br />\n<br>\nOperating directly on standard sensor data, Locate 3D brings spatial intelligence to robotics and augmented reality applications in real-world settings.</p>\n</blockquote>\n<p><a href=\"https://ai.meta.com/research/publications/locate-3d-real-world-object-localization-via-self-supervised-learning-in-3d/\">Paper</a></p>\n<blockquote class=\"blockquote\">\n<p>We present Locate 3D, a model for localizing objects in 3D scenes from referring expressions like “the small coffee table between the sofa and the lamp.” Locate 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities. Notably, Locate 3D operates directly on sensor observation streams (posed RGB-D frames), enabling real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA, a novel self-supervised learning (SSL) algorithm applicable to sensor point clouds. It takes as input a 3D pointcloud featurized using 2D foundation models (CLIP, DINO). Subsequently, masked prediction in latent space is employed as a pretext task to aid the self-supervised learning of contextualized pointcloud features. Once trained, the 3D-JEPA encoder is finetuned alongside a language-conditioned decoder to jointly predict 3D masks and bounding boxes. Additionally, we introduce Locate 3D Dataset, a new dataset for 3D referential grounding, spanning multiple capture setups with over 130K annotations. This enables a systematic study of generalization capabilities as well as a stronger model.</p>\n</blockquote>\n</div>\n","tags":["meta","ai","3d","research"]},{"title":"ZeroSearch - Incentivize the Search Capability of LLMs without Searching","contentType":"bookmarks","date":"2025-05-09 20:31","url":"/responses/zero-search-alibaba/","content":"<div class=\"response-target\" >→ <a href=\"https://alibaba-nlp.github.io/ZeroSearch/\" >https://alibaba-nlp.github.io/ZeroSearch/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) <strong>Uncontrolled Document Quality</strong>: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) <strong>Prohibitively High API Costs</strong>: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce <strong>ZeroSearch</strong>, a reinforcement learning framework that <strong>enhances the search capabilities of LLMs without interacting with real search engines</strong>. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model’s reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that <strong>ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module</strong>. Remarkably, <strong>a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it</strong>. Furthermore, it generalizes well across both base and instruction-tuned models of varying sizes and is compatible with a wide range of RL algorithms.</p>\n</blockquote>\n</div>\n","tags":["ai","search","llm","research"]},{"title":"A Survey of AI Agent Protocols","contentType":"bookmarks","date":"2025-05-09 20:23","url":"/responses/survey-ai-agent-protocols/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2504.16736\" >https://arxiv.org/abs/2504.16736</a></div><div class=\"response-content\" ><p>It'll be interesting to see once many of the exiting protocols begin to converge towards a standard. There's a ton of existing well-adopted protocol standards out there that when stitched together could create a compelling and native experience. I think eventually we'll get to a place where agents are native parts of the OSI stack and the world wide web.</p>\n<blockquote class=\"blockquote\">\n<p>The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: <strong>there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence.</strong> In this paper, we provide the first comprehensive analysis of existing agent protocols, proposing a systematic two-dimensional classification that differentiates context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore the future landscape of agent protocols by identifying critical research directions and characteristics necessary for next-generation protocols. These characteristics include adaptability, privacy preservation, and group-based interaction, as well as trends toward layered architectures and collective intelligence infrastructures. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.</p>\n</blockquote>\n</div>\n","tags":["ai","agents","protocols","research"]},{"title":"TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching","contentType":"bookmarks","date":"2025-05-05 20:03","url":"/responses/telograf/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2505.00562\" >https://arxiv.org/abs/2505.00562</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. <strong>In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications.</strong> We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at <a href=\"https://github.com/mengyuest/TeLoGraF\">this https URL</a></p>\n</blockquote>\n</div>\n","tags":["ai","graphs","neuralnetworks","research"]},{"title":"CORG: Generating Answers from Complex, Interrelated Contexts","contentType":"bookmarks","date":"2025-05-05 19:53","url":"/responses/corg-context-organizer-framework/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2505.00023\" >https://arxiv.org/abs/2505.00023</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, <strong>we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator.</strong> Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.</p>\n</blockquote>\n</div>\n","tags":["rag","ai","knowledge","research"]},{"title":"Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks","contentType":"bookmarks","date":"2025-05-05 19:50","url":"/responses/self-generate-incontext-examples-llm-agents-improve-sequential-decision-making/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2505.00234\" >https://arxiv.org/abs/2505.00234</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that <strong>automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.</strong></p>\n</blockquote>\n</div>\n","tags":["ai","agents","planning","research"]},{"title":"ALL about RSS","contentType":"bookmarks","date":"2025-04-28 21:07","url":"/responses/all-about-rss/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/AboutRSS/ALL-about-RSS\" >https://github.com/AboutRSS/ALL-about-RSS</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A list of RSS related stuff: tools, services, communities and tutorials, etc.</p>\n</blockquote>\n</div>\n","tags":["rss","indieweb","feeds"]},{"title":"s1: Simple test-time scaling","contentType":"bookmarks","date":"2025-02-24 20:54","url":"/responses/s1-simple-test-time-scaling/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2501.19393\" >https://arxiv.org/abs/2501.19393</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending &amp;quot;Wait&amp;quot; multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at <a href=\"https://github.com/simplescaling/s1\">this https URL</a></p>\n</blockquote>\n</div>\n","tags":["ai","research","s1"]},{"title":"HuggingFace AI Agents Course","contentType":"bookmarks","date":"2025-02-10 17:20","url":"/responses/huggingface-ai-agents-course/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/learn/agents-course/unit0/introduction\" >https://huggingface.co/learn/agents-course/unit0/introduction</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This free course will take you on a journey, from beginner to expert, in understanding, using and building AI agents.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>At the end of this course you’ll understand how Agents work and how to build your own Agents using the latest libraries and tools.</p>\n</blockquote>\n</div>\n","tags":["huggingface","ai","agents","course"]},{"title":"The Illustrated DeepSeek-R1","contentType":"bookmarks","date":"2025-02-03 20:20","url":"/responses/illustrated-deepseek-r1/","content":"<div class=\"response-target\" >→ <a href=\"https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1\" >https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Just like most existing LLMs, DeepSeek-R1 generates one token at a time, except it excels at solving math and reasoning problems because it is able to spend more time processing a problem through the process of generating thinking tokens that explain its chain of thought.</p>\n</blockquote>\n<p><img src=\"https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png\" class=\"img-fluid\" alt=\"DeepSeek R1 Training Recipe\" /></p>\n</div>\n","tags":["deepseek","ai","visualization","generativeai","genai","llm","learning"]},{"title":"A Year of Curiosity","contentType":"bookmarks","date":"2025-01-08 21:27","url":"/responses/year-of-curiosity/","content":"<div class=\"response-target\" >→ <a href=\"https://nesslabs.com/a-year-of-curiosity\" >https://nesslabs.com/a-year-of-curiosity</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>When we encounter something new and interesting, our brains release dopamine – the same neurotransmitter associated with reward and pleasure. This creates a simple pattern: the more we learn, the more we want to learn.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This drive to explore and understand isn’t just nice to have – it’s essential to who we are as humans.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>There’s no single “right” way to be curious. Learning practical skills like coding or cooking, diving into topics like history or science, joining groups of people who share your interests… These are all great ways to inject more curiosity into your life.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<ul>\n<li><strong>Monthly:</strong> Design one tiny experiment at the beginning of each month This could be as simple as exploring a topic you know nothing about, trying a new hobby, or doing something that pushes you out of your comfort zone...<br />\n<br></li>\n<li><strong>Weekly:</strong> Every week, take 10 to 15 minutes to conduct a weekly review. What went well this week? What didn’t go as planned? What will you focus on next week? This could mean doubling down on what worked or tweaking something that didn’t...<br />\n<br></li>\n<li><strong>Daily:</strong> Create at least one moment of curiosity in your day, no matter how small. Experiment with a new recipe or tool, have one meaningful conversation, try a journaling prompt, or take a different route to work. Even one minute of curiosity a day can add up to a much richer life.</li>\n</ul>\n</blockquote>\n</div>\n","tags":["innovation","exploration","curiosity","growth"]},{"title":"Agents","contentType":"bookmarks","date":"2025-01-07 11:23","url":"/responses/agents-huyen/","content":"<div class=\"response-target\" >→ <a href=\"https://huyenchip.com/2025/01/07/agents.html\" >https://huyenchip.com/2025/01/07/agents.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The unprecedented capabilities of foundation models have opened the door to agentic applications that were previously unimaginable. These new capabilities make it finally possible to develop autonomous, intelligent agents to act as our assistants, coworkers, and coaches. They can help us create a website, gather data, plan a trip, do market research, manage a customer account, automate data entry, prepare us for interviews, interview our candidates, negotiate a deal, etc. The possibilities seem endless, and the potential economic value of these agents is enormous.<br />\n<br>\nThis section will start with an overview of agents and then continue with two aspects that determine the capabilities of an agent: tools and planning. Agents, with their new modes of operations, have new modes of failure. This section will end with a discussion on how to evaluate agents to catch these failures.</p>\n</blockquote>\n</div>\n","tags":["ai","agents"]},{"title":"Agents Whitepaper","contentType":"bookmarks","date":"2025-01-06 21:50","url":"/responses/kaggle-agents-whitepaper/","content":"<div class=\"response-target\" >→ <a href=\"https://www.kaggle.com/whitepaper-agents\" >https://www.kaggle.com/whitepaper-agents</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Humans are fantastic at messy pattern recognition tasks. However, they often rely on tools - like books, Google Search, or a calculator - to supplement their prior knowledge before arriving at a conclusion. Just like humans, Generative AI models can be trained to use tools to access real-time information or suggest a real-world action. For example, a model can leverage a database retrieval tool to access specific information, like a customer's purchase history, so it can generate tailored shopping recommendations. Alternatively, based on a user's query, a model can make various API calls to send an email response to a colleague or complete a financial transaction on your behalf. To do so, the model must not only have access to a set of external tools, it needs the ability to plan and execute any task in a self- directed fashion. This combination of reasoning, logic, and access to external information that are all connected to a Generative AI model invokes the concept of an agent, or a program that extends beyond the standalone capabilities of a Generative AI model. This whitepaper dives into all these and associated aspects in more detail.</p>\n</blockquote>\n</div>\n","tags":["agents","ai","whitepaper"]},{"title":"Openverse","contentType":"bookmarks","date":"2024-12-17 16:53","url":"/responses/openverse/","content":"<div class=\"response-target\" >→ <a href=\"https://openverse.org/\" >https://openverse.org/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Openverse is a tool that allows openly licensed and public domain works to be discovered and used by everyone.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Openverse searches across more than 800 million images and audio tracks from open APIs and the Common Crawl dataset. We aggregate works from multiple public repositories, and facilitate reuse through features like one-click attribution.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Openverse is the successor to CC Search which was launched by Creative Commons in 2019</p>\n</blockquote>\n</div>\n","tags":["openverse","opensource","creativecommons"]},{"title":"NotebookLlama: An Open Source version of NotebookLM","contentType":"bookmarks","date":"2024-11-01 09:32","url":"/responses/notebook-llama-oss-notebook-lm/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/meta-llama/llama-recipes/tree/main/recipes/quickstart/NotebookLlama\" >https://github.com/meta-llama/llama-recipes/tree/main/recipes/quickstart/NotebookLlama</a></div><div class=\"response-content\" ><p>This is cool! Already looking at how I can repurpose this sample for a project I have in mind.</p>\n</div>\n","tags":["ai","sample","postcast","texttospeech","programming","sample","tutorial"]},{"title":"DEFCON Media Server","contentType":"bookmarks","date":"2024-10-10 20:41","url":"/responses/defcon-media-server/","content":"<div class=\"response-target\" >→ <a href=\"https://media.defcon.org/\" >https://media.defcon.org/</a></div><div class=\"response-content\" ><p>Videos, Slides, Presentations, and assets from DEFCON.</p>\n</div>\n","tags":["defcon","media","persentations","videos","slides"]},{"title":"Transformer Explainer: Interactive Learning of Text-Generative Models","contentType":"bookmarks","date":"2024-08-14 21:51 -05:00","url":"/responses/transformer-explainer-interactive-learning-text-generative-models/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2408.04619\" >https://arxiv.org/abs/2408.04619</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Transformers have revolutionized machine learning, yet their inner workings remain opaque to many. We present Transformer Explainer, an interactive visualization tool designed for non-experts to learn about Transformers through the GPT-2 model. Our tool helps users understand complex Transformer concepts by integrating a model overview and enabling smooth transitions across abstraction levels of mathematical operations and model structures. It runs a live GPT-2 instance locally in the user's browser, empowering users to experiment with their own input and observe in real-time how the internal components and parameters of the Transformer work together to predict the next tokens. Our tool requires no installation or special hardware, broadening the public's education access to modern generative AI techniques. Our open-sourced tool is available at this https URL. A video demo is available at this https URL.</p>\n</blockquote>\n<ul>\n<li><a href=\"https://poloclub.github.io/transformer-explainer/\">Tool</a></li>\n<li><a href=\"https://youtu.be/ECR4oAwocjs\">Video</a></li>\n</ul>\n</div>\n","tags":["ai","transformer","neuralnetwork","learning","gpt"]},{"title":"HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction","contentType":"bookmarks","date":"2024-08-14 21:49 -05:00","url":"/responses/hybridrag-knowledge-graphs-vector-rag-info-extraction/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2408.04948\" >https://arxiv.org/abs/2408.04948</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&amp;amp;A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&amp;amp;A format, and hence provide a natural set of pairs of ground-truth Q&amp;amp;As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain</p>\n</blockquote>\n</div>\n","tags":["ai","rag","graph","database","knowledgegraph","kb","vector"]},{"title":"GPT-4o System Card","contentType":"bookmarks","date":"2024-08-08 20:16 -05:00","url":"/responses/gpt4o-system-card/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/index/gpt-4o-system-card/\" >https://openai.com/index/gpt-4o-system-card/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>GPT-4o is an autoregressive omni model, which accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It\\u2019s trained end-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network.<br />\n<br>\nGPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window)2 in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.<br />\n<br>\nIn line with our commitment to building AI safely and consistent with our voluntary commitments to the White House3, we are sharing the GPT-4o System Card, which includes our Preparedness Framework(opens in a new window)5 evaluations. In this System Card, we provide a detailed look at GPT-4o\\u2019s capabilities, limitations, and safety evaluations across multiple categories, with a focus on speech-to-speech (voice)A while also evaluating text and image capabilities, and the measures we\\u2019ve taken to enhance safety and alignment. We also include third party assessments on general autonomous capabilities, as well as discussion of potential societal impacts of GPT-4o text and vision capabilities.</p>\n</blockquote>\n</div>\n","tags":["ai","openai","gpt-4o","documentation","llm"]},{"title":"LongROPE: Extending LLM Context Window Beyond 2 Million Tokens","contentType":"bookmarks","date":"2024-07-30 14:16 -05:00","url":"/responses/longrope-extending-llm-context-2m-tokens/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2402.13753\" >https://arxiv.org/abs/2402.13753</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.</p>\n</blockquote>\n<p><a href=\"https://github.com/jshuadvd/LongRoPE\">GitHub Repo</a></p>\n</div>\n","tags":["ai","llm","longrope","microsoft","research","msr"]},{"title":"Nyxt Emacs Hacks","contentType":"bookmarks","date":"2024-07-28 16:53 -05:00","url":"/responses/nyxt-emacs-hacks/","content":"<div class=\"response-target\" >→ <a href=\"https://nyxt.atlas.engineer/article/emacs-hacks.org\" >https://nyxt.atlas.engineer/article/emacs-hacks.org</a></div><div class=\"response-content\" ><p>Saving this guide for future reference as I set up my elfeed / Nyxt capture workflows for the website.</p>\n<p>Additional articles that might be helpful.</p>\n<p><a href=\"https://ag91.github.io/blog/2021/07/09/org-capture-in-nyxt-taking-notes-while-browsing/\">Org capture in Nyxt: Taking Notes While Browsing</a></p>\n</div>\n","tags":["nyxt","emacs","orgmode","capture","templates","workflow","lisp"]},{"title":"Generative AI Handbook: A Roadmap for Learning Resources","contentType":"bookmarks","date":"2024-06-06 22:18","url":"/responses/genai-handbook-learning-resource-brown/","content":"<div class=\"response-target\" >→ <a href=\"https://genai-handbook.github.io/\" >https://genai-handbook.github.io/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This document aims to serve as a handbook for learning the key concepts underlying modern artificial intelligence systems. Given the speed of recent development in AI, there really isn’t a good textbook-style source for getting up-to-speed on the latest-and-greatest innovations in LLMs or other generative models, yet there is an abundance of great explainer resources (blog posts, videos, etc.) for these topics scattered across the internet. My goal is to organize the “best” of these resources into a textbook-style presentation, which can serve as a roadmap for filling in the prerequisites towards individual AI-related learning goals. My hope is that this will be a “living document”, to be updated as new innovations and paradigms inevitably emerge, and ideally also a document that can benefit from community input and contribution. This guide is aimed at those with a technical background of some kind, who are interested in diving into AI either out of curiosity or for a potential career. I’ll assume that you have some experience with coding and high-school level math, but otherwise will provide pointers for filling in any other prerequisites.</p>\n</blockquote>\n</div>\n","tags":["generativeai","genai","handbook","learning","resource"]},{"title":"Calculus Made Easy","contentType":"bookmarks","date":"2024-04-25 23:01","url":"/responses/calculus-made-easy/","content":"<div class=\"response-target\" >→ <a href=\"https://calculusmadeeasy.org/\" >https://calculusmadeeasy.org/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Calculus Made Easy is a book on calculus originally published in 1910 by Silvanus P. Thompson, considered a classic and elegant introduction to the subject.</p>\n</blockquote>\n<ul>\n<li><a href=\"https://www.gutenberg.org/ebooks/33283\">Project Gutenberg PDF</a></li>\n</ul>\n</div>\n","tags":["calculus","book","math"]},{"title":"The Internet Used To Be Fun","contentType":"bookmarks","date":"2024-04-25 22:56","url":"/responses/internet-used-to-be-fun-kwon/","content":"<div class=\"response-target\" >→ <a href=\"https://projects.kwon.nyc/internet-is-fun/\" >https://projects.kwon.nyc/internet-is-fun/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>I’ve been meaning to write some kind of Important Thinkpiece™ on the glory days of the early internet, but every time I sit down to do it, I find another, better piece that someone else has already written. So for now, here’s a collection of articles that to some degree answer the question “Why have a personal website?” with “Because it’s fun, and the internet used to be fun.”</p>\n</blockquote>\n<p>This is a great catalog of posts about the personal web. Courtesy of <a href=\"https://kwon.nyc/\">Rachel Kwon</a></p>\n</div>\n","tags":["internet","indieweb","personalweb","online","blogging","blogs"]},{"title":"Grow Your Own Services","contentType":"bookmarks","date":"2024-04-25 22:53","url":"/responses/grow-your-own-services/","content":"<div class=\"response-target\" >→ <a href=\"https://growyourown.services/\" >https://growyourown.services/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This is a site encouraging non-technical people and organisations to create their own online services such as websites, social networks, personal clouds, instant messaging etc.</p>\n</blockquote>\n</div>\n","tags":["selfhosting","indieweb","openweb","internet","community","cloud","messaging","website","socialnetworks","websites","online"]},{"title":"Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models","contentType":"bookmarks","date":"2024-04-10 22:02","url":"/responses/griffin-mix-linear-recurrence-local-attention-language-models/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2402.19427\" >https://arxiv.org/abs/2402.19427</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.</p>\n</blockquote>\n</div>\n","tags":["griffin","ai","research","architecture","rnn","attention","transformers","llm"]},{"title":"DE-COP: Detecting Copyrighted Content in Language Models Training Data","contentType":"bookmarks","date":"2024-04-10 22:00","url":"/responses/decop-detecting-copyright-llm-training-data/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2402.09910\" >https://arxiv.org/abs/2402.09910</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give ≈ 4% accuracy. Our code and datasets are available at this https URL</p>\n</blockquote>\n<p><a href=\"https://github.com/avduarte333/DE-COP_Method\">Repo</a></p>\n</div>\n","tags":["ai","copyright","research","llm","data"]},{"title":"Using LLM to select the right SQL Query from candidates","contentType":"bookmarks","date":"2024-04-10 21:59","url":"/responses/llm-text-to-sql-right-candidates/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2401.02115\" >https://arxiv.org/abs/2401.02115</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Text-to-SQL models can generate a list of candidate SQL queries, and the best query is often in the candidate list, but not at the top of the list. An effective re-rank method can select the right SQL query from the candidate list and improve the model's performance. Previous studies on code generation automatically generate test cases and use them to re-rank candidate codes. However, automatic test case generation for text-to-SQL is an understudied field. We propose an automatic test case generation method that first generates a database and then uses LLMs to predict the ground truth, which is the expected execution results of the ground truth SQL query on this database. To reduce the difficulty for LLMs to predict, we conduct experiments to search for ways to generate easy databases for LLMs and design easy-to-understand prompts. Based on our test case generation method, we propose a re-rank method to select the right SQL query from the candidate list. Given a candidate list, our method can generate test cases and re-rank the candidate list according to their pass numbers on these test cases and their generation probabilities. The experiment results on the validation dataset of Spider show that the performance of some state-of-the-art models can get a 3.6% improvement after applying our re-rank method.</p>\n</blockquote>\n</div>\n","tags":["ai","research","sql","llm"]},{"title":"Far Out Guides","contentType":"bookmarks","date":"2024-04-07 19:38","url":"/responses/far-out-guides-hiking-app/","content":"<div class=\"response-target\" >→ <a href=\"https://faroutguides.com/\" >https://faroutguides.com/</a></div><div class=\"response-content\" ><p>Outdoor nagivation app for long-distance trails</p>\n</div>\n","tags":["outdoors","app","hiking","technology"]},{"title":"Molly White's Blogroll","contentType":"bookmarks","date":"2024-04-07 17:50","url":"/responses/blogroll-molly-white/","content":"<div class=\"response-target\" >→ <a href=\"https://www.mollywhite.net/blogroll/\" >https://www.mollywhite.net/blogroll/</a></div><div class=\"response-content\" ><p>Bookmarking for reference.</p>\n<p>I'm already subscribed to many of these websites and publications. However, there's several new ones I found that I think will eventually make their rotation into my <a href=\"/collections/blogroll/\">blogroll</a>.</p>\n</div>\n","tags":["blogroll","indieweb","rss","blogging","web","internet","smallweb","community"]},{"title":"Large Language Models Are Zero-Shot Time Series Forecasters","contentType":"bookmarks","date":"2024-04-04 11:07","url":"/responses/llm-time-series-forecasting/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/ngruver/llmtime\" >https://github.com/ngruver/llmtime</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.</p>\n</blockquote>\n<p><a href=\"https://github.com/ngruver/llmtime\">Github Repo</a></p>\n</div>\n","tags":["forecasting","llm","ai","research"]},{"title":"ReALM: Reference Resolution As Language Modeling","contentType":"bookmarks","date":"2024-04-04 11:04","url":"/responses/apple-realm-research-paper/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2403.20329\" >https://arxiv.org/abs/2403.20329</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of references, with our smallest model obtaining absolute gains of over 5% for on-screen references. We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it.</p>\n</blockquote>\n</div>\n","tags":["ai","research","apple","gpt"]},{"title":"Releasing Common Corpus: the largest public domain dataset for training LLMs","contentType":"bookmarks","date":"2024-03-20 22:12","url":"/responses/common-corpus-llm-training-dataset/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/blog/Pclanglais/common-corpus\" >https://huggingface.co/blog/Pclanglais/common-corpus</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We announce today the release of Common Corpus on HuggingFace:<br />\n<br></p>\n<ul>\n<li>Common Corpus is the largest public domain dataset released for training LLMs.</li>\n<li>Common Corpus includes 500 billion words from a wide diversity of cultural heritage initiatives.</li>\n<li>Common Corpus is multilingual and the largest corpus to date in English, French, Dutch, Spanish, German and Italian.</li>\n<li>Common Corpus shows it is possible to train fully open LLMs on sources without copyright concerns.</li>\n</ul>\n</blockquote>\n</div>\n","tags":["ai","data","llm","huggingface","nlp"]},{"title":"Machine Learning for Games Course - HuggingFace","contentType":"bookmarks","date":"2024-03-19 22:35","url":"/responses/machine-learning-for-games-course-huggingface/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/learn/ml-games-course/unit0/introduction\" >https://huggingface.co/learn/ml-games-course/unit0/introduction</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Welcome to the course that will teach you the most fascinating topic in game development: how to use powerful AI tools and models to create unique game experiences.<br />\n<br>\nNew AI models are revolutionizing the Game Industry in two impactful ways:<br />\n<br></p>\n<ul>\n<li>On how we make games:\n<ul>\n<li>Generate textures using AI</li>\n<li>Using AI voice actors for the voices.<br />\n<br></li>\n</ul>\n</li>\n<li>How we create gameplay:\n<ul>\n<li>Crafting smart Non-Playable Characters (NPCs) using large language models.<br />\n<br>\nThis course will teach you:<br />\n<br></li>\n</ul>\n</li>\n<li>How to integrate AI models for innovative gameplay, featuring intelligent NPCs.</li>\n<li>How to use AI tools to help your game development pipeline.</li>\n</ul>\n</blockquote>\n</div>\n","tags":["machinelearning","games","huggingface","course","ai","ml"]},{"title":"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training","contentType":"bookmarks","date":"2024-03-18 21:02","url":"/responses/mm1-multimodal-llm-pretraining/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2403.09611\" >https://arxiv.org/abs/2403.09611</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, consisting of both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.</p>\n</blockquote>\n</div>\n","tags":["ai","mm1","llm","multimodal","apple"]},{"title":"Algorithms for Modern Hardware","contentType":"bookmarks","date":"2024-03-17 21:20","url":"/responses/algorithms-modern-hardware-book/","content":"<div class=\"response-target\" >→ <a href=\"https://en.algorithmica.org/hpc/\" >https://en.algorithmica.org/hpc/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This is an upcoming high performance computing book titled “Algorithms for Modern Hardware” by Sergey Slotin.<br />\n<br>\nIts intended audience is everyone from performance engineers and practical algorithm researchers to undergraduate computer science students who have just finished an advanced algorithms course and want to learn more practical ways to speed up a program than by going from O(nlog⁡n)O(nlogn) to O(nlog⁡log⁡n)O(nloglogn).</p>\n</blockquote>\n</div>\n","tags":["book","algorithms","hardware"]},{"title":"Diffusion Models From Scratch","contentType":"bookmarks","date":"2024-03-17 21:03","url":"/responses/diffusion-models-from-scratch-duan/","content":"<div class=\"response-target\" >→ <a href=\"https://www.tonyduan.com/diffusion/index.html\" >https://www.tonyduan.com/diffusion/index.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Here, we'll cover the derivations from scratch to provide a rigorous understanding of the core ideas behind diffusion. What assumptions are we making? What properties arise as a result?<br />\n<br>\nA reference [codebase] is written from scratch, which provides minimalist re-production of the MNIST example below. It clocks in at under 500 lines of code.<br />\n<br>\nEach page takes up to an hour to read thoroughly. Approximately a lecture each.</p>\n</blockquote>\n</div>\n","tags":["diffusion","ai","concepts","models"]},{"title":"Diffusion models from scratch, from a new theoretical perspective","contentType":"bookmarks","date":"2024-03-17 21:02","url":"/responses/diffusion-models-from-scratch-yuan/","content":"<div class=\"response-target\" >→ <a href=\"https://www.chenyang.co/diffusion.html\" >https://www.chenyang.co/diffusion.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This tutorial aims to introduce diffusion models from an optimization perspective as introduced in our paper (joint work with Frank Permenter). It will go over both theory and code, using the theory to explain how to implement diffusion models from scratch. By the end of the tutorial, you will learn how to implement training and sampling code for a toy dataset, which will also work for larger datasets and models.</p>\n</blockquote>\n</div>\n","tags":["diffusion","ai","concepts","models"]},{"title":"Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators","contentType":"bookmarks","date":"2024-03-17 20:47","url":"/responses/fast-inner-product-dnn-accelerators/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2311.12224\" >https://arxiv.org/abs/2311.12224</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We introduce a new algorithm called the Free-pipeline Fast Inner Product (FFIP) and its hardware architecture that improve an under-explored fast inner-product algorithm (FIP) proposed by Winograd in 1968. Unlike the unrelated Winograd minimal filtering algorithms for convolutional layers, FIP is applicable to all machine learning (ML) model layers that can mainly decompose to matrix multiplication, including fully-connected, convolutional, recurrent, and attention/transformer layers. We implement FIP for the first time in an ML accelerator then present our FFIP algorithm and generalized architecture which inherently improve FIP's clock frequency and, as a consequence, throughput for a similar hardware cost. Finally, we contribute ML-specific optimizations for the FIP and FFIP algorithms and architectures. We show that FFIP can be seamlessly incorporated into traditional fixed-point systolic array ML accelerators to achieve the same throughput with half the number of multiply-accumulate (MAC) units, or it can double the maximum systolic array size that can fit onto devices with a fixed hardware budget. Our FFIP implementation for non-sparse ML models with 8 to 16-bit fixed-point inputs achieves higher throughput and compute efficiency than the best-in-class prior solutions on the same type of compute platform.</p>\n</blockquote>\n<p><a href=\"https://github.com/trevorpogue/algebraic-nnhw\">GitHub</a></p>\n</div>\n","tags":["ai","math","acceleration","neuralnetworks","dnn"]},{"title":"Ente - Private cloud storage for photos, videos, etc.","contentType":"bookmarks","date":"2024-03-02 15:49","url":"/responses/ente-private-media-cloud-storage/","content":"<div class=\"response-target\" >→ <a href=\"https://ente.io/\" >https://ente.io/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Store, share, and rediscover your memories with absolute privacy</p>\n</blockquote>\n</div>\n","tags":["e2ee","photos","cloud","storage","product"]},{"title":"Hypertext Gardens: Delightful Vistas","contentType":"bookmarks","date":"2024-02-28 21:44","url":"/responses/hypertext-gardens-delightful-vistas/","content":"<div class=\"response-target\" >→ <a href=\"http://www.eastgate.com/garden/Enter.html\" >http://www.eastgate.com/garden/Enter.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The time, care, and expense devoted to creating and promoting a hypertext are lost if readers arrive, glance around, and click elsewhere. How can the craft of hypertext invite readers to stay, to explore, and to reflect?</p>\n</blockquote>\n</div>\n","tags":["openweb","digitalgarden","indieweb"]},{"title":"The AI Study Guide: Azure Machine Learning Edition","contentType":"bookmarks","date":"2024-02-21 15:18","url":"/responses/ai-study-guide-azure-ml-edition/","content":"<div class=\"response-target\" >→ <a href=\"https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/the-ai-study-guide-azure-machine-learning-edition/ba-p/4063656\" >https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/the-ai-study-guide-azure-machine-learning-edition/ba-p/4063656</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The AI Study Guide: Discover Machine Learning with these free Azure resources<br />\n<br>\nWelcome to the February edition of the Azure AI Study Guide. Every month I’ll be spilling the tea on the best and newest tools for skilling up on Azure AI. This month we’re putting on our thinking caps to investigate Azure Machine Learning (ML). I’ll give you a quick breakdown of what it is, then we’ll explore a four-week roadmap of our top FREE resources for you to continue your AI learning journey! And as a bonus, stay tuned to the end to see what makes machine learning and generative AI a dynamic duo.</p>\n</blockquote>\n</div>\n","tags":["azure","tutorial","machinelearning","ml","ai","azureml"]},{"title":"HuggingFace - Open Source AI Cookbook","contentType":"bookmarks","date":"2024-02-16 22:36","url":"/responses/open-source-ai-cookbook-hf/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/learn/cookbook/index\" >https://huggingface.co/learn/cookbook/index</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The Open-Source AI Cookbook is a collection of notebooks illustrating practical aspects of building AI applications and solving various machine learning tasks using open-source tools and models.</p>\n</blockquote>\n</div>\n","tags":["ai","tutorial","huggingface","opensource","ml"]},{"title":"Magic.dev","contentType":"bookmarks","date":"2024-02-15 20:18","url":"/responses/magic-dev/","content":"<div class=\"response-target\" >→ <a href=\"https://magic.dev/\" >https://magic.dev/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Magic is working on frontier-scale code models to build a coworker, not just a copilot.</p>\n</blockquote>\n</div>\n","tags":["ai","copilot","magic","code","agi"]},{"title":"TWiT's Lesser Known RSS Feeds","contentType":"bookmarks","date":"2024-02-14 22:35","url":"/responses/twit-lesser-known-rss-feeds/","content":"<div class=\"response-target\" >→ <a href=\"https://twit.tv/posts/inside-twit/twits-lesser-known-rss-feeds\" >https://twit.tv/posts/inside-twit/twits-lesser-known-rss-feeds</a></div><div class=\"response-content\" ><p>Subscribed!</p>\n<blockquote class=\"blockquote\">\n<p>Many people are unaware that TWiT also has RSS feeds designed for news aggregators like Feedly, NetNewsWire, Mozilla Thunderbird, and Akregator. These feeds are not meant for podcast apps but are specifically designed for news aggregators. You can copy any of the RSS feed links below into your RSS feed reader of choice and get updates on the latest TWiT blog posts, articles, or podcasts as soon as they are published.</p>\n</blockquote>\n</div>\n","tags":["twit","rss","tech","news","community"]},{"title":"GraphRAG: Unlocking LLM discovery on narrative private data ","contentType":"bookmarks","date":"2024-02-13 21:50","url":"/responses/ms-research-graphrag/","content":"<div class=\"response-target\" >→ <a href=\"https://www.microsoft.com/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/\" >https://www.microsoft.com/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Perhaps the greatest challenge – and opportunity – of LLMs is extending their powerful capabilities to solve problems beyond the data on which they have been trained, and to achieve comparable results with data the LLM has never seen.  This opens new possibilities in data investigation, such as identifying themes and semantic concepts with context and grounding on datasets.  In this post, we introduce GraphRAG, created by Microsoft Research, as a significant advance in enhancing the capability of LLMs.</p>\n</blockquote>\n</div>\n","tags":["research","ai","rag","llm","data","microsoft"]},{"title":"Every Noise at Once","contentType":"bookmarks","date":"2024-02-13 21:19","url":"/responses/every-noise-at-once/","content":"<div class=\"response-target\" >→ <a href=\"https://everynoise.com/engenremap.html\" >https://everynoise.com/engenremap.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Every Noise at Once is an ongoing attempt at an algorithmically-generated, readability-adjusted scatter-plot of the musical genre-space, based on data tracked and analyzed for 6,291 genre-shaped distinctions by Spotify as of 2023-11-19. The calibration is fuzzy, but in general down is more organic, up is more mechanical and electric; left is denser and more atmospheric, right is spikier and bouncier.</p>\n</blockquote>\n</div>\n","tags":["music","genres","discovery","spotify"]},{"title":"SQL for Data Scientists in 100 Queries","contentType":"bookmarks","date":"2024-02-06 20:07","url":"/responses/sql-for-data-scientists-100-queries/","content":"<div class=\"response-target\" >→ <a href=\"https://gvwilson.github.io/sql-tutorial/\" >https://gvwilson.github.io/sql-tutorial/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>notes and working examples that instructors can use to perform a lesson</p>\n</blockquote>\n</div>\n","tags":["sql","datascience","tutorial"]},{"title":"LangChain - OpenGPTs","contentType":"bookmarks","date":"2024-01-31 21:30","url":"/responses/langchain-opengpts/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.langchain.dev/opengpts/\" >https://blog.langchain.dev/opengpts/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A little over two months ago, on the heels of OpenAI dev day, we launched OpenGPTs: a take on what an open-source GPT store may look like. It was powered by an early version of LangGraph - an extension of LangChain aimed at building agents as graphs. At the time, we did not highlight this new package much, as we had not publicly launched it and were still figuring out the interface. We finally got around to launching LangGraph two weeks ago, and over the past weekend we updated OpenGPTs to fully use LangGraph (as well as added some new features). We figure now is as good of time as any to do a technical deep-dive on OpenGPTs and what powers it.<br />\n<br>\nIn this blog, we will talk about:<br />\n<br></p>\n<ul>\n<li>MessageGraph: A particular type of graph that OpenGPTs runs on</li>\n<li>Cognitive architectures: What the 3 different types of cognitive architectures OpenGPTs supports are, and how they differ</li>\n<li>Persistence: How persistence is baked in OpenGPTs via LangGraph checkpoints.</li>\n<li>Configuration: How we use LangChain primitives to configure all these different bots.</li>\n<li>New models: what new models we support</li>\n<li>New tools: what new tools we support</li>\n<li>astream_events: How we are using this new method to stream tokens and intermediate steps</li>\n</ul>\n</blockquote>\n</div>\n","tags":["langchain","gpt","ai","opengpt","langgraph","messagegraph"]},{"title":"Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages (RWKV-v5)","contentType":"bookmarks","date":"2024-01-29 20:27","url":"/responses/eagle-7b-rkwv/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers\" >https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Eagle 7B is a 7.52B parameter model that:<br />\n<br>\nBuilt on the <a href=\"https://wiki.rwkv.com/\">RWKV-v5 architecture</a> (a linear transformer with 10-100x+ lower inference cost)<br />\n<br>\nRanks as the <a href=\"https://blog.rwkv.com/p/the-worlds-greenest-ai-model-rwkvs\">world’s greenest 7B model (per token)</a><br />\n<br>\nTrained on 1.1 Trillion Tokens across 100+ languages<br />\n<br>\nOutperforms all 7B class models in multi-lingual benchmarks<br />\n<br>\nApproaches Falcon (1.5T), LLaMA2 (2T), Mistral (&amp;gt;2T?) level of performance in English evals<br />\n<br>\nTrade blows with MPT-7B (1T) in English evals<br />\n<br>\n<a href=\"https://www.isattentionallyouneed.com/\">All while being an “Attention-Free Transformer”</a><br />\n<br>\nIs a foundation model, with a very small instruct tune - further fine-tuning is required for various use cases!</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We are releasing RWKV-v5 Eagle 7B, <a href=\"https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as\">licensed as Apache 2.0 license, under the Linux Foundation</a>, and can be used personally or commercially without restrictions</p>\n</blockquote>\n<p><a href=\"https://huggingface.co/RWKV/v5-Eagle-7B/\">Download from HuggingFace</a></p>\n</div>\n","tags":["ai","llm","rwkv","deeplearning","neuralnetwork"]},{"title":"Getting Started With CUDA for Python Programmers","contentType":"bookmarks","date":"2024-01-29 20:23","url":"/responses/get-started-cuda-python-programmers/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=nOxKexn3iBo\" >https://www.youtube.com/watch?v=nOxKexn3iBo</a></div><div class=\"response-content\" ><p><a href=\"https://www.youtube.com/watch?v=nOxKexn3iBo\" title=\"A YouTube Video Tutorial by Jeremy Howard to help Python programmers get started with CUDA\"><img src=\"http://img.youtube.com/vi/nOxKexn3iBo/0.jpg\" class=\"img-fluid\" alt=\"A YouTube Video Tutorial by Jeremy Howard to help Python programmers get started with CUDA\" /></a></p>\n<blockquote class=\"blockquote\">\n<p>In this comprehensive video tutorial, Jeremy Howard from answer.ai demystifies the process of programming NVIDIA GPUs using CUDA, and simplifies the perceived complexities of CUDA programming. Jeremy emphasizes the accessibility of CUDA, especially when combined with PyTorch's capabilities, allowing for programming directly in notebooks rather than traditional compilers and terminals. To make CUDA more approachable to Python programmers, Jeremy shows step by step how to start with Python implementations, and then convert them largely automatically to CUDA. This approach, he argues, simplifies debugging and development.<br />\n<br>\nThe tutorial is structured in a hands-on manner, encouraging viewers to follow along in a Colab notebook. Jeremy uses practical examples, starting with converting an RGB image to grayscale using CUDA, demonstrating the process step-by-step. He further explains the memory layout in GPUs, emphasizing the differences from CPU memory structures, and introduces key CUDA concepts like streaming multi-processors and CUDA cores.<br />\n<br>\nJeremy then delves into more advanced topics, such as matrix multiplication, a critical operation in deep learning. He demonstrates how to implement matrix multiplication in Python first and then translates it to CUDA, highlighting the significant performance gains achievable with GPU programming. The tutorial also covers CUDA's intricacies, such as shared memory, thread blocks, and optimizing CUDA kernels.<br />\n<br>\nThe tutorial also includes a section on setting up the CUDA environment on various systems using Conda, making it accessible for a wide range of users.<br />\n<br>\nThis is lecture 3 of the &amp;quot;CUDA Mode&amp;quot; series (but you don't need to watch the others first). The notebook is available in the lecture3 folder here: <a href=\"https://github.com/cuda-mode/lecture2\">https://github.com/cuda-mode/lecture2</a>...</p>\n</blockquote>\n</div>\n","tags":["gpu","cuda","python","programming"]},{"title":"OpenAI Microscope","contentType":"bookmarks","date":"2024-01-23 22:15","url":"/responses/openai-microscope/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/research/microscope\" >https://openai.com/research/microscope</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We’re introducing OpenAI Microscope, a collection of visualizations of every significant layer and neuron of eight vision “model organisms” which are often studied in interpretability. Microscope makes it easier to analyze the features that form inside these neural networks, and we hope it will help the research community as we move towards understanding these complicated systems.</p>\n</blockquote>\n</div>\n","tags":["openai","interpretability","ai","generativeai","visualization"]},{"title":"Willow Protocol Specifications","contentType":"bookmarks","date":"2024-01-17 20:38","url":"/responses/willow-protocol/","content":"<div class=\"response-target\" >→ <a href=\"https://willowprotocol.org/\" >https://willowprotocol.org/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A protocol for peer-to-peer data stores. The best parts? Fine-grained permissions, a keen approach to privacy, destructive edits, and a dainty bandwidth and memory footprint.</p>\n</blockquote>\n</div>\n","tags":["willow","protocol","p2p","privacy","data"]},{"title":"Stable Code 3B - Coding on the Edge","contentType":"bookmarks","date":"2024-01-17 20:36","url":"/responses/stable-code-3b-stability/","content":"<div class=\"response-target\" >→ <a href=\"https://stability.ai/news/stable-code-2024-llm-code-completion-release\" >https://stability.ai/news/stable-code-2024-llm-code-completion-release</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Stable Code 3B is a 3 billion parameter Large Language Model (LLM), allowing accurate and responsive code completion at a level on par with models such as CodeLLaMA 7b that are 2.5x larger.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Operates offline even without a GPU on common laptops such as a MacBook Air.</p>\n</blockquote>\n</div>\n","tags":["ai","stabilityai\"","code","software","softwaredevelopment","llm"]},{"title":"Sampling for Text Generation","contentType":"bookmarks","date":"2024-01-17 20:32","url":"/responses/sampling-text-generation-huyen/","content":"<div class=\"response-target\" >→ <a href=\"https://huyenchip.com//2024/01/16/sampling.html\" >https://huyenchip.com//2024/01/16/sampling.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>ML models are probabilistic. Imagine that you want to know what’s the best cuisine in the world. If you ask someone this question twice, a minute apart, their answers both times should be the same. If you ask a model the same question twice, its answer can change.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This probabilistic nature makes AI great for creative tasks.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>However, this probabilistic nature also causes inconsistency and hallucinations. It’s fatal for tasks that depend on factuality. Recently, I went over 3 months’ worth of customer support requests of an AI startup I advise and found that ⅕ of the questions are because users don’t understand or don’t know how to work with this probabilistic nature.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>To understand why AI’s responses are probabilistic, we need to understand how models generate responses, a process known as sampling (or decoding). This post consists of 3 parts.\n<br></p>\n<pre><code>1. Sampling: sampling strategies and sampling variables including temperature, top-k, and top-p.\n2. Test time sampling: sampling multiple outputs to help improve a model’s performance.\n3. Structured outputs: how to get models to generate outputs in a certain format.\n</code></pre>\n</blockquote>\n</div>\n","tags":["ai","generativeai","statistics","llm"]},{"title":"SingSong - Generating musical accompaniments from singing","contentType":"bookmarks","date":"2024-01-17 19:39","url":"/responses/singsong-ai-music-accompaniment/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2301.12662\" >https://arxiv.org/abs/2301.12662</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We present SingSong, a system that generates instrumental music to accompany input vocals, potentially offering musicians and non-musicians alike an intuitive new way to create music featuring their own voice. To accomplish this, we build on recent developments in musical source separation and audio generation. Specifically, we apply a state-of-the-art source separation algorithm to a large corpus of music audio to produce aligned pairs of vocals and instrumental sources. Then, we adapt AudioLM (Borsos et al., 2022) -- a state-of-the-art approach for unconditional audio generation -- to be suitable for conditional &amp;quot;audio-to-audio&amp;quot; generation tasks, and train it on the source-separated (vocal, instrumental) pairs. In a pairwise comparison with the same vocal inputs, listeners expressed a significant preference for instrumentals generated by SingSong compared to those from a strong retrieval baseline.\nSound examples at <a href=\"https://g.co/magenta/singsong\">this https URL</a></p>\n</blockquote>\n<p>AI can now help you create a backing track to all the songs you make up about your pets.</p>\n</div>\n","tags":["ai","generativeai","music"]},{"title":"AI for economists - prompts and resources","contentType":"bookmarks","date":"2024-01-15 16:49","url":"/responses/ai-for-economists-prompts/","content":"<div class=\"response-target\" >→ <a href=\"https://sites.google.com/view/lastunen/ai-for-economists\" >https://sites.google.com/view/lastunen/ai-for-economists</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This page contains example prompts and responses intended to showcase how generative AI, namely LLMs like GPT-4, can benefit economists.\n<br>\nExample prompts are shown from six domains: ideation and feedback; writing; background research; coding; data analysis; and mathematical derivations.\n<br>\nThe framework as well as some of the prompts and related notes come from Korinek, A. 2023. <a href=\"https://www.aeaweb.org/articles?id=10.1257/jel.20231736\">“Generative AI for Economic Research: Use Cases and Implications for Economists“</a>, Journal of Economic Literature, 61 (4): 1281–1317.\n<br>\nEach application area includes 1-3 prompts and responses from an LLM, often from the field of development economics, along with brief notes. The prompts will be updated periodically.</p>\n</blockquote>\n</div>\n","tags":["ai","economy","promptengineering","llm","gpt"]},{"title":"Clarkesworld - Sci-fi and Fantasy Magazine","contentType":"bookmarks","date":"2024-01-15 15:00","url":"/responses/clarkesworld-sci-fi-fantasy-magazine/","content":"<div class=\"response-target\" >→ <a href=\"https://clarkesworldmagazine.com/\" >https://clarkesworldmagazine.com/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Clarkesworld is a monthly science fiction and fantasy magazine first published in October 2006. Each issue contains interviews, thought-provoking articles, and between six and eight works of original fiction.</p>\n</blockquote>\n</div>\n","tags":["magazine","scifi","fantasy","online","fiction","nonfiction","art"]},{"title":"Every - Daily Newsletter","contentType":"bookmarks","date":"2024-01-15 11:08","url":"/responses/every-newsletter/","content":"<div class=\"response-target\" >→ <a href=\"https://every.to/\" >https://every.to/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Every is a daily newsletter founded in 2020. Every day, we publish a long-form essay to make you smarter about technology, productivity, and AI.</p>\n</blockquote>\n</div>\n","tags":["newsletter","technology","productivity","ai","essay","internet"]},{"title":"Cool URIs don't change","contentType":"bookmarks","date":"2024-01-13 10:23","url":"/responses/cool-uris-dont-change/","content":"<div class=\"response-target\" >→ <a href=\"https://www.w3.org/Provider/Style/URI\" >https://www.w3.org/Provider/Style/URI</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Keeping URIs so that they will still be around in 2, 20 or 200 or even 2000 years is clearly not as simple as it sounds. However, all over the Web, webmasters are making decisions which will make it really difficult for themselves in the future. Often, this is because they are using tools whose task is seen as to present the best site in the moment, and no one has evaluated what will happen to the links when things change. The message here is, however, that many, many things can change and your URIs can and should stay the same. They only can if you think about how you design them.</p>\n</blockquote>\n</div>\n","tags":["url","http","standard","protocol","web","internet"]},{"title":"Where is all of the Fediverse?","contentType":"bookmarks","date":"2024-01-13 10:15","url":"/responses/where-is-all-fediverse/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.benjojo.co.uk/post/who-hosts-the-fediverse-instances\" >https://blog.benjojo.co.uk/post/who-hosts-the-fediverse-instances</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Here we can see that Fastly and Cloudflare make up over 50% of the entire fediverse network.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...for the population using Cloudflare, a fair number of them (30%) appear to be hosting the instances behind a home broadband connection.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...the German hosting provider Hetzner hosts over 51% of the entire network!</p>\n</blockquote>\n</div>\n","tags":["fediverse","analytics","web"]},{"title":"Marimo","contentType":"bookmarks","date":"2024-01-13 09:38","url":"/responses/marimo-notebook/","content":"<div class=\"response-target\" >→ <a href=\"https://marimo.io/\" >https://marimo.io/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>marimo is an open-source reactive notebook for Python — reproducible, git-friendly, executable as a script, and shareable as an app.</p>\n</blockquote>\n</div>\n","tags":["notebook","programming","literateprogramming"]},{"title":"IndieWeb assimilation","contentType":"bookmarks","date":"2024-01-13 09:05","url":"/responses/indieweb-assimilation-shellshark/","content":"<div class=\"response-target\" >→ <a href=\"https://shellsharks.com/indieweb\" >https://shellsharks.com/indieweb</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Principle Mechanics<br />\n<br><br />\nThis section does not provide exhaustive coverage of how to implement IndieWeb functionality. Instead, I simply summarize five core primitives which I feel comprise an IndieWeb site. For a more official gauge on where a site scores within the IndieWeb spectrum, consider leveraging IndieMark!<br />\n<br>\nHosting: You need a place to host your site and store your content. There are a lot of great options out there. Ideally, choose one that allows you the ability to make some under-the-hood changes and does not limit your content portability.<br />\n<br>\nSyndication: Share your content with the world! There are two preferred methods for syndication, PESOS and POSSE. This resource does a great job explaining both! For more examples of how this is done, check this and this out. RSS is a great starting point for helping others subscribe to new content on your site.<br />\n<br>\nWriting: Though your site could simply serve as a more static point/identity on the web, with little to no “content” being regularly added, I recommend writing!<br />\n<br>\nInteractivity: One of the more advanced concepts within the IndieWeb world, the ability to bake in native comments, replies, likes, etc is a greay way to build community. This interactivity helps mitigate reliance on centralized social networks for communication within Indie communities. One example of IndieWeb interactivity is Webmentions.<br />\n<br>\nIdentity: Make it unique, make it fun, make it yours. The corporate web is sterile and suffocating. Let’s bring back the whimsy of the old web.</p>\n</blockquote>\n</div>\n","tags":["indieweb","guide","catalog"]},{"title":"My AI Timelines Have Sped Up (Again)","contentType":"bookmarks","date":"2024-01-11 10:07","url":"/responses/ai-pipelines-sped-up-again-alexirpan/","content":"<div class=\"response-target\" >→ <a href=\"https://www.alexirpan.com/2024/01/10/ai-timelines-2024.html\" >https://www.alexirpan.com/2024/01/10/ai-timelines-2024.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...computers are useful, ML models are useful, and even if models fail to scale, people will want to fit GPT-4 sized models on their phone. It seems reasonable to assume the competing factions will figure something out.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Data seems like the harder question. (Or at least the one I feel qualified talking about.) We have already crossed the event horizon of trying to train on everything on the Internet. It’s increasingly difficult for labs to differentiate themselves on publicly available data. Differentiation is instead coming from non-public high-quality data to augment public low-quality data.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>All the scaling laws have followed power laws so far, including dataset size. Getting more data by hand doesn’t seem good enough to cross to the next thresholds. We need better means to get good data.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>A long time ago, when OpenAI still did RL in games / simulation, they were very into self-play. You run agents against copies of themselves, score their interactions, and update the models towards interactions with higher reward. Given enough time, they learn complex strategies through competition.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I think it’s possible we’re at the start of a world where self-play or self-play-like ideas work to improve LLM capabilities. Drawing an analogy, the environment is the dialogue, actions are text generated from an LLM, and the reward is from whatever reward model you have. Instead of using ground truth data, our models may be at a point where they can generate data that’s good enough to train on.</p>\n</blockquote>\n</div>\n","tags":["ai","predictions","agi","data","llm","technology","ml","computervision"]},{"title":"Introducing the GPT Store","contentType":"bookmarks","date":"2024-01-11 10:01","url":"/responses/introducing-gpt-store/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/blog/introducing-the-gpt-store\" >https://openai.com/blog/introducing-the-gpt-store</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>It’s been two months since we announced GPTs, and users have already created over 3 million custom versions of ChatGPT. Many builders have shared their GPTs for others to use. Today, we're starting to roll out the GPT Store to ChatGPT Plus, Team and Enterprise users so you can find useful and popular GPTs.</p>\n</blockquote>\n</div>\n","tags":["openai","gpt","ai"]},{"title":"Python 3.13 gets a JIT","contentType":"bookmarks","date":"2024-01-09 09:53","url":"/responses/python-3-13-gets-jit/","content":"<div class=\"response-target\" >→ <a href=\"https://tonybaloney.github.io/posts/python-gets-a-jit.html\" >https://tonybaloney.github.io/posts/python-gets-a-jit.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In late December 2023 (Christmas Day to be precise), CPython core developer <a href=\"https://github.com/python/cpython/pull/113465\">Brandt Bucher submitted a little pull-request</a> to the Python 3.13 branch adding a JIT compiler.</p>\n</blockquote>\n</div>\n","tags":["python","programming","compiler"]},{"title":"TIOBE Index for January 2024","contentType":"bookmarks","date":"2024-01-07 22:52","url":"/responses/tiobe-index-january-2024/","content":"<div class=\"response-target\" >→ <a href=\"https://www.tiobe.com/tiobe-index/\" >https://www.tiobe.com/tiobe-index/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>For the first time in the history of the TIOBE index, C# has won the programming language of the year award. Congratulations! C# has been a top 10 player for more than 2 decades and now that it is catching up with the big 4 languages, it won the well-deserved award by being the language with the biggest uptick in one year (+1.43%).</p>\n</blockquote>\n<p>Exciting to see F# almost break into the Top 20 at number 22 with 0.77%.</p>\n</div>\n","tags":["programming","dotnet","technology","fsharp","csharp","programminglanguages"]},{"title":"Ferret: Refer and Ground Anything Anywhere at Any Granularity","contentType":"bookmarks","date":"2023-12-25 20:12","url":"/responses/apple-ml-ferret/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2310.07704\" >https://arxiv.org/abs/2310.07704</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination. Code and data will be available at this https URL</p>\n</blockquote>\n<p><a href=\"https://github.com/apple/ml-ferret\">Code</a></p>\n</div>\n","tags":["ai","ml","llm","mllm","largelanguagemodel","multimodal","multimodallargelanguagemodel","apple","opensource"]},{"title":"VideoPoet: A large language model for zero-shot video generation","contentType":"bookmarks","date":"2023-12-23 19:06","url":"/responses/videopoet-llm-zero-shot-video-generation/","content":"<div class=\"response-target\" >→ <a href=\"https://sites.research.google/videopoet/\" >https://sites.research.google/videopoet/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>VideoPoet is a simple modeling method that can convert any autoregressive language model or large language model (LLM) into a high-quality video generator. It contains a few simple components:</p>\n<ul>\n<li>A pre-trained MAGVIT V2 video tokenizer and a SoundStream audio tokenizer transform images, video, and audio clips with variable lengths into a sequence of discrete codes in a unified vocabulary. These codes are compatible with text-based language models, facilitating an integration with other modalities, such as text.</li>\n<li>An autoregressive language model learns across video, image, audio, and text modalities to autoregressively predict the next video or audio token in the sequence.</li>\n<li>A mixture of multimodal generative learning objectives are introduced into the LLM training framework, including text-to-video, text-to-image, image-to-video, video frame continuation, video inpainting and outpainting, video stylization, and video-to-audio. Furthermore, such tasks can be composed together for additional zero-shot capabilities (e.g., text-to-audio).</li>\n</ul>\n<p>This simple recipe shows that language models can synthesize and edit videos with a high degree of temporal consistency. VideoPoet demonstrates state-of-the-art video generation, in particular in producing a wide range of large, interesting, and high-fidelity motions. The VideoPoet model supports generating videos in square orientation, or portrait to tailor generations towards short-form content, as well as supporting audio generation from a video input.</p>\n</blockquote>\n<p><a href=\"https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html\">Blog Post</a></p>\n</div>\n","tags":["ai","genai","videopoet","generativeai","google"]},{"title":"Midjourney v6","contentType":"bookmarks","date":"2023-12-22 08:57","url":"/responses/midjourney-v6-release/","content":"<div class=\"response-target\" >→ <a href=\"https://mid-journey.ai/midjourney-v6-release/\" >https://mid-journey.ai/midjourney-v6-release/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The Dev Team gonna let the community test an alpha-version of Midjourney v6 model over the winter break, starting tonight, December 21st, 2023.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>What’s new with the Midjourney v6 base model?</p>\n<ul>\n<li>Much more accurate prompt following as well as longer prompts,</li>\n<li>Improved coherence, and model knowledge,</li>\n<li>Improved image prompting and remix mode,</li>\n<li>Minor text drawing ability (you must write your text in “quotations” and --style raw or lower --stylize values may help)</li>\n<li>/imagine a photo of the text &amp;quot;Hello World!&amp;quot; written with a marker on a sticky note --ar 16:9 --v 6</li>\n<li>Improved upscalers, with both 'subtle‘ and 'creative‘ modes (increases resolution by 2x) (you’ll see buttons for these under your images after clicking U1/U2/U3/U4)</li>\n</ul>\n</blockquote>\n</div>\n","tags":["computervision","ai","midjourney"]},{"title":"LangChain State of AI 2023","contentType":"bookmarks","date":"2023-12-22 08:32","url":"/responses/langchain-state-of-ai-2023/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.langchain.dev/langchain-state-of-ai-2023/\" >https://blog.langchain.dev/langchain-state-of-ai-2023/</a></div><div class=\"response-content\" >\n<blockquote class=\"blockquote\">\n<p>Retrieval has emerged as the dominant way to combine your data with LLMs.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...42% of complex queries involve retrieval</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...about 17% of complex queries are part of an agent.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>OpenAI has emerged as the leading LLM provider of 2023, and Azure (with more enterprise guarantees) has seized that momentum well.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>On the open source model side, we see Hugging Face (4th), Fireworks AI (6th), and Ollama (7th) emerge as the main ways users interact with those models.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>A lot of attention recently has been given to open source models, with more and more providers racing to host them at cheaper and cheaper costs. So how exactly are developers accessing these open source models?</p>\n<p>We see that the people are mainly running them locally, with options to do so like Hugging Face, LlamaCpp, Ollama, and GPT4All ranking high.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>Vectorstores are emerging as the primary way to retrieve relevant context.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...local vectorstores are the most used, with Chroma, FAISS, Qdrant and DocArray all ranking in the top 5.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Of the hosted offerings, Pinecone leads the pack as the only hosted vectorstore in the top 5. Weaviate follows next, showing that vector-native databases are currently more used than databases that add in vector functionality.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Of databases that have added in vector functionality, we see Postgres (PGVector), Supabase, Neo4j, Redis, Azure Search, and Astra DB leading the pack.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>OpenAI reigns supreme</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Open source providers are more used, with Hugging Face coming in 2nd most use</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>On the hosted side, we see that Vertex AI actually beats out AzureOpenAI</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>the most common retrieval strategy we see is not a built-in one but rather a custom one.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>After that, we see more familiar names popping up:</p>\n<ul>\n<li><strong>Self Query</strong> - which extracts metadata filters from user's questions</li>\n<li><strong>Hybrid Search</strong> - mainly through provider specific integrations like Supabase and Pinecone</li>\n<li><strong>Contextual Compression</strong> - which is postprocessing of base retrieval results</li>\n<li><strong>Multi Query</strong> - transforming a single query into multiple, and then retrieving results for all</li>\n<li><strong>TimeWeighted VectorStore</strong> - give more preference to recent documents</li>\n</ul>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...the majority of them use an LLM to evaluate the outputs. While some have expressed concern and hesitation around this, we are bullish on this as an approach and see that in practice it has emerged as the dominant way to test.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...nearly 40% of evaluators are custom evaluators. This is in line with the fact that we've observed that evaluation is often really specific to the application being worked on, and there's no one-size-fits-all evaluator to rely on.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>...most people are still primarily concerned with the correctness of their application (as opposed to toxicity, prompt leakage, or other guardrails</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...low usage of Exact Matching as an evaluation technique [suggests] that judging correctness is often quite complex (you can't just compare the output exactly as is)</p>\n</blockquote>\n</div>\n","tags":["ai","llm","opensource","report"]},{"title":"Eigensolutions: composability as the antidote to overfit ","contentType":"bookmarks","date":"2023-12-21 22:38","url":"/responses/eigensolutions-composability-antidote-overfitting-verou/","content":"<div class=\"response-target\" >→ <a href=\"https://lea.verou.me/blog/2023/eigensolutions/\" >https://lea.verou.me/blog/2023/eigensolutions/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>tl;dr: Overfitting happens when solutions don’t generalize sufficiently and is a hallmark of poor design. Eigensolutions are the opposite: solutions that generalize so much they expose links between seemingly unrelated use cases. Designing eigensolutions takes a mindset shift from linear design to composability.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The eigensolution is a solution that addresses several key use cases, that previously appeared unrelated.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...it takes a mindset shift, from the linear Use case → Idea → Solution process to composability. Rather than designing a solution to address only our driving use cases, step back and ask yourself: can we design a solution as a composition of smaller, more general features, that could be used together to address a broader set of use cases?</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Contrary to what you may expect, eigensolutions can actually be quite hard to push to stakeholders:</p>\n<ul>\n<li>Due to their generality, they often require significantly higher engineering effort to implement. Quick-wins are easier to sell: they ship faster and add value sooner. In my 11 years designing web technologies, I have seen many beautiful, elegant eigensolutions be vetoed due to implementation difficulties in favor of far more specific solutions — and often this was the right decision, it’s all about the cost-benefit.</li>\n<li>Eigensolutions tend to be lower level primitives, which are more flexible, but can also involve higher friction to use than a solution that is tailored to a specific use case.</li>\n</ul>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Eigensolutions tend to be lower level primitives. They enable a broad set of use cases, but may not be the most learnable or efficient way to implement all of them, compared to a tailored solution. In other words, they make complex things possible, but do not necessarily make common things easy.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Instead of implementing tailored solutions ad-hoc (risking overfitting), they can be implemented as shortcuts: higher level abstractions using the lower level primitive. Done well, shortcuts provide dual benefit: not only do they reduce friction for common cases, they also serve as teaching aids for the underlying lower level feature. This offers a very smooth ease-of-use to power curve: if users need to go further than what the shortcut provides, they can always fall back on the lower level primitive to do so.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In an ideal world, lower level primitives and higher level abstractions would be designed and shipped together. However, engineering resources are typically limited, and it often makes sense to ship one before the other, so we can provide value sooner.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This can happen in either direction:</p>\n<ul>\n<li>Lower level primitive first. Shortcuts to make common cases easy can ship at a later stage, and demos and documentation to showcase common “recipes” can be used as a stopgap meanwhile. This prioritizes use case coverage over optimal UX, but it also allows collecting more data, which can inform the design of the shortcuts implemented.</li>\n<li>Higher level abstraction first, as an independent, ostensibly ad hoc feature. Then later, once the lower level primitive ships, it is used to “explain” the shortcut, and make it more powerful. This prioritizes optimal UX over use case coverage: we’re not covering all use cases, but for the ones we are covering, we’re offering a frictionless user experience.</li>\n</ul>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...despite the name eigensolution, it’s still all about the use cases: eigensolutions just expose links between use cases that may have been hard to detect, but seem obvious in retrospect...Requiring all use cases to precede any design work can be unnecessarily restrictive, as frequently solving a problem improves our understanding of the problem.</p>\n</blockquote>\n</div>\n","tags":["design","product"]},{"title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory","contentType":"bookmarks","date":"2023-12-20 18:30","url":"/responses/llm-flash-inference-limited-memory/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2312.11514\" >https://arxiv.org/abs/2312.11514</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their intensive computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters on flash memory but bringing them on demand to DRAM. Our method involves constructing an inference cost model that harmonizes with the flash memory behavior, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this flash memory-informed framework, we introduce two principal techniques. First, &amp;quot;windowing'&amp;quot; strategically reduces data transfer by reusing previously activated neurons, and second, &amp;quot;row-column bundling&amp;quot;, tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.</p>\n</blockquote>\n</div>\n","tags":["llm","ai","hardware"]},{"title":"It’s Time to Dismantle the Technopoly","contentType":"bookmarks","date":"2023-12-20 10:12","url":"/responses/dismantle-technology-newport/","content":"<div class=\"response-target\" >→ <a href=\"https://www.newyorker.com/tech/annals-of-technology/its-time-to-dismantle-the-technopoly\" >https://www.newyorker.com/tech/annals-of-technology/its-time-to-dismantle-the-technopoly</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...according to [Neil Postman], we no longer live in a technocratic era. We now inhabit what he calls technopoly. In this third technological age, Postman argues, the fight between invention and traditional values has been resolved, with the former emerging as the clear winner. The result is the “submission of all forms of cultural life to the sovereignty of technique and technology.” Innovation and increased efficiency become the unchallenged mechanisms of progress, while any doubts about the imperative to accommodate the shiny and new are marginalized. “Technopoly eliminates alternatives to itself in precisely the way Aldous Huxley outlined in Brave New World,” Postman writes. “It does not make them illegal. It does not make them immoral. It does not even make them unpopular. It makes them invisible and therefore irrelevant.” Technopoly, he concludes, “is totalitarian technocracy.”</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>What I didn’t realize back in 2016, however, was that, although the grip of technopoly was strong, it was also soon to weaken.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>A major source of this destabilization was the Trump-Clinton election cycle...Where once they had seen platforms like Facebook as useful and in some sense mandatory, they started treating them more warily.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This emerging resistance to the technopoly mind-set doesn’t fall neatly onto a spectrum with techno-optimism at one end and techno-skepticism at the other. Instead, it occupies an orthogonal dimension we might call techno-selectionism. This is a perspective that accepts the idea that innovations can significantly improve our lives but also holds that we can build new things without having to accept every popular invention as inevitable. Techno-selectionists believe that we should continue to encourage and reward people who experiment with what comes next. But they also know that some experiments end up causing more bad than good. Techno-selectionists can be enthusiastic about artificial intelligence, say, while also taking a strong stance on settings where we should block its use. They can marvel at the benefits of the social Internet without surrendering their kids’ mental lives to TikTok.</p>\n</blockquote>\n</div>\n","tags":["technology","culture","media","internet"]},{"title":"OpenAI - Prompt engineering","contentType":"bookmarks","date":"2023-12-19 19:37","url":"/responses/openai-prompting-guide/","content":"<div class=\"response-target\" >→ <a href=\"https://platform.openai.com/docs/guides/prompt-engineering\" >https://platform.openai.com/docs/guides/prompt-engineering</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This guide shares strategies and tactics for getting better results from large language models (sometimes referred to as GPT models) like GPT-4. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.</p>\n</blockquote>\n</div>\n","tags":["openai","llm","promptengineering","ai","guide"]},{"title":"Solo - an AI website builder for solopreneurs","contentType":"bookmarks","date":"2023-12-14 10:10","url":"/responses/solo-mozilla-ai-website-builder/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.mozilla.org/en/mozilla/introducing-solo-ai-website-builder/\" >https://blog.mozilla.org/en/mozilla/introducing-solo-ai-website-builder/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today we are excited to introduce a new Mozilla Innovation Project, Solo, an AI website builder for solopreneurs.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>If you scour Yelp, it appears a third of businesses lack a website. However, building a website not only provides you with a presence that you own and control but it is also good for business.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Our survey data shows that the majority of solopreneurs rely upon their “tech buddy” to help build their website. As a result, the websites become stale and harder to maintain as it relies on a call to their buddy. Others without a “tech buddy” try popular website authoring tools and then abandon because it’s simply too hard to author and curate content.</p>\n<p>Using AI to generate the content of your site and source your images, which a solopreneur can then revise into their own unique voice and style levels the playing field. Solo takes this a step further and can also scrape your existing business Yelp or other page so you have an online presence that is totally authentic to you.</p>\n</blockquote>\n</div>\n","tags":["ai","mozilla","web","internet"]},{"title":"MemoryCache - Augmenting Local AI with Browser Data","contentType":"bookmarks","date":"2023-12-12 19:37","url":"/responses/memorycache-personal-ai/","content":"<div class=\"response-target\" >→ <a href=\"https://future.mozilla.org/blog/introducing-memorycache/\" >https://future.mozilla.org/blog/introducing-memorycache/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>MemoryCache, a Mozilla Innovation Project, is an early exploration project that augments an on-device, personal model with local files saved from the browser to reflect a more personalized and tailored experience through the lens of privacy and agency.</p>\n</blockquote>\n\n<p><a href=\"https://memorycache.ai/\">https://memorycache.ai/</a></p>\n</div>\n","tags":["ai","web","privacy","opensource"]},{"title":"Mozilla Innovation Week - Explore the Future of AI with Mozilla","contentType":"bookmarks","date":"2023-12-12 19:34","url":"/responses/mozilla-innovation-week-dec-2023/","content":"<div class=\"response-target\" >→ <a href=\"https://future.mozilla.org/innovation-week/\" >https://future.mozilla.org/innovation-week/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Mozilla’s Innovation Week is a journey into the future of technology, where AI is not just a buzzword, but a reality we're actively shaping. Here, we're not just talking about innovation – we're living it through a series of AI-driven explorations.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>With that in mind, Innovation Week is more than a showcase. It's a platform for collaboration and inspiration. It's about bringing together ideas, people, and technology to pave the way for a more open and responsible future.</p>\n</blockquote>\n</div>\n","tags":["ai","opensource","mozilla"]},{"title":"Bash One-Liners for LLMs","contentType":"bookmarks","date":"2023-12-12 19:21","url":"/responses/llamafile-llm-oneliners-tunney/","content":"<div class=\"response-target\" >→ <a href=\"https://justine.lol/oneliners/\" >https://justine.lol/oneliners/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>I spent the last month working with Mozilla to launch an open source project called llamafile which is the new best way to run an LLM on your own computer. So far things have been going pretty smoothly. The project earned 5.6k stars on GitHub, 1073 upvotes on Hacker News, and received press coverage from Hackaday. Yesterday I cut a 0.3 release so let's see what it can do.</p>\n</blockquote>\n</div>\n","tags":["llama","ai","llm","opensource"]},{"title":"Mixtral 8x7B on Apple Silicon with MLX","contentType":"bookmarks","date":"2023-12-12 19:19","url":"/responses/mlx-mixtral-8x7b/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/ml-explore/mlx-examples/tree/main/mixtral\" >https://github.com/ml-explore/mlx-examples/tree/main/mixtral</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Run the Mixtral1 8x7B mixture-of-experts (MoE) model in MLX on Apple silicon.</p>\n</blockquote>\n</div>\n","tags":["mlx","ai","frameworks"]},{"title":"State of the Word 2023","contentType":"bookmarks","date":"2023-12-12 19:17","url":"/responses/state-of-the-word-2023/","content":"<div class=\"response-target\" >→ <a href=\"https://wordpress.org/state-of-the-word/\" >https://wordpress.org/state-of-the-word/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>State of the Word is the annual keynote address delivered by the WordPress project’s co-founder, Matt Mullenweg, celebrating the progress of the open source project and offering a glimpse into its future.</p>\n</blockquote>\n<p><a href=\"https://www.youtube.com/watch?v=c7M4mBVgP3Y\" title=\"State of the Word 2023\"><img src=\"http://img.youtube.com/vi/c7M4mBVgP3Y/0.jpg\" class=\"img-fluid\" alt=\"State of the Word 2023\" /></a></p>\n</div>\n","tags":["wordpress","blogging","web","opensource","community"]},{"title":"Steering at the Frontier: Extending the Power of Prompting ","contentType":"bookmarks","date":"2023-12-12 19:14","url":"/responses/medprompt-promptbase-extending-power-prompting/","content":"<div class=\"response-target\" >→ <a href=\"https://www.microsoft.com/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/\" >https://www.microsoft.com/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...steering GPT-4 with a modified version of Medprompt achieves the highest score ever achieved on the complete MMLU.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>To achieve a new SoTA on MMLU, we extended Medprompt to Medprompt+ by adding a simpler prompting method and formulating a policy for deriving a final answer by integrating outputs from both the base Medprompt strategy and the simple prompts. The synthesis of a final answer is guided by a control strategy governed by GPT-4 and inferred confidences of candidate answers.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>While systematic prompt engineering can yield maximal performance, we continue to explore the out-of-the-box performance of frontier models with simple prompts. It’s important to keep an eye on the native power of GPT-4 and how we can steer the model with zero- or few-shot prompting strategies.</p>\n</blockquote>\n</div>\n","tags":["ai","promptengineering"]},{"title":"promptbase","contentType":"bookmarks","date":"2023-12-12 19:13","url":"/responses/microsoft-promptbase/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/microsoft/promptbase\" >https://github.com/microsoft/promptbase</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>promptbase is an evolving collection of resources, best practices, and example scripts for eliciting the best performance from foundation models like GPT-4.</p>\n</blockquote>\n</div>\n","tags":["ai","promptengineering"]},{"title":"LLM360: Towards Fully Transparent Open-Source LLMs","contentType":"bookmarks","date":"2023-12-12 19:02","url":"/responses/llm360-transparent-open-source-llm/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2312.06550\" >https://arxiv.org/abs/2312.06550</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at this <a href=\"https://www.llm360.ai/\">https URL</a>). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.</p>\n</blockquote>\n\n<p><a href=\"https://www.llm360.ai/\">https://www.llm360.ai/</a></p>\n</div>\n","tags":["ai","research"]},{"title":"Phi-2: The surprising power of small language models","contentType":"bookmarks","date":"2023-12-12 19:00","url":"/responses/microsoft-phi-2/","content":"<div class=\"response-target\" >→ <a href=\"https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/\" >https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We are now releasing Phi-2, a 2.7 billion-parameter language model that demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters. On complex benchmarks Phi-2 matches or outperforms models up to 25x larger, thanks to new innovations in model scaling and training data curation.</p>\n</blockquote>\n</div>\n","tags":["ai","slm","llm"]},{"title":"Answer.AI - A new old kind of R&D lab","contentType":"bookmarks","date":"2023-12-12 18:57","url":"/responses/answer-ai-rd-lab/","content":"<div class=\"response-target\" >→ <a href=\"https://www.answer.ai/posts/2023-12-12-launch.html\" >https://www.answer.ai/posts/2023-12-12-launch.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Jeremy Howard (founding CEO, previously co-founder of Kaggle and fast.ai) and Eric Ries (founding director, previously creator of Lean Startup and the Long-Term Stock Exchange) today launched Answer.AI, a new kind of AI R&amp;amp;D lab which creates practical end-user products based on foundational research breakthroughs. The creation of Answer.AI is supported by an investment of USD10m from Decibel VC. Answer.AI will be a fully-remote team of deep-tech generalists—the world’s very best, regardless of where they live, what school they went to, or any other meaningless surface feature.</p>\n</blockquote>\n</div>\n","tags":["ai","research","vc"]},{"title":"Introducing Stable LM Zephyr 3B","contentType":"bookmarks","date":"2023-12-11 20:46","url":"/responses/introducing-stable-lm-zephyr-3b/","content":"<div class=\"response-target\" >→ <a href=\"https://stability.ai/news/stablelm-zephyr-3b-stability-llm\" >https://stability.ai/news/stablelm-zephyr-3b-stability-llm</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Stable LM Zephyr 3B is a 3 billion parameter Large Language Model (LLM), 60% smaller than 7B models, allowing accurate, and responsive output on a variety of devices without requiring high-end hardware.</p>\n</blockquote>\n</div>\n","tags":["llm","ai","edge"]},{"title":"The Geometry of Truth: Dataexplorer","contentType":"bookmarks","date":"2023-12-11 20:19","url":"/responses/geometry-of-truth-data-explorer/","content":"<div class=\"response-target\" >→ <a href=\"https://saprmarks.github.io/geometry-of-truth/dataexplorer/\" >https://saprmarks.github.io/geometry-of-truth/dataexplorer/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This page contains interactive charts for exploring how large language models represent truth. It accompanies the paper <a href=\"https://arxiv.org/abs/2310.06824\">The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets</a> by Samuel Marks and Max Tegmark.</p>\n<p>To produce these visualizations, we first extract <a href=\"https://ai.meta.com/blog/large-language-model-llama-meta-ai/\">LLaMA-13B</a> representations of factual statements. These representations live in a 5120-dimensional space, far too high-dimensional for us to picture, so we use <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">PCA</a> to select the two directions of greatest variation for the data. This allows us to produce 2-dimensional pictures of 5120-dimensional data. See this footnote for more details.<a href=\"https://saprmarks.github.io/geometry-of-truth/dataexplorer/#fn:1\">1</a></p>\n</blockquote>\n</div>\n","tags":["ai","interpretability","llm"]},{"title":"State of AI Report - 2023","contentType":"bookmarks","date":"2023-12-11 20:13","url":"/responses/2023-state-of-ai-report/","content":"<div class=\"response-target\" >→ <a href=\"https://docs.google.com/presentation/d/156WpBF_rGvf4Ecg19oM1fyR51g4FAmHV3Zs0WLukrLQ/edit?usp=sharing\" >https://docs.google.com/presentation/d/156WpBF_rGvf4Ecg19oM1fyR51g4FAmHV3Zs0WLukrLQ/edit?usp=sharing</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Key themes in the 2023 Report include:</p>\n<ul>\n<li><strong>GPT-4 is the master of all it surveys (for now)</strong>, beating every other LLM on both classic benchmarks and exams designed to evaluate humans, validating the power of proprietary architectures and reinforcement learning from human feedback.</li>\n<li><strong>Efforts are growing to try to clone or surpass proprietary performance</strong>, through smaller models, better datasets, and longer context. These could gain new urgency, amid concerns that human-generated data may only be able to sustain AI scaling trends for a few more years.</li>\n<li><strong>LLMs and diffusion models continue to drive real-world breakthroughs</strong>, especially in the life sciences, with meaningful steps forward in both molecular biology and drug discovery.</li>\n<li><strong>Compute is the new oil</strong>, with NVIDIA printing record earnings and startups wielding their GPUs as a competitive edge. As the US tightens its restrictions on trade restrictions on China and mobilizes its allies in the chip wars, NVIDIA, Intel, and AMD have started to sell export-control proof chips at scale.</li>\n<li><strong>GenAI saves the VC world</strong>, as amid a slump in tech valuations, AI startups focused on generative AI applications (including video, text, and coding), raised over $18 billion from VC and corporate investors.</li>\n<li><strong>The safety debate has exploded into the mainstream</strong>, prompting action from governments and regulators around the world. However, this flurry of activity conceals profound divisions within the AI community and a lack of concrete progress towards global governance, as governments around the world pursue conflicting approaches.</li>\n<li><strong>Challenges mount in evaluating state of the art models</strong>, as standard LLMs often struggle with robustness. Considering the stakes, as “vibes-based” approach isn’t good enough.</li>\n</ul>\n</blockquote>\n\n<p><a href=\"https://www.stateof.ai/\">State of AI Website</a></p>\n</div>\n","tags":["ai"]},{"title":"OnnxStream - Stable Diffusion XL 1.0 Base on a Raspberry Pi Zero 2","contentType":"bookmarks","date":"2023-12-11 20:11","url":"/responses/onnxstream/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/vitoplantamura/OnnxStream/\" >https://github.com/vitoplantamura/OnnxStream/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Generally major machine learning frameworks and libraries are focused on minimizing inference latency and/or maximizing throughput, all of which at the cost of RAM usage. So I decided to write a super small and hackable inference library specifically focused on minimizing memory consumption: OnnxStream.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>OnnxStream is based on the idea of decoupling the inference engine from the component responsible of providing the model weights, which is a class derived from WeightsProvider. A WeightsProvider specialization can implement any type of loading, caching and prefetching of the model parameters. For example a custom WeightsProvider can decide to download its data from an HTTP server directly, without loading or writing anything to disk (hence the word &amp;quot;Stream&amp;quot; in &amp;quot;OnnxStream&amp;quot;). Three default WeightsProviders are available: DiskNoCache, DiskPrefetch and Ram.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>OnnxStream can consume even 55x less memory than OnnxRuntime with only a 50% to 200% increase in latency (on CPU, with a good SSD, with reference to the SD 1.5's UNET - see the Performance section below).</p>\n</blockquote>\n</div>\n","tags":["ai","onnx"]},{"title":"Evaluating LLMs is a minefield","contentType":"bookmarks","date":"2023-12-11 20:08","url":"/responses/evaluating-llms-minefield/","content":"<div class=\"response-target\" >→ <a href=\"https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/\" >https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...many things can go wrong when we are trying to evaluate LLMs’ performance on a certain task or behavior in a certain scenario.</p>\n<p>It has big implications for reproducibility: both for research on LLMs and research that uses LLMs to answer a question in social science or any other field.</p>\n</blockquote>\n</div>\n","tags":["ai","llms","evaluation"]},{"title":"Vector databases: A comparison and guide for 2023","contentType":"bookmarks","date":"2023-12-11 20:05","url":"/responses/vector-db-comparison-guide-2023/","content":"<div class=\"response-target\" >→ <a href=\"https://benchmark.vectorview.ai/vectordbs.html\" >https://benchmark.vectorview.ai/vectordbs.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Picking a vector database can be hard. Scalability, latency, costs, and even compliance hinge on this choice. For those navigating this terrain, I've embarked on a journey to sieve through the noise and compare the leading vector databases of 2023. I’ve included the following vector databases in the comparision: Pinecone, Weviate, Milvus, Qdrant, Chroma, Elasticsearch and PGvector. The data behind the comparision comes from <a href=\"https://ann-benchmarks.com/\">ANN Benchmarks</a>, the docs and internal benchmarks of each vector database and from digging in open source github repos.</p>\n</blockquote>\n</div>\n","tags":["vector","databases"]},{"title":"MemGPT - Towards LLMs as Operating Systems","contentType":"bookmarks","date":"2023-12-11 19:58","url":"/responses/memgpt-llm-operating-system/","content":"<div class=\"response-target\" >→ <a href=\"https://memgpt.ai/\" >https://memgpt.ai/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Teach LLMs to manage their own memory for unbounded context!</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In MemGPT, a fixed-context LLM processor is augmented with a tiered memory system and a set of functions that allow it to manage its own memory. Main context is the (fixed-length) LLM input. MemGPT parses the LLM text ouputs at each processing cycle, and either yields control or executes a function call, which can be used to move data between main and external context. When the LLM generates a function call, it can request immediate return of execution to chain together functions. In the case of a yield, the LLM will not be run again until the next external event trigger (e.g. a user message or scheduled interrupt).</p>\n</blockquote>\n</div>\n","tags":["ai","os","llm"]},{"title":"Best Practices for LLM Evaluation of RAG Applications","contentType":"bookmarks","date":"2023-12-11 19:54","url":"/responses/best-practices-rag-application-evaluation-databricks/","content":"<div class=\"response-target\" >→ <a href=\"https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG\" >https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This blog represents the first in a series of investigations we’re running at Databricks to provide learnings on LLM evaluation.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Recently, the LLM community has been exploring the use of “LLMs as a judge” for automated evaluation with many using powerful LLMs such as GPT-4 to do the evaluation for their LLM outputs.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Using the Few Shots prompt with GPT-4 didn’t make an obvious difference in the consistency of results.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Including few examples for GPT-3.5-turbo-16k significantly improves the consistency of the scores, and makes the result usable.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...evaluation results can’t be transferred between use cases and we need to build use-case-specific benchmarks in order to properly evaluate how good a model can meet customer needs.</p>\n</blockquote>\n</div>\n","tags":["ai","rag","evaluation","llm"]},{"title":"MLflow 2.8 with LLM-as-a-judge metrics and Best Practices for LLM Evaluation of RAG Applications","contentType":"bookmarks","date":"2023-12-11 19:51","url":"/responses/mlflow-2-8-llm-as-judge-rag-evaluation/","content":"<div class=\"response-target\" >→ <a href=\"https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part?utm_source=twitter&amp;amp;amp;utm_medium=organic-social\" >https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part?utm_source=twitter&amp;amp;utm_medium=organic-social</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>LLM-as-a-judge is one promising tool in the suite of evaluation techniques necessary to measure the efficacy of LLM-based applications.</p>\n</blockquote>\n</div>\n","tags":["mlflow","ai","llm","evaluation"]},{"title":"Metadata-Curated Language-Image Pre-training (MetaCLIP) - Demystifying CLIP Data","contentType":"bookmarks","date":"2023-12-11 19:41","url":"/responses/metaclip-demistifying-clip-data/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2309.16671\" >https://arxiv.org/abs/2309.16671</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outperforms CLIP's data on multiple standard benchmarks. In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy, surpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintaining the same training budget, attains 72.4%. Our observations hold across various model sizes, exemplified by ViT-H achieving 80.5%, without any bells-and-whistles. Curation code and training data distribution on metadata is made available at this <a href=\"https://github.com/facebookresearch/MetaCLIP\">https URL</a>.</p>\n</blockquote>\n\n<p><a href=\"https://github.com/facebookresearch/MetaCLIP\">MetaCLIP</a></p>\n</div>\n","tags":["ai","opensource","clip"]},{"title":"OpenAgents: An Open Platform for Language Agents in the Wild","contentType":"bookmarks","date":"2023-12-11 19:39","url":"/responses/openagents-platform-language-agents/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2310.10634\" >https://arxiv.org/abs/2310.10634</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs). Current language agent frameworks aim to facilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level designs. We present OpenAgents, an open platform for using and hosting language agents in the wild of everyday life. OpenAgents includes three agents: (1) Data Agent for data analysis with Python/SQL and data tools; (2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web browsing. OpenAgents enables general users to interact with agent functionalities through a web user interface optimized for swift responses and common failures while offering developers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate the challenges and opportunities, aspiring to set a foundation for future research and development of real-world language agents.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","agents"]},{"title":"The Foundation Model Transparency Index","contentType":"bookmarks","date":"2023-12-11 19:37","url":"/responses/foundational-model-transparency-index-stanford/","content":"<div class=\"response-target\" >→ <a href=\"https://crfm.stanford.edu/fmti/\" >https://crfm.stanford.edu/fmti/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A comprehensive assessment of the transparency of foundation model developers</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Context</strong>. Foundation models like GPT-4 and Llama 2 are used by millions of people. While the societal impact of these models is rising, transparency is on the decline. If this trend continues, foundation models could become just as opaque as social media platforms and other previous technologies, replicating their failure modes.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Design</strong>. We introduce the Foundation Model Transparency Index to assess the transparency of foundation model developers. We design the Index around 100 transparency indicators, which codify transparency for foundation models, the resources required to build them, and their use in the AI supply chain.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>Execution</strong>. For the 2023 Index, we score 10 leading developers against our 100 indicators. This provides a snapshot of transparency across the AI ecosystem. All developers have significant room for improvement that we will aim to track in the future versions of the Index.</p>\n</blockquote>\n</div>\n","tags":["ai","opensource","transparency"]},{"title":"The New Kings of Open Source AI (Oct 2023 Recap)","contentType":"bookmarks","date":"2023-12-11 19:23","url":"/responses/new-kings-open-source-ai/","content":"<div class=\"response-target\" >→ <a href=\"https://www.latent.space/p/oct-2023\" >https://www.latent.space/p/oct-2023</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Mistral 7B, released at the tail end of Sept 2023, is both Apache 2.0 and smaller but better than Llama 2, and is now rumored to be raising $400m at $2.5b valuation from a16z.</p>\n</blockquote>\n</div>\n","tags":["ai","opensource","newsletter"]},{"title":"Mixtral of experts","contentType":"bookmarks","date":"2023-12-11 18:50","url":"/responses/mixtral-of-experts/","content":"<div class=\"response-target\" >→ <a href=\"https://mistral.ai/news/mixtral-of-experts/\" >https://mistral.ai/news/mixtral-of-experts/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.</p>\n<p>Mixtral has the following capabilities.</p>\n<ul>\n<li>It gracefully handles a context of 32k tokens.</li>\n<li>It handles English, French, Italian, German and Spanish.</li>\n<li>It shows strong performance in code generation.</li>\n<li>It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench.</li>\n</ul>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the “experts”) to process the token and combine their output additively.</p>\n<p>This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","opensource"]},{"title":"The Future of RSS is Textcasting","contentType":"bookmarks","date":"2023-12-11 18:40","url":"/responses/future-rss-textcasting/","content":"<div class=\"response-target\" >→ <a href=\"https://kottke.org/23/11/the-future-of-rss-is-textcasting-1\" >https://kottke.org/23/11/the-future-of-rss-is-textcasting-1</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Here’s the philosophy:</p>\n<ul>\n<li>The goal is interop between social media apps and the features writers need.</li>\n<li>What we’re doing: Moving documents between networked apps. We need a set of common features in order for it to work.</li>\n<li>The features are motivated by the needs of writers. Not by programmers or social media company execs.</li>\n</ul>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>It’s a proposal to build, using technologies we already have and understand very well, a very simple social media protocol that is completely agnostic about what editor you use to write your posts and what viewer you choose to read it. Writer/authors would have more control over styling, links, media enclosures, etc., and readers would have more control over how and where they consume it. It’s decentralized social media, but without the need to peer through ActivityPub or anybody else’s API and squeeze our toothpaste through its tubes.</p>\n</blockquote>\n\n<p><a href=\"http://textcasting.org/\">Textcasting.org</a></p>\n</div>\n","tags":["rss","openweb","internet","socialmedia"]},{"title":"omg.lol: an oasis on the internet","contentType":"bookmarks","date":"2023-12-11 08:51","url":"/responses/omg-lol-internet-oasis-watson/","content":"<div class=\"response-target\" >→ <a href=\"https://blakewatson.com/journal/omg-lol-an-oasis-on-the-internet/\" >https://blakewatson.com/journal/omg-lol-an-oasis-on-the-internet/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The main thing you are getting with omg.lol is one or more subdomains, which are referred to as addresses.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<ul>\n<li><strong>Email forwarding</strong>: You get an email address, you@omg.lol, which you can forward to any email address.</li>\n<li><strong>Web Page</strong>: This is your link-in-bio one-pager to do whatever you want with. By default this is where your main address (eg, you.omg.lol) points. It’s the flagship feature of omg.lol. It comes with a markdown editor that has some fancy features baked into it. You get a selection of built-in themes but you also have the freedom to go wild with your own CSS.</li>\n<li><strong>DNS</strong>: You have the ability to use your omg.lol subdomain however you wish by way of a friendly DNS panel.</li>\n<li><strong>Now Page</strong>: This is a type of page you can use to let people know what’s going on in your life. It’s broader than a social media post but more immediately relevant than an about page. It comes with the same fancy markdown editor and you can optionally appear in omg.lol’s Now Garden.</li>\n<li><strong>Statuslog</strong>: This is a place to post statuses. It’s really just a fun, silly alternative to other social media platforms but without follows and likes and such. These can cross-post to Mastodon if you want.</li>\n<li><strong>Weblog</strong>: A full-fledge blogging platform. I’m not aware of all its features but it’s pretty powerful. It comes with fancy markdown support and has all the bloggy things you need like tags and RSS. A good example of a very custom blog on omg.lol is Apple Annie’s Weblog. But it’s worth noting you use it right out of the box without design customization if you want.</li>\n<li><strong>Pastebin</strong>: It’s just a pastebin for storing text snippets. Super simple and friendly like all of the omg.lol services.</li>\n<li><strong>Pics</strong>: It’s an image hosting service labeled as being “super-beta” as of the time of this writing. But it does what it says on the tin. You can host images there and they also show up on the some.pics image feed.</li>\n<li><strong>PURLs</strong>: Persistent uniform resource locators. This is a URL redirection service. You get you.omg.lol/whatever and you.url.lol/whatever. You can use these the way you would use similar services and they come with a basic hit counter and way to preview the URL before following it.</li>\n<li><strong>Switchboard</strong>: This is a powerful routing system that lets you point the variants of your address wherever you want, be it a destination on the omg.lol platform or an external website. Most omg.lol services have their own domain so you end up with a variety of options. Just as an example, you get a tilde address (ie, omg.lol/~you). Mine points to my tilde.club webpage.</li>\n<li><strong>Keys</strong>: A place to store public keys—SSH, PGP, etc.</li>\n<li><strong>Proofs</strong>: A service for verifying ownership or control of a particular web property at a particular moment in time. For example, here is proof that I controlled blakewatson.com as of December 10, 2023.</li>\n<li><strong>API access</strong>: Most, if not all, omg.lol services have an API you can use to interact with them. Total nerd freedom. 🤯</li>\n</ul>\n</blockquote>\n</div>\n","tags":["web","community"]},{"title":"Amazing Newsletters","contentType":"bookmarks","date":"2023-12-10 10:29","url":"/responses/amazing-newsletters/","content":"<div class=\"response-target\" >→ <a href=\"https://amazingnewsletters.com/\" >https://amazingnewsletters.com/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Find the best newsletters to subscribe to!</p>\n</blockquote>\n</div>\n","tags":["newsletters"]},{"title":"To Everyone Who Asks For ‘Just A Little’ Of Your Time: Here’s What It Costs To Say Yes","contentType":"bookmarks","date":"2023-12-09 10:08","url":"/responses/what-it-cost-to-say-yes-holiday/","content":"<div class=\"response-target\" >→ <a href=\"https://thoughtcatalog.com/ryan-holiday/2017/01/to-everyone-who-asks-for-just-a-little-of-your-time/\" >https://thoughtcatalog.com/ryan-holiday/2017/01/to-everyone-who-asks-for-just-a-little-of-your-time/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Makers...need to have large blocks of uninterrupted, unscheduled time to do what they do. To create and think.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I keep a maker’s schedule because I believe that anything else is anathema to deep work or creativity.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Seneca writes that if all the geniuses in history were to get together, none would be able explain our baffling relationship with time. He says,</p>\n<blockquote class=\"blockquote\">\n<p>No person would give up even an inch of their estate, and the slightest dispute with a neighbor can mean hell to pay; yet we easily let others encroach on our lives—worse, we often pave the way for those who will take it over. No person hands out their money to passers-by, but to how many do each of us hand out our lives! We’re tight-fisted with property and money, yet think too little of wasting time, the one thing about which we should all be the toughest misers.</p>\n</blockquote>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Time? Time is our most irreplaceable asset—we cannot buy more of it. We cannot get a second of it back. We can only hope to waste as little as possible. Yet somehow we treat it as most renewable of all resources.</p>\n</blockquote>\n</div>\n","tags":["productivity","timemanagement"]},{"title":"Practical Ways To Increase Product Velocity","contentType":"bookmarks","date":"2023-12-09 10:03","url":"/responses/practical-ways-increase-product-velocity/","content":"<div class=\"response-target\" >→ <a href=\"https://staysaasy.com/management/2023/12/07/accelerating-product-velocity.html\" >https://staysaasy.com/management/2023/12/07/accelerating-product-velocity.html</a></div><div class=\"response-content\" >\n<blockquote class=\"blockquote\">\n<p>Create a culture that favors begging forgiveness (and reversing decisions quickly) rather than asking permission. Invest in infrastructure such as progressive / cancellable rollouts. Use asynchronous written docs to get people aligned (“comment in this doc by Friday if you disagree with the plan”) rather than meetings (“we’ll get approval at the next weekly review meeting”).</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>Unclear thinking is a reliable cause of slowness, and gets revealed under a microscope.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Bonus points for documenting plans in writing. One of the largest advantages of a strong writing culture is that it forces much clearer narratives than meetings, powerpoint, or five Slack threads spread over 8 business days.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>No matter what your job function is, part of your role is ensuring that your engineering team has enough time to get their vital metrics in order. Especially if you’re a product leader, it’s essential that you resist the temptation to push relentlessly for more features and give your engineering counterparts the room to get fit.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>...it’s especially important to build a strong relationship with all of your engineering partners, and especially these trusted guides.</p>\n</blockquote>\n</div>\n","tags":["product","productivity"]},{"title":"SatCLIP - A Global, General-Purpose Geographic Location Encoder","contentType":"bookmarks","date":"2023-12-08 07:40","url":"/responses/microsoft-satclip/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/microsoft/satclip\" >https://github.com/microsoft/satclip</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>SatCLIP trains location and image encoders via contrastive learning, by matching images to their corresponding locations. This is analogous to the CLIP approach, which matches images to their corresponding text. Through this process, the location encoder learns characteristics of a location, as represented by satellite imagery. For more details, check out our <a href=\"https://arxiv.org/abs/2311.17179\">paper</a>.</p>\n</blockquote>\n</div>\n","tags":["ai","opensource"]},{"title":"DOS_deck","contentType":"bookmarks","date":"2023-12-08 07:36","url":"/responses/dos-deck/","content":"<div class=\"response-target\" >→ <a href=\"https://dosdeck.com/\" >https://dosdeck.com/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>DOS_deck is built upon the foundation of JS-DOS, which, in turn, relies on DOSBox. Together, they breathe new life into MS-DOS games by bringing them to your browser. However, there's a twist. Games from that era were designed for keyboard and mouse input, without established standards for interaction or control patterns. Here at DOS_deck, a tremendous effort was put into creating a seamless experience, enabling you to effortlessly navigate and play these games, ideally with the comfort of a controller in hand.</p>\n</blockquote>\n</div>\n","tags":["gaming","dos","retro","web"]},{"title":"Awesome App Defaults","contentType":"bookmarks","date":"2023-12-06 21:53","url":"/responses/app-defaults-catalog/","content":"<div class=\"response-target\" >→ <a href=\"https://defaults.rknight.me/\" >https://defaults.rknight.me/</a></div><div class=\"response-content\" ><p>Aggregated list of App Defaults blog posts inspired by <a href=\"https://listen.hemisphericviews.com/097\">Hemispheric Views 097 - Duel of the Defaults!</a></p>\n</div>\n","tags":["community","blogging","apps"]},{"title":"RSS Club","contentType":"bookmarks","date":"2023-12-06 21:37","url":"/responses/rss-club/","content":"<div class=\"response-target\" >→ <a href=\"https://daverupert.com/rss-club/\" >https://daverupert.com/rss-club/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>RSS Club is a collection of blogs (personal and otherwise) committed to providing RSS-only content. It’s like a newsletter delivered to your feed reader in order to celebrate the medium of RSS and breakaway from social media.</p>\n</blockquote>\n</div>\n","tags":["rss","community"]},{"title":"Long context prompting for Claude 2.1","contentType":"bookmarks","date":"2023-12-06 21:07","url":"/responses/claude-long-context-prompting/","content":"<div class=\"response-target\" >→ <a href=\"https://www.anthropic.com/index/claude-2-1-prompting\" >https://www.anthropic.com/index/claude-2-1-prompting</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<ul>\n<li>Claude 2.1 recalls information very well across its 200,000 token context window</li>\n<li>However, the model can be reluctant to answer questions based on an individual sentence in a document, especially if that sentence has been injected or is out of place</li>\n<li>A minor prompting edit removes this reluctance and results in excellent performance on these tasks</li>\n</ul>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>What can users do if Claude is reluctant to respond to a long context retrieval question? We’ve found that a minor prompt update produces very different outcomes in cases where Claude is capable of giving an answer, but is hesitant to do so. When running the same evaluation internally, adding just one sentence to the prompt resulted in near complete fidelity throughout Claude 2.1’s 200K context window</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We achieved significantly better results on the same evaluation by adding the sentence “Here is the most relevant sentence in the context:” to the start of Claude’s response. This was enough to raise Claude 2.1’s score from 27% to 98% on the original evaluation.</p>\n</blockquote>\n</div>\n","tags":["prompts","ai","llm"]},{"title":"Introducing Gemini","contentType":"bookmarks","date":"2023-12-06 20:57","url":"/responses/introducing-gemini-google/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.google/technology/ai/google-gemini-ai/\" >https://blog.google/technology/ai/google-gemini-ai/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research. It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, operate across and combine different types of information including text, code, audio, image and video.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Gemini is also our most flexible model yet — able to efficiently run on everything from data centers to mobile devices. Its state-of-the-art capabilities will significantly enhance the way developers and enterprise customers build and scale with AI.</p>\n<p>We’ve optimized Gemini 1.0, our first version, for three different sizes:</p>\n<ul>\n<li>Gemini Ultra — our largest and most capable model for highly complex tasks.</li>\n<li>Gemini Pro — our best model for scaling across a wide range of tasks.</li>\n<li>Gemini Nano — our most efficient model for on-device tasks.</li>\n</ul>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We designed Gemini to be natively multimodal, pre-trained from the start on different modalities. Then we fine-tuned it with additional multimodal data to further refine its effectiveness. This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models — and its capabilities are state of the art in nearly every domain.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Our first version of Gemini can understand, explain and generate high-quality code in the world’s most popular programming languages, like Python, Java, C++, and Go.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>On TPUs, Gemini runs significantly faster than earlier, smaller and less-capable models.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Starting today, Bard will use a fine-tuned version of Gemini Pro for more advanced reasoning, planning, understanding and more.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We’re also bringing Gemini to Pixel. Pixel 8 Pro is the first smartphone engineered to run Gemini Nano, which is powering new features like Summarize in the Recorder app and rolling out in Smart Reply in Gboard, starting with WhatsApp — with more messaging apps coming next year.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Starting on December 13, developers and enterprise customers can access Gemini Pro via the Gemini API in Google AI Studio or Google Cloud Vertex AI.</p>\n</blockquote>\n</div>\n","tags":["ai","lmm"]},{"title":"Introducing llamafile","contentType":"bookmarks","date":"2023-12-05 21:54","url":"/responses/introducing-llamafile/","content":"<div class=\"response-target\" >→ <a href=\"https://hacks.mozilla.org/2023/11/introducing-llamafile/\" >https://hacks.mozilla.org/2023/11/introducing-llamafile/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today we’re announcing the first release of llamafile and inviting the open source community to participate in this new project.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>llamafile lets you turn large language model (LLM) weights into executables.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We achieved all this by combining two projects that we love: llama.cpp (a leading open source LLM chatbot framework) with Cosmopolitan Libc (an open source project that enables C programs to be compiled and run on a large number of platforms and architectures). It also required solving several interesting and juicy problems along the way, such as adding GPU and dlopen() support to Cosmopolitan;</p>\n</blockquote>\n</div>\n","tags":["ai","llm","opensource"]},{"title":"PeerTube v6 is out","contentType":"bookmarks","date":"2023-12-05 21:51","url":"/responses/peertube-v6-released/","content":"<div class=\"response-target\" >→ <a href=\"https://framablog.org/2023/11/28/peertube-v6-is-out-and-powered-by-your-ideas/\" >https://framablog.org/2023/11/28/peertube-v6-is-out-and-powered-by-your-ideas/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The sixth major version is being released today and we are very proud !</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Protect your videos with passwords !</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Video storyboard : preview what’s coming !</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Upload a new version of your video !</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Get chapters in your videos !</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Stress tests, performance and config recommandations</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>…and there’s always more !</p>\n</blockquote>\n</div>\n","tags":["fediverse","peertube","video","opensource"]},{"title":"AI and Mass Spying","contentType":"bookmarks","date":"2023-12-05 21:12","url":"/responses/ai-mass-spying-schneier/","content":"<div class=\"response-target\" >→ <a href=\"https://www.schneier.com/blog/archives/2023/12/ai-and-mass-spying.html\" >https://www.schneier.com/blog/archives/2023/12/ai-and-mass-spying.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Surveillance facilitates social control, and spying will only make this worse. Governments around the world already use mass surveillance; they will engage in mass spying as well.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Mass surveillance ushered in the era of personalized advertisements; mass spying will supercharge that industry...The tech monopolies that are currently keeping us all under constant surveillance won’t be able to resist collecting and using all of that data.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We could limit this capability. We could prohibit mass spying. We could pass strong data-privacy rules. But we haven’t done anything to limit mass surveillance. Why would spying be any different?</p>\n</blockquote>\n</div>\n","tags":["ai","privacy","internet","surveillance"]},{"title":"AI Alliance Launches","contentType":"bookmarks","date":"2023-12-05 21:03","url":"/responses/ai-alliance/","content":"<div class=\"response-target\" >→ <a href=\"https://thealliance.ai/news\" >https://thealliance.ai/news</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>AI Alliance Launches as an International Community of Leading Technology Developers, Researchers, and Adopters Collaborating Together to Advance Open, Safe, Responsible AI</p>\n</blockquote>\n</div>\n","tags":["ai","opensource"]},{"title":"You Need Feeds","contentType":"bookmarks","date":"2023-11-11 17:49","url":"/responses/you-need-feeds/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youneedfeeds.com/\" >https://www.youneedfeeds.com/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>YOU NEED FEEDS.</p>\n<p>A web feed is a special listing of the latest content from your favourite site.  News, music, video and more - whatever is new, web feeds will show you.  What's more, you can combine your favourite feeds using a feed reader application - and suddenly the whole web comes to you.</p>\n<p>You don't have to do the work of staying on top any more.  You can now visit a single site, or use a single app, and see everything that's new and interesting.  You choose the content.  You're in control.</p>\n</blockquote>\n</div>\n","tags":["rss","feeds","guides"]},{"title":"Understanding Deep Learning","contentType":"bookmarks","date":"2023-10-14 21:35","url":"/responses/understanding-deep-learning-book/","content":"<div class=\"response-target\" >→ <a href=\"https://udlbook.github.io/udlbook/\" >https://udlbook.github.io/udlbook/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The title of this book is “Understanding Deep Learning” to distinguish it from volumes that cover coding and other practical aspects. This text is primarily about the ideas that underlie deep learning. The first part of the book introduces deep learning models and discusses how to train them, measure their performance, and improve this performance. The next part considers architectures that are specialized to images, text, and graph data. These chapters require only introductory linear algebra, calculus, and probability and should be accessible to any second-year undergraduate in a quantitative discipline. Subsequent parts of the book tackle generative models and reinforcement learning. These chapters require more knowledge of probability and calculus and target more advanced students.</p>\n</blockquote>\n</div>\n","tags":["book","deeplearning","ai"]},{"title":"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)","contentType":"bookmarks","date":"2023-10-14 21:32","url":"/responses/dawn-lmms-early-explorations-gtp4v/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2309.17421\" >https://arxiv.org/abs/2309.17421</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI's innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution: <a href=\"https://cdn.openai.com/contributions/gpt-4v.pdf\">this https URL</a></p>\n</blockquote>\n</div>\n","tags":["ai","lmm"]},{"title":"Chain-of-Verification Reduces Hallucination in Large Language Models","contentType":"bookmarks","date":"2023-10-14 21:09","url":"/responses/chain-of-verification-prompting-technique/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2309.11495\" >https://arxiv.org/abs/2309.11495</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (COVE) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show COVE decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","promptengineering"]},{"title":"Multimodality and Large Multimodal Models (LMMs)","contentType":"bookmarks","date":"2023-10-14 18:51","url":"/responses/multimodality-large-multimodal-models-lmm-huyen/","content":"<div class=\"response-target\" >→ <a href=\"https://huyenchip.com/2023/10/10/multimodal.html\" >https://huyenchip.com/2023/10/10/multimodal.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This post covers multimodal systems in general, including LMMs. It consists of 3 parts.</p>\n<p>Part 1 covers the context for multimodality, including why multimodal, different data modalities, and types of multimodal tasks.<br />\nPart 2 discusses the fundamentals of a multimodal system, using the examples of CLIP, which lays the foundation for many future multimodal systems, and Flamingo, whose impressive performance gave rise to LMMs.<br />\nPart 3 discusses some active research areas for LMMs, including generating multimodal outputs and adapters for more efficient multimodal training, covering newer multimodal systems such as BLIP-2, LLaVA, LLaMA-Adapter V2, LAVIN, etc.</p>\n</blockquote>\n</div>\n","tags":["ai","lmm"]},{"title":"HuggingFace: Text Embeddings Inference","contentType":"bookmarks","date":"2023-10-14 12:54","url":"/responses/hf-text-embedding-inference/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/huggingface/text-embeddings-inference\" >https://github.com/huggingface/text-embeddings-inference</a></div><div class=\"response-content\" ><p>A blazing fast inference solution for text embeddings models.</p>\n</div>\n","tags":["ai","embeddings","text","inference","ml"]},{"title":"Scaffolded LLMs as natural language computers","contentType":"bookmarks","date":"2023-09-28 21:00","url":"/responses/scaffolded-llms-natural-language-computers/","content":"<div class=\"response-target\" >→ <a href=\"https://www.beren.io/2023-04-11-Scaffolded-LLMs-natural-language-computers/\" >https://www.beren.io/2023-04-11-Scaffolded-LLMs-natural-language-computers/</a></div><div class=\"response-content\" ><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43C3igfmMrE9Qoyfe/paczewfk3yzexgdaokto\" class=\"img-fluid\" alt=\"ReACT LLM Pattern Image\" /></p>\n<p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43C3igfmMrE9Qoyfe/sehstj9d7rfgcv6ifg2y\" class=\"img-fluid\" alt=\"Image of high-level CPU architecture\" /></p>\n</div>\n","tags":["ai","llm","cpu"]},{"title":"Creating the First Confidential GPUs","contentType":"bookmarks","date":"2023-09-28 20:59","url":"/responses/creating-first-confidential-gpus/","content":"<div class=\"response-target\" >→ <a href=\"https://queue.acm.org/detail.cfm?id=3623391\" >https://queue.acm.org/detail.cfm?id=3623391</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The team at NVIDIA brings confidentiality and integrity to user code and data for accelerated computing.</p>\n</blockquote>\n</div>\n","tags":["ai","security","privacy","gpu"]},{"title":"The AI Attack Surface Map v1.0","contentType":"bookmarks","date":"2023-09-28 20:45","url":"/responses/ai-attack-surface-map-v1/","content":"<div class=\"response-target\" >→ <a href=\"https://danielmiessler.com/p/the-ai-attack-surface-map-v1-0/\" >https://danielmiessler.com/p/the-ai-attack-surface-map-v1-0/</a></div><div class=\"response-content\" ><p><img src=\"https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/29651bf8-3dd7-4c94-90e4-0bab3398db60/ai-attack-surface-categories-miessler-1.png\" class=\"img-fluid\" alt=\"AI Surface Attack Map\" /></p>\n</div>\n","tags":["ai","security","cybersecurity"]},{"title":"Mistral 7B Model","contentType":"bookmarks","date":"2023-09-28 10:16","url":"/responses/mistral-ai/","content":"<div class=\"response-target\" >→ <a href=\"https://mistral.ai/\" >https://mistral.ai/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Mistral-7B-v0.1 is a small, yet powerful model adaptable to many use-cases. Mistral 7B is better than Llama 2 13B on all benchmarks, has natural coding abilities, and 8k sequence length. It’s released under Apache 2.0 licence. We made it easy to deploy on any cloud, and of course on your gaming GPU.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","opensource"]},{"title":"Meta announces AI experiences in Facebook, Instagram, WhatsApp","contentType":"bookmarks","date":"2023-09-28 10:12","url":"/responses/meta-announces-ai-experiences-fb-whatsapp-ig/","content":"<div class=\"response-target\" >→ <a href=\"https://about.fb.com/news/2023/09/introducing-ai-powered-assistants-characters-and-creative-tools/\" >https://about.fb.com/news/2023/09/introducing-ai-powered-assistants-characters-and-creative-tools/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<ul>\n<li>We’re starting to roll out AI stickers across our apps, and soon you’ll be able to edit your images or even co-create them with friends on Instagram using our new AI editing tools, restyle and backdrop.</li>\n<li>We’re introducing Meta AI in beta, an advanced conversational assistant that’s available on WhatsApp, Messenger, and Instagram, and is coming to Ray-Ban Meta smart glasses and Quest 3. Meta AI can give you real-time information and generate photorealistic images from your text prompts in seconds to share with friends. (Available in the US only)</li>\n<li>We’re also launching 28 more AIs in beta, with unique interests and personalities. Some are played by cultural icons and influencers, including Snoop Dogg, Tom Brady, Kendall Jenner, and Naomi Osaka.</li>\n<li>Over time, we’re making AIs for businesses and creators available, and releasing our AI studio for people and developers to build their own AIs.</li>\n<li>These new AI experiences also come with a new set of challenges for our industry. We’re rolling out our new AIs slowly and have built in safeguards.</li>\n</ul>\n</blockquote>\n</div>\n","tags":["meta","ai","facebook"]},{"title":"Introducing Raspberry Pi 5","contentType":"bookmarks","date":"2023-09-28 10:10","url":"/responses/raspberry-pi-5/","content":"<div class=\"response-target\" >→ <a href=\"https://www.raspberrypi.com/products/raspberry-pi-5/\" >https://www.raspberrypi.com/products/raspberry-pi-5/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The everything computer. Optimised.</p>\n<p>With 2–3× the speed of the previous generation, and featuring silicon designed in‑house for the best possible performance, we’ve redefined the Raspberry Pi experience.</p>\n<p>Coming October 2023</p>\n</blockquote>\n</div>\n","tags":["raspberrypi","selfhosting"]},{"title":"Carton - Run any ML model from any programming language","contentType":"bookmarks","date":"2023-09-28 10:03","url":"/responses/carton-ml-models/","content":"<div class=\"response-target\" >→ <a href=\"https://carton.run/\" >https://carton.run/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Carton makes it easy to run any ML model from any programming language.</p>\n</blockquote>\n</div>\n","tags":["ai","ml","mlops"]},{"title":"FlowiseAI - Drag and drop for LLM flows","contentType":"bookmarks","date":"2023-09-25 20:54","url":"/responses/flowiseai-visual-llm-flow/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/FlowiseAI/Flowise\" >https://github.com/FlowiseAI/Flowise</a></div><div class=\"response-content\" ><p><img src=\"https://github.com/FlowiseAI/Flowise/raw/main/images/flowise.gif\" class=\"img-fluid\" alt=\"GIF of FlowiseAI LLM visual tool\" /></p>\n</div>\n","tags":["ai","tools"]},{"title":"Why Open Source AI Will Win","contentType":"bookmarks","date":"2023-09-25 20:48","url":"/responses/why-open-source-ai-will-win-varun/","content":"<div class=\"response-target\" >→ <a href=\"https://varunshenoy.substack.com/p/why-open-source-ai-will-win\" >https://varunshenoy.substack.com/p/why-open-source-ai-will-win</a></div><div class=\"response-content\" ></div>\n","tags":["opensource","ai"]},{"title":"Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes","contentType":"bookmarks","date":"2023-09-25 20:45","url":"/responses/distilling-step-x-step-outperforming-large-models-small-data/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html\" >https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In <a href=\"https://arxiv.org/abs/2305.02301\">“Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes”</a>, presented at ACL2023, we set out to tackle this trade-off between model size and training data collection cost. We introduce distilling step-by-step, a new simple mechanism that allows us to train smaller task-specific models with much less training data than required by standard fine-tuning or distillation approaches that outperform few-shot prompted LLMs’ performance. We demonstrate that the distilling step-by-step mechanism enables a 770M parameter T5 model to outperform the few-shot prompted 540B PaLM model using only 80% of examples in a benchmark dataset, which demonstrates a more than 700x model size reduction with much less training data required by standard approaches.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","slm","optimization"]},{"title":"How I rewired my brain in six weeks","contentType":"bookmarks","date":"2023-09-25 20:42","url":"/responses/rewired-brain-six-weeks-bbc/","content":"<div class=\"response-target\" >→ <a href=\"https://www.bbc.com/future/article/20230912-how-i-hacked-my-brain\" >https://www.bbc.com/future/article/20230912-how-i-hacked-my-brain</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>There is growing evidence that simple, everyday changes to our lives can alter our brains and change how they work. Melissa Hogenboom put herself into a scanner to find out.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I was surprised that something as simple as mindfulness can play such a crucial role in keeping our minds healthy. Research has shown that mindfulness is a simple but powerful way to enhance several cognitive functions. It can improve attention, relieve pain and reduce stress. Research has found that after only a few months of mindfulness training, certain depression and anxiety symptoms can ease – though as with any complex mental health problem, this may of course vary depending on individual circumstances.</p>\n</blockquote>\n</div>\n","tags":["neuroscience","habits","mindfulness","cognition"]},{"title":"vim + llm = 🔥","contentType":"bookmarks","date":"2023-09-25 20:39","url":"/responses/vim-llm-hacks-thacker/","content":"<div class=\"response-target\" >→ <a href=\"http://josephthacker.com/ai/2023/09/18/vim-llm-hacks.html\" >http://josephthacker.com/ai/2023/09/18/vim-llm-hacks.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>... I learned that you could be inside vim, but manipulate the entire file as if you were piping the contents of the file into a command. The output of the command does in-line replacement of the entire file with those changes. That sounds confusing, but it just means you can be inside a vim file and do :%!grep test and it’ll remove all lines that don’t contain test, for example.</p>\n<p>This post is a simple showcase of taking that concept, but throwing an llm into the mix to add more dynamic functionality.</p>\n</blockquote>\n</div>\n","tags":["vi","linux","llm","ai","softwaredevelopment"]},{"title":"Next-Gen CPU Acceleration: AVX For Generative AI","contentType":"bookmarks","date":"2023-09-25 20:34","url":"/responses/next-gen-cpu-acceleration-avx-gen-ai/","content":"<div class=\"response-target\" >→ <a href=\"https://www.youtube.com/watch?v=bskEGP0r3hE\" >https://www.youtube.com/watch?v=bskEGP0r3hE</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The future is AVX10, so says Intel. Recently a document was released showcasing a post-AVX512 world, and to explain why this matters, I've again invited the Chips And Cheese crew onto the channel. Chester and George answer my questions on AVX10 and why it matters! Visit <a href=\"http://www.chipsandcheese.com\">http://www.chipsandcheese.com</a> to learn more!</p>\n</blockquote>\n<p><a href=\"https://www.youtube.com/watch?v=bskEGP0r3hE\" title=\"YouTube video Next-Gen CPU Acceleration: AVX For Generative AI\"><img src=\"http://img.youtube.com/vi/bskEGP0r3hE/0.jpg\" class=\"img-fluid\" alt=\"YouTube video Next-Gen CPU Acceleration: AVX For Generative AI\" /></a></p>\n</div>\n","tags":["ai","hardware","hpc"]},{"title":"ChatGPT can now see, hear, and speak","contentType":"bookmarks","date":"2023-09-25 10:12","url":"/responses/chatgpt-voice-image-capabilities/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/blog/chatgpt-can-now-see-hear-and-speak\" >https://openai.com/blog/chatgpt-can-now-see-hear-and-speak</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We are beginning to roll out new voice and image capabilities in ChatGPT. They offer a new, more intuitive type of interface by allowing you to have a voice conversation or show ChatGPT what you’re talking about.</p>\n</blockquote>\n</div>\n","tags":["ai","chatgpt","openai","llm","machinelearning","ml"]},{"title":"Amazon and Anthropic announce strategic collaboration to advance generative AI ","contentType":"bookmarks","date":"2023-09-25 10:11","url":"/responses/aws-anthropic-partnership/","content":"<div class=\"response-target\" >→ <a href=\"https://www.aboutamazon.com/news/company-news/amazon-aws-anthropic-ai\" >https://www.aboutamazon.com/news/company-news/amazon-aws-anthropic-ai</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Anthropic selects AWS as its primary cloud provider and will train and deploy its future foundation models on AWS Trainium and Inferentia chips, taking advantage of AWS’s high-performance, low-cost machine learning accelerators.</p>\n</blockquote>\n</div>\n","tags":["ai","cloud"]},{"title":"DALL·E 3","contentType":"bookmarks","date":"2023-09-22 10:18","url":"/responses/dall-e-3/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/dall-e-3\" >https://openai.com/dall-e-3</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>DALL·E 3 understands significantly more nuance and detail than our previous systems, allowing you to easily translate your ideas into exceptionally accurate images.</p>\n</blockquote>\n</div>\n","tags":["dalle","ai","openai","images","llm","ml"]},{"title":"Matrix 2.0: The Future of Matrix","contentType":"bookmarks","date":"2023-09-22 10:10","url":"/responses/matrix-2-0/","content":"<div class=\"response-target\" >→ <a href=\"https://matrix.org/blog/2023/09/matrix-2-0/\" >https://matrix.org/blog/2023/09/matrix-2-0/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>TL;DR: If you want to play with a shiny new Matrix 2.0 client, head over to <a href=\"https://element.io/blog/element-x-ignition/\">Element X</a>.</p>\n</blockquote>\n</div>\n","tags":["matrix","messaging","communication","opensource","protocols","decentralization","openweb","web","internet"]},{"title":"Optimizing your LLM in production ","contentType":"bookmarks","date":"2023-09-17 12:58","url":"/responses/optimizing-llm-production-hf/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/blog/optimize-llm\" >https://huggingface.co/blog/optimize-llm</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In this blog post, we will go over the most effective techniques at the time of writing this blog post to tackle these challenges for efficient LLM deployment:</p>\n<ol>\n<li><strong>Lower Precision:</strong> Research has shown that operating at reduced numerical precision, namely 8-bit and 4-bit, can achieve computational advantages without a considerable decline in model performance.</li>\n</ol>\n<p><strong>2. Flash Attention:</strong> Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.</p>\n<p><strong>3. Architectural Innovations:</strong> Considering that LLMs are always deployed in the same way during inference, namely autoregressive text generation with a long input context, specialized model architectures have been proposed that allow for more efficient inference. The most important advancement in model architectures hereby are Alibi, Rotary embeddings, Multi-Query Attention (MQA) and Grouped-Query-Attention (GQA).</p>\n</blockquote>\n</div>\n","tags":["ai","llm","production","engineering","software","mlops","aiops","opensource"]},{"title":"Software²","contentType":"bookmarks","date":"2023-09-17 12:54","url":"/responses/software-squared-jiang/","content":"<div class=\"response-target\" >→ <a href=\"https://blog.minch.co/2022/11/15/software-squared.html\" >https://blog.minch.co/2022/11/15/software-squared.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A new generation of AIs that become increasingly general by producing their own training data</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We are currently at the cusp of transitioning from “learning from data” to “learning what data to learn from” as the central focus of AI research.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>If deep learning can be described as “Software 2.0”—software that programs itself based on example inputs/output pairs, then this promising, data-centric paradigm, in which software effectively improves itself by searching for its own training data, can be described as a kind of “Software²”. This paradigm inherits the benefits of Software 2.0 while improving on its core, data-bound weaknesses: While deep learning (Software 2.0) requires the programmer to manually provide training data for each new task, Software² recasts data as software that models or searches the world to produce its own, potentially unlimited, training tasks and data.</p>\n</blockquote>\n</div>\n","tags":["ai","software","data"]},{"title":"PointLLM: Empowering Large Language Models to Understand Point Clouds","contentType":"bookmarks","date":"2023-09-17 12:52","url":"/responses/point-llm-point-clouds/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/OpenRobotLab/PointLLM\" >https://github.com/OpenRobotLab/PointLLM</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We introduce PointLLM, a multi-modal large language model capable of understanding colored point clouds of objects. It perceives object types, geometric structures, and appearance without concerns for ambiguous depth, occlusion, or viewpoint dependency. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establish two benchmarks: Generative 3D Object Classification and 3D Object Captioning, assessed through three different evaluation methods.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","generativeai","multimodal","research"]},{"title":"How consumers are using Generative AI","contentType":"bookmarks","date":"2023-09-17 12:40","url":"/responses/a16z-how-consumers-using-generative-ai/","content":"<div class=\"response-target\" >→ <a href=\"https://a16z.com/how-are-consumers-using-generative-ai/\" >https://a16z.com/how-are-consumers-using-generative-ai/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p><strong>1. Most leading products are built from the “ground up” around generative AI</strong><br />\nLike ChatGPT, the majority of products on this list didn’t exist a year ago—80% of these websites are new\nOf the 50 companies on the list, only 5 are products of, or acquisitions by, pre-existing big tech companies...\nOf the remaining list members, a whopping 48% are completely bootstrapped, with no outside funding, according to PitchBook data.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>2. ChatGPT has a massive lead, for now…</strong><br />\nChatGPT represents 60% of monthly traffic to the entire top 50 list, with an estimated 1.6 billion monthly visits and 200 million monthly users (as of June 2023). This makes ChatGPT the 24th most visited website globally.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>3. LLM assistants (like ChatGPT) are dominant, but companionship and creative tools are on the rise</strong><br />\nGeneral LLM chatbots represent 68% of total consumer traffic to the top 50 list.\nHowever, two other categories have started to drive significant usage in recent months—AI companions (such as CharacterAI) and content generation tools (such as Midjourney and ElevenLabs). Within the broader content generation category, image generation is the top use case with 41% of traffic, followed by prosumer writing tools at 26%, and video generation at 8%.\nAnother category worth mentioning? Model hubs. There are only 2 on the list, but they drive significant traffic—Civitai (for images) and Hugging Face both rank in the top 10. This is especially impressive because consumers are typically visiting these sites to download models to run locally, so web traffic is likely an underestimate of actual usage.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>4. Early “winners” have emerged, but most product categories are up for grabs</strong><br />\nGood news for builders: despite the surge in interest in generative AI, in many categories there is not yet a runway success.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>5. Acquisition for top products is entirely organic—and consumers are willing to pay!</strong><br />\nThe majority of companies on this list have no paid marketing (at least, that SimilarWeb is able to attribute). There is significant free traffic “available” via X, Reddit, Discord, and email, as well as word of mouth and referral growth.\nAnd consumers are willing to pay for GenAI. 90% of companies on the list are already monetizing, nearly all of them via a subscription model. The average product on the list makes $21/month (for users on monthly plans)—yielding $252 annually.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p><strong>6. Mobile apps are still emerging as a GenAI platform</strong><br />\nConsumer AI products have, thus far, been largely browser-first, rather than app-first. Even ChatGPT took 6 months to launch a mobile app!\nWhy aren’t more AI companies building on mobile? The browser is a natural starting place to reach the broadest base of consumers. Many AI companies have small teams and likely don’t want to fragment their focus and resources across Web, iOS, and Android.\nGiven that the average consumer now spends 36 minutes more per day on mobile than desktop (4.1 hours vs. 3.5 hours), we expect to see more mobile-first GenAI products emerge as the technology matures.</p>\n</blockquote>\n</div>\n","tags":["ai","generativeai","product","business","venturecapital","vc"]},{"title":"Coqui 🐸 XTTS","contentType":"bookmarks","date":"2023-09-17 12:25","url":"/responses/coqui-xtts-text-to-speech/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/spaces/coqui/xtts\" >https://huggingface.co/spaces/coqui/xtts</a></div><div class=\"response-content\" ><p><a href=\"https://huggingface.co/coqui/XTTS-v1\">XTTS</a> is a Voice generation model that lets you clone voices into different languages by using just a quick 3-second audio clip.</p>\n<p>XTTS is built on previous research, like Tortoise, with additional architectural innovations and training to make cross-language voice cloning and multilingual speech generation possible.</p>\n</div>\n","tags":["ai","generativeai","speech","huggingface","ml","tts","opensource"]},{"title":"Introducing Würstchen: Fast Diffusion for Image Generation ","contentType":"bookmarks","date":"2023-09-17 12:15","url":"/responses/wurstchen-fast-image-generation/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/blog/wuerstchen\" >https://huggingface.co/blog/wuerstchen</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Würstchen is a diffusion model, whose text-conditional component works in a highly compressed latent space of images. Why is this important? Compressing data can reduce computational costs for both training and inference by orders of magnitude. Training on 1024×1024 images is way more expensive than training on 32×32. Usually, other works make use of a relatively small compression, in the range of 4x - 8x spatial compression. Würstchen takes this to an extreme. Through its novel design, it achieves a 42x spatial compression! This had never been seen before, because common methods fail to faithfully reconstruct detailed images after 16x spatial compression. Würstchen employs a two-stage compression, what we call Stage A and Stage B. Stage A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found in the  paper). Together Stage A and B are called the Decoder, because they decode the compressed images back into pixel space. A third model, Stage C, is learned in that highly compressed latent space. This training requires fractions of the compute used for current top-performing models, while also allowing cheaper and faster inference. We refer to Stage C as the Prior.</p>\n</blockquote>\n</div>\n","tags":["ai","generativeai","imagegeneration","huggingface","diffusion","ml"]},{"title":"Efficient Controllable Generation for SDXL with T2I-Adapters ","contentType":"bookmarks","date":"2023-09-17 12:11","url":"/responses/t2i-adapters-efficient-controllable-generation/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/blog/t2i-sdxl-adapters\" >https://huggingface.co/blog/t2i-sdxl-adapters</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p><a href=\"https://huggingface.co/papers/2302.08453\">T2I-Adapter</a> is an efficient plug-and-play model that provides extra guidance to pre-trained text-to-image models while freezing the original large text-to-image models. T2I-Adapter aligns internal knowledge in T2I models with external control signals. We can train various adapters according to different conditions and achieve rich control and editing effects.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Over the past few weeks, the Diffusers team and the T2I-Adapter authors have been collaborating to bring the support of T2I-Adapters for Stable Diffusion XL (SDXL) in diffusers. In this blog post, we share our findings from training T2I-Adapters on SDXL from scratch, some appealing results, and, of course, the T2I-Adapter checkpoints on various conditionings (sketch, canny, lineart, depth, and openpose)!</p>\n</blockquote>\n</div>\n","tags":["ai","generativeai","imagegeneration","deeplearning","ml","huggingface"]},{"title":"Spread Your Wings: Falcon 180B is here ","contentType":"bookmarks","date":"2023-09-12 10:35","url":"/responses/falcon-180b-announcement/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/blog/falcon-180b\" >https://huggingface.co/blog/falcon-180b</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, we're excited to welcome TII's Falcon 180B to HuggingFace! Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII's RefinedWeb dataset. This represents the longest single-epoch pretraining for an open model.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","opensource","huggingface"]},{"title":"The history and legacy of Visual Basic","contentType":"bookmarks","date":"2023-09-12 10:33","url":"/responses/visual-basic-history-retool/","content":"<div class=\"response-target\" >→ <a href=\"https://retool.com/visual-basic/\" >https://retool.com/visual-basic/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>How Visual Basic became the world's most dominant programming environment, its sudden fall from grace, and why its influence is still shaping the future of software development.</p>\n</blockquote>\n</div>\n","tags":["visualbasic","windows"]},{"title":"Modular: Mojo🔥 - It’s finally here!","contentType":"bookmarks","date":"2023-09-10 10:37","url":"/responses/mojo-available-local-download/","content":"<div class=\"response-target\" >→ <a href=\"https://www.modular.com/blog/mojo-its-finally-here\" >https://www.modular.com/blog/mojo-its-finally-here</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today, we’re excited to announce the next big step in Mojo’s evolution: Mojo is now available for <a href=\"https://developer.modular.com/\">local download</a> – beginning with Linux systems, and adding Mac and Windows in coming releases.</p>\n</blockquote>\n</div>\n","tags":["ai","python","programminglanguages"]},{"title":"textual-web","contentType":"bookmarks","date":"2023-09-10 10:35","url":"/responses/textualize-textual-web/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/Textualize/textual-web\" >https://github.com/Textualize/textual-web</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Textual Web publishes Textual apps and terminals on the web.</p>\n</blockquote>\n</div>\n","tags":["tui","terminal","web"]},{"title":"Dread Central - The Overlook Motel","contentType":"bookmarks","date":"2023-09-08 22:30","url":"/responses/dread-central-overlook-motel/","content":"<div class=\"response-target\" >→ <a href=\"https://www.dreadcentral.com/the-overlook-motel/\" >https://www.dreadcentral.com/the-overlook-motel/</a></div><div class=\"response-content\" ><p>I'm a fan of finding hidden gems and overlooked films, especially when it comes to horror. It's how I came across some of my favorites like Hell House and Terrifier. That's why I was excited to run into The Overlook Motel series from Dread Central which spotlights these kinds of films.</p>\n<p><a href=\"https://www.youtube.com/watch?v=erM90Tf5wIo&amp;amp;list=PLh5in8jMKw0Q9LbTqEothBHBnJZXWSnLI\" title=\"Overlook Motel Dread Central Playlist\"><img src=\"http://img.youtube.com/vi/erM90Tf5wIo/0.jpg\" class=\"img-fluid\" alt=\"Overlook Motel Dread Central Playlist\" /></a></p>\n</div>\n","tags":["movies","horror"]},{"title":"Set up Your Onion Service","contentType":"bookmarks","date":"2023-09-06 00:14","url":"/responses/set-up-onion-service/","content":"<div class=\"response-target\" >→ <a href=\"https://community.torproject.org/onion-services/setup/\" >https://community.torproject.org/onion-services/setup/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This guide shows you how to set up an Onion Service for your website. For the technical details of how the Onion Service protocol works, see our Onion Service protocol page.</p>\n</blockquote>\n</div>\n","tags":["tor","internet","privacy","guide"]},{"title":"Cory Doctorow: Interoperability Can Save the Open Web","contentType":"bookmarks","date":"2023-09-06 00:09","url":"/responses/interoperability-can-save-open-web-doctorow/","content":"<div class=\"response-target\" >→ <a href=\"https://spectrum.ieee.org/doctorow-interoperability\" >https://spectrum.ieee.org/doctorow-interoperability</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In his new book <a href=\"https://www.versobooks.com/products/3035-the-internet-con\">The Internet Con: How to Seize the Means of Computation</a>, author <a href=\"https://www.eff.org/about/staff/cory-doctorow\">Cory Doctorow</a> presents a strong case for disrupting Big Tech. While the dominance of Internet platforms like Twitter, Facebook, Instagram, or Amazon is often taken for granted, Doctorow argues that these walled gardens are fenced in by legal structures, not feats of engineering. Doctorow proposes forcing interoperability—any given platform’s ability to interact with another—as a way to break down those walls and to make the Internet freer and more democratic.</p>\n</blockquote>\n</div>\n","tags":["openweb","interoperability","internet","opensource"]},{"title":"Rethinking trust in direct messages in the AI era","contentType":"bookmarks","date":"2023-09-06 00:05","url":"/responses/rethinking-trust-dm-ai-era/","content":"<div class=\"response-target\" >→ <a href=\"https://www.microsoft.com/en-us/research/blog/rethinking-trust-in-direct-messages-in-the-ai-era/\" >https://www.microsoft.com/en-us/research/blog/rethinking-trust-in-direct-messages-in-the-ai-era/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This blog post is a part of a series exploring our research in privacy, security, and cryptography. For the previous post, see <a href=\"https://www.microsoft.com/en-us/research/blog/research-trends-in-privacy-security-and-cryptography\">https://www.microsoft.com/en-us/research/blog/research-trends-in-privacy-security-and-cryptography</a>. While AI has the potential to massively increase productivity, this power can be used equally well for malicious purposes, for example, to automate the creation of sophisticated scam messages. In this post, we explore threats AI can pose for online communication ecosystems and outline a high-level approach to mitigating these threats.</p>\n</blockquote>\n</div>\n","tags":["security","ai","privacy","cryptography","communication","messaging","socialmedia","trust"]},{"title":"Perplexity: Interactive LLM visualization","contentType":"bookmarks","date":"2023-09-06 00:02","url":"/responses/perplexity-interactive-llm-visualizations/","content":"<div class=\"response-target\" >→ <a href=\"https://perplexity.vercel.app/\" >https://perplexity.vercel.app/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>I built this little tool to help me understand what it's like to be an autoregressive language model. For any given passage of text, it augments the original text with highlights and annotations that tell me how &amp;quot;surprising&amp;quot; each token is to the model, and which other tokens the model thought were most likely to occur in its place. Right now, the LM I'm using is the smallest version of GPT-2, with 124M parameters.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","visualization","tools"]},{"title":"Can LLMs learn from a single example?","contentType":"bookmarks","date":"2023-09-05 23:59","url":"/responses/can-llms-learn-single-example/","content":"<div class=\"response-target\" >→ <a href=\"https://www.fast.ai/posts/2023-09-04-learning-jumps/\" >https://www.fast.ai/posts/2023-09-04-learning-jumps/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Summary: recently while fine-tuning a large language model (LLM) on multiple-choice science exam questions, we observed some highly unusual training loss curves. In particular, it appeared the model was able to rapidly memorize examples from the dataset after seeing them just once. This astonishing feat contradicts most prior wisdom about neural network sample efficiency. Intrigued by this result, we conducted a series of experiments to validate and better understand this phenomenon. It’s early days, but the experiments support the hypothesis that the models are able to rapidly remember inputs. This might mean we have to re-think how we train and use LLMs.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","finetuning","fastai"]},{"title":"Teaching with AI","contentType":"bookmarks","date":"2023-09-01 18:32","url":"/responses/teching-with-ai-openai/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/blog/teaching-with-ai\" >https://openai.com/blog/teaching-with-ai</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We’re releasing a guide for teachers using ChatGPT in their classroom—including suggested prompts, an explanation of how ChatGPT works and its limitations, the efficacy of AI detectors, and bias.</p>\n</blockquote>\n</div>\n","tags":["ai","education","openai","guide"]},{"title":"Llama 2 7B/13B are now available in Web LLM","contentType":"bookmarks","date":"2023-08-31 18:57","url":"/responses/web-llm-llama-2/","content":"<div class=\"response-target\" >→ <a href=\"https://webllm.mlc.ai/\" >https://webllm.mlc.ai/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Llama 2 7B/13B are now available in Web LLM!</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Llama 2 70B is also supported.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This project brings large-language model and LLM-based chatbot to web browsers. Everything runs inside the browser with no server support and accelerated with WebGPU. This opens up a lot of fun opportunities to build AI assistants for everyone and enable privacy while enjoying GPU acceleration.</p>\n</blockquote>\n</div>\n","tags":["ai","webgpu","llama","llm"]},{"title":"Making Large Language Models Work For You","contentType":"bookmarks","date":"2023-08-29 19:57","url":"/responses/making-llms-work-for-you-willison/","content":"<div class=\"response-target\" >→ <a href=\"https://simonwillison.net/2023/Aug/27/wordcamp-llms/\" >https://simonwillison.net/2023/Aug/27/wordcamp-llms/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>My goal today is to provide practical, actionable advice for getting the most out of Large Language Models—both for personal productivity but also as a platform that you can use to build things that you couldn’t build before.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","wordpress","presentation"]},{"title":"Against Waldenponding","contentType":"bookmarks","date":"2023-08-24 21:43","url":"/responses/against-waldenponding-rao/","content":"<div class=\"response-target\" >→ <a href=\"https://studio.ribbonfarm.com/p/against-waldenponding\" >https://studio.ribbonfarm.com/p/against-waldenponding</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Waldenponding (after Thoreau's Walden Pond experiment on which Walden is based). The crude caricature is &amp;quot;smash your smart phone and go live in a log cabin to reclaim your attention and your life from being hacked by evil social media platforms.&amp;quot;</p>\n</blockquote>\n</div>\n","tags":["internet","society","technology"]},{"title":"Increasing the surface area of blogging","contentType":"bookmarks","date":"2023-08-24 21:25","url":"/responses/increasing-surface-area-blogging-critchlow/","content":"<div class=\"response-target\" >→ <a href=\"https://tomcritchlow.com/2022/04/21/new-rss/\" >https://tomcritchlow.com/2022/04/21/new-rss/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>RSS is kind of an invisible technology. People call RSS dead because you can’t see it. There’s no feed, no login, no analytics. RSS feels subsurface.</p>\n<p>Come to think of it - all the interesting bits of blogging are invisible. The discussion has moved to Twitter, or discords, or DMs. Trackbacks aren’t a thing anymore. So when you see someone blogging all you see is the blog post. The branching replies and conversations are either invisible or hard to track down.</p>\n<p>But I believe we’re living in a golden age of RSS. Blogging is booming. My feed reader has 280 feeds in it.</p>\n<p>How do we increase the surface area of RSS and blogging?</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>I think there’s something quietly radical about making your feed reader open by default. It increases the surface area of RSS so others can discover content more easily. It makes blogging more visible.</p>\n<p>The nice thing about RSS and OPML is that’s a very extensible spec. The file format is flexible, you can define your own schema and fields. This might open up new kinds of publishing.</p>\n</blockquote>\n</div>\n","tags":["rss","internet","blog","postcast","feeds","openweb","openstandards"]},{"title":"About Feeds","contentType":"bookmarks","date":"2023-08-24 21:21","url":"/responses/about-feeds/","content":"<div class=\"response-target\" >→ <a href=\"https://aboutfeeds.com/\" >https://aboutfeeds.com/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>About Feeds is a free site from Matt Webb. I made this site because using web feeds for the first time is hard, and we can fix that.</p>\n</blockquote>\n</div>\n","tags":["rss","internet","blog","openweb","openstandards"]},{"title":"Consciousness is a Big Suitcase","contentType":"bookmarks","date":"2023-08-24 21:01","url":"/responses/consciousness-big-suitcase-minsky/","content":"<div class=\"response-target\" >→ <a href=\"https://www.edge.org/conversation/marvin_minsky-consciousness-is-a-big-suitcase\" >https://www.edge.org/conversation/marvin_minsky-consciousness-is-a-big-suitcase</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Marvin Minsky is the leading light of AI—artificial intelligence, that is. He sees the brain as a myriad of structures. Scientists who, like Minsky, take the strong AI view believe that a computer model of the brain will be able to explain what we know of the brain's cognitive abilities. Minsky identifies consciousness with high-level, abstract thought, and believes that in principle machines can do everything a conscious human being can do.</p>\n</blockquote>\n</div>\n","tags":["ai","consciousness","computerscience"]},{"title":"SerenityOS","contentType":"bookmarks","date":"2023-08-24 18:57","url":"/responses/serenity-os/","content":"<div class=\"response-target\" >→ <a href=\"https://serenityos.org/\" >https://serenityos.org/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A graphical Unix-like operating system for desktop computers!</p>\n<p>SerenityOS is a love letter to '90s user interfaces with a custom Unix-like core. It flatters with sincerity by stealing beautiful ideas from various other systems.</p>\n<p>Roughly speaking, the goal is a marriage between the aesthetic of late-1990s productivity software and the power-user accessibility of late-2000s *nix.</p>\n<p>This is a system by us, for us, based on the things we like.</p>\n</blockquote>\n</div>\n","tags":["os","linux","90s","serenityos","retro"]},{"title":"Introducing Code Llama, a state-of-the-art large language model for coding","contentType":"bookmarks","date":"2023-08-24 12:43","url":"/responses/meta-code-llama/","content":"<div class=\"response-target\" >→ <a href=\"https://ai.meta.com/blog/code-llama-large-language-model-coding/\" >https://ai.meta.com/blog/code-llama-large-language-model-coding/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Takeaways</p>\n<ul>\n<li>Code Llama is a state-of-the-art LLM capable of generating code, and natural language about code, from both code and natural language prompts.</li>\n<li>Code Llama is free for research and commercial use.</li>\n<li>Code Llama is built on top of Llama 2 and is available in three models:\n<ul>\n<li>Code Llama, the foundational code model;</li>\n<li>Codel Llama - Python specialized for Python;</li>\n<li>and Code Llama - Instruct, which is fine-tuned for understanding natural language instructions.</li>\n</ul>\n</li>\n<li>In our own benchmark testing, Code Llama outperformed state-of-the-art publicly available LLMs on code tasks</li>\n</ul>\n</blockquote>\n</div>\n","tags":["ai","llm","llama","code","opensource","meta"]},{"title":"Announcing Python in Excel","contentType":"bookmarks","date":"2023-08-22 17:38","url":"/responses/announcing-python-in-excel/","content":"<div class=\"response-target\" >→ <a href=\"https://techcommunity.microsoft.com/t5/excel-blog/announcing-python-in-excel-combining-the-power-of-python-and-the/ba-p/3893439\" >https://techcommunity.microsoft.com/t5/excel-blog/announcing-python-in-excel-combining-the-power-of-python-and-the/ba-p/3893439</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today we’re announcing a significant evolution in the analytical capabilities available within Excel by releasing a Public Preview of Python in Excel. Python in Excel makes it possible to natively combine Python and Excel analytics within the same workbook - with no setup required. With Python in Excel, you can type Python directly into a cell, the Python calculations run in the Microsoft Cloud, and your results are returned to the worksheet, including plots and visualizations.</p>\n</blockquote>\n</div>\n","tags":["python","excel","dataanalytics","ai"]},{"title":"Large Language Models with Semantic Search","contentType":"bookmarks","date":"2023-08-16 22:43","url":"/responses/llm-semantic-search-deeplearningai-course/","content":"<div class=\"response-target\" >→ <a href=\"https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/\" >https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Keyword search has been a common method for search for many years. But for content-rich websites like news media sites or online shopping platforms, the keyword search capability can be limiting. Incorporating large language models (LLMs) into your search can significantly enhance the user experience by allowing them to ask questions and find information in a much easier way.</p>\n<p>This course teaches the techniques needed to leverage LLMs into search.</p>\n<p>Throughout the lessons, you’ll explore key concepts like dense retrieval, which elevates the relevance of retrieved information, leading to improved search results beyond traditional keyword search, and reranking, which injects the intelligence of LLMs into your search system, making it faster and more effective.</p>\n</blockquote>\n</div>\n","tags":["ai","llm","course","retrievalaugmentedgeneration"]},{"title":"Patterns for Building LLM-based Systems and Products","contentType":"bookmarks","date":"2023-08-16 22:20","url":"/responses/patterns-for-building-llm-systems-products-yan/","content":"<div class=\"response-target\" >→ <a href=\"https://eugeneyan.com/writing/llm-patterns/\" >https://eugeneyan.com/writing/llm-patterns/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>There are seven key patterns. They’re also organized along the spectrum of improving performance vs. reducing cost/risk, and closer to the data vs. closer to the user.</p>\n<ul>\n<li>Evals: To measure performance</li>\n<li>RAG: To add recent, external knowledge</li>\n<li>Fine-tuning: To get better at specific tasks</li>\n<li>Caching: To reduce latency &amp;amp; cost</li>\n<li>Guardrails: To ensure output quality</li>\n<li>Defensive UX: To anticipate &amp;amp; manage errors gracefully</li>\n<li>Collect user feedback: To build our data flywheel</li>\n</ul>\n</blockquote>\n<p>Addendum: <a href=\"https://eugeneyan.com/writing/llm-problems/\">how to match these LLM patterns to potential problems</a></p>\n</div>\n","tags":["ai","llm","designpatterns"]},{"title":"HuggingFace Candle","contentType":"bookmarks","date":"2023-08-16 21:54","url":"/responses/huggingface-candle/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/huggingface/candle\" >https://github.com/huggingface/candle</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) and ease of use.</p>\n</blockquote>\n</div>\n","tags":["ai","machinelearning","opensource","huggingface"]},{"title":"Open challenges in LLM research","contentType":"bookmarks","date":"2023-08-16 21:47","url":"/responses/open-challenges-llm-research-huyen/","content":"<div class=\"response-target\" >→ <a href=\"https://huyenchip.com/2023/08/16/llm-research-open-challenges.html\" >https://huyenchip.com/2023/08/16/llm-research-open-challenges.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<ol>\n<li>Reduce and measure hallucinations</li>\n<li>Optimize context length and context construction</li>\n<li>Incorporate other data modalities</li>\n<li>Make LLMs faster and cheaper</li>\n<li>Design a new model architecture</li>\n<li>Develop GPU alternatives</li>\n<li>Make agents usable</li>\n<li>Improve learning from human preference</li>\n<li>Improve the efficiency of the chat interface</li>\n<li>Build LLMs for non-English languages</li>\n</ol>\n</blockquote>\n</div>\n","tags":["ai","llm","research"]},{"title":"Jupyter AI Brings Generative AI to Notebooks","contentType":"bookmarks","date":"2023-08-16 21:43","url":"/responses/jupyter-ai-notebooks/","content":"<div class=\"response-target\" >→ <a href=\"https://www.infoq.com/news/2023/08/jupyter-ai-notebooks/\" >https://www.infoq.com/news/2023/08/jupyter-ai-notebooks/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The open-source Project Jupyter, used by millions for data science and machine learning, has released Jupyter AI, a free tool bringing powerful generative AI capabilities to Jupyter notebooks.</p>\n</blockquote>\n<p><a href=\"https://jupyter-ai.readthedocs.io/en/latest/\">https://jupyter-ai.readthedocs.io/en/latest/</a></p>\n<blockquote class=\"blockquote\">\n<p>Jupyter AI, which brings generative AI to Jupyter. Jupyter AI provides a user-friendly and powerful way to explore generative AI models in notebooks and improve your productivity in JupyterLab and the Jupyter Notebook. More specifically, Jupyter AI offers:</p>\n<ul>\n<li><p>An %%ai magic that turns the Jupyter notebook into a reproducible generative AI playground. This works anywhere the IPython kernel runs (JupyterLab, Jupyter Notebook, Google Colab, VSCode, etc.).</p>\n</li>\n<li><p>A native chat UI in JupyterLab that enables you to work with generative AI as a conversational assistant.</p>\n</li>\n<li><p>Support for a wide range of generative model providers and models (AI21, Anthropic, Cohere, Hugging Face, OpenAI, SageMaker, etc.).</p>\n</li>\n</ul>\n</blockquote>\n</div>\n","tags":["ai","notebooks","development"]},{"title":"Tildeverse","contentType":"bookmarks","date":"2023-07-12 22:25","url":"/responses/tildeverse/","content":"<div class=\"response-target\" >→ <a href=\"https://tildeverse.org/\" >https://tildeverse.org/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>a loose association of like-minded tilde communities.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>tildes are pubnixes in the spirit of <a href=\"http://tilde.club/\">tilde.club</a>, which was created in 2014 by <a href=\"https://medium.com/message/tilde-club-i-had-a-couple-drinks-and-woke-up-with-1-000-nerds-a8904f0a2ebf\">paul ford</a>.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Public Access UNIX Systems (PAUS) are a type of server that provide various services to a multi-user community.  They first began in the early 1980's and continue today. Early servers ran various flavors of UNIX, hence the name Public Access &amp;quot;UNIX&amp;quot; Systems, but later generations saw a large mix of Unix-variants and, of course, GNU/Linux. To recognize the many different operating systems online today, these systems are increasingly referred to generically as &amp;quot;pubnixes&amp;quot;. - <a href=\"https://github.com/cwmccabe/pubnixhist\">Pubnix Hist</a></p>\n</blockquote>\n</div>\n","tags":["linux","community","retrocomputing"]},{"title":"Hacking the Planet (with Notcurses) - A Guide to TUIs and Character Graphics","contentType":"bookmarks","date":"2023-07-12 22:21","url":"/responses/hack-the-planet-notcurses-tuis/","content":"<div class=\"response-target\" >→ <a href=\"https://nick-black.com/htp-notcurses.pdf\" >https://nick-black.com/htp-notcurses.pdf</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A TUI (text user interface) is a holistic model, view, and controller implemented using character graphics.\nTUIs, like WIMP3 GUIs, freely move the cursor around their rectilinear display, as opposed to line-oriented\nCLIs and their ineluctable marches through the scrolling region.\nGiven the same interactive task</p>\n<p>• A TUI implementation is almost certainly a smaller memory and disk footprint than a GUI,<br />\n• a good TUI implementation might introduce less latency, and<br />\n• a properly-done TUI implementation can often be significantly more portable.</p>\n</blockquote>\n</div>\n","tags":["linux","terminal","tui"]},{"title":"Kaz Nejatian (COO, Shopify): Why Shopify Elevated the Non-Manager Career Path and Ditched Meetings","contentType":"bookmarks","date":"2023-07-12 20:20","url":"/responses/interview-shopify-coo-career-path-meetings/","content":"<div class=\"response-target\" >→ <a href=\"https://creatoreconomy.so/p/kaz-coo-shopify-craft-and-no-meetings\" >https://creatoreconomy.so/p/kaz-coo-shopify-craft-and-no-meetings</a></div><div class=\"response-content\" >\n<blockquote class=\"blockquote\">\n<p>The difference is in what you spend time on.</p>\n<p>&amp;quot;Most people get satisfaction from building — from actually creating things.&amp;quot;</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>As companies scale, optics start playing a larger role. People start spending more time on internal docs than actually talking to customers. How do you prevent this from happening at Shopify?</p>\n<p>In most product reviews, product managers spend way too much time preparing the perfect presentation for execs.</p>\n<p>At Shopify, our approach to product reviews is different. We want to see how the product actually works by playing with the demo or diving into the code.</p>\n<p>We want our PMs to be extremely user-focused, to take full ownership over problems, and to have a high tolerance for risk.</p>\n<p>If these attributes aren't present, product managers tend to become &amp;quot;keepers of strategy.” You end up with smart, highly credentialed individuals spending all their time writing strategy memos to increase their team size so that they can write even more strategy memos.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>In early 2023, Shopify initiated operation “Chaos Monkey” to:</p>\n<ol>\n<li>Cancel all meetings with 3+ people</li>\n<li>Reinstate “no meeting Wednesdays”</li>\n<li>Remove needless Slack channels</li>\n</ol>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Does Shopify have a strong writing culture to help people communicate without meetings?</p>\n<p>Yes, we try to make async decisions as much as we can. We do this in a few ways:</p>\n<ol>\n<li><p>One of our mantras is “Do things, tell people.” You’ll see this plastered on our walls if you come to Shopify’s office.</p>\n</li>\n<li><p>We built an operating system called GSD (get shit done). This internal tool emphasizes frequent written updates, which are much easier to digest than constant meetings.</p>\n</li>\n</ol>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>A meeting is a bug that some other process didn’t work out.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>We focus on the mission. We want to be the all-in-one commerce platform for people to start and grow businesses. We try to avoid getting distracted by other side quests.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The main thing is to keep the main thing the main thing.</p>\n</blockquote>\n</div>\n","tags":["career","productivity","product"]},{"title":"Announcing xAI ","contentType":"bookmarks","date":"2023-07-12 20:15","url":"/responses/x-ai-launch/","content":"<div class=\"response-target\" >→ <a href=\"https://x.ai/\" >https://x.ai/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Today we announce the formation of xAI.</p>\n<p>The goal of xAI is to understand the true nature of the universe.</p>\n</blockquote>\n</div>\n","tags":["ai"]},{"title":"Introducing Keras Core: Keras for TensorFlow, JAX, and PyTorch.","contentType":"bookmarks","date":"2023-07-11 16:22","url":"/responses/keras-core-announcement/","content":"<div class=\"response-target\" >→ <a href=\"https://keras.io/keras_core/announcement/\" >https://keras.io/keras_core/announcement/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We're excited to share with you a new library called Keras Core, a preview version of the future of Keras. In Fall 2023, this library will become Keras 3.0. Keras Core is a full rewrite of the Keras codebase that rebases it on top of a modular backend architecture. It makes it possible to run Keras workflows on top of arbitrary frameworks — starting with TensorFlow, JAX, and PyTorch.</p>\n<p>Keras Core is also a drop-in replacement for tf.keras, with near-full backwards compatibility with tf.keras code when using the TensorFlow backend. In the vast majority of cases you can just start importing it via import keras_core as keras in place of from tensorflow import keras and your existing code will run with no issue — and generally with slightly improved performance, thanks to XLA compilation.</p>\n</blockquote>\n</div>\n","tags":["deeplearning","ai","keras","jax","tensorflow","pytorch"]},{"title":"512KB Club","contentType":"bookmarks","date":"2023-06-29 10:56","url":"/responses/512kb-club/","content":"<div class=\"response-target\" >→ <a href=\"https://512kb.club/\" >https://512kb.club/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The 512KB Club is a collection of performance-focused web pages from across the Internet. To qualify your website must satisfy both of the following requirements:</p>\n<ol>\n<li>It must be an actual site that contains a reasonable amount of information, not just a couple of links on a page (more info here).</li>\n<li>Your total UNCOMPRESSED web resources must not exceed 512KB.</li>\n</ol>\n</blockquote>\n</div>\n","tags":["web","smallweb","community"]},{"title":"1MB Club","contentType":"bookmarks","date":"2023-06-29 10:53","url":"/responses/1mb-club/","content":"<div class=\"response-target\" >→ <a href=\"https://1mb.club/\" >https://1mb.club/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>1MB Club is a growing collection of performance-focused web pages weighing less than 1 megabyte.</p>\n</blockquote>\n</div>\n","tags":["web","smallweb","community"]},{"title":"Reclaim Open 2023 Resources","contentType":"bookmarks","date":"2023-06-19 22:04","url":"/responses/reclaim-open-2023-resources/","content":"<div class=\"response-target\" >→ <a href=\"https://reclaimopen.com/\" >https://reclaimopen.com/</a></div><div class=\"response-content\" ><p>List of resources from the Reclaim Open 2023 Conference.</p>\n<ul>\n<li><a href=\"https://reclaimopen.com/\">Website</a></li>\n<li><a href=\"https://watch.reclaimed.tech/reclaim-open\">Videos</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=WzogFtdBVg8\">KEYNOTE: We Have Never Been Social: Web 2.0 and What Went Wrong</a></li>\n<li><a href=\"https://digitalmediacookbook.com/\">Digital Media Cookbook</a></li>\n<li><a href=\"https://minicomp.github.io/wiki/\">Minimal Computing (Minicomp) Wiki</a></li>\n<li><a href=\"https://go-dh.github.io/mincomp/thoughts/2016/10/03/tldr/#minimal-connectivity\">Minimal Definitions</a></li>\n<li><a href=\"https://press.uchicago.edu/ucp/books/book/chicago/F/bo3773600.html\">Book: From Counterculture to Cyberculture</a></li>\n<li><a href=\"http://darcyd.com/fragmented_future.pdf\">Article: Fragmented Future</a></li>\n</ul>\n</div>\n","tags":["reclaimopen","opensource","openweb","edtech"]},{"title":"Orca: Progressive Learning from Complex Explanation Traces of GPT-4","contentType":"bookmarks","date":"2023-06-06 23:29","url":"/responses/orca-lm-progressive-learning-gpt-4/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/pdf/2306.02707.pdf\" >https://arxiv.org/pdf/2306.02707.pdf</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Recent research has focused on enhancing the capability of smaller models\nthrough imitation learning, drawing on the outputs generated by large\nfoundation models (LFMs). A number of issues impact the quality of these\nmodels, ranging from limited imitation signals from shallow LFM outputs;\nsmall scale homogeneous training data; and most notably a lack of rigorous\nevaluation resulting in overestimating the small model’s capability as they\ntend to learn to imitate the style, but not the reasoning process of LFMs. To\naddress these challenges, we develop Orca, a 13-billion parameter model\nthat learns to imitate the reasoning process of LFMs. Orca learns from\nrich signals from GPT-4 including explanation traces; step-by-step thought\nprocesses; and other complex instructions, guided by teacher assistance from\nChatGPT. To promote this progressive learning, we tap into large-scale and\ndiverse imitation data with judicious sampling and selection. Orca surpasses\nconventional state-of-the-art instruction-tuned models such as Vicuna-13B\nby more than 100% in complex zero-shot reasoning benchmarks like Big-\nBench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity\nwith ChatGPT on the BBH benchmark and shows competitive performance\n(4 pts gap with optimized system message) in professional and academic\nexaminations like the SAT, LSAT, GRE, and GMAT, both in zero-shot\nsettings without CoT; while trailing behind GPT-4. Our research indicates\nthat learning from step-by-step explanations, whether these are generated\nby humans or more advanced AI models, is a promising direction to improve\nmodel capabilities and skills.</p>\n</blockquote>\n</div>\n","tags":["ai","finetuning","gpt"]},{"title":"Gorilla: Large Language Model Connected with Massive APIs","contentType":"bookmarks","date":"2023-06-03 22:37","url":"/responses/gorilla-apis-llm/","content":"<div class=\"response-target\" >→ <a href=\"https://gorilla.cs.berkeley.edu/\" >https://gorilla.cs.berkeley.edu/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs.</p>\n</blockquote>\n<p><a href=\"https://github.com/ShishirPatil/gorilla\">Code</a></p>\n</div>\n","tags":["ai","opensource","llm"]},{"title":"Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold","contentType":"bookmarks","date":"2023-06-01 22:52","url":"/responses/draggan/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2305.10973\" >https://arxiv.org/abs/2305.10973</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object's rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion.</p>\n</blockquote>\n<p><a href=\"https://github.com/XingangPan/DragGAN\">Code</a></p>\n</div>\n","tags":["ai","computer-vision"]},{"title":"StarCoder: A State-of-the-Art LLM for Code ","contentType":"bookmarks","date":"2023-06-01 22:50","url":"/responses/hf-starcoder-llm/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/blog/starcoder\" >https://huggingface.co/blog/starcoder</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>StarCoder and StarCoderBase are Large Language Models for Code (Code LLMs) trained on permissively licensed data from GitHub, including from 80+ programming languages, Git commits, GitHub issues, and Jupyter notebooks. Similar to LLaMA, we trained a ~15B parameter model for 1 trillion tokens. We fine-tuned StarCoderBase model for 35B Python tokens, resulting in a new model that we call StarCoder.</p>\n</blockquote>\n</div>\n","tags":["ai","code"]},{"title":"LoRA: Low-Rank Adaptation of Large Language Models","contentType":"bookmarks","date":"2023-06-01 22:45","url":"/responses/microsoft-lora-llm/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/microsoft/LoRA\" >https://github.com/microsoft/LoRA</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>LoRA reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights. This vastly reduces the storage requirement for large language models adapted to specific tasks and enables efficient task-switching during deployment all without introducing inference latency. LoRA also outperforms several other adaptation methods including adapter, prefix-tuning, and fine-tuning.</p>\n</blockquote>\n<p><a href=\"https://arxiv.org/abs/2106.09685\">Paper</a></p>\n</div>\n","tags":["ai","finetune","llm"]},{"title":"Shap-E: Generating Conditional 3D Implicit Functions","contentType":"bookmarks","date":"2023-06-01 22:41","url":"/responses/openai-shap-e/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2305.02463\" >https://arxiv.org/abs/2305.02463</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space.</p>\n</blockquote>\n<p><a href=\"https://github.com/openai/shap-e\">Code</a></p>\n</div>\n","tags":["ai","openai","3D"]},{"title":"Massively Multilingual Speech (MMS)","contentType":"bookmarks","date":"2023-05-22 23:43","url":"/responses/meta-mms-tts-stt/","content":"<div class=\"response-target\" >→ <a href=\"https://ai.facebook.com/blog/multilingual-model-speech-recognition/\" >https://ai.facebook.com/blog/multilingual-model-speech-recognition/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The Massively Multilingual Speech (MMS) project expands speech technology from about 100 languages to over 1,000 by building a single multilingual speech recognition model supporting over 1,100 languages (more than 10 times as many as before), language identification models able to identify over 4,000 languages (40 times more than before), pretrained models supporting over 1,400 languages, and text-to-speech models for over 1,100 languages. Our goal is to make it easier for people to access information and to use devices in their preferred language.</p>\n</blockquote>\n<ul>\n<li><a href=\"https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/\">Paper</a></li>\n<li><a href=\"https://github.com/facebookresearch/fairseq/tree/main/examples/mms\">Code</a></li>\n</ul>\n</div>\n","tags":["ai","nlp","speech-to-text","text-to-speech"]},{"title":"PaLM 2","contentType":"bookmarks","date":"2023-05-11 20:39","url":"/responses/PaLM-2/","content":"<div class=\"response-target\" >→ <a href=\"https://ai.google/discover/palm2\" >https://ai.google/discover/palm2</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>PaLM 2 is our next generation large language model that builds on Google’s legacy of breakthrough research in machine learning and responsible AI.</p>\n<p>It excels at advanced reasoning tasks, including code and math, classification and question answering, translation and multilingual proficiency, and natural language generation better than our previous state-of-the-art LLMs, including PaLM. It can accomplish these tasks because of the way it was built – bringing together compute-optimal scaling, an improved dataset mixture, and model architecture improvements.</p>\n<p>PaLM 2 is grounded in Google’s approach to building and deploying AI responsibly. It was evaluated rigorously for its potential harms and biases, capabilities and downstream uses in research and in-product applications. It’s being used in other state-of-the-art models, like Med-PaLM 2 and Sec-PaLM, and is powering generative AI features and tools at Google, like Bard and the PaLM API.</p>\n</blockquote>\n<p><a href=\"https://ai.google/static/documents/palm2techreport.pdf\">Technical Report</a></p>\n</div>\n","tags":["ai"]},{"title":"Transformers Agent","contentType":"bookmarks","date":"2023-05-11 20:35","url":"/responses/huggingface-transformer-agents/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/docs/transformers/en/transformers_agents\" >https://huggingface.co/docs/transformers/en/transformers_agents</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Transformers Agent...provides a natural language API on top of transformers: we define a set of curated tools and design an agent to interpret natural language and to use these tools. It is extensible by design; we curated some relevant tools, but we’ll show you how the system can be extended easily to use any tool developed by the community.</p>\n</blockquote>\n</div>\n","tags":["transformers","ai"]},{"title":"Copilot for Docs","contentType":"bookmarks","date":"2023-05-11 20:32","url":"/responses/copilot-docs/","content":"<div class=\"response-target\" >→ <a href=\"https://githubnext.com/projects/copilot-for-docs\" >https://githubnext.com/projects/copilot-for-docs</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Whether you’re learning a new library or API or you’ve been using it for years, it can feel like the documentation gets in your way more than it helps. Maybe the tutorials are too basic, or the reference manual is too sketchy, or the relevant information is split across multiple pages full of irrelevant details.</p>\n<p>We’re exploring a way to get you the information you need, faster. By surfacing the most relevant content for questions with tailored summaries that help connect the dots, Copilot for docs saves developers from scouring reams of documentation.</p>\n</blockquote>\n</div>\n","tags":["ai","documentation","developers"]},{"title":"ChatGPT Prompt Engineering for Developers","contentType":"bookmarks","date":"2023-05-11 20:29","url":"/responses/chatgpt-prompt-engineering-developers/","content":"<div class=\"response-target\" >→ <a href=\"https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/\" >https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In ChatGPT Prompt Engineering for Developers, you will learn how to use a large language model (LLM) to quickly build new and powerful applications.  Using the OpenAI API, you’ll be able to quickly build capabilities that learn to innovate and create value in ways that were cost-prohibitive, highly technical, or simply impossible before now.</p>\n</blockquote>\n</div>\n","tags":["ai","gpt","promptengineering"]},{"title":"ImageBind: One Embedding Space To Bind Them All","contentType":"bookmarks","date":"2023-05-09 21:02","url":"/responses/imagebind/","content":"<div class=\"response-target\" >→ <a href=\"https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/\" >https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...ImageBind, the first AI model capable of binding information from six modalities. The model learns a single embedding, or shared representation space, not just for text, image/video, and audio, but also for sensors that record depth (3D), thermal (infrared radiation), and inertial measurement units (IMU), which calculate motion and position.</p>\n</blockquote>\n<ul>\n<li><a href=\"https://dl.fbaipublicfiles.com/imagebind/imagebind_final.pdf\">Paper</a></li>\n<li><a href=\"https://imagebind.metademolab.com/\">Demo</a></li>\n<li><a href=\"https://github.com/facebookresearch/ImageBind\">Code</a></li>\n</ul>\n</div>\n","tags":["ai","embeddings"]},{"title":"Language models can explain neurons in language models","contentType":"bookmarks","date":"2023-05-09 20:59","url":"/responses/large-language-models-explain-neural-network-neurons/","content":"<div class=\"response-target\" >→ <a href=\"https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html\" >https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This paper applies automation to the problem of scaling an interpretability technique to all the neurons in a large language model. Our hope is that building on this approach of automating interpretability will enable us to comprehensively audit the safety of models before deployment.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Our technique seeks to explain what patterns in text cause a neuron to activate. It consists of three steps:</p>\n<ol>\n<li>Explain the neuron's activations using GPT-4</li>\n<li>Simulate activations using GPT-4, conditioning on the explanation</li>\n<li>Score the explanation by comparing the simulated and real activations</li>\n</ol>\n</blockquote>\n</div>\n","tags":["ai","llm","deeplearning"]},{"title":"PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware","contentType":"bookmarks","date":"2023-04-20 21:32","url":"/responses/peft-parameter-efficient-fine-tuning/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/blog/peft\" >https://huggingface.co/blog/peft</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...as models get larger and larger, full fine-tuning becomes infeasible to train on consumer hardware. In addition, storing and deploying fine-tuned models independently for each downstream task becomes very expensive, because fine-tuned models are the same size as the original pretrained model. Parameter-Efficient Fine-tuning (PEFT) approaches are meant to address both problems!</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs. This also overcomes the issues of catastrophic forgetting, a behaviour observed during the full finetuning of LLMs. PEFT approaches have also shown to be better than fine-tuning in the low-data regimes and generalize better to out-of-domain scenarios. It can be applied to various modalities, e.g., image classification and stable diffusion dreambooth.</p>\n</blockquote>\n</div>\n","tags":["ai","finetuning"]},{"title":"Wikipedia embeddings dataset","contentType":"bookmarks","date":"2023-04-20 21:23","url":"/responses/wikipedia-embeddings/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings\" >https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings</a></div><div class=\"response-content\" ></div>\n","tags":["ai","dataset","embeddings"]},{"title":"LLaVA: Large Language and Vision Assistant","contentType":"bookmarks","date":"2023-04-18 21:34","url":"/responses/llava-language-vision-assistant/","content":"<div class=\"response-target\" >→ <a href=\"https://llava-vl.github.io/\" >https://llava-vl.github.io/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>LLaVA represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks in the language domain, but the idea is less explored in the multimodal field.</p>\n<pre><code>1. Multimodal Instruct Data. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data.\n2. LLaVA Model. We introduce LLaVA (Large Language-and-Vision Assistant), an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.\n3. Performance. Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%.\n4. Open-source. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.\n</code></pre>\n</blockquote>\n</div>\n","tags":["ai","language","vision"]},{"title":"WebGPU API","contentType":"bookmarks","date":"2023-04-15 22:57","url":"/responses/webgpu-api/","content":"<div class=\"response-target\" >→ <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API\" >https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The WebGPU API enables web developers to use the underlying system's GPU (Graphics Processing Unit) to carry out high-performance computations and draw complex images that can be rendered in the browser.</p>\n</blockquote>\n</div>\n","tags":["ai","web","openstandards"]},{"title":"Consistency Models","contentType":"bookmarks","date":"2023-04-14 14:22","url":"/responses/openai-consistency-models/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/openai/consistency_models\" >https://github.com/openai/consistency_models</a></div><div class=\"response-content\" ><p>Paper: https://arxiv.org/abs/2303.01469</p>\n<blockquote class=\"blockquote\">\n<p>Diffusion models have made significant breakthroughs in image, audio, and video generation, but they depend on an iterative generation process that causes slow sampling speed and caps their potential for real-time applications. To overcome this limitation, we propose consistency models, a new family of generative models that achieve <strong>high sample quality without adversarial training</strong>. They <strong>support fast one-step generation</strong> by design, while still allowing for few-step sampling to trade compute for sample quality. They also <strong>support zero-shot data editing</strong>, like image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either as a way to distill pre-trained diffusion models, or as standalone generative models. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step generation. For example, we achieve the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained as standalone generative models, consistency models also outperform single-step, non-adversarial generative models on standard benchmarks like CIFAR-10, ImageNet 64x64 and LSUN 256x256.</p>\n</blockquote>\n</div>\n","tags":["openai","ai"]},{"title":"Free Dolly","contentType":"bookmarks","date":"2023-04-12 23:06","url":"/responses/free-dolly-llm/","content":"<div class=\"response-target\" >→ <a href=\"https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm\" >https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm</a></div><div class=\"response-content\" ></div>\n","tags":["ai","llm","oss"]},{"title":"Generative Agents: Interactive Simulacra of Human Behavior","contentType":"bookmarks","date":"2023-04-11 11:21","url":"/responses/llm-generative-agents/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/pdf/2304.03442.pdf\" >https://arxiv.org/pdf/2304.03442.pdf</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In this paper, we introduce generative agents—computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors...</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.</p>\n</blockquote>\n</div>\n","tags":["ai","llm"]},{"title":"Koala: A Dialogue Model for Academic Research","contentType":"bookmarks","date":"2023-04-05 23:28","url":"/responses/koala-dialogue-model/","content":"<div class=\"response-target\" >→ <a href=\"https://bair.berkeley.edu/blog/2023/04/03/koala/\" >https://bair.berkeley.edu/blog/2023/04/03/koala/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...Koala, a chatbot trained by fine-tuning Meta’s LLaMA on dialogue data gathered from the web.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Many of the most capable LLMs require huge computational resources to train, and oftentimes use large and proprietary datasets. This suggests that in the future, highly capable LLMs will be largely controlled by a small number of organizations, and both users and researchers will pay to interact with these models without direct access to modify and improve them on their own. On the other hand, recent months have also seen the release of increasingly capable freely available or (partially) open-source models, such as LLaMA. These systems typically fall short of the most capable closed models, but their capabilities have been rapidly improving. This presents the community with an important question: will the future see increasingly more consolidation around a handful of closed-source models, or the growth of open models with smaller architectures that approach the performance of their larger but closed-source cousins?</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Our results suggest that learning from high-quality datasets can mitigate some of the shortcomings of smaller models, maybe even matching the capabilities of large closed-source models in the future.</p>\n</blockquote>\n</div>\n","tags":["ai","llm"]},{"title":"From Deep Learning Foundations to Stable Diffusion","contentType":"bookmarks","date":"2023-04-04 21:35","url":"/responses/deep-learning-foundations-stable-diffusion/","content":"<div class=\"response-target\" >→ <a href=\"https://www.fast.ai/posts/part2-2023.html\" >https://www.fast.ai/posts/part2-2023.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>From Deep Learning Foundations to Stable Diffusion...is part 2 of Practical Deep Learning for Coders.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In this course, containing over 30 hours of video content, we implement the astounding Stable Diffusion algorithm from scratch!</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Schillace Laws of Semantic AI","contentType":"bookmarks","date":"2023-03-29 21:01","url":"/responses/schillace-laws-semantic-ai/","content":"<div class=\"response-target\" >→ <a href=\"https://learn.microsoft.com/semantic-kernel/howto/schillacelaws\" >https://learn.microsoft.com/semantic-kernel/howto/schillacelaws</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The &amp;quot;Schillace Laws&amp;quot; were formulated after working with a variety of Large Language Model (LLM) AI systems to date. Knowing them will accelerate your journey into this exciting space of reimagining the future of software engineering.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"OpenFlamingo: An open-source framework for training vision-language models with in-context learning","contentType":"bookmarks","date":"2023-03-28 22:00","url":"/responses/laion-openflamingo-vision-llm/","content":"<div class=\"response-target\" >→ <a href=\"https://laion.ai/blog/open-flamingo/\" >https://laion.ai/blog/open-flamingo/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...OpenFlamingo, an open-source reproduction of DeepMind's Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs).</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models","contentType":"bookmarks","date":"2023-03-28 21:53","url":"/responses/cerebras-gpt/","content":"<div class=\"response-target\" >→ <a href=\"https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/\" >https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Cerebras open sources seven GPT-3 models from 111 million to 13 billion parameters. Trained using the Chinchilla formula, these models set new benchmarks for accuracy and compute efficiency.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Today’s release is designed to be used by and reproducible by anyone. All models, weights, and checkpoints are available on Hugging Face and GitHub under the Apache 2.0 license. Additionally, we provide detailed information on our training methods and performance results in our forthcoming paper. The Cerebras CS-2 systems used for training are also available on-demand via Cerebras Model Studio.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Prompt Engineering","contentType":"bookmarks","date":"2023-03-21 21:47","url":"/responses/prompt-engineering-weng/","content":"<div class=\"response-target\" >→ <a href=\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\" >https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Alpaca: A Strong Open-Source Instruction-Following Model","contentType":"bookmarks","date":"2023-03-14 11:06","url":"/responses/alpaca-model-stanford/","content":"<div class=\"response-target\" >→ <a href=\"https://crfm.stanford.edu/2023/03/13/alpaca.html\" >https://crfm.stanford.edu/2023/03/13/alpaca.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We are releasing our findings about an instruction-following language model, dubbed Alpaca, which is fine-tuned from Meta’s LLaMA 7B model. We train the Alpaca model on 52K instruction-following demonstrations generated in the style of self-instruct using text-davinci-003. Alpaca shows many behaviors similar to OpenAI’s text-davinci-003, but is also surprisingly small and easy/cheap to reproduce.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"SPQA: The AI-based Architecture That’ll Replace Most Existing Software","contentType":"bookmarks","date":"2023-03-14 10:52","url":"/responses/spqa-ai-based-architecture-replace-software/","content":"<div class=\"response-target\" >→ <a href=\"https://danielmiessler.com/blog/spqa-ai-architecture-replace-existing-software/\" >https://danielmiessler.com/blog/spqa-ai-architecture-replace-existing-software/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>AI-based applications will be completely different than those we have today. The new architecture will be a far more elegant, four-component structure based around GPTs: State, Policy, Questions, and Action.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>A security program using SPQA</p>\n<ol>\n<li><p><strong>Choose the base model</strong> — You start with the latest and greatest overall GPT model from OpenAI, Google, Meta, McKinsey, or whoever. Lots of companies will have one. Let’s call it OpenAI’s GPT-6. It already knows so incredibly much about security, biotech, project management, scheduling, meetings, budgets, incident response, and audit preparedness that you might be able to survive with it alone. But you need more personalized context.</p>\n</li>\n<li><p><strong>Train your custom model</strong> — Then you train your custom model which is based on your own data, which will stack on top of GPT-6. This is all the stuff in the STATE section above. It’s your company’s telemetry and context. Logs. Docs. Finances. Chats. Emails. Meeting transcripts. Everything. It’s a small company and there are compression algorithms as part of the Custom Model Generation (CMG) product we use, so it’s a total of 312TB of data. You train your custom model on that.</p>\n</li>\n<li><p><strong>Train your policy model</strong> — Now you train another model that’s all about your company’s desires. The mission, the goals, your anti-goals, your challenges, your strategies. This is the guidance that comes from humans that we’re using to steer the ACTION part of the architecture. When we ask it to make stuff for us, and build out our plans, it’ll do so using the guardrails captured here in the POLICY.</p>\n</li>\n<li><p><strong>Tell the system to take the following actions</strong> — Now the models are combined. We have GPT-6, stacked with our STATE model, also stacked with our POLICY model, and together they know us better than we know ourselves.</p>\n</li>\n</ol>\n</blockquote>\n</div>\n","tags":[]},{"title":"Large language models are having their Stable Diffusion moment","contentType":"bookmarks","date":"2023-03-14 10:43","url":"/responses/large-language-models-stable-diffusion-moment/","content":"<div class=\"response-target\" >→ <a href=\"https://simonwillison.net/2023/Mar/11/llama/\" >https://simonwillison.net/2023/Mar/11/llama/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The race is on to release the first fully open language model that gives people ChatGPT-like capabilities on their own devices.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Introduction to Data-Centric AI","contentType":"bookmarks","date":"2023-02-23 10:25","url":"/responses/introduction-data-centric-ai-mit/","content":"<div class=\"response-target\" >→ <a href=\"https://dcai.csail.mit.edu/\" >https://dcai.csail.mit.edu/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Data-Centric AI (DCAI) is an emerging science that studies techniques to improve datasets, which is often the best way to improve performance in practical ML applications. While good data scientists have long practiced this manually via ad hoc trial/error and intuition, DCAI considers the improvement of data as a systematic engineering discipline.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>This is the first-ever course on DCAI. This class covers algorithms to find and fix common issues in ML data and to construct better datasets, concentrating on data used in supervised learning tasks like classification. All material taught in this course is highly practical, focused on impactful aspects of real-world ML applications, rather than mathematical details of how particular models work. You can take this course to learn practical techniques not covered in most ML classes, which will help mitigate the “garbage in, garbage out” problem that plagues many real-world ML applications.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Software 2.0","contentType":"bookmarks","date":"2023-02-23 10:14","url":"/responses/software-2-0-karpathy/","content":"<div class=\"response-target\" >→ <a href=\"https://karpathy.medium.com/software-2-0-a64152b37c35\" >https://karpathy.medium.com/software-2-0-a64152b37c35</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The “classical stack” of Software 1.0 is what we’re all familiar with — it is written in languages such as Python, C++, etc...Software 2.0 is written in much more abstract, human unfriendly language, such as the weights of a neural network.</p>\n</blockquote>\n\n<ul>\n<li>Computationally homogeneous</li>\n<li>Simple to bake into silicon</li>\n<li>Constant running time</li>\n<li>Constant memory use</li>\n<li>Highly portable</li>\n<li>Agile</li>\n<li>Modules can meld into an optimal whole</li>\n<li>It is better than you</li>\n</ul>\n\n<blockquote class=\"blockquote\">\n<p>At the end of the optimization we’re left with large networks that work well, but it’s very hard to tell how. Across many applications areas, we’ll be left with a choice of using a 90% accurate model we understand, or 99% accurate model we don’t.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The 2.0 stack can fail in unintuitive and embarrassing ways ,or worse, they can “silently fail”</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...the existence of adversarial examples and attacks highlights the unintuitive nature of this stack.</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>If you recognize Software 2.0 as a new and emerging programming paradigm instead of simply treating neural networks as a pretty good classifier in the class of machine learning techniques, the extrapolations become more obvious, and it’s clear that there is much more work to do.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>In particular, we’ve built up a vast amount of tooling that assists humans in writing 1.0 code, such as powerful IDEs with features like syntax highlighting, debuggers, profilers, go to def, git integration, etc. In the 2.0 stack, the programming is done by accumulating, massaging and cleaning datasets. Who is going to develop the first Software 2.0 IDEs, which help with all of the workflows in accumulating, visualizing, cleaning, labeling, and sourcing datasets?</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Github is a very successful home for Software 1.0 code. Is there space for a Software 2.0 Github? In this case repositories are datasets and commits are made up of additions and edits of the labels.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Traditional package managers and related serving infrastructure like pip, conda, docker, etc. help us more easily deploy and compose binaries. How do we effectively deploy, share, import and work with Software 2.0 binaries? What is the conda equivalent for neural networks?</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Zero-shot image-to-text generation with BLIP-2","contentType":"bookmarks","date":"2023-02-15 14:04","url":"/responses/blip-2-zero-shot-image-to-text-generation/","content":"<div class=\"response-target\" >→ <a href=\"https://huggingface.co/blog/blip-2\" >https://huggingface.co/blog/blip-2</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This guide introduces BLIP-2 from Salesforce Research that enables a suite of state-of-the-art visual-language models that are now available in 🤗 Transformers. We'll show you how to use it for image captioning, prompted image captioning, visual question-answering, and chat-based prompting.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>BLIP-2 bridges the modality gap between vision and language models by adding a lightweight Querying Transformer (Q-Former) between an off-the-shelf frozen pre-trained image encoder and a frozen large language model. Q-Former is the only trainable part of BLIP-2; both the image encoder and language model remain frozen.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"What Is ChatGPT Doing…and Why Does It Work?","contentType":"bookmarks","date":"2023-02-14 23:30","url":"/responses/what-is-chatgpt-why-it-works-wolfram/","content":"<div class=\"response-target\" >→ <a href=\"https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/\" >https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"LMOps - Enabling AI capabilities with LLMs and Generative AI models","contentType":"bookmarks","date":"2023-02-07 09:49","url":"/responses/microsoft-lmops-github/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/microsoft/LMOps\" >https://github.com/microsoft/LMOps</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>LMOps is a research initiative on fundamental research and technology for building AI products w/ foundation models, especially on the general technology for enabling AI capabilities w/ LLMs and Generative AI models.</p>\n<ul>\n<li>Better Prompts: Promptist, Extensible prompts</li>\n<li>Longer Context: Structured prompting, Length-Extrapolatable Transformers</li>\n<li>Knowledge Augmentation (TBA)</li>\n<li>Fundamentals</li>\n</ul>\n</blockquote>\n</div>\n","tags":[]},{"title":"Install Manjaro WSL - Windows 10 & 11","contentType":"bookmarks","date":"2023-01-29 14:56","url":"/responses/windows-central-install-manjaro-wsl-guide/","content":"<div class=\"response-target\" >→ <a href=\"https://www.windowscentral.com/how-install-manjaro-wsl-windows-10-and-11\" >https://www.windowscentral.com/how-install-manjaro-wsl-windows-10-and-11</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Mathematics for Machine Learning and Data Science","contentType":"bookmarks","date":"2023-01-25 17:02","url":"/responses/math-machine-learning-data-science-mooc/","content":"<div class=\"response-target\" >→ <a href=\"https://www.coursera.org/specializations/mathematics-for-machine-learning-and-data-science\" >https://www.coursera.org/specializations/mathematics-for-machine-learning-and-data-science</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Master the Toolkit of AI and Machine Learning. Mathematics for Machine Learning and Data Science is a beginner-friendly specialization where you’ll learn the fundamental mathematics toolkit of machine learning: calculus, linear algebra, statistics, and probability.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Remote Sensing Datasets Repo","contentType":"bookmarks","date":"2023-01-24 10:01","url":"/responses/ml-remote-sensing-datasets/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/satellite-image-deep-learning/remote-sensing-datasets\" >https://github.com/satellite-image-deep-learning/remote-sensing-datasets</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"How to implement Q&A against your documentation with GPT3, embeddings and Datasette","contentType":"bookmarks","date":"2023-01-19 14:19","url":"/responses/implement-q-a-documentation-gpt-willison/","content":"<div class=\"response-target\" >→ <a href=\"http://simonwillison.net/2023/Jan/13/semantic-search-answers/#atom-everything\" >http://simonwillison.net/2023/Jan/13/semantic-search-answers/#atom-everything</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Here's how to do this:</p>\n<ul>\n<li>Run a text search (or a semantic search, described later) against your documentation to find content that looks like it could be relevant to the user's question.</li>\n<li>Grab extracts of that content and glue them all together into a blob of text.</li>\n<li>Construct a prompt consisting of that text followed by &amp;quot;Given the above content, answer the following question: &amp;quot; and the user's question</li>\n<li>Send the whole thing through the GPT-3 API and see what comes back</li>\n</ul>\n</blockquote>\n</div>\n","tags":[]},{"title":"A Visual Guide to SSH Tunnels","contentType":"bookmarks","date":"2023-01-18 10:52","url":"/responses/visual-guide-ssh-tunnels/","content":"<div class=\"response-target\" >→ <a href=\"https://iximiuz.com/en/posts/ssh-tunnels/\" >https://iximiuz.com/en/posts/ssh-tunnels/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"A Big Pile of Personal Developer & Designer Blogs in an OPML File","contentType":"bookmarks","date":"2023-01-11 15:38","url":"/responses/coyier-big-list-dev-designer-blogs-opml-file/","content":"<div class=\"response-target\" >→ <a href=\"https://chriscoyier.net/2023/01/05/a-big-pile-of-personal-developer-designer-blogs-in-an-opml-file/\" >https://chriscoyier.net/2023/01/05/a-big-pile-of-personal-developer-designer-blogs-in-an-opml-file/</a></div><div class=\"response-content\" ><p>OPML File: <a href=\"https://chriscoyier.s3.amazonaws.com/personal-developer-blogs.xml\">https://chriscoyier.s3.amazonaws.com/personal-developer-blogs.xml</a></p>\n</div>\n","tags":[]},{"title":"nanoGPT - The simplest, fastest repository for training/finetuning medium-sized GPTs.","contentType":"bookmarks","date":"2023-01-03 22:35","url":"/responses/nanoGPT/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/karpathy/nanoGPT\" >https://github.com/karpathy/nanoGPT</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The simplest, fastest repository for training/finetuning medium-sized GPTs. It's a re-write of minGPT, which I think became too complicated, and which I am hesitant to now touch. Still under active development, currently working to reproduce GPT-2 on OpenWebText dataset. The code itself aims by design to be plain and readable: train.py is a ~300-line boilerplate training loop and model.py a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"SWYX - Fave Podcasts 2022","contentType":"bookmarks","date":"2023-01-02 12:44","url":"/responses/swyx-favorite-podcasts-2022/","content":"<div class=\"response-target\" >→ <a href=\"https://www.swyx.io/fave-podcasts-2022/\" >https://www.swyx.io/fave-podcasts-2022/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"ThinkPad Buyer's Guide","contentType":"bookmarks","date":"2022-12-29 16:44","url":"/responses/used-thinkpad-buyers-guide/","content":"<div class=\"response-target\" >→ <a href=\"https://www.bobble.tech/free-stuff/used-thinkpad-buyers-guide\" >https://www.bobble.tech/free-stuff/used-thinkpad-buyers-guide</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Transformers from Scratch","contentType":"bookmarks","date":"2022-12-28 10:03","url":"/responses/e2eml-transformers-from-scratch/","content":"<div class=\"response-target\" >→ <a href=\"https://e2eml.school/transformers.html\" >https://e2eml.school/transformers.html</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Appropedia - Sustainability Wiki","contentType":"bookmarks","date":"2022-12-27 14:54","url":"/responses/appropedia-sustaibility-wiki/","content":"<div class=\"response-target\" >→ <a href=\"https://www.appropedia.org/Welcome_to_Appropedia\" >https://www.appropedia.org/Welcome_to_Appropedia</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Comprehensive Rust Course","contentType":"bookmarks","date":"2022-12-22 11:37","url":"/responses/comprehensive-rust-course/","content":"<div class=\"response-target\" >→ <a href=\"https://google.github.io/comprehensive-rust/\" >https://google.github.io/comprehensive-rust/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This is a four day Rust course developed by the Android team. The course covers the full spectrum of Rust, from basic syntax to advanced topics like generics and error handling. It also includes Android-specific content on the last day.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Norvig AlphaCode Review","contentType":"bookmarks","date":"2022-12-16 22:03","url":"/responses/norvig-pytudes-alphacode/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/norvig/pytudes/blob/main/ipynb/AlphaCode.ipynb\" >https://github.com/norvig/pytudes/blob/main/ipynb/AlphaCode.ipynb</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Large language models have recently shown an ability to solve a variety of problems. In this notebook we consider programming problems (as solved by AlphaCode) and mathematics problems (as solved by Minerva). The questions we would like to get at are:</p>\n<ul>\n<li>In the future, what role will these generative models play in assisting a programmer or mathematician?</li>\n<li>What will be a workflow that incorporates these models?</li>\n<li>How will other existing tools (such as programming languages) change to accomodate this workflow?</li>\n</ul>\n</blockquote>\n</div>\n","tags":[]},{"title":"The GPT-3 Architecture, on a Napkin","contentType":"bookmarks","date":"2022-12-13 21:11","url":"/responses/gpt-3-architecture-napkin/","content":"<div class=\"response-target\" >→ <a href=\"https://dugas.ch/artificial_curiosity/GPT_architecture.html\" >https://dugas.ch/artificial_curiosity/GPT_architecture.html</a></div><div class=\"response-content\" ><p><img src=\"https://dugas.ch/artificial_curiosity/img/GPT_architecture/fullarch.png\" class=\"img-fluid\" alt=\"Drawing of GPT-3 Architecture\" /></p>\n</div>\n","tags":[]},{"title":"Glow | GitHub","contentType":"bookmarks","date":"2022-12-07 18:05","url":"/responses/glow-markdown-cli/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/charmbracelet/glow\" >https://github.com/charmbracelet/glow</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Render markdown on the CLI</p>\n</blockquote>\n<p><img src=\"https://camo.githubusercontent.com/bd591b74af8a6991894c8a84ab8d48f05ce7f66975b325d31f6954c836ddab27/68747470733a2f2f73747566662e636861726d2e73682f676c6f772f676c6f772d312e332d747261696c65722d6769746875622e676966\" class=\"img-fluid\" alt=\"Glow CLI Tool GIF\" /></p>\n</div>\n","tags":[]},{"title":"Waydroid","contentType":"bookmarks","date":"2022-12-06 22:29","url":"/responses/waydroid-linux/","content":"<div class=\"response-target\" >→ <a href=\"https://waydro.id/\" >https://waydro.id/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A container-based approach to boot a full Android system on a regular GNU/Linux system like Ubuntu.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"EmacsConf 2022 Videos","contentType":"bookmarks","date":"2022-12-06 10:25","url":"/responses/emacsconf-2022-videos/","content":"<div class=\"response-target\" >→ <a href=\"https://toobnix.org/c/emacsconf/videos\" >https://toobnix.org/c/emacsconf/videos</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Course: NLP Demystified","contentType":"bookmarks","date":"2022-12-03 18:53","url":"/responses/nlp-demystified-course/","content":"<div class=\"response-target\" >→ <a href=\"https://www.nlpdemystified.org/\" >https://www.nlpdemystified.org/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Using Elfeed to view videos","contentType":"bookmarks","date":"2022-12-02 21:58","url":"/responses/elfeed-view-videos/","content":"<div class=\"response-target\" >→ <a href=\"https://medium.com/emacs/using-elfeed-to-view-videos-6dfc798e51e6\" >https://medium.com/emacs/using-elfeed-to-view-videos-6dfc798e51e6</a></div><div class=\"response-content\" ><p>Slightly modified the original script to use <a href=\"https://streamlink.github.io/\">Streamlink</a> and lower quality to 240p for bandwith and resource purposes.</p>\n<pre><code class=\"language-lisp\">(require 'elfeed)\n<p>(defun elfeed-v-mpv (url)\n&amp;quot;Watch a video from URL in MPV&amp;quot;\n(async-shell-command (format &amp;quot;streamlink -p mpv %s 240p&amp;quot; url)))</p>\n<p>(defun elfeed-view-mpv (&amp;amp;optional use-generic-p)\n&amp;quot;Youtube-feed link&amp;quot;\n(interactive &amp;quot;P&amp;quot;)\n(let ((entries (elfeed-search-selected)))\n(cl-loop for entry in entries\ndo (elfeed-untag entry 'unread)\nwhen (elfeed-entry-link entry)\ndo (elfeed-v-mpv it))\n(mapc #'elfeed-search-update-entry entries)\n(unless (use-region-p) (forward-line))))</p>\n<p>(define-key elfeed-search-mode-map (kbd &amp;quot;v&amp;quot;) 'elfeed-view-mpv)\n</code></pre></p>\n</div>\n","tags":[]},{"title":"ChatGPT: Optimizing Language Models for Dialogue","contentType":"bookmarks","date":"2022-12-01 11:12","url":"/responses/openai-chatgpt/","content":"<div class=\"response-target\" >→ <a href=\"https://openai.com/blog/chatgpt/\" >https://openai.com/blog/chatgpt/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests. ChatGPT is a sibling model to <a href=\"https://openai.com/blog/instruction-following/\">InstructGPT</a>, which is trained to follow an instruction in a prompt and provide a detailed response.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Coping strategies for the serial project hoarder","contentType":"bookmarks","date":"2022-11-29 10:00","url":"/responses/documentation-coping-serial-project-hoarder/","content":"<div class=\"response-target\" >→ <a href=\"https://simonwillison.net/2022/Nov/26/productivity/\" >https://simonwillison.net/2022/Nov/26/productivity/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Consider timezones: engineers in Madrid and engineers in San Francisco had almost no overlap in their working hours. Good asynchronous communication was essential.</p>\n<p>Over time, I noticed that the teams that were most effective at this scale were the teams that had a strong culture of documentation and automated testing.</p>\n<p>As I started to work on my own array of smaller personal projects, I found that the same discipline that worked for large teams somehow sped me up, when intuitively I would have expected it to slow me down.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"The Art of Command Line","contentType":"bookmarks","date":"2022-11-24 20:43","url":"/responses/art-of-command-line/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/jlevy/the-art-of-command-line\" >https://github.com/jlevy/the-art-of-command-line</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Stable Diffusion 2.0 Release","contentType":"bookmarks","date":"2022-11-24 20:39","url":"/responses/stable-diffusion-v2/","content":"<div class=\"response-target\" >→ <a href=\"https://stability.ai/blog/stable-diffusion-v2-release\" >https://stability.ai/blog/stable-diffusion-v2-release</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The Stable Diffusion 2.0 release includes robust text-to-image models trained using a brand new text encoder (OpenCLIP), developed by LAION with support from Stability AI, which greatly improves the quality of the generated images compared to earlier V1 releases.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Stable Diffusion 2.0 also includes an Upscaler Diffusion model that enhances the resolution of images by a factor of 4.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"alt.interoperability.adversarial","contentType":"bookmarks","date":"2022-10-30 12:16","url":"/responses/alt-interoperability-adversarial/","content":"<div class=\"response-target\" >→ <a href=\"https://www.eff.org/deeplinks/2019/11/altinteroperabilityadversarial\" >https://www.eff.org/deeplinks/2019/11/altinteroperabilityadversarial</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"How to communicate effectively as a developer","contentType":"bookmarks","date":"2022-10-28 20:57","url":"/responses/communicating-effectively-developer/","content":"<div class=\"response-target\" >→ <a href=\"https://www.karlsutt.com/articles/communicating-effectively-as-a-developer/\" >https://www.karlsutt.com/articles/communicating-effectively-as-a-developer/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Communicating effectively as an engineer means empathically increasing the resolution of your writing.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...“low-resolution writing”...There is very little context, too much reliance on pronouns and unclear references. The writing is not emphatic —the reader has to spend extra energy to work out what is being said</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Longer-form writing gives you an opportunity to dive deeper into why you are saying what you are saying. It is a chance to educate, to teach, to help understand and to level up.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The quality of the API documentation will carry an astronomical amount of leverage. This leverage will work in both directions. Genuinely helpful documentation is the difference between being swamped by support requests from frustrated API users and significantly increasing the usage of your service. Happy users beget more happy users.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Spoken words get forgotten. Written words are shared, preserved, and become the basis of a company's culture. <a href=\"https://twitter.com/david_perell/status/1584933536439881728\">source</a></p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>High resolution, empathic writing...You will have to spend more energy to make your writing easy to follow. You will have to grapple with your own confusion and holes in your understanding. You will have to figure out what the appropriate density for your writing is.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>It's not about you, though.. It's about them.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Not only does a single recipient benefit from your extra effort, what if ten people read your good work? A hundred? A thousand? What if the company CEO reads it? Taking writing seriously at work or in your organisation and putting in the effort to delight the reader will, over time, compound into a massive body of quality writing that benefits everyone. It is a literal win-win-win</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Produce writing you would read with delight if you were on the other end.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"From Deep Learning Foundations to Stable Diffusion","contentType":"bookmarks","date":"2022-10-19 23:52","url":"/responses/fastai-from-dl-stable-diffusion/","content":"<div class=\"response-target\" >→ <a href=\"https://www.fast.ai/posts/part2-2022-preview.html\" >https://www.fast.ai/posts/part2-2022-preview.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>In total, we’re releasing four videos, with around 5.5 hours of content, covering the following topics (the lesson numbers start at “9”, since this is a continuation of Practical Deep Learning for Coders part 1, which had 8 lessons):</p>\n<ul>\n<li>Lesson 9 by Jeremy Howard: How to use Diffusers pipelines; What are the conceptual parts of Stable Diffusion</li>\n<li>Lesson 9A by Jonathan Whitaker: A deep dive into Stable Diffusion concepts and code</li>\n<li>Lesson 9B by Wasim Lorgat and Tanishq Abraham: The math of diffusion</li>\n<li>Lesson 10 by Jeremy Howard: Creating a custom diffusion pipeline; Starting “from the foundations”</li>\n</ul>\n</blockquote>\n</div>\n","tags":[]},{"title":"A Brief History & Ethos of the Digital Garden","contentType":"bookmarks","date":"2022-10-19 00:03","url":"/responses/appleton-garden-history/","content":"<div class=\"response-target\" >→ <a href=\"https://maggieappleton.com/garden-history\" >https://maggieappleton.com/garden-history</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The conversational feed design of email inboxes, group chats, and InstaTwitBook is fleeting – they're only concerned with self-assertive immediate thoughts that rush by us in a few moments...But streams only surface the Zeitgeisty ideas of the last 24 hours...Gardens present information in a richly linked landscape that grows slowly over time...The garden helps us move away from time-bound streams and into contextual knowledge spaces.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>The Six Patterns of Gardening:</p>\n<ol>\n<li>Topography over Timelines - Gardens are organised around contextual relationships and associative links; the concepts and themes within each note determine how it's connected to others.</li>\n<li>Continuous Growth - Gardens are never finished, they're constantly growing, evolving, and changing.</li>\n<li>Imperfection &amp;amp; Learning in Public - Gardens are imperfect by design. They don't hide their rough edges or claim to be a permanent source of truth.</li>\n<li>Playful, Personal, and Experimental - Gardens are non-homogenous by nature. You can plant the same seeds as your neighbour, but you'll always end up with a different arrangement of plants.</li>\n<li>Intercropping &amp;amp; Content Diversity - Gardens are not just a collection of interlinked words...Podcasts, videos, diagrams, illustrations, interactive web animations, academic papers, tweets, rough sketches, and code snippets should all live and grow in the garden.</li>\n<li>Independent Ownership - Gardening is about claiming a small patch of the web for yourself, one you fully own and control... If you give it a bit of forethought, you can build your garden in a way that makes it easy to transfer and adapt. Platforms and technologies will inevitably change. Using old-school, reliable, and widely used web native formats like HTML/CSS is a safe bet.</li>\n</ol>\n</blockquote>\n</div>\n","tags":[]},{"title":"Bidirectional Attention Flow (BiDAF) ML.NET ONNX Sample","contentType":"bookmarks","date":"2022-10-18 17:16","url":"/responses/bidaf-mlnet-onnx/","content":"<div class=\"response-target\" >→ <a href=\"https://gist.github.com/luisquintanilla/164176ec414e465246d6323aa62b38df\" >https://gist.github.com/luisquintanilla/164176ec414e465246d6323aa62b38df</a></div><div class=\"response-content\" ><p>This sample shows how to use a pretrained Bidirectional Attention Flow (BiDAF) ONNX model in ML.NET for answering a query about a given context paragraph.</p>\n</div>\n","tags":[]},{"title":"The Illustrated Stable Diffusion","contentType":"bookmarks","date":"2022-10-18 09:25","url":"/responses/illustrated-stable-diffusion/","content":"<div class=\"response-target\" >→ <a href=\"https://jalammar.github.io/illustrated-stable-diffusion/\" >https://jalammar.github.io/illustrated-stable-diffusion/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"The Illustrated GPT-2","contentType":"bookmarks","date":"2022-10-17 19:03","url":"/responses/illustrated-gpt-2/","content":"<div class=\"response-target\" >→ <a href=\"http://jalammar.github.io/illustrated-gpt2/\" >http://jalammar.github.io/illustrated-gpt2/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Understanding HTML with Large Language Models","contentType":"bookmarks","date":"2022-10-14 15:29","url":"/responses/understanding-html-large-language-models/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/abs/2210.03945\" >https://arxiv.org/abs/2210.03945</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding...have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Out of the LLMs we evaluate, we show evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Kaggle - State of Data Science and Machine Learning 2022","contentType":"bookmarks","date":"2022-10-14 13:59","url":"/responses/kaggle-state-data-science-ml-2022/","content":"<div class=\"response-target\" >→ <a href=\"https://www.kaggle.com/kaggle-survey-2022\" >https://www.kaggle.com/kaggle-survey-2022</a></div><div class=\"response-content\" >\n<blockquote class=\"blockquote\">\n<p>Python and SQL remain the two most common programming skills for data scientists</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>VSCode is now used by over 50% of working data scientists</p>\n</blockquote>\n<p>Notebooks are a popular environment as well.</p>\n<blockquote class=\"blockquote\">\n<p>Colab notebooks are the most popular cloud-based Jupyter notebook environment</p>\n</blockquote>\n<p>Makes sense especially since Kaggle is owned by Google.</p>\n\n<blockquote class=\"blockquote\">\n<p>Kaggle DS &amp;amp; ML Survey 2022 Scikit-learn is the most popular ML framework while PyTorch has been growing steadily year-over-year</p>\n</blockquote>\n<p>LightGBM, XGBoost are also among the top frameworks.</p>\n<blockquote class=\"blockquote\">\n<p>Transformer architectures are becoming more popular for deep learning models (both image and text data)</p>\n</blockquote>\n\n<blockquote class=\"blockquote\">\n<p>All major cloud computing providers saw strong year over year growth in 2022</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Specialized hardware like Tensor Processing Units (TPUs) is gaining initial traction with Kaggle data scientists</p>\n</blockquote>\n\n<ul>\n<li><a href=\"https://storage.googleapis.com/kaggle-media/surveys/Kaggle%20State%20of%20Machine%20Learning%20and%20Data%20Science%20Report%202022.pdf\">PDF</a></li>\n<li><a href=\"https://www.kaggle.com/c/kaggle-survey-2022/data\">Dataset</a></li>\n</ul>\n</div>\n","tags":[]},{"title":"State of AI Report 2022 ","contentType":"bookmarks","date":"2022-10-12 19:07","url":"/responses/2022-state-of-ai-report/","content":"<div class=\"response-target\" >→ <a href=\"https://www.stateof.ai/2022-report-launch.html\" >https://www.stateof.ai/2022-report-launch.html</a></div><div class=\"response-content\" ><p>Key takeaways:</p>\n<ol>\n<li>New independent research labs are rapidly open sourcing the closed source output of major labs.</li>\n<li>Safety is gaining awareness among major AI research entities</li>\n<li>The China-US AI research gap has continued to widen</li>\n<li>AI-driven scientific research continues to lead to breakthroughs</li>\n</ol>\n</div>\n","tags":[]},{"title":"Advanced Programming in the UNIX Environment","contentType":"bookmarks","date":"2022-10-12 18:29","url":"/responses/cs631-advanced-programming-unix-env/","content":"<div class=\"response-target\" >→ <a href=\"https://stevens.netmeister.org/631/\" >https://stevens.netmeister.org/631/</a></div><div class=\"response-content\" ><p>In this course, students will learn to develop complex system-level software in the C programming language while gaining an intimate understanding of the Unix operating system (and all OS that belong to this family, such as Linux, the BSDs, and even Mac OS X) and its programming environment.</p>\n<p>Topics covered will include the user/kernel interface, fundamental concepts of Unix, user authentication, basic and advanced I/O, fileystems, signals, process relationships, and interprocess communication. Fundamental concepts of software development and maintenance on Unix systems (development and debugging tools such as &amp;quot;make&amp;quot; and &amp;quot;gdb&amp;quot;) will also be covered.</p>\n</div>\n","tags":[]},{"title":"ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks","contentType":"bookmarks","date":"2022-10-11 20:22","url":"/responses/impnet-undetectable-backdoors-compiled-nn/","content":"<div class=\"response-target\" >→ <a href=\"https://arxiv.org/pdf/2210.00108.pdf\" >https://arxiv.org/pdf/2210.00108.pdf</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...backdoors can be added during compilation, circumventing any safeguards in the data preparation and model training stages.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>some backdoors, such as ImpNet, can only be reliably detected at the stage where they are inserted and removing them anywhere else presents a significant challenge.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>machine-learning model security requires assurance of provenance along the entire technical pipeline, including the data, model architecture, compiler, and hardware specification.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Indieweb Bookmark Post","contentType":"bookmarks","date":"2022-10-10 20:39","url":"/responses/indieweb-bookmark-post/","content":"<div class=\"response-target\" >→ <a href=\"https://indieweb.org/bookmark\" >https://indieweb.org/bookmark</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A bookmark (or linkblog) is a post that is primarily comprised of a URL, often title text from that URL, sometimes optional text describing, tagging, or quoting from its contents.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Bookmarks are useful for saving things to read later or build a recommended reading list.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Testing Web Design Color Contrast","contentType":"bookmarks","date":"2022-09-22 20:06","url":"/responses/testing-web-design-color-contrast/","content":"<div class=\"response-target\" >→ <a href=\"https://web.dev/testing-web-design-color-contrast/\" >https://web.dev/testing-web-design-color-contrast/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>W3C’s Web Accessibility Initiative provides strategies, standards, and resources to ensure that the internet is accessible for as many people as possible. The guidelines that underpin these standards are called the Web Content Accessibility Guidelines, or WCAG.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Color contrast is an important piece of the puzzle for accessibility on the web, and adhering to it makes the web more usable for the greatest number of people in the most varied situations.</p>\n</blockquote>\n<p>Apps to test contrast:</p>\n<ul>\n<li>Pika</li>\n<li>VisBug</li>\n<li>Chrome Dev Tools</li>\n</ul>\n</div>\n","tags":[]},{"title":"Resources for navigating complex leadership work","contentType":"bookmarks","date":"2022-09-21 19:19","url":"/responses/resources-navigating-complex-leadership-work/","content":"<div class=\"response-target\" >→ <a href=\"https://larahogan.me/blog/management-resource-library/\" >https://larahogan.me/blog/management-resource-library/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Resource on:</p>\n<p>Influence &amp;amp; managing up: Enact positive change for yourself, your team, or the whole organization.</p>\n<p>Leading through crises: Strengthen your support network, meet your team where they’re at, and weather the tough times.</p>\n<p>Cross-functional relationships: Strengthen relationships by creating role clarity and creatively supporting one another.</p>\n<p>One-on-ones: Set your teammates up for success during your one-on-one meetings!</p>\n<p>Hiring: Build consistent, repeatable, and equitable interviews and onboarding plans.</p>\n<p>Meetings: Support participants, hone the content, and nail the meeting goal.</p>\n<p>Feedback &amp;amp; performance reviews: Everyone deserves clear, actionable feedback!</p>\n<p>Communication &amp;amp; team dynamics: Plan ahead, facilitate well, and create clarity.</p>\n<p>Adapting your approach: As your work context and team evolves, your leadership approach will need to evolve, too.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"What are Distance Metrics in Vector Search?","contentType":"bookmarks","date":"2022-09-21 12:44","url":"/responses/distance-metrics-vector-search/","content":"<div class=\"response-target\" >→ <a href=\"https://weaviate.io/blog/2022/09/Distance-Metrics-in-Vector-Search.html\" >https://weaviate.io/blog/2022/09/Distance-Metrics-in-Vector-Search.html</a></div><div class=\"response-content\" ><p>Metrics covered:</p>\n<ul>\n<li>Cosine</li>\n<li>Dot Product</li>\n<li>L2-Squared</li>\n<li>Manhattan</li>\n<li>Hamming</li>\n</ul>\n</div>\n","tags":[]},{"title":"1% Rule","contentType":"bookmarks","date":"2022-09-21 12:39","url":"/responses/1-percent-rule/","content":"<div class=\"response-target\" >→ <a href=\"https://en.wikipedia.org/wiki/1%25_rule\" >https://en.wikipedia.org/wiki/1%25_rule</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...about 1% of Internet users create content, while 99% are just consumers of that content</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"What I've Learned From Users","contentType":"bookmarks","date":"2022-09-20 21:21","url":"/responses/what-ive-learned-from-users/","content":"<div class=\"response-target\" >→ <a href=\"http://paulgraham.com/users.html\" >http://paulgraham.com/users.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>What have I learned from YC's users, the startups we've funded?</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...most startups have the same problems.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...the batch that broke YC was a powerful demonstration of how individualized the process of advising startups has to be.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...founders can be [bad] at realizing what their problems are. Founders will sometimes come in to talk about some problem, and we'll discover another much bigger one in the course of the conversation.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Often founders know what their problems are, but not their relative importance.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Focus is doubly important for early stage startups, because not only do they have a hundred different problems, they don't have anyone to work on them except the founders. If the founders focus on things that don't matter, there's no one focusing on the things that do.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Speed defines startups. Focus enables speed. YC improves focus.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>Why are founders uncertain about what to do? Partly because startups almost by definition are doing something new, which means no one knows how to do it yet, or in most cases even what &amp;quot;it&amp;quot; is.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"How to Ditch Facebook Without Losing Your Friends","contentType":"bookmarks","date":"2022-09-20 20:30","url":"/responses/ditch-facebook-without-losing-friends/","content":"<div class=\"response-target\" >→ <a href=\"https://www.eff.org/deeplinks/2022/09/how-ditch-facebook-without-losing-your-friends-or-family-customers-or-communities\" >https://www.eff.org/deeplinks/2022/09/how-ditch-facebook-without-losing-your-friends-or-family-customers-or-communities</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>disgruntled Facebook users keep using the service because they don’t want to leave behind their friends, family, communities and customers.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>“How to Ditch Facebook Without Losing Your Friends” explains the rationale behind these proposals - and offers a tour of what it would be like to use a federated, interoperable Facebook, from setting up your account to protecting your privacy and taking control of your own community’s moderation policies, overriding the limits and permissions that Facebook has unilaterally imposed on its users.</p>\n</blockquote>\n<ul>\n<li><a href=\"https://archive.org/details/interoperable-facebook\">Video</a></li>\n<li><a href=\"https://archive.org/details/interoperable-facebook-fictional-scenarios\">Highlight reel</a></li>\n<li><a href=\"https://www.eff.org/files/2022/09/12/interoperablefacebook-2022_0.pdf\">PDF</a></li>\n</ul>\n</div>\n","tags":[]},{"title":"Firefox Profile Maker","contentType":"bookmarks","date":"2022-09-20 20:27","url":"/responses/firefox-profile-maker/","content":"<div class=\"response-target\" >→ <a href=\"https://ffprofile.com/\" >https://ffprofile.com/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This tool will help you to create a Firefox profile with the defaults you like.</p>\n<p>You select which features you want to enable and disable and in the end you get a download link for a zip-file with your profile template.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"WordPress+IndieWeb as the OS of the Open Social Web","contentType":"bookmarks","date":"2022-09-16 11:44","url":"/responses/wordpress-indieweb-os-open-social-web/","content":"<div class=\"response-target\" >→ <a href=\"https://www.zylstra.org/blog/2022/09/wordpressindieweb-as-the-os-of-the-open-social-web/\" >https://www.zylstra.org/blog/2022/09/wordpressindieweb-as-the-os-of-the-open-social-web/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...2022 Netherlands WordCamp edition in Arnhem [presentation] on turning all WordPress sites into fully IndieWeb enabled sites. Meaning turning well over a third of the web into the open social web. Outside all the silos.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"The Notecard System","contentType":"bookmarks","date":"2022-09-14 09:29","url":"/responses/rh-notecard-system/","content":"<div class=\"response-target\" >→ <a href=\"https://ryanholiday.net/the-notecard-system-the-key-for-remembering-organizing-and-using-everything-you-read/\" >https://ryanholiday.net/the-notecard-system-the-key-for-remembering-organizing-and-using-everything-you-read/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Lisp is a chameleon","contentType":"bookmarks","date":"2022-09-14 09:17","url":"/responses/lisp-is-a-chameleon/","content":"<div class=\"response-target\" >→ <a href=\"http://www.paulgraham.com/chameleon.html\" >http://www.paulgraham.com/chameleon.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>When Lisp adopts a new paradigm it not only replicates existing practice, but goes beyond it to become a testbed for advancing the state of the art. Why has Lisp been able to adapt so easily when other languages have not? One reason is that Lisp is a programmable programming language.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"MOOC - 850+ Free Courses","contentType":"bookmarks","date":"2022-09-14 09:14","url":"/responses/850-free-mooc-courses/","content":"<div class=\"response-target\" >→ <a href=\"https://www.freecodecamp.org/news/new-online-courses/\" >https://www.freecodecamp.org/news/new-online-courses/</a></div><div class=\"response-content\" ><p>Subjects:</p>\n<ul>\n<li>Science</li>\n<li>Programming</li>\n<li>Education &amp;amp; Teaching</li>\n<li>Health &amp;amp; Medicine</li>\n<li>Social Sciences</li>\n<li>Business</li>\n<li>Information Security (InfoSec)</li>\n<li>Mathematics</li>\n<li>Computer Science</li>\n<li>Data Science</li>\n<li>Personal Development</li>\n<li>Humanities</li>\n<li>Engineering</li>\n<li>Art &amp;amp; Design</li>\n</ul>\n</div>\n","tags":[]},{"title":"Open Building Institute","contentType":"bookmarks","date":"2022-09-11 11:58","url":"/responses/open-building-institute/","content":"<div class=\"response-target\" >→ <a href=\"https://www.openbuildinginstitute.org/\" >https://www.openbuildinginstitute.org/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>The OBI system is open source, collaborative and distributed.</p>\n<p>Our focus is on low cost and rapidly-built structures that are modular, ecological, and energy efficient.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Metablogging","contentType":"bookmarks","date":"2022-09-09 09:49","url":"/responses/metablogging-ideaspace/","content":"<div class=\"response-target\" >→ <a href=\"https://ideaspace.substack.com/p/metablogging\" >https://ideaspace.substack.com/p/metablogging</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...internally public blogs written by members of the...squad detailing what they’re working on and thinking about.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Coding for Economists","contentType":"bookmarks","date":"2022-09-08 19:22","url":"/responses/coding-for-economists/","content":"<div class=\"response-target\" >→ <a href=\"https://aeturrell.github.io/coding-for-economists/intro.html\" >https://aeturrell.github.io/coding-for-economists/intro.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...a guide for economists on what programming is, why it’s useful, and how to do it.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":" google/haskell-trainings","contentType":"bookmarks","date":"2022-09-06 20:16","url":"/responses/google-haskell-trainings/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/google/haskell-trainings\" >https://github.com/google/haskell-trainings</a></div><div class=\"response-content\" ><p>This repository contains the source for the slides and the exercises used in the Haskell trainings at Google.</p>\n</div>\n","tags":[]},{"title":"Open Source Alternative To","contentType":"bookmarks","date":"2022-09-06 18:51","url":"/responses/open-source-alternativeto/","content":"<div class=\"response-target\" >→ <a href=\"https://www.opensourcealternative.to/\" >https://www.opensourcealternative.to/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Discover 350+ popular open source alternatives to your proprietary SaaS.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Open Library","contentType":"bookmarks","date":"2022-09-05 18:29","url":"/responses/open-library/","content":"<div class=\"response-target\" >→ <a href=\"https://openlibrary.org/\" >https://openlibrary.org/</a></div><div class=\"response-content\" ><p>Open Library is an initiative of the Internet Archive, a 501(c)(3) non-profit, building a digital library of Internet sites and other cultural artifacts in digital form.</p>\n</div>\n","tags":[]},{"title":"A Deep Dive into Transformers with TensorFlow and Keras: Part 1","contentType":"bookmarks","date":"2022-09-05 12:05","url":"/responses/deep-dive-transformers-tf-keras-pt1/","content":"<div class=\"response-target\" >→ <a href=\"https://pyimagesearch.com/2022/09/05/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-1/\" >https://pyimagesearch.com/2022/09/05/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-1/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"The Feed Directory - Pine.blog","contentType":"bookmarks","date":"2022-09-04 22:27","url":"/responses/pine-blog-feed-directory/","content":"<div class=\"response-target\" >→ <a href=\"https://pine.blog/search\" >https://pine.blog/search</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Find feeds for all of your favorite sites and keep up with everything they post!</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Taking Note: How to Create Commomplace with Evernote","contentType":"bookmarks","date":"2022-09-04 20:18","url":"/responses/how-to-create-commonplace-book-evernote/","content":"<div class=\"response-target\" >→ <a href=\"https://evernote.com/blog/how-to-create-commonplace-with-evernote/\" >https://evernote.com/blog/how-to-create-commonplace-with-evernote/</a></div><div class=\"response-content\" >\n<blockquote class=\"blockquote\">\n<p>Commonplace is an ideal medium for the curation and cultivation of intellectual ideas, thoughts, and knowledge. It’s also a proven and timeless system, tested by folks from a spectrum of backgrounds including authors, professors, and scientists.</p>\n</blockquote>\n<ul>\n<li>Remember things</li>\n<li>Write to recall</li>\n<li>Understand reading</li>\n<li>Personal reference system</li>\n<li>Filter ideas</li>\n<li>Unleash creativity</li>\n</ul>\n\n<blockquote class=\"blockquote\">\n<p>The most profound power of your commonplace is being able to thumb through reading and review material. Your commonplace book isn’t just a filing cabinet—it’s an evolving record of your life and observations.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Buck Rogers in the 25th Century A.D.","contentType":"bookmarks","date":"2022-09-04 18:01","url":"/responses/buck-rogers-25-century-ad/","content":"<div class=\"response-target\" >→ <a href=\"https://buckrogers.neocities.org/\" >https://buckrogers.neocities.org/</a></div><div class=\"response-content\" ></div>\n","tags":[]},{"title":"Small Web","contentType":"bookmarks","date":"2022-09-02 00:03","url":"/responses/small-web-jayless-wiki/","content":"<div class=\"response-target\" >→ <a href=\"https://www.jayeless.net/wiki/small-web.html\" >https://www.jayeless.net/wiki/small-web.html</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...the kind of web they define themselves against; that kind of bloated, corporate, algorithm-ruled and ad-ridden mess that constitutes the majority of highly-trafficked websites these day.</p>\n</blockquote>\n<blockquote class=\"blockquote\">\n<p>...for me, the term “Small Web” refers to a couple of main things: independence from tech giants, and websites that are lightweight and high-performance.</p>\n</blockquote>\n<p>Related concepts:</p>\n<ul>\n<li>A late 90s-style, hand-crafted web</li>\n<li>Alternative protocols, like Gemini</li>\n<li>An independent web</li>\n</ul>\n</div>\n","tags":[]},{"title":"Blot - Blogging Platform","contentType":"bookmarks","date":"2022-08-30 22:20","url":"/responses/blot-blogging-platform/","content":"<div class=\"response-target\" >→ <a href=\"https://blot.im/\" >https://blot.im/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Blot is a blogging platform with no interface. It turns a folder into a website.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"osmlab/awesome-openstreetmap","contentType":"bookmarks","date":"2022-08-12 08:12","url":"/responses/osmlab-awesome-osm/","content":"<div class=\"response-target\" >→ <a href=\"https://github.com/osmlab/awesome-openstreetmap\" >https://github.com/osmlab/awesome-openstreetmap</a></div><div class=\"response-content\" ><p>Curated list of awesome OpenSteetMap-projects</p>\n</div>\n","tags":[]},{"title":"The Yesterweb","contentType":"bookmarks","date":"2022-08-03 22:24","url":"/responses/yesterweb/","content":"<div class=\"response-target\" >→ <a href=\"https://yesterweb.org/\" >https://yesterweb.org/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>This site is dedicated to a community (and a larger movement) about the internet how it's changed. We are creating, discovering and enjoying websites and digital spaces.</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"Reimagine the internet","contentType":"bookmarks","date":"2022-08-03 21:21","url":"/responses/columbia-reimagine-internet/","content":"<div class=\"response-target\" >→ <a href=\"https://knightcolumbia.org/events/reimagine-the-internet\" >https://knightcolumbia.org/events/reimagine-the-internet</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>A virtual conference exploring what the internet could become over the next decade</p>\n</blockquote>\n<p>Sessions:</p>\n<ul>\n<li>Pioneering Alternative Models for Community on the Internet</li>\n<li>Misinformation, Disinformation, and Media Literacy in a Less-Centralized Social Media Universe</li>\n<li>Interoperability and Alternative Social Media</li>\n<li>Lessons from Experiments in Local Community-Building</li>\n<li>Deplatforming and Innovation</li>\n<li>New Directions in Social Media Research</li>\n</ul>\n</div>\n","tags":[]},{"title":"MasterWiki","contentType":"bookmarks","date":"2022-08-03 18:48","url":"/responses/masterwiki/","content":"<div class=\"response-target\" >→ <a href=\"https://masterwiki.how/\" >https://masterwiki.how/</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>...masterWiki is the direct adaptation of MasterClass' video courses translated into wikiHow-style how-to guides...</p>\n</blockquote>\n</div>\n","tags":[]},{"title":"The Optimal iPhone Settings and Best Apps for Productivity, Focus, and Health","contentType":"bookmarks","date":"2022-08-01 17:50","url":"/responses/iphone-settings-focus-productivity-health/","content":"<div class=\"response-target\" >→ <a href=\"https://betterhumans.pub/how-to-set-up-your-iphone-for-productivity-focus-and-your-own-longevity-bb27a68cc3d8\" >https://betterhumans.pub/how-to-set-up-your-iphone-for-productivity-focus-and-your-own-longevity-bb27a68cc3d8</a></div><div class=\"response-content\" ><blockquote class=\"blockquote\">\n<p>Here’s how to read the post:</p>\n<ul>\n<li>Level 1 — Casual. Read the headlines — figure out the details yourself. Most of this isn’t rocket science.</li>\n<li>Level 2 — Tutorial. Read the steps underneath the headline. I’ve spelled out every step so that you can save your brain power for something else.</li>\n<li>Level 3 — Productivity Nerd. Below the tutorial steps, I’ve included discussion of the behavior design implications. This is for true productivity nerds, i.e. the readers of Better Humans.</li>\n</ul>\n</blockquote>\n<p>Optimize First for Single Tasking</p>\n<p>#1. Turn OFF (almost) all notifications<br />\n#2. Hide social media slot machines<br />\n#3. Hide messaging slot machines<br />\n#4. Disable app review requests<br />\n#5. Turn on Do Not Disturb<br />\n#6. Be strategic about your wallpaper<br />\n#7. Turn off Raise to Wake<br />\n#8. Add the Screen Time widget<br />\n#9. Add Content Restrictions<br />\n#10. (Optional) Use Restrictions to turn off Safari<br />\n#11. Organize your Apps and Folders alphabetically</p>\n<p>Switch to Google Cloud to Work Faster</p>\n<p>#12. Choose GMail<br />\n#13. Choose Google Calendar<br />\n#14. Replace Apple Maps with Google Maps<br />\n#15. Install the GBoard keyboard for faster typing<br />\n#16. Switch to Google Photos</p>\n<p>Install These Apps for Productivity</p>\n<p>#17. Use Evernote for all note taking, to-do lists, everything<br />\n#18. The Case for Calm as your go-to meditation app<br />\n#19. Install the right goal tracker for you<br />\n#20. Store all your passwords in a password manager, probably LastPass<br />\n#21. Use Numerical as your default calculator<br />\n#22. Put the Camera app in your toolbar<br />\n#23. Use this Doppler Radar app<br />\n#24. Use this Pomodoro app<br />\n#25. Use Brain.fm for background noise</p>\n<p>Use These Apps and Configurations for Deep Learning</p>\n<p>#26. Subscribe to these podcasts<br />\n#27. Install the Kindle app but never read it in bed<br />\n#28. Use Safari this way<br />\n#29. Organize your home screen for deep learning over shallow learning</p>\n<p>Use These Apps and Configurations for Longevity</p>\n<p>#30. Track steps this way<br />\n#31. Prefer Time Restricted Eating Over Calorie Counting<br />\n#32. Schedule Night Shift<br />\n#33. Set up Medical ID</p>\n<p>Make The Finishing Touches with These Configurations</p>\n<p>#34. Change Siri to a man<br />\n#35. Change your phone’s name<br />\n#36. Turn off advertising tracking<br />\n#37. Set auto-lock to the maximum time<br />\n#38. Set your personal hotspot password to a random three word phrase<br />\n#39. Turn on control center everywhere<br />\n#40. Turn on Background App Refresh<br />\n#41. Delete Garage Band<br />\n#42. Develop verbal memory for talking to Siri<br />\n#43. Set up these text replacement shortcuts<br />\n#44. Set your address<br />\n#45. Backup this way</p>\n</div>\n","tags":[]},{"title":"Open Source and Organization Workshop","contentType":"bookmarks","date":"2022-07-27 22:01","url":"/responses/open-source-organization-workshop/","content":"<div class=\"response-target\" >→ <a href=\"https://klemet.github.io/Workshop-Organization-EN/index.html\" >https://klemet.github.io/Workshop-Organization-EN/index.html</a></div><div class=\"response-content\" >\n<ul>\n<li>Discover and learn to use the basic functions of the following software:\n<ul>\n<li>Joplin</li>\n<li>Zotero</li>\n<li>Nextcloud</li>\n</ul>\n</li>\n<li>Learn how to use these different software programs together to manage information using the following methods:</li>\n<li>The Zettelkasten method</li>\n<li>The P.A.R.A method</li>\n<li>The inbox method</li>\n</ul>\n</div>\n","tags":[]},{"title":"Practical Deep Learning for Coders 2022","contentType":"bookmarks","date":"2022-07-26 21:11","url":"/responses/practical-deep-learning-coders-2022/","content":"<div class=\"response-target\" >→ <a href=\"https://www.fast.ai/2022/07/21/dl-coders-22/\" >https://www.fast.ai/2022/07/21/dl-coders-22/</a></div><div class=\"response-content\" ><p>A free course designed for people with some coding experience, who want to learn how to apply deep learning and machine learning to practical problems.</p>\n<p>Lessons</p>\n<ol>\n<li>Getting started</li>\n<li>Deployment</li>\n<li>Neural net foundations</li>\n<li>Natural Language (NLP)</li>\n<li>From-scratch Model</li>\n<li>Random forests</li>\n<li>Collaborative filtering and embeddings</li>\n<li>Convolutions (CNNs)</li>\n</ol>\n</div>\n","tags":[]}]</script><script type="application/json" id="remainingContentData-snippets">[{"title":"Webmentions Verification","contentType":"snippets","date":"09/25/2022 15:13 -05:00","url":"/resources/snippets/webmentions-verification/","content":"\n<p>Sample script that shows how to perform Webmention verification per <a href=\"https://www.w3.org/TR/webmention/#webmention-verification\">Webmentions specification</a>.</p>\n\n<pre><code class=\"language-bash\">dotnet fsi webmention-verification.fsx\n</code></pre>\n\n<p><strong>webmention-verification.fsx</strong></p>\n<pre><code class=\"language-fsharp\">// https://www.w3.org/TR/webmention/#webmention-verification\n<p>#r &amp;quot;nuget:FSharp.Data&amp;quot;\n#r &amp;quot;nuget: Microsoft.AspNetCore.WebUtilities, 2.2.0&amp;quot;</p>\n<p>open System\nopen System.Net\nopen System.Net.Http\nopen System.Net.Http.Headers\nopen System.Collections.Generic\nopen Microsoft.AspNetCore.WebUtilities\nopen FSharp.Data</p>\n<p>type WebmentionVerificationResult =\n| TaggedMention of {| Replies: bool; Likes: bool; Reposts: bool|}\n| UntaggedMention\n| Error of string</p>\n<p>let getFormContent (request:HttpRequestMessage) =\nasync {\nlet! content = request.Content.ReadAsStringAsync() |&amp;gt; Async.AwaitTask\nlet query = QueryHelpers.ParseQuery(content)\nlet source = query[&amp;quot;source&amp;quot;] |&amp;gt; Seq.head\nlet target = query[&amp;quot;target&amp;quot;] |&amp;gt; Seq.head</p>\n<pre><code>    return source,target\n}\n</code></pre>\n<p>let cont =<br />\ndict [\n(&amp;quot;source&amp;quot;,&amp;quot;https://raw.githubusercontent.com/lqdev/fsadvent-2021-webmentions/main/reply.html&amp;quot;)\n(&amp;quot;target&amp;quot;,&amp;quot;https://webmention.rocks/test/1&amp;quot;)\n]</p>\n<p>let buildSampleRequestMessage (content:IDictionary&amp;lt;string,string&amp;gt;) =</p>\n<pre><code>let reqMessage = new HttpRequestMessage()\nreqMessage.Content &amp;amp;lt;- new FormUrlEncodedContent(content)\n\nreqMessage\n</code></pre>\n<p>let req = buildSampleRequestMessage cont</p>\n<p>// verification</p>\n<p>let source,target =\nreq\n|&amp;gt; getFormContent\n|&amp;gt; Async.RunSynchronously</p>\n<p>let getMentionUsingCssSelector (doc:HtmlDocument) (selector:string) (target:string) =\ndoc.CssSelect(selector)\n|&amp;gt; List.map(fun x -&amp;gt; x.AttributeValue(&amp;quot;href&amp;quot;))\n|&amp;gt; List.filter(fun x -&amp;gt; x = target)</p>\n<p>let hasMention (mentions:string list) =\nmentions |&amp;gt; List.isEmpty |&amp;gt; not</p>\n<p>let verifyWebmentions (source:string) (target:string)=\nasync {\nuse client = new HttpClient()\nlet reqMessage = new HttpRequestMessage(new HttpMethod(&amp;quot;Get&amp;quot;), source)\nreqMessage.Headers.Accept.Clear()</p>\n<pre><code>    // Only accept text/html content\n    reqMessage.Headers.Accept.Add(new MediaTypeWithQualityHeaderValue(&amp;amp;quot;text/html&amp;amp;quot;))\n    \n    // Get document\n    let! res = client.SendAsync(reqMessage) |&amp;amp;gt; Async.AwaitTask\n    \n    // Verify webmention\n    let webmentions = \n        match res.IsSuccessStatusCode with \n        | true -&amp;amp;gt;\n            // Get document contents\n            let body = \n                async {\n                    return! res.Content.ReadAsStringAsync() |&amp;amp;gt; Async.AwaitTask\n                } |&amp;amp;gt; Async.RunSynchronously\n\n            // Parse document\n            let doc = HtmlDocument.Parse(body)\n\n            // Get links tagged as replies using microformats\n            let replies = \n                getMentionUsingCssSelector doc &amp;amp;quot;.u-in-reply-to&amp;amp;quot; target\n\n            // Get links tagged as likes using microformats\n            let likes = \n                getMentionUsingCssSelector doc &amp;amp;quot;.u-in-like-of&amp;amp;quot; target\n\n            // Get links tagged as repost using microformats\n            let shares = \n                getMentionUsingCssSelector doc &amp;amp;quot;.u-in-repost-of&amp;amp;quot; target\n\n            // Get untagged mentions\n            let mentions = \n                getMentionUsingCssSelector doc &amp;amp;quot;a&amp;amp;quot; target\n\n            // Collect all tagged mentions\n            let knownInteractions = \n                [replies;likes;shares] \n                |&amp;amp;gt; List.collect(id)\n\n            // Choose tagged mentions before untagged mentions\n            match knownInteractions.IsEmpty,mentions.IsEmpty with \n            | true,true -&amp;amp;gt; Error &amp;amp;quot;Target not mentioned&amp;amp;quot;\n            | true,false | false,false -&amp;amp;gt; \n                TaggedMention \n                    {|\n                        Replies = hasMention replies \n                        Likes = hasMention likes\n                        Reposts = hasMention shares\n                    |}\n            | false,true -&amp;amp;gt; UntaggedMention \n\n        | false -&amp;amp;gt; \n            Error &amp;amp;quot;Unable to get source&amp;amp;quot;\n    return webmentions            \n}\n</code></pre>\n<p>verifyWebmentions source target\n|&amp;gt; Async.RunSynchronously\n</code></pre></p>\n\n<pre><code class=\"language-text\">Interactions { \n    Likes = false\n    Replies = true\n    Shares = false }\n</code></pre>\n","tags":["f#","indieweb","webmentions","internet","web","social","interactive","script"]},{"title":"Webmentions Request Verification","contentType":"snippets","date":"09/23/2022 15:41 -05:00","url":"/resources/snippets/webmentions-request-verification/","content":"\n<p>Sample script that shows how to perform Webmention request verification per <a href=\"https://www.w3.org/TR/webmention/#request-verification\">Webmentions specification</a>.</p>\n\n<pre><code class=\"language-bash\">dotnet fsi request-verification.fsx\n</code></pre>\n\n<p><strong>request-verification.fsx</strong></p>\n<pre><code class=\"language-fsharp\">// https://www.w3.org/TR/webmention/#request-verification\n<p>// 1. Send response with 202 Accepted to acknowledge successful request\n// 2. DONE: Check that the protocol is http or https\n// 3. DONE: Source URL is different than Target URL\n// 4. DONE Check that Target URL is a valid resource</p>\n<p>#r &amp;quot;nuget: Microsoft.AspNetCore.WebUtilities, 2.2.0&amp;quot;</p>\n<p>open System\nopen System.Net\nopen System.Net.Http\nopen System.Collections.Generic\nopen Microsoft.AspNetCore.WebUtilities</p>\n<p>type RequestVerificationResult =\n| Ok of HttpRequestMessage\n| Error of string</p>\n<p>// Parse Form URL Encoded string\nlet getFormContent (request:HttpRequestMessage) =\nasync {\nlet! content = request.Content.ReadAsStringAsync() |&amp;gt; Async.AwaitTask\nlet query = QueryHelpers.ParseQuery(content)\nlet source = query[&amp;quot;source&amp;quot;] |&amp;gt; Seq.head\nlet target = query[&amp;quot;target&amp;quot;] |&amp;gt; Seq.head</p>\n<pre><code>    return source,target\n}\n</code></pre>\n<p>// Check protocol is HTTP or HTTPS\nlet checkProtocol (request: RequestVerificationResult) =\nmatch request with\n| Ok m -&amp;gt;\nlet source,target =\nasync {\nreturn! getFormContent(m)\n} |&amp;gt; Async.RunSynchronously</p>\n<pre><code>    let isProtocolValid = \n        match source.StartsWith(&amp;amp;quot;http&amp;amp;quot;),target.StartsWith(&amp;amp;quot;http&amp;amp;quot;) with\n        | true,true -&amp;amp;gt; Ok m\n        | true,false -&amp;amp;gt; Error &amp;amp;quot;Target invalid protocol&amp;amp;quot;\n        | false,true -&amp;amp;gt;  Error &amp;amp;quot;Source invalid protocol&amp;amp;quot;\n        | false,false -&amp;amp;gt; Error &amp;amp;quot;Source and Target invalid protocol&amp;amp;quot;\n\n    isProtocolValid\n| Error s -&amp;amp;gt; Error $&amp;amp;quot;{s}&amp;amp;quot;\n</code></pre>\n<p>// Check the URLs are not the same\nlet checkUrlsSame (request:RequestVerificationResult) =\nmatch request with\n| Ok m -&amp;gt;\nlet source,target =\nasync {\nreturn! getFormContent(m)\n} |&amp;gt; Async.RunSynchronously\nlet check =\nmatch source.Equals(target) with\n| true -&amp;gt; Error &amp;quot;Urls are the same&amp;quot;\n| false -&amp;gt;  Ok m\ncheck\n| Error s -&amp;gt; Error s</p>\n<p>// Helper functions\nlet uriIsMine (url:string) =\nlet uri = new Uri(url)\nuri.Host.Equals(&amp;quot;lqdev.me&amp;quot;) || uri.Host.Equals(&amp;quot;www.luisquintanilla.me&amp;quot;) || uri.Host.Equals(&amp;quot;luisquintanilla.me&amp;quot;)</p>\n<p>let isValid (url:string) (msg:HttpResponseMessage) =\nlet isMine = uriIsMine url\nisMine &amp;amp; msg.IsSuccessStatusCode</p>\n<p>// Check URL is a valid resource\n// Valid means, the URL is one of my domains and returns a non-400 or 500 HTML status code\nlet checkUrlValidResource (request:RequestVerificationResult) =\nmatch request with\n| Ok m -&amp;gt;\nlet res =\nasync {\nlet! source,target = getFormContent(m)\nuse client = new HttpClient()\nlet reqMessage = new HttpRequestMessage(HttpMethod.Head, target)\nlet! resp = client.SendAsync(reqMessage) |&amp;gt; Async.AwaitTask\nreturn isValid target resp\n} |&amp;gt; Async.RunSynchronously\nmatch res with\n| true -&amp;gt; Ok m\n| false -&amp;gt; Error &amp;quot;Target is not a valid resource&amp;quot;\n| Error s -&amp;gt; Error s</p>\n<p>// Combine validation steps into single function\nlet validate =\ncheckProtocol &amp;gt;&amp;gt; checkUrlsSame &amp;gt;&amp;gt; checkUrlValidResource</p>\n<p>// Test application\nlet buildSampleRequestMessages (content:IDictionary&amp;lt;string,string&amp;gt;) =</p>\n<pre><code>let reqMessage = new HttpRequestMessage()\nreqMessage.Content &amp;amp;lt;- new FormUrlEncodedContent(content)\n\nlet liftedReqMessage = Ok reqMessage\nliftedReqMessage\n</code></pre>\n<p>let sampleContent = [\ndict [\n(&amp;quot;source&amp;quot;,&amp;quot;http://lqdev.me&amp;quot;)\n(&amp;quot;target&amp;quot;,&amp;quot;http://lqdev.me&amp;quot;)\n]\ndict [\n(&amp;quot;source&amp;quot;,&amp;quot;http://://lqdev.me&amp;quot;)\n(&amp;quot;target&amp;quot;,&amp;quot;protocol://lqdev.me&amp;quot;)\n]<br />\ndict [\n(&amp;quot;source&amp;quot;,&amp;quot;http://lqdev.me&amp;quot;)\n(&amp;quot;target&amp;quot;,&amp;quot;http://github.com/lqdev&amp;quot;)\n]\ndict [\n(&amp;quot;source&amp;quot;,&amp;quot;http://github.com/lqdev&amp;quot;)\n(&amp;quot;target&amp;quot;,&amp;quot;http://lqdev.me&amp;quot;)\n]<br />\n]</p>\n<p>sampleContent\n|&amp;gt; List.map(buildSampleRequestMessages)\n|&amp;gt; List.map(validate)\n</code></pre></p>\n\n<pre><code class=\"language-text\">[  \n  Error &amp;quot;Urls are the same&amp;quot;; \n  Error &amp;quot;Target invalid protocol&amp;quot;;\n  Error &amp;quot;Target is not a valid resource&amp;quot;;\n  Ok\n    Method: GET, RequestUri: '&amp;lt;null&amp;gt;', Version: 1.1, Content: System.Net.Http.FormUrlEncodedContent, Headers:\n    {\n        Content-Type: application/x-www-form-urlencoded\n        Content-Length: 67\n    }\n    {\n        Content = System.Net.Http.FormUrlEncodedContent;\n        Headers = seq [];\n        Method = GET;\n        Options = seq [];\n        Properties = seq [];\n        RequestUri = null;\n        Version = 1.1;\n        VersionPolicy = RequestVersionOrLower;\n    }\n]\n</code></pre>\n","tags":["f#","indieweb","webmentions","internet","web","social","interactive","script"]},{"title":"Create a new Matrix user using the CLI","contentType":"snippets","date":"08/31/2022 19:40 -05:00","url":"/resources/snippets/create-matrix-user-cli/","content":"\n<p>Create a new user in your <a href=\"https://matrix.org/\">Matrix</a> <a href=\"https://github.com/matrix-org/synapse/\">Synapse homeserver</a> using <a href=\"https://manpages.debian.org/buster/matrix-synapse/register_new_matrix_user.1.en.html\">register_new_matrix_user</a> CLI utility. This is helpful when creating accounts on a homeserver where registrations are closed.</p>\n\n<p>N/A</p>\n\n<pre><code class=\"language-bash\">register_new_matrix_user --user user1 --password p@ssword --config homeserver-config.yaml\n</code></pre>\n<ul>\n<li><strong>--user</strong>: Local part of the new user. Will prompt if omitted.</li>\n<li><strong>--password</strong>: New password for user. Will prompt if omitted. Supplying the password on the command line is not recommended. Use the STDIN instead.</li>\n<li><strong>--config</strong>: Path to server config file containing the shared secret.</li>\n</ul>\n","tags":["bash","linux","matrix","synapse","communication"]},{"title":"Upgrade Matrix Synapse homeserver","contentType":"snippets","date":"08/31/2022 19:40 -05:00","url":"/resources/snippets/update-matrix-synapse-homeserver/","content":"\n<p>Upgrade a <a href=\"https://matrix.org/\">Matrix</a> <a href=\"https://github.com/matrix-org/synapse/\">Synapse homeserver</a> using pip. For more information, see the official article on <a href=\"https://matrix-org.github.io/synapse/develop/upgrade\">upgrading between Synapse versions</a>.</p>\n\n<pre><code class=\"language-bash\">./update-matrix-homeserver.sh\n</code></pre>\n\n<h3>update-matrix-homeserver.sh</h3>\n<pre><code class=\"language-bash\"># Initialize Python virtual environment\nsource ./env/bin/activate\n\n\n<p>pip install --upgrade matrix-synapse</p>\n\n<p>synctl restart</p>\n\n<p>curl http://localhost:8008/_synapse/admin/v1/server_version</p>\n\n<p>deactivate\n</code></pre></p>\n","tags":["matrix","synapse","homeserver","selfhost","python","internet","network"]},{"title":"Analyze RSS feeds with FSharp.Data XML Type Provider","contentType":"snippets","date":"08/30/2022 12:50 -05:00","url":"/resources/snippets/fsharp-data-rss-parser/","content":"\n<p>Use the <a href=\"https://fsprojects.github.io/FSharp.Data/library/XmlProvider.html\">FSharp.Data XML Type Provider</a> to load and analyze RSS feeds.</p>\n\n<pre><code class=\"language-bash\">dotnet fsi rss-parser.fsx \n</code></pre>\n\n<h3>rss-parser.fsx</h3>\n<pre><code class=\"language-fsharp\">// Install NuGet packages\n#r &amp;quot;nuget:FSharp.Data&amp;quot;\n<p>// Import NuGet packages\nopen System.Xml.Linq\nopen FSharp.Data</p>\n<p>// Define Rss type using XML Type Provider\ntype Rss = XmlProvider&amp;lt;&amp;quot;http://luisquintanilla.me/posts/index.xml&amp;quot;&amp;gt;</p>\n<p>// Load RSS feed using Rss type\nlet blogFeed = Rss.Load(&amp;quot;http://luisquintanilla.me/posts/index.xml&amp;quot;)</p>\n<p>// Get Feed Title\nblogFeed.Channel.Title</p>\n<p>// Get the 5 latest posts\nblogFeed.Channel.Items\n|&amp;gt; Array.sortByDescending(fun item -&amp;gt; item.PubDate)\n|&amp;gt; Array.take 5</p>\n<p>// Get the title and URL of 5 latest posts\nblogFeed.Channel.Items\n|&amp;gt; Array.sortByDescending(fun item -&amp;gt; item.PubDate)\n|&amp;gt; Array.take 5\n|&amp;gt; Array.map(fun item -&amp;gt; {|Title=item.Title;Url=item.Link|})\n</code></pre></p>\n","tags":["xml","dotnet","fsharp","typeprovider","rss"]},{"title":"OPML File Generator","contentType":"snippets","date":"08/14/2022 13:45 -05:00","url":"/resources/snippets/opml-file-generator/","content":"\n<p>Script to take information stored in a JSON file and converts it into OPML format. This works for RSS readers as well as podcast clients that support OPML import.</p>\n\n<pre><code class=\"language-bash\">dotnet fsi opml-generator.fsx &amp;quot;&amp;lt;feed-title&amp;gt;&amp;quot; &amp;quot;&amp;lt;author-url&amp;gt;&amp;quot; &amp;quot;&amp;lt;data-file-path&amp;gt;&amp;quot;\n</code></pre>\n<pre><code class=\"language-bash\">dotnet fsi opml-generator.fsx &amp;quot;My Blogroll&amp;quot; &amp;quot;http://lqdev.me&amp;quot; &amp;quot;blogroll.json&amp;quot;\n</code></pre>\n\n<h3>opml-generator.fsx</h3>\n<pre><code class=\"language-fsharp\">open System.IO\nopen System.Linq\nopen System.Text.Json\nopen System.Xml.Linq\n<p>type OpmlMetadata =\n{\nTitle: string\nOwnerId: string\n}</p>\n<p>type Outline =\n{\nTitle: string\nType: string\nHtmlUrl: string\nXmlUrl: string\n}</p>\n<p>let opmlFeed (head:XElement) =\nXElement(XName.Get &amp;quot;opml&amp;quot;,\nXAttribute(XName.Get &amp;quot;version&amp;quot;, &amp;quot;2.0&amp;quot;),\nhead,\nXElement(XName.Get &amp;quot;body&amp;quot;))</p>\n<p>let headElement (metadata:OpmlMetadata) =\nXElement(XName.Get &amp;quot;head&amp;quot;,\nXElement(XName.Get &amp;quot;title&amp;quot;, metadata.Title),\nXElement(XName.Get &amp;quot;ownerId&amp;quot;, metadata.OwnerId))</p>\n<p>let outlineElement (data:Outline) =\nXElement(XName.Get &amp;quot;outline&amp;quot;,\nXAttribute(XName.Get &amp;quot;title&amp;quot;, data.Title),\nXAttribute(XName.Get &amp;quot;text&amp;quot;, data.Title),<br />\nXAttribute(XName.Get &amp;quot;type&amp;quot;, data.Type),\nXAttribute(XName.Get &amp;quot;htmlUrl&amp;quot;, data.HtmlUrl),\nXAttribute(XName.Get &amp;quot;xmlUrl&amp;quot;, data.XmlUrl))</p>\n<p>let loadLinks (filePath:string) =\nFile.ReadAllText(filePath)\n|&amp;gt; fun x -&amp;gt; x |&amp;gt; JsonSerializer.Deserialize&amp;lt;Outline array&amp;gt;</p>\n<p>let buildOpmlFeed (title:string) (ownerId:string) (filePath:string) =\nlet fileName = Path.GetFileNameWithoutExtension(filePath)</p>\n<pre><code>let head = \n    {\n        Title=title\n        OwnerId=ownerId\n    }\n    |&amp;amp;gt; headElement\n\nlet links = filePath |&amp;amp;gt; loadLinks |&amp;amp;gt; Array.map(outlineElement) \n\nlet feed =  opmlFeed head \nfeed.Descendants(XName.Get &amp;amp;quot;body&amp;amp;quot;).First().Add(links)\nFile.WriteAllText($&amp;amp;quot;{fileName}.opml&amp;amp;quot;, feed.ToString())\n</code></pre>\n<p>let args = fsi.CommandLineArgs\nlet title = args[1]\nlet ownerId = args.[2]\nlet dataPath = args.[3]</p>\n<p>buildOpmlFeed title ownerId dataPath\n</code></pre></p>\n<h3>blogroll.json</h3>\n<pre><code class=\"language-json\">[\n    {\n        &amp;quot;Title&amp;quot;: &amp;quot;Blogroll.org&amp;quot;,\n        &amp;quot;Type&amp;quot;: &amp;quot;rss&amp;quot;,\n        &amp;quot;HtmlUrl&amp;quot;: &amp;quot;https://blogroll.org/&amp;quot;,\n        &amp;quot;XmlUrl&amp;quot;: &amp;quot;https://blogroll.org/feed/&amp;quot;\n    },\n    {\n        &amp;quot;Title&amp;quot;: &amp;quot;Cheapskate's Guide&amp;quot;,\n        &amp;quot;Type&amp;quot;: &amp;quot;rss&amp;quot;,\n        &amp;quot;HtmlUrl&amp;quot;: &amp;quot;https://cheapskatesguide.org/&amp;quot;,\n        &amp;quot;XmlUrl&amp;quot;: &amp;quot;https://cheapskatesguide.org/cheapskates-guide-rss-feed.xml&amp;quot;\n    },\n    {\n        &amp;quot;Title&amp;quot;: &amp;quot;JWZ&amp;quot;,\n        &amp;quot;Type&amp;quot;: &amp;quot;rss&amp;quot;,\n        &amp;quot;HtmlUrl&amp;quot;: &amp;quot;https://www.jwz.org/blog/&amp;quot;,\n        &amp;quot;XmlUrl&amp;quot;: &amp;quot;https://cdn.jwz.org/blog/feed/&amp;quot;\n    }    \n]\n</code></pre>\n","tags":["dotnet","fsharp","script"]},{"title":"Download Main Wikipedia Page as PDF","contentType":"snippets","date":"08/11/2022 19:19 -05:00","url":"/resources/snippets/curl-download-wikipedia-mainpage/","content":"\n<p>Download the main page of Wikipedia locally as a PDF</p>\n\n<pre><code class=\"language-bash\">curl -o Main_Page.pdf https://en.wikipedia.org/api/rest_v1/page/pdf/Main_Page\n</code></pre>\n\n<h3>script.sh</h3>\n<pre><code class=\"language-bash\">curl -o Main_Page.pdf https://en.wikipedia.org/api/rest_v1/page/pdf/Main_Page\n</code></pre>\n","tags":["bash","linux"]},{"title":"QR Code Generator","contentType":"snippets","date":"08/03/2022 20:07 -05:00","url":"/resources/snippets/qr-code-generator/","content":"\n<p>Script to generate a QR Code and save as PNG image from a URL</p>\n\n<pre><code class=\"language-bash\">dotnet fsi qr-code-generator.fsx &amp;quot;my-qr-code.svg&amp;quot; &amp;quot;https://twitter.com/user-profile&amp;quot;\n</code></pre>\n\n<h3>qr-code-generator.fsx</h3>\n<pre><code class=\"language-fsharp\">open System.Threading\n<p>printfn &amp;quot;Loading packages&amp;quot;</p>\n<p>#r &amp;quot;nuget:Net.Codecrete.QrCodeGenerator&amp;quot;</p>\n<p>Thread.Sleep(5000)</p>\n<p>printfn &amp;quot;Loaded QrCodeGenerator&amp;quot;</p>\n<p>open Net.Codecrete.QrCodeGenerator\nopen System.IO\nopen System.Text</p>\n<p>let createQrCode (savePath:string) (target:string) =\nlet qr = QrCode.EncodeText(target,QrCode.Ecc.High)\nlet svgString = qr.ToSvgString(4)\nFile.WriteAllText(savePath,svgString, Encoding.UTF8)</p>\n<p>let args = fsi.CommandLineArgs\nlet savePath = args.[1]\nlet target = args.[2]</p>\n<p>createQrCode savePath target\n</code></pre></p>\n","tags":["dotnet","f#","script"]}]</script><script type="application/json" id="remainingContentData-wiki">[{"title":"Pandoc","contentType":"wiki","date":"04/25/2024 22:00","url":"/resources/wiki/pandoc/","content":"\n<p>If you need to convert files from one markup format into another, pandoc is your swiss-army knife.</p>\n<ul>\n<li><a href=\"https://pandoc.org/\">Website</a></li>\n</ul>\n\n<ul>\n<li><a href=\"https://pandoc.org/demos.html\">List of recipes</a></li>\n</ul>\n<h3>Emacs org-mode to Microsoft Word</h3>\n<p>Personally, I like drafting and structuring documents in org-mode. However, for sharing and collaboration, that usually takes place in Microsoft Word. This is a command I use on a regular basis.</p>\n<pre><code class=\"language-bash\">pandoc -s -o &amp;lt;document-name&amp;gt;.docx &amp;lt;document-name&amp;gt;.org\n</code></pre>\n","tags":["pandoc","plaintext","docx","documents","word","org","emacs","utility","linux"]},{"title":"GitHub Actions for Azure","contentType":"wiki","date":"04/23/2024 23:04","url":"/resources/wiki/github-actions-azure/","content":"\n<p>How to use GitHub Actions with Azure</p>\n<ul>\n<li><a href=\"https://learn.microsoft.com/azure/developer/github/github-actions\">Documentation</a></li>\n</ul>\n<h3>Authenticate to Azure from GitHub</h3>\n<ul>\n<li><a href=\"https://learn.microsoft.com/azure/developer/github/connect-from-azure?tabs=azure-cli%2Clinux\">Documentation</a></li>\n</ul>\n<p>This workflow is useful when using the <a href=\"https://github.com/azure/login?tab=readme-ov-file#github-actions-for-deploying-to-azure\">az login action in GitHub Actions</a>.</p>\n","tags":["github","cicd","githubactions","azure"]},{"title":"Garbage Collections NixOS","contentType":"wiki","date":"03/10/2024 14:01","url":"/resources/wiki/nixos-garbage-collection/","content":"\n<p>One of the nice things about NixOS is, whenever you update the system, old configurations remain in place. That way if something were to go wrong, you can always revert back. However, there are tradeoffs. Each old configuration takes up space on your hard drive. If you're updating your system every week or every few weeks, this adds up. The following is a general guide on how to clean these up.</p>\n\n<p>This command deletes every generation, except the most current one.</p>\n<pre><code class=\"language-bash\">nix-env --delete-generations old\n</code></pre>\n<p>If you'd like to keep a few of the most recent ones. Say, from the last 14 days, you can pass an argument.</p>\n<pre><code class=\"language-bash\">nix-env --delete-generations 14d\n</code></pre>\n\n<pre><code class=\"language-bash\">nix-store --gc\n</code></pre>\n\n<p>The following utility will delete older generations across all profiles on your system.</p>\n<pre><code class=\"language-bash\">nix-collect-garbage -d\n</code></pre>\n\n<p><a href=\"https://nixos.org/manual/nix/stable/package-management/garbage-collection.html\">NixOS Garbage Collection</a></p>\n","tags":["nixos","system","os","linux"]},{"title":"Install .NET packages on NixOS from GitHub","contentType":"wiki","date":"12/17/2023 17:56","url":"/resources/wiki/nixos-dotnet-packages-source/","content":"\n<p>This guide shows how to build a .NET package from GitHub source. The project used to illustrate the process can be found on <a href=\"https://github.com/lqdev/fitch\">GitHub</a>.</p>\n\n<p>The first thing you'll want to do is create a derivation for your package.</p>\n<p>Create a file for your package. In this case, I called mine <em>fitch.nix</em>.</p>\n<p>Since I'm building a .NET package from the GitHub source, I use <code>buildDotnetModule</code>, <code>dotnetCorePackages</code>, and <code>fetchFromGithub</code>.</p>\n<p>To get the rev, use the <code>git log</code> command. It's the hash of the latest commit.</p>\n\n<pre><code class=\"language-nix\">{\n  fetchFromGitHub,\n  buildDotnetModule,\n  dotnetCorePackages\n}:\n<p>buildDotnetModule {\nname = &amp;quot;fitch&amp;quot;;</p>\n<p>src = fetchFromGitHub {\nowner = &amp;quot;lqdev&amp;quot;;\nrepo = &amp;quot;fitch&amp;quot;;\nrev = &amp;quot;e5fb91ddf57eb5611e0e313af29126e590cd149f&amp;quot;;\nsha256 = &amp;quot;&amp;quot;;\n};</p>\n<p>projectFile = &amp;quot;src/fitch.fsproj&amp;quot;;\nexecutables = &amp;quot;fitch&amp;quot;;\ndotnet-sdk = dotnetCorePackages.sdk_8_0;\ndotnet-runtime = dotnetCorePackages.runtime_8_0;\nnugetDeps = ./deps.nix;\n}\n</code></pre></p>\n<p>At this point the sha256 will be empty because you'll get it later in the process.</p>\n\n<pre><code class=\"language-nix\">let\n  pkgs = import &amp;lt;nixpkgs&amp;gt; { };\nin\n{\n  fitch = pkgs.callPackage ./fitch.nix { };\n}\n</code></pre>\n\n<p>Create a file called <code>deps.nix</code>. This will contain the dependencies you need for your application.</p>\n<pre><code class=\"language-bash\">touch deps.nix\n</code></pre>\n<p>Then, run <code>nix-build -A fitch.fetch-deps</code>. This should fail because of the empty SHA.</p>\n\n<p>The empty SHA will fail. However, the error message will contain the actual SHA value. Copy that and paste it into the sha256 property.</p>\n<pre><code class=\"language-nix\">{\n  lib    \n  fetchFromGitHub,\n  buildDotnetModule,\n  dotnetCorePackages\n}:\n<p>buildDotnetModule {\nname = &amp;quot;fitch&amp;quot;;</p>\n<p>src = fetchFromGitHub {\nowner = &amp;quot;lqdev&amp;quot;;\nrepo = &amp;quot;fitch&amp;quot;;\nrev = &amp;quot;e5fb91ddf57eb5611e0e313af29126e590cd149f&amp;quot;;\nsha256 = &amp;quot;74xddAUGQNVliVs5o3zQQEAUAoF9r8iBmiiBr4qrUgw=&amp;quot;;\n};</p>\n<p>projectFile = &amp;quot;src/fitch.fsproj&amp;quot;;\nexecutables = &amp;quot;fitch&amp;quot;;\ndotnet-sdk = dotnetCorePackages.sdk_7_0;\ndotnet-runtime = dotnetCorePackages.runtime_7_0;\nnugetDeps = ./deps.nix;\n}\n</code></pre></p>\n\n<p>Now that you have the correct SHA, try getting the dependencies again. These dependencies are created in a series of steps:</p>\n<ol>\n<li>Run the following command <code>sudo nix-build -A fitch.fetch-deps</code>. The result of this command is an executable script called <em>result</em>.</li>\n<li>Run the <em>result</em> script. <code>sudo ./result deps.nix</code>. This will generate the lockfile called <em>deps.nix</em> and is referenced by the <code>nugetDeps</code> property.</li>\n</ol>\n\n<p>Once everything is set up, run <code>nix-build -A fitch</code>. This will package and install the package onto your system.</p>\n\n<p>To add the newly built package to your <em>configuration.nix</em>, add it to your packages:</p>\n<pre><code class=\"language-nix\">environment.systemPackages = with pkgs; [\n    #...\n    (callPackage ./fitch.nix {})\n    #...\n];\n</code></pre>\n<p>Then, run <code>nixos-rebuild switch</code>. This should now install the package onto your PC.</p>\n\n<p>In my case, since I want fitch to run when the terminal starts, I edit my bash configuration in the <em>configuration.nix</em> file.</p>\n<pre><code class=\"language-nix\">programs.bash = {\n    interactiveShellInit = &amp;quot;fitch&amp;quot;;\n}\n</code></pre>\n\n<ul>\n<li><a href=\"https://ryantm.github.io/nixpkgs/languages-frameworks/dotnet/\">Dotnet Nixpkgs</a></li>\n<li><a href=\"https://wuffs.org/blog/joining-the-nixos-pyramid-scheme\">Joining the NixOS Pyramid Scheme</a></li>\n<li><a href=\"https://nix.dev/tutorials/learning-journey/packaging-existing-software\">Packaging existing software with Nix</a></li>\n</ul>\n","tags":["dotnet","nixos","linux","sysadmin",""]},{"title":"Plain text","contentType":"wiki","date":"10/14/2023 12:56","url":"/resources/wiki/plaintext/","content":"\n<p>This guide provides an overview of plain text and how it can be used for a variety of tasks and advantages over rich text and custom text formats.</p>\n\n<p>In general, plain text is text that isn't formatted and generally uses common characters.</p>\n\n<ul>\n<li>Simplicity: Focus on the content rather than the formatting or medium</li>\n<li>Portability: Any text editor can create and display plain text. That means you're not locked in to any particular tool or service.</li>\n<li>Local-first: Because you can create plain text files using any phone or computer, it means your files belong to you and you have access to them any time regardless of network connectivity. While you can upload and sync your files with your preferred cloud storage provider for redundancy and backup, it's not a requirement.</li>\n<li>Longevity: Computers have been able to create and display plain text for decades. Since you don't need proprietary file formats or tools to create and consume plain text, it's likely many decades from now you'll be able to read plain text.</li>\n</ul>\n\n<p>You can write plain text on its own. However, you can format your plain text to provide additional visual cues, information, and context. Some popular plain formats and markup languages include:</p>\n<ul>\n<li><a href=\"https://daringfireball.net/projects/markdown/\">Markdown</a></li>\n<li><a href=\"https://orgmode.org/worg/org-syntax.html\">Org</a></li>\n<li><a href=\"https://www.latex-project.org/\">LaTeX</a></li>\n</ul>\n\n<p>Below are a few popular plain text editors you can use to read and create plain text files:</p>\n<ul>\n<li><a href=\"https://www.gnu.org/software/emacs/\">Emacs</a></li>\n<li><a href=\"https://www.vim.org/\">vi/vim</a></li>\n<li><a href=\"https://code.visualstudio.com/\">VS Code</a></li>\n<li><a href=\"https://apps.microsoft.com/detail/9msmlrh6lzf3?hl=en-us&amp;amp;gl=US\">Notepad</a></li>\n</ul>\n\n<h3>Note taking</h3>\n<p>The easiest way to start taking notes using plain text is to open any text editor of choice and start capturing your thoughts.</p>\n<h3>Productivity / Personal Knowledge Management</h3>\n<p>An extension to note taking is personal knowlege management and productivity. Whether it's scheduling appointments, journaling, tracking habits (exercise, reading, meditation), or managing to-do lists, you can use plain text for these tasks. Some tools and formats that can help with that include:</p>\n<ul>\n<li><a href=\"https://orgmode.org/\">Emacs Org Mode</a></li>\n<li><a href=\"http://todotxt.org/\">todo.txt</a></li>\n</ul>\n<h3>Email</h3>\n<p>While e-mail clients support HTML, in general e-mail communication is predomenantly text-based. Therefore, plain text is all you need. If you wanted to only use plain text, e-mail clients enable you to configure them to only support plain text. While your HTML e-mails may look weird, there are some advantages.</p>\n<p>The <a href=\"https://useplaintext.email/\">use plaintext email guide</a> can provide guidance as to how to get started using plaintext only for e-mails</p>\n<h3>Personal finance</h3>\n<p>There are many apps for managing your personal finance. However, if you want to keep it simple, you can track your personal finances using plain text as well. Some tools that can help you do that include:</p>\n<ul>\n<li><a href=\"https://ledger-cli.org/\">Ledger</a></li>\n</ul>\n<h3>Databases</h3>\n<p>When you think of a database, you might think of a complicated system hosted in a server. While database systems are optimized for enterprise production workloads, at the end of the day, the purpose of a database is to store data. That means plain text can work for that too. Some tools that can help you here include:</p>\n<ul>\n<li><a href=\"https://www.gnu.org/software/recutils/\">GNU Recutils</a></li>\n</ul>\n<h3>Blogging and building websites</h3>\n<p>The more capable our computing devices have become, our software has suffered from inflation by becoming larger. However, larger doesn't mean the software is richer. If you're just trying to transmit information, you can upload your raw text files containing your blog posts or website to a server and they can be displayed in the browser. Examples of these sites include:</p>\n<ul>\n<li><a href=\"https://minim.blog/\">minim.blog</a></li>\n<li><a href=\"https://250kb.club/\">250kb club</a></li>\n<li><a href=\"https://512kb.club/\">512kb club</a></li>\n</ul>\n<p>Often you can build these websites with the help of static site generators such as. The <a href=\"https://jamstack.org/generators/\">Static Site Generators catalog</a> provides more details on popular ones.</p>\n<p>For hosting, you can do so for free using offerings like GitHub Pages. The <a href=\"https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll\">Setting up a GitHub Pages site with Jekyll</a> can help you get started there.</p>\n\n<ul>\n<li><a href=\"https://plaintext-productivity.net\">Plaintext Productivity</a></li>\n<li><a href=\"https://plaintextproject.online/articles.html\">The Plain Text Project</a></li>\n<li><a href=\"http://doc.norang.ca/org-mode.html\">Org Mode - Organize Your Life In Plain Text!</a></li>\n<li><a href=\"http://www.markwk.com/plain-text-life.html\">The Plain Text Life: Note Taking, Writing and Life Organization Using Plain Text Files</a></li>\n<li><a href=\"https://sive.rs/plaintext\">Write plain text files</a></li>\n</ul>\n","tags":["plaintext","productivity","email","notes","personalknowledgemanagement","pkm","wiki","calendar","finance","tools","databases","writing"]},{"title":"Running Waydroid on NixOS using X11","contentType":"wiki","date":"10/05/2023 22:25","url":"/resources/wiki/waydroid-nixos-x11/","content":"\n\n<ul>\n<li><a href=\"https://search.nixos.org/packages?channel=23.05&amp;amp;show=weston&amp;amp;from=0&amp;amp;size=50&amp;amp;sort=relevance&amp;amp;type=packages&amp;amp;query=weston\">weston</a> - Wayland compositor</li>\n</ul>\n\n<pre><code class=\"language-nix\">virtualisation = {\n    waydroid = {\n        enable = true;\n    };\n};\n<p>environment.systemPackages = with pkgs; [weston];\n</code></pre></p>\n<h3>Initialize Waydroid</h3>\n<p>This downloads the LineageOS image and installs it</p>\n<pre><code class=\"language-bash\"># Fetch WayDroid images.\n# You can add the parameters &amp;quot;-s GAPPS -f&amp;quot; to have GApps support.\nsudo waydroid init\n</code></pre>\n<h3>Start Waydroid</h3>\n<ol>\n<li><p>Open the terminal and start the waydroid container</p>\n<pre><code class=\"language-bash\">sudo systemctl start waydroid-container\n</code></pre>\n</li>\n<li><p>Check status of container</p>\n<pre><code class=\"language-bash\">sudo journalctl -u waydroid-container\n</code></pre>\n</li>\n<li><p>Open terminal and start weston</p>\n<pre><code class=\"language-bash\">weston\n</code></pre>\n</li>\n<li><p>In the weston window, open a terminal and start Waydroid sesson in the background</p>\n<pre><code class=\"language-bash\">waydroid session start &amp;amp;\n</code></pre>\n</li>\n<li><p>Start the UI</p>\n<pre><code class=\"language-bash\">waydroid show-full-ui\n</code></pre>\n</li>\n</ol>\n<h3>Stop Waydroid</h3>\n<ol>\n<li><p>Close weston window</p>\n</li>\n<li><p>Stop waydroid container</p>\n<pre><code class=\"language-bash\">sudo systemctl stop waydroid-container\n</code></pre>\n</li>\n</ol>\n<h4>Remove user data</h4>\n<p>Only do this if you don't want to persist anything from that container</p>\n<pre><code class=\"language-bash\"># Removing images and user data\nsudo rm -r /var/lib/waydroid/* ~/.local/share/waydroid\n</code></pre>\n<h3>Update Android</h3>\n<pre><code class=\"language-bash\">sudo waydroid upgrade\n</code></pre>\n<h3>General Usage</h3>\n<pre><code class=\"language-bash\"># Start Android UI\nwaydroid show-full-ui\n\n<p>waydroid app list</p>\n\n<p>waydroid app launch &amp;lt;application name&amp;gt;</p>\n\n<p>waydroid app install &amp;lt;/path/to/app.apk&amp;gt;</p>\n\n<p>sudo waydroid shell</p>\n\n<p>waydroid prop set persist.waydroid.width 608\n</code></pre></p>\n\n<ul>\n<li><a href=\"https://nixos.wiki/wiki/WayDroid\">NixOS Wiki - WayDroid</a></li>\n<li><a href=\"https://unix.stackexchange.com/questions/732485/script-to-start-android-lineageos-with-waydroid-in-an-x11-session\">Script to start LineageOS on X11</a></li>\n<li><a href=\"https://github.com/waydroid/waydroid/issues/195#issuecomment-953926526\">Any plan to support X11?</a></li>\n</ul>\n","tags":["android","linux","nixos","x11","waydroid"]},{"title":"Matrix Synapse Server Administration","contentType":"wiki","date":"06/11/2023 21:13","url":"/resources/wiki/matrix-synapse-admin/","content":"\n<p>The following is a guide for managing <a href=\"https://github.com/matrix-org/synapse\">Matrix Synapse</a> servers.</p>\n\n<ol>\n<li><p>Initialize Python virtual environment</p>\n<pre><code class=\"language-bash\">source env/bin/activate\n</code></pre>\n</li>\n<li><p>Stop server</p>\n<pre><code class=\"language-bash\">synctl stop\n</code></pre>\n</li>\n<li><p>Upgrade Synapse package</p>\n<pre><code class=\"language-bash\">pip install --upgrade matrix-synapse[postgres]\n</code></pre>\n</li>\n<li><p>Restart server</p>\n<pre><code class=\"language-bash\">synctl restart\n</code></pre>\n</li>\n<li><p>Deactivate Python virtual environment</p>\n<pre><code class=\"language-bash\">deactivate\n</code></pre>\n</li>\n<li><p>Check version</p>\n<pre><code class=\"language-bash\">curl http://localhost:8008/_synapse/admin/v1/server_version\n</code></pre>\n</li>\n</ol>\n\n<h3>Delete old media</h3>\n<ol>\n<li><p>Run the following command to clean local meadia older than 30 days.</p>\n<pre><code class=\"language-bash\">curl -X POST -H &amp;quot;Authorization: Bearer access_token&amp;quot; http://localhost:8008/_synapse/admin/v1/media/delete?before_ts=$(date +%s000 --date &amp;quot;30 days ago&amp;quot;)\n</code></pre>\n</li>\n<li><p>Start PSQL</p>\n<pre><code class=\"language-bash\">psql\n</code></pre>\n</li>\n<li><p>Connect to database</p>\n<pre><code class=\"language-sql\">\\c synapse\n</code></pre>\n</li>\n<li><p>Clean up unreferenced itemes</p>\n<pre><code class=\"language-bash\">VACUUM FULL;\n</code></pre>\n</li>\n</ol>\n<h3>Delete items in database</h3>\n<ol>\n<li><p>Start PSQL</p>\n<pre><code class=\"language-bash\">psql\n</code></pre>\n</li>\n<li><p>Connect to database</p>\n<pre><code class=\"language-sql\">\\c synapse\n</code></pre>\n</li>\n<li><p>Run the following SQL script</p>\n<pre><code class=\"language-sql\">DROP FUNCTION IF EXISTS synapse_clean_redacted_messages();\nCREATE FUNCTION synapse_clean_redacted_messages()\n    RETURNS void AS $$\n    DECLARE\n    BEGIN\n        UPDATE events SET content = '{}' FROM redactions AS rdc\n            WHERE events.event_id = rdc.redacts\n            AND (events.type = 'm.room.encrypted' OR events.type = 'm.room.message');\n    END;\n$$ LANGUAGE 'plpgsql';\n<p>DROP FUNCTION IF EXISTS synapse_get_server_name();\nCREATE FUNCTION synapse_get_server_name()\nRETURNS text AS $$\nDECLARE\n_someUser TEXT;\n_serverName TEXT;\nBEGIN\nselect user_id from account_data limit 1 INTO _someUser;\nselect regexp_replace(_someUser, '^.*:', ':') INTO _serverName;\nRETURN _serverName;\nEND;\n$$ LANGUAGE 'plpgsql';</p>\n<p>DROP FUNCTION IF EXISTS synapse_get_unused_rooms();\nCREATE FUNCTION synapse_get_unused_rooms()\nRETURNS TABLE(room_id TEXT) AS $$\nDECLARE\nBEGIN\nRETURN QUERY SELECT r.room_id FROM rooms AS r WHERE r.room_id NOT IN (\nSELECT DISTINCT(m.room_id) FROM room_memberships as m\nINNER JOIN current_state_events as c\nON m.event_id = c.event_id\nAND m.room_id = c.room_id\nAND m.user_id = c.state_key\nWHERE c.type = 'm.room.member'\nAND m.membership = 'join'\nAND m.user_id LIKE concat('%', synapse_get_server_name())\n);\nEND;\n$$ LANGUAGE 'plpgsql';</p>\n<p>DROP FUNCTION IF EXISTS synapse_clean_unused_rooms();\nCREATE FUNCTION synapse_clean_unused_rooms()\nRETURNS void AS $$\nDECLARE\n_count INT;\nBEGIN\nCREATE TEMP TABLE synapse_clean_unused_rooms__tmp\nON COMMIT DROP\nAS SELECT room_id FROM synapse_get_unused_rooms();</p>\n<pre><code>    SELECT COUNT(*) FROM synapse_clean_unused_rooms__tmp INTO _count;\n    RAISE NOTICE 'synapse_clean_unused_rooms() Cleaning up % unused rooms', _count;\n\n    DELETE FROM event_forward_extremities AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: event_forward_extremities';\n\n    DELETE FROM event_backward_extremities AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: event_backward_extremities';\n\n    DELETE FROM event_edges AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: event_edges';\n\n    DELETE FROM room_depth AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: room_depth';\n\n    DELETE FROM events AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: events';\n\n    DELETE FROM event_json AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: event_json';\n\n    DELETE FROM state_events AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: state_events';\n\n    DELETE FROM current_state_events AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: current_state_events';\n\n    DELETE FROM room_memberships AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: room_memberships';\n\n    DELETE FROM destination_rooms AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: destination_rooms';\n\n    DELETE FROM event_failed_pull_attempts AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: event_failed_pull_attempts';\n\n    DELETE FROM rooms AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: rooms';\n\n    DELETE FROM room_aliases AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: room_aliases';\n\n    DELETE FROM state_groups AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: state_groups';\n\n    DELETE FROM state_groups_state AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: state_groups_state';\n\n    DELETE FROM receipts_graph AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: receipts_graph';\n\n    DELETE FROM receipts_linearized AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: receipts_linearized';\n\n    DELETE FROM room_tags AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: room_tags';\n\n    DELETE FROM room_tags_revisions AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: room_tags_revisions';\n\n    DELETE FROM room_account_data AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: room_account_data';\n\n    DELETE FROM event_push_actions AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: event_push_actions';\n\n    DELETE FROM pusher_throttle AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: pusher_throttle';\n\n    DELETE FROM event_reports AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: event_reports';\n\n    DELETE FROM stream_ordering_to_exterm AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: stream_ordering_to_exterm';\n\n    DELETE FROM event_auth AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: event_auth';\n\n    DELETE FROM appservice_room_list AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y);\n    RAISE NOTICE 'DONE: appservice_room_list';\nEND;\n</code></pre>\n<p>$$ LANGUAGE 'plpgsql';</p>\n<p>SELECT synapse_clean_redacted_messages();\nSELECT synapse_clean_unused_rooms();\n</code></pre></p>\n</li>\n<li><p>Clean up unreferenced tuples</p>\n<pre><code class=\"language-sql\">VACUUM FULL;\n</code></pre>\n</li>\n</ol>\n\n<ul>\n<li><a href=\"https://matrix-org.github.io/synapse/latest/welcome_and_overview.html\">Matrix Synapse Docs</a></li>\n<li><a href=\"https://matrix-org.github.io/synapse/latest/upgrade.html\">Upgrading Synapse</a></li>\n<li><a href=\"https://matrix-org.github.io/synapse/v1.38/admin_api/media_admin_api.html\">Admin Media API</a></li>\n<li><a href=\"https://github.com/xwiki-labs/synapse_scripts\">Synapse Scripts</a></li>\n</ul>\n","tags":["messaging","how-to","matrix","self-host","synapse","sql","python"]},{"title":"Linux ls Commands","contentType":"wiki","date":"01/02/2023 15:22","url":"/resources/wiki/linux-ls-commands/","content":"\n<p>An overview of <code>ls</code> Linux commands</p>\n\n<p>Display information about a directory and its contents</p>\n\n<p>Display information about the CPU architecture</p>\n\n<p>Display information about GPU architecture</p>\n\n<p>Display information about block devices</p>\n\n<p>List ranges of available memory with their online status.</p>\n\n<p>Display information about PCI devices</p>\n\n<p>Display information about USB devices</p>\n","tags":["linux","commandline","cli","hardware","software"]},{"title":"Configure opacity - Manjaro","contentType":"wiki","date":"12/11/2022 18:56","url":"/resources/wiki/configure-opacity-manjaro/","content":"\n<p>This article shows how to configure background opacity for the kitty terminal emulator using picom and i3.</p>\n<p>By defaut, i3 doesn't support composting. Programs like picom add support for it.</p>\n<p>In addition, terminal emulators like kitty are hardware accelerated which means there's opportunities to offload from the CPU and save power.</p>\n\n<ol>\n<li><p>Install picom</p>\n<pre><code class=\"language-bash\">sudo pacman -S picom\n</code></pre>\n</li>\n<li><p>Move default configuration file to <em>~/.config/picom/picom.conf</em></p>\n</li>\n<li><p>Define opacity rule:</p>\n<pre><code class=\"language-bash\">opacity-rule = [&amp;quot;90:class_g = 'kitty'&amp;quot;];\n</code></pre>\n</li>\n<li><p>Save changes</p>\n</li>\n</ol>\n\n<ol>\n<li><p>Install picom</p>\n<pre><code class=\"language-bash\">sudo pacman -S kitty\n</code></pre>\n</li>\n<li><p>Move default configuration file to <em>~/.config/kitty/kitty.conf</em></p>\n</li>\n<li><p>Set background opacity:</p>\n<pre><code class=\"language-bash\">background_opacity 0.90\n</code></pre>\n</li>\n<li><p>Save changes</p>\n</li>\n</ol>\n\n<ol>\n<li><p>Open your i3 configuration and add the following line to it:</p>\n<pre><code class=\"language-bash\">exec_always picom\n</code></pre>\n</li>\n<li><p>Restart i3.</p>\n</li>\n</ol>\n\n<ul>\n<li><a href=\"https://wiki.archlinux.org/title/Picom#Opacity\">Picom</a></li>\n<li><a href=\"https://wiki.archlinux.org/title/Kitty\">Kitty</a></li>\n<li><a href=\"https://www.reddit.com/r/i3wm/comments/g62qy0/kitty_terminal_emulator_transparency_not_working/\">Kitty emulator not working - Reddit</a></li>\n</ul>\n","tags":["i3","wm","linux","manjaro","kitty","terminal","wiki","configuration","picom"]},{"title":"Intel GPU Tools - Linux","contentType":"wiki","date":"12/11/2022 18:05","url":"/resources/wiki/intel-gpu-tools/","content":"\n<p>This article talks about Intel GPU tools for Linux to monitor usage of integrated Intel GPUs</p>\n\n<p>The scenarios in which I've used this tool is to confirm that my browser is leveraging hardware acceleration. By default, Firefox doesn't enable hardware acceleration on Linux. As a result, everything runs on the CPU causing unnecessary load and faster battery drain. To alleviate that, you can enable hardware acceleration and make sure that the GPU is being used by running tools like <code>intel_gpu_top</code>.</p>\n\n<p>To install the tools, run the following command</p>\n<pre><code class=\"language-bash\">sudo pacman -S intel-gpu-tools\n</code></pre>\n\n<p>To view GPU activity, run the following command:</p>\n<pre><code class=\"language-bash\">sudo intel_gpu_top\n</code></pre>\n<p>This will display process graphs of GPU usage.</p>\n\n<ul>\n<li><a href=\"https://wiki.archlinux.org/title/Intel_graphics\">Intel Grapics - Arch Wiki</a></li>\n<li><a href=\"https://archlinux.org/packages/community/x86_64/intel-gpu-tools/\">Intel GPU Tools</a></li>\n<li><a href=\"https://support.mozilla.org/en-US/kb/upgrade-graphics-drivers-use-hardware-acceleration\">Firefox hardware acceleration</a></li>\n<li><a href=\"https://support.mozilla.org/en-US/kb/performance-settings\">Firefox Performance Settings</a></li>\n</ul>\n","tags":["linux","intel","gpu","manjaro","performances"]},{"title":"Hardware Video Acceleration - Linux","contentType":"wiki","date":"12/11/2022 17:36","url":"/resources/wiki/video-hardware-acceleration-linux/","content":"\n<p>This article talks about using hardware video acceleration so the video card encodes/decodes video offloading the CPU and saving power.</p>\n\n<p>To install the Intel media driver</p>\n<pre><code class=\"language-bash\">sudo pacman -S intel-media-driver\n</code></pre>\n\n<p>For mpv, make sure that decoding and video acceleration is enabled. To do so:</p>\n<ol>\n<li><p>Create or edit the user config file in <em>~/.config/mpv/mpv.conf</em> with the following settings:</p>\n<pre><code class=\"language-bash\">vo=gpu\nhwdec=auto\n</code></pre>\n</li>\n<li><p>Reboot your PC.</p>\n</li>\n</ol>\n\n<p>Use <code>intel_gpu_top</code> and make sure that when you play video using tools like VLC or MPV, the video bar is active.</p>\n\n<ul>\n<li><a href=\"https://wiki.archlinux.org/title/Hardware_video_acceleration\">Hardware Video Acceleration - Intel GPU</a></li>\n<li><a href=\"/resources/wiki/intel-gpu-tools\">Intel GPU Tools</a></li>\n<li><a href=\"https://github.com/mpv-player/mpv/blob/master/etc/mpv.conf\">Sample MPV Conf</a></li>\n<li><a href=\"https://mpv.io/manual/master/#configuration-files\">MPV Configuration Files</a></li>\n</ul>\n","tags":["performance","wiki","hardware","video","hardwareacceleration","linux","arch","manjaro"]},{"title":"Power Management Manjaro","contentType":"wiki","date":"12/11/2022 16:15","url":"/resources/wiki/power-management-manjaro/","content":"\n<p>The following shows how you can optimize your computer using Powertop to generate runing recommendations and apply those optimizations at boot using an rc-local service.</p>\n\n<ol>\n<li><p>Install powertop</p>\n<pre><code class=\"language-bash\">sudo pacman -S powertop\n</code></pre>\n</li>\n<li><p>Run powertop report (on battery power)</p>\n<pre><code>sudo powertop -r\n</code></pre>\n<p>After a few seconds, a file called <em>powertop.html</em> is created.</p>\n</li>\n<li><p>Open the <em>powertop.html</em> file in your browser and inspect the <strong>Tuning</strong> tab for tuning recommendations.</p>\n</li>\n</ol>\n\n<ol>\n<li><p>Create <em>rc.local</em> file in <code>/etc</code> directory with the following content</p>\n<pre><code class=\"language-bash\">#!/bin/sh -e\n<p>echo &amp;quot;Hello World&amp;quot;</p>\n\n<p>exit 0\n</code></pre></p>\n</li>\n<li><p>Create <em>rc-local.service</em> file in <code>/etc/systemd/system</code> directory with the following content</p>\n<pre><code class=\"language-bash\">#  This file is part of systemd.\n#\n#  systemd is free software; you can redistribute it and/or modify it\n#  under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n<p>[Unit]\nDescription=/etc/rc.local Compatibility\nConditionPathExists=/etc/rc.local</p>\n<p>[Service]\nType=oneshot\nExecStart=/etc/rc.local\nTimeoutSec=0\nStandardOutput=tty\nRemainAfterExit=yes\nSysVStartPriority=99</p>\n<p>[Install]\nWantedBy=multi-user.target\n</code></pre></p>\n</li>\n<li><p>Enable and start rc-local service</p>\n<pre><code class=\"language-bash\">sudo systemctl enable rc-local\nsudo systemctl start rc-local\n</code></pre>\n</li>\n<li><p>Confirm that the service has successfully started</p>\n<pre><code class=\"language-bash\">sudo systemctl status rc-local\n</code></pre>\n</li>\n</ol>\n\n<ul>\n<li><a href=\"https://wiki.archlinux.org/title/Powertop\">Arch Wiki - Powertop</a></li>\n</ul>\n","tags":["manjaro","bash","powermanagement","tech","wiki","linux"]},{"title":"Commonplace Book","contentType":"wiki","date":"09/04/2022 20:25","url":"/resources/wiki/commonplace-book/","content":"\n<p>A way to store notes and knowledge discovered and learned. This can include quotes from books, articles, courses, and any other sources of information.</p>\n\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Commonplace_book\">Commonplace Book</a></li>\n<li><a href=\"https://evernote.com/blog/how-to-create-commonplace-with-evernote/\">Taking Note: How To Create a Commonplace with Evernote</a></li>\n<li><a href=\"https://medium.com/read-smart/what-is-a-commonplace-book-db392bda730c\">What is a commonplace book?</a></li>\n<li><a href=\"https://stevenberlinjohnson.com/the-glass-box-and-the-commonplace-book-639b16c4f3bb\">The Glass Box and The Commonplace Book</a></li>\n</ul>\n","tags":["knowledgemanagement","commonplace","notetaking","information","technology","internet","books"]},{"title":"Markdig Advanced Extensions","contentType":"wiki","date":"09/03/2022 19:26","url":"/resources/wiki/markdig-advanced-extensions/","content":"\n<p>Markdig advanced extensions are a series of CommonMark compliant syntax extensions for the markdown language.</p>\n\n<p>Tasks lists allow you to create unorder list items with checkboxes.</p>\n<h3>Syntax</h3>\n<pre><code class=\"language-markdown\">- [ ] Item1\n- [x] Item2\n- [ ] Item3\n</code></pre>\n<h3>Output</h3>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"> Item1</li>\n<li class=\"task-list-item\"> Item2</li>\n<li class=\"task-list-item\"> Item3</li>\n</ul>\n\n<h3>Pipe tables</h3>\n<h4>Syntax</h4>\n<pre><code class=\"language-markdown\">| a | b | c\n| - | - | - \n| 1 | 2 | 3\n</code></pre>\n<h4>Output</h4>\n<p>a</p>\n<p>b</p>\n<p>c</p>\n<p>1</p>\n<p>2</p>\n<p>3</p>\n\n<p>Support for diagram syntax such as <a href=\"https://mermaid-js.github.io/mermaid/\">Mermaid</a>.</p>\n<h3>Syntax</h3>\n<pre><code class=\"language-markdown\">```mermaid\nflowchart LR\n    Start --&amp;gt; Stop\n```\n</code></pre>\n<h3>Output</h3>\n<pre><code class=\"language-mermaid\">flowchart LR;\n    Start --&amp;gt; Stop;\n</code></pre>\n\n<p>Support for emojis.</p>\n<h3>Syntax</h3>\n<pre><code class=\"language-markdown\">We all need :), it makes us :muscle:. (and :ok_hand:).\n</code></pre>\n<h3>Output</h3>\n<p>We all need 😃, it makes us 💪. (and 👌).</p>\n\n<ul>\n<li><a href=\"https://github.com/xoofx/markdig/tree/master/src/Markdig.Tests/Specs\">Markdig Specs</a></li>\n</ul>\n","tags":["markdown","markdig","dotnet","fsharp","csharp","website"]},{"title":"Unicode Emojis","contentType":"wiki","date":"09/03/2022 19:26","url":"/resources/wiki/unicode-emojis/","content":"\n<p>Unicode emoji characters and sequences.</p>\n\n<h3>Unicode</h3>\n<pre><code class=\"language-html\">&amp;lt;span&amp;gt;&amp;amp;#x1F642;&amp;lt;/span&amp;gt;\n</code></pre>\n<p><span>🙂</span></p>\n<h3>CLDR Name</h3>\n<pre><code class=\"language-markdown\">I am :slightly_smiling_face:\n</code></pre>\n<p>I am 🙂</p>\n\n<p>Emoji</p>\n<p>Unicode</p>\n<p>CLDR Name</p>\n<p>🙂</p>\n<p>U+1F642</p>\n<p><code>:slightly_smiling_face:</code></p>\n<p>😂</p>\n<p>U+1F602</p>\n<p><code>:joy:</code></p>\n<p>🙃</p>\n<p>U+1F643</p>\n<p><code>:upside_down_face:</code></p>\n<p>😑</p>\n<p>U+1F611</p>\n<p><code>:expressionless:</code></p>\n<p>🙄</p>\n<p>U+1F644</p>\n<p><code>:roll_eyes:</code></p>\n<p>🤮</p>\n<p>U+1F92E</p>\n<p><code>:vomiting_face:</code></p>\n<p>🤯</p>\n<p>U+1F92F</p>\n<p><code>:exploding_head:</code></p>\n<p>😕</p>\n<p>U+1F615</p>\n<p><code>:confused:</code></p>\n<p>🙁</p>\n<p>U+1F641</p>\n<p><code>:slightly_frowning_face:</code></p>\n<p>😮</p>\n<p>U+1F62E</p>\n<p><code>:open_mouth:</code></p>\n<p>😡</p>\n<p>U+1F621</p>\n<p><code>:rage:</code></p>\n<p>👻</p>\n<p>U+1F47B</p>\n<p><code>:ghost:</code></p>\n<p>👽</p>\n<p>U+1F47D</p>\n<p><code>:alien:</code></p>\n<p>🤖</p>\n<p>U+1F916</p>\n<p><code>:robot:</code></p>\n<p>❤️</p>\n<p>U+2764</p>\n<p><code>:heart:</code></p>\n<p>:bison:</p>\n<p>U+1F9AC</p>\n<p><code>:bison:</code></p>\n<p>:bagel:</p>\n<p>U+1F96F</p>\n<p><code>:bagel:</code></p>\n<p>🍿</p>\n<p>U+1F37F</p>\n<p><code>:popcorn:</code></p>\n<p>🍺</p>\n<p>U+1F37A</p>\n<p><code>:beer:</code></p>\n<p>🏜️</p>\n<p>U+1F3DC</p>\n<p><code>:desert:</code></p>\n<p>🚀</p>\n<p>U+1F680</p>\n<p><code>:rocket:</code></p>\n<p>🛸</p>\n<p>U+1F6F8</p>\n<p><code>:flying_saucer:</code></p>\n<p>⏰</p>\n<p>U+23F0</p>\n<p><code>:alarm_clock:</code></p>\n<p>🌌</p>\n<p>U+1F30C</p>\n<p><code>:milky_way:</code></p>\n<p>🔥</p>\n<p>U+1F525</p>\n<p><code>:fire:</code></p>\n<p>🎃</p>\n<p>U+1F383</p>\n<p><code>:jack_o_lantern:</code></p>\n<p>🏈</p>\n<p>U+1F3C8</p>\n<p><code>:football:</code></p>\n<p>🎧</p>\n<p>U+1F3A7</p>\n<p><code>:headphones:</code></p>\n<p>📻</p>\n<p>U+1F4FB</p>\n<p><code>:radio:</code></p>\n<p>💻</p>\n<p>U+1F4BB</p>\n<p><code>:computer:</code></p>\n<p>📖</p>\n<p>U+1F4D6</p>\n<p><code>:book:</code></p>\n<p>📓</p>\n<p>U+1F4D3</p>\n<p><code>:notebook:</code></p>\n<p>🔖</p>\n<p>U+1F516</p>\n<p><code>:bookmark:</code></p>\n<p>📰</p>\n<p>U+1F4F0</p>\n<p><code>:newspaper:</code></p>\n<p>🏷️</p>\n<p>U+1F3F7</p>\n<p><code>:label:</code></p>\n<p>✉️</p>\n<p>U+2709</p>\n<p><code>:envelope:</code></p>\n<p>✏️</p>\n<p>U+270F</p>\n<p><code>:pencil2:</code></p>\n<p>📝</p>\n<p>U+1F4DD</p>\n<p><code>:memo:</code></p>\n<p>📍</p>\n<p>U+1F4CD</p>\n<p><code>:round_pushpin:</code></p>\n<p>🔒</p>\n<p>U+1F512</p>\n<p><code>:lock:</code></p>\n<p>🔐</p>\n<p>U+1F510</p>\n<p><code>:closed_lock_with_key:</code></p>\n<p>🔗</p>\n<p>U+1F517</p>\n<p><code>:link:</code></p>\n<p>🙋‍♂️</p>\n<p>U+1F64B</p>\n<p><code>:raising_hand_man:</code></p>\n\n<p><a href=\"https://unicode.org/emoji/charts/full-emoji-list.html\">Full Emojis List, v14.0</a></p>\n","tags":["technology","web","unicode","internet","opensource","emojis"]},{"title":"Machine Learning","contentType":"wiki","date":"08/26/2022 08:15","url":"/resources/wiki/machine-learning/","content":"\n<p>Machine learning is a subset of artificial intelligence.</p>\n\n<ul>\n<li><a href=\"/resources/wiki/ml-frameworks\">Frameworks</a></li>\n</ul>\n\n<ul>\n<li><a href=\"https://landscape.lfai.foundation/\">LF AI &amp;amp; Data Foundation Interactive Landscape</a></li>\n</ul>\n","tags":["technology","machinelearning","artificialintelligence","index"]},{"title":"Machine Learning Frameworks","contentType":"wiki","date":"08/26/2022 08:10","url":"/resources/wiki/ml-frameworks/","content":"\n<p>Machine learning frameworks provide APIs for training and consuming machine learning models.</p>\n\n<ul>\n<li><a href=\"https://dot.net/ml\">ML.NET</a></li>\n<li><a href=\"https://scikit-learn.org/stable/index.html\">Scikit-Learn</a></li>\n<li><a href=\"https://www.tensorflow.org/\">TensorFlow</a></li>\n<li><a href=\"https://keras.io/\">Keras</a></li>\n<li><a href=\"https://pytorch.org/\">PyTorch</a></li>\n<li><a href=\"https://www.fast.ai/\">Fast.AI</a></li>\n<li><a href=\"https://onnx.ai/\">ONNX</a></li>\n<li><a href=\"https://huggingface.co/\">HuggingFace</a></li>\n</ul>\n","tags":["technology","artificialintelligence","machinelearning","software"]},{"title":"Privacy Guides","contentType":"wiki","date":"08/25/2022 21:30","url":"/resources/wiki/privacy-guides/","content":"\n<p>Privacy guides contain tips, information, and recommendations on physical and digital privacy.</p>\n\n<ul>\n<li><a href=\"https://www.techlore.tech/\">Techlore</a></li>\n<li><a href=\"https://www.privacyguides.org/\">Privacy Guides</a></li>\n<li><a href=\"https://www.techlore.tech/goincognito\">Go Incognito</a></li>\n<li><a href=\"https://privacy.do/\">Privacy.do</a></li>\n<li><a href=\"https://prism-break.org/\">PRISM Break</a></li>\n<li><a href=\"https://dt.gl/tag/privacy-cookbook/\">Decentralize Today - Privacy Cookbook</a></li>\n</ul>\n","tags":["technology","privacy"]},{"title":"Neofetch","contentType":"wiki","date":"08/25/2022 15:19","url":"/resources/wiki/neofetch/","content":"\n<p>Neofetch is a command-line system information tool. Neofetch displays information about your operating system, software and hardware.</p>\n\n<ol>\n<li><p>Open the terminal</p>\n</li>\n<li><p>Run the following command</p>\n<pre><code class=\"language-bash\">pacman -S neofetch\n</code></pre>\n</li>\n</ol>\n\n<p>The configuration file for neofetch is located at <em>$HOME/.config/neofetch/config.conf</em></p>\n<p>For more information on configuration options, see the <a href=\"https://github.com/dylanaraps/neofetch/wiki/Customizing-Info\">Customizing info article</a>.</p>\n\n<ol>\n<li><p>Open the terminal and enter the following command:</p>\n<pre><code class=\"language-bash\">neofetch\n</code></pre>\n</li>\n</ol>\n\n<ol>\n<li><p>Open your <em>~/.bashrc</em> file in your preferred text editor.</p>\n</li>\n<li><p>Add the following line to your file and save it.</p>\n<pre><code class=\"language-bash\">neofetch\n</code></pre>\n</li>\n<li><p>Refresh your terminal with the following command.</p>\n<pre><code class=\"language-bash\">source ~/.bashrc\n</code></pre>\n</li>\n</ol>\n\n<ul>\n<li><a href=\"https://github.com/dylanaraps/neofetch\">Neofetch GitHub Repository</a></li>\n</ul>\n","tags":["technology","linux","tools","terminal"]},{"title":"Privacy","contentType":"wiki","date":"08/25/2022 15:18","url":"/resources/wiki/privacy/","content":"\n<p>Privacy is the right of individuals to selectively disclose and share information about themselves.</p>\n\n<ul>\n<li><a href=\"/resources/wiki/privacy-guides\">Privacy Guides</a></li>\n<li><a href=\"/resources/wiki/alternative-frontends\">Alternative Frontends</a></li>\n</ul>\n\n<ul>\n<li><a href=\"https://www.inteltechniques.com/podcast.html\">Privacy, Security &amp;amp; OSINT Show</a></li>\n<li><a href=\"https://surveillancereport.tech/\">Surveillance Report</a></li>\n</ul>\n\n<ul>\n<li><a href=\"https://yewtu.be/channel/UCs6KfncB4OV6Vug4o_bzijg\">Techlore</a></li>\n</ul>\n","tags":["technology","privacy","index"]},{"title":"Wiki","contentType":"wiki","date":"08/25/2022 15:13","url":"/resources/wiki/wiki/","content":"\n<p>A wiki is a publication on subjects related to a particular topic. It often serves as a knowledge base.</p>\n\n<ul>\n<li><a href=\"https://www.wikipedia.org/\">Wikipedia</a></li>\n<li><a href=\"https://indieweb.org/wiki\">IndieWeb Wiki</a></li>\n<li><a href=\"https://wiki.archlinux.org/\">Arch Wiki</a></li>\n</ul>\n\n<ul>\n<li><a href=\"https://massive.wiki/\">Massive Wiki</a></li>\n<li><a href=\"https://indieweb.org/wiki-page\">IndieWeb Wikipage</a></li>\n<li><a href=\"https://hapgood.us/2015/10/17/the-garden-and-the-stream-a-technopastoral/\">The Garden and the Stream: A Technopastoral</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/List_of_wikis\">List of Wikis</a></li>\n</ul>\n","tags":["education","information","wiki","index"]},{"title":"Alternative Frontends","contentType":"wiki","date":"08/25/2022 15:05","url":"/resources/wiki/alternative-frontends/","content":"\n<p>Alternative frontends are third-party frontends that display information from web services like social media. These frontends are generally open-source and privacy-respecting.</p>\n\n<ul>\n<li><a href=\"https://nitter.net/\">Nitter</a> (Twitter)</li>\n<li><a href=\"https://yewtu.be\">Invidious</a> (YouTube)</li>\n<li><a href=\"http://teddit.net/\">Teddit</a> (Reddit)</li>\n</ul>\n","tags":["technology","internet","socialmedia","opensource","privacy"]}]</script><script type="application/json" id="remainingContentData-reviews">[{"title":"The Diaries of Adam and Eve","contentType":"reviews","date":"2025-01-19 18:45 -05:00","url":"/reviews/diaries-adam-eve-twain/","content":"<img src=\"https://prodimage.images-bn.com/pimages/9781420963748_p0_v1_s600x595.jpg\" alt=\"The Diaries of Adam and Eve cover\" class=\"book-cover\"><div class=\"author\" >by Mark Twain</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.7/5</div>\n","tags":[]},{"title":"The Holographic Universe: The Revolutionary Theory of Reality","contentType":"reviews","date":"2025-01-12 19:45 -05:00","url":"/reviews/holographic-universe-talbot/","content":"<img src=\"https://www.harpercollins.com/cdn/shop/files/9780062014108_7a36a0e4-28b5-4682-8f34-8e65c11c356e.jpg?v=1727712600&amp;amp;width=350\" alt=\"The Holographic Universe: The Revolutionary Theory of Reality cover\" class=\"book-cover\"><div class=\"author\" >by Michael Talbot</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.0/5</div>\n","tags":[]},{"title":"The Doors of Perception","contentType":"reviews","date":"2025-01-12 19:35 -05:00","url":"/reviews/doors-of-perception-huxley/","content":"<img src=\"https://prodimage.images-bn.com/pimages/9780061729072_p0_v4_s600x595.jpg\" alt=\"The Doors of Perception cover\" class=\"book-cover\"><div class=\"author\" >by Aldous Huxley</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.2/5</div>\n","tags":[]},{"title":"Concentration & Inner Awakening: According to Tibetan Buddhism","contentType":"reviews","date":"11/18/2024 17:54 -05:00","url":"/reviews/concentration-inner-awakening-tibetan-buddhism/","content":"<img src=\"https://prodimage.images-bn.com/pimages/9788196472900_p0_v1_s600x595.jpg\" alt=\"Concentration &amp;amp; Inner Awakening: According to Tibetan Buddhism cover\" class=\"book-cover\"><div class=\"author\" >by Fernand Schwarz</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 2.7/5</div>\n","tags":[]},{"title":"Soul Boom: Why We Need a Spiritual Revolution","contentType":"reviews","date":"11/18/2024 16:59 -05:00","url":"/reviews/soul-boom/","content":"<img src=\"https://www.hachettebookgroup.com/wp-content/uploads/2023/10/9780306828270.jpg?resize=683,1024\" alt=\"Soul Boom: Why We Need a Spiritual Revolution cover\" class=\"book-cover\"><div class=\"author\" >by Rainn Wilson</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.6/5</div>\n","tags":[]},{"title":"Jung: A Very Short Introduction","contentType":"reviews","date":"09/27/2024 07:47 -05:00","url":"/reviews/jung-very-short-introduction-stevens/","content":"<img src=\"https://covers.powells.com/9780192854582.jpg\" alt=\"Jung: A Very Short Introduction cover\" class=\"book-cover\"><div class=\"author\" >by Anthony Stevens</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.8/5</div>\n","tags":[]},{"title":"The Creative Act: A Way of Being","contentType":"reviews","date":"09/27/2024 07:40 -05:00","url":"/reviews/creative-act-way-of-being-rubin/","content":"<img src=\"https://images2.penguinrandomhouse.com/cover/9780593653425\" alt=\"The Creative Act: A Way of Being cover\" class=\"book-cover\"><div class=\"author\" >by Rick Rubin</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.8/5</div>\n","tags":[]},{"title":"Project Hail Mary","contentType":"reviews","date":"08/01/2024 20:12 -05:00","url":"/reviews/project-hail-mary/","content":"<img src=\"https://images2.penguinrandomhouse.com/cover/9780593135211\" alt=\"Project Hail Mary cover\" class=\"book-cover\"><div class=\"author\" >by Andy Weir</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 5.0/5</div>\n","tags":[]},{"title":"Simple: Conquering the Crisis of Complexity","contentType":"reviews","date":"07/14/2024 13:19 -05:00","url":"/reviews/simple-siegel-etzkorn/","content":"<img src=\"https://www.hachettebookgroup.com/wp-content/uploads/2024/01/9781455509683.jpg?resize=759,1024\" alt=\"Simple: Conquering the Crisis of Complexity cover\" class=\"book-cover\"><div class=\"author\" >by Alan Siegel and Irene Etzkorn</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 3.3/5</div>\n","tags":[]},{"title":"Eat That Frog!","contentType":"reviews","date":"07/09/2024 21:05 -05:00","url":"/reviews/eat-that-frog-tracy/","content":"<img src=\"https://images2.penguinrandomhouse.com/cover/9781626569416\" alt=\"Eat That Frog! cover\" class=\"book-cover\"><div class=\"author\" >by Brian Tracy</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 3.2/5</div>\n","tags":[]},{"title":"Desert Oracle: Volume 1","contentType":"reviews","date":"04/23/2024 22:03 -05:00","url":"/reviews/desert-oracle-vol-1/","content":"<img src=\"https://s3.amazonaws.com/mcdbookimages/books/cover_images/000/000/224/reg/9780374139681_FC.jpg?1650979924\" alt=\"Desert Oracle: Volume 1 cover\" class=\"book-cover\"><div class=\"author\" >by Ken Layne</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.5/5</div>\n","tags":[]},{"title":"Lurking: How a Person Became a User","contentType":"reviews","date":"04/23/2024 22:03 -05:00","url":"/reviews/lurking-how-person-becomes-user/","content":"<img src=\"https://s3.amazonaws.com/mcdbookimages/books/cover_images/000/000/155/reg/9780374716325_FC.jpg?1647524149\" alt=\"Lurking: How a Person Became a User cover\" class=\"book-cover\"><div class=\"author\" >by Joanne McNeil</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 3.9/5</div>\n","tags":[]},{"title":"Notes From The Undergrond","contentType":"reviews","date":"03/31/2024 19:31 -05:00","url":"/reviews/notes-from-the-underground-dostoyevsky/","content":"<img src=\"https://images3.penguinrandomhouse.com/cover/9781400041916\" alt=\"Notes From The Undergrond cover\" class=\"book-cover\"><div class=\"author\" >by Fyodor Dostoyevsky</div><div class=\"status\" >InProgress</div>\n","tags":[]},{"title":"Slow Productivity: The Lost Art of Accomplishment Without Burnout","contentType":"reviews","date":"03/23/2024 12:42 -05:00","url":"/reviews/slow-productivity-newport/","content":"<img src=\"https://images1.penguinrandomhouse.com/cover/9780593544853\" alt=\"Slow Productivity: The Lost Art of Accomplishment Without Burnout cover\" class=\"book-cover\"><div class=\"author\" >by Cal Newport</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.2/5</div>\n","tags":[]},{"title":"Agency","contentType":"reviews","date":"02/19/2024 15:19 -05:00","url":"/reviews/agency-gibson/","content":"<img src=\"https://images4.penguinrandomhouse.com/cover/9781101986943\" alt=\"Agency cover\" class=\"book-cover\"><div class=\"author\" >by William Gibson</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 2.5/5</div>\n","tags":[]},{"title":"The Declassification Engine: What History Reveals About America's Top Secrets","contentType":"reviews","date":"02/06/2024 18:52 -05:00","url":"/reviews/the-declassification-engine/","content":"<img src=\"https://images4.penguinrandomhouse.com/cover/9781101973677\" alt=\"The Declassification Engine: What History Reveals About America&amp;#39;s Top Secrets cover\" class=\"book-cover\"><div class=\"author\" >by Matthew Connelly</div><div class=\"status\" >InProgress</div>\n","tags":[]},{"title":"A City on Mars - Can we settle space, should we settle space, and have we really thought this through?","contentType":"reviews","date":"01/16/2024 19:52 -05:00","url":"/reviews/a-city-on-mars/","content":"<img src=\"https://images3.penguinrandomhouse.com/cover/9781984881724\" alt=\"A City on Mars - Can we settle space, should we settle space, and have we really thought this through? cover\" class=\"book-cover\"><div class=\"author\" >by Kelly Weinersmith and Zach Weinersmith</div><div class=\"status\" >Wishlist</div>\n","tags":[]},{"title":"How to Sell a Haunted House","contentType":"reviews","date":"01/16/2024 19:52 -05:00","url":"/reviews/how-to-sell-a-haunted-house/","content":"<img src=\"https://images4.penguinrandomhouse.com/cover/9780593201275\" alt=\"How to Sell a Haunted House cover\" class=\"book-cover\"><div class=\"author\" >by Grady Hendrix</div><div class=\"status\" >InProgress</div>\n","tags":[]},{"title":"Palo Alto - A History of California, Capitalism, and the World","contentType":"reviews","date":"01/15/2024 15:42 -05:00","url":"/reviews/palo-alto-history-california/","content":"<img src=\"https://www.hachettebookgroup.com/wp-content/uploads/2024/01/9780316592024-1.jpg\" alt=\"Palo Alto - A History of California, Capitalism, and the World cover\" class=\"book-cover\"><div class=\"author\" >by Malcom Harris</div><div class=\"status\" >Wishlist</div>\n","tags":[]},{"title":"Making It in America - The Almost Impossible Quest to Manufacture in the U.S.A. (And How It Got That Way)","contentType":"reviews","date":"01/15/2024 15:23 -05:00","url":"/reviews/making-it-in-america/","content":"<img src=\"https://images3.penguinrandomhouse.com/cover/9780593316887\" alt=\"Making It in America - The Almost Impossible Quest to Manufacture in the U.S.A. (And How It Got That Way) cover\" class=\"book-cover\"><div class=\"author\" >by Rachel Slade</div><div class=\"status\" >InProgress</div>\n","tags":[]},{"title":"Thrill Switch","contentType":"reviews","date":"12/27/2023 14:08 -05:00","url":"/reviews/thrill-switch/","content":"<img src=\"https://images.squarespace-cdn.com/content/v1/5875f6a815d5dba00bbed0ea/1671223905780-BSJU33CS2HOQO8MIPRDB/thrillswitch+cover.jpg?format=1000w\" alt=\"Thrill Switch cover\" class=\"book-cover\"><div class=\"author\" >by Tim Hawking</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.2/5</div>\n","tags":[]},{"title":"The Internet Con: How to Seize the Means of Computation ","contentType":"reviews","date":"09/28/2023 20:27 -05:00","url":"/reviews/the-internet-con-doctorow/","content":"<img src=\"https://covers.openlibrary.org/b/id/14418873-L.jpg\" alt=\"The Internet Con: How to Seize the Means of Computation  cover\" class=\"book-cover\"><div class=\"author\" >by Cory Doctorow</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.0/5</div>\n","tags":[]},{"title":"AHH! That's What I Call Horror: An Anthology of '90s Horror","contentType":"reviews","date":"06/18/2023 19:04 -05:00","url":"/reviews/ahh-thats-what-i-call-horror/","content":"<img src=\"https://images-na.ssl-images-amazon.com/images/S/compressed.photo.goodreads.com/books/1665505215i/62883181.jpg\" alt=\"AHH! That&amp;#39;s What I Call Horror: An Anthology of &amp;#39;90s Horror cover\" class=\"book-cover\"><div class=\"author\" >by Chelsea Pumpkins (Editor), S.E. Denton, Bridget D. Brave, Christopher O&amp;#39;Halloran, Caleb Stephens, Edith Lockwood, C.B. Jones, Mathew Wend, P.L. McMillan, Carson Winter, Patrick Barb, J.W. Donley, J.V. Gachs, Damien B.Raphael</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.1/5</div>\n","tags":[]},{"title":"Mycelium Running: How Mushrooms Can Help Save the World","contentType":"reviews","date":"05/20/2023 22:10 -05:00","url":"/reviews/mycelium-running/","content":"<img src=\"https://covers.openlibrary.org/b/id/838922-L.jpg\" alt=\"Mycelium Running: How Mushrooms Can Help Save the World cover\" class=\"book-cover\"><div class=\"author\" >by Paul Stamets</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.8/5</div>\n","tags":[]},{"title":"User Not Found","contentType":"reviews","date":"05/12/2023 21:17 -05:00","url":"/reviews/user-not-found/","content":"<img src=\"https://futuretensebooks.com/wp-content/uploads/2018/10/cover4.png\" alt=\"User Not Found cover\" class=\"book-cover\"><div class=\"author\" >by Felicity Fenton</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.0/5</div>\n","tags":[]},{"title":"Armageddon 2419 A.D.","contentType":"reviews","date":"01/22/2023 13:45 -05:00","url":"/reviews/armageddon-2419-ad/","content":"<img src=\"https://covers.openlibrary.org/b/id/12621838-L.jpg\" alt=\"Armageddon 2419 A.D. cover\" class=\"book-cover\"><div class=\"author\" >by Philip Francis Nowlan</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 3.2/5</div>\n","tags":[]},{"title":"Unfuck Your Writing: Write Better, Reach Readers, & Share Your Inner World","contentType":"reviews","date":"10/21/2022 20:57 -05:00","url":"/reviews/unfuck-your-writing/","content":"<img src=\"https://microcosmpublishing.com/previews/unfuck-your-writing_lg.jpg\" alt=\"Unfuck Your Writing: Write Better, Reach Readers, &amp;amp; Share Your Inner World cover\" class=\"book-cover\"><div class=\"author\" >by Joe Biel, Faith Harper</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 3.0/5</div>\n","tags":[]},{"title":"On Writing","contentType":"reviews","date":"10/09/2022 19:10 -05:00","url":"/reviews/on-writing/","content":"<img src=\"https://covers.openlibrary.org/b/id/11942411-M.jpg\" alt=\"On Writing cover\" class=\"book-cover\"><div class=\"author\" >by Stephen King</div><div class=\"status\" >InProgress</div>\n","tags":[]},{"title":"Building a Second Brain:A Proven Method to Organize Your Digital Life and Unlock Your Creative Potential","contentType":"reviews","date":"09/05/2022 18:19 -05:00","url":"/reviews/building-a-second-brain/","content":"<img src=\"https://covers.openlibrary.org/b/id/12372866-M.jpg\" alt=\"Building a Second Brain:A Proven Method to Organize Your Digital Life and Unlock Your Creative Potential cover\" class=\"book-cover\"><div class=\"author\" >by Tiago Forte</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 4.0/5</div>\n","tags":[]},{"title":"Hackers: Heroes of the Computer Revolution","contentType":"reviews","date":"09/05/2022 18:19 -05:00","url":"/reviews/hackers-heroes-computer-revolution/","content":"<img src=\"https://covers.openlibrary.org/b/id/10512955-M.jpg\" alt=\"Hackers: Heroes of the Computer Revolution cover\" class=\"book-cover\"><div class=\"author\" >by Steven Levy</div><div class=\"status\" >InProgress</div>\n","tags":[]},{"title":"Stand by for Mars!","contentType":"reviews","date":"09/05/2022 16:52 -05:00","url":"/reviews/stand-by-for-mars/","content":"<img src=\"https://covers.openlibrary.org/b/id/12622152-M.jpg\" alt=\"Stand by for Mars! cover\" class=\"book-cover\"><div class=\"author\" >by Carey Rockwell</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 3.8/5</div>\n","tags":[]},{"title":"Ten arguments for deleting your social media accounts right now","contentType":"reviews","date":"09/05/2022 16:52 -05:00","url":"/reviews/ten-arguments-delete-social-media/","content":"<img src=\"https://covers.openlibrary.org/b/id/12504011-M.jpg\" alt=\"Ten arguments for deleting your social media accounts right now cover\" class=\"book-cover\"><div class=\"author\" >by Jaron Lanier</div><div class=\"status\" >Read</div><div class=\"rating\">Rating: 3.2/5</div>\n","tags":[]}]</script><div class="load-more-section" id="loadMoreSection"><button class="load-more-btn" id="loadMoreBtn" type="button" data-chunk-size="10">Load More (1109 items remaining)</button><div class="loading-indicator hidden" id="loadingIndicator"><div class="loading-spinner"></div>Loading more content...</div></div></main><button class="back-to-top" id="backToTopBtn" type="button" title="Back to top" aria-label="Scroll back to top of page"><span class="icon" aria-hidden="true">↑</span></button></div></div></main><script src="/assets/lib/jquery/jquery.slim.min.js"></script><script src="/assets/lib/boostrap/bootstrap.min.js"></script><script src="/assets/lib/highlight/highlight.min.js"></script><script src="/assets/lib/highlight/highlight.fsharp.min.js"></script><script src="/assets/lib/highlight/highlight.nix.min.js"></script><script src="/assets/js/timeline.js"></script><script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script><script type="application/javascript">mermaid.initialize({startOnLoad:true});</script><script type="application/javascript">hljs.initHighlightingOnLoad();</script></body><footer><a rel="me" href="https://toot.lqdev.tech/@lqdev"></a><a rel="me" href="https://github.com/lqdev"></a><a rel="me" href="https://twitter.com/ljquintanilla"></a><a rel="me" href="https://www.linkedin.com/in/lquintanilla01/"></a><a rel="me" href="mailto:lqdev@outlook.com"></a></footer></html>