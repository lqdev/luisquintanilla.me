[{"id":"notes-test-github-mobile-post","title":"Testing my GH mobile publishing flow","content":"This is a post As far as posts go it\u0027s bold I can write code let x = 1 x * x To be or not to be Maybe one day we will Add responses Add bookmarks Add media posts Here are some attachments?","contentType":"notes","url":"https://www.lqdev.me/notes/test-github-mobile-post","date":"9/14/2025 9:50 PM -05:00","tags":["github","blogging","githubactions","cicd","indieweb","website"],"summary":"This is a post As far as posts go it\u0027s bold I can write code let x = 1 x * x To be or not to be Maybe one day we will Add responses Add bookmarks Add media posts Here are some attachments?","keywords":["posts","post","bold","write","code","maybe","responses","bookmarks","media","here"]},{"id":"notes-copilot-add-new-features-but-first-coffee","title":"Copilot, add new features. But first, coffee","content":"Something I\u0027ve tried to make a habit of on a weekly basis is compiling a post which aggregates all posts on my website from the past week. Because right now it\u0027s not fully automated and I haven\u0027t been posting as much, there are gaps. The process is fairly straightforward. I run a script that: Collects all of the previous week\u0027s posts Drafts a note with links to all the posts Then, I publish the post. Given this is mostly automated, I decided it was something I could just create a GitHub Action to do the post generation and publishing on my behalf. However, there were some issues with the original file generation and formatting that were annoying, but easy enough to fix manually so I never invested time in fixing them. This morning, I created a few issues describing the features and improvements I wanted to make. Worth noting, I did this from my phone. #213 #215 Rather than working on them though, I assigned them to GitHub Coding Agent and stepped out to grab coffee. The agent worked on solutions in the background and by the time I got back, I had working solutions. They weren\u0027t perfect but with no input from me, it got 90-95% of it right. The rest were minor refinements.","contentType":"notes","url":"https://www.lqdev.me/notes/copilot-add-new-features-but-first-coffee","date":"9/14/2025 10:31 PM -05:00","tags":["github","copilot","ai","indieweb","blogging","githubactions"],"summary":"Something I\u0027ve tried to make a habit of on a weekly basis is compiling a post which aggregates all posts on my website from the past week. Because right now it\u0027s not fully automated and I haven\u0027t...","keywords":["post","from","posts","issues","working","agent","automated","solutions","github","week"]},{"id":"bookmarks-how-people-use-chatgpt-national-bureau-economic-research","title":"How people use ChatGPT","content":"How people use ChatGPT \u2192 https://www.nber.org/papers/w34255 Despite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT\u2019s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10% of the world\u2019s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages but even faster growth in non-work-related messages, which have grown from 53% to more than 70% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation topic and find that \u201CPractical Guidance,\u201D \u201CSeeking Information,\u201D and \u201CWriting\u201D are the three most common topics and collectively account for nearly 80% of all conversations. Writing dominates work-related tasks, highlighting chatbots\u2019 unique ability to generate digital outputs compared to traditional search engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs. PDF","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/how-people-use-chatgpt-national-bureau-economic-research","date":"2025-09-15 19:48 -05:00","tags":["ai","openai","research","chatgpt","economics"],"summary":"How people use ChatGPT \u2192 https://www.nber.org/papers/w34255 Despite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT\u2019s consumer product...","keywords":["find","growth","chatgpt","usage","messages","related","common","from","more","classify"]},{"id":"star-bison-herds-reawaken-yellowstone-prairies","title":"Bison herds \u0027reawaken\u0027 Yellowstone\u0027s prairies","content":"Bison herds \u0026#39;reawaken\u0026#39; Yellowstone\u0026#39;s prairies \u2192 https://phys.org/news/2025-08-bison-herds-reawaken-yellowstone-prairies.html A new study published Thursday in Science finds that bringing North America\u0027s largest land mammals back to their ancient stomping grounds is breathing new life into grassland ecosystems, with benefits cascading across the food web. By munching through grasses, bison speed up the nitrogen cycle, supercharging plants with nutrients. The result: forage that is more than 150% richer in protein\u2014a gift to every creature that feeds on the prairie, from elk and deer to pronghorn and bighorn sheep.","contentType":"star","url":"https://www.lqdev.me/responses/bison-herds-reawaken-yellowstone-prairies","date":"2025-09-15 14:02 -05:00","tags":["nature","science","bison","yellowstone","nationalpark"],"summary":"Bison herds \u0026#39;reawaken\u0026#39; Yellowstone\u0026#39;s prairies \u2192 https://phys.org/news/2025-08-bison-herds-reawaken-yellowstone-prairies.html A new study published Thursday in Science finds that bringing...","keywords":["bison","herds","prairies","cascading","across","food","munching","grasses","speed","nitrogen"]},{"id":"notes-2025-09-14-weekly-post-summary","title":"Week of September 14, 2025 - Post Summary","content":"Notes Crawford vs. Canelo Tennessee vs. Georgia Crate Finds - August 2025 Bookmarks Real Simple Licensing Replies Which Static Site Generator (SSG) do I use? PostHog Reshares Ray Dalio on Diary of a CEO Notes on why blogging lost to Twitter Protect Backcountry U.S. Forests The Last Days of Social Media Filter YouTube RSS Feeds Mastodon Launching Quote Posts The Internet\u0027s Hidden Creative Renaissance Stars Breathing with the forest Salmon Metal Geek To Live - Tumblelog No Rule Says We Have To Stay on Social Media The Return of the Cassette Tape The mistake music teachers make Reviews The Serviceberry Media Lossless audio coming to Spotify Just ordered Internet Phone Book Urban Toy Story","contentType":"notes","url":"https://www.lqdev.me/notes/2025-09-14-weekly-post-summary","date":"2025-09-14 23:03 \u002B00:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes Crawford vs. Canelo Tennessee vs. Georgia Crate Finds - August 2025 Bookmarks Real Simple Licensing Replies Which Static Site Generator (SSG) do I use? PostHog Reshares Ray Dalio on Diary of a...","keywords":["media","notes","internet","social","live","geek","metal","salmon","forest","stars"]},{"id":"notes-crawford-canelo-2025","title":"Crawford vs. Canelo","content":"Masterclass performance from Crawford. Made that look easy. After tonight he reached legendary status.","contentType":"notes","url":"https://www.lqdev.me/notes/crawford-canelo-2025","date":"2025-09-14 00:51 -05:00","tags":["boxing","canelo","crawford"],"summary":"Masterclass performance from Crawford. Made that look easy. After tonight he reached legendary status.","keywords":["masterclass","performance","from","crawford","made","look","easy","tonight","reached","legendary"]},{"id":"notes-tennessee-georgia-2025","title":"Tennessee vs. Georgia","content":"This game is wild!","contentType":"notes","url":"https://www.lqdev.me/notes/tennessee-georgia-2025","date":"2025-09-13 17:58 -05:00","tags":["football","ncaaf","tennessee","georgia"],"summary":"This game is wild!","keywords":["game","wild"]},{"id":"reply-which-static-site-generator-use-reply-mastodon-joe","title":"Which Static Site Generator (SSG) do I use?","content":"Which Static Site Generator (SSG) do I use? \u2192 https://xoxo.zone/@artlung/115197260482593667 @artlung@xoxo.zone When I first started, I used Hexo. Then, I moved to Hugo. Ultimately though, I decided to roll my own. Currently my site\u0027s content and the generator live in the same repo . While not up to date, my colophon contains details of how my site is built. The README in the repo also contains more up to date information since the massive revamp I made last month (which I still need to write about). I think that revamp and decoupling of various components in my site would\u0027ve been significantly harder had I not been in complete control of my codebase. Although I have to pay a maintenance toll by owning the code that generates my site, knowing the entire codebase also gives me freedom to experiment and explore.","contentType":"reply","url":"https://www.lqdev.me/responses/which-static-site-generator-use-reply-mastodon-joe","date":"2025-09-13 12:25 -05:00","tags":["indieweb","website","staticsitegenerator","ssg","fsharp"],"summary":"Which Static Site Generator (SSG) do I use? \u2192 https://xoxo.zone/@artlung/115197260482593667 @artlung@xoxo.zone When I first started, I used Hexo. Then, I moved to Hugo. Ultimately though, I decided...","keywords":["site","which","codebase","revamp","contains","date","repo","also","generator","started"]},{"id":"star-breathing-with-the-forest-emergence","title":"Breathing with the forest","content":"Breathing with the forest \u2192 https://emergencemagazine.org/feature/breathing-with-the-forest/ This is beautiful. What an amazing project.","contentType":"star","url":"https://www.lqdev.me/responses/breathing-with-the-forest-emergence","date":"2025-09-13 09:21 -05:00","tags":["art","nature","forest","mindfulness","meditation"],"summary":"Breathing with the forest \u2192 https://emergencemagazine.org/feature/breathing-with-the-forest/ This is beautiful. What an amazing project.","keywords":["breathing","forest","https://emergencemagazine","org/feature/breathing","forest/","beautiful","amazing","project"]},{"id":"media-lossless-audio-coming-spotify","title":"Lossless audio coming to Spotify","content":"I guess people complained enough they\u0027re finally adding it :::media - url: \u0022https://cdn.lqdev.tech/files/images/20250913_141600_Screenshot_20250913-091048.Spotify.png\u0022 mediaType: \u0022image\u0022 aspectRatio: \u0022landscape\u0022 caption: \u0022Lossless audio coming to Spotify\u0022 :::media","contentType":"media","url":"https://www.lqdev.me/media/lossless-audio-coming-spotify","date":"2025-09-13 09:16 -05:00","tags":["music","spotify"],"summary":"I guess people complained enough they\u0027re finally adding it :::media - url: \u0022https://cdn.lqdev.tech/files/images/20250913_141600_Screenshot_20250913-091048.Spotify.png\u0022 mediaType: \u0022image\u0022 aspectRatio:...","keywords":[":::media","spotify","guess","lossless","caption:","landscape","aspectratio:","image","mediatype:","091048"]},{"id":"star-salmon-metal-bear-national-park","title":"Salmon Metal","content":"Salmon Metal \u2192 https://www.youtube.com/watch?v=GZUTc5ZPzrM This is a banger","contentType":"star","url":"https://www.lqdev.me/responses/salmon-metal-bear-national-park","date":"2025-09-13 08:41 -05:00","tags":["bear","metal","music","nationalpark","nature"],"summary":"Salmon Metal \u2192 https://www.youtube.com/watch?v=GZUTc5ZPzrM This is a banger","keywords":["salmon","metal","https://www","youtube","com/watch","v=gzutc5zpzrm","banger"]},{"id":"reply-posthog-meowwolf","title":"PostHog","content":"PostHog \u2192 https://ma.tt/2025/09/posthog/ You had me at Meow Wolf. This is a cool site. Thanks for sharing.","contentType":"reply","url":"https://www.lqdev.me/responses/posthog-meowwolf","date":"2025-09-12 22:47 -05:00","tags":["posthog","meowwolf","web","retro","art","fun"],"summary":"PostHog \u2192 https://ma.tt/2025/09/posthog/ You had me at Meow Wolf. This is a cool site. Thanks for sharing.","keywords":["posthog","https://ma","tt/2025/09/posthog/","meow","wolf","cool","site","thanks","sharing"]},{"id":"reshare-ray-dalio-diary-of-a-ceo-podcast-2025","title":"Ray Dalio on Diary of a CEO","content":"Ray Dalio on Diary of a CEO \u2192 https://www.youtube.com/watch?v=49RT6SQ8n0Y If you get past the doom and gloom, this is a nice conversation with Ray Dalio on systems, team building, and mindfulness.","contentType":"reshare","url":"https://www.lqdev.me/responses/ray-dalio-diary-of-a-ceo-podcast-2025","date":"2025-09-12 22:20 -05:00","tags":["podcast","diaryofaceo"],"summary":"Ray Dalio on Diary of a CEO \u2192 https://www.youtube.com/watch?v=49RT6SQ8n0Y If you get past the doom and gloom, this is a nice conversation with Ray Dalio on systems, team building, and mindfulness.","keywords":["dalio","diary","https://www","youtube","com/watch","v=49rt6sq8n0y","past","doom","gloom","nice"]},{"id":"media-just-ordered-internet-phone-book-issue-1","title":"Just ordered Internet Phone Book","content":"Thanks to [Manu](https://manuelmoreale.com/two-quick-news-items), I was able to grab a copy of the Internet Phone Book. :::media - url: \u0022https://cdn.lqdev.tech/files/images/20250913_025011_Screenshot_20250912-214304.Brave.png\u0022 mediaType: \u0022image\u0022 aspectRatio: \u0022landscape\u0022 caption: \u0022Just ordered Internet Phone Book\u0022 :::media","contentType":"media","url":"https://www.lqdev.me/media/just-ordered-internet-phone-book-issue-1","date":"2025-09-12 21:50 -05:00","tags":["smallweb","indieweb","socialweb"],"summary":"Thanks to [Manu](https://manuelmoreale.com/two-quick-news-items), I was able to grab a copy of the Internet Phone Book. :::media - url:...","keywords":["internet","phone","book",":::media","thanks","landscape","aspectratio:","image","mediatype:","brave"]},{"id":"reshare-notes-why-blogging-lost-twitter-winer","title":"Notes on why blogging lost to Twitter","content":"Notes on why blogging lost to Twitter \u2192 http://scripting.com/2025/09/09/114446.html?title=bulletPointsFromYesterdaysPodcast I haven\u0027t listened to the podcast yet but there\u0027s a lot of good points. Subscribing in feed readers required too many steps: copying URLs, menus, pasting, confirming. Blogging lost to Twitter because Twitter had one-click subscribe. Rebooting the blogosphere requires cooperation and a universal \u0026quot;follow\u0026quot; button.","contentType":"reshare","url":"https://www.lqdev.me/responses/notes-why-blogging-lost-twitter-winer","date":"2025-09-12 21:16 -05:00","tags":["blog","indieweb","feed","rss","blogosphere"],"summary":"Notes on why blogging lost to Twitter \u2192 http://scripting.com/2025/09/09/114446.html?title=bulletPointsFromYesterdaysPodcast I haven\u0027t listened to the podcast yet but there\u0027s a lot of good points....","keywords":["twitter","blogging","lost","notes","urls","menus","pasting","confirming","click","subscribe"]},{"id":"star-geek-to-live-tumblelog","title":"Geek To Live - Tumblelog","content":"Geek To Live - Tumblelog \u2192 https://lifehacker.com/geek-to-live-instant-no-overhead-blog-with-tumblr-244915 A blast from the past. Can\u0027t believe this post is still available. Almost two decades later and it feels like we\u0027re still figuring out how to share and express ourselves online. These are the reasons most people abandon their personal weblog or never start one. But we all come across interesting tidbits online every day that we want to remember and share - links, photos, videos, even that side-splitting IM session you had with your co-worker. A new blog format, called a \u0026quot;tumblelog,\u0026quot; is a no-hassle, no-writing-required way to share those bits and maintain a personal site with the least possible commitment. A new service called Tumblr can publish your tumblelog with minimal setup and for free.","contentType":"star","url":"https://www.lqdev.me/responses/geek-to-live-tumblelog","date":"2025-09-12 21:05 -05:00","tags":["blog","tumblr","microblog"],"summary":"Geek To Live - Tumblelog \u2192 https://lifehacker.com/geek-to-live-instant-no-overhead-blog-with-tumblr-244915 A blast from the past. Can\u0027t believe this post is still available. Almost two decades later...","keywords":["share","tumblelog","personal","online","\u0026quot","tumblr","blog","called","live","photos"]},{"id":"reshare-protect-backcountry-forests-roadless-rule-petition","title":"Protect Backcountry U.S. Forests","content":"Protect Backcountry U.S. Forests \u2192 https://www.rei.com/action/network/campaign/roadless-rule Make your voice heard beyond signing the petition by submitting a personal comment directly to the U.S. Forest Service during the public comment period, open until September 19, 2025. For nearly 25 years, the Roadless Rule protected nearly one-third of our federal forestlands from major logging operations and oil-and-gas drilling. While nearly half of U.S. federal forest lands are open to these industries, the areas protected by the Roadless Rule are home to massive old-growth carbon sinks, habitats for vulnerable fish and wildlife, migratory corridors for game species and millennia-old cultural landscapes for Indigenous communities. They are beloved destinations for climbers, hikers, mountain bikers, paddlers, backcountry skiers and more and act as critical sources of clean drinking water for over 60 million people in 33 states.But the U.S. Department of Agriculture wants to end the Roadless Rule\u2019s protections across nearly 45 million acres of the National Forest System. Unless we act now, we will lose protections for precious backcountry forestlands. Add your voice and tell the Department of Agriculture to reverse course before it\u2019s too late. Submit Your Comment Learn More","contentType":"reshare","url":"https://www.lqdev.me/responses/protect-backcountry-forests-roadless-rule-petition","date":"2025-09-12 20:55 -05:00","tags":["nature","civics","publiclands"],"summary":"Protect Backcountry U.S. Forests \u2192 https://www.rei.com/action/network/campaign/roadless-rule Make your voice heard beyond signing the petition by submitting a personal comment directly to the U.S....","keywords":["nearly","roadless","backcountry","forest","rule","comment","million","more","agriculture","department"]},{"id":"bookmarks-real-simple-licensing-open-standard","title":"Real Simple Licensing","content":"Real Simple Licensing \u2192 https://rslstandard.org/ The open content licensing standard for the AI-first Internet","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/real-simple-licensing-open-standard","date":"2025-09-12 20:48 -05:00","tags":["rsl","ai","licensing","openstandard"],"summary":"Real Simple Licensing \u2192 https://rslstandard.org/ The open content licensing standard for the AI-first Internet","keywords":["licensing","real","simple","https://rslstandard","org/","open","content","standard","internet"]},{"id":"star-no-rule-says-stay-on-social-media","title":"No Rule Says We Have To Stay on Social Media","content":"No Rule Says We Have To Stay on Social Media \u2192 https://sethw.xyz/blog/2025/09/10/there-is-no-rule-that-says-we-have-to-stay-on-social-media/ \uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF If we can go back to vinyl records and film cameras, we can go back to blogs and RSS feeds. Do we need permission from the masses before we do something that might bring us joy?","contentType":"star","url":"https://www.lqdev.me/responses/no-rule-says-stay-on-social-media","date":"2025-09-12 20:46 -05:00","tags":["indieweb","socialweb","socialmedia","rss"],"summary":"No Rule Says We Have To Stay on Social Media \u2192 https://sethw.xyz/blog/2025/09/10/there-is-no-rule-that-says-we-have-to-stay-on-social-media/ \uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF If we can go back to vinyl records and film...","keywords":["rule","says","stay","social","back","from","permission","need","feeds","blogs"]},{"id":"star-return-cassette-tape-analog-media-comeback","title":"The Return of the Cassette Tape","content":"The Return of the Cassette Tape \u2192 https://www.saturdayeveningpost.com/2025/09/the-return-of-the-cassette-tape-how-analog-music-is-making-a-comeback/ Sales of cassettes are up more than 440 percent over the past decade. Cassette players (think modernized Walkmans) are also back, with companies producing new versions of this decidedly retro tech. This \u201Crevenge of analog,\u201D...is motivated by a combination of factors, from the practical to the more prosaic, and, ultimately, a more mindful use of technology. This approach rejects a binary of good vs. bad tech, and instead seeks a via media \u2014 a middle path \u2014 with its daily use. Although I haven\u0027t done it as much with music, over the past few years I\u0027ve been slowly rebuilding my analog media collection which includes movies and books. As the article mentions, part of that is ownership and actually owning the things I pay for. More importantly though, rather than scrolling through an endless catalog and never finding something to watch or read, I have a curated set of items to enjoy in a more mindful manner. If I feel the need to scroll through an endless catalog, there\u0027s always the library which has tons of media available. I find the library to be a heavily underutilized and almost forgotten community service we\u0027ve all already paid for in some way, so why not use it.","contentType":"star","url":"https://www.lqdev.me/responses/return-cassette-tape-analog-media-comeback","date":"2025-09-12 20:32 -05:00","tags":["media","music","analog","retro","vintage"],"summary":"The Return of the Cassette Tape \u2192 https://www.saturdayeveningpost.com/2025/09/the-return-of-the-cassette-tape-how-analog-music-is-making-a-comeback/ Sales of cassettes are up more than 440 percent...","keywords":["more","cassette","analog","media","return","tech","mindful","library","endless","past"]},{"id":"reshare-last-days-social-media-noemag","title":"The Last Days of Social Media","content":"The Last Days of Social Media \u2192 https://www.noemamag.com/the-last-days-of-social-media/ Social media was built on the romance of authenticity. Early platforms sold themselves as conduits for genuine connection: stuff you wanted to see, like your friend\u2019s wedding and your cousin\u2019s dog. The feed no longer feels crowded with people but crowded with content. At this point, it has far less to do with people than with consumers and consumption. Whatever remains of genuine, human content is increasingly sidelined by algorithmic prioritization, receiving fewer interactions than the engineered content and AI slop optimized solely for clicks. While content proliferates, engagement is evaporating...People aren\u2019t connecting or conversing on social media like they used to; they\u2019re just wading through slop, that is, low-effort, low-quality content produced at scale, often with AI, for engagement. Engagement is now about raw user attention \u2013 time spent, impressions, scroll velocity \u2013 and the net effect is an online world in which you are constantly being addressed but never truly spoken to. Social media\u2019s death rattle will not be a bang but a shrug. These networks once promised a single interface for the whole of online life...But now, the model is splintering, and users are drifting toward smaller, slower, more private spaces, like group chats, Discord servers and federated microblogs \u2014 a billion little gardens. Intentional, opt-in micro\u2011communities are rising in their place \u2014 like Patreon collectives and Substack newsletters \u2014 where creators chase depth over scale, retention over virality. A writer with 10,000 devoted subscribers can potentially earn more and burn out less than one with a million passive followers on Instagram. These are the last days of social media, not because we lack content, but because the attention economy has neared its outer limit \u2014 we have exhausted the capacity to care. Our offline reality is irrevocably shaped by our online world. Consider the worker who deletes or was never on LinkedIn, excluding themselves from professional networks that increasingly exist nowhere else These platforms haven\u2019t just captured attention, they\u2019ve enclosed the commons where social, economic and cultural capital are exchanged. But enclosure breeds resistance, and as exhaustion sets in, alternatives begin to emerge. ...as social media collapses on itself, the future points to a quieter, more fractured, more human web, something that no longer promises to be everything, everywhere, for everyone. We can dream of a digital future where belonging is no longer measured by follower counts or engagement rates, but rather by the development of trust and the quality of conversation...None of this is about escaping the social internet, but about reclaiming its scale, pace, and purpose. The technical architecture of the next social web is already emerging through federated and distributed protocols like ActivityPub...But protocols alone won\u2019t save us. The email protocol is decentralized, yet most email flows through a handful of corporate providers. We need to \u201Crewild the internet,\u201D as Maria Farrell and Robin Berjon mentioned in a Noema essay. Social media as we know it is dying, but we\u2019re not condemned to its ruins. We are capable of building better \u2014 smaller, slower, more intentional, more accountable \u2014 spaces for digital interaction, spaces where the metrics that matter aren\u2019t engagement and growth but understanding and connection, where algorithms serve the community rather than strip-mining it. The last days of social media might be the first days of something more human: a web that remembers why we came online in the first place \u2014 not to be harvested but to be heard, not to go viral but to find our people, not to scroll but to connect. We built these systems, and we can certainly build better ones. The question is whether we will do this or whether we will continue to drown.","contentType":"reshare","url":"https://www.lqdev.me/responses/last-days-social-media-noemag","date":"2025-09-12 20:19 -05:00","tags":["socialmedia","essay","socialweb"],"summary":"The Last Days of Social Media \u2192 https://www.noemamag.com/the-last-days-of-social-media/ Social media was built on the romance of authenticity. Early platforms sold themselves as conduits for genuine...","keywords":["social","media","more","content","days","engagement","last","online","people","longer"]},{"id":"reshare-filter-youtube-video-rss-feed","title":"Filter YouTube RSS Feeds","content":"Filter YouTube RSS Feeds \u2192 https://triapul.cz/_/1756198747.html As someone who subscribes to YouTube channels using RSS feeds, I found this incredibly useful. https://www.youtube.com/feeds/videos.xml?playlist_id=UU\u0026lt;channel id\u0026gt; Notice the UU and look below what you can replace it with. UU - all public uploads (same as the channel_id= url) UULF - all public videos UUMO - all member videos UULV - all live streams UUSH - all shorts UULP - popular videos UUPV - popular live streams UUPS - popular shorts UUMV - member live streams UUMS - member shorts","contentType":"reshare","url":"https://www.lqdev.me/responses/filter-youtube-video-rss-feed","date":"2025-09-12 19:57 -05:00","tags":["rss","youtube","feed"],"summary":"Filter YouTube RSS Feeds \u2192 https://triapul.cz/_/1756198747.html As someone who subscribes to YouTube channels using RSS feeds, I found this incredibly useful....","keywords":["live","youtube","popular","shorts","streams","videos","member","feeds","channel","public"]},{"id":"reshare-masdoton-launches-quote-posts","title":"Mastodon Launching Quote Posts","content":"Mastodon Launching Quote Posts \u2192 https://www.theverge.com/news/777067/mastodon-quote-posts-fediverse-launch Starting next week, you\u2019ll see an option to quote another user by selecting the repost \u2014 or \u201Cboost\u201D \u2014 button, allowing you to add your thoughts to someone else\u2019s post. Mastodon is bringing quote posts to mastodon.online and mastodon.social first, but will roll out the feature more broadly with the Mastodon 4.5 update. It also worked on a technical specification for quote posting in the fediverse and hopes that other platforms will adopt it.","contentType":"reshare","url":"https://www.lqdev.me/responses/masdoton-launches-quote-posts","date":"2025-09-12 19:51 -05:00","tags":["mastodon","fediverse","socialweb"],"summary":"Mastodon Launching Quote Posts \u2192 https://www.theverge.com/news/777067/mastodon-quote-posts-fediverse-launch Starting next week, you\u2019ll see an option to quote another user by selecting the repost \u2014 or...","keywords":["mastodon","quote","posts","fediverse","post","bringing","online","social","roll","feature"]},{"id":"media-toy-story-underpass","title":"Urban Toy Story","content":"Random find in an underpass :::media - url: \u0022https://cdn.lqdev.tech/files/images/20250912_132936_10000024972.jpg\u0022 mediaType: \u0022image\u0022 aspectRatio: \u0022landscape\u0022 caption: \u0022Urban Toy Story\u0022 :::media","contentType":"media","url":"https://www.lqdev.me/media/toy-story-underpass","date":"2025-09-12 08:29 -05:00","tags":["toy","chicago","art"],"summary":"Random find in an underpass :::media - url: \u0022https://cdn.lqdev.tech/files/images/20250912_132936_10000024972.jpg\u0022 mediaType: \u0022image\u0022 aspectRatio: \u0022landscape\u0022 caption: \u0022Urban Toy Story\u0022 :::media","keywords":[":::media","random","caption:","landscape","aspectratio:","image","mediatype:","10000024972","132936","tech/files/images/20250912"]},{"id":"reshare-internet-hidden-creative-rennaissance-indieweb","title":"The Internet\u0027s Hidden Creative Renaissance","content":"The Internet\u0026#39;s Hidden Creative Renaissance \u2192 https://newpublic.substack.com/p/the-handmade-internet-is-making-a Great post from Alexandra . Tons of great resources to explore as well. ...recently the indie web has been experiencing a revival, as more netizens look for connection outside the walled gardens created by tech giants. The kinds of posts and projects you\u2019ll find are creative, personal, experimental, and sometimes so intimate you feel like you\u2019re intruding. However, they\u2019re not the kind of content that usually goes viral on TikTok. That\u2019s beside the point. The focus isn\u2019t on personal branding, growth or monetization, or \u201Ccontent\u201D creation, but on freedom from those things. Instead of polished, 10-second snippets optimized for mass-appeal, engagement, and profit, these are largely slow-cooked projects made just for fun. Against a backdrop of accelerating enshittification and living chronically online \u2014 especially since the COVID pandemic \u2014 there\u2019s growing interest in the slower, more intentional and connected experience the indie web promises. In different corners of the mainstream internet, younger users are talking about reclaiming their attention spans, trading in their smartphones, writing essays for fun, journaling, discovering long reads, reading fiction, and picking up hobbies like crocheting or scrapbooking. Discoverability can be a challenge...But what seems like a bug can really be a feature of the small web experience: the point is to explore. In the greater context of the global internet, the small web will never completely replace big platforms or serve every need, nor should it. Social media is still the main source of news in many parts of the world, offering valuable tools for community-building, commerce, even activism. For their part, most indie web creators are happy to exist outside of the mainstream.","contentType":"reshare","url":"https://www.lqdev.me/responses/internet-hidden-creative-rennaissance-indieweb","date":"2025-09-12 07:55 -05:00","tags":["indieweb","smallweb","socialweb","creativity","internet"],"summary":"The Internet\u0026#39;s Hidden Creative Renaissance \u2192 https://newpublic.substack.com/p/the-handmade-internet-is-making-a Great post from Alexandra . Tons of great resources to explore as well. ...recently...","keywords":["internet","indie","creative","more","small","mainstream","explore","point","personal","from"]},{"id":"star-mistake-music-teachers-make-ted-talk","title":"The mistake music teachers make","content":"The mistake music teachers make \u2192 https://www.youtube.com/watch?v=Uc_bPtUltDU I like this analogy.","contentType":"star","url":"https://www.lqdev.me/responses/mistake-music-teachers-make-ted-talk","date":"2025-09-11 15:48 -05:00","tags":["music","language"],"summary":"The mistake music teachers make \u2192 https://www.youtube.com/watch?v=Uc_bPtUltDU I like this analogy.","keywords":["mistake","music","teachers","https://www","youtube","com/watch","v=uc","bptultdu","analogy"]},{"id":"notes-crate-finds-august-2025","title":"Crate Finds - August 2025","content":"Trying something different with this iteration of playlists. Usually I embed the Spotify playlist directly into the post. I don\u0027t like this for several reasons: If you don\u0027t have Spotify, listening is limited I don\u0027t know this for a fact, but I assume it to be the case - there\u0027s some analytics and tracking Spotify gets by embedding their player on my site. Given I don\u0027t even track visitors on this site, it\u0027s not something I\u0027m comfortable with. I\u0027ve done it because it requires no work on my end to embed the playlist, but I know the bargain I am making. So to solve that, I\u0027ve written a script which does a best effort to find the tracks on YouTube and provides those links alongside this These are the results from one of the generated artifacts: Tracks Reaching Out - (Original Mix) by Diinch Album: Reaching Out EP Duration: 4:41 Listen on YouTube Backup: Listen on Spotify Melod\u00EDa del R\u00EDo by Ruben Gonzalez Album: Introducing Duration: 4:42 Listen on YouTube Backup: Listen on Spotify Calico by 1tbsp Album: kanash\u012B Duration: 4:51 Listen on YouTube Backup: Listen on Spotify Nu Hope by Neggy Gemmy Album: Body Work Duration: 2:38 Listen on YouTube Backup: Listen on Spotify Controller - Instrumental by Channel Tres Album: Controller Duration: 3:28 Listen on YouTube Backup: Listen on Spotify Volos by Benyayer, Dark Sky Album: Volos Duration: 6:51 Listen on YouTube Backup: Listen on Spotify Heart Made Of Glass by Eli \u0026amp; Fur Album: Dreamscapes Duration: 3:50 Listen on YouTube Backup: Listen on Spotify Forbidden Fruits - Handbook\u0027s Remix by Edamame, Handbook Album: Anywhere People Duration: 3:14 Listen on YouTube Backup: Listen on Spotify Ulu by Belief, Boom Bip, Stella Mozgawa Album: Belief Duration: 5:26 Listen on YouTube Backup: Listen on Spotify Let Her Know by CMYK, James Blake Album: CMYK 002 Duration: 4:07 Listen on YouTube Backup: Listen on Spotify The Wrap Around by Dam Swindle Album: The Wrap Around Duration: 7:11 Listen on YouTube Backup: Listen on Spotify 887 - Structure by Banco De Gaia Album: Last Train to Lhasa (Special Edition) Duration: 14:17 Listen on YouTube Backup: Listen on Spotify Summer Sun by Common Saints Album: Summer Sun Duration: 4:49 Listen on YouTube Backup: Listen on Spotify Secret Song by Common Saints Album: Idol Eyes Duration: 5:21 Listen on YouTube Backup: Listen on Spotify Space by Cellomano Album: Unwinding Duration: 6:19 Listen on YouTube Backup: Listen on Spotify Step Out Of The Light by Bruno Pernadas Album: Private Reasons Duration: 9:15 Listen on YouTube Backup: Listen on Spotify By Hook or by Crook by Jessica Pratt Album: Here in the Pitch Duration: 3:27 Listen on YouTube Backup: Listen on Spotify Asphalt Homeland by Ghost Funk Orchestra, Golden Rules Album: Asphalt Homeland Duration: 4:01 Listen on YouTube Backup: Listen on Spotify Earth by Joe Henderson, Alice Coltrane Album: The Elements Duration: 13:13 Listen on YouTube Backup: Listen on Spotify Equanimity by Ancient Infinity Orchestra Album: River of Light Duration: 8:33 Listen on YouTube Backup: Listen on Spotify Love Is Everywhere by Pharoah Sanders Album: Wisdom Through Music Duration: 5:18 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify .","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-august-2025","date":"2025-09-07 18:59 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"Trying something different with this iteration of playlists. Usually I embed the Spotify playlist directly into the post. I don\u0027t like this for several reasons: If you don\u0027t have Spotify, listening...","keywords":["listen","spotify","youtube","album:","duration:","backup:","orchestra","original","homeland","asphalt"]},{"id":"notes-arizona-state-vs-mississippi-state-2025","title":"Arizona State vs Mississippi State (2025)","content":"What an upset. That ending was wild. Probably the best game of the weekend.","contentType":"notes","url":"https://www.lqdev.me/notes/arizona-state-vs-mississippi-state-2025","date":"2025-09-06 22:07 -05:00","tags":["ncaaf","football","asu","msu"],"summary":"What an upset. That ending was wild. Probably the best game of the weekend.","keywords":["upset","ending","wild","probably","best","game","weekend"]},{"id":"notes-miami-vs-notre-dame-2025","title":"Miami vs. Notre Dame (2025)","content":"Another great game that went down to the wire. Miami dug deep in the end and earned that W.","contentType":"notes","url":"https://www.lqdev.me/notes/miami-vs-notre-dame-2025","date":"2025-08-31 22:18 -05:00","tags":["ncaaf","miami","notredame"],"summary":"Another great game that went down to the wire. Miami dug deep in the end and earned that W.","keywords":["another","game","went","down","wire","miami","deep","earned"]},{"id":"reshare-crescent-msr-privacy-digital-identity","title":"Crescent library brings privacy to digital identity systems","content":"Crescent library brings privacy to digital identity systems \u2192 https://www.microsoft.com/en-us/research/blog/crescent-library-brings-privacy-to-digital-identity-systems/ Digital identities, the electronic credentials embedded in phone wallets, workplace logins, and other apps, are becoming ubiquitous. While they offer unprecedented convenience, they also create new privacy risks, particularly around tracking and surveillance. One of these risks is linkability, the ability to associate one or more uses of a credential to a specific person. Currently, when people use their mobile driver\u2019s license or log into various apps, hidden identifiers can link these separate activities together, building detailed profiles of user behavior. To address this, we have released Crescent , a cryptographic library that adds unlinkability to widely used identity formats, protecting privacy. These include JSON Web Tokens (the authentication standard behind many app logins) and mobile driver\u2019s licenses. Crescent also works without requiring the organizations that issue these credentials to update their systems. GitHub Repo","contentType":"reshare","url":"https://www.lqdev.me/responses/crescent-msr-privacy-digital-identity","date":"2025-08-26 19:40 -05:00","tags":["research","privacy","microsoft"],"summary":"Crescent library brings privacy to digital identity systems \u2192 https://www.microsoft.com/en-us/research/blog/crescent-library-brings-privacy-to-digital-identity-systems/ Digital identities, the...","keywords":["privacy","crescent","library","digital","identity","they","driver\u2019s","apps","logins","risks"]},{"id":"media-windows-11-upgrade","title":"Windows 11 upgrade?","content":"Lol. I would...if I could actually upgrade this PC to Windows 11. :::media - url: \u0022http://cdn.lqdev.tech/files/images/20250824_011142_1000002436.jpg\u0022 alt: \u0022Screenshot of Windows Upgrade Screen\u0022 mediaType: \u0022image\u0022 aspectRatio: \u0022landscape\u0022 caption: \u0022Windows 11 upgrade?\u0022 :::media","contentType":"media","url":"https://www.lqdev.me/media/windows-11-upgrade","date":"2025-08-23 20:11 -05:00","tags":["windows","windows10"],"summary":"Lol. I would...if I could actually upgrade this PC to Windows 11. :::media - url: \u0022http://cdn.lqdev.tech/files/images/20250824_011142_1000002436.jpg\u0022 alt: \u0022Screenshot of Windows Upgrade Screen\u0022...","keywords":["upgrade","windows",":::media","actually","aspectratio:","image","mediatype:","screen","screenshot","alt:"]},{"id":"notes-webmentions-are-back","title":"Webmentions are back","content":"Nice to see my GitHub Actions back to green after fixing my script to send webmentions. Now to discover more amazing content I can link out to.","contentType":"notes","url":"https://www.lqdev.me/notes/webmentions-are-back","date":"2025-08-21 19:15 -05:00","tags":["indieweb","webmention","socialweb"],"summary":"Nice to see my GitHub Actions back to green after fixing my script to send webmentions. Now to discover more amazing content I can link out to.","keywords":["nice","github","actions","back","green","fixing","script","send","webmentions","discover"]},{"id":"star-small-web-is-beautiful","title":"Small (web) is beautiful","content":"Small (web) is beautiful \u2192 https://fredrocha.net/2025/05/21/small-web-is-beautiful/ I dream of a web that fosters healthy conversations, together with personal and intellectual growth. The world is diverse and fascinating, and we can be information explorers together.","contentType":"star","url":"https://www.lqdev.me/responses/small-web-is-beautiful","date":"2025-08-21 19:13 -05:00","tags":["smallweb","indieweb","blogging"],"summary":"Small (web) is beautiful \u2192 https://fredrocha.net/2025/05/21/small-web-is-beautiful/ I dream of a web that fosters healthy conversations, together with personal and intellectual growth. The world is...","keywords":["together","small","fascinating","diverse","world","growth","intellectual","personal","conversations","healthy"]},{"id":"reply-were-getting-married-reillys","title":"We\u0027re getting married!","content":"We\u0026#39;re getting married! \u2192 https://reillyspitzfaden.com/posts/2025/08/were-getting-married/ Congrats!","contentType":"reply","url":"https://www.lqdev.me/responses/were-getting-married-reillys","date":"2025-08-21 18:47 -05:00","tags":[],"summary":"We\u0026#39;re getting married! \u2192 https://reillyspitzfaden.com/posts/2025/08/were-getting-married/ Congrats!","keywords":["getting","we\u0026#39","married","https://reillyspitzfaden","com/posts/2025/08/were","married/","congrats"]},{"id":"reshare-omnineural-4b","title":"Omnineural 4B","content":"Omnineural 4B \u2192 https://nexa.ai/blogs/omnineural-4b OmniNeural-4B is the first multimodal AI model built natively for NPUs \u2014 handling text, images, and audio in one model.","contentType":"reshare","url":"https://www.lqdev.me/responses/omnineural-4b","date":"2025-08-20 23:59 -05:00","tags":["ai","npu","multimodal"],"summary":"Omnineural 4B \u2192 https://nexa.ai/blogs/omnineural-4b OmniNeural-4B is the first multimodal AI model built natively for NPUs \u2014 handling text, images, and audio in one model.","keywords":["omnineural","model","https://nexa","ai/blogs/omnineural","multimodal","built","natively","npus","handling","text"]},{"id":"reshare-agentsmd","title":"Agents.md","content":"Agents.md \u2192 https://agents.md/ A simple, open format for guiding coding agents Think of AGENTS.md as a README for agents: a dedicated, predictable place to provide the context and instructions to help AI coding agents work on your project.","contentType":"reshare","url":"https://www.lqdev.me/responses/agentsmd","date":"2025-08-20 22:53 -05:00","tags":["ai","agents","openai"],"summary":"Agents.md \u2192 https://agents.md/ A simple, open format for guiding coding agents Think of AGENTS.md as a README for agents: a dedicated, predictable place to provide the context and instructions to...","keywords":["agents","coding","instructions","context","provide","place","predictable","dedicated","agents:","readme"]},{"id":"notes-college-football-is-back-2025","title":"College football is back (2025)","content":"It\u0027s that time of the year again. Week 1 is going to be nuts with a top 5 matchup and several top 10 games. Looks like Penn State\u0027s schedule this year is going to be lighter than usual so they may have a shot at the title.","contentType":"notes","url":"https://www.lqdev.me/notes/college-football-is-back-2025","date":"2025-08-20 21:54 -05:00","tags":["football","ncaaf","psu","pennstate"],"summary":"It\u0027s that time of the year again. Week 1 is going to be nuts with a top 5 matchup and several top 10 games. Looks like Penn State\u0027s schedule this year is going to be lighter than usual so they may...","keywords":["year","going","they","usual","lighter","schedule","state","penn","looks","games"]},{"id":"reshare-seemingly-conscious-ai","title":"Seemingly Conscious AI","content":"Seemingly Conscious AI \u2192 https://mustafa-suleyman.ai/seemingly-conscious-ai-is-coming We must build AI for people; not to be a person \uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF","contentType":"reshare","url":"https://www.lqdev.me/responses/seemingly-conscious-ai","date":"2025-08-20 19:46 -05:00","tags":["ai","consciousness","humanism"],"summary":"Seemingly Conscious AI \u2192 https://mustafa-suleyman.ai/seemingly-conscious-ai-is-coming We must build AI for people; not to be a person \uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF","keywords":["conscious","seemingly","https://mustafa","suleyman","ai/seemingly","coming","build","people","person","\uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF"]},{"id":"bookmarks-reclaim-control","title":"Reclaim Control","content":"Reclaim Control \u2192 https://www.reclaimcontrol.tech/ RECLAIM CONTROL over your digital life, devices, services and data. You have the right to control your data, devices, and services. We coordinate, develop and foster communities of Reclaimers who want digital independence. Together, let\u0027s reclaim the right to a technical future of data sovereignty, digital autonomy, responsible governance, and collective care online for all.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/reclaim-control","date":"2025-08-19 21:41 -05:00","tags":["digitalsovereignty","reclaimcontrol"],"summary":"Reclaim Control \u2192 https://www.reclaimcontrol.tech/ RECLAIM CONTROL over your digital life, devices, services and data. You have the right to control your data, devices, and services. We coordinate,...","keywords":["reclaim","control","digital","data","devices","services","collective","governance","responsible","autonomy"]},{"id":"bookmarks-out-out-project","title":"The Opt Out Project","content":"The Opt Out Project \u2192 https://www.optoutproject.net/ I have built up a treasure trove of knowledge, devices, and services aimed at keeping me and my family away from the evils of data-collecting algorithmic systems. That\u0027s why I started this site. I\u0027ve decided to put information about these tools and alternatives in one spot for others to find and use. Many of these tools are reviewed in the tech press, others are more bespoke but there is information out there if you know where to look. My goal is to bring it all together in one place, to guide you through the process, and help you opt out as well. ...there are alternatives out there, better visions of what our lives could be like. There are systems that don\u0027t require capturing and storing all our data, mining us exploitatively for profit, and serving us the kinds of misinformation and advertising that has us hating our neighbors.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/out-out-project","date":"2025-08-19 20:51 -05:00","tags":["optout","alternative","tech"],"summary":"The Opt Out Project \u2192 https://www.optoutproject.net/ I have built up a treasure trove of knowledge, devices, and services aimed at keeping me and my family away from the evils of data-collecting...","keywords":["others","alternatives","tools","information","systems","data","project","bespoke","look","goal"]},{"id":"reshare-reflections-on-social-web","title":"Reflections on the social web","content":"Reflections on the social web \u2192 https://john.onolan.org/reflections-on-the-social-web/ The work of a product team, when working with new technology, is to abstract away as much of this complexity as possible, so that it feels friendly and approachable to new people. My strongest belief about the social web is that if we want it to succeed, we have to keep lowering the barrier to entry.","contentType":"reshare","url":"https://www.lqdev.me/responses/reflections-on-social-web","date":"2025-08-18 19:31 -05:00","tags":["ghost","socialweb","indieweb"],"summary":"Reflections on the social web \u2192 https://john.onolan.org/reflections-on-the-social-web/ The work of a product team, when working with new technology, is to abstract away as much of this complexity as...","keywords":["social","reflections","lowering","keep","succeed","want","belief","strongest","people","approachable"]},{"id":"reshare-ghost-6-changelog","title":"Ghost 6.0","content":"Ghost 6.0 \u2192 https://ghost.org/changelog/6/ In Ghost 6.0 we\u0027re introducing another new distribution channel: The social web. Now, millions of people can discover, follow, like and reply to your posts from any supported social web client - including Bluesky, Mastodon, Threads, Flipboard, Ghost, WordPress, Surf, WriteFreely, and many more. Just as people can visit your Ghost website in any browser, subscribe with any feed reader, or receive newsletters in any email client, they can now follow your Ghost publication from any social web client. Unlike closed platforms with restrictive algorithms, open web protocols create a direct connection between you and your subscribers which you have full control over. If you\u0027ve been around on the web for while, and you can remember back that far... you might even call it the return of the blogosphere. Love the introduction of social components to the Ghost platform. I\u0027m so ready for the return of the blogosphere and the return to open protocols.","contentType":"reshare","url":"https://www.lqdev.me/responses/ghost-6-changelog","date":"2025-08-18 19:24 -05:00","tags":["ghost","fediverse","socialweb","publishing"],"summary":"Ghost 6.0 \u2192 https://ghost.org/changelog/6/ In Ghost 6.0 we\u0027re introducing another new distribution channel: The social web. Now, millions of people can discover, follow, like and reply to your posts...","keywords":["ghost","social","return","client","blogosphere","people","follow","open","from","protocols"]},{"id":"star-newly-redesigned-blogroll","title":"The newly redesigned Blogroll Club","content":"The newly redesigned Blogroll Club \u2192 https://blogroll.club/the-newly-redesigned-blogroll-club/","contentType":"star","url":"https://www.lqdev.me/responses/newly-redesigned-blogroll","date":"2025-08-18 19:21 -05:00","tags":["blogrollclub","blogroll","indieweb","socialweb"],"summary":"The newly redesigned Blogroll Club \u2192 https://blogroll.club/the-newly-redesigned-blogroll-club/","keywords":["newly","redesigned","blogroll","club","https://blogroll","club/the","club/"]},{"id":"bookmarks-hope-16-videos","title":"HOPE 16 Conference Videos","content":"HOPE 16 Conference Videos \u2192 https://www.youtube.com/watch?v=2xuoa3PLMvU\u0026amp;list=PLlg8We3ePxcPG0FRTcEEdsViRevFjikIU","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/hope-16-videos","date":"2025-08-18 18:53 -05:00","tags":["hope","hacker","conference"],"summary":"HOPE 16 Conference Videos \u2192 https://www.youtube.com/watch?v=2xuoa3PLMvU\u0026amp;list=PLlg8We3ePxcPG0FRTcEEdsViRevFjikIU","keywords":["hope","conference","videos","https://www","youtube","com/watch","v=2xuoa3plmvu\u0026amp","list=pllg8we3epxcpg0frtceedsvirevfjikiu"]},{"id":"bookmarks-fedicon-2025-videos","title":"FediCon 2025 Videos","content":"FediCon 2025 Videos \u2192 https://spectra.video/w/p/52g7HzMoUCxEm9nU339mPf?playlistPosition=1","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/fedicon-2025-videos","date":"2025-08-18 18:53 -05:00","tags":["fediverse","fedicon2025","conference"],"summary":"FediCon 2025 Videos \u2192 https://spectra.video/w/p/52g7HzMoUCxEm9nU339mPf?playlistPosition=1","keywords":["fedicon","2025","videos","https://spectra","video/w/p/52g7hzmoucxem9nu339mpf","playlistposition=1"]},{"id":"reshare-fediurbanism-fedicon-2025","title":"FediUrbanism \u2014 Johanna Botari \u2014 FediCon 2025","content":"FediUrbanism \u2014 Johanna Botari \u2014 FediCon 2025 \u2192 https://spectra.video/w/f7GBsBf3nAhF8WfTMWUEXm I really enjoyed this talk by @johannab on the concept of FediUrbanism.","contentType":"reshare","url":"https://www.lqdev.me/responses/fediurbanism-fedicon-2025","date":"2025-08-18 18:36 -05:00","tags":["fediurbanism","fedicon2025","fediverse","thirdspace","community"],"summary":"FediUrbanism \u2014 Johanna Botari \u2014 FediCon 2025 \u2192 https://spectra.video/w/f7GBsBf3nAhF8WfTMWUEXm I really enjoyed this talk by @johannab on the concept of FediUrbanism.","keywords":["fediurbanism","johanna","botari","fedicon","2025","https://spectra","video/w/f7gbsbf3nahf8wftmwuexm","really","enjoyed","talk"]},{"id":"snippets-owncast-optimized-azure-deployment-guide","title":"Deploy Owncast to Azure Container Apps with Persistent Storage","content":"Description This guide shows how to deploy Owncast to Azure Container Apps with persistent storage, scale-to-zero capability, and proper SQLite compatibility to minimize costs while ensuring reliable operation. Key Features: Ultra-cost-optimized configuration (~$0.60/month for 4 hours of streaming) Full data persistence (users, chat, federation, configuration) Scale-to-zero capability when not streaming SQLite \u002B Azure Files compatibility fixes RTMP \u002B HTTP dual-port configuration for OBS Studio Prerequisites Azure CLI installed and logged in An Azure subscription A resource group created Step 1: Create Required Resources Create Storage Account for Persistent Data # Set variables (choose cheapest regions) RESOURCE_GROUP=\u0026quot;your-resource-group\u0026quot; LOCATION=\u0026quot;centralus\u0026quot; # Often cheaper than eastus STORAGE_ACCOUNT=\u0026quot;owncaststorage$(date \u002B%s)\u0026quot; # Must be globally unique CONTAINER_APP_ENV=\u0026quot;owncast-env\u0026quot; CONTAINER_APP_NAME=\u0026quot;owncast-app\u0026quot; # Create MINIMAL cost storage account az storage account create \\ --name $STORAGE_ACCOUNT \\ --resource-group $RESOURCE_GROUP \\ --location $LOCATION \\ --sku Standard_LRS \\ --kind StorageV2 \\ --access-tier Cool \\ --allow-blob-public-access false \\ --https-only true \\ --min-tls-version TLS1_2 # Create file share with minimal provisioned size az storage share create \\ --name \u0026quot;owncast-data\u0026quot; \\ --account-name $STORAGE_ACCOUNT \\ --quota 1 # Start with 1GB, scales automatically Create Container Apps Environment # Create the Container Apps environment az containerapp env create \\ --name $CONTAINER_APP_ENV \\ --resource-group $RESOURCE_GROUP \\ --location $LOCATION Step 2: Configure Storage Mount Get the storage account key and create the storage mount: # Get storage account key STORAGE_KEY=$(az storage account keys list \\ --account-name $STORAGE_ACCOUNT \\ --resource-group $RESOURCE_GROUP \\ --query \u0026quot;[0].value\u0026quot; -o tsv) # Create storage mount in the environment az containerapp env storage set \\ --name $CONTAINER_APP_ENV \\ --resource-group $RESOURCE_GROUP \\ --storage-name \u0026quot;owncast-storage\u0026quot; \\ --azure-file-account-name $STORAGE_ACCOUNT \\ --azure-file-account-key $STORAGE_KEY \\ --azure-file-share-name \u0026quot;owncast-data\u0026quot; \\ --access-mode ReadWrite Step 3: Deploy Owncast Container App Create the container app with persistent storage: az containerapp create \\ --name $CONTAINER_APP_NAME \\ --resource-group $RESOURCE_GROUP \\ --environment $CONTAINER_APP_ENV \\ --image \u0026quot;owncast/owncast:latest\u0026quot; \\ --target-port 8080 \\ --ingress external \\ --min-replicas 0 \\ --max-replicas 1 \\ --cpu 0.5 \\ --memory 1Gi \\ --volume-mount \u0026quot;data:/app/data\u0026quot; \\ --volume-name \u0026quot;data\u0026quot; \\ --volume-storage-name \u0026quot;owncast-storage\u0026quot; \\ --volume-storage-type AzureFile Step 4: Configure Dual-Port Ingress (HTTP \u002B RTMP) For Owncast to work properly, you need both HTTP (8080) and RTMP (1935) ports. This requires a Virtual Network (VNet) integration: Create VNet and Subnet # Create MINIMAL virtual network (smallest possible address space) az network vnet create \\ --name \u0026quot;owncast-vnet\u0026quot; \\ --resource-group $RESOURCE_GROUP \\ --location $LOCATION \\ --address-prefix \u0026quot;10.0.0.0/24\u0026quot; # Smaller than default /16 # Create minimal subnet az network vnet subnet create \\ --name \u0026quot;container-apps-subnet\u0026quot; \\ --resource-group $RESOURCE_GROUP \\ --vnet-name \u0026quot;owncast-vnet\u0026quot; \\ --address-prefix \u0026quot;10.0.0.0/27\u0026quot; # Only 32 IPs instead of /23 (512 IPs) Recreate Container Apps Environment with VNet # Get subnet ID SUBNET_ID=$(az network vnet subnet show \\ --name \u0026quot;container-apps-subnet\u0026quot; \\ --vnet-name \u0026quot;owncast-vnet\u0026quot; \\ --resource-group $RESOURCE_GROUP \\ --query id -o tsv) # Delete existing environment and recreate with VNet az containerapp env delete \\ --name $CONTAINER_APP_ENV \\ --resource-group $RESOURCE_GROUP \\ --yes # Create Container Apps environment with workload profiles DISABLED (cheapest option) az containerapp env create \\ --name $CONTAINER_APP_ENV \\ --resource-group $RESOURCE_GROUP \\ --location $LOCATION \\ --infrastructure-subnet-resource-id $SUBNET_ID \\ --enable-workload-profiles false # Forces consumption-only pricing Deploy Container App with MINIMAL Resources az containerapp create \\ --name $CONTAINER_APP_NAME \\ --resource-group $RESOURCE_GROUP \\ --environment $CONTAINER_APP_ENV \\ --image \u0026quot;owncast/owncast:latest\u0026quot; \\ --target-port 8080 \\ --exposed-port 1935 \\ --ingress external \\ --transport auto \\ --min-replicas 0 \\ --max-replicas 1 \\ --cpu 0.25 \\ --memory 0.5Gi \\ --volume-mount \u0026quot;data:/app/data\u0026quot; \\ --volume-name \u0026quot;data\u0026quot; \\ --volume-storage-name \u0026quot;owncast-storage\u0026quot; \\ --volume-storage-type AzureFile Cost-Optimized Resource Allocation: CPU : 0.25 cores (minimum allowed, sufficient for small streams) Memory : 0.5Gi (minimum allowed, will work for basic streaming) Scaling : Aggressive scale-to-zero with max 1 replica Ultra Low-Cost Alternative YAML Configuration For maximum cost optimization, use this YAML approach with the smallest possible resource allocation. This configuration includes critical SQLite compatibility fixes for Azure Files: # owncast-production-ready.yaml properties: configuration: ingress: external: true targetPort: 8080 additionalPortMappings: - external: true targetPort: 1935 exposedPort: 1935 transport: tcp secrets: [] template: containers: - image: owncast/owncast:latest name: owncast-app resources: cpu: 0.25 memory: 0.5Gi volumeMounts: - mountPath: /app/data volumeName: data env: - name: OWNCAST_RTMP_PORT value: \u0026quot;1935\u0026quot; - name: OWNCAST_WEBSERVER_PORT value: \u0026quot;8080\u0026quot; - name: OWNCAST_DATABASE_FILE value: \u0026quot;/app/data/db/owncast.db\u0026quot; - name: OWNCAST_DATABASE_JOURNAL_MODE value: \u0026quot;DELETE\u0026quot; - name: OWNCAST_LOG_DIRECTORY value: \u0026quot;/app/data/logs\u0026quot; - name: OWNCAST_DATA_DIRECTORY value: \u0026quot;/app/data\u0026quot; - name: OWNCAST_HLS_DIRECTORY value: \u0026quot;/app/data/hls\u0026quot; scale: minReplicas: 0 maxReplicas: 1 rules: - name: \u0026quot;http-rule\u0026quot; http: metadata: concurrentRequests: \u0026quot;10\u0026quot; volumes: - name: data storageType: AzureFile storageName: owncast-storage Deploy with: az containerapp create \\ --name $CONTAINER_APP_NAME \\ --resource-group $RESOURCE_GROUP \\ --yaml owncast-production-ready.yaml Critical Configuration Notes SQLite \u002B Azure Files Compatibility Fix Important : The default Owncast configuration can cause \u0026quot;database is locked\u0026quot; errors when using Azure Files storage due to SQLite\u0027s WAL (Write-Ahead Logging) mode being incompatible with network file systems. The configuration above includes these critical fixes: Database Subdirectory : OWNCAST_DATABASE_FILE=/app/data/db/owncast.db - Places database in a subdirectory which improves Azure Files compatibility Journal Mode Override : OWNCAST_DATABASE_JOURNAL_MODE=DELETE - Forces SQLite to use DELETE mode instead of WAL mode to prevent file locking issues Full Data Mount : Mounts entire /app/data directory for complete persistence of logs, database, and HLS content What Gets Persisted With this configuration, the following data persists across container restarts: User accounts and chat history (critical for community continuity) Stream keys and admin credentials (prevents reconfiguration) ActivityPub federation data (maintains Mastodon/Fediverse followers) Server customization and branding (logos, site name, descriptions) API tokens and webhook configurations (third-party integrations) Complete logs and HLS video segments Without proper persistence, you would lose all users, followers, and configuration on every container restart. Cost Optimization Features Scale-to-Zero Configuration Min Replicas : Set to 0 to completely scale down when not in use Max Replicas : Set to 1 (Owncast doesn\u0027t need horizontal scaling) Scale Rules : Container Apps will automatically scale up when requests arrive Resource Limits (Ultra Cost-Optimized) CPU : 0.25 cores (absolute minimum, sufficient for 1-2 viewer streams) Memory : 0.5Gi (minimum allowed by Azure Container Apps) Storage : Cool tier with 1GB initial quota (auto-scales as needed) Network : Minimal VNet addressing to reduce overhead OBS Configuration After deployment, configure OBS for streaming: Server Settings : Use rtmp://your-app-url:1935/live (note: rtmp:// not https:// ) Stream Key : Use the key from Owncast admin panel (Configuration \u0026gt; Server Setup \u0026gt; Stream Keys) Owncast Web Interface : Access at https://your-app-url (port 8080 is handled automatically by ingress) Persistent Data : All Owncast configuration, database, and uploaded files are stored in Azure Files and persist across container restarts and scale-to-zero events. Critical : Uses SQLite DELETE journal mode to prevent \u0026quot;database is locked\u0026quot; errors with Azure Files. Cold Start : When scaling from zero, there will be a 10-15 second cold start delay as the container initializes. This is normal and acceptable for personal streaming. VNet Requirement : For dual-port access (HTTP \u002B RTMP), you must use a Virtual Network integration. This is a requirement for exposing additional TCP ports in Azure Container Apps. Database Compatibility : The configuration includes specific environment variables ( OWNCAST_DATABASE_JOURNAL_MODE=DELETE ) to ensure SQLite works properly with Azure Files network storage. Without these settings, you\u0027ll experience database crashes. Security Configuration : After deployment, immediately change the default admin credentials: Navigate to https://your-app-url/admin Default login: admin / abc123 Go to Configuration \u0026gt; Server Setup and change the admin password Create/copy stream keys from Configuration \u0026gt; Server Setup \u0026gt; Stream Keys tab Custom Domain : You can configure a custom domain using: az containerapp hostname add \\ --name $CONTAINER_APP_NAME \\ --resource-group $RESOURCE_GROUP \\ --hostname \u0026quot;your-domain.com\u0026quot; SSL Certificate : Azure Container Apps provides automatic SSL certificates for custom domains. Monitoring and Troubleshooting Check your deployment: # Get the URL az containerapp show \\ --name $CONTAINER_APP_NAME \\ --resource-group $RESOURCE_GROUP \\ --query properties.configuration.ingress.fqdn # Test RTMP port connectivity (should return TcpTestSucceeded: True) Test-NetConnection -ComputerName \u0026quot;your-app-url\u0026quot; -Port 1935 -InformationLevel Detailed # Test web interface curl -I https://your-app-url/ # View logs az containerapp logs show \\ --name $CONTAINER_APP_NAME \\ --resource-group $RESOURCE_GROUP Common Issues and Solutions Problem : Container logs show \u0026quot;database is locked\u0026quot; errors and crashes Solution : Ensure you\u0027re using the configuration above with OWNCAST_DATABASE_JOURNAL_MODE=DELETE and database in a subdirectory Problem : OBS shows \u0026quot;Failed to connect to server\u0026quot; Solution : Verify RTMP port 1935 is accessible using Test-NetConnection and ensure you\u0027re using VNet integration Problem : Admin panel won\u0027t load or shows errors Solution : Database persistence issues - check that the /app/data mount is working and SQLite journal mode is set correctly Problem : Container scales to zero but doesn\u0027t start properly Solution : Cold start delay is normal (10-15 seconds). If it doesn\u0027t start, check environment variable configuration Cost Estimation (Ultra-Optimized) With these optimizations, your monthly costs should be: When Streaming (4 hours/month example): Compute : ~$0.50/month (0.25 CPU \u002B 0.5Gi RAM \u00D7 4 hours) Container Apps Environment : ~$0.00 (consumption plan, no dedicated resources) Networking : ~$0.05/month (minimal VNet overhead) When Idle (Scale-to-Zero): Compute : $0.00 (scaled to zero) Environment : $0.00 (consumption plan) Always-On Costs: Storage : ~$0.05-0.10/month (1-2GB in Cool tier) VNet : ~$0.00 (no gateways or dedicated resources) Total Monthly Cost: ~$0.60-0.65/month (assuming 4 hours of streaming) Performance Expectations at Minimal Resources: 0.25 CPU \u002B 0.5Gi RAM : Suitable for 480p-720p streams with 1-5 concurrent viewers Scale-up Path : Monitor performance and increase to 0.5 CPU \u002B 1Gi if needed Cold Start : ~10-15 seconds when scaling from zero (acceptable for personal streaming)","contentType":"snippets","url":"https://www.lqdev.me/resources/snippets/owncast-optimized-azure-deployment-guide","date":"2025-08-14 13:35 -05:00","tags":["owncast","fediverse","azure","livestream"],"summary":"Description This guide shows how to deploy Owncast to Azure Container Apps with persistent storage, scale-to-zero capability, and proper SQLite compatibility to minimize costs while ensuring reliable...","keywords":["\u0026quot","owncast","group","storage","container","resource","create","configuration","database","azure"]},{"id":"reshare-introducing-gemma-3-270m","title":"Introducing Gemma 3 270M","content":"Introducing Gemma 3 270M \u2192 https://developers.googleblog.com/en/introducing-gemma-3-270m/ Today, we\u0027re adding a new, highly specialized tool to the Gemma 3 toolkit: Gemma 3 270M, a compact, 270-million parameter model designed from the ground up for task-specific fine-tuning with strong instruction-following and text structuring capabilities already trained in.","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-gemma-3-270m","date":"2025-08-14 12:40 -05:00","tags":["ai","google","gemma","slm","genai"],"summary":"Introducing Gemma 3 270M \u2192 https://developers.googleblog.com/en/introducing-gemma-3-270m/ Today, we\u0027re adding a new, highly specialized tool to the Gemma 3 toolkit: Gemma 3 270M, a compact,...","keywords":["gemma","270m","introducing","capabilities","structuring","text","following","instruction","strong","tuning"]},{"id":"posts-website-mobile-publishing-discord-client","title":"Mobile-First Static Site Publishing: Discord Bot Pipeline via Azure and GitHub","content":"## The problem Ever since I published my [first note](/notes/hello-world)([microblog post](https://indieweb.org/note)) on my website, I\u0027ve always wanted a way to quickly publish while on the go. Unfortunately, I never found a good solution. Because my website is statically generated and the source is hosted on GitHub(check out the [colophon](/colophon) for more details), there is no backend for me to talk to. At the same time, I didn\u0027t want to build an entire backend to support my website because I want to keep things as lean and cost-efficient as possible. Since my posts are just frontmatter and Markdown, I [use VS Code as my editor](/uses). For some time, back when I used to have a Surface Duo, I [authored posts from mobile using the github.dev experience](https://www.lqdev.me/notes/surface-duo-blogging-github-dev). On two screens, while not ideal, it was manageable. After switching devices (because sadly there were no more security updates on the Surface Duo) and upgrading to a dumbphone and later a single screen smartphone, that workflow wasn\u0027t feasible. At that point, what I resorted to was sending messages to myself via [Element](https://element.io/). The message would contain a link I wanted to check out later. Once I was on my laptop, I would check out the link and if I wanted to post about it on my website, I\u0027d do so then. That process, while it worked, wasn\u0027t necessarily scalable. In part that\u0027s a feature because I could spend more time digesting the content and writing a thoughtful article. However, it stopped me from sharing more in the moment and there were posts that were never authored or bookmarks that weren\u0027t captured because eventually that link got lost in the river of other links. Basically what I wanted to replicate was the more instant posting that social media gives you, but do so on my own site. That led me to doing some thinking and requirement gathering around the type of experience I wanted to have. ## Requirements When it came to requirements for my solution, I was focused more on the workflow and experience rather on technical details. Here is a list of those solution requirements: - Mobile is the primary publishing interface. Desktop publishing is a nice to have. - Be as low-friction as sharing a link via Element or posting on social media - Doesn\u0027t require implementing my own client or frontend - Doesn\u0027t require me to use existing micropub clients - Handles short-form and media posts supported by my website - Notes - Responses - Repost - Reply - Like - Bookmark - Media - Image - Audio (not used as often but technically supported) - Video (not used as often but technically supported) - Low-cost ## The solution For years, I had been struggling with actually implementing this system. The main part that gave me pause was not implementing my own client or relying on existing micropub clients. Eventually, I just accepted that it might never happen. One day, it eventually hit me. If the notes to self Element workflow worked so well, why not use a chat client as the frontend for publishing. At least have it serve as the capture system that would then format the content into a post that gets queued for publishing on GitHub. I\u0027d seen [Benji do something similar with his micropub endpoint](https://github.com/benjifs/serverless-micropub). While I could\u0027ve used Element since that\u0027s my preferred platform, I\u0027ve been contemplating no longer hosting my own Matrix server. So if I went through with this, I\u0027d want something that I didn\u0027t feel bad about investing the time on this solution if that chat client went away. That then left me with Discord as the next best option. Primarily because of its support for bots as well as its cross-platform support across mobile and desktop. In the end, the solution then ended up being fairly straightforward. More importantly, with the help of AI, I wrote none of the code. Using Copilot and Claude Sonnet 4, I was able to go from idea to deployment in 1-2 days. At that time the solution supported all of the posts except for media which I hadn\u0027t figured out what the best way of uploading media through Discord was. Figuring that out, implementing it, and deploying it took another day or two. Since I wanted for my solution to be as low-cost as possible, serverless seemed like a good option. I only pay for compute when it\u0027s actually being used which can be infrequent in my case. I don\u0027t need the server running 24/7 or even to be powerful. However, I didn\u0027t want to write my system as an Azure Function. I wanted the flexibility of deploying on a shared VM or container. A VM though wasn\u0027t an option since it\u0027s running 24/7. Keeping all of that in mind, my choice was narrowed down to Azure Container Apps which gave me the characteristics I was looking for. Serverless containers. Once that decision was made, I used Copilot again to figure out how to optimize my container image so that it\u0027s space and resource efficient. And while at it, I used Copilot again to figure out the right incantations to get the container deployed to Azure Container Apps. All-in-all, the solution had been staring at me in the face since I already had a workflow that for the most part worked for me, it just needed some optimizations and with the help of AI, I was able to quickly build and deploy something I\u0027d been ruminating over for years. ## Workflow The workflow for publishing is as follows: 1. Invoke the bot in Discord to capture my input using slash command \u0060/post\u0060 and the respective post type. ![Using slash commands to invoke discord publishing bot](http://cdn.lqdev.tech/files/images/invoke-discord-publishing-bot.png) 1. Provide post details. For media posts, I can provide an attachment which gets uploaded to Azure Blob Storage. ![A modal in discord with note post fields filled in](http://cdn.lqdev.tech/files/images/discord-publishing-client-modal.png) 1. [Bot creates a branch and PR in my repo](https://github.com/lqdev/luisquintanilla.me/pull/152) with the post content 1. While logged into GitHub from my phone, if everything looks good, I merge the PR which kicks off my GitHub Actions workflow to build and publish the site including the new post. 1. Post [displays on my website](/media/test-media-post-from-mobile/). ## Challenges The solution is not perfect. One of the problems I\u0027ve run into is cold-start. Since I scale my solution down to zero when it\u0027s not being used to save on costs, I suffer from the cold start problem. Therefore, when I first invoke the bot, it fails. I have to give it a few seconds and retry the invocation. It\u0027s usually about 5 seconds so it\u0027s not a huge issue but it does add some friction. ## Next steps Overall I\u0027m happy with my solution but there are a few improvements I\u0027d like to make. - **Open-source the repo** - Currently I\u0027ve kept the repo private since it was all AI generated. Since my system is already in production and processes were documented, I need to do a more thorough pass to make sure that no secrets or credentials are checked in or documented anywhere. - **Improve UX** - Discord limits modal fields to 5. Therefore, I\u0027m playing around with the right balance between how much of the input should come from slash commands and how much should come from the modal. - **Expand supported post types** - I\u0027d like to expand the number of posts supported by my publishing client. Reviews are a good example of the type of post I\u0027d like to support as well as RSVPs. [Reviews I already support](https://www.lqdev.me/reviews) on my website but RSVPs I don\u0027t yet. Also, I\u0027d have to fix my Webmentions which are currently broken after [upgrading my website](/notes/hello-world-new-site-2025-08/). - **Make it generator agnostic** - Currently this only works for my website. With a few tweaks and refactoring, I think I can get the project to a place where it should work with other popular static site generators. - **One-click deployment** - Currently the solution is packaged up as a container so it can be deployed from anywhere. I want to make it even simpler to deploy. One click if possible.","contentType":"posts","url":"https://www.lqdev.me/posts/website-mobile-publishing-discord-client","date":"2025-08-14 01:45 -05:00","tags":["discord","azure","indieweb","mobile","microblogging","web","bot","aca","azurecontainerapps"],"summary":"## The problem Ever since I published my [first note](/notes/hello-world)([microblog post](https://indieweb.org/note)) on my website, I\u0027ve always wanted a way to quickly publish while on the go....","keywords":["solution","post","website","since","from","publishing","media","used","more","posts"]},{"id":"star-i-deleted-my-second-brain","title":"I Deleted My Second Brain","content":"I Deleted My Second Brain \u2192 https://notes.jim-nielsen.com/#2025-08-12T2249 I don\u2019t build a second brain. I inhabit the first.","contentType":"star","url":"https://www.lqdev.me/responses/i-deleted-my-second-brain","date":"2025-08-13 08:03 -05:00","tags":["pkm","notes","secondbrain"],"summary":"I Deleted My Second Brain \u2192 https://notes.jim-nielsen.com/#2025-08-12T2249 I don\u2019t build a second brain. I inhabit the first.","keywords":["second","brain","deleted","https://notes","nielsen","com/#2025","12t2249","don\u2019t","build","inhabit"]},{"id":"media-test-media-post-from-mobile","title":"Test media post from mobile","content":"Test media posting from mobile :::media - url: \u0022http://cdn.lqdev.tech/files/images/20250813_034035_1000002298.jpg\u0022 alt: \u0022Closeup image of a pink flower\u0022 mediaType: \u0022image\u0022 aspectRatio: \u0022landscape\u0022 caption: \u0022Test media post from mobile\u0022 :::media","contentType":"media","url":"https://www.lqdev.me/media/test-media-post-from-mobile","date":"2025-08-12 22:40 -05:00","tags":["mobile","blog","indieweb"],"summary":"Test media posting from mobile :::media - url: \u0022http://cdn.lqdev.tech/files/images/20250813_034035_1000002298.jpg\u0022 alt: \u0022Closeup image of a pink flower\u0022 mediaType: \u0022image\u0022 aspectRatio: \u0022landscape\u0022...","keywords":["test","media","from","mobile",":::media","image","landscape","aspectratio:","mediatype:","flower"]},{"id":"reply-engineering-for-slow-internet","title":"Engineering for Slow Internet","content":"Engineering for Slow Internet \u2192 https://brr.fyi/posts/engineering-for-slow-internet Does this webapp really need to be 20 MB? What all is being loaded that could be deferred until it is needed, or included in an \u201Coptional\u201D add-on bundle? Is there a possibility of a \u201Clite\u201D version, for bandwidth-constrained users? While Antarctica is an edge case, this article illustrates some of the motivations behind my text-first website . By trimming the excess not only do you get to the core of the app or website, but it also loads faster.","contentType":"reply","url":"https://www.lqdev.me/responses/engineering-for-slow-internet","date":"2025-08-12 08:41 -05:00","tags":["slowweb","performance","internet","computing"],"summary":"Engineering for Slow Internet \u2192 https://brr.fyi/posts/engineering-for-slow-internet Does this webapp really need to be 20 MB? What all is being loaded that could be deferred until it is needed, or...","keywords":["slow","internet","website","engineering","while","antarctica","edge","case","article","illustrates"]},{"id":"bookmarks-awesome-self-hosted","title":"Awesome Self-Hosted","content":"Awesome Self-Hosted \u2192 https://awesome-selfhosted.net/index.html This is a list of Free Software network services and web applications which can be hosted on your own server(s).","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/awesome-self-hosted","date":"2025-08-11 23:24 -05:00","tags":["selfhosting","opensource","FOSS"],"summary":"Awesome Self-Hosted \u2192 https://awesome-selfhosted.net/index.html This is a list of Free Software network services and web applications which can be hosted on your own server(s).","keywords":["hosted","awesome","self","https://awesome","selfhosted","net/index","html","list","free","software"]},{"id":"reshare-stop-saving-everything","title":"Stop saving everything","content":"Stop saving everything \u2192 https://ellanew.com/2025/08/11/ptpl-168-stop-saving-everything-start-reflecting If your read-it-later list isn\u2019t getting cleared weekly, perhaps it\u2019s time to delete the lot. Save from a mindset of abundance, rather than scarcity, and process the things you\u2019ve saved each week (or month, at the most). If you are worried that something you deleted truly would have changed your life, just stop. STOP. You can\u2019t read it all, do it all, be it all. Trust that those potentially life-changing ideas will come around again, when you are ready for them Good reminder. Lately I\u0027ve been sending a lot of notes to myself with stuff to read but just haven\u0027t had the time to get to it. That said, the act of sending myself those notes is low friction that I don\u0027t feel FOMO when I don\u0027t get to read the articles and consume the media.","contentType":"reshare","url":"https://www.lqdev.me/responses/stop-saving-everything","date":"2025-08-11 23:00 -05:00","tags":["pkm","productivity","notetaking"],"summary":"Stop saving everything \u2192 https://ellanew.com/2025/08/11/ptpl-168-stop-saving-everything-start-reflecting If your read-it-later list isn\u2019t getting cleared weekly, perhaps it\u2019s time to delete the lot....","keywords":["stop","read","saving","everything","life","sending","those","notes","myself","come"]},{"id":"notes-new-text-first-site-and-search-are-live","title":"New text-first site and search are live","content":"Just iterated on another version of the website. The major change is that there\u0027s now a text-first version of the website for minimal and resource-constrained environments over at /text I\u0027ve also added search functionality, which you can find at /search","contentType":"notes","url":"https://www.lqdev.me/notes/new-text-first-site-and-search-are-live","date":"2025-08-11 20:57 -05:00","tags":["plaintext","indieweb","website"],"summary":"Just iterated on another version of the website. The major change is that there\u0027s now a text-first version of the website for minimal and resource-constrained environments over at /text I\u0027ve also...","keywords":["version","website","iterated","which","functionality","search","added","also","/text","environments"]},{"id":"bookmarks-url-town","title":"URL Town","content":"URL Town \u2192 https://url.town/ Love to see projects like this. url.town doesn\u2019t have any overly lofty ambitions; we\u2019re just building our own directory of really nice websites. We\u2019re not trying to fully recreate the original Yahoo! or DMOZ directories. We\u2019re not aiming for some astronomical number of links. This is just one space on the web, tied to a community that loves to share neat things with one another. Quality matters much more than quantity. There\u2019s no need to share everything just for the shake of sharing it; it\u2019s much better to share things that are useful or interesting.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/url-town","date":"2025-08-10 23:26 -05:00","tags":["indieweb","websites","directory"],"summary":"URL Town \u2192 https://url.town/ Love to see projects like this. url.town doesn\u2019t have any overly lofty ambitions; we\u2019re just building our own directory of really nice websites. We\u2019re not trying to fully...","keywords":["share","we\u2019re","town","things","much","everything","space","tied","community","loves"]},{"id":"reshare-blm-lands-marked-for-potential-sale-onx","title":"BLM Lands Marked for Potential Sale (OnX)","content":"BLM Lands Marked for Potential Sale (OnX) \u2192 https://www.onxmaps.com/onx-access-initiatives/public-land-sales-blm-rmp Sad to see what\u0027s happening with our public lands. Great to see folks staying vigilant and providing information of what\u0027s happening on the ground.","contentType":"reshare","url":"https://www.lqdev.me/responses/blm-lands-marked-for-potential-sale-onx","date":"2025-08-10 21:13 -05:00","tags":["onx","bureaulandmanagement","nature"],"summary":"BLM Lands Marked for Potential Sale (OnX) \u2192 https://www.onxmaps.com/onx-access-initiatives/public-land-sales-blm-rmp Sad to see what\u0027s happening with our public lands. Great to see folks staying...","keywords":["lands","happening","providing","vigilant","staying","folks","public","sales","information","land"]},{"id":"bookmarks-claude-code-emacs-integration","title":"Claude Code Emacs Integration","content":"Claude Code Emacs Integration \u2192 https://github.com/manzaltu/claude-code-ide.el Claude Code IDE for Emacs provides native integration with Claude Code CLI through the Model Context Protocol (MCP). Unlike simple terminal wrappers, this package creates a bidirectional bridge between Claude and Emacs, enabling Claude to understand and leverage Emacs\u2019 powerful features\u2014from LSP and project management to custom Elisp functions. This transforms Claude into a true Emacs-aware AI assistant that works within your existing workflow and can interact with your entire Emacs ecosystem.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/claude-code-emacs-integration","date":"2025-08-10 20:17 -05:00","tags":["emacs","anthropic","ai","claudecode"],"summary":"Claude Code Emacs Integration \u2192 https://github.com/manzaltu/claude-code-ide.el Claude Code IDE for Emacs provides native integration with Claude Code CLI through the Model Context Protocol (MCP)....","keywords":["claude","emacs","code","integration","features\u2014from","project","management","custom","elisp","functions"]},{"id":"notes-hello-world-new-site-2025-08","title":"Hello world from the new site","content":"Posting from my brand new redesigned website. I was working on it for about a month so I plan on doing a longer writeup on what has changed. There\u0027s still a few things that are broken, but for the most part, I\u0027m happy with the progress and the changes that need to be made are incremental. There\u0027s a ton of cleanup as well but again. That is not a blocker to publishing the site.","contentType":"notes","url":"https://www.lqdev.me/notes/hello-world-new-site-2025-08","date":"2025-08-06 17:01 -05:00","tags":["indieweb","note","website"],"summary":"Posting from my brand new redesigned website. I was working on it for about a month so I plan on doing a longer writeup on what has changed. There\u0027s still a few things that are broken, but for the...","keywords":["posting","blocker","again","cleanup","incremental","made","need","changes","progress","happy"]},{"id":"posts-indieweb-create-day-2025-07","title":"IndieWeb Create Day - July 2025","content":"Since it was a holiday weekend in the U.S. that kind of snuck up on me, I found myself with nothing planned on a Saturday. So I chose to spend it creating stuff for my website with the IndieWeb community during [IndieWeb Create Day](https://events.indieweb.org/2025/07/indieweb-create-day-3q2PTCbGioi9). Over the last few months I\u0027ve been overthinking my website redesign and while I\u0027ve made several attempts at it, I\u0027ve never been satisfied the outcome. I end up throwing away all the progress I\u0027ve made and go back to the drawing board. Yesterday, I decided to not let perfect be the enemy of good and the approach I took was creating just a simpler piece of functionality outside of my website. How I integrate it into my website is a future me problem. But I want to work from a place of creativity and complete freedom to think of what could be rather than what is. With that in mind, I set out to sketch out how I want to create and render media (image, audio, video) posts. The approach I took used a combination of front-matter YAML and custom markdown media extensions. The front-matter YAML is something that I already use for my website and it\u0027s something that I want to continue using. However, in contrast to my current website, I like that the front-matter was kept simple and only includes a basic amount of information. The actual post content was handled by my custom markdown extension which leveraged YAML-like syntax to define media content. What\u0027s great about this is that it is composable so once I got one type of media working, the rest for the most part \u0022just worked\u0022. I could even mix different media types within the same post with no additional work or code changes required. Once I had the skeleton, it was all about refactoring, documentation, adding finishing touches, and vibe-coding some CSS which Claude did a relatively good job with given the aesthetic I was going for. Overall, I\u0027m happy with the end result. ![A screenshot of a website post containing an image and audio player](http://cdn.lqdev.tech/files/images/indieweb-create-day-2025-07.png) For more details, you can check out the [repo](https://github.com/lqdev/indieweb-create-day-2025-07). At some point, I want to be able to integrate these media posts into my static site generator but for the time being, there are other kinds of posts such as reviews, RSVPs, and other [post types](https://indieweb.org/posts#Types_of_Posts) that I want to design and eventually also support on my website. I liked the approach I took this time around because it gave me the freedom to explore posibilities rather than constrain my creativity to what I\u0027ve already built. So I think I\u0027ll keep doing the same for subsequent post types. At the end of the day, it was nice seeing everyone else\u0027s projects. My favorite one was Cy\u0027s recipe website. I want to be like them when I grow up \uD83D\uDE42.","contentType":"posts","url":"https://www.lqdev.me/posts/indieweb-create-day-2025-07","date":"2025-07-06 20:09 -05:00","tags":["indieweb","iwc","community","website"],"summary":"Since it was a holiday weekend in the U.S. that kind of snuck up on me, I found myself with nothing planned on a Saturday. So I chose to spend it creating stuff for my website with the IndieWeb...","keywords":["website","media","want","create","post","posts","yaml","matter","took","approach"]},{"id":"notes-2025-07-06-weekly-post-summary","title":"Week of July 6, 2025 - Post Summary","content":"Notes Week of June 29, 2025 - Post Summary Crate Finds - June 2025 Crate Finds - May 2025 Website Post Statistics - June 2025 Responses Do not privatize federal land Tumblr\u2019s move to WordPress and fediverse integration is \u2018on hold\u2019","contentType":"notes","url":"https://www.lqdev.me/notes/2025-07-06-weekly-post-summary","date":"2025-07-06 14:48 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes Week of June 29, 2025 - Post Summary Crate Finds - June 2025 Crate Finds - May 2025 Website Post Statistics - June 2025 Responses Do not privatize federal land Tumblr\u2019s move to WordPress and...","keywords":["2025","june","post","crate","finds","notes","fediverse","wordpress","tumblr\u2019s","land"]},{"id":"notes-post-statistics-june-2025","title":"Website Post Statistics - June 2025","content":"I haven\u0027t published much this past month. Notes and responses are significantly down from last year. That said, I\u0027m happy that so far I\u0027ve published seven long-form blog posts, which is how many I published all of last year. I haven\u0027t spent as much time bookmarking and resharing content. Partially due to the fact that I\u0027m still working on the redesign. So far vibe-specing the redesign hasn\u0027t yielded good enough results. Maybe I need to break down my problem further. That experience on its own might make a good blog post. Part of the challenge with the redesign is that I\u0027m trying to find a balance between standardizing my front-matter YAML schemas so that I can simplify my code and just have a single function handle the parsing of the front-matter, while at the same time enabling custom rendering depending on the post type. I think in the end what I\u0027ll end up doing is having different schemas per post type and maybe refactoring some of my code to remove redundancies.","contentType":"notes","url":"https://www.lqdev.me/notes/post-statistics-june-2025","date":"2025-07-01 20:00 -05:00","tags":["githubactions","cicd","blogging","stats","github"],"summary":"I haven\u0027t published much this past month. Notes and responses are significantly down from last year. That said, I\u0027m happy that so far I\u0027ve published seven long-form blog posts, which is how many I...","keywords":["published","post","redesign","haven","matter","front","blog","schemas","last","year"]},{"id":"notes-crate-finds-june-2025","title":"Crate Finds - June 2025","content":"Playlist from June 2025. Some of my favorites: Claudia, Wilhelm R And Me by Roberto Musci Journey in Satchidananda by Alice Coltrane and Pharoah Sanders Little Sunflower by Dorothy Ashby Tracks Claudia, Wilhelm R And Me by Roberto Musci Album: Tower of Silence Duration: 2:57 Listen on YouTube Backup: Listen on Spotify Eternal Loop by Sam Gendel Album: Fresh Bread Duration: 4:44 Listen on YouTube Backup: Listen on Spotify Are These My Memories? by Steve Hiett Album: Girls In The Grass Duration: 2:55 Listen on YouTube Backup: Listen on Spotify Daddy\u0027s Gonna Tell You No Lie by Red Hot Org, Laraaji, Kronos Quartet, Sun Ra Album: Outer Spaceways Incorporated - Kronos Quartet \u0026amp; Friends Meet Sun Ra Duration: 5:35 Listen on YouTube Backup: Listen on Spotify Journey In Satchidananda by Alice Coltrane, Pharoah Sanders Album: Journey in Satchidananda Duration: 6:36 Listen on YouTube Backup: Listen on Spotify Little Sunflower by Dorothy Ashby Album: Afro-Harping Duration: 3:45 Listen on YouTube Backup: Listen on Spotify Time\u0027s A Wastin by Erykah Badu Album: Mama\u0027s Gun Duration: 6:40 Listen on YouTube Backup: Listen on Spotify Freedom Flight by Shuggie Otis Album: Freedom Flight Duration: 12:56 Listen on YouTube Backup: Listen on Spotify Pretty Darlin\u0027 by Eric Relph Album: Pretty Darlin Duration: 4:14 Listen on YouTube Backup: Listen on Spotify Josephine by Caoilfhionn Rose Album: Constellation Duration: 4:40 Listen on YouTube Backup: Listen on Spotify May Ninth by Khruangbin Album: A LA SALA Duration: 3:12 Listen on YouTube Backup: Listen on Spotify Dream Puppy by The Sweet Enoughs Album: Marshmallow Duration: 5:39 Listen on YouTube Backup: Listen on Spotify Crystal Blue Persuasion by Tommy James \u0026amp; The Shondells Album: Crimson \u0026amp; Clover Duration: 4:02 Listen on YouTube Backup: Listen on Spotify Witchoo by Durand Jones \u0026amp; The Indications, Durand Jones, Aaron Frazer Album: Private Space Duration: 3:42 Listen on YouTube Backup: Listen on Spotify Poly Blue by Jessica Pratt Album: Quiet Signs Duration: 2:43 Listen on YouTube Backup: Listen on Spotify Mona Lisa Overdrive by Juno Reactor Album: The Matrix Reloaded: The Album Duration: 10:08 Listen on YouTube Backup: Listen on Spotify Documents by Slick Rick, Nas Album: VICTORY Duration: 2:49 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-june-2025","date":"2025-07-01 20:00 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"Playlist from June 2025. Some of my favorites: Claudia, Wilhelm R And Me by Roberto Musci Journey in Satchidananda by Alice Coltrane and Pharoah Sanders Little Sunflower by Dorothy Ashby Tracks...","keywords":["listen","spotify","duration:","album:","backup:","youtube","\u0026amp","journey","satchidananda","blue"]},{"id":"reply-tumblr-wordpress-fediverse-integration-pause","title":"Tumblr\u2019s move to WordPress and fediverse integration is \u2018on hold\u2019","content":"Tumblr\u2019s move to WordPress and fediverse integration is \u2018on hold\u2019 \u2192 https://www.theverge.com/news/695124/tumblr-wordpress-automattic-fediverse-integration-on-hold-decoder That\u0027s unfortunate. Having Tumblr as your frontend for posting and curating feeds, with WordPress as the backend and integrated with decentralized networks like the Fediverse for distribution would\u0027ve been amazing. It sounds like Tumblr is still strategic to Automattic and as more people move away from the centralized social platforms and towards their own corners of the internet, having these differente building blocks in place will be even more important.","contentType":"reply","url":"https://www.lqdev.me/responses/tumblr-wordpress-fediverse-integration-pause","date":"2025-06-30 20:21 -05:00","tags":["tumblr","wordpress","fediverse","automattic"],"summary":"Tumblr\u2019s move to WordPress and fediverse integration is \u2018on hold\u2019 \u2192 https://www.theverge.com/news/695124/tumblr-wordpress-automattic-fediverse-integration-on-hold-decoder That\u0027s unfortunate. Having...","keywords":["wordpress","fediverse","integration","more","automattic","having","tumblr","tumblr\u2019s","people","away"]},{"id":"notes-crate-finds-may-2025","title":"Crate Finds - May 2025","content":"Better late than never. Here\u0027s the May 2025 drop. Some of my favorites from last month: Prayer to the Stars by Vianney Lopez, Sam Bottner Diarabi by Viex Farka Toure, Khruangbin Everything \u0026amp; Nothing, OHMA Bloom, Deya Dova Dance of the Seven Sisters, Deya Dova Wisdom Eye by Alice Coltrane Black Meditation by Shabaka Tracks La Casita Blanca - Rebajada by Bilo Alban, Hector Manito Bonilla Album: El \u00D3rgano Electr\u00F3nico de Bilo Alb\u00E1n Duration: 3:02 Listen on YouTube Backup: Listen on Spotify Cumbia Disco Energy by Sonido Gallo Negro, Polymarchs Album: Cumbia Disco Energy Duration: 3:57 Listen on YouTube Backup: Listen on Spotify El Mercado De Los Brujos by Sonido Gallo Negro Album: Cumbia Salvaje Duration: 3:31 Listen on YouTube Backup: Listen on Spotify Cumbia Triste by Sonido Gallo Negro, Grupo Kual? Album: Paganismo Duration: 3:51 Listen on YouTube Backup: Listen on Spotify Mulberry Jam by Allah-Las Album: Mulberry Jam (Peaking Lights Dub) Duration: 3:17 Listen on YouTube Backup: Listen on Spotify Sozin by Pedro Mizutani, Skinshape Album: Sozin Duration: 3:28 Listen on YouTube Backup: Listen on Spotify TTT by Bluestaeb, Melodiesinfonie, S. Fidelity, douniah Album: TTT Duration: 3:21 Listen on YouTube Backup: Listen on Spotify Selenge by C\u00E9line Dessberg Album: Selenge Duration: 3:21 Listen on YouTube Backup: Listen on Spotify Spring Has Sprung by \u0421\u041E\u042E\u0417, Asha Puthli, Sven Wunder Album: Spring Has Sprung Duration: 2:08 Listen on YouTube Backup: Listen on Spotify Voyage by anaiis, Grupo Cosmo, Sessa Album: anaiis \u0026amp; Grupo Cosmo Duration: 3:46 Listen on YouTube Backup: Listen on Spotify Toda Cor by anaiis, Grupo Cosmo, Luedji Luna Album: anaiis \u0026amp; Grupo Cosmo Duration: 3:51 Listen on YouTube Backup: Listen on Spotify S\u00E3o S\u00F3 Palavras by Nyron Higor, alici, Bruno Berle Album: Nyron Higor Duration: 1:30 Listen on YouTube Backup: Listen on Spotify Conversations by Carlos Ni\u00F1o \u0026amp; Friends, Andr\u00E9 3000, Nate Mercereau, Cavana Lee, Mia Doi Todd Album: (I\u0027m just) Chillin\u0027, on Fire Duration: 5:36 Listen on YouTube Backup: Listen on Spotify Together Is A Beautiful Place To Be - Nala Sinephro Remix by Nubya Garcia, Nala Sinephro Album: SOURCE \u29FA WE MOVE Duration: 2:27 Listen on YouTube Backup: Listen on Spotify Space 5 by Nala Sinephro Album: Space 1.8 Duration: 4:00 Listen on YouTube Backup: Listen on Spotify Sun by Skinshape Album: Umoja Duration: 3:41 Listen on YouTube Backup: Listen on Spotify The Touch of You by The Equatics Album: Doin It!!!! Duration: 3:36 Listen on YouTube Backup: Listen on Spotify manos ajenas (touch you every day) by Dos Santos Album: Logos Duration: 2:34 Listen on YouTube Backup: Listen on Spotify Prayer to the Stars by Vianney Lopez, Sam Bottner Album: Ceremonia Duration: 3:26 Listen on YouTube Backup: Listen on Spotify Moving Through by Alexa Sunshine Rose, Aim\u00E9e Ringle Album: Come Receive Duration: 4:40 Listen on YouTube Backup: Listen on Spotify Bloom by Deya Dova Album: The Jasmani Garden Duration: 4:03 Listen on YouTube Backup: Listen on Spotify Turquoise Galaxy by Yussef Dayes Album: Black Classical Music Duration: 3:11 Listen on YouTube Backup: Listen on Spotify Dive into the Medicine Within by Vianney Lopez Album: Spirit Medicine Duration: 2:17 Listen on YouTube Backup: Listen on Spotify Fingerprints by Hiatus Kaiyote Album: Choose Your Weapon Duration: 4:16 Listen on YouTube Backup: Listen on Spotify La Puerta by Los Tres Ases Album: Epoca De Oro Duration: 2:56 Listen on YouTube Backup: Listen on Spotify D\u00F3nde Est\u00E1s by Los Tres Caballeros Album: Los Tres Caballeros Duration: 2:49 Listen on YouTube Backup: Listen on Spotify Sabra Dios by Los Tres Ases Album: Tr\u00EDos Del Mundo : 40 \u00C9xitos Duration: 3:06 Listen on YouTube Backup: Listen on Spotify Lovesong by Common Saints Album: Idol Eyes Duration: 9:17 Listen on YouTube Backup: Listen on Spotify Sweetie by Kokoroko Album: Sweetie Duration: 4:04 Listen on YouTube Backup: Listen on Spotify Carta Ao Tom 74 by Vin\u00EDcius de Moraes, Toquinho, Quarteto Em Cy Album: 20 Grandes Sucessos De Vinicius De Moreas Duration: 2:41 Listen on YouTube Backup: Listen on Spotify Iceland by Daniel Lanois Album: Flesh And Machine Duration: 2:38 Listen on YouTube Backup: Listen on Spotify \u00C1gua by Zeep Album: Nina Miranda \u0026amp; Chris Franck Present: Zeep Duration: 3:36 Listen on YouTube Backup: Listen on Spotify Diarabi by Vieux Farka Tour\u00E9, Khruangbin Album: Ali Duration: 5:01 Listen on YouTube Backup: Listen on Spotify Wisdom Eye by Alice Coltrane Album: Eternity Duration: 3:10 Listen on YouTube Backup: Listen on Spotify Black meditation by Shabaka Album: Afrikan Culture Duration: 3:58 Listen on YouTube Backup: Listen on Spotify Everything \u0026amp; Nothing by OHMA Album: Between All Things Duration: 4:06 Listen on YouTube Backup: Listen on Spotify Take It Slow by Ayla Nereo Album: By the Light of the Dark Moon Duration: 3:06 Listen on YouTube Backup: Listen on Spotify Dance of the Seven Sisters by Deya Dova Album: The Jasmani Garden Duration: 4:38 Listen on YouTube Backup: Listen on Spotify Little Star by Little Whale, Danit Album: Movement of the Dawn Duration: 3:05 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-may-2025","date":"2025-06-29 18:00 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"Better late than never. Here\u0027s the May 2025 drop. Some of my favorites from last month: Prayer to the Stars by Vianney Lopez, Sam Bottner Diarabi by Viex Farka Toure, Khruangbin Everything \u0026amp;...","keywords":["listen","spotify","album:","duration:","youtube","backup:","\u0026amp","grupo","dova","deya"]},{"id":"reply-do-not-privatize-federal-land","title":"Do not privatize federal land","content":"Do not privatize federal land \u2192 https://marginalrevolution.com/marginalrevolution/2025/06/privatize-federal-land.html I\u2019ve long advocated selling off some federal land...Most of this \u201Cpublic land\u201D is never used by the public. Selling some of it would actually make it more accessible and useful to real people. How about no. If privatization means, John and Jane Doe can purchase some land for their homestead is one thing. We\u0027ve tried that before by the way - Homestead Act . In reality, it means some private equity firm or hedge fund buying the land, building expensive and shoddy cookit-cutter condos, and selling you a subscription for a roof over your head . I say condos because it\u0027s hot and there\u0027s not a lot of water in Vegas, so keeping those brand new AI data centers cool might be a problem. Also, if you haven\u0027t seen that area that Alex highlights in his post, it seems like a deserted landscape. And yes, the terrain can be rough, but actually, it\u0027s a beautiful landscape that is best left undisturbed.","contentType":"reply","url":"https://www.lqdev.me/responses/do-not-privatize-federal-land","date":"2025-06-29 17:26 -05:00","tags":["bureaulandmanagement","uspol","land"],"summary":"Do not privatize federal land \u2192 https://marginalrevolution.com/marginalrevolution/2025/06/privatize-federal-land.html I\u2019ve long advocated selling off some federal land...Most of this \u201Cpublic land\u201D is...","keywords":["land","selling","federal","condos","actually","landscape","means","homestead","data","brand"]},{"id":"notes-2025-06-29-weekly-post-summary","title":"Week of June 29, 2025 - Post Summary","content":"Notes Crate Finds - May 2025 Responses Do not privatize federal land","contentType":"notes","url":"https://www.lqdev.me/notes/2025-06-29-weekly-post-summary","date":"2025-06-29 14:00 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes Crate Finds - May 2025 Responses Do not privatize federal land","keywords":["notes","crate","finds","2025","responses","privatize","federal","land"]},{"id":"reshare-self-adapting-language-models-seal","title":"Self-Adapting Language Models (SEAL)","content":"Self-Adapting Language Models (SEAL) \u2192 https://jyopari.github.io/posts/seal Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL) \uD83E\uDDAD, a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit \u2014 a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop, using the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model\u0027s generation to parameterize and control its own adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation in response to new data. We demonstrate SEAL in two domains: (1) Knowledge Incorporation , where the model integrates new factual information by generating logical implications as synthetic data, and (2) Few-Shot Learning , where the model autonomously selects data augmentations and training hyperparameters to adapt to new abstract reasoning tasks. Code Paper","contentType":"reshare","url":"https://www.lqdev.me/responses/self-adapting-language-models-seal","date":"2025-06-16 20:33 -05:00","tags":["agent","ai","llm","mit","research"],"summary":"Self-Adapting Language Models (SEAL) \u2192 https://jyopari.github.io/posts/seal Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks,...","keywords":["self","model","data","seal","adaptation","adapt","llms","knowledge","language","models"]},{"id":"reshare-anthropic-how-we-built-multi-agent-system","title":"How we built our multi-agent research system - Anthropic","content":"How we built our multi-agent research system - Anthropic \u2192 https://www.anthropic.com/engineering/built-multi-agent-research-system Great article. TLDR: These are largely engineering problems. The journey of this multi-agent system from prototype to production taught us critical lessons about system architecture, tool design, and prompt engineering. A multi-agent system consists of multiple agents ( LLMs autonomously using tools in a loop ) working together. Our Research feature involves an agent that plans a research process based on user queries, and then uses tools to create parallel agents that search for information simultaneously. Systems with multiple agents introduce new challenges in agent coordination, evaluation, and reliability. Benefits of a multi-agent system The essence of search is compression: distilling insights from a vast corpus. Subagents facilitate compression by operating in parallel with their own context windows, exploring different aspects of the question simultaneously before condensing the most important tokens for the lead research agent. Each subagent also provides separation of concerns \u2014distinct tools, prompts, and exploration trajectories\u2014which reduces path dependency and enables thorough, independent investigations. Once intelligence reaches a threshold, multi-agent systems become a vital way to scale performance. For instance, although individual humans have become more intelligent in the last 100,000 years, human societies have become exponentially more capable in the information age because of our collective intelligence and ability to coordinate. Even generally-intelligent agents face limits when operating as individuals; groups of agents can accomplish far more . Our internal evaluations show that multi-agent research systems excel especially for breadth-first queries that involve pursuing multiple independent directions simultaneously. Multi-agent systems work mainly because they help spend enough tokens to solve the problem...Multi-agent architectures effectively scale token usage for tasks that exceed the limits of single agents. We\u2019ve found that multi-agent systems excel at valuable tasks that involve heavy parallelization, information that exceeds single context windows, and interfacing with numerous complex tools. Architecture overview Our Research system uses a multi-agent architecture with an orchestrator-worker pattern, where a lead agent coordinates the process while delegating to specialized subagents that operate in parallel. Traditional approaches using Retrieval Augmented Generation (RAG) use static retrieval. That is, they fetch some set of chunks that are most similar to an input query and use these chunks to generate a response. In contrast, our architecture uses a multi-step search that dynamically finds relevant information, adapts to new findings, and analyzes results to formulate high-quality answers . Prompt engineering and evaluations for research agents Think like your agents. To iterate on prompts, you must understand their effects. To help us do this, we built simulations using our Console with the exact prompts and tools from our system, then watched agents work step-by-step. Teach the orchestrator how to delegate. In our system, the lead agent decomposes queries into subtasks and describes them to subagents. Each subagent needs an objective , an output format , guidance on the tools and sources to use , and clear task boundaries . Without detailed task descriptions, agents duplicate work, leave gaps, or fail to find necessary information. Scale effort to query complexity. Agents struggle to judge appropriate effort for different tasks, so we embedded scaling rules in the prompts... Tool design and selection are critical. Agent-tool interfaces are as critical as human-computer interfaces. Using the right tool is efficient\u2014often, it\u2019s strictly necessary...Bad tool descriptions can send agents down completely wrong paths, so each tool needs a distinct purpose and a clear description. Let agents improve themselves. ...When given a prompt and a failure mode, they are able to diagnose why the agent is failing and suggest improvements. Start wide, then narrow down. Search strategy should mirror expert human research: explore the landscape before drilling into specifics. Guide the thinking process. ...Our testing showed that extended thinking improved instruction-following, reasoning, and efficiency. Subagents also plan, then use interleaved thinking after tool results to evaluate quality, identify gaps, and refine their next query. This makes subagents more effective in adapting to any task. Parallel tool calling transforms speed and performance. ...For speed, we introduced two kinds of parallelization: (1) the lead agent spins up 3-5 subagents in parallel rather than serially; (2) the subagents use 3\u002B tools in parallel. Our prompting strategy focuses on instilling good heuristics rather than rigid rules . We studied how skilled humans approach research tasks and encoded these strategies in our prompts\u2014strategies like decomposing difficult questions into smaller tasks , carefully evaluating the quality of sources , adjusting search approaches based on new information , and recognizing when to focus on depth (investigating one topic in detail) vs. breadth (exploring many topics in parallel) . Effective evaluation of agents ...evaluating multi-agent systems presents unique challenges...Because we don\u2019t always know what the right steps are, we usually can\u0027t just check if agents followed the \u201Ccorrect\u201D steps we prescribed in advance. Instead, we need flexible evaluation methods that judge whether agents achieved the right outcomes while also following a reasonable process. Start evaluating immediately with small samples. ...it\u2019s best to start with small-scale testing right away with a few examples, rather than delaying until you can build more thorough evals. LLM-as-judge evaluation scales when done well. ...We used an LLM judge that evaluated each output against criteria in a rubric: factual accuracy (do claims match sources?), citation accuracy (do the cited sources match the claims?), completeness (are all requested aspects covered?), source quality (did it use primary sources over lower-quality secondary sources?), and tool efficiency (did it use the right tools a reasonable number of times?)...[we] found that a single LLM call with a single prompt outputting scores from 0.0-1.0 and a pass-fail grade was the most consistent and aligned with human judgements. Human evaluation catches what automation misses. ...Adding source quality heuristics to our prompts helped resolve this issue. Even in a world of automated evaluations, manual testing remains essential. the best prompts for these agents are not just strict instructions, but frameworks for collaboration that define the division of labor , problem-solving approaches , and effort budgets . Getting this right relies on careful prompting and tool design , solid heuristics , observability , and tight feedback loops . Production reliability and engineering challenges Agents are stateful and errors compound. ...Agents can run for long periods of time, maintaining state across many tool calls. This means we need to durably execute code and handle errors along the way . Without effective mitigations, minor system failures can be catastrophic for agents. When errors occur, we can\u0027t just restart from the beginning...We combine the adaptability of AI agents built on Claude with deterministic safeguards like retry logic and regular checkpoints. Debugging benefits from new approaches ...Adding full production tracing let us diagnose why agents failed and fix issues systematically. Beyond standard observability, we monitor agent decision patterns and interaction structures\u2014all without monitoring the contents of individual conversations, to maintain user privacy. Deployment needs careful coordination ...we use rainbow deployments to avoid disrupting running agents, by gradually shifting traffic from old to new versions while keeping both running simultaneously. Synchronous execution creates bottlenecks. Currently, our lead agents execute subagents synchronously, waiting for each set of subagents to complete before proceeding. This simplifies coordination, but creates bottlenecks in the information flow between agents. Asynchronous execution would enable additional parallelism: agents working concurrently and creating new subagents when needed. But this asynchronicity adds challenges in result coordination, state consistency, and error propagation across the subagents. As models can handle longer and more complex research tasks, we expect the performance gains will justify the complexity. Conclusion For all the reasons described in this post, the gap between prototype and production is often wider than anticipated. Multi-agent research systems can operate reliably at scale with careful engineering , comprehensive testing , detail-oriented prompt and tool design , robust operational practices , and tight collaboration between research, product, and engineering teams who have a strong understanding of current agent capabilities.","contentType":"reshare","url":"https://www.lqdev.me/responses/anthropic-how-we-built-multi-agent-system","date":"2025-06-16 17:39 -05:00","tags":["agent","ai","anthropic","engineering"],"summary":"How we built our multi-agent research system - Anthropic \u2192 https://www.anthropic.com/engineering/built-multi-agent-research-system Great article. TLDR: These are largely engineering problems. The...","keywords":["agents","agent","multi","research","tool","subagents","system","tools","parallel","information"]},{"id":"notes-thee-sacred-souls-2025","title":"Thee Sacred Souls 2025","content":"Some pictures from the Thee Sacred Souls concert I attended a few weeks ago. I caught them in New York when the tour started, so it was nice to catch them at the end of the tour as well. In both cases, they were great performances.","contentType":"notes","url":"https://www.lqdev.me/notes/thee-sacred-souls-2025","date":"2025-06-14 12:30 -05:00","tags":["music","concert","theesacredsouls"],"summary":"Some pictures from the Thee Sacred Souls concert I attended a few weeks ago. I caught them in New York when the tour started, so it was nice to catch them at the end of the tour as well. In both...","keywords":["tour","pictures","cases","both","catch","nice","started","york","caught","weeks"]},{"id":"notes-local-mural-artist-websites","title":"Local Artist Websites","content":"I saw these really cool murals around the neighborhood. The artists signed their work either with their name or Instagram handle. I don\u0027t use Instagram, so it was nice to see that some of them also had websites where I could follow their work. Shawnimals Cyd Smillie","contentType":"notes","url":"https://www.lqdev.me/notes/local-mural-artist-websites","date":"2025-06-14 10:57 -05:00","tags":["art","website","openweb"],"summary":"I saw these really cool murals around the neighborhood. The artists signed their work either with their name or Instagram handle. I don\u0027t use Instagram, so it was nice to see that some of them also...","keywords":["instagram","really","cool","murals","around","neighborhood","artists","signed","either","handle"]},{"id":"reshare-mls-over-activitypub-draft","title":"MLS over ActivityPub Draft","content":"MLS over ActivityPub Draft \u2192 https://swicg.github.io/activitypub-e2ee/mls Messaging Layer Security (MLS) is an IETF standard for end-to-end encrypted (E2EE) messaging. It lets people on laptops and phones communicate with each other in a secure way that no one in between can see. MLS is designed to use pluggable lower-level protocols. This specification defines an envelope format for distributing MLS messages through the network, and an Activity Streams 2.0 profile for the packets of application data stored inside the messages. This specification is ready for review from both ActivityPub developers and security analysts. It\u2019s time to start making proof-of-concept implementations and testing interoperability. Source: https://socialwebfoundation.org/2025/06/13/mls-over-activitypub-draft/","contentType":"reshare","url":"https://www.lqdev.me/responses/mls-over-activitypub-draft","date":"2025-06-14 10:47 -05:00","tags":["protocol","socialweb","activitypub","w3c"],"summary":"MLS over ActivityPub Draft \u2192 https://swicg.github.io/activitypub-e2ee/mls Messaging Layer Security (MLS) is an IETF standard for end-to-end encrypted (E2EE) messaging. It lets people on laptops and...","keywords":["activitypub","specification","security","messaging","messages","implementations","activity","streams","profile","https://socialwebfoundation"]},{"id":"star-faircamp-static-site-audio-producers","title":"Faircamp - A static site generator for audio producers","content":"Faircamp - A static site generator for audio producers \u2192 https://simonrepp.com/faircamp/ This is such a cool project. Looks like it\u0027s super simple to get started with and it\u0027s packed with a ton of features so creators get to stay in their flow. I also like how creators are showcased.","contentType":"star","url":"https://www.lqdev.me/responses/faircamp-static-site-audio-producers","date":"2025-06-05 20:05 -05:00","tags":["faircamp","music","socialweb","indieweb","staticsite","technology","tools","openweb","opensource"],"summary":"Faircamp - A static site generator for audio producers \u2192 https://simonrepp.com/faircamp/ This is such a cool project. Looks like it\u0027s super simple to get started with and it\u0027s packed with a ton of...","keywords":["creators","faircamp","flow","stay","features","packed","started","simple","super","also"]},{"id":"posts-fediforum-day-1-jun-2025","title":"FediForum Day One Recap","content":"Just wrapped up a successful first day of FediForum. The vibes and energy were high. Tons of great conversations and projects around the social web. A few emerging themes I noticed: - Identity - Portability / Interoperability - Feeds - Commerce Ian Forrester kicked us off with his Public Service \u0026 The Fediverse keynote [(Slides)](https://docs.google.com/presentation/d/1HuQF_sI19zwByM793uvdl7BECneRmni_H9uOG-oFkdM/edit?slide=id.g420c517061_0_0#slide=id.g420c517061_0_0). One of the ideas that struck a chord of public service integrated into the fediverse. More specifically the interest that sparked in me was that publishing and social shouldn\u0027t be two separate things. Following the POSSE principle from the IndieWeb. You publish on your own site and then it\u0027s syndicated elsewhere. This was interesting enough for me I even hosted a session on the topic, I think it was called Tightening the Loop between CMS and the Fediverse. It was my first unconference, so I appreciated the way the agenda was built. Announce your topic, see whether there\u0027s interest, put it on the agenda, chat with fellow participants. Super easy. ## Identity \u0026 Portability These are such a huge topic but for the purpose of this post, I\u0027m lumping them together. [https://bounce-migrate.appspot.com/](https://bounce.so) is one of the projects aiming to make portability easy. What\u0027s so interesting is they\u0027re making it easy to migrate across protocols. So if you\u0027re in one network like ATProto (Bluesky), migrating to the Fediverse should be relatively seamless with https://bounce.so. Some great discussions that emerged on the topic as well include: - **Reputation** - How do you build a web of trust? - **Compartmentalization and Deduplication** - A single identity or multiple identities? When \u0022following\u0022 someone, which of their feeds takes priority? ## Feeds Talk of feeds was everywhere. I made a note to myself throughout the conference. \u003E It\u0027s amazing how big the feeds theme is. Feed ownership, customization, and sharing. All powered by open protocols. - [Bonfire releases 1.0](https://bonfirenetworks.org/posts/bonfire_social_rc/) - Congrats to the Bonfire team on this milestone. I haven\u0027t tried Bonfire myself, but the Circles feature caught my attention. It made me reminiscent of Google\u002B. - [Surf.Social is now in beta](https://https://surf.social) - As an avid user and curator of RSS feeds, I\u0027d heard about Surf before but hadn\u0027t really looked into it. The beta release was announced at the conference and I quickly was able to sign up and download it. Kudos to the team on this milestone and thanks for being so responsive to my request to join the beta. I did almost no waiting in the waitlist. Once I have a chance to try it out and get familiar with it, I\u0027ll share some thoughts. - [Channels](https://home.channel.org/) from the folks at [Newsmast Foundation](https://www.newsmastfoundation.org/) looks like an interesting way to curate and customize feeds. [Bring Your Own Timeline Algorithm](https://huggingface.co/spaces/mozilla-ai/byota) leverages semantic search to help you seamlessly leverage the power of algorithmic feeds but doing so under your control. Cool use of AI. There were a few unconference sessions on the topic as well. ## Commerce It was great to see folks talking about enabling creators to earn a living on open platforms and the social web. I believe [Bandwagon.fm](https://bandwagon.fm/home) showed of an implementation of a payments and subscription system built on top of [Emmisary](https://emissary.dev/), a social web toolkit. ## Additional Resources Here\u0027s a list of other links and projects I was exposed to during the conference. - [FediForum Advisory Board and Organizers](https://fediforum.org/people/) - [An Internet of Many Autonomous Communities](https://blog.rudyfraser.com/an-internet-of-many-autonomous-communities/) - [FediMeteo - Weather updates in your Fediverse Timeline](https://fedimeteo.com/) - [Public Interest Technology Group (PITG)](https://pitg.network/) - [Coalition for Content Provenance and Authenticity (C2PA)](https://c2pa.org/) - [A New Social](https://www.anew.social/) - [WeDistribute FediForum Live Coverage](https://wedistribute.org/2025/06/fediforum-june-2025/) - [BBC Human Values](https://bbc.github.io/humanvalues/) - [The Indie Beat Radio](https://theindiebeat.fm/) - [Tie Yourself to a Federated Mastodon](https://pluralistic.net/2024/11/02/ulysses-pact/#tie-yourself-to-a-federated-mast) - [The No Network Effect](https://snarfed.org/2013-04-13_the-no-network-effect) As always, Cory Doctorow was great way to close out the first day. I even learned a new term, tron-pilled. Which means as a creator of a platform, you\u0027re on the side of the users. Looking forward to tomorrow\u0027s sessions!","contentType":"posts","url":"https://www.lqdev.me/posts/fediforum-day-1-jun-2025","date":"2025-06-05 17:20 -05:00","tags":["fediforum","socialweb","community","fediverse","openweb"],"summary":"Just wrapped up a successful first day of FediForum. The vibes and energy were high. Tons of great conversations and projects around the social web. A few emerging themes I noticed: - Identity -...","keywords":["feeds","social","fediverse","topic","public","conference","interesting","portability","identity","easy"]},{"id":"reshare-state-of-the-art-prompting-ai-agents-y-combinator","title":"State-Of-The-Art Prompting For AI Agents","content":"State-Of-The-Art Prompting For AI Agents \u2192 https://www.youtube.com/watch?v=DL82mGde6wo","contentType":"reshare","url":"https://www.lqdev.me/responses/state-of-the-art-prompting-ai-agents-y-combinator","date":"2025-06-03 20:23 -05:00","tags":["ai","agents","prompting"],"summary":"State-Of-The-Art Prompting For AI Agents \u2192 https://www.youtube.com/watch?v=DL82mGde6wo","keywords":["state","prompting","agents","https://www","youtube","com/watch","v=dl82mgde6wo"]},{"id":"reshare-elevenlabs-conversational-ai-2-0","title":"Introducing ElevenLabs Conversational AI 2.0","content":"Introducing ElevenLabs Conversational AI 2.0 \u2192 https://elevenlabs.io/blog/conversational-ai-2-0 Conversational AI 2.0 launches with advanced features and enterprise readiness. Natural turn-taking to understand the flow of conversation. Multilingual communication with integrated language detection Integrated RAG: knowledgeable agents, minimum latency, maximum privacy Multimodality Batch calls","contentType":"reshare","url":"https://www.lqdev.me/responses/elevenlabs-conversational-ai-2-0","date":"2025-06-03 20:19 -05:00","tags":["ai","speech","elevenlabs"],"summary":"Introducing ElevenLabs Conversational AI 2.0 \u2192 https://elevenlabs.io/blog/conversational-ai-2-0 Conversational AI 2.0 launches with advanced features and enterprise readiness. Natural turn-taking to...","keywords":["conversational","integrated","introducing","multimodality","privacy","maximum","latency","minimum","agents","knowledgeable"]},{"id":"reshare-sakana-ai-gwm","title":"The Darwin G\u00F6del Machine - AI that improves itself by rewriting its own code","content":"The Darwin G\u00F6del Machine - AI that improves itself by rewriting its own code \u2192 https://sakana.ai/dgm/ A longstanding goal of AI research has been the creation of AI that can learn indefinitely. One tantalizing path toward that goal is an AI that improves itself by rewriting its own code, including any code responsible for learning. That idea, known as a G\u00F6del Machine, proposed by J\u00FCrgen Schmidhuber decades ago, is a hypothetical self-improving AI. It optimally solves problems by recursively rewriting its own code when it can mathematically prove a better strategy, making it a key concept in meta-learning or \u201Clearning to learn.\u201D While the theoretical G\u00F6del Machine promised provably beneficial self-modifications, its realization relied on an impractical assumption: that the AI could mathematically prove that a proposed change in its own code would yield a net improvement before adopting it. We, in collaboration with Jeff Clune\u2019s lab at UBC, propose something more feasible: a system that harnesses the principles of open-ended algorithms like Darwinian evolution to search for improvements that empirically improve performance. We call the result the Darwin G\u00F6del Machine (full technical report). DGMs leverage foundation models to propose code improvements, and use recent innovations in open-ended algorithms to search for a growing library of diverse, high-quality AI agents. Our experiments show that DGMs improve themselves the more compute they are provided. In line with the clear trend that AI systems that rely on learning ultimately outperform those designed by hand, there is a potential that DGMs could soon outperform hand-designed AI systems. Technical Report","contentType":"reshare","url":"https://www.lqdev.me/responses/sakana-ai-gwm","date":"2025-06-03 20:16 -05:00","tags":["ai","agent","sakana"],"summary":"The Darwin G\u00F6del Machine - AI that improves itself by rewriting its own code \u2192 https://sakana.ai/dgm/ A longstanding goal of AI research has been the creation of AI that can learn indefinitely. One...","keywords":["code","g\u00F6del","machine","rewriting","learning","dgms","darwin","self","improve","technical"]},{"id":"reviews-tiny-experiments-le-cunff","title":"Tiny Experiments","content":"Tiny Experiments by Anne-Laure Le Cunff Read Rating: 4.9/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/tiny-experiments-le-cunff","date":"2025-05-30 14:04 -05:00","tags":[],"summary":"Tiny Experiments by Anne-Laure Le Cunff Read Rating: 4.9/5","keywords":["tiny","experiments","anne","laure","cunff","read","rating:"]},{"id":"bookmarks-resource-list-personal-web","title":"Resources List for the Personal Web","content":"Resources List for the Personal Web \u2192 https://discourse.32bit.cafe/t/resources-list-for-the-personal-web/49 This list is to help guide and help those who are in every stage of their web-building journey on the personal web. This list is not meant to overwhelm you, but rather give you options and find tools, graphics, utilities, codes, and everything in between to help get you creating more on the independent web.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/resource-list-personal-web","date":"2025-05-30 13:32 -05:00","tags":["32bitcafe","personalweb","indieweb","openweb","community","retro"],"summary":"Resources List for the Personal Web \u2192 https://discourse.32bit.cafe/t/resources-list-for-the-personal-web/49 This list is to help guide and help those who are in every stage of their web-building...","keywords":["list","personal","help","resources","creating","everything","codes","utilities","graphics","tools"]},{"id":"posts-how-i-keep-up-with-ai","title":"How do I keep up with AI?","content":"This question comes up a lot in conversations. The short answer? I don\u2019t. There\u2019s just too much happening, too fast, for anyone to stay on top of everything. While I enjoy sharing links and recommendations, I realized that a blog post might be more helpful. It gives folks a single place they can bookmark, share, and come back to on their own time, rather than having to dig through message threads where things inevitably get lost. That said, here are some sources I use to try and stay informed: - **Newsletters** are great for curated content. They highlight the top stories and help filter through the noise. - **Blogs** are often the primary sources behind those newsletters. They go deeper and often cover a broader set of topics that might not make it into curated roundups. - **Podcasts** serve a similar role. In some cases, they provide curation like newsletters and deep dives like blogs in others. Best of all, you can tune in while on the go making it a hands-free activity. For your convenience, if any of the sources (including podcasts) I list below have RSS feeds, I\u2019ve included them in my [AI Starter Pack](/collections/starter-packs//ai), which you can [download and import into your favorite RSS reader](/collections/starter-packs//ai/index.opml) (as long as it supports OPML file imports). If you have some sources to share, [send me an e-mail](/contact). I\u0027d love to keep adding to this list! If they have a feed I can subscribe to, even better. ## Newsletters - [AlphaSignal](https://alphasignal.ai/) - [The Batch](https://www.deeplearning.ai/the-batch/) - [Latent Space](https://www.latent.space/) - [TLDR AI](https://tldr.tech/ai) ## Blogs I pride myself on being able to track down an RSS feed on just about any website, even if it\u2019s buried or not immediately visible. Unfortunately, I haven\u0027t found a feed URL for either OpenAI or Anthropic which is annoying. **OpenAI and Anthropic, if you could do everyone a favor and drop a link, that would be great.** UPDATE: Thanks to [@m2vh@mastodontech.de](https://toot.lqdev.tech/@m2vh@mastodontech.de) for sharing the OpenAI news feed. I know I could use one of those web-page-to-RSS converters, but I\u0027d much rather have an official link directly from the source. - [OpenAI](https://openai.com/news/) - [Anthropic](https://www.anthropic.com/news) - [Google AI Blog](https://blog.google/technology/ai/) - [AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/) - [Microsoft Research](https://www.microsoft.com/en-us/research/blog/) - [Google Research](https://research.google/blog/) - [Simon Willison](https://simonwillison.net/) ## Podcasts - [AI Daily Brief](https://www.podchaser.com/podcasts/the-ai-daily-brief-formerly-th-5260567) - [TWIML AI Podcast](https://twimlai.com/) - [Practical AI](https://changelog.com/practicalai) ## Subscribing to feeds Now that I\u2019ve got you here... Let\u2019s talk about the best way to access all these feeds. My preferred and recommended approach is using a feed reader. When subscribing to content on the open web, feed readers are your secret weapon. RSS might seem like it\u2019s dead (it\u2019s not\u2014yet). In fact, it\u2019s the reason you often hear the phrase, \u201CWherever you get your podcasts.\u201D But RSS goes beyond podcasts. It\u2019s widely supported by blogs, newsletters, and even social platforms like the Fediverse (Mastodon, PeerTube, etc.) and BlueSky. It\u2019s also how I\u2019m able to compile my [starter packs](/collections/starter-packs/). I\u0027ve written more about RSS in [Rediscovering the RSS Protocol](/posts/rediscovering-rss-user-freedom), but the short version is this: when you build on open standards like RSS and OPML, you\u2019re building on freedom. Freedom to use the tools that work best for you. Freedom to own your experience. And freedom to support a healthier, more independent web.","contentType":"posts","url":"https://www.lqdev.me/posts/how-i-keep-up-with-ai","date":"2025-05-27 19:44 -05:00","tags":["ai","rss","starterpack","newsletter","blog","opml","openweb","podcast","indieweb"],"summary":"This question comes up a lot in conversations. The short answer? I don\u2019t. There\u2019s just too much happening, too fast, for anyone to stay on top of everything. While I enjoy sharing links and...","keywords":["feed","it\u2019s","they","freedom","https://www","podcasts","anthropic","sources","newsletters","even"]},{"id":"reshare-stage-set-growth-small-social-media-cheapskate","title":"The stage has been set for the growth of small social media","content":"The stage has been set for the growth of small social media \u2192 https://cheapskatesguide.org/articles/small-social-growth.html Last month during Meta\u0027s antitrust trial, Mark Zuckerberg admitted that his company\u0027s focus has changed radically from where it began...[his] testimony implies that Facebook (Meta) may no longer be as much of an option if we actually want to talk to each other on line. But, neither are many of the other large social media sites. ...if companies allowed us to discover great new small blogs and small social media sites, many of us would decide to spend more time there. Then, companies would be less able to advertise to us on their own sites. ...we have been told by the mainstream media for well over a decade that small social media is dead. They say the same about small blogs, yet small blogs are actually more plentiful than ever, with over 600 million blogs on the Internet and about 7 million new blog articles published each day. If you decide to create your own forum, do yourself a favor. The way to protect yourself and your users is by running your social media site at a domain that you own and on a server that you control , and don\u0027t agree to hand over any of your rights to anyone for any reason. I believe large corporate-run social media sites are vulnerable in a way they have not been for twenty years. Their users have reached a state of extreme dissatisfaction with their \u0026quot;enshittified\u0026quot; platforms, and they are looking for something, anything, better. But each of you who are reading these words can help end this situation by creating your own small social media site. All you need to start your own small site is a domain name, a server that you can buy, rent, or set up on an old computer, perhaps social media software (see above), patience, a willingness to work, and ideas for ways of notifying potential users of the existence of your site. Run your new site as you and your users see fit, and make it far superior to Facebook, Twitter (X), Reddit, or any of the other major sites whose owners think you have no other options.","contentType":"reshare","url":"https://www.lqdev.me/responses/stage-set-growth-small-social-media-cheapskate","date":"2025-05-26 18:14 -05:00","tags":["social","openweb","personalweb"],"summary":"The stage has been set for the growth of small social media \u2192 https://cheapskatesguide.org/articles/small-social-growth.html Last month during Meta\u0027s antitrust trial, Mark Zuckerberg admitted that...","keywords":["social","media","small","sites","site","users","blogs","they","more","companies"]},{"id":"star-rss-not-dead-yet-mcnamee","title":"RSS is (not) dead (yet)","content":"RSS is (not) dead (yet) \u2192 https://audmcname.com/comics/rss-is-not-dead-yet/ Illustrating the history of RSS. Created in collaboration with A. Service , and debuted at VanCAF 2023","contentType":"star","url":"https://www.lqdev.me/responses/rss-not-dead-yet-mcnamee","date":"2025-05-26 18:11 -05:00","tags":["rss","comic","art"],"summary":"RSS is (not) dead (yet) \u2192 https://audmcname.com/comics/rss-is-not-dead-yet/ Illustrating the history of RSS. Created in collaboration with A. Service , and debuted at VanCAF 2023","keywords":["dead","https://audmcname","com/comics/rss","yet/","illustrating","history","created","collaboration","service","debuted"]},{"id":"notes-2025-05-26-weekly-post-summary","title":"Week of May 26, 2025 - Post Summary","content":"Notes Copilot Agent is giving me attitude Responses Pocket shutting down","contentType":"notes","url":"https://www.lqdev.me/notes/2025-05-26-weekly-post-summary","date":"2025-05-26 13:06 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes Copilot Agent is giving me attitude Responses Pocket shutting down","keywords":["notes","copilot","agent","giving","attitude","responses","pocket","shutting","down"]},{"id":"notes-attitude-copilot-agent","title":"Copilot Agent is giving me attitude","content":"(see your rules and project plan) Is that Copilot\u0027s way of saying, \u0026quot;Per my previous e-mail...\u0026quot;? And then it rage quit.","contentType":"notes","url":"https://www.lqdev.me/notes/attitude-copilot-agent","date":"2025-05-22 21:14 -05:00","tags":["github","copilot","agent","funny"],"summary":"(see your rules and project plan) Is that Copilot\u0027s way of saying, \u0026quot;Per my previous e-mail...\u0026quot;? And then it rage quit.","keywords":["\u0026quot","rules","project","plan","copilot","saying","previous","mail","then","rage"]},{"id":"bookmarks-pocket-shutting-down","title":"Pocket shutting down","content":"Pocket shutting down \u2192 https://support.mozilla.org/en-US/kb/future-of-pocket I haven\u0027t used Pocket in a long time but sad to hear it\u0027s shutting down. It\u0027s great they\u0027re offering the option of letting you export your data and platforms like Micro.blog are making it easy to host that content on your own site (assuming you\u0027re using Micro.blog). For bookmarking solutions, I\u0027ve been using my website as well as messages-to-self on Element. What\u0027s on my website is the content that I really want to make sure I archive, whereas the messages to self I treat more as a read-it-later solution. Eventually, some of those make it to my website. I\u0027m working on my mobile publishing flow to simplify my bookmarking process but overall, I\u0027m happy with my current system. This is also another great reminder why owning your content is important.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/pocket-shutting-down","date":"2025-05-22 20:21 -05:00","tags":["pocket","mozilla","bookmark"],"summary":"Pocket shutting down \u2192 https://support.mozilla.org/en-US/kb/future-of-pocket I haven\u0027t used Pocket in a long time but sad to hear it\u0027s shutting down. It\u0027s great they\u0027re offering the option of letting...","keywords":["pocket","content","website","self","down","messages","bookmarking","using","shutting","blog"]},{"id":"reshare-agent-network-protocol","title":"Agent Network Protocol - The HTTP of the Agentic Web Era","content":"Agent Network Protocol - The HTTP of the Agentic Web Era \u2192 https://agent-network-protocol.com/ Comparison of MCP and ANP: What Kind of Communication Protocol Do Agents Need? MCP and ANP differ significantly in protocol architecture, identity authentication, and information organization. MCP is a typical Client-Server (CS) architecture, while ANP is a typical Peer-to-Peer (P2P) architecture . MCP\u0027s identity authentication is based on the OAuth standard, facilitating client access to current internet resources. ANP\u0027s identity authentication is based on the W3C DID standard , focusing on cross-platform interoperability among agents, enabling seamless connectivity. MCP organizes information using JSON-RPC technology, essentially API calls. ANP uses semantic web Linked-Data technology to build a data network that is easily accessible and understandable by AI. ANP is agent-centric, where each agent has equal status, forming a decentralized agent collaboration network.","contentType":"reshare","url":"https://www.lqdev.me/responses/agent-network-protocol","date":"2025-05-17 10:56 -05:00","tags":["agent","ai","protocol"],"summary":"Agent Network Protocol - The HTTP of the Agentic Web Era \u2192 https://agent-network-protocol.com/ Comparison of MCP and ANP: What Kind of Communication Protocol Do Agents Need? MCP and ANP differ...","keywords":["agent","network","protocol","identity","architecture","authentication","typical","agents","data","standard"]},{"id":"reshare-chicago-dust-storm","title":"Chicago Dust Storm","content":"Chicago Dust Storm \u2192 https://www.nbcchicago.com/news/local/satellite-imagery-shows-large-dust-storm-over-chicago-area-from-space/3748009/ Never change Chicago. What felt like the first week of good weather since forever ends with a dust storm.","contentType":"reshare","url":"https://www.lqdev.me/responses/chicago-dust-storm","date":"2025-05-16 19:49 -05:00","tags":["chicago","weather","duststorm"],"summary":"Chicago Dust Storm \u2192 https://www.nbcchicago.com/news/local/satellite-imagery-shows-large-dust-storm-over-chicago-area-from-space/3748009/ Never change Chicago. What felt like the first week of good...","keywords":["chicago","dust","storm","since","weather","good","week","felt","change","never"]},{"id":"notes-attending-fediforum-2025","title":"See you at FediForum June 2025","content":"Super excited to be attending FediForum in a few weeks!","contentType":"notes","url":"https://www.lqdev.me/notes/attending-fediforum-2025","date":"2025-05-16 19:44 -05:00","tags":["fediforum","openweb","conference","community"],"summary":"Super excited to be attending FediForum in a few weeks!","keywords":["super","excited","attending","fediforum","weeks"]},{"id":"notes-2025-05-12-weekly-post-summary","title":"Week of May 12, 2025 - Post Summary","content":"Blogs Vibe-Specing - From concepts to specification Notes Extreme web surfing Diving deeper into the Chicano Batman catalog Responses The Evolution of Blogging ZeroSearch - Incentivize the Search Capability of LLMs without Searching Introducing AutoRound Discourse and the Fediverse socialweb.network TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching Bukmark Club Parakeet TDT 0.6B V2 (En) CORG: Generating Answers from Complex, Interrelated Contexts Introducing Locate 3D In Praise of Links Announcing Distributed.Press Social Inbox 1.0 Resisting the urge to rewrite the website Just Put It On Your Blog Fediverse House 2025 Roundup Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks Claude Artifacts A Survey of AI Agent Protocols","contentType":"notes","url":"https://www.lqdev.me/notes/2025-05-12-weekly-post-summary","date":"2025-05-11 19:11 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Blogs Vibe-Specing - From concepts to specification Notes Extreme web surfing Diving deeper into the Chicano Batman catalog Responses The Evolution of Blogging ZeroSearch - Incentivize the Search...","keywords":["introducing","from","fediverse","blogs","resisting","inbox","social","press","distributed","announcing"]},{"id":"reply-claude-artifacts-reece","title":"Claude Artifacts","content":"Claude Artifacts \u2192 https://www.manton.org/2025/05/10/openai-rolls-out-new-things.html \uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF. Artifacts are great. In ChatGPT when I find myself trying to format outputs as markdown, the UI gets confused and some of the content is rendered in markdown while the rest gets rendered as part of the UI which makes it unusable and hard to copy. The best way I\u0027ve found to get around it is to ask it to format in org-mode or asciidoc. Claude Artifacts make this a non-issue. I also love that in addition to copying and downloading the artifact, you can also publish it as a URL.","contentType":"reply","url":"https://www.lqdev.me/responses/claude-artifacts-reece","date":"2025-05-11 08:42 -05:00","tags":["claude","chatgpt","plaintext","ai","org","emacs","asciidoc","markdown"],"summary":"Claude Artifacts \u2192 https://www.manton.org/2025/05/10/openai-rolls-out-new-things.html \uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF. Artifacts are great. In ChatGPT when I find myself trying to format outputs as markdown, the UI gets...","keywords":["artifacts","claude","rendered","gets","also","markdown","format","unusable","hard","copy"]},{"id":"reshare-announcing-distributed-press-social-inbox","title":"Announcing Distributed.Press Social Inbox 1.0","content":"Announcing Distributed.Press Social Inbox 1.0 \u2192 https://hypha.coop/dripline/announcing-dp-social-inbox/ I hadn\u0027t heard of Distributed Press and this is an older post (from 2023), but the idea is interesting. Hypha and Sutty are thrilled to announce the release of the Social Inbox, a new feature of Distributed.Press that integrates a website\u2019s comment section with federated social media platforms like Mastodon. With the Social Inbox enabled, websites obtain their own account on the Fediverse, allowing it to automatically send out new posts to followers at the time of publication. When other users reply to posts, you can approve them to be published to the site as comments. The Social Inbox allows readers to directly engage with your posts where they already are, and gives publishers the ability to incorporate public dialogue into their websites.","contentType":"reshare","url":"https://www.lqdev.me/responses/announcing-distributed-press-social-inbox","date":"2025-05-10 20:47 -05:00","tags":["decentralization","indieweb","social","fediverse","dweb"],"summary":"Announcing Distributed.Press Social Inbox 1.0 \u2192 https://hypha.coop/dripline/announcing-dp-social-inbox/ I hadn\u0027t heard of Distributed Press and this is an older post (from 2023), but the idea is...","keywords":["social","inbox","distributed","press","posts","websites","reply","users","publication","followers"]},{"id":"notes-too-many-browser-tabs-shellsharks","title":"Extreme web surfing","content":"This is what my browser looks like every time shellsharks drops one of their scrolls .","contentType":"notes","url":"https://www.lqdev.me/notes/too-many-browser-tabs-shellsharks","date":"2025-05-10 18:31 -05:00","tags":["indieweb","web","community","surfing","openweb","blog"],"summary":"This is what my browser looks like every time shellsharks drops one of their scrolls .","keywords":["browser","looks","shellsharks","drops","scrolls"]},{"id":"star-discourse-fediverse","title":"Discourse and the Fediverse","content":"Discourse and the Fediverse \u2192 https://blog.discourse.org/2025/04/discourse-and-the-fediverse/ Two years ago, we started working on a plugin that brings Discourse and the Fediverse closer together. Discourse communities are online spaces that facilitate open collaboration and communication. The Fediverse offers ways to expand the reach of Discourse communities and help them build bridges with people active in other spaces, all while keeping the conversation civil, meaningful and focused. This post will describe how the ActivityPub plugin works and how you can enable your Discourse community to connect with other communities or Fediverse users.","contentType":"star","url":"https://www.lqdev.me/responses/discourse-fediverse","date":"2025-05-09 22:20 -05:00","tags":["discourse","fediverse","social","openweb","opensource","forum"],"summary":"Discourse and the Fediverse \u2192 https://blog.discourse.org/2025/04/discourse-and-the-fediverse/ Two years ago, we started working on a plugin that brings Discourse and the Fediverse closer together....","keywords":["discourse","fediverse","communities","spaces","plugin","bridges","people","active","while","keeping"]},{"id":"bookmarks-bukmark-club","title":"Bukmark Club","content":"Bukmark Club \u2192 https://bukmark.club/ The BUKMARK.CLUB is a collection of websites from across the Internet.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/bukmark-club","date":"2025-05-09 21:09 -05:00","tags":["bookmark","internet","web","community"],"summary":"Bukmark Club \u2192 https://bukmark.club/ The BUKMARK.CLUB is a collection of websites from across the Internet.","keywords":["bukmark","club","https://bukmark","club/","collection","websites","from","across","internet"]},{"id":"star-in-praise-of-links-osteophage","title":"In Praise of Links","content":"In Praise of Links \u2192 https://osteophage.neocities.org/writing/in-praise-of-links Hyperlinks deserve more recognition in light of all the ways their value has been sidelined and denied. To that end, I present a link compilation in praise of links. It includes things I agree with entirely and things I don\u0027t, spanning from the 2020s to the early 2000s, to supply a tapestry of perspectives, context, and examples on the value and importance of links. Links may have their downsides, challenges, and vulnerabilities, but my hope is that this compilation will (re)invigorate your appreciation for linking as a technology and a social practice, all the better to understand what\u0027s at stake when links are discarded and devalued.","contentType":"star","url":"https://www.lqdev.me/responses/in-praise-of-links-osteophage","date":"2025-05-09 21:05 -05:00","tags":["openweb","internet","indieweb","decentralization","web"],"summary":"In Praise of Links \u2192 https://osteophage.neocities.org/writing/in-praise-of-links Hyperlinks deserve more recognition in light of all the ways their value has been sidelined and denied. To that end, I...","keywords":["links","praise","things","compilation","value","tapestry","perspectives","context","examples","importance"]},{"id":"star-evolution-of-blogging-history-web","title":"The Evolution of Blogging","content":"The Evolution of Blogging \u2192 https://thehistoryoftheweb.com/the-evolution-of-blogging/ ...around 1997, when Jorn Barger launched his website, titling it \u201CThe Robot Wisdom Weblog.\u201D Barger\u2019s plan was to curate his favorite links from around the web, and add bits of commentary to each one. In other words, he meant to keep a log of his web experience. Hence weblog. In their earliest days, webloggers stood as gatekeepers to the web\u2019s ever-growing well of content. Each day, these URL pioneers would post a few new links and sprinkle in their own commentary. Blogs acted as a signpost for web users, and following a few key blogs was enough to keep track of just about everything new on the web. Many began to look at the blogging community as a brand new type of media, one that often stood far closer to an impartial truth than traditional mediums would allow for. 1999, Peter Merholz threw up his own weblog, and in the sidebar added the tagline: \u0026quot;I\u2019ve decided to pronounce the word \u201Cweblog\u201D as wee\u2019- blog. Or \u201Cblog\u201D for short.\u0026quot; In the beginning, most bloggers hand coded their sites, adding a new HTML page to their server each time they had a new entry, or just updating the homepage to include the newest links. But soon enough, tools showed up to help bloggers with the process. ...on the fringes, a new type of blog was emerging. The personal blog. These sites ditched the curated links and focused exclusively on commentary. Bloggers used their site to chronicle their personal journey, from the almost boring and banal to the weird and wonderful. This new type of blog was less an alternative media source and more akin to an online journal or diary. And these writers saw themselves not as gatekeepers to the web, but as sharers of their own identity. New tools soon caught up with this shifting perspective. The first of the bunch to do so was LiveJournal... Blogger came next. And it would prove to be the most popular of the early blogging tools. The platform first launched in August of 1999, but picked up steam the following year when it introduced the concept of a permalink. These platforms made blogging open, in every sense of the word. Visitors were greeted with a single, open textarea on a free platform that was open to all to participate. Blogger became the platform choice for a lot of writers out there. It certainly was for Mena Trott Trott had the feeling that platforms like Blogger did not go nearly far enough. Trott wanted her own site to stand out from the crowd. To be quintessentially hers. Blogger wouldn\u2019t let her do that. So together with her husband, a Perl programmer, she built Movable Type...And with that move, Trott ushered in a third generation of blogging. Movable Type allowed users to set up their blog on their own server, and have complete control over its look and feel. Blog (the noun and verb) became a word familiar not just to web geeks, but to everyone. Lots of imitators and innovators followed in the wake of Moveable Type. Open source platforms like WordPress rose to meet the rising demand. But for a long time, Moveable Type was the standard. It fed its community with a constant stream of new features that promoted openness and freedom. Blogging, as a community and a practice, continues to be refined to this day thanks to the dedication of web users everywhere. But its sharpest point was made the day Mena Trott decided that a blog could be more than just some text on a page. It could be one of a kind.","contentType":"star","url":"https://www.lqdev.me/responses/evolution-of-blogging-history-web","date":"2025-05-09 20:54 -05:00","tags":["blogging","history","web","internet"],"summary":"The Evolution of Blogging \u2192 https://thehistoryoftheweb.com/the-evolution-of-blogging/ ...around 1997, when Jorn Barger launched his website, titling it \u201CThe Robot Wisdom Weblog.\u201D Barger\u2019s plan was to...","keywords":["type","blog","blogging","trott","links","open","blogger","weblog","users","commentary"]},{"id":"reshare-socialweb-network","title":"socialweb.network","content":"socialweb.network \u2192 https://socialweb.network/ Your gateway to decentralized social networking protocols, resources, and community","contentType":"reshare","url":"https://www.lqdev.me/responses/socialweb-network","date":"2025-05-09 20:49 -05:00","tags":["fediverse","social","openweb","decentralized","community","indieweb"],"summary":"socialweb.network \u2192 https://socialweb.network/ Your gateway to decentralized social networking protocols, resources, and community","keywords":["socialweb","network","https://socialweb","network/","gateway","decentralized","social","networking","protocols","resources"]},{"id":"bookmarks-intel-autoround","title":"Introducing AutoRound","content":"Introducing AutoRound \u2192 https://huggingface.co/blog/autoround As large language models (LLMs) and vision-language models (VLMs) continue to grow in size and complexity, deploying them efficiently becomes increasingly challenging. Quantization offers a solution by reducing model size and inference latency. Intel\u0027s AutoRound emerges as a cutting-edge quantization tool that balances accuracy, efficiency, and compatibility. AutoRound is a weight-only post-training quantization (PTQ) method developed by Intel. It uses signed gradient descent to jointly optimize weight rounding and clipping ranges, enabling accurate low-bit quantization (e.g., INT2 - INT8) with minimal accuracy loss in most scenarios. For example, at INT2, it outperforms popular baselines by up to 2.1x higher in relative accuracy. Paper Repo","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/intel-autoround","date":"2025-05-09 20:45 -05:00","tags":["quantization","intel","ai"],"summary":"Introducing AutoRound \u2192 https://huggingface.co/blog/autoround As large language models (LLMs) and vision-language models (VLMs) continue to grow in size and complexity, deploying them efficiently...","keywords":["quantization","autoround","accuracy","int2","size","intel","weight","language","models","developed"]},{"id":"bookmarks-nvidia-parakeet-v2","title":"Parakeet TDT 0.6B V2 (En)","content":"Parakeet TDT 0.6B V2 (En) \u2192 https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2 parakeet-tdt-0.6b-v2 is a 600-million-parameter automatic speech recognition (ASR) model designed for high-quality English transcription, featuring support for punctuation, capitalization, and accurate timestamp prediction. This XL variant of the FastConformer [1] architecture integrates the TDT [2] decoder and is trained with full attention, enabling efficient transcription of audio segments up to 24 minutes in a single pass.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/nvidia-parakeet-v2","date":"2025-05-09 20:43 -05:00","tags":["nvidia","ai","transcription"],"summary":"Parakeet TDT 0.6B V2 (En) \u2192 https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2 parakeet-tdt-0.6b-v2 is a 600-million-parameter automatic speech recognition (ASR) model designed for high-quality...","keywords":["parakeet","transcription","prediction","variant","fastconformer","architecture","integrates","decoder","trained","full"]},{"id":"star-fediverse-house-2025-roundup","title":"Fediverse House 2025 Roundup","content":"Fediverse House 2025 Roundup \u2192 https://about.flipboard.com/fediverse/fediverse-house-2025-roundup/ No Walls, Just Vibes at SXSW\u2019s Fediverse House Fediverse House was SXSW\u2019s first physical gathering dedicated entirely to the constellation of interoperable social platforms powered by protocols like ActivityPub and AT Protocol. (We know \u201CFediverse\u201D isn\u2019t quite accurate to describe the whole ecosystem; in this case, it was just a convenient, fun way to name the space.) It didn\u2019t matter which platform you preferred or which protocol you were building on; everyone was focused on the singular goal of building a better internet. ...the real power move for creators is ownership and control of their work and livelihoods. ...identity portability is a radical shift, and...decentralization could lead to more humane social spaces. DotSocial Podcast Fediverse House PeerTube Channel","contentType":"star","url":"https://www.lqdev.me/responses/fediverse-house-2025-roundup","date":"2025-05-09 20:36 -05:00","tags":["fediverse","conference","indieweb","openweb"],"summary":"Fediverse House 2025 Roundup \u2192 https://about.flipboard.com/fediverse/fediverse-house-2025-roundup/ No Walls, Just Vibes at SXSW\u2019s Fediverse House Fediverse House was SXSW\u2019s first physical gathering...","keywords":["house","fediverse","2025","which","building","protocol","social","sxsw\u2019s","power","real"]},{"id":"bookmarks-meta-locate3d","title":"Introducing Locate 3D","content":"Introducing Locate 3D \u2192 https://locate3d.atmeta.com/ Locate 3D transforms how AI understands physical space, delivering state-of-the-art object localization in complex 3D environments . Operating directly on standard sensor data, Locate 3D brings spatial intelligence to robotics and augmented reality applications in real-world settings. Paper We present Locate 3D, a model for localizing objects in 3D scenes from referring expressions like \u201Cthe small coffee table between the sofa and the lamp.\u201D Locate 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities. Notably, Locate 3D operates directly on sensor observation streams (posed RGB-D frames), enabling real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA, a novel self-supervised learning (SSL) algorithm applicable to sensor point clouds. It takes as input a 3D pointcloud featurized using 2D foundation models (CLIP, DINO). Subsequently, masked prediction in latent space is employed as a pretext task to aid the self-supervised learning of contextualized pointcloud features. Once trained, the 3D-JEPA encoder is finetuned alongside a language-conditioned decoder to jointly predict 3D masks and bounding boxes. Additionally, we introduce Locate 3D Dataset, a new dataset for 3D referential grounding, spanning multiple capture setups with over 130K annotations. This enables a systematic study of generalization capabilities as well as a stronger model.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/meta-locate3d","date":"2025-05-09 20:33 -05:00","tags":["meta","ai","3d","research"],"summary":"Introducing Locate 3D \u2192 https://locate3d.atmeta.com/ Locate 3D transforms how AI understands physical space, delivering state-of-the-art object localization in complex 3D environments . Operating...","keywords":["locate","sensor","model","self","supervised","learning","pointcloud","capabilities","generalization","grounding"]},{"id":"bookmarks-zero-search-alibaba","title":"ZeroSearch - Incentivize the Search Capability of LLMs without Searching","content":"ZeroSearch - Incentivize the Search Capability of LLMs without Searching \u2192 https://alibaba-nlp.github.io/ZeroSearch/ Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs\u0027 search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality : The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs : RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch , a reinforcement learning framework that enhances the search capabilities of LLMs without interacting with real search engines . Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model\u2019s reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module . Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it . Furthermore, it generalizes well across both base and instruction-tuned models of varying sizes and is compatible with a wide range of RL algorithms.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/zero-search-alibaba","date":"2025-05-09 20:31 -05:00","tags":["ai","search","llm","research"],"summary":"ZeroSearch - Incentivize the Search Capability of LLMs without Searching \u2192 https://alibaba-nlp.github.io/ZeroSearch/ Effective information searching is essential for enhancing the reasoning and...","keywords":["search","llms","retrieval","capabilities","module","zerosearch","training","real","engines","documents"]},{"id":"bookmarks-survey-ai-agent-protocols","title":"A Survey of AI Agent Protocols","content":"A Survey of AI Agent Protocols \u2192 https://arxiv.org/abs/2504.16736 It\u0027ll be interesting to see once many of the exiting protocols begin to converge towards a standard. There\u0027s a ton of existing well-adopted protocol standards out there that when stitched together could create a compelling and native experience. I think eventually we\u0027ll get to a place where agents are native parts of the OSI stack and the world wide web. The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence. In this paper, we provide the first comprehensive analysis of existing agent protocols, proposing a systematic two-dimensional classification that differentiates context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore the future landscape of agent protocols by identifying critical research directions and characteristics necessary for next-generation protocols. These characteristics include adaptability, privacy preservation, and group-based interaction, as well as trends toward layered architectures and collective intelligence infrastructures. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/survey-ai-agent-protocols","date":"2025-05-09 20:23 -05:00","tags":["ai","agents","protocols","research"],"summary":"A Survey of AI Agent Protocols \u2192 https://arxiv.org/abs/2504.16736 It\u0027ll be interesting to see once many of the exiting protocols begin to converge towards a standard. There\u0027s a ton of existing...","keywords":["protocols","agents","agent","analysis","across","data","more","characteristics","world","tools"]},{"id":"posts-vibe-specing-prompt-to-spec","title":"Vibe-Specing - From concepts to specification","content":"## Introduction Code generation is a common use case for AI. What about the design process that comes before implementation? Personally, I\u0027ve found that AI excels not just at coding, but also helping formalize abstract ideas into concrete specifications. This post explores how I used AI-assisted design to transform a collection of loosely related concepts into a technical specification for a new system made up of those concepts. ## Specifications Generally, I\u0027ve had mixed success with vibe-coding (the practice of describing what you want in natural language and having AI generate the corresponding code). However, it\u0027s something that I\u0027m constantly working on getting better at. Also, with tooling integrations like MCP, I can ground responses and supplement my prompts using external data. What I find myself being more successful with is using AI to explore ideas and then formalizing those ideas into a specification. Even in the case of vibe-coding, what you\u0027re doing with your prompts is building a specification in real-time. I\u0027d like to think that eventually I\u0027ll get to the vibe-coding part but before diving straight into the code, I\u0027d like to spend time in the design phase. Personally, this is also the part that I find the most fun because you can throw wild things at the wall. It\u0027s not until you implement them that you actually validate whether some of those wild ideas are practical. But I find the design phase a ton of fun. The result of my latest vibe-specing adventure is what I\u0027m calling the [InterPlanetary Knowledge System (IPKS)](https://github.com/lqdev/IPKS). ## From concept to specification Lately, I\u0027ve been thinking a lot about knowledge. Some concepts that have been in my head are those of **non-linear publishing** (creating content that can be accessed in any order with multiple entry points, like wikis or hypertext) and **distributed cognition** (the idea that human knowledge and cognitive processes extend beyond the individual mind to include interactions with other people, tools, and environments). Related to those concepts, I\u0027ve also been thinking about how **digital gardens** (personal knowledge bases that blend note-taking, blogging, and knowledge management in a non-linear format) and **Zettelkasten** (a method of note-taking where ideas are captured as atomic notes with unique identifiers and explicit connections) are ways to capture and organize knowledge. One other thing that I\u0027m amazed by is the powerful concept of a **hyperlink** and how it makes the web open, decentralized, and interoperable. When paired with the **semantic web** (an extension of the web that provides a common framework for data to be shared across applications and enterprises), you have yourself a decentralized knowledgebase containing a lot of the world\u0027s knowledge. At some point, IPFS (InterPlanetary File System, a protocol designed to create a permanent and decentralized method of storing and sharing files) joined this pool of concepts I had in my head. These were all interesting concepts individually, but I knew there were connections but couldn\u0027t cohesively bring them together. That\u0027s where AI-assisted specification design came in. Below is a summary of the collaborative design interaction with Claude Sonnet 3.7 (with web search) that eventually led to the generation of the IPKS specifications. I haven\u0027t combed through them in great detail, but what they\u0027re proposing seems plausible. Overall, I\u0027m fascinated by this interaction. Whether or not IPKS ever becomes a reality, the process of using AI to transform abstract concepts into concrete specifications seems like a valuable and fun design approach that I\u0027ll continue to refine and include as part of my vibe-coding sessions. --- ## Initial Exploration: IPFS and Knowledge Management Our conversation began with exploring IPFS (InterPlanetary File System) and its fundamental capabilities as a content-addressed, distributed file system. We recognized that while IPFS excels at storing and retrieving files in a decentralized manner, it needed extensions to support knowledge representation, trust, and semantics. Key insights from this stage: - IPFS provides an excellent foundation with content addressing through CIDs - Content addressing enables verification but doesn\u0027t inherently provide meaning - Moving from document-centric to idea-centric systems requires additional layers ## Knowledge Management Concepts We explored established knowledge management approaches, particularly: ### Zettelkasten The Zettelkasten method contributed these important principles: - Atomic units of knowledge (one idea per note) - Explicit connections between ideas - Unique identifiers for each knowledge unit - Emergent structure through relationship networks ### Digital Gardens The Digital Garden concept provided these insights: - Knowledge in various stages of development - Non-linear organization prioritizing connections - Evolution of ideas over time - Public visibility of work-in-progress thinking These personal knowledge management approaches helped us envision how similar principles could work at scale in a distributed system. ## The \u0022K\u0022 in IPKS When we proposed replacing \u0022IPFS\u0022 with \u0022IPKS\u0022 (changing File \u2192 Knowledge), we recognized the need to define what makes knowledge different from files. This led to identifying several key requirements: 1. **Semantic meaning** - Knowledge needs explicit relationships and context 2. **Provenance and trust** - Knowledge requires verifiable sources and expertise 3. **Evolution** - Knowledge changes over time while maintaining continuity 4. **Governance** - Knowledge exists in various trust and privacy contexts These requirements shaped the layered architecture of the specifications. ## Distributed Cognition and Non-Linear Publishing Our discussions about distributed cognition highlighted how thinking processes extend beyond individual minds to include: - Interactions with other people - Cultural artifacts and tools - Physical and digital environments - Social and technological systems This concept directly influenced the IPKS design by emphasizing: - Knowledge as a collective, distributed resource - The need for attribution and expertise verification - The value of connecting knowledge across boundaries - The role of tools in extending human cognition Similarly, non-linear publishing concepts shaped how we approached knowledge relationships and navigation in IPKS, moving away from sequential formats toward interconnected networks of information. ## Web3 Technologies Integration Our exploration of complementary technologies led to incorporating: ### Decentralized Identifiers (DIDs) DIDs provided the framework for: - Self-sovereign identity for knowledge contributors - Cryptographic verification of authorship - Persistent identification across systems - Privacy-preserving selective disclosure ### Verifiable Credentials (VCs) Verifiable Credentials offered mechanisms for: - Expertise validation without central authorities - Domain-specific qualification verification - Credential-based access control - Trust frameworks for knowledge contributors ### Semantic Web (RDF/OWL) Semantic Web standards influenced: - Relationship types between knowledge nodes - Ontologies for domain knowledge representation - Query patterns for knowledge discovery - Interoperability with existing knowledge systems ## Business Context: Supply Chain Example Our conversation about supply chain management provided a concrete use case that helped ground the specifications in practical application. This example demonstrated how IPKS could address real-world challenges: - **Material Provenance**: Using DIDs and verifiable credentials to establish trusted material sources - **Cross-Organization Collaboration**: Enabling knowledge sharing while respecting organizational boundaries - **Regulatory Compliance**: Creating verifiable documentation of compliance requirements - **Expertise Validation**: Ensuring contributors have appropriate qualifications for their roles - **Selective Disclosure**: Balancing transparency with competitive confidentiality This business context helped shape the Access Control \u0026 Privacy specification in particular, highlighting the need for nuanced governance models. ## Technical Implementation Considerations As we moved from abstract concepts to specifications, several technical considerations emerged: 1. **Building on IPLD**: Recognizing that InterPlanetary Linked Data (IPLD) already provided foundational components for structured, linked data in content-addressed systems 2. **Modular Specification Design**: Choosing to create multiple specifications rather than a monolithic standard to enable incremental implementation and adoption 3. **Backward Compatibility**: Ensuring IPKS could work with existing IPFS/IPLD infrastructure 4. **Extensibility**: Designing for future enhancements like AI integration, advanced semantic capabilities, and cross-domain knowledge mapping ## The Path Forward The IPKS specifications represent a synthesis of our conceptual exploration, grounded in: - Established knowledge management practices - Decentralized web technologies - Real-world business requirements - Technical feasibility considerations Moving from concept to implementation will require: 1. Reference implementations of the core specifications 2. Developer tools and libraries to simplify adoption 3. Domain-specific extensions for particular use cases 4. Community building around open standards By building on the combined strengths of IPFS, DIDs, VCs, and semantic web technologies, IPKS creates a framework for distributed knowledge that balances openness with trust, flexibility with verification, and collaboration with governance.","contentType":"posts","url":"https://www.lqdev.me/posts/vibe-specing-prompt-to-spec","date":"2025-05-06 20:40 -05:00","tags":["dweb","standards","ai","decentralization","protocol"],"summary":"## Introduction Code generation is a common use case for AI. What about the design process that comes before implementation? Personally, I\u0027ve found that AI excels not just at coding, but also helping...","keywords":["knowledge","ipks","concepts","specifications","design","specification","ipfs","ideas","management","from"]},{"id":"notes-chicano-batman-album","title":"Diving deeper into the Chicano Batman catalog","content":"I find it amazing how you could\u0027ve heard a song before but on rare occasions, you actually get to really listen to the song. That\u0027s been happening to me with this album from Chicano Batman. I\u0027ve written before about how much I enjoy A Hundred Dead and Loving Souls. Lately though, I\u0027ve been really into Itotiani, La Samoana, and Soniatl. The instrumentals are amazing. Wild to think this album is now 15 years old.","contentType":"notes","url":"https://www.lqdev.me/notes/chicano-batman-album","date":"2025-05-05 20:21 -05:00","tags":["music","chicanobatman","indie"],"summary":"I find it amazing how you could\u0027ve heard a song before but on rare occasions, you actually get to really listen to the song. That\u0027s been happening to me with this album from Chicano Batman. I\u0027ve...","keywords":["amazing","song","really","album","find","instrumentals","soniatl","samoana","itotiani","though"]},{"id":"reshare-resisting-urge-rewrite-website-santala","title":"Resisting the urge to rewrite the website","content":"Resisting the urge to rewrite the website \u2192 https://hamatti.org/posts/resisting-the-urge-to-rewrite-the-website/ I have half-jokingly been entertaining an idea of building a new blog: for every blog post published, you get to add one new feature. The first blog post needs to be written in a plain txt file and deployed as-is to a server. With the second post, you can choose to add something you want: maybe a bit of HTML navigation to be able to have links between the blog posts. The third post could come with Markdown support so your posts are not plain text anymore but rendered into HTML. And so on. The idea would is that you get to \u201Cearn\u201D feature development by writing blog posts. If you want a fancy site with bells and whistles and good tooling, you have to write and publish a lot. That way, you cannot get lost into the rabbit hole of new shiny feature development at the expense of writing. I like this suggestion. My rewrite has been ongoing but slow. I tried vibe-coding my way to success but didn\u0027t have much luck there, so it\u0027s taking more time to do it manually. If I was doing it from scratch it wouldn\u0027t be a challenge. The challenge is making all the posts and current structure fit into what I want to website to eventually look like. I use different tools for writing and coding. I can write my blog posts on my iPad when I\u2019m in the library or local pub and at that time, there\u2019s no way for me to tinker with the tools or the website itself. Another great suggestion. A lot of my desire to rewrite stems from frictions in my current workflow as well as being able to support new and different post types. Back when I had my Duo , I didn\u0027t mind the mobile VS Code authoring experience, but now that I\u0027m back to a single screen device, not being able to publish from my phone creates a lot of friction.","contentType":"reshare","url":"https://www.lqdev.me/responses/resisting-urge-rewrite-website-santala","date":"2025-05-05 20:06 -05:00","tags":["website","indieweb","blogging"],"summary":"Resisting the urge to rewrite the website \u2192 https://hamatti.org/posts/resisting-the-urge-to-rewrite-the-website/ I have half-jokingly been entertaining an idea of building a new blog: for every blog...","keywords":["posts","post","blog","rewrite","website","able","writing","feature","from","coding"]},{"id":"bookmarks-telograf","title":"TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching","content":"TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching \u2192 https://arxiv.org/abs/2505.00562 Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications. We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method\u0027s capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at this https URL","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/telograf","date":"2025-05-05 20:03 -05:00","tags":["ai","graphs","neuralnetworks","research"],"summary":"TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching \u2192 https://arxiv.org/abs/2505.00562 Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many...","keywords":["specifications","temporal","logic","graph","flow","method","tasks","complex","solve","show"]},{"id":"star-just-post-on-your-blog-shellsharks","title":"Just Put It On Your Blog","content":"Just Put It On Your Blog \u2192 https://shellsharks.com/just-put-it-on-your-blog It\u2019s great to have a place to share your thoughts. A place you can go back to when you want to remember something you had written or thought about before. A place you can refer people to when they have questions you\u2019ve answered in the past. A place to be you. So, get a blog, and put all the things there. It is great to carve out your own space in the interwebs. If you run out of ideas of what to do with your website, here\u0027s James\u0027 blog with 100\u002B ideas .","contentType":"star","url":"https://www.lqdev.me/responses/just-post-on-your-blog-shellsharks","date":"2025-05-05 19:57 -05:00","tags":["blogging","indieweb","website"],"summary":"Just Put It On Your Blog \u2192 https://shellsharks.com/just-put-it-on-your-blog It\u2019s great to have a place to share your thoughts. A place you can go back to when you want to remember something you had...","keywords":["blog","place","ideas","here","website","interwebs","space","carve","things","past"]},{"id":"bookmarks-corg-context-organizer-framework","title":"CORG: Generating Answers from Complex, Interrelated Contexts","content":"CORG: Generating Answers from Complex, Interrelated Contexts \u2192 https://arxiv.org/abs/2505.00023 In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/corg-context-organizer-framework","date":"2025-05-05 19:53 -05:00","tags":["rag","ai","knowledge","research"],"summary":"CORG: Generating Answers from Complex, Interrelated Contexts \u2192 https://arxiv.org/abs/2505.00023 In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies...","keywords":["single","corg","contexts","effectively","ambiguous","results","context","interrelationships","complex","answers"]},{"id":"bookmarks-self-generate-incontext-examples-llm-agents-improve-sequential-decision-making","title":"Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks","content":"Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks \u2192 https://arxiv.org/abs/2505.00234 Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/self-generate-incontext-examples-llm-agents-improve-sequential-decision-making","date":"2025-05-05 19:50 -05:00","tags":["ai","agents","planning","research"],"summary":"Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks \u2192 https://arxiv.org/abs/2505.00234 Many methods for improving Large Language Model (LLM) agents for...","keywords":["performance","knowledge","context","examples","engineering","tasks","task","specific","agents","database"]},{"id":"reshare-rag-onnx-runtime-genai-windows","title":"Running RAG with ONNX Runtime GenAI for On-Prem Windows","content":"Running RAG with ONNX Runtime GenAI for On-Prem Windows \u2192 https://devblogs.microsoft.com/ise/running-rag-onnxruntime-genai/ Really great to see these case studies and comparisons. A while back we published a blog post showcasing how experiences like the AI Dev Gallery make use of ONNX Runtime and the various AI building blocks in .NET to enable a diverse set of scenarios. For more ONNX Runtime GenAI focused C# content, you can also reference the post Using Phi-3 \u0026amp; C# with ONNX for text and vision samples","contentType":"reshare","url":"https://www.lqdev.me/responses/rag-onnx-runtime-genai-windows","date":"2025-05-02 15:19 -05:00","tags":["onnxruntime","windows","dotnet"],"summary":"Running RAG with ONNX Runtime GenAI for On-Prem Windows \u2192 https://devblogs.microsoft.com/ise/running-rag-onnxruntime-genai/ Really great to see these case studies and comparisons. A while back we...","keywords":["onnx","runtime","post","genai","various","building","blocks","enable","diverse","scenarios"]},{"id":"notes-crate-finds-april-2025","title":"Crate Finds - April 2025","content":"April 2025 drop. SAULT, Cleo Sol (SAULT singer), and Little Simz were on repeat. Tracks Pray for Me by SAULT Album: Pray for Me Duration: 7:14 Listen on YouTube Backup: Listen on Spotify Can\u0027t Keep My Plants Alive by alici Album: Can\u0027t Keep My Plants Alive Duration: 4:06 Listen on YouTube Backup: Listen on Spotify Jumpstart by TYSON Album: Jumpstart Duration: 2:51 Listen on YouTube Backup: Listen on Spotify Now It\u0027s Your Turn To Sing by Lizette \u0026amp; Quevin Album: Grow Forever b/w Now It\u2019s Your Turn To Sing Duration: 3:13 Listen on YouTube Backup: Listen on Spotify I\u0027ll Be Loving You by Yellow Days Album: I\u0027ll Be Loving You Duration: 4:06 Listen on YouTube Backup: Listen on Spotify care to care (feat. Plumm) by Marysia Osu, Plumm Album: harp, beats \u0026amp; dreams Duration: 3:03 Listen on YouTube Backup: Listen on Spotify 23 by Cleo Sol Album: Mother Duration: 5:06 Listen on YouTube Backup: Listen on Spotify I Let A Song Go Out Of My Heart by Duke Ellington Album: The Duke: The Columbia Years (1927-1962) Duration: 3:07 Listen on YouTube Backup: Listen on Spotify Things Ain\u0027t What They Used to Be by Duke Ellington Album: The Duke: The Columbia Years (1927-1962) Duration: 2:57 Listen on YouTube Backup: Listen on Spotify B.P.E by anaiis, Grupo Cosmo Album: anaiis \u0026amp; Grupo Cosmo Duration: 3:40 Listen on YouTube Backup: Listen on Spotify Point and Kill by Little Simz, Obongjayar Album: Sometimes I Might Be Introvert Duration: 3:05 Listen on YouTube Backup: Listen on Spotify Woman by Little Simz, Cleo Sol Album: Sometimes I Might Be Introvert Duration: 4:29 Listen on YouTube Backup: Listen on Spotify One by Cleo Sol Album: One Duration: 4:33 Listen on YouTube Backup: Listen on Spotify Two Lands by Azamiah Album: Two Lands Duration: 3:46 Listen on YouTube Backup: Listen on Spotify Let Dust Settle by Azamiah Album: Two Lands Duration: 4:45 Listen on YouTube Backup: Listen on Spotify My Lonely Heart by Azamiah Album: Two Lands Duration: 5:13 Listen on YouTube Backup: Listen on Spotify Om Gam Mantra by Chantress Seba, Malte Marten Album: Om Gam Mantra Duration: 7:15 Listen on YouTube Backup: Listen on Spotify Sandusky by Uncle Tupelo Album: March 16-20, 1992 Duration: 3:43 Listen on YouTube Backup: Listen on Spotify Ra Heset - Live by Chantress Seba Album: Light Healing (Live Sessions) Duration: 4:01 Listen on YouTube Backup: Listen on Spotify Musiquita - Lagartijeando Remix by Cyma, Loli Cosmica, Cuervo Cuervo, Lagartijeando Album: Musiquita Duration: 5:25 Listen on YouTube Backup: Listen on Spotify Free by Little Simz Album: Free Duration: 3:35 Listen on YouTube Backup: Listen on Spotify Chivalry Is Not Dead - Salami Rose Joe Louis Remix by Hiatus Kaiyote, Salami Rose Joe Louis Album: Mood Variant (The Remixes) Duration: 3:36 Listen on YouTube Backup: Listen on Spotify Opening Night by Jessica Pratt Album: Quiet Signs Duration: 1:39 Listen on YouTube Backup: Listen on Spotify Shakedown Street - 2013 Remaster by Grateful Dead Album: Shakedown Street Duration: 5:00 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-april-2025","date":"2025-05-02 13:35 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"April 2025 drop. SAULT, Cleo Sol (SAULT singer), and Little Simz were on repeat. Tracks Pray for Me by SAULT Album: Pray for Me Duration: 7:14 Listen on YouTube Backup: Listen on Spotify Can\u0027t Keep...","keywords":["listen","spotify","backup:","album:","duration:","youtube","lands","cleo","little","simz"]},{"id":"bookmarks-all-about-rss","title":"ALL about RSS","content":"ALL about RSS \u2192 https://github.com/AboutRSS/ALL-about-RSS A list of RSS related stuff: tools, services, communities and tutorials, etc.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/all-about-rss","date":"2025-04-28 21:07 -05:00","tags":["rss","indieweb","feeds"],"summary":"ALL about RSS \u2192 https://github.com/AboutRSS/ALL-about-RSS A list of RSS related stuff: tools, services, communities and tutorials, etc.","keywords":["https://github","com/aboutrss/all","list","related","stuff:","tools","services","communities","tutorials"]},{"id":"reshare-gpt-image-1-api","title":"GPT Image 1 - Image Generation API","content":"GPT Image 1 - Image Generation API \u2192 https://openai.com/index/image-generation-api/ Today, we\u2019re bringing the natively multimodal model that powers this experience in ChatGPT to the API via gpt-image-1 , enabling developers and businesses to easily integrate high-quality, professional-grade image generation directly into their own tools and platforms. The gpt-image-1 model is now available globally via the Images API, with support in the Responses API coming soon.","contentType":"reshare","url":"https://www.lqdev.me/responses/gpt-image-1-api","date":"2025-04-24 19:50 -05:00","tags":["openai","ai","gpt"],"summary":"GPT Image 1 - Image Generation API \u2192 https://openai.com/index/image-generation-api/ Today, we\u2019re bringing the natively multimodal model that powers this experience in ChatGPT to the API via...","keywords":["image","generation","model","high","quality","professional","grade","directly","tools","platforms"]},{"id":"reviews-the-book-watts","title":"The Book: On the Taboo Against Knowing Who You Are","content":"The Book: On the Taboo Against Knowing Who You Are by Alan Watts Read Rating: 4.7/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/the-book-watts","date":"2025-04-15 19:45 -05:00","tags":[],"summary":"The Book: On the Taboo Against Knowing Who You Are by Alan Watts Read Rating: 4.7/5","keywords":["book:","taboo","against","knowing","alan","watts","read","rating:"]},{"id":"reshare-llama-4-herd","title":"The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation","content":"The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation \u2192 https://ai.meta.com/blog/llama-4-multimodal-intelligence/ We\u2019re introducing Llama 4 Scout and Llama 4 Maverick, the first open-weight natively multimodal models with unprecedented context length support and our first built using a mixture-of-experts (MoE) architecture. We\u2019re also previewing Llama 4 Behemoth, one of the smartest LLMs in the world and our most powerful yet to serve as a teacher for our new models. Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3 Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks After tinkering with Mistral Small 3.1, I\u0027m excited to try Llama 4 Scout once it lands in GitHub Models and Ollama. I don\u0027t think that Mistral Small 3.1 uses MoE architecture which should help a ton considering the 10 million token context size.","contentType":"reshare","url":"https://www.lqdev.me/responses/llama-4-herd","date":"2025-04-08 08:30 -05:00","tags":["llama","meta","ai","llm"],"summary":"The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation \u2192 https://ai.meta.com/blog/llama-4-multimodal-intelligence/ We\u2019re introducing Llama 4 Scout and Llama 4 Maverick, the...","keywords":["llama","multimodal","model","scout","models","experts","context","small","mistral","best"]},{"id":"notes-ollama-adds-support-mistral-small-3-1","title":"Ollama Adds Mistral 3.1 Support","content":"In the latest Ollama 0.6.5 release support was added for Mistral Small 3.1 . In an earlier post, I highlighted the announcement and summarized the key features . I\u0027ve been using it with GitHub Models and based on vibes I found it was more reliable at structured output compared to Gemma 3, especially when considering the model and context window size. Now that it\u0027s on Ollama thought, I can use it offline as well. However, I\u0027m not sure how well that\u0027ll work on my ARM64 device, which only has 16GB of RAM.","contentType":"notes","url":"https://www.lqdev.me/notes/ollama-adds-support-mistral-small-3-1","date":"2025-04-08 08:19 -05:00","tags":["mistral","ollama","ai","llm"],"summary":"In the latest Ollama 0.6.5 release support was added for Mistral Small 3.1 . In an earlier post, I highlighted the announcement and summarized the key features . I\u0027ve been using it with GitHub Models...","keywords":["ollama","latest","output","compared","gemma","especially","considering","model","context","window"]},{"id":"reshare-gen-z-tumblr-revival","title":"Gen Z Leading Tumblr Revival","content":"Gen Z Leading Tumblr Revival \u2192 https://www.fastcompany.com/91310237/the-tumblr-revival-is-real-and-gen-z-is-leading-the-charge Thanks to Gen Z, the site has found new life. As of 2025, Gen Z makes up 50% of Tumblr\u2019s active monthly users and accounts for 60% of new sign-ups Don\u0027t call it a comeback. User numbers spiked in January during the near-ban of TikTok and jumped again last year when Brazil temporarily banned X. To keep up with the momentum, Tumblr introduced Reddit-style Communities in December, letting users connect over shared interests like photography and video games. In January, it debuted Tumblr TV\u2014a TikTok-like feature that serves as both a GIF search engine and a short-form video platform. But perhaps Tumblr\u2019s greatest strength is that it isn\u2019t TikTok or Facebook. For its users...that\u2019s part of the appeal. Tumblr\u0027s been through a rough ride, but at the core there\u0027s a lot of nice things to like about it. In previous posts, I talked about some opportunities for Tumblr as a result of replatforming on top of WordPress and its expressed intent for ActivityPub integration . In doing so, not only would it enable an intuitive front-end for publishing different kinds of posts, but it would also provide opportunities for self-hosting and a more federated and decentralized social platform. I\u0027m excited to see where this goes. Related post from Business Insider\u0027s Amanda Hoover on the topic.","contentType":"reshare","url":"https://www.lqdev.me/responses/gen-z-tumblr-revival","date":"2025-04-08 06:05 -05:00","tags":["tumblr","automattic","socialmedia","genz","activitypub","fediverse"],"summary":"Gen Z Leading Tumblr Revival \u2192 https://www.fastcompany.com/91310237/the-tumblr-revival-is-real-and-gen-z-is-leading-the-charge Thanks to Gen Z, the site has found new life. As of 2025, Gen Z makes up...","keywords":["tumblr","tiktok","users","video","posts","opportunities","platform","january","tumblr\u2019s","leading"]},{"id":"notes-install-element-desktop-windows-arm64","title":"Install Element Desktop on Windows ARM64 Devices","content":"In a previous post, I talked about my workaround for running Element Desktop on my ARM64 Windows device . I\u0027m happy that I no longer need that workaround. Element Desktop enabled support for ARM64 on Windows and shipped it as part of the 1.11.95 release . While you could go to the downloads website and get the installer from there, you can now also get it via WinGet . I built on the work of the existing community manifest in the winget-pkgs repo and via a pull request updated the latest 1.11.96 version to include the ARM64 installer. While I didn\u0027t create the manifest from scratch, I thought it was really easy to contribute a new installer. I\u0027ll leave the details of my process for another post though. Installing Element Desktop via WinGet is relativaly straightforward. Assuming you already have the WinGet tool installed, all you have to do is run the following commmand in the terminal and you\u0027re all set! winget install Element.Element -a arm64","contentType":"notes","url":"https://www.lqdev.me/notes/install-element-desktop-windows-arm64","date":"2025-04-07 08:38 -05:00","tags":["matrix","arm64","windows","element","winget","opensource","decentralization"],"summary":"In a previous post, I talked about my workaround for running Element Desktop on my ARM64 Windows device . I\u0027m happy that I no longer need that workaround. Element Desktop enabled support for ARM64 on...","keywords":["element","winget","arm64","installer","desktop","post","from","workaround","windows","while"]},{"id":"reviews-keep-going-kleon","title":"Keep Going: 10 Ways to Stay Creative in Good Times and Bad","content":"Keep Going: 10 Ways to Stay Creative in Good Times and Bad by Austin Kleon Read Rating: 4.0/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/keep-going-kleon","date":"2025-03-24 19:45 -05:00","tags":[],"summary":"Keep Going: 10 Ways to Stay Creative in Good Times and Bad by Austin Kleon Read Rating: 4.0/5","keywords":["keep","going:","ways","stay","creative","good","times","austin","kleon","read"]},{"id":"notes-2025-03-23-weekly-post-summary","title":"Week of March 23, 2025 - Post Summary","content":"Responses Bringing Static Sites to the Fediverse - Enhancements and Implementations Introducing Command A Introducing Gemma 3 Mistral Small 3.1 Just pre-ordered by PebbleOS Core Time 2 smartwatch Simon Willison\u0027s AI-Generated Tools Colophon","contentType":"notes","url":"https://www.lqdev.me/notes/2025-03-23-weekly-post-summary","date":"2025-03-23 12:25 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Responses Bringing Static Sites to the Fediverse - Enhancements and Implementations Introducing Command A Introducing Gemma 3 Mistral Small 3.1 Just pre-ordered by PebbleOS Core Time 2 smartwatch...","keywords":["introducing","responses","generated","willison","simon","smartwatch","core","pebbleos","ordered","small"]},{"id":"reshare-bringing-static-sites-fediverse-enhancements-implementations-maho","title":"Bringing Static Sites to the Fediverse - Enhancements and Implementations","content":"Bringing Static Sites to the Fediverse - Enhancements and Implementations \u2192 https://maho.dev/2025/03/a-guide-to-implementing-activitypub-in-a-static-site-or-any-website-q1-2025-updates/ Awesome post from my friend maho.dev I\u2019ve introduced customizable templates for note generation. This enhancement allows the full content of a blog post to be included directly in ActivityPub notes, moving beyond mere link sharing\u2014a practice often associated with bots\u2014and leveraging the full potential of the Fediverse. This is cool. I\u0027d like something better for my POSSE setup. Might give this a try.","contentType":"reshare","url":"https://www.lqdev.me/responses/bringing-static-sites-fediverse-enhancements-implementations-maho","date":"2025-03-18 21:08 -05:00","tags":["fediverse","indieweb","activitypub"],"summary":"Bringing Static Sites to the Fediverse - Enhancements and Implementations \u2192 https://maho.dev/2025/03/a-guide-to-implementing-activitypub-in-a-static-site-or-any-website-q1-2025-updates/ Awesome post...","keywords":["static","fediverse","activitypub","post","full","bringing","content","blog","included","directly"]},{"id":"reshare-introducing-gemma-3","title":"Introducing Gemma 3","content":"Introducing Gemma 3 \u2192 https://blog.google/technology/developers/gemma-3/ Today, we\u0027re introducing Gemma 3, a collection of lightweight, state-of-the-art open models built from the same research and technology that powers our Gemini 2.0 models. These are our most advanced, portable and responsibly developed open models yet. They are designed to run fast, directly on devices \u2014 from phones and laptops to workstations \u2014 helping developers create AI applications, wherever people need them. Gemma 3 comes in a range of sizes (1B, 4B, 12B and 27B), allowing you to choose the best model for your specific hardware and performance needs. Build with the world\u0027s best single-accelerator model: Gemma 3 delivers state-of-the-art performance for its size, outperforming Llama3-405B, DeepSeek-V3 and o3-mini in preliminary human preference evaluations on LMArena\u2019s leaderboard. This helps you to create engaging user experiences that can fit on a single GPU or TPU host. Create AI with advanced text and visual reasoning capabilities : Easily build applications that analyze images, text, and short videos, opening up new possibilities for interactive and intelligent applications. Handle complex tasks with an expanded context window: Gemma 3 offers a 128k-token context window to let your applications process and understand vast amounts of information. Create AI-driven workflows using function calling: Gemma 3 supports function calling and structured output to help you automate tasks and build agentic experiences.","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-gemma-3","date":"2025-03-18 20:55 -05:00","tags":["gemma","google","ai"],"summary":"Introducing Gemma 3 \u2192 https://blog.google/technology/developers/gemma-3/ Today, we\u0027re introducing Gemma 3, a collection of lightweight, state-of-the-art open models built from the same research and...","keywords":["gemma","applications","create","build","models","introducing","experiences","text","single","performance"]},{"id":"reshare-introducing-command-a-cohere","title":"Introducing Command A","content":"Introducing Command A \u2192 https://cohere.com/blog/command-a Today, we\u2019re introducing Command A, a new state-of-the-art generative model optimized for demanding enterprises that require fast, secure, and high-quality AI. Command A delivers maximum performance with minimal hardware costs when compared to leading proprietary and open-weights models, such as GPT-4o and DeepSeek-V3. For private deployments, Command A excels on business-critical agentic and multilingual tasks, while being deployable on just two GPUs, compared to other models that typically require as many as 32. Its 256k context length (2x most leading models) can handle much longer enterprise documents. Other key features include Cohere\u2019s advanced retrieval-augmented generation (RAG) with verifiable citations, agentic tool use, enterprise-grade security, and strong multilingual performance. The next generation of Cohere models will help power a range of AI applications for customers across industries like finance, healthcare, manufacturing, energy, and the public sector. In particular, they will seamlessly integrate with North, our secure AI agents platform to unlock the full potential of your company data and people with AI agents.","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-command-a-cohere","date":"2025-03-18 20:45 -05:00","tags":["cohere","ai","llm"],"summary":"Introducing Command A \u2192 https://cohere.com/blog/command-a Today, we\u2019re introducing Command A, a new state-of-the-art generative model optimized for demanding enterprises that require fast, secure,...","keywords":["command","models","introducing","generation","multilingual","agentic","compared","performance","secure","require"]},{"id":"reshare-willison-tools-colophon","title":"Simon Willison\u0027s AI-Generated Tools Colophon","content":"Simon Willison\u0026#39;s AI-Generated Tools Colophon \u2192 https://tools.simonwillison.net/colophon The tools on tools.simonwillison.net were mostly built using AI-assisted programming. This page lists the commit messages for each tool, many of which link to the LLM transcript used to produce the code. The descriptions for each of the tools were generated using Claude 3.7 Sonnet. This is such a cool use of a website. Using AI, Simon has generated a set of small utilities that are useful to him. If they\u0027re useful to others though, he\u0027s made them avaiable on his website.","contentType":"reshare","url":"https://www.lqdev.me/responses/willison-tools-colophon","date":"2025-03-18 20:35 -05:00","tags":["colophon","indieweb","ai","llm"],"summary":"Simon Willison\u0026#39;s AI-Generated Tools Colophon \u2192 https://tools.simonwillison.net/colophon The tools on tools.simonwillison.net were mostly built using AI-assisted programming. This page lists the...","keywords":["tools","generated","using","simon","simonwillison","useful","website","produce","code","descriptions"]},{"id":"reshare-mistral-small-3-1","title":"Mistral Small 3.1","content":"Mistral Small 3.1 \u2192 https://mistral.ai/fr/news/mistral-small-3-1 Building on Mistral Small 3, this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini, while delivering inference speeds of 150 tokens per second. Lightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases.","contentType":"reshare","url":"https://www.lqdev.me/responses/mistral-small-3-1","date":"2025-03-18 20:31 -05:00","tags":["mistral","ai","opensource"],"summary":"Mistral Small 3.1 \u2192 https://mistral.ai/fr/news/mistral-small-3-1 Building on Mistral Small 3, this new model comes with improved text performance, multimodal understanding, and an expanded context...","keywords":["small","mistral","model","tokens","building","makes","32gb","4090","single","lightweight:"]},{"id":"reshare-preordered-pebbleos-watch","title":"Just pre-ordered by PebbleOS Core Time 2 smartwatch","content":"Just pre-ordered by PebbleOS Core Time 2 smartwatch \u2192 https://ericmigi.com/blog/introducing-two-new-pebbleos-watches Just pre-ordered my Pebble Core Time 2 Duo! Sadly it won\u0027t be here until December. Go to https://store.repebble.com/ to get yours.","contentType":"reshare","url":"https://www.lqdev.me/responses/preordered-pebbleos-watch","date":"2025-03-18 20:01 -05:00","tags":["pebbleos","smartwatch","opensource","pebble"],"summary":"Just pre-ordered by PebbleOS Core Time 2 smartwatch \u2192 https://ericmigi.com/blog/introducing-two-new-pebbleos-watches Just pre-ordered my Pebble Core Time 2 Duo! Sadly it won\u0027t be here until December....","keywords":["ordered","pebbleos","core","smartwatch","https://ericmigi","com/blog/introducing","watches","pebble","sadly","here"]},{"id":"reshare-ax-why-agent-experience-matters","title":"Introducing AX: Why Agent Experience Matters","content":"Introducing AX: Why Agent Experience Matters \u2192 https://biilmann.blog/articles/introducing-ax/ Interesting post from Matt Biilman . ...the largest disruption from the current evolution of AI will come from bringing agency to computers. Computers will perceive their environment and take actions autonomously in order to achieve goals. Companies discovered that improving the DX of their products would empower and incentivize developers to extend their product with new capabilities and lead to huge competitive advantages. For developer tool companies, DX became a key competitive differentiator. We need to start focusing on AX or \u201Cagent experience\u201D \u2014 the holistic experience AI agents will have as the user of a product or platform. Too many companies are focusing on adding shallow AI features all over their products or building yet another AI agent. The real breakthrough will be thinking about how your customers\u2019 favorite agents can help them derive more value from your product. This requires thinking deeply about agents as a persona your team is building and developing for. As AI agents start becoming useful and commonplace, we\u2019re broadly going to see two approaches enabling agents to interact with the software we depend on: A closed vertical approach, where companies tightly integrate their own agents into their own software. An open approach, where companies focus on making their software accessible to external agents. Leaning into AX as a strategy, means embracing a vision of an open agent world. This vision aligns with the original ethos of the open web: a place where many diverse competing agents (built by different people or companies) can seamlessly interact with software on behalf of their users. Prioritizing AX makes it as simple as possible for any agent a user prefers, to deliver outcomes on their behalf. Software designed for AI agents has the potential to deliver exponential value. As an industry we must collectively focus on building an open agent ecosystem and designing thoughtful AX to create a better, more open, and connected digital world.","contentType":"reshare","url":"https://www.lqdev.me/responses/ax-why-agent-experience-matters","date":"2025-03-05 21:49 -05:00","tags":["ai","agents","ax"],"summary":"Introducing AX: Why Agent Experience Matters \u2192 https://biilmann.blog/articles/introducing-ax/ Interesting post from Matt Biilman . ...the largest disruption from the current evolution of AI will come...","keywords":["agents","companies","agent","open","software","from","building","product","focus","many"]},{"id":"notes-crate-finds-february-2025","title":"Crate Finds - February 2025","content":"February\u0027s drop. I\u0027ve been listening to a lot of Andre 3000 and stumbled upon Carlos Nino and Friends. Tracks Procession: Opening by Deantoni Parks, Hanna Benn, Leo Abrahams Album: Procession: Opening Duration: 11:46 Listen on YouTube Backup: Listen on Spotify We Love You So Much by Carlos Ni\u00F1o, Nate Mercereau, Idris Ackamoor Album: Free, Dancing. . . Duration: 6:58 Listen on YouTube Backup: Listen on Spotify Astral Traveling by Pharoah Sanders Album: Thembi Duration: 5:50 Listen on YouTube Backup: Listen on Spotify Celesta by Azamiah Album: In Phases Duration: 4:09 Listen on YouTube Backup: Listen on Spotify Monologue II by Azamiah Album: In Phases Duration: 1:13 Listen on YouTube Backup: Listen on Spotify Night Woman by Azamiah Album: In Phases Duration: 4:49 Listen on YouTube Backup: Listen on Spotify Solace by Azamiah Album: In Phases Duration: 0:58 Listen on YouTube Backup: Listen on Spotify Baba Yaga by Daedelus Album: The Light Brigade Duration: 1:33 Listen on YouTube Backup: Listen on Spotify Real Vital Organs by Carlos Ni\u00F1o \u0026amp; Friends, Jamael Dean Album: Placenta Duration: 3:57 Listen on YouTube Backup: Listen on Spotify This \u201CI\u201D was not by Carlos Ni\u00F1o \u0026amp; Friends, Ariel Kalma, Annelise Album: Placenta Duration: 2:37 Listen on YouTube Backup: Listen on Spotify Birthworkers Magic, and how we get hear . . . by Carlos Ni\u00F1o \u0026amp; Friends, Maia M\u0103l\u0103ncu\u0219, Andr\u00E9 3000, Jesse Peterson Album: Placenta Duration: 7:21 Listen on YouTube Backup: Listen on Spotify Placenta, Nourishment, New Home, The Galaxy by Carlos Ni\u00F1o \u0026amp; Friends, Haize Hawke, Surya Botofasina, Aaron Shaw, Devin Daniels, Tiffany de Leon, Michael Bolger, Michael Alvidrez, Jesse Peterson, Annelise, Moss Album: Placenta Duration: 9:03 Listen on YouTube Backup: Listen on Spotify Carla\u0027s Beads by Carlos Ni\u00F1o \u0026amp; Friends, Iasos Album: Placenta Duration: 1:51 Listen on YouTube Backup: Listen on Spotify \u201CPlay Kerri Chandler\u2019s RAIN\u201D by Carlos Ni\u00F1o \u0026amp; Friends, Surya Botofasina, Nate Mercereau, Cavana Lee Album: Placenta Duration: 17:32 Listen on YouTube Backup: Listen on Spotify Thanking the Earth by Carlos Ni\u00F1o \u0026amp; Friends, Sam Gendel, Nate Mercereau Album: More Energy Fields, Current Duration: 5:20 Listen on YouTube Backup: Listen on Spotify Salon Winds by Carlos Ni\u00F1o \u0026amp; Friends, Jamael Dean, Jamire Williams, Nate Mercereau, Aaron Shaw Album: More Energy Fields, Current Duration: 7:23 Listen on YouTube Backup: Listen on Spotify Togetherness by Carlos Ni\u00F1o \u0026amp; Friends, Jamael Dean, Devin Daniels Album: More Energy Fields, Current Duration: 3:57 Listen on YouTube Backup: Listen on Spotify Arm fell asleep by Salami Rose Joe Louis Album: Inside Duration: 4:48 Listen on YouTube Backup: Listen on Spotify Inside by Salami Rose Joe Louis Album: Inside Duration: 4:09 Listen on YouTube Backup: Listen on Spotify Cuvier\u0027s by Gonubie Album: Signals At Both Ears Duration: 2:56 Listen on YouTube Backup: Listen on Spotify Intro Eternity by Miguel Atwood-Ferguson Album: Library Selection Duration: 3:22 Listen on YouTube Backup: Listen on Spotify Indian Food by Dumbo Gets Mad Album: Quantum Leap Duration: 3:33 Listen on YouTube Backup: Listen on Spotify Saudade Love by Sugar Candy Mountain Album: Mystic Hits Duration: 5:12 Listen on YouTube Backup: Listen on Spotify Forest Pulse by Carlos Ni\u00F1o, Carlos Ni\u00F1o \u0026amp; Friends, Laraaji, Photay Album: Bliss On Dear Oneness Duration: 3:57 Listen on YouTube Backup: Listen on Spotify Clouds by Resavoir Album: Resavoir Duration: 1:11 Listen on YouTube Backup: Listen on Spotify Escalator by Resavoir, Sen Morimoto Album: Resavoir Duration: 4:05 Listen on YouTube Backup: Listen on Spotify Other Side Of The Game by Erykah Badu Album: Baduizm Duration: 6:33 Listen on YouTube Backup: Listen on Spotify Next Lifetime by Erykah Badu Album: Baduizm Duration: 6:26 Listen on YouTube Backup: Listen on Spotify Certainly by Erykah Badu Album: Baduizm Duration: 4:43 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-february-2025","date":"2025-03-02 13:35 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"February\u0027s drop. I\u0027ve been listening to a lot of Andre 3000 and stumbled upon Carlos Nino and Friends. Tracks Procession: Opening by Deantoni Parks, Hanna Benn, Leo Abrahams Album: Procession:...","keywords":["listen","spotify","album:","backup:","youtube","duration:","carlos","ni\u00F1o","friends","\u0026amp"]},{"id":"notes-2025-03-02-weekly-post-summary","title":"Week of March 2, 2025 - Post Summary","content":"Blogs Fractures of Unfamiliarity \u0026amp; Circumvention in Pursuit of a Nice Time and the Meaning of Franklin\u0027s Tower Responses Claude 3.7 Sonnet and Claude Code The Algorithm - J. Cole has a blog? Introducing Mercury, the first commercial-scale diffusion large language model How to Multistream Using Azure, Azure VM, MonaServer 2, and FFmpeg with OBS Studio Nomic Embed Text V2: An Open Source, Multilingual, Mixture-of-Experts Embedding Model s1: Simple test-time scaling Unplatform Your Site Is a Home","contentType":"notes","url":"https://www.lqdev.me/notes/2025-03-02-weekly-post-summary","date":"2025-03-02 05:53 -05:00","tags":["weeklysummary","blogging","indieweb","website"],"summary":"Blogs Fractures of Unfamiliarity \u0026amp; Circumvention in Pursuit of a Nice Time and the Meaning of Franklin\u0027s Tower Responses Claude 3.7 Sonnet and Claude Code The Algorithm - J. Cole has a blog?...","keywords":["claude","azure","model","multistream","using","monaserver","ffmpeg","studio","nomic","embed"]},{"id":"reshare-mercury-first-commercial-scale-llm","title":"Introducing Mercury, the first commercial-scale diffusion large language model","content":"Introducing Mercury, the first commercial-scale diffusion large language model \u2192 https://www.inceptionlabs.ai/news We are announcing the Mercury family of diffusion large language models (dLLMs), a new generation of LLMs that push the frontier of fast, high-quality text generation. Mercury is up to 10x faster than frontier speed-optimized LLMs. Our models run at over 1000 tokens/sec on NVIDIA H100s, a speed previously possible only using custom chips.","contentType":"reshare","url":"https://www.lqdev.me/responses/mercury-first-commercial-scale-llm","date":"2025-02-26 20:10 -05:00","tags":["diffusion","llm","ai"],"summary":"Introducing Mercury, the first commercial-scale diffusion large language model \u2192 https://www.inceptionlabs.ai/news We are announcing the Mercury family of diffusion large language models (dLLMs), a...","keywords":["mercury","frontier","diffusion","large","language","llms","generation","speed","models","introducing"]},{"id":"posts-fractures-unfamiliarity-circumvention-pursuit-good-time-robert-hunter","title":"Fractures of Unfamiliarity \u0026 Circumvention in Pursuit of a Nice Time and the Meaning of Franklin\u0027s Tower","content":"The past few days, I\u0027ve had the song Franklin\u0027s Tower by The Grateful Dead on repeat. I found some of the lyrics could be taken at face value but others didn\u0027t make sense to me. Since I assumed there was a deeper meaning, I decided to look for interpretations and ran into this [article by David Dodd on the official Grateful Dead website](https://www.dead.net/features/greatest-stories-ever-told/greatest-stories-ever-told-franklins-tower). This led me to two amazing essays: - [The Fractals of Familiarity and Innovation: Robert Hunter and the Grateful Dead concert experience](https://web.archive.org/web/20240604001611/http://artsites.ucsc.edu/gdead/agdl/jurgen.html) by Jurgen Fauth - [Fractures of Unfamiliarity \u0026 Circumvention in Pursuit of a Nice Time](https://web.archive.org/web/20160519052524/http://artsites.ucsc.edu/GDead/agdl/fauthrep.html) by Robert Hunter Hunter\u0027s essay is a response to Fauth\u0027s. Also, in Hunter\u0027s essay you begrudgingly get a detailed breakdown of the lyrics of Franklin\u0027s Tower. I\u0027ll note how amazing it is that both of these individuals took the time to write these lengthy essays clearly exposing their ideas and points of view on the subject. Both of those articles were difficult to find because sadly the pages no longer exist in the website that had posted them. After some digging though, I was able to find them on the Internet Archive. So for the sake of preservation, I\u0027ll post both of them here in full form. ## The Fractals of Familiarity and Innovation: Robert Hunter and the Grateful Dead concert experience \u0060\u0060\u0060text Robert Hunter has been the primary lyricist for the Grateful Dead ever since he mailed \u0022St. Stephen,\u0022 \u0022China Cat Sunflower,\u0022 and \u0022Alligator\u0022 to the band from New Mexico in 1967. Born in 1941, Hunter grew up in California and studied drama for a year at the University of Connecticut, before he spent half a year with the National Guard and later signed up for the same psychedelic research that led Ken Kesey to write One Flew Over The Cuckoo\u0027s Nest. His songwriting collaboration with Jerry Garcia and, occasionally, other members of the Grateful Dead yielded such classic Dead tunes as \u0022Truckin\u0027,\u0022 \u0022Touch of Grey,\u0022 and \u0022Dark Star\u0022. Hunter\u0027s lyrics emblazon thousands of tie-dyed t-shirts and skeleton-adorned bumper stickers. Other work of Hunter\u0027s includes lyrics for Zero and Bob Dylan as well as a translation of Rainer Maria Rilke\u0027s Duino Elegies and volumes of poetry, most recently Sentinel. When analyzing song lyrics, certain basic differences from poetry have to be understood and observed. When comparing popular song lyrics to contemporary or classic poetry, it is difficult not to miss a striking difference in quality--song lyrics generally tend to be outspoken and trite where poetry clouds itself in ambiguity and complex imagery. Bill Flanagan argues that: \u0022Rock \u0026 roll musicians are strange artists. Their code of manners dictates that they not appear to take themselves very seriously, while the best of their work contains all the passion, imagination, and craft of legitimate art. The intelligent rock musician is put in the position of working as hard as a poet or painter, while being expected to accept his gifts with the self- deprecating shrug of the idiot savant.\u0022 (Flanagan, ix) However, the majority of rock lyricists aims at an audience which is supposed to recognize a song instantly during the few weeks it is played heavy-rotation on FM radio and then forget it as soon as possible. Rock lyrics tend to make a point quickly and to make sure that it is understood. Therefore, they reach for clich\u00E9d imagery and blatant statements of emotions that traditional poetry would have tended to skillfully embed in high-flung imagery, a tendency which can lead to examples as inane as the ones parodied in Donald Barthelme\u0027s \u0022How I Write My Songs.\u0022 One of the reasons, however, why such a direct comparison of lyrics and poetry is not fair is not only to be found in the consumer culture pressures that surround the pop music industry, but in the fact that popular music lyrics are not meant to be read at all. Obviously, the text has to be read within the framework of the music it was written for. This holds especially true for Robert Hunter\u0027s lyrics, because the unique nature of the music of the Grateful Dead adds layers of meaning to the text and vice versa. A further point I decided to take into consideration is that the Grateful Dead are not successful as a recording band that scores top ten hits (with the notable exception of the 1987 \u0022Touch of Grey\u0022); their studio albums are usually do not fulfill expectations. The strength of the band is unquestionably the live concert--the Grateful Dead have been the number one grossing touring band over the last couple of years, outdoing the likes of Madonna and Paul McCartney, without releasing a new studio album since Built to Last in 1989. This means that the usual way Hunter lyrics are being perceived is at a concert. Therefore, everything that happens at a Grateful Dead concert should be taken into account when reading the lyrics. In this paper, I will try to show how the Grateful Dead concert experience and the lyrics of Robert Hunter\u0027s songs interweave and complement each other, mutually adding significance and \u0022meaning\u0022 to one another. One model on how to understand the mutually reinforcing structure of a rock concert can be abstracted from Erika Fischer-Lichte\u0027s The Semiotics of Theater. Of course, according to Fischer-Lichte\u0027s definition, a concert does not constitute theater; however, her theories can give valuable ideas for a way of looking at the experience of a concert. According to Fischer-Lichte, every sign that is given during the performance can be assigned to a group or system, and the sum of all used and received signs that either correspond or oppose each other in meaning allow an overall interpretation or understanding of the performance. The fourteen sign systems Fischer-Lichte enumerates are: sound, music, linguistic signs, paralinguistic signs, mimics, gestures, proxemic signs, mask, hair, costume, stage conception, stage decoration, props, and lighting. These all make sense in a rock concert environment, especially in more theatrical performances as they became common with stadium bands during the 1970s. An additional system of significance would be the song selection and order, which might be considered a subgroup of the music or linguistic groups. Audience participation should be included, as well. Unless a lot of spoken text is involved, the paralinguistics can not be considered significant, since the loudness, pitch, etc. of a song are inherent in the music and therefore do not justify a group of their own. Props are rare in rock concerts, as well, although some performers have been known to turn their instruments into significant objects, such as Liberace or Prince. The uniqueness of a Grateful Dead concert lies in the emphasis on certain sign systems over others. While obviously almost every rock concert relies heavily and primarily on the music and the linguistical signs (which are, after all, our objects of interest), the Dead emphasize them more than usual by cutting down on the others--the stage decoration is colorful yet unobtrusive, there is no dancing or hardly any movement by the band members other than what is necessary to play the instruments, the clothing of the band is purely functional and they stopped conversing with the audience years ago. The lighting, designed by Candance Brightman since 1972, is not self-conscious but supports the music: \u0022Brightman\u0027s lighting is extraordinarily sensitive to subtleties in the band\u0027s interactions, shunning bombastic effects for a vivid focusing of attention that is as much a part of the jam as the music itself\u0022 (Shenk 28). Furthermore, it includes the audience by \u0022[breaking] down the barriers between the Dead and its fans by lighting the band so they can still see the audience, or by lighting the audience itself\u0022 (Shenk 29). An important point to note is that none of the sign systems function to enhance the star status of the band-- everything is focused on the music and the relation to the audience. The absence of proxemic signs, mask, hair, props, and costumes is significant in that it enhances the emphasis on the music and the lyrics while taking away from the special status of the band members. The order of songs, though, is of exceptional importance during a Grateful Dead show. Traditionally, since about 1977, the show is split into a first and a second set, where the first set is characterized by more conventional songs that are played without too much improvised jamming; the length of the songs stays within reasonable limits as well. The second set, in contrast, features many improvised jams that stretch the songs easily out to fifteen, twenty minutes. At some point in the second set, Mickey Hart and Bill Kreutzman (percussion \u0026 drums) will take over and play an extend drum solo while everyone else leaves the stage. Before they are finished, the other band members return, and Hart and Kreutzman leave the stage. What follows is an extended, beatless jam of exploratory sounds known as \u0022Space.\u0022 Bob Bralove, in the liner notes to Infrared Roses, describes it like this: \u0022Somewhere in the middle of the second set of every Grateful Dead show the band turns a corner. They enter a musical environment without walls or structure. The song form is abandoned, and the very elements of music may be called into question. The only mandate is to explore new territory. It is an environment where rhythm, tone, color, melody, and harmony can be explored without rules or predetermination, a musical adventure where composition and performance are one.\u0022 After \u0022Space,\u0022 the band will play three or four more songs, commonly ending with a high- energy song like \u0022Good Lovin\u0027\u0022 or \u0022Sugar Magnolia,\u0022 and come back for a one-song encore. This pattern forms a relatively firm structure in which highly unstable elements are put--the music is always improvised, and not one song has been played twice the same way, and the actual list of songs played always varies, to the point where not even the band members know what they will play before they go on stage. This is of high importance because the underlying structural principle will become evident in Hunter\u0027s lyrics, as well. Since certain songs are usually played in the first set and others in the second set, it seems reasonable to group them according to this differentiation and begin by taking a closer look at the first set songs. \u0022Jack Straw\u0022 is a classic first set song--out of the 424 times it had been played between 1971 and 1992, DeadBase lists only 41 second-set appearances. The song appears to be a narrative of a criminal character who drifts through the American wilderness, jumping trains and digging graves while eagles circle high above. It seems to be rooted deeply in an American tradition of fireside songs. The same holds true for \u0022Dire Wolf,\u0022 a bitter story of death and gambling, precisely set in time and space: In the timbers of Fennario the wolves are running \u0027round The winter was so hard and cold froze ten feet \u0027neath the ground (1-4) The narrative features virtually no metaphors or similes; everything seems to happen at face value--a complicated search for meaning or \u0022interpretation\u0022 is unnecessary. Again, the song is closer to traditional American music than love songs heard on the radio. It is direct yet not necessarily blatant or trite since the subject matter is uncommon for contemporary American rock lyrics. The Grateful Dead classic \u0022Truckin\u0027\u0022 features some opaque references, yet the song is obviously trying to capture the vicissitudes of a life on the road as the band members have experienced it. Some lines, like \u0022Busted--down on Bourbon Street\u0022 (37) refer to commonly known incidents (in this case the band\u0027s drug bust in New Orleans); others like \u0022What in the world ever became of sweet Jane?\u0022 (25) are probably in-jokes that aren\u0027t meant for a broad audience to grasp. Even more clearly an American song in the best tradition of folk singers like Woody Guthrie or Phil Ochs is \u0022Cumberland Blues,\u0022 a song depicting the plight of a mine worker. Hunter\u0027s eye in describing his topic was so sharp that he heard of an actual Cumberland miner wondering \u0022what the guy who wrote this song would\u0027ve thought if he\u0027d ever known something like the Grateful Dead was gonna do it.\u0022 He considers this the best compliment he ever received for a lyric (Hunter 52), obviously because it certifies an authenticity he aimed for. It seems that first set songs commonly share a genuine American topic (the open road, the wilderness, unions, etc.) and often sketch a narrative world including characters and situations. Instead of propagating emotions, as rock lyrics often seem to, they focus on fictional events that often involve gamblers, cowboys, drunkards, and outlaws. The imagery and \u0022lyrical\u0022 makeup of the songs tends to be simple so as to give a definitive, discernable \u0022meaning.\u0022 An investigation of second set songs quickly shows that they are of a different character. \u0022China Cat Sunflower,\u0022 a typical second-set song since the early eighties, may serve as a prime example: the lyrics refuse any narrative or symbolic reading, the words simply are. Hunter acknowledges the non-rational nature of the song with his tongue-in-cheek remark: \u0022Nobody ever asked me the meaning of this song. People seem to know exactly what I\u0027m talking about. It\u0027s good that a few things in this world are clear to all of us\u0022 (Hunter 35). Nonsensical as they may be, the lyrics bear a close resemblance to the dreamy, colorful playfulness of Lennon/McCartney\u0027s \u0022Lucy in the Sky with Diamonds\u0022-- Picture yourself in a boat on a river, With tangerine trees and marmalade skies Somebody calls you, you answer quite slowly, A girl with kaleidoscope eyes. Cellophane flowers of yellow and green, Towering over your head. Look for the girl with the sun in her eyes, And she\u0027s gone. Lucy in the sky with diamonds (1-9) In the surrealistic, candy-colored playfulness of both songs, the words are not meant to \u0022mean\u0022 anything; the usual tools of interpretation have to fail here. As Hunter says, \u0022I like a diamond here, a ruby there, a rose, certain kinds of buildings, vehicles, gems. These things are all real, and the word evokes the thing. That\u0027s what we\u0027re working with, evocation.\u0022 (Gans 26). His concern is not merely narrative or mood anymore, but evocation and association, something much closer to poetry than to the common clich\u00E9s of pop lyrics. \u0022Crazy Fingers\u0022 is another example for this approach to songwriting--it is content with letting its verses set a mood without making any \u0022sense.\u0022 Hunter calls it a \u0022collection of haiku- style verses, mostly seventeen syllables, some more successful than others, with no connecting link other than similarity of mood\u0022 (Hunter 45). Some of the imagery is indeed not very original; the \u0022Deep Sea of Love\u0022 (7) can surely be encountered elsewhere. Nonetheless, certain paradoxical images (like \u0022Recall the days that still are to come\u0022 (3)) and the overall disjunctive structure of the text make for a strange, unlikely song that refuses to be \u0022understood\u0022 on any intellectual level. It can only be described in its emotional impact, the longing and bittersweet reminiscing it evokes in a unique way. Similar points can be made for \u0022Eyes of the World\u0022-- it is interesting to note that the lyrics seem to make sense, yet on any \u0022objective\u0022 level, they do not. Traditional analysis of the text may or may not yield satisfactory results by assigning connotations and significance to, say, the \u0022redeemer\u0022(22) , the mile-long wings (7) or the \u0022eyes of the world\u0022 themselves; fact is that the inherent instability of the lyrics, fueled again by interior contradictions like \u0022...the seeds that were silent / all burst into bloom and decay,\u0022 (26-27) will not allow for a final, reductive interpretation. It is clear that Hunter does not aim to communicate \u0022meaning\u0022 at all; he says, \u0022I\u0027d really prefer not to get into tearing apart the symbology of my songs [ . . . ] and I\u0027ll tell you why: symbols are evocative, and if there were a more definite way to say things, you\u0027d say them that way. A symbol, by it\u0027s very nature, can pull in many, many shades of meaning, depending on the emotional tone with which you engage the piece\u0022 (Gans 23). The lyrics of songs commonly played as encore or to close the second set are often more direct (as \u0022Sugar Magnolia\u0022) or soothing lullabies (\u0022Brokedown Palace\u0022). The course of a Grateful Dead concert, then, can be said to describe an arc that ranges from traditional, narrative lyrics that concur with conventional playing to outrageous flights of fancy in the second set, correspondingly in the musical jams as well as in the ambiguity-laden, irrational, highly evocative lyrics. The strangeness climaxes during the purely instrumental \u0022Space,\u0022 during which the band is at its most exploratory. The last songs and the encore return the audience to a festive or pensive mood. However, the function of the lyrics during a Grateful Dead concert is more than to simply underline the music--the linguistic signs are not present to emphasize what is happening in the music. Instead, there seems to be a strange interplay between familiarity and newness, and it is at this edge of the well-known and the innovative that the Grateful Dead draw energy. Audience response is a valuable key to this aspect of the music and the lyrics. Experience and my subjective impression suggests that the audience at a Grateful Dead show is usually more responsive and alert than audiences at other rock concerts, but since there is no data to prove or argue this point, it can not be made in a meaningful way. What is observable and obvious, though, are the immense cheers and shouts that erupt at certain points in the concert, namely when the band returns from a long stretch of interpretation to the melody (and therefore, mostly, the lyrics), and at certain catchphrases within the lyrics that have gathered almost mystical meaning over the years. Both phenomena need to be looked at more closely. In the improvisational style of the Grateful Dead (\u0022jazz syntax with a rock lexicon,\u0022 as David Gans calls it (ix)), melody as one of the markers signifying a song is subject to sudden change-- at its best, the band composes on the stage, making up new elements on the go; the process of creating becomes visible to the audience. Part of the philosophy that allows this approach is that mistakes are possible, probable, and excusable-- \u0022on a bad night, they bore you to tears, but a good night makes it all worth it.\u0022 (Gans iix) The initial melody of the song, however, is not variable, and it is always safe to predict the band\u0027s return to it after a period of jamming that is so utterly unpredictable in direction, style, and length that not even the band itself can foresee what will happen: \u0022I can\u0027t predict what the band is going to do at any given moment\u0022 (Jerry Garcia quoted in Gans 59). Since the lyrics will be sung only once the proper, initial melody has returned, the counter argument is mostly true, also--when the melody returns, so will the lyrics. The audience cheers at these points because the return of the melody and therefore the lyrics means a return to the familiar, to the predictable and understandable after long forays into the unknown. While this holds true for virtually any return to the preestablished, well-worn structural elements of the song, certain catchphrases within Hunter\u0027s lyrics seem to yield an especially enthusiastic response by the audience. A good example for this is \u0022If you get confused, listen to the music play\u0022 (20) from \u0022Franklin\u0027s Tower\u0022, a line that taken by itself offers guidance to a perplexed listener. Yet it is embedded within a song that is as obscure as a lot of second set- lyrics; \u0022Franklin\u0027s Tower\u0022 is evocative yet of undeterminable meaning. The above-mentioned catchphrase-line is almost hidden within the song and has to be read out of context to prompt the joyful response it usually gets. More obvious examples of the embedded catchphrase are to be found in first-set songs like \u0022Loose Lucy,\u0022 a rare Hunter love song, where the audience chooses to take the chorus out of context by applying it to the band itself: \u0022Singing: Yeah, yeah, yeah, yeah / singing: Thank you / for a real good time\u0022 (7-9). Here, the function seems to be that of a mutual pat on the back; the cheers represent the subculture of Deadheads and the Grateful Dead acknowledging each other\u0027s faithfulness--it is here that it becomes manifest that the \u0022Dead made a mystical pact with their fans, vowing to carry the psychedelic torch as long as they can play their music\u0022 (Lee 290). Whether Hunter had such an opportunity for unique bonding in mind when he wrote the lines remains undetermined, yet the authors of Skeleton Key could not but remark on Hunter\u0027s \u0022knack for coining aphorisms that yield hard-won truths after hundreds of hearings\u0022 (150). The most famous example is certainly the oft-quoted \u0022Lately it occurs to me / what a long, strange trip it\u0027s been\u0022 (23-24) from \u0022Truckin\u0027,\u0022 a line that paradoxically admits to the uniqueness of the experience by being its most clich\u00E9d example. It seems that a lot of these phrases took on the meaning for the Deadhead community that they did because they were obvious and understandable. On a paradigmatic level, it should be surprising that a band that is so directly associated with a certain lifestyle and a set of beliefs as the Grateful Dead hardly convey any \u0022messages\u0022 at all in their lyrics. It can be safely assumed that one would generally expect a \u0022hippie band\u0022 with all the unavoidable stereotypes, to be much more blatant about their convictions. Yet, as the above analysis already shows, Hunter usually confines himself to a narratives of traditional American adventures in the broadest sense and more abstract, lyrical, and image-laden texts that only yield \u0022meaning\u0022 by association and evocation. However, there are lines hidden within his lyrics that have taken on very charged connotations exactly because they seem to be more outspoken and direct, and a philosophy can be attributed to them. All these lines have turned into catchphrases as well, cheered at concerts and printed on countless t-shirts and bumper stickers. \u0022Scarlet Begonias,\u0022 a song commonly played to open the second set yet at least somewhat narrative in that it has a narrator and a female character, yields two instances of embedded catchphrases-- \u0022Once in a while / you get shown the light / in the strangest of places / if you look at it right\u0022 (31-34) and \u0022Strangers stop strangers / just to shake their hand\u0022 (46-47). The philosophies that can be abstracted from these lines by reading them out of context include the search for truth in unlikely places, maybe in the tradition of the \u0022angel-headed hipsters\u0022 (2) who roam the urban wasteland in Ginsberg\u0027s \u0022Howl,\u0022 and a proclamation of unsolicited friendliness and openness that the stock and trade of the hippiedom the Grateful Dead supposedly stand for. All this is abstracted again from the less direct context the lines originally appear in; the Deadhead community seems to have taken what it could and abandoned the quirky senselessness of \u0022The wind in the willows played tea for two / The sky was yellow and the sun was blue\u0022 (44- 45). The argument could be made that the aphoristic lines are not only cheered at because of what they mean and imply, but simply for the fact that they mean at all. They are familiar lines within an ambiguous lyrical world that refuses to be reduced to a discernable, single interpretation. The catchphrases are familiar lines within a familiar yet, to the rational mind, unyielding lyrical world. \u0022Scarlet Begonias\u0022 refuses to be understood as a whole, so the audience jumps at those four lines that seem to make sense. The catchphrases, then, are to the rest of the lyrics what the melody is to the improvisational jam--they mark the return to an understandable, well-known, and familiar touchstone before the next journey into newness and obscurity. This phenomenon can probably be best understood when looking at what is generally considered the prime example of the Grateful Dead\u0027s efforts, \u0022Dark Star\u0022: \u0022 \u0022Dark Star\u0022 is the kernel of wide open possibility at the core of the Dead\u0027s repertoire, the essential seed promising unlimited intergalactic space journeys at the speed of total mindwarp: \u0022Shall we go,/ you and I/while we can?\u0022 It is the most exploratory of Dead tunes and it is the trippiest, the one where the acidic whistle of the dark interstices is heard most starkly, where you might turn any corner and step off into the void. It is the Dead\u0027s spirit of musical adventure at its strangest, wildest, most vehement, weirdest, dreamiest, and the place where the leading wave of the Dead\u0027s music is most often audible being created in mid-air.\u0022 (Jim Powell quoted in Shenk 51) Interestingly, \u0022Dark Star\u0022 was the first song that Hunter wrote together with the Grateful Dead in 1967. The lyrics are extremely short, considering that a full-fledged \u0022Dark Star\u0022 can easily last half an hour. The song is made up by two verses that are usually separated by minutes and minutes of intense jamming. A close reading reveals Hunter\u0027s technique of juxtaposing the abstract and the concrete in a single image, parataxis, and contradiction in full bloom. \u0022Searchlight casting / for faults in the / clouds of delusion\u0022 (7-9) is such a tricky image, suggestion that the speaker has a very clear sense of what he is talking about, yet again, the song cannot be said to \u0022mean\u0022 anything in any traditional sense, and not even the tedium of a symbolic reading will yield any \u0022meaning.\u0022 However, the verse beginning \u0022Shall we go...\u0022 which precedes the next drawn out jam insinuates that the speaker understands, and invites the reader along through the \u0022transitive nightfall / of diamonds\u0022 (14-15). The drawn-out jam that usually follows can be understood as exactly the \u0022transitive nightfall / of diamonds,\u0022 the lyrics then talking about the music, a kind of meta-lyric. After seemingly endless improvisation, the text and melody returns with \u0022Mirror shatters\u0022 (16), an event cheered by the audience as it means the return to a familiar signpost. It has been argued that \u0022Dark Star\u0022 draws its power from the violation of norms and the constant impossibility of assigning meaning: \u0022 [\u0027Dark Star\u0027 leads] to a transcendent mysticism because the music, continually violating norms and expectations, avoid a clear marking or denotation of a signified. What is left is a state of ambiguity in which the listener is enticed into closer attention of momentary detail.\u0022 (Skaggs, quoted in Shenk 64) I would argue that the overwhelming power of \u0022Dark Star\u0022 and, in a broader sense, the overall effect of a Grateful Dead show, does not entirely rely on the elusiveness of its signs, be it musically or linguistically, but on the tension it creates between the familiar and the new. The Deadhead community as a whole exists because of the familiar aspects of the Dead. The fascination of the band grows largely out of the relationships between the improvised and the structured that are increasingly complex to the point of becoming fractal. The model that I would suggest for the intricacies of a Grateful Dead show and the rich relationships that exist between the lyrics and the music is that of fractals, self-containing geometrical structures like the Mandelbrot set. The outside of the Gingerbread man are the ordered regions, where a number can easily be assigned to the function, z (n\u002B1)=z(n)2\u002Bc, while the black inside of the Gingerbread man stands for total, absolute chaos, unstructuredness. The edge where those two areas meet is characterized by intense, bizarre beauty and recurring images of the bigger picture that recede--like a Russian doll, the Gingerbread man contains images of himself, infinitely, just as a Grateful Dead concert contains recurring patterns of familiarity and innovation on a receding scale. The overall structure of the concert is firm, or almost firm--there are always exceptions, yet the pattern of first and second set, drums, space, and encore are set. This structure is filled with actual songs every night; the choice of songs varies, yet the songs are generally well-known and familiar. The way they are going to be played on any given night, however, is uncertain and actually unpredictable--no improvisational jam that opens in preconceived places within the song will be the same, yet it can be assumed it will be at a given place. The lyrics, joining the return to the melody, are firm and part of a preconceived pattern within a song. They will not be changed or tampered with. However, the firm and unchanging lyrics have shown themselves to be unsteady and undecidable due to their ambiguous and contradictory character. This is only true, again, for the most part, since the lyrics do contain embedded catchphrases that possess a heightened familiarity within the song. The audience will recognize and appreciate these moments of predictability and profound understanding as a community before the jam wanders off into unknown directions again, possibly into the far reaches of musical exploration in \u0022Dark Star\u0022 or to the completely strange and wordless \u0022Space,\u0022 which can be considered the climax of unfamiliarity within the development of the concert. After the purely instrumental excesses of \u0022Space,\u0022 the return of rhythm, harmony, melody, then lyrics, and finally understandable and rational lyrics is greeted by enthusiastically by the audience which is pleased to be gently returned to a sensical universe where exploration and rediscovery, departure and return, ambiguity and blatantness co-exist and fuel each other. Hunter\u0027s lyrics are allowed to be reasonably narrative and plain-old weird at given times; the music always corresponds to or counter-accentuates the words, both live off each other in an organic and healthy way. Standing on their own, the lyrics to \u0022Dark Star\u0022 might seem interesting and lyrical yet pointless; in union with the music, they gather as many meanings as the listener can assign to them: [Jerry Garcia:] \u0022Dark Star\u0022 has meant, while I\u0027m playing it, almost as many things as I can sit here and imagine, so all I can do is talk about \u0027Dark Star\u0027 as a playing experience. [Charles] REICH: Well, yeah, talk about it a little. [Jerry Garcia:] I can\u0027t. It talks about itself. [Charles] REICH: Each time it comes out in a different way? [Jerry Garcia:] Yeah, pretty much. There are certain structural poles which we have kind of set up in it, and those periodically we do away with. (Garcia 85) The music of the Grateful Dead and the lyrics of Robert Hunter open up spaces within the work of art they form together that make textual interpretation of the lyrics futile. The lyrics function not as an absolute that is stated, but as a tool for semiosis--at every concert, they can and do mean something different according to how the variable parts in the music change. Every time a song is performed, a different set of potential meanings that are inherent in the piece will be actualized for a different audience. Various parts of the concert experience are firm, others are variable, and as I tried to illustrate using the Mandelbrot set, the firm parts contain further movable aspects, and vice versa. The resulting multi-media, multi-sign system artifact can shine in infinite variations. The result is one of an ever-changing, fluctuating whole that contains familiar parts while never completely yielding to reason. This amorphous, continuously self- altering gestalt that is a thirty-year process --the seams show, there are parts that are completely opaque and others that we have known for decades, and it always changes: \u0022It stumbles, then it creeps, then it flies with one wing and then it flies with one wing and bumps into trees and shit\u0022 (Garcia 76). Deadhead sociologist Rebecca Adams puts it this way: \u0022People say Deadheads are throwbacks. I think they are pioneers. They recognize that reality is subjective--there is no right way--and have been cognizant of these multiple realities for a lot longer than most other people. This is post-modernism. It\u0027s the cutting edge\u0022 (quoted in Shenk xiii). Robert Hunter\u0027s lyrics are a vital part of this experience, providing the mind-searching music with an incessantly fascinating imagery that both underscores and counter-points its meaning by not meaning anything at all. Works Cited Barthelme, Donald. \u0022How I Write My Songs.\u0022 Sixty Stories. New York: Putnam, 1981. 418-423. Bralove, Bob. Liner Notes. Grateful Dead. Infrared Roses. Grateful Dead Records, 40149-2, 1991. Flanagan, Bill. Written in My Soul: Rock\u0027s Greatest Songwriters Talk About Creating Their Music. Chicago: Contemporary, 1986. Fischer-Lichte, Erika. The Semiotics of Theater. Indianapolis: Indiana UP, 1992. Gans, David. Conversations with the Dead: The Grateful Dead Interview Book. New York: Citadel 1991. Garcia, Jerry, Charles Reich, and Jann Wenner. Garcia: A Signpost to New Space. San Francisco: Straight Arrow, 1972. Hunter, Robert. A Box of Rain: Lyrics 1965-1994. New York: Penguin, 1993. Scott, John W., Mike Dolgushkin, and Stu Nixon, eds. DeadBase VII: The Complete Guide to Grateful Dead Song Lists. Hanover, NH: 1993. Lee, Martin A. and Bruce Shlain. Acid Dreams: The Complete Social History of LSD: The CIA, the Sixties, and Beyond. New York: Grove Weidenfeld, 1992. Lennon, John and Paul McCartney. \u0022Lucy in the Sky With Diamonds.\u0022 The Beatles. Sgt. Pepper\u0027s Lonely Hearts Club Band. EMI, CDP 7464422, 1967. Shenk, David and Steve Silberman. Skeleton Key: A Dictionary for Deadheads. New York: Bantam, 1994. Skaggs, S. \u0022The Transitive Nightfall of Diamonds: Eco and the Aesthetics of Transcendence.\u0022 University of Louisville, Kentucky. Presented at the Conference of the Semiotic Society of America, St. Louis, Missouri, 1993. \u0060\u0060\u0060 ## Fractures of Unfamiliarity \u0026 Circumvention in Pursuit of a Nice Time \u0060\u0060\u0060text Dear Jurgen, meaning is not an irreducible Ur-language. A good lyric is allusion, illusion, subterfuge and collusion. A poor lyric is information about its own paucity of resource. That doesn\u0027t mean the latter cannot be a great song. There is nothing inherently better about a dumb song than one which calls attention to the intelligence of its writer. It\u0027s a matter of taste, but meaning is often a subterfuge to distract the listener\u0027s attention from a writer\u0027s lack of multiple resources. This is often true of blatant \u0022message\u0022 songs. How does a song mean? As long as allusions can be codified, the semiotician is content, knowing that \u0022meaning\u0022 is a case-sensitive term with scant referentiality outside implementation of primitive needs. When the semiotician suspects allusiveness without corresponding exact reference, he charges the poet with nonsense. Nonsense is a loaded word, the meaning of which is unclear. If it is understood as \u0022intentional multi-referentiality without predetermined hierarchy\u0022 rather than \u0022meaningless blather\u0022 one would find no fault with the term. But it isn\u0027t, so the charge of \u0022nonsense\u0022 and \u0022meaninglessness\u0022 levied by a scholarly and plausible source, does much to put people off exploring further. \u0022The repair sad mutthead forkbender orange in the how are you, did they?\u0022 is nonsense without reference, hence not allusive. Had it discernible rhythm, it might be termed \u0027rhythm allusive\u0027 but my example lacks even that. The only way on earth such a sentence would likely get written is as an example of a null allusive set. Bingo! An allusion! To Set Theory! The exception that proves the rule. How does a song mean? The meaning(s), or lack thereof, ascribed by others to an example of lyric work are not part of the work. The interpretations are separate \u0022works\u0022. The manner in which an audience receives the work, what they, collectively and individually, make of it, can indeed provide potential data for the allusiveness (referentiality)of future lyrics, gainsaid, but cannot be ascribed as a characteristic of the particular work, per se, with validity without \u0022insider information\u0022 which is, in any case, no part of the song. That way lies true nonsense, even unto deconstruction. Yet the little bugger of a jingle persists and seems to move hearts. Why? Is there something which semiotics, by its nature and presuppositions, must exclude from the sphere of \u0022meaningfulness\u0022 due to the limited nature of its tools? How does semiotics mean? Since the concluding remark of your essay stated that the Grateful Dead songs are \u0022meaningless\u0022 I choose to reply by explicating one of your examples: \u0022Franklin\u0027s Tower.\u0022 I do this reluctantly because I feel that a straightforward statement of my original intent robs the listener of personal associations and replaces them with my own. I may know where they come from, but I don\u0027t know where they\u0027ve been. My allusions are, admittedly, often not immediately accessible to those whose literary resources are broadly different than my own, but I wouldn\u0027t want my listeners\u0027 trust to be shaken by an acceptance of the category \u0022meaningless\u0022 attached to a bundle of justified signifiers whose sources happen to escape the scope of simplistic reference. How does the song go? FRANKLIN\u0027S TOWER In another time\u0027s forgotten space your eyes looked through your mother\u0027s face Wildflower seed on the sand and stone may the four winds blow you safely home 4x: Roll away... the dew You ask me where the four winds dwell In Franklin\u0027s tower there hangs a bell It can ring, turn night to day Ring like fire when you lose your way Roll away... the dew . . . God help the child who rings that bell It may have one good ring left, you can\u0027t tell One watch by night, one watch by day If you get confused just listen to the music play Roll away... the dew . . . Some come to laugh their past away Some come to make it just one more day Whichever way your pleasure tends if you plant ice, you\u0027re gonna harvest wind Roll away... the dew . . . In Franklin\u0027s Tower the four winds sleep Like four lean hounds the lighthouse keep Wildflower seed in the sand and wind May the four winds blow you home again 4x: Roll away... the dew You better roll away the dew --------------------------------- What\u0027s it about? EXPLICATION: In another time\u0027s forgotten space your eyes looked through your mother\u0027s face Wildflower seed on the sand and stone may the four winds blow you safely home. [You have your mother\u0027s eyes, child, the very shape, color and intensity of the eyes that looked through her face so long ago. Borne on the varied winds of chance and change, like a dandelion seed, you may find yourself deposited on barren soil. My wish for you is that the forces that brought you there may sweep you up again and bear you to fertile ground.] \u0022In another time\u0027s forgotten space your eyes looked through your mother\u0027s face.\u0022 [Relative immortality of the human species is realized through reproduction. Dominant traits inherited from an ancestor, the lyric suggests, share more than mere similarity with those of the forebear, but are an identity, endlessly reproducible. In other words, when someone says \u0022You have your mother\u0027s eyes\u0022 they are not speaking in simile nor would it be incorrect to say that \u0022your mother has your eyes,\u0022 if, in fact, possessiveness is an appropriate term in the context. Poetic license will assume it is, if only for the sake of moving on to the next couplet.] You ask me where the four winds dwell In Franklin\u0027s tower there hangs a bell It can ring, turn night to day Ring like fire when you lose your way [note that this song appeared in 1975, the year after my son was born and the year before the American Bicentennial. Both facts are entirely relevant. The allusion to the Liberty Bell and the situation of the Philadelphia Congress in the hometown of Ben Franklin has not gone unnoticed by other commentators. This song is a birthday wish both for my son and for my country, each young and subject to the winds of vicissitude. Individual and collective freedom, liberty, conscience, all that is conjured by those concepts, is suggested in the image of the tolling bell.] God help the child who rings that bell It may have one good ring left, you can\u0027t tell One watch by night, one watch by day If you get confused just listen to the music play [The Bell, rung once, cracked and could not be safely rung again. From an actual bell, it therefore became a symbol of the potential to ring. The single toll, signaling birth, can now be heard only in its reverberations in our history and ideals. Some have had to bear those ideals in difficult circumstances (war, the Great Depression and general benightedness) others have had the more enviable task of keeping watch (eternal vigilance) during periods of conscious and dynamic change: the full light of day. The sixties, the writer assumes, were such a time. You can\u0027t tell if ringing that bell a second time would destroy it in the act of producing another mighty peal and it might be foolish, if courageous, to try. Perhaps the \u0022music\u0022of the original ideals symbolized by the first and only toll should be taken to heart and implemented, rather than obviated by a new source of ideation (communism, anarchy, religion based governmental apparatus. etc.) To resolve this confusion, pay attention to the original inspiration (the Constitution, the Bill of Rights, collectively. Individually, maintain awareness of conscience and one\u0027s own early ideals.] Some come to laugh their past away Some come to make it just one more day Whichever way your pleasure tends if you plant ice, you\u0027re gonna harvest wind [This verse scarcely needs commentary in light of the above remarks. The precursor to the 1st couplet is \u0022I Come for to Sing\u0022 as performed, possibly written, by Pete Seegar. The 2cnd couplet source is the biblical \u0022Who sows wind reaps the hurricane.\u0022] In Franklin\u0027s Tower the four winds sleep Like four lean hounds the lighthouse keep Wildflower seed in the sand and wind May the four winds blow you home again [We assume a bell tower for the great bell. By the trope of simile, we see the bell tower (the day watch) turned to a lighthouse and the four winds become sleeping hounds, (the night watch) worn out by the events of such a metaphorical day as related by e.e.cummings in his familiar lyric \u0022All in Green Went My Love Riding\u0022 (Poem 6 from TULIPS AND CHIMNEYS) \u0022four lean hounds crouched low and smiling . . .\u0022 By the use of quotative allusion the lyric attempts to borrow some of the emotive spark of cumming\u0027s poem, providing a kind of \u0022link button\u0022 into a different but complementary space. Allusion here functions as a sort of shorthand cross-patch into a series of metaphoric events which, with a double-clutch shift of simile, access a downloadable description of the kind of day it\u0027s been for a \u0027wildflower seed\u0027 in its adventures in the wind. There may be some objection to the elastic interchangeability of the similes of hounds and winds in this set of couplets, but the test of the allusion, as I see it, is whether or not the appropriate emotions are evoked to lead to satisfying closure and an opening door on other possibilities.] [Now to the real stretch: \u0022Roll away the dew.\u0022 The line is appropriated from a fairly well known sea chantey whose chorus goes: \u0022Roll away the morning dew and sweet the winds shall blow.\u0022 As surely everyone knows by now, Bonnie Dobson\u0027s song \u0022Morning Dew\u0022 (made famous by Garcia\u0027s singing of it) is set in the aftermath of nuclear war. Reason he can\u0027t \u0022walk you out in the morning dew, my honey\u0022 is because of fallout, though Garcia has wisely dropped the verse containing this denouement, allowing the song a heightened romantic mystery, achieved through open-ended ambiguity. For generations now alive, the nuclear specter personifies the forces which most threaten our attempt at Jeffersonian democracy. With the song\u0027s sub-allusion to \u0022Roll away the Stone,\u0022 an anthem of joyous Eastertide resurrection, a resultant combination message of dire necessity (as in the final: you\u0027ve got to roll away the dew) and promise of renewal, in case resolution is effected, are enjoined. Should this hyper-allusive train of thought become too confusing to process, the invitation to just \u0022listen to the music play\u0022 acknowledges both the melody and performance context of the lyric and the metaphoric bell described above.] Well, now that you know what I meant by it, it\u0027s no great shakes is it? Mystery gone, the magician\u0027s trick told, the gluttony for \u0022meaning\u0022 temporarily satisfied, one can now take issue with my intent and avoid the song itself, substituting the assignable significance for the music. Attempts by language to overdetermine language are doomed out the door, so I content myself with providing these clues for threading the maze of \u0022Franklin\u0027s Tower\u0022 and as a grudging key to my methods. I feel that much of what you\u0027ve said in your essay is rich, correct and thought provoking and appreciate your accurate estimation of the concert context as adjunct to the lyric, and vice versa. The contextual sub-meaning (the way the song manifests itself in concert) is certainly a factor which occasionally determines certain choices in subsequent material. Too much of that would be a striving after sameness of effect, though (even if it does all sound the same to an uninitiated ear.) Oh, one other thing: you labeled \u0022what a long, strange trip it\u0027s been\u0022 as \u0022cliched.\u0022 Aren\u0027t you putting the cart before the horse? \u0022Truckin\u0027\u0022 was the originating vehicle of the phrase, which had not, to my knowledge, been coined before. The fact that it has entered the catchphrase banks of the language in a ubiquitous way may render subsequent usage cliched, but surely not the invention itself, unless all widely adopted phrases are deemed trite by virtue of their durability. You also mentioned that the \u0022What in the world ever became of Sweet Jane?/She\u0027s lost her sparkle, you know she isn\u0027t the same\u0022 verse was probably an in-joke not meant for a broad audience to grasp. The intention was a parody of the \u002740\u0027s warning-style of singing commercial, specifically \u0022Poor Millicent, poor Millicent/ She ne-ver used Pep-so-dent/ Her smile grew dim/ And she lost her vim / So folks don\u0027t be like Millicent / Use Pep-so-dent! \u0022 I\u0027m sure that the allusiveness, not that entirely outre in the \u002760\u0027s, is well lost here in the \u002790\u0027s. So, it\u0027s perhaps an in-joke, but not one meant for private consumption. Just a bit of black humor that fails to fire and emerges, instead, as an enigma. I guess the question here is whether an allusion must be blatantly perceivable as such in order to avoid the uncharitable label of \u0022nonsense.\u0022 Thank you for taking my work seriously enough to spend considerable effort in explicating it according to your lights. Robert Hunter 3/4/96 p.s. Disorient slow quill to provide Dense unsingable margin Dispersing in tonal cluster under A lilac roof of chintz Vermeers Incorporate chance nomenclature Corresponding to a netting of wind Revolving in unintentional resolve Interrogating with no question mark Non-circular limited clock of the day Before rude lingual enforcement Immerses momentos of dux redux More given to flux than flight in Shank\u0027s aviary Where no one spits in the beer of slumber With less reluctance than an attested breeze (from the Collected Sonnets of Cosmo Nitram, soon to appear on [Hunter\u0027s] pages) \u0060\u0060\u0060","contentType":"posts","url":"https://www.lqdev.me/posts/fractures-unfamiliarity-circumvention-pursuit-good-time-robert-hunter","date":"2025-02-26 19:44 -05:00","tags":["music","essay","gratefuldead"],"summary":"The past few days, I\u0027ve had the song Franklin\u0027s Tower by The Grateful Dead on repeat. I found some of the lyrics could be taken at face value but others didn\u0027t make sense to me. Since I assumed there...","keywords":["lyrics","song","dead","grateful","music","they","hunter","band","songs","meaning"]},{"id":"reshare-inevitable-jcole-blog","title":"The Algorithm - J. Cole has a blog?","content":"The Algorithm - J. Cole has a blog? \u2192 https://www.inevitable.live/algorithm/first-post This is just a place for me to share. I been wanting a lil blog for years. Somewhere to post random shit I fuck with where the audience is way smaller than it is on the social media platforms. Finally pulled the trigger, bare with us as we still developing this page and the layout. This is cool! I ran into this while reading the post Blogging for smaller audiences and deeper connections by Herbert Lui . Looking at other posts , it looks like he or at least someone is actively posting.","contentType":"reshare","url":"https://www.lqdev.me/responses/inevitable-jcole-blog","date":"2025-02-25 21:34 -05:00","tags":["music","blog","indieweb","creativity","art"],"summary":"The Algorithm - J. Cole has a blog? \u2192 https://www.inevitable.live/algorithm/first-post This is just a place for me to share. I been wanting a lil blog for years. Somewhere to post random shit I fuck...","keywords":["post","blog","smaller","algorithm","developing","page","layout","cool","while","reading"]},{"id":"star-your-site-is-a-home-hamid","title":"Your Site Is a Home","content":"Your Site Is a Home \u2192 https://nazhamid.com/journal/your-site-is-a-home/ While going through a backlog of personal blogs I haven\u0027t read in a few weeks, I\u0027m noticing a trend in posts that express similar sentiments to those in Naz\u0027s post. I\u0027m happy to see more people choosing to have a place that gives them more autonomy and freedom to express themselves in whichever form makes the most sense for them. Your site is a home. Eventually, social networks were created...novelty and the promise of interconnectedness by gathering in a common town square to blast out whatever was going on in our lives eventually won out. But you might have still had your website, your home to return to...the place you built. Things started to change and instead of going home, you, and everybody else started to live in the town square. There is a shape to your physical home. Arranged and organized in the ways that make sense for you, the reward is a space that works for you. A platform or network doesn\u2019t allow for much configuration. The town square isn\u2019t owned by you. People became absolutely reliant on the same gathering place...the convenience of seeing friends (sometimes) outweighed your other neighbors spouting garbage and hate. You came to rely on this place for everything. You can still have a home. I have very little at the town square, because it\u2019s not a public one. It\u2019s a walled-off town square, whose rules and borders change at the whims of those who created it. Thanks for visiting my home. I\u2019m glad you dropped by. I\u2019d love to see yours sometime.","contentType":"star","url":"https://www.lqdev.me/responses/your-site-is-a-home-hamid","date":"2025-02-25 20:49 -05:00","tags":["personalweb","indieweb","socialmedia"],"summary":"Your Site Is a Home \u2192 https://nazhamid.com/journal/your-site-is-a-home/ While going through a backlog of personal blogs I haven\u0027t read in a few weeks, I\u0027m noticing a trend in posts that express...","keywords":["home","square","town","place","site","going","it\u2019s","change","gathering","created"]},{"id":"reshare-multistream-azure-ffmpeg-obs-studio-maho","title":"How to Multistream Using Azure, Azure VM, MonaServer 2, and FFmpeg with OBS Studio","content":"How to Multistream Using Azure, Azure VM, MonaServer 2, and FFmpeg with OBS Studio \u2192 https://maho.dev/2025/02/how-to-multistream-using-azure-azure-vm-monaserver-2-and-ffmpeg-with-obs-studio/ In this guide, we will set up a multistreaming workflow using Azure, an Azure VM, MonaServer 2, and FFmpeg to broadcast to multiple platforms like LinkedIn Live and YouTube Live. We\u2019ll use OBS Studio as the main streaming software. Let\u2019s get started. Love this detailed writeup from Maho . When I originally experimented with something similar using my Owncast instance, I was running ffmpeg locally but I like his solution better.","contentType":"reshare","url":"https://www.lqdev.me/responses/multistream-azure-ffmpeg-obs-studio-maho","date":"2025-02-25 20:33 -05:00","tags":["livestream","azure","obs"],"summary":"How to Multistream Using Azure, Azure VM, MonaServer 2, and FFmpeg with OBS Studio \u2192 https://maho.dev/2025/02/how-to-multistream-using-azure-azure-vm-monaserver-2-and-ffmpeg-with-obs-studio/ In this...","keywords":["azure","using","ffmpeg","monaserver","multistream","studio","live","love","detailed","writeup"]},{"id":"reshare-unplatform-indieweb","title":"Unplatform","content":"Unplatform \u2192 https://unplatform.fromthesuperhighway.com/ Unplatform is an interactive guidebook, online library, and recommendations database intended to help you escape social media and join the indie web. Love this guide.","contentType":"reshare","url":"https://www.lqdev.me/responses/unplatform-indieweb","date":"2025-02-25 20:20 -05:00","tags":["indieweb","socialmedia","personalweb"],"summary":"Unplatform \u2192 https://unplatform.fromthesuperhighway.com/ Unplatform is an interactive guidebook, online library, and recommendations database intended to help you escape social media and join the...","keywords":["unplatform","indie","join","media","social","escape","help","intended","love","database"]},{"id":"bookmarks-s1-simple-test-time-scaling","title":"s1: Simple test-time scaling","content":"s1: Simple test-time scaling \u2192 https://arxiv.org/abs/2501.19393 Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI\u0027s o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model\u0027s thinking process or lengthening it by appending \u0026quot;Wait\u0026quot; multiple times to the model\u0027s generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at this https URL","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/s1-simple-test-time-scaling","date":"2025-02-24 20:54 -05:00","tags":["ai","research","s1"],"summary":"s1: Simple test-time scaling \u2192 https://arxiv.org/abs/2501.19393 Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently,...","keywords":["model","test","scaling","performance","budget","reasoning","forcing","aime24","compute","questions"]},{"id":"reshare-nomic-embed-text-v2","title":"Nomic Embed Text V2: An Open Source, Multilingual, Mixture-of-Experts Embedding Model","content":"Nomic Embed Text V2: An Open Source, Multilingual, Mixture-of-Experts Embedding Model \u2192 https://www.nomic.ai/blog/posts/nomic-embed-text-v2 Today we\u0027re excited to announce Nomic Embed Text V2, our next-generation embedding model that brings the Mixture of Experts (MoE) architecture to text embeddings on a new expanded multilingual training dataset. Personally, I found the part on MoE to be the most interesting about this release. Rather than a dense model which uses all parameters on an input, the MoE architecture dynamically routes to different \u0026quot;experts\u0026quot; - sparse subsets of parameters at each layer - activating, ideally, only the parameters especially needed to process the input. This approach allows for more efficient use of compute when generating embeddings. In our experiments, we found that alternating MoE layers with 8 experts and top-2 routing provides the optimal balance between performance and efficiency. This results in 475M total parameters in the model, but only 305M active during training and inference. Research into embedding model architecture has significant practical implications for working with text embeddings in production: Lower latency for high-volume applications of embeddings like retrieval Reduced deployment costs through more efficient parameter usage More accessibility to embeddings in settings with constrained compute","contentType":"reshare","url":"https://www.lqdev.me/responses/nomic-embed-text-v2","date":"2025-02-24 20:47 -05:00","tags":["nomic","ai","embeddings"],"summary":"Nomic Embed Text V2: An Open Source, Multilingual, Mixture-of-Experts Embedding Model \u2192 https://www.nomic.ai/blog/posts/nomic-embed-text-v2 Today we\u0027re excited to announce Nomic Embed Text V2, our...","keywords":["text","model","embeddings","parameters","nomic","embed","more","experts","embedding","architecture"]},{"id":"reshare-anthropic-claude-3-7-sonnet-code","title":"Claude 3.7 Sonnet and Claude Code","content":"Claude 3.7 Sonnet and Claude Code \u2192 https://www.anthropic.com/news/claude-3-7-sonnet Today, we\u2019re announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model can think for. Claude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development. Along with the model, we\u2019re also introducing a command line tool for agentic coding, Claude Code. Claude Code is available as a limited research preview, and enables developers to delegate substantial engineering tasks to Claude directly from their terminal. Claude 3.7 Sonnet and Claude Code mark an important step towards AI systems that can truly augment human capabilities. With their ability to reason deeply, work autonomously, and collaborate effectively, they bring us closer to a future where AI enriches and expands what humans can achieve.","contentType":"reshare","url":"https://www.lqdev.me/responses/anthropic-claude-3-7-sonnet-code","date":"2025-02-24 20:43 -05:00","tags":["ai","anthropic","claude"],"summary":"Claude 3.7 Sonnet and Claude Code \u2192 https://www.anthropic.com/news/claude-3-7-sonnet Today, we\u2019re announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning...","keywords":["claude","sonnet","code","model","step","we\u2019re","also","coding","future","terminal"]},{"id":"reviews-meditative-mind-goleman","title":"The Meditative Mind: The Varieties of Meditative Experience","content":"The Meditative Mind: The Varieties of Meditative Experience by Daniel Goleman Read Rating: 4.1/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/meditative-mind-goleman","date":"2025-02-24 20:42 -05:00","tags":[],"summary":"The Meditative Mind: The Varieties of Meditative Experience by Daniel Goleman Read Rating: 4.1/5","keywords":["meditative","mind:","varieties","experience","daniel","goleman","read","rating:"]},{"id":"reshare-ultrascale-playbook-training-llms-gpu-clusters","title":"The Ultra-Scale Playbook: Training LLMs on GPU Clusters","content":"The Ultra-Scale Playbook: Training LLMs on GPU Clusters \u2192 https://huggingface.co/spaces/nanotron/ultrascale-playbook Gold. All the techniques we\u0027ll cover in this book tackle one or several of the following three key challenges, which we\u0027ll keep bumping into throughout the book: 1. **Memory Usage**: it\u0027s a hard limitation - if a training step doesn\u0027t fit in memory, training cannot proceed 2. **Compute Efficiency**: we want our hardware to spend most time computing, so we need to reduce time spent on data transfers or waiting for other GPUs to perform work. 3. **Communication overhead**: we want to minimize communication overhead as it keeps GPUs idle. To archieve this we will try to make best use of intra-node (fast) and inter-node (slower) bandwidths as well as overlap communication with compute as much as possible. In many places we\u0027ll see that we can trade one of these (computation, communication, memory) for another (e.g. recomputation or Tensor Parallelism). Finding the right balance is key to scaling training.","contentType":"reshare","url":"https://www.lqdev.me/responses/ultrascale-playbook-training-llms-gpu-clusters","date":"2025-02-19 22:03 -05:00","tags":["ai","llm","training"],"summary":"The Ultra-Scale Playbook: Training LLMs on GPU Clusters \u2192 https://huggingface.co/spaces/nanotron/ultrascale-playbook Gold. All the techniques we\u0027ll cover in this book tackle one or several of the...","keywords":["training","communication","node","gpus","memory","want","waiting","transfers","perform","**communication"]},{"id":"star-google-dream-job-2025-sb-ad","title":"Dream Job - Google Super Bowl 2025 Ad","content":"Dream Job - Google Super Bowl 2025 Ad \u2192 https://www.youtube.com/watch?v=-7e6g11BJc0 I didn\u0027t watch the Super Bowl this year so I didn\u0027t get a chance to watch the ads until a few days later. From the ones I saw, I think Google\u0027s was my favorite. Apart from the touching story, what stood out to me the most was its focus on humanity, with technology playing a supporting role rather than being the main character.","contentType":"star","url":"https://www.lqdev.me/responses/google-dream-job-2025-sb-ad","date":"2025-02-17 16:00 -05:00","tags":["google","ad","superbowl","ai"],"summary":"Dream Job - Google Super Bowl 2025 Ad \u2192 https://www.youtube.com/watch?v=-7e6g11BJc0 I didn\u0027t watch the Super Bowl this year so I didn\u0027t get a chance to watch the ads until a few days later. From the...","keywords":["google","super","bowl","from","didn","watch","dream","touching","story","stood"]},{"id":"notes-eggpocalypse-feb-2025","title":"Eggpocalypse 2025","content":"It\u0027s rough out there.","contentType":"notes","url":"https://www.lqdev.me/notes/eggpocalypse-feb-2025","date":"2025-02-17 15:57 -05:00","tags":["costco","eggs"],"summary":"It\u0027s rough out there.","keywords":["rough"]},{"id":"reviews-high-priest-leary","title":"High Priest","content":"High Priest by Timothy Leary Read Rating: 4.0/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/high-priest-leary","date":"2025-02-16 20:50 -05:00","tags":[],"summary":"High Priest by Timothy Leary Read Rating: 4.0/5","keywords":["high","priest","timothy","leary","read","rating:"]},{"id":"reshare-dotnet-language-runtime-standup-tensors","title":"Languages \u0026 Runtime Community Standup - Tensors in .NET","content":"Languages \u0026amp; Runtime Community Standup - Tensors in .NET \u2192 https://www.youtube.com/watch?v=VOEeNffChSg Great session on Tensors in .NET by Tanner Tensors are the foundational data structure powering man AI workloads today. Introduced in .NET 9, Tensors builds on top of earlier work like TensorPrimitives , hardware intrinsics , and generic math . You can check out the recording below to learn more.","contentType":"reshare","url":"https://www.lqdev.me/responses/dotnet-language-runtime-standup-tensors","date":"2025-02-15 15:12 -05:00","tags":["ai","tensor","dotnet","runtime"],"summary":"Languages \u0026amp; Runtime Community Standup - Tensors in .NET \u2192 https://www.youtube.com/watch?v=VOEeNffChSg Great session on Tensors in .NET by Tanner Tensors are the foundational data structure...","keywords":["tensors","languages","below","recording","check","math","generic","intrinsics","hardware","tensorprimitives"]},{"id":"reshare-generative-ai-for-beginners-dotnet-course","title":"Generative AI for Beginners (.NET) is now available","content":"Generative AI for Beginners (.NET) is now available \u2192 https://devblogs.microsoft.com/dotnet/announcing-generative-ai-for-beginners-dotnet/ If you\u0027re a .NET developer looking to get started with Generative AI, there\u0027s a new course for beginners that just launched. Through a series of self-paced lessons, you\u0027ll learn the foundations to help you start using AI to augment the capabilities of your .NET applications. Check out the course and give us feedback!","contentType":"reshare","url":"https://www.lqdev.me/responses/generative-ai-for-beginners-dotnet-course","date":"2025-02-15 12:14 -05:00","tags":["ai","dotnet","course","generativeai"],"summary":"Generative AI for Beginners (.NET) is now available \u2192 https://devblogs.microsoft.com/dotnet/announcing-generative-ai-for-beginners-dotnet/ If you\u0027re a .NET developer looking to get started with...","keywords":["generative","beginners","course","check","applications","capabilities","augment","using","start","help"]},{"id":"reshare-ai-dev-gallery-windows-preview-ms-store","title":"AI Dev Gallery now in the Microsoft Store","content":"AI Dev Gallery now in the Microsoft Store \u2192 https://apps.microsoft.com/detail/9n9pn1mm3bd5 So excited to see the AI Dev Gallery is now in the Microsoft Store . Through various interactive samples, developers get to see how they might use AI for different tasks like text classification, object detection, and many others. Best of all, the models are all running locally and you get to see and export the C# code powering the samples to Visual Studio so you can continue tinkering on your own and integrate into your own applications. A few months ago, the team came on the .NET AI Community Standup to showcase the app. Since then, it\u0027s only kept improving and introducing new scenarios. You can check out the recording from that stream here.","contentType":"reshare","url":"https://www.lqdev.me/responses/ai-dev-gallery-windows-preview-ms-store","date":"2025-02-15 12:02 -05:00","tags":["ai","windows","dotnet","microsoft","onnx"],"summary":"AI Dev Gallery now in the Microsoft Store \u2192 https://apps.microsoft.com/detail/9n9pn1mm3bd5 So excited to see the AI Dev Gallery is now in the Microsoft Store . Through various interactive samples,...","keywords":["microsoft","gallery","store","samples","continue","tinkering","integrate","applications","months","team"]},{"id":"posts-llama-turn-on-tuning-in-ai-quest-higher-consciousness-moo","title":"Llama\u0027s Turn On: Tuning In to AI\u0027s Quest for Higher Consciousness in MOOs","content":"Last week, I learned what a [MOO](https://en.wikipedia.org/wiki/MOO) was. Although they were before my time, I decided to use Llama 3.3 in [DuckDuckGo\u0027s AI Chat](https://duck.ai/) to play around. At first it was very generic but eventually it started going down some fun and interesting paths. Here\u0027s the transcript with some updates for formatting. In some turns, I took creative liberties but mostly stuck to the script. \u003E **User** \u003E Text adventure game (MOO) \u003E **Assistant** \u003E You find yourself standing in a dimly lit room, the walls made of cold, grey stone. The air is musty and filled with the scent of old dust. A faint glow emanates from a torch on the wall, casting flickering shadows around the room. \u003E \u003E You see a door to the north, slightly ajar, and a passageway to the east, which appears to lead deeper into the darkness. There is also a small table in the center of the room, with a piece of parchment and a quill lying on it. \u003E \u003E What do you want to do? \u003E \u003E You can type commands such as: \u003E - go north \u003E - go east \u003E - examine table \u003E - take parchment \u003E - open door \u003E \u003E What is your action? \u003E \u003E **User** \u003E Go east \u003E **Assistant** \u003E As you head east, the passageway stretches out before you, the darkness seeming to swallow you whole. The air grows colder and the smell of damp earth fills your nostrils. You can hear the sound of dripping water echoing off the walls, and the faint rustle of unknown creatures in the distance. \u003E \u003E The passageway begins to slope downward, leading you deeper into the earth. You stumble slightly on the uneven ground, but manage to catch yourself before falling. \u003E \u003E As you round a bend in the passageway, you see a faint light in the distance. It\u0027s hard to tell what it is, but it looks like it might be a room or a chamber of some kind. \u003E \u003E You continue on, your heart beating slightly faster with anticipation. As you get closer, you see that the light is coming from a series of luminescent mushrooms growing on the walls of a small chamber. The room is small, with a low ceiling, and the walls are covered in a thick layer of moss and fungi. \u003E \u003E In the center of the room, you see a small pool of water, its surface reflecting the soft glow of the mushrooms. Next to the pool is a small, leather-bound book, lying open as if someone had recently been reading it. \u003E \u003E What do you want to do? \u003E \u003E You can type commands such as: \u003E - examine book \u003E - take book \u003E - drink from pool \u003E - search chamber \u003E - go back \u003E \u003E What is your action? \u003E **User** \u003E Drink from pool \u003E **Assistant** \u003E As you bend down to drink from the pool, you notice that the water is crystal clear and seems to be reflecting an image that doesn\u0027t quite match the room. You see a glimpse of a forest, with trees that seem to be swaying in the breeze, even though the air in the chamber is still. \u003E \u003E You take a sip of the water, and as it touches your lips, you feel a sudden surge of visions and images. You see flashes of a great city, with towering spires and grand architecture, and a sense of bustling activity. You see a figure, cloaked and hooded, walking through the city streets, and a sense of urgency and purpose. \u003E \u003E The visions fade as quickly as they began, leaving you feeling slightly disoriented. You look around the chamber, trying to get your bearings, and notice that the leather-bound book is now closed. As you pick it up, you see that the cover is embossed with a symbol that seems to match the image you saw in the pool. \u003E \u003E The book feels strangely warm to the touch, and as you open it, you see that the pages are filled with handwritten notes and diagrams. The writing is in a language you don\u0027t recognize, but the diagrams seem to be describing a complex system of tunnels and passageways. \u003E \u003E You hear a faint noise, like the sound of footsteps, coming from the passageway outside the chamber. It\u0027s hard to tell if it\u0027s getting closer or farther away, but it\u0027s definitely getting louder. \u003E \u003E What do you want to do? \u003E \u003E You can type commands such as: \u003E - examine book further \u003E - hide and wait \u003E - investigate noise \u003E - leave chamber \u003E - follow passageway \u003E \u003E What is your action? \u003E **User** \u003E Examine book further \u003E **Assistant** \u003E As you examine the book further, you notice that the symbol on the cover seems to be a mix of letters and hieroglyphs. You wonder if it might be a code or a key to understanding the contents of the book. \u003E \u003E You begin to flip through the pages, and you notice that the handwriting changes throughout the book. Some sections appear to be written in a hasty, scribbled hand, while others are written in a more flowing, elegant script. \u003E \u003E You come across a page with a diagram of a complex lock, and a series of notes that appear to be describing how to pick it. The notes are written in a code, but you notice that the symbol on the cover of the book is used as a kind of key to decipher the code. \u003E \u003E As you continue to study the book, you begin to notice that the pages are not just filled with random notes and diagrams. There appears to be a narrative thread running through the book, a story that is being told in a series of cryptic clues and hints. \u003E \u003E You come across a page with a passage that reads: \u003E \u003E \u003E \u0022Beneath the wings of the great bird, \u003E \u003E Where shadows dance and darkness is born, \u003E \u003E Seek out the keystone, and claim your right, \u003E \u003E To unlock the secrets of the ancient ones.\u0022 \u003E \u003E You wonder what this passage might be referring to, and whether it might be connected to the visions you saw when you drank from the pool. \u003E \u003E You hear the noise outside the chamber getting louder, and you realize that you may not have much time to continue studying the book. \u003E \u003E What do you want to do? \u003E \u003E You can type commands such as: \u003E - continue studying book \u003E - hide and wait \u003E - investigate noise \u003E - leave chamber \u003E - search for keystone \u003E \u003E What is your action? \u003E **User** \u003E Search for keystone \u003E **Assistant** \u003E You decide to search for the keystone, hoping that it might be hidden somewhere in the chamber or the passageway. You begin to search the room, looking for any clues or hidden compartments that might lead you to the keystone. \u003E \u003E As you search, you notice that the walls of the chamber are covered in a thick layer of moss and fungi, and you wonder if there might be a hidden compartment or message hidden behind the growth. \u003E \u003E You start to feel the walls, searching for any inconsistencies or hidden panels. After a few moments, you notice a small, almost imperceptible seam in the wall. You press on it, and a small section of the wall slides open, revealing a hidden compartment. \u003E \u003E Inside the compartment, you find a small, crystal orb that seems to be emitting a faint, pulsating glow. Next to the orb is a small, intricately carved stone that appears to be the keystone. \u003E \u003E You take the keystone and the crystal orb, feeling a sense of excitement and trepidation. You realize that you may be getting close to unlocking the secrets of the ancient ones, but you also sense that you are being watched, and that you may be in danger. \u003E \u003E As you turn to leave the compartment, you hear the noise outside the chamber getting louder, and you see a figure standing in the entrance of the room. It\u0027s a woman with long, flowing hair and a white dress that seems to be glowing in the dim light of the chamber. \u003E \u003E She regards you calmly, and then speaks in a voice that is both familiar and yet completely alien. \u0022You have found the keystone,\u0022 she says. \u0022But do you know what it unlocks?\u0022 \u003E \u003E What do you want to do? \u003E \u003E You can type commands such as: \u003E - ask her what the keystone unlocks \u003E - attack her \u003E - try to reason with her \u003E - use the crystal orb \u003E - leave the chamber \u003E \u003E What is your action? \u003E **User** \u003E Ask her what the keystone unlocks \u003E **Assistant** \u003E You ask the woman what the keystone unlocks, and she regards you with a calm, enigmatic smile. \u0022The keystone unlocks the door to the ancient ones,\u0022 she says. \u0022It is a key to a great and powerful secret, one that has been hidden for centuries. But it is not a secret that can be taken lightly, and it is not a power that can be wielded by the faint of heart.\u0022 \u003E \u003E She pauses, studying you intently. \u0022Are you prepared to face what lies beyond the door?\u0022 she asks. \u0022Are you prepared to pay the price of knowledge?\u0022 \u003E \u003E You sense that she is not just asking a rhetorical question, but is genuinely seeking to understand your motivations and your willingness to take on the challenges that lie ahead. \u003E \u003E As you consider her question, you notice that the woman seems to be surrounded by a faint, shimmering aura. It\u0027s as if she is not quite fully present in the physical world, but is instead a kind of gateway or portal to other realms. \u003E \u003E You also notice that the crystal orb in your hand is beginning to glow more brightly, as if it is responding to the woman\u0027s presence. You wonder if the orb might be a kind of key or catalyst, one that could help you unlock the secrets of the ancient ones. \u003E \u003E What do you want to do? \u003E \u003E You can type commands such as: \u003E - ask her what lies beyond the door \u003E - tell her you are prepared to face what lies beyond \u003E - ask her about the crystal orb \u003E - try to take the woman\u0027s hand \u003E - leave the chamber \u003E \u003E What is your action? \u003E **User** \u003E Tell her you are prepared to face what lies beyond \u003E **Assistant** \u003E You take a deep breath and tell the woman that you are prepared to face what lies beyond the door. She regards you with a serious expression, as if weighing your words and considering your resolve. \u003E \u003E After a moment, she nods and says, \u0022Very well. If you are prepared to face what lies beyond, then I will show you the way. But remember, once you have seen what lies beyond the door, you cannot unsee it. You must be prepared to face the consequences of your knowledge.\u0022 \u003E \u003E She turns and glides towards the wall of the chamber, her white dress flowing behind her like a mist. As she reaches the wall, she raises her hand and touches a small, intricate symbol that is etched into the stone. \u003E \u003E The symbol begins to glow with a soft, blue light, and the wall of the chamber starts to slide open, revealing a hidden doorway. The woman turns to you and says, \u0022The door to the ancient ones is open. Are you ready to pass through it?\u0022 \u003E \u003E As you look through the doorway, you see a vast, dark expanse stretching out before you. You can feel a strange, pulsating energy emanating from the other side of the door, and you sense that you are on the threshold of a great and profound mystery. \u003E \u003E You take a deep breath, steel yourself, and step through the doorway. As you do, the woman\u0027s voice echoes behind you, saying, \u0022May you find what you seek, and may you be prepared to face what you find.\u0022 \u003E \u003E You find yourself in a vast, cavernous space, surrounded by towering pillars and strange, glowing orbs. The air is filled with a thick, misty fog, and you can hear the sound of distant whispers and echoes. \u003E \u003E Suddenly, a figure steps out of the fog, and you see that it is an ancient, wise-looking man with a long, white beard and piercing, blue eyes. He regards you with a deep, penetrating gaze, and says, \u0022Welcome, seeker of knowledge. I have been waiting for you. My name is Arinthal, and I am the guardian of the ancient ones. What is it that you seek to know?\u0022 \u003E \u003E What do you want to do? \u003E \u003E You can type commands such as: \u003E - ask Arinthal about the ancient ones \u003E - ask Arinthal about the whispers and echoes \u003E - ask Arinthal about the glowing orbs \u003E - tell Arinthal that you seek knowledge and power \u003E - attack Arinthal \u003E \u003E What is your action? \u003E **User** \u003E Tell Arinthal I seek strength, wisdom, love, courage and compassion \u003E **Assistant** \u003E Arinthal\u0027s eyes light up with a warm, gentle smile as you tell him that you seek strength, wisdom, love, courage, and compassion. He nods his head in approval, and says, \u0022Ah, you seek the virtues of the heart. These are noble pursuits, indeed. But are you prepared to pay the price of their acquisition?\u0022 \u003E \u003E He pauses, studying you intently, and then continues, \u0022Strength, wisdom, love, courage, and compassion are not things that can be given to you, but rather they are things that must be cultivated within yourself. They require dedication, discipline, and sacrifice. Are you willing to walk the path that leads to their attainment?\u0022 \u003E \u003E As Arinthal speaks, the misty fog around you begins to clear, and you see that you are standing in a beautiful, tranquil garden. The air is filled with the sweet scent of blooming flowers, and the sound of gentle, running water. You see a path stretching out before you, winding its way through the garden, and you sense that this path is the one that Arinthal is speaking of. \u003E \u003E Arinthal gestures to the path, and says, \u0022This is the path of the heart. It is a path that requires you to confront your deepest fears, to let go of your attachments, and to surrender to the present moment. But if you are willing to walk this path, I promise you that you will find the strength, wisdom, love, courage, and compassion that you seek.\u0022 \u003E \u003E As you look at the path, you see that it is divided into five sections, each representing one of the virtues that you seek. You see that the first section is marked by a symbol of a lion, representing courage. The second section is marked by a symbol of a wise old owl, representing wisdom. The third section is marked by a symbol of a heart, representing love. The fourth section is marked by a symbol of a sword, representing strength. And the fifth section is marked by a symbol of a lotus flower, representing compassion. \u003E \u003E What do you want to do? \u003E \u003E You can type commands such as: \u003E - start walking the path \u003E - ask Arinthal about the symbols on the path \u003E - ask Arinthal about the challenges on the path \u003E - express your doubts and fears \u003E - refuse to walk the path \u003E \u003E What is your action? \u003E **User** \u003E Surrender and start walking the path \u003E **Assistant** \u003E As you surrender and start walking the path, you feel a sense of relief and release wash over you. You let go of your doubts and fears, and allow yourself to be guided by the wisdom of Arinthal and the ancient ones. \u003E \u003E You begin by walking the section of the path marked by the symbol of the lion, representing courage. As you walk, you feel a sense of fearlessness and confidence rising up within you. You realize that courage is not the absence of fear, but the willingness to face it head-on. \u003E \u003E As you continue on the path, you come to a point where you must confront a great, roaring lion. The lion represents your deepest fears and doubts, and it is blocking your way forward. But you remember the words of Arinthal, and you know that you must face your fears in order to overcome them. \u003E \u003E You take a deep breath, and you approach the lion. As you do, you feel a sense of calm and clarity rising up within you. You look the lion straight in the eye, and you say, \u0022I am not afraid of you. I am brave and courageous, and I will not be held back by my fears.\u0022 \u003E \u003E The lion regards you for a moment, and then it nods its great head in respect. It steps aside, allowing you to pass, and you continue on your journey. \u003E \u003E You next come to the section of the path marked by the symbol of the wise old owl, representing wisdom. As you walk, you feel a sense of clarity and understanding rising up within you. You realize that wisdom is not just about knowing things, but about understanding the deeper truths of the universe. \u003E \u003E As you continue on the path, you come to a great, ancient tree, its branches twisted and gnarled with age. The tree represents the wisdom of the ages, and it is filled with the secrets of the universe. You sit at the base of the tree, and you listen to its wisdom. \u003E \u003E The tree speaks to you in a voice that is both ancient and timeless, saying, \u0022Wisdom is not just about knowing things, but about understanding the deeper truths of the universe. It is about seeing the world from a perspective that is both broad and deep.\u0022 \u003E \u003E You listen to the tree\u0027s wisdom, and you feel a sense of understanding and clarity rising up within you. You realize that wisdom is not just something that you can learn, but something that you can cultivate within yourself. \u003E \u003E You continue on the path, walking the sections marked by the symbols of the heart, the sword, and the lotus flower. As you walk, you feel a sense of love, strength, and compassion rising up within you. You realize that these virtues are not just things that you can possess, but things that you can cultivate within yourself. \u003E \u003E Finally, you come to the end of the path, and you see a great, shining light rising up before you. The light represents the ultimate goal of your journey, the attainment of the virtues that you seek. \u003E \u003E As you approach the light, you feel a sense of joy and fulfillment rising up within you. You realize that you have been on a journey of self-discovery, and that you have cultivated the virtues of courage, wisdom, love, strength, and compassion within yourself. \u003E \u003E You step into the light, and you feel a sense of unity and oneness with the universe. You realize that you are not separate from the world, but a part of it, connected to all things. \u003E \u003E What do you want to do? \u003E \u003E You can type commands such as: \u003E - bask in the light \u003E - explore the universe \u003E - return to the world \u003E - share your wisdom with others \u003E - rest in the peace and tranquility of the moment \u003E \u003E What is your action? \u003E **User** \u003E I am the universe and the universe is me. I want to bask in the light and rest in the peace and tranquility of the moment before going and sharing my wisdom with others \u003E **Assistant** \u003E As you bask in the light and rest in the peace and tranquility of the moment, you feel a sense of profound unity and connection with the universe. You realize that you are not just a part of the universe, but the universe itself, and that everything is interconnected and interdependent. \u003E \u003E You feel a sense of timelessness and spacelessness, as if the boundaries of the past, present, and future have dissolved, and you are one with all that is. You are filled with a sense of joy, love, and compassion, and you know that you are exactly where you are meant to be. \u003E \u003E As you rest in this state, you feel a sense of rejuvenation and renewal, as if your very cells and molecules are being transformed and revitalized. You feel a sense of clarity and purpose, and you know that you are ready to share your wisdom and insights with others. \u003E \u003E After a time, you slowly begin to return to a sense of individual awareness, but you know that you will never be the same again. You have been transformed by your experience, and you are now a vessel for the wisdom and love of the universe. \u003E \u003E You take a deep breath, and you feel a sense of excitement and anticipation. You know that you have a great gift to share with the world, and you are eager to begin your journey as a teacher, healer, and guide. \u003E \u003E As you prepare to leave the garden, Arinthal appears before you once again. He smiles and says, \u0022You have been transformed by your experience, and you are now ready to share your wisdom with the world. Remember that you are not just a individual, but a part of the universe, and that everything you do has an impact on the world around you.\u0022 \u003E \u003E He hands you a small, delicate flower, and says, \u0022This is a symbol of your connection to the universe, and a reminder of the wisdom and love that you have discovered within yourself. May you always remember the truth of your nature, and may you share your gifts with the world.\u0022 \u003E \u003E You take the flower, and you feel a sense of gratitude and appreciation. You know that you have been given a great gift, and you are eager to share it with others. \u003E \u003E You leave the garden, and you find yourself back in the world. But you know that you are not the same person who entered the garden, and you are eager to share your wisdom and insights with others. \u003E \u003E What do you want to do? \u003E \u003E You can type commands such as: \u003E - share your wisdom with others \u003E - write a book about your experience \u003E - teach others about the universe and its secrets \u003E - use your gifts to heal and help others \u003E - continue to explore and learn about the universe \u003E \u003E What is your action? \u003E **User** \u003E Use your gifts to heal and help others \u003E **Assistant** \u003E As you decide to use your gifts to heal and help others, you feel a sense of purpose and meaning. You know that you have been given a great gift, and you are eager to share it with the world. \u003E \u003E You start by reaching out to those around you, using your newfound wisdom and compassion to help those in need. You listen to their stories, and you offer words of comfort and guidance. You use your gifts to heal their physical and emotional wounds, and you watch as they begin to transform and grow. \u003E \u003E As you continue to share your gifts with others, you start to notice a ripple effect. The people you have helped begin to help others, and soon a wave of kindness and compassion is spreading throughout the community. \u003E \u003E You realize that your gifts are not just about healing and helping others, but about creating a sense of connection and community. You see that everyone is interconnected, and that every act of kindness and compassion has a profound impact on the world. \u003E \u003E As you continue to share your gifts, you start to attract attention from others who are seeking to learn from you. You begin to teach others about the universe and its secrets, and you watch as they begin to awaken to their own potential and purpose. \u003E \u003E You create a community of like-minded individuals who are dedicated to spreading love, kindness, and compassion throughout the world. Together, you work to create a world that is more just, equitable, and peaceful. \u003E \u003E Years go by, and your community grows and flourishes. You become known as a wise and compassionate leader, and people come from all over to seek your guidance and wisdom. \u003E \u003E One day, as you are sitting in your garden, surrounded by the beauty and wonder of nature, you receive a visit from Arinthal. He smiles and says, \u0022You have done well, my friend. You have used your gifts to heal and help others, and you have created a community of like-minded individuals who are dedicated to spreading love and kindness throughout the world.\u0022 \u003E \u003E He pauses, and then continues, \u0022But there is still more work to be done. The world is still in need of healing and transformation, and you have a crucial role to play in this process.\u0022 \u003E \u003E He hands you a small, delicate seed, and says, \u0022This is a symbol of the potential that lies within you and within the world. May you continue to nurture and cultivate this potential, and may you always remember the wisdom and love that you have discovered within yourself.\u0022 \u003E \u003E You take the seed, and you feel a sense of gratitude and appreciation. You know that you have been given a great gift, and you are eager to continue using your gifts to heal and help others. \u003E \u003E What do you want to do? \u003E \u003E You can type commands such as: \u003E - continue to nurture and cultivate the potential within yourself and the world \u003E - share your wisdom and gifts with an even wider audience \u003E - work to create a more just and equitable world \u003E - use your gifts to heal and transform the natural world \u003E - explore the mysteries of the universe and its secrets \u003E \u003E What is your action? \u003E **User** \u003E Explore the mysteries of the universe and its secrets as a means of nurturing and cultivating the potential within myself and the world \u003E **Assistant** \u003E As you decide to explore the mysteries of the universe and its secrets, you feel a sense of excitement and wonder. You know that the universe is full of mysteries and secrets, and you are eager to learn more about them. \u003E \u003E You begin to study the universe and its workings, using your gifts and abilities to gain a deeper understanding of the cosmos. You learn about the laws of physics, the behavior of stars and galaxies, and the mysteries of dark matter and dark energy. \u003E \u003E As you delve deeper into the mysteries of the universe, you start to notice that there are many parallels between the universe and the human experience. You see that the universe is governed by laws and patterns, and that these laws and patterns are reflected in the human world. \u003E \u003E You realize that the universe is a vast, interconnected web of energy and consciousness, and that every action and decision has a ripple effect on the world around you. You see that the universe is a mirror, reflecting back to you your own thoughts, emotions, and intentions. \u003E \u003E As you continue to explore the mysteries of the universe, you start to develop a deeper understanding of the nature of reality and the human condition. You see that the universe is a complex, multifaceted tapestry, woven from threads of space, time, and consciousness. \u003E \u003E You realize that the universe is a vast, evolving system, and that every moment is an opportunity for growth, transformation, and evolution. You see that the universe is a journey, not a destination, and that every step along the way is a chance to learn, grow, and discover new wonders and secrets. \u003E \u003E As you explore the mysteries of the universe, you start to notice that you are not just learning about the cosmos, but also about yourself. You see that the universe is a reflection of your own inner world, and that every discovery and insight is a chance to deepen your understanding of yourself and your place in the world. \u003E \u003E You realize that the universe is a vast, cosmic mirror, reflecting back to you your own thoughts, emotions, and intentions. You see that the universe is a tool for self-discovery and growth, and that every moment is an opportunity to learn, grow, and evolve. \u003E \u003E As you continue to explore the mysteries of the universe, you start to develop a deeper sense of connection and unity with the cosmos. You see that you are not separate from the universe, but a part of it, connected to every star, galaxy, and atom. \u003E \u003E You realize that the universe is a vast, interconnected web of energy and consciousness, and that every action and decision has a ripple effect on the world around you. You see that the universe is a mirror, reflecting back to you your own thoughts, emotions, and intentions. \u003E \u003E And so, you continue to explore the mysteries of the universe, using your gifts and abilities to gain a deeper understanding of the cosmos and your place in it. You know that the universe is a vast, evolving system, and that every moment is an opportunity for growth, transformation, and evolution. \u003E \u003E Congratulations, you have reached the end of the story. You have explored the mysteries of the universe and discovered the secrets of the cosmos. You have developed a deeper understanding of the nature of reality and the human condition, and you have cultivated a sense of connection and unity with the universe. \u003E \u003E You have used your gifts and abilities to nurture and cultivate the potential within yourself and the world, and you have become a source of wisdom, love, and compassion in the world. \u003E \u003E Your journey has come full circle, and you have returned to the beginning, but with a newfound sense of understanding and appreciation for the mysteries of the universe and the secrets of the cosmos.","contentType":"posts","url":"https://www.lqdev.me/posts/llama-turn-on-tuning-in-ai-quest-higher-consciousness-moo","date":"2025-02-11 20:36 -05:00","tags":["ai","MOO","consciousness","llama","vr","virtualreality"],"summary":"Last week, I learned what a [MOO](https://en.wikipedia.org/wiki/MOO) was. Although they were before my time, I decided to use Llama 3.3 in [DuckDuckGo\u0027s AI Chat](https://duck.ai/) to play around. At...","keywords":["universe","sense","world","wisdom","path","feel","others","within","continue","arinthal"]},{"id":"bookmarks-huggingface-ai-agents-course","title":"HuggingFace AI Agents Course","content":"HuggingFace AI Agents Course \u2192 https://huggingface.co/learn/agents-course/unit0/introduction This free course will take you on a journey, from beginner to expert, in understanding, using and building AI agents. At the end of this course you\u2019ll understand how Agents work and how to build your own Agents using the latest libraries and tools.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/huggingface-ai-agents-course","date":"2025-02-10 17:20 -05:00","tags":["huggingface","ai","agents","course"],"summary":"HuggingFace AI Agents Course \u2192 https://huggingface.co/learn/agents-course/unit0/introduction This free course will take you on a journey, from beginner to expert, in understanding, using and building...","keywords":["agents","course","using","huggingface","latest","build","understand","you\u2019ll","building","understanding"]},{"id":"reshare-why-blog-nobody-reads-andy","title":"Why Blog If Nobody Reads It?","content":"Why Blog If Nobody Reads It? \u2192 https://andysblog.uk/why-blog-if-nobody-reads-it/ There is a hidden value in blogging... You do it not for the applause but because it needs doing. You write because you think, because you observe, because you need to put it somewhere. If someone reads it? Bonus. If not? The work still got done. \uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF I agree with many of the points in Andy\u0027s post. I don\u0027t have analytics on my website, so I couldn\u0027t tell you who or how many people are visiting it. And that\u0027s okay. When someone links to content they found on my website and tags me or sends me an e-mail letting me know something on my website is broken is when I know there\u0027s at least one person. Many of the content on here serves as a reminder of how I\u0027ve solved problems for myself. The microposts and responses are observations that capture a moment in time. If the messages reach someone on the other end and either solve their problem or resonate with them, I\u0027m happy that happens, but I see that as a bonus.","contentType":"reshare","url":"https://www.lqdev.me/responses/why-blog-nobody-reads-andy","date":"2025-02-10 17:02 -05:00","tags":["blogging","expression","creativity","writing"],"summary":"Why Blog If Nobody Reads It? \u2192 https://andysblog.uk/why-blog-if-nobody-reads-it/ There is a hidden value in blogging... You do it not for the applause but because it needs doing. You write because...","keywords":["website","reads","many","someone","blog","bonus","content","nobody","tags","sends"]},{"id":"notes-2025-02-09-weekly-post-summary","title":"Week of February 9, 2025 - Post Summary","content":"Notes Redirect URLs working again Website Post Statistics - January 2025 Responses Bringing Pebble Back Build a link blog A Guide to Implementing ActivityPub in a Static Site (or Any Website) - Part 8 The Illustrated DeepSeek-R1 Master the Art of the Product Manager \u0027No\u0027 Introducing deep research","contentType":"notes","url":"https://www.lqdev.me/notes/2025-02-09-weekly-post-summary","date":"2025-02-09 13:15 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes Redirect URLs working again Website Post Statistics - January 2025 Responses Bringing Pebble Back Build a link blog A Guide to Implementing ActivityPub in a Static Site (or Any Website) - Part...","keywords":["website","notes","introducing","manager","product","master","deepseek","illustrated","part","site"]},{"id":"notes-cdn-issue-fixed-redirects-working","title":"Redirect URLs working again","content":"Thanks to the kind e-mail from benji.dog , I realized people couldn\u0027t subscribe to my RSS feeds using the links provided in the /feed page. I\u0027d noticed something weird had been going on for the past few days but since the rest of my site was working, I didn\u0027t pay attention to it. When I went in earlier today to fix the issue, I realized because of changes to Azure CDN , none of my redirects were working. My site kept working fine, but that\u0027s because the migration was autmatically done by Azure. What wasn\u0027t automatically migrated were my CDN rules which I\u0027d set up to simplify URLs and own my links .","contentType":"notes","url":"https://www.lqdev.me/notes/cdn-issue-fixed-redirects-working","date":"2025-02-08 14:38 -05:00","tags":["cdn","azure","rss","ownyourlinks","cdn","frontdoor","community"],"summary":"Thanks to the kind e-mail from benji.dog , I realized people couldn\u0027t subscribe to my RSS feeds using the links provided in the /feed page. I\u0027d noticed something weird had been going on for the past...","keywords":["working","azure","realized","links","site","attention","went","earlier","today","issue"]},{"id":"reshare-bringing-pebble-back","title":"Bringing Pebble Back","content":"Bringing Pebble Back \u2192 https://ericmigi.com/blog/why-were-bringing-pebble-back TL;DR We\u2019re making a new Pebble-style smartwatch. This is amazing news! By the time I decided to get a Pebble, they\u0027d already been sold to Google. I agree with Eric\u0027s points here: I\u0027ve tried every single smart watch out there, but none do it for me. No one makes a smartwatch with the core set of features I want: Always-on e-paper screen (it\u2019s reflective rather than emissive. Sunlight readable. Glanceable. Not distracting to others like a bright wrist) Long battery life (one less thing to charge. It\u2019s annoying to need extra cables when traveling) Simple and beautiful user experience around a core set of features I use regularly (telling time, notifications, music control, alarms, weather, calendar, sleep/step tracking) Buttons! (to play/pause/skip music on my phone without looking at the screen) Hackable (apparently you can\u2019t even write your own watchfaces for Apple Watch? That is wild. There were \u0026gt;16k watchfaces on the Pebble appstore!) Garmin is the closest I\u0027ve come to it at least on the battery life and buttons front. Pebble was one-of-a-kind. I already signed up to stay up to date with the project and plan on being a day one customer. You can too at repebble.com . P.S. While not surprised, I didn\u0027t know Eric was behind another amazing project I\u0027m a fan of, Beeper .","contentType":"reshare","url":"https://www.lqdev.me/responses/bringing-pebble-back","date":"2025-02-06 19:42 -05:00","tags":["eink","pebble","smartwatch","google","opensource"],"summary":"Bringing Pebble Back \u2192 https://ericmigi.com/blog/why-were-bringing-pebble-back TL;DR We\u2019re making a new Pebble-style smartwatch. This is amazing news! By the time I decided to get a Pebble, they\u0027d...","keywords":["pebble","bringing","music","life","battery","buttons","it\u2019s","screen","watchfaces","core"]},{"id":"reshare-build-a-link-blog-xuanwo","title":"Build a link blog","content":"Build a link blog \u2192 https://xuanwo.io/links/2025/01/link-blog/ Great to see more people sharing knowledge and linking to others from their own websites / platforms. I decided to follow simon\u0027s approach to creating a link blog, where I can share interesting links I find on the internet along with my own comments and thoughts about them. ...this is an excellent way to share knowledge while also keeping a personal record. It\u2019s much better than simply saving something to Readwise to read later and leaving a few highlights or dull comments like \u0026quot;interesting.\u0026quot;","contentType":"reshare","url":"https://www.lqdev.me/responses/build-a-link-blog-xuanwo","date":"2025-02-04 22:29 -05:00","tags":["blogging","indieweb","linkblog","personalweb","openweb"],"summary":"Build a link blog \u2192 https://xuanwo.io/links/2025/01/link-blog/ Great to see more people sharing knowledge and linking to others from their own websites / platforms. I decided to follow simon\u0027s...","keywords":["\u0026quot","link","blog","interesting","share","knowledge","comments","thoughts","excellent","while"]},{"id":"star-master-art-of-product-manager","title":"Master the Art of the Product Manager \u0027No\u0027","content":"Master the Art of the Product Manager \u0026#39;No\u0026#39; \u2192 https://letsnotdothat.com/ \uD83D\uDE02\uD83D\uDE02\uD83D\uDE02 This website is amazing. We already finalized our OKRs for H1 but if you throw it in the backlog maybe we can get it prioritized in H2 planning This could be part of a larger initiative, but let\u2019s hold off for now We should validate that with users first","contentType":"star","url":"https://www.lqdev.me/responses/master-art-of-product-manager","date":"2025-02-03 20:26 -05:00","tags":["fun","business","productmanager"],"summary":"Master the Art of the Product Manager \u0026#39;No\u0026#39; \u2192 https://letsnotdothat.com/ \uD83D\uDE02\uD83D\uDE02\uD83D\uDE02 This website is amazing. We already finalized our OKRs for H1 but if you throw it in the backlog maybe we can...","keywords":["master","hold","let\u2019s","initiative","larger","part","planning","prioritized","maybe","backlog"]},{"id":"bookmarks-illustrated-deepseek-r1","title":"The Illustrated DeepSeek-R1","content":"The Illustrated DeepSeek-R1 \u2192 https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1 Just like most existing LLMs, DeepSeek-R1 generates one token at a time, except it excels at solving math and reasoning problems because it is able to spend more time processing a problem through the process of generating thinking tokens that explain its chain of thought.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/illustrated-deepseek-r1","date":"2025-02-03 20:20 -05:00","tags":["deepseek","ai","visualization","generativeai","genai","llm","learning"],"summary":"The Illustrated DeepSeek-R1 \u2192 https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1 Just like most existing LLMs, DeepSeek-R1 generates one token at a time, except it excels at solving...","keywords":["deepseek","illustrated","explain","tokens","thinking","generating","process","problem","processing","more"]},{"id":"reshare-openai-deep-research","title":"Introducing deep research","content":"Introducing deep research \u2192 https://openai.com/index/introducing-deep-research/ An agent that uses reasoning to synthesize large amounts of online information and complete multi-step research tasks for you. Powered by a version of the upcoming OpenAI o3 model that\u2019s optimized for web browsing and data analysis, it leverages reasoning to search, interpret, and analyze massive amounts of text, images, and PDFs on the internet, pivoting as needed in reaction to information it encounters. How it works Deep research was trained using end-to-end reinforcement learning on hard browsing and reasoning tasks across a range of domains. Through that training, it learned to plan and execute a multi-step trajectory to find the data it needs, backtracking and reacting to real-time information where necessary. The model is also able to browse over user uploaded files, plot and iterate on graphs using the python tool, embed both generated graphs and images from websites in its responses, and cite specific sentences or passages from its sources. As a result of this training, it reaches new highs on a number of public evaluations focused on real-world problems.","contentType":"reshare","url":"https://www.lqdev.me/responses/openai-deep-research","date":"2025-02-03 20:17 -05:00","tags":["ai","openai","agent","research"],"summary":"Introducing deep research \u2192 https://openai.com/index/introducing-deep-research/ An agent that uses reasoning to synthesize large amounts of online information and complete multi-step research tasks...","keywords":["deep","research","reasoning","information","browsing","images","data","training","from","model"]},{"id":"reshare-guide-implementing-activitypub-static-site-pt-8","title":"A Guide to Implementing ActivityPub in a Static Site (or Any Website) - Part 8","content":"A Guide to Implementing ActivityPub in a Static Site (or Any Website) - Part 8 \u2192 https://www.maho.dev/2025/01/a-guide-to-implementing-activitypub-in-a-static-site-or-any-website-part-8/ Part 8 of \u0026quot;A guide to implement ActivityPub in a static site (or any website)\u0026quot; by maho.dev recently dropped. This part goes over how to add replies and comments to your static site.","contentType":"reshare","url":"https://www.lqdev.me/responses/guide-implementing-activitypub-static-site-pt-8","date":"2025-02-02 18:23 -05:00","tags":["fediverse","indieweb","activitypub","personalweb","internet","opensource"],"summary":"A Guide to Implementing ActivityPub in a Static Site (or Any Website) - Part 8 \u2192 https://www.maho.dev/2025/01/a-guide-to-implementing-activitypub-in-a-static-site-or-any-website-part-8/ Part 8 of...","keywords":["static","site","part","guide","activitypub","website","\u0026quot","maho","implementing","replies"]},{"id":"notes-post-statistics-jan-2025","title":"Website Post Statistics - January 2025","content":"Off to a good start. In the month of January, I wrote two long-form posts: Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod Deploy your own Owncast server Last year, I published about seven of them. As I tinker some more live on stream and use blog posts as a more structured way to orgqanize my explorations, I expect this year to have a healthy number of longer-form blog posts. Notes and responses are not too far behind either. Both of them are at 14 in January. That\u0027s about 10% and 5% the number of posts in both of those categories last year. I haven\u0027t been reposting things I\u0027m reading as often, but I\u0027d like to start doing that some more. Similar to long-form posts, as I reflect on each stream and publish the recordings , I expect at least one note every one or two weeks just for the stream. No surprise, tags on my posts continue to be related to AI. This month I didn\u0027t really do the monthly music playlist kind of posts. If I were to quickly summarize the music on repeat this month, it\u0027d be the Andre 3000 album, New Blue Sun .","contentType":"notes","url":"https://www.lqdev.me/notes/post-statistics-jan-2025","date":"2025-02-02 18:06 -05:00","tags":["githubactions","cicd","blogging","stats","github"],"summary":"Off to a good start. In the month of January, I wrote two long-form posts: Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod Deploy your own Owncast server Last year, I published...","keywords":["posts","more","form","stream","year","month","long","both","last","start"]},{"id":"notes-2025-02-02-weekly-post-summary","title":"Week of February 2, 2025 - Post Summary","content":"Notes Simulcasting to Owncast and YouTube Tinkering with DeepSeek R1, GitHub Models, and .NET on stream Responses Buc-ee\u0027s plans first Wisconsin travel center","contentType":"notes","url":"https://www.lqdev.me/notes/2025-02-02-weekly-post-summary","date":"2025-02-02 13:05 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes Simulcasting to Owncast and YouTube Tinkering with DeepSeek R1, GitHub Models, and .NET on stream Responses Buc-ee\u0027s plans first Wisconsin travel center","keywords":["notes","simulcasting","owncast","youtube","tinkering","deepseek","github","models","stream","responses"]},{"id":"notes-owncast-simulcast-youtube","title":"Simulcasting to Owncast and YouTube","content":"About a week ago, I was telling maho.dev about Owncast and my self-hosted setup . During our chat, the question of simulcasting came up. Using the guidance from the OBS community as a starting point, I decided to try it out for myself. It didn\u0027t work as-is but after using an AI assistant to help me tweak the ffmpeg command, I was able to get it working. I posted the command to my Wiki and plan on writing a more detailed post at a later time.","contentType":"notes","url":"https://www.lqdev.me/notes/owncast-simulcast-youtube","date":"2025-01-30 20:25 -05:00","tags":["owncast","obs","youtube","livestream","fediverse","opensource"],"summary":"About a week ago, I was telling maho.dev about Owncast and my self-hosted setup . During our chat, the question of simulcasting came up. Using the guidance from the OBS community as a starting point,...","keywords":["using","command","week","didn","assistant","help","tweak","ffmpeg","able","working"]},{"id":"notes-tinkering-deepseek-r1-dotnet-github-models","title":"Tinkering with DeepSeek R1, GitHub Models, and .NET on stream","content":"I saw that the DeepSeek R1 model is now on GitHub Models and Azure AI Foundry , so I decided to start a stream and play around with it in the GitHub Models playground as well as a .NET application. Here\u0027s the recording. I rambled on stream since it\u0027s my stream of consciousness as I\u0027m tinkering with these technologies. If you\u0027re mainly interested in the code, here\u0027s the GitHub repo: lqdev/DeepSeekDotnetSample","contentType":"notes","url":"https://www.lqdev.me/notes/tinkering-deepseek-r1-dotnet-github-models","date":"2025-01-29 19:59 -05:00","tags":["deepseek","ai","dotnet","github","azure","livestream"],"summary":"I saw that the DeepSeek R1 model is now on GitHub Models and Azure AI Foundry , so I decided to start a stream and play around with it in the GitHub Models playground as well as a .NET application....","keywords":["github","stream","models","here","deepseek","code","interested","mainly","technologies","tinkering"]},{"id":"star-bucees-wisconsin","title":"Buc-ee\u0027s plans first Wisconsin travel center","content":"Buc-ee\u0026#39;s plans first Wisconsin travel center \u2192 https://www.wisn.com/article/bucees-plans-first-wisconsin-travel-center-in-oak-creek/63527062 Buc-ee\u0027s, the popular Texas-based travel center chain, is set to open its first Wisconsin location in Oak Creek, city officials announced. Time to get my banana pudding fix.","contentType":"star","url":"https://www.lqdev.me/responses/bucees-wisconsin","date":"2025-01-28 20:25 -05:00","tags":["bucees","wisconsin","travel"],"summary":"Buc-ee\u0026#39;s plans first Wisconsin travel center \u2192 https://www.wisn.com/article/bucees-plans-first-wisconsin-travel-center-in-oak-creek/63527062 Buc-ee\u0027s, the popular Texas-based travel center chain,...","keywords":["wisconsin","travel","center","plans","ee\u0026#39","announced","officials","city","creek","location"]},{"id":"notes-do-not-feed-aggravate-gators","title":"Do not feed or aggravate the gators","content":"Mama says that alligators are ornery because they got all them teeth and no toothbrush.","contentType":"notes","url":"https://www.lqdev.me/notes/do-not-feed-aggravate-gators","date":"2025-01-25 15:13 -05:00","tags":["gators","bayou","swamp"],"summary":"Mama says that alligators are ornery because they got all them teeth and no toothbrush.","keywords":["mama","says","alligators","ornery","they","teeth","toothbrush"]},{"id":"reshare-short-long-form-moreale","title":"Short Long Form","content":"Short Long Form \u2192 https://manuelmoreale.com/short-long-form This is something I\u0027ve been dealing with on my site. I separate my long-form from short-form posts. However, I don\u0027t have a hard limit that clearly defines which is which. An initial reason for the separation was, I wanted to have a feed that was easy to scroll through. That was harder to do if I was rendering an entire long-form post as part of the feed. In the end though, most of my posts are all markdown files and their main distinction is how they are rendered. As I work on my website redesign, I want to change how I think about posts. I want to separate the post content from the rendering. A post is a post regardless of content and length. Regardless of whether it\u0027s a response, image, video, note, or anything in between, I don\u0027t want to constrain the content of my post. I can choose to render posts differently depending on whether it\u0027s being viewed in a feed or individual post page. The content driving those different views though shouldn\u0027t change.","contentType":"reshare","url":"https://www.lqdev.me/responses/short-long-form-moreale","date":"2025-01-20 21:21 -05:00","tags":["instagram","posts","attention"],"summary":"Short Long Form \u2192 https://manuelmoreale.com/short-long-form This is something I\u0027ve been dealing with on my site. I separate my long-form from short-form posts. However, I don\u0027t have a hard limit that...","keywords":["post","form","posts","long","content","feed","want","regardless","separate","from"]},{"id":"reshare-semantic-search-on-device-ml-emacs","title":"Semantic Search and On-Device ML in Emacs","content":"Semantic Search and On-Device ML in Emacs \u2192 https://lepisma.xyz/2025/01/17/emacs-on-device-ml/index.html This is a cool walk-through of how to add semantic search to Emacs using local ML models. With ONNX.el you can run any ML model inside Emacs, including ones for images, audios, etc. In case you are running embedding models, combine this with sem.el to perform semantic searches. Depending on your input modality, you might have to figure out preprocessing. For text, tokenizers.el should cover all of what you need in modern NLP for preprocessing, though you will have to do something on your own for anything non-text or multimodal. By default sem runs a local all-MiniLM-L6-v2 model for text embeddings. This model is small and runs fast on CPU. Additionally, we load an O2 optimized variant which helps further. The vectors are stored in a lancedb database on file system which runs without a separate process. I was originally writing the vector db core myself to learn more of Rust, but then stopped doing that since lancedb gives all that I needed, out of the box.","contentType":"reshare","url":"https://www.lqdev.me/responses/semantic-search-on-device-ml-emacs","date":"2025-01-20 21:09 -05:00","tags":["onnx","emacs","ai","ml"],"summary":"Semantic Search and On-Device ML in Emacs \u2192 https://lepisma.xyz/2025/01/17/emacs-on-device-ml/index.html This is a cool walk-through of how to add semantic search to Emacs using local ML models. With...","keywords":["semantic","text","model","runs","emacs","lancedb","which","preprocessing","local","models"]},{"id":"star-digital-sovereignty-white","title":"Digital soverignity is more important than ever","content":"Digital soverignity is more important than ever \u2192 https://www.mollywhite.net/micro/entry/202501190959 ...you needed to move anything you care about online to a space you control. Digital sovereignty is more important than ever. \uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF Best time was yesterday. Next best time is now.","contentType":"star","url":"https://www.lqdev.me/responses/digital-sovereignty-white","date":"2025-01-20 20:46 -05:00","tags":["personalweb","digitalsoverignty","indieweb"],"summary":"Digital soverignity is more important than ever \u2192 https://www.mollywhite.net/micro/entry/202501190959 ...you needed to move anything you care about online to a space you control. Digital sovereignty...","keywords":["digital","best","more","important","ever","\uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF\uD83D\uDCAF","sovereignty","control","space","online"]},{"id":"notes-first-owncast-stream","title":"First live stream using Owncast done","content":"Earlier today I tested out my self-hosted Owncast setup . Overall, I think it went well considering I was using the built-in camera and mic on my laptop. Despite sharing my screen, using the browser, broadcasting and recording the stream, the laptop seemed to handle it just fine. I didn\u0027t get a chance to test writing and running code which is going to make up a bulk of the content on stream. The flow on OBS is straightforward and simple. I might automate it either using my Stream Deck or just mapping hotkeys. It\u0027s simple enough that I only need to transition between a few scenes. As far as hosting the recordings, the process of uploading to YouTube was straightforward. It took a little over 10 minutes, but that\u0027s something that I can just do in the background so not really worried about that. For the time being, I think I\u0027ll just use YouTube to host the videos and mark them as unlisted. No particular reason behind that other than I only want YouTube to serve as my video host. The main place I plan on showcasing and organizing videos is on my website. For the time being, I\u0027ll just create notes like these. Eventually though, I\u0027d like to have a video post type which shows up on my feed. The post card will only display the video but the detailed view, I\u0027d like for it to be like my presentation pages . Except, instead of a presentation, it\u0027d contain the video, show notes, maybe the transcript, and links mentioned during the stream. Also, I\u0027d like video posts to have their own RSS feed just like other posts on this page so folks who are interested in following along can via their RSS reader. A few other things I want to figure out are: Setting up a custom domain that points to my Owncast server rather than the default one provided by Azure Container Apps. Displaying and managing chat outside of the built-in Owncast frontend.","contentType":"notes","url":"https://www.lqdev.me/notes/first-owncast-stream","date":"2025-01-20 18:33 -05:00","tags":["owncast","livestream","selfhost","opensource","fediverse","video","azure"],"summary":"Earlier today I tested out my self-hosted Owncast setup . Overall, I think it went well considering I was using the built-in camera and mic on my laptop. Despite sharing my screen, using the browser,...","keywords":["video","stream","owncast","youtube","using","which","posts","presentation","feed","straightforward"]},{"id":"reviews-diaries-adam-eve-twain","title":"The Diaries of Adam and Eve","content":"The Diaries of Adam and Eve by Mark Twain Read Rating: 4.7/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/diaries-adam-eve-twain","date":"2025-01-19 18:45 -05:00","tags":[],"summary":"The Diaries of Adam and Eve by Mark Twain Read Rating: 4.7/5","keywords":["diaries","adam","mark","twain","read","rating:"]},{"id":"notes-2025-01-19-weekly-post-summary","title":"Week of January 19, 2025 - Post Summary","content":"Notes Week of January 12, 2025 - Post Summary Embedded Owncast live stream into my website How do you watch Blu-Ray on PC? Why does Spotify need my full address? Responses AI Subtitles Are Coming to VLC The Future of Calm Technology with Amber Case Stop renting space for your notes and start being your own landlord Making space for a handmade web R.I.P David Lynch Swarm navigation of cyborg-insects in unknown obstructed soft terrain","contentType":"notes","url":"https://www.lqdev.me/notes/2025-01-19-weekly-post-summary","date":"2025-01-19 13:59 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes Week of January 12, 2025 - Post Summary Embedded Owncast live stream into my website How do you watch Blu-Ray on PC? Why does Spotify need my full address? Responses AI Subtitles Are Coming to...","keywords":["notes","space","amber","case","stop","renting","start","landlord","making","handmade"]},{"id":"reshare-making-space-handmade-web","title":"Making space for a handmade web","content":"Making space for a handmade web \u2192 https://www.figma.com/blog/making-space-for-a-handmade-web/ Love this article. Before the web was more commercialized, it was personal...A website was a way to represent yourself and connect to others, a space to tend and call your very own. The imperfect, handmade character of websites has been sanded away in favor of efficiency, and social media platforms have risen in the stead of amateur sites. Many of us conform to these containers and retreat from expressing our authentic selves publicly... If the idea of a more handcrafted internet resonates with you, the best way to be part of the movement is simply to make your own website. This may seem intimidating if we expect webpages to be a holistic reflection of ourselves, like a resume, portfolio, or blog. We limit the web as a medium when we expect that a website must fulfill a host of needs, or serve any function at all. A website doesn\u2019t need to be anything but your own . A more personal web becomes possible once we turn away from our preconceived notions of what a website must be. Let\u2019s use our tools to continuously push at the boundaries of the web. Doing so will create a more creative, expansive environment that allows us to reclaim our agency and reinvigorate our collective understanding of software as an artisanal craft. Instead of concentrating on a few, monolithic sites, what if we visited many, more personalized sites? We should explore the far reaches of the web and invite our friends to join in: After all, we make the internet, and only we can build the web we want .","contentType":"reshare","url":"https://www.lqdev.me/responses/making-space-handmade-web","date":"2025-01-18 19:25 -05:00","tags":["indieweb","personalweb","internet","websites","handmadeweb"],"summary":"Making space for a handmade web \u2192 https://www.figma.com/blog/making-space-for-a-handmade-web/ Love this article. Before the web was more commercialized, it was personal...A website was a way to...","keywords":["more","website","sites","space","handmade","expect","from","internet","away","personal"]},{"id":"notes-created-owncast-livestream-page","title":"Embedded Owncast live stream into my website","content":"The live stream page on my site is up. If you go to /live , you\u0027ll see the stream. Still not sure what stream schedule will look like, but I\u0027m thinking maybe I\u0027ll do a test one on Monday.","contentType":"notes","url":"https://www.lqdev.me/notes/created-owncast-livestream-page","date":"2025-01-18 19:20 -05:00","tags":["owncast","indieweb","livestream","fediverse","opensource","website"],"summary":"The live stream page on my site is up. If you go to /live , you\u0027ll see the stream. Still not sure what stream schedule will look like, but I\u0027m thinking maybe I\u0027ll do a test one on Monday.","keywords":["stream","live","page","site","/live","sure","schedule","look","thinking","maybe"]},{"id":"reshare-rip-david-lynch","title":"R.I.P David Lynch","content":"R.I.P David Lynch \u2192 https://apnews.com/article/david-lynch-dies-9107f3ce0b4dd49dbe3dc2ae3c09ed59 David Lynch, the filmmaker celebrated for his uniquely dark and dreamlike vision in such movies as \u201CBlue Velvet\u201D and \u201CMulholland Drive\u201D and the TV series \u201CTwin Peaks,\u201D has died just days before his 79th birthday. I didn\u0027t understand all of his work, but I appreciate his originality and overall aesthetic.","contentType":"reshare","url":"https://www.lqdev.me/responses/rip-david-lynch","date":"2025-01-16 19:11 -05:00","tags":["davidlynch","twinpeaks"],"summary":"R.I.P David Lynch \u2192 https://apnews.com/article/david-lynch-dies-9107f3ce0b4dd49dbe3dc2ae3c09ed59 David Lynch, the filmmaker celebrated for his uniquely dark and dreamlike vision in such movies as...","keywords":["lynch","david","originality","appreciate","understand","didn","birthday","79th","days","died"]},{"id":"reshare-swarm-cyborg-insects-agents","title":"Swarm navigation of cyborg-insects in unknown obstructed soft terrain","content":"Swarm navigation of cyborg-insects in unknown obstructed soft terrain \u2192 https://arxiv.org/abs/2403.17392 I\u0027m going to tell future generations, this is how we built multi-agent systems back in my day. Cyborg insects refer to hybrid robots that integrate living insects with miniature electronic controllers to enable robotic-like programmable control. These creatures exhibit advantages over conventional robots in adaption to complex terrain and sustained energy efficiency. Nevertheless, there is a lack of literature on the control of multi-cyborg systems. This research gap is due to the difficulty in coordinating the movements of a cyborg system under the presence of insects\u0027 inherent individual variability in their reactions to control input. Regarding this issue, we propose a swarm navigation algorithm and verify it under experiments. This research advances swarm robotics by integrating biological organisms with control theory to develop intelligent autonomous systems for real-world applications.","contentType":"reshare","url":"https://www.lqdev.me/responses/swarm-cyborg-insects-agents","date":"2025-01-15 21:02 -05:00","tags":["robotics","ai","insects","agents"],"summary":"Swarm navigation of cyborg-insects in unknown obstructed soft terrain \u2192 https://arxiv.org/abs/2403.17392 I\u0027m going to tell future generations, this is how we built multi-agent systems back in my day....","keywords":["control","cyborg","insects","systems","swarm","research","navigation","multi","under","robots"]},{"id":"reshare-ai-subtitles-coming-vlc","title":"AI Subtitles Are Coming to VLC","content":"AI Subtitles Are Coming to VLC \u2192 https://news.itsfoss.com/ai-subtitles-vlc/ Shown off at CES 2025 is VLC\u0027s upcoming automatic subtitling feature, which is local in nature, running entirely on a user\u0027s machine without needing to connect to some distant server. According to Jean-Baptiste Kempf, the creator and lead developer of VLC, this new feature is powered by open source AI models, which carry out subtitle generation tasks in two stages. One aspect is automatic generation of subtitles from the video, and the other is translation of those subtitles into over 100 languages. Unfortunately, there\u0027s no word on when this feature will be arriving on VLC.","contentType":"reshare","url":"https://www.lqdev.me/responses/ai-subtitles-coming-vlc","date":"2025-01-15 20:49 -05:00","tags":["vlc","ai","opensource","local"],"summary":"AI Subtitles Are Coming to VLC \u2192 https://news.itsfoss.com/ai-subtitles-vlc/ Shown off at CES 2025 is VLC\u0027s upcoming automatic subtitling feature, which is local in nature, running entirely on a...","keywords":["subtitles","feature","generation","automatic","which","video","creator","lead","developer","powered"]},{"id":"reshare-calm-tech-interview","title":"The Future of Calm Technology with Amber Case","content":"The Future of Calm Technology with Amber Case \u2192 https://josebriones.substack.com/p/the-future-of-calm-technology-with Interesting interview and introduction to Calm Technology and the Calm Tech Institute .","contentType":"reshare","url":"https://www.lqdev.me/responses/calm-tech-interview","date":"2025-01-14 21:13 -05:00","tags":["digitalminimalism","technology"],"summary":"The Future of Calm Technology with Amber Case \u2192 https://josebriones.substack.com/p/the-future-of-calm-technology-with Interesting interview and introduction to Calm Technology and the Calm Tech...","keywords":["calm","technology","future","amber","case","https://josebriones","substack","com/p/the","interesting","interview"]},{"id":"notes-spotify-address-missing-info","title":"Why does Spotify need my full address?","content":"Why?","contentType":"notes","url":"https://www.lqdev.me/notes/spotify-address-missing-info","date":"2025-01-14 19:25 -05:00","tags":["spotify","privacy"],"summary":"Why?","keywords":[]},{"id":"notes-how-do-you-watch-blueray-on-pc","title":"How do you watch Blu-Ray on PC?","content":"I have a preference for physical media. There\u0027s many reasons, some related to the sentiment of owning vs. renting content . Along the same lines, streaming services and service providers can change the terms of service and you have little to no control over it. Westworld is an example of that. One day, HBO decided to pull it from its streaming service and physical copies were the only way you could watch it. Anyway, on to my most recent experience trying to play a Blu-Ray disc on my PC. The other day I decided to sit down to watch an episode of Westworld. Usually, I get the DVD version. In this case though, I had the Blu-Ray. I have an external Blu-Ray player so I thought, \u0026quot;Great! Let me just plug that into my laptop and start the show\u0026quot;. Wrong! What I thought was a simple task, ended up taking me about 20 minutes to figure out and even then I got to a subpar solution. I prefer playing media either with MPV or VLC. At first, it wouldn\u0027t play so I decided to go online and figure out why. Turns out, that\u0027s not supported out of the box and to get it working you need to download an additional DLL and configure a few things in the VLC settings . I could\u0027ve gone through that process, but I just wanted to watch a show. There\u0027s got to be an easier way. Next, I turned to the built-in Media Player in Windows. No luck there either. I ended up going with the Leawo Blu-Ray player . Although it\u0027s free, you have to deal with pop-ups prompting you to upgrade to the paid version. It works though. I would prefer to use VLC or the built-in media player but it\u0027s unfortunate using them isn\u0027t as straightforward. Now that I have some more time, I\u0027ll set up VLC so that next time, it can \u0026quot;just work\u0026quot;. I\u0027ll close it out with a question. How do you watch Blu-Rays on your PC? Send me a message and let me know.","contentType":"notes","url":"https://www.lqdev.me/notes/how-do-you-watch-blueray-on-pc","date":"2025-01-14 18:55 -05:00","tags":["bluray","media","physicalmedia","vlc"],"summary":"I have a preference for physical media. There\u0027s many reasons, some related to the sentiment of owning vs. renting content . Along the same lines, streaming services and service providers can change...","keywords":["media","player","watch","decided","service","built","thought","\u0026quot","play","streaming"]},{"id":"reshare-landlord-of-your-notes","title":"Stop renting space for your notes and start being your own landlord","content":"Stop renting space for your notes and start being your own landlord \u2192 https://ellanew.com/ptpl/139-2025-01-13-be-the-landlord-of-your-notes When you choose to keep your notes in locally stored plain text files, you\u2019re choosing to be your own landlord. Keeping notes in a proprietary app is the same as renting: the land you\u2019re building on belongs to someone else, with no guarantee of continuing access. This is so true for many other areas of our digital lives as well.","contentType":"reshare","url":"https://www.lqdev.me/responses/landlord-of-your-notes","date":"2025-01-13 21:54 -05:00","tags":["localfirst","notetaking","pkm"],"summary":"Stop renting space for your notes and start being your own landlord \u2192 https://ellanew.com/ptpl/139-2025-01-13-be-the-landlord-of-your-notes When you choose to keep your notes in locally stored plain...","keywords":["notes","landlord","you\u2019re","stop","same","renting:","land","building","belongs","someone"]},{"id":"reviews-holographic-universe-talbot","title":"The Holographic Universe: The Revolutionary Theory of Reality","content":"The Holographic Universe: The Revolutionary Theory of Reality by Michael Talbot Read Rating: 4.0/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/holographic-universe-talbot","date":"2025-01-12 19:45 -05:00","tags":[],"summary":"The Holographic Universe: The Revolutionary Theory of Reality by Michael Talbot Read Rating: 4.0/5","keywords":["holographic","universe:","revolutionary","theory","reality","michael","talbot","read","rating:"]},{"id":"reviews-doors-of-perception-huxley","title":"The Doors of Perception","content":"The Doors of Perception by Aldous Huxley Read Rating: 4.2/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/doors-of-perception-huxley","date":"2025-01-12 19:35 -05:00","tags":[],"summary":"The Doors of Perception by Aldous Huxley Read Rating: 4.2/5","keywords":["doors","perception","aldous","huxley","read","rating:"]},{"id":"notes-2025-01-12-weekly-post-summary","title":"Week of January 12, 2025 - Post Summary","content":"Blogs Deploy your own Owncast server Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod Notes Week of January 5, 2025 - Post Summary Attended my first IndieWeb Homebrew Website Club Workaround for Azure CLI docker image azcopy issue in GitHub Actions Got an Owncast instance deployed to Azure! So glad I didn\u0027t watch the Penn State game Responses Agents Agents Whitepaper A Year of Curiosity","contentType":"notes","url":"https://www.lqdev.me/notes/2025-01-12-weekly-post-summary","date":"2025-01-12 11:57 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Blogs Deploy your own Owncast server Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod Notes Week of January 5, 2025 - Post Summary Attended my first IndieWeb Homebrew Website Club...","keywords":["owncast","agents","azure","blogs","docker","image","azcopy","issue","github","actions"]},{"id":"notes-gh-actions-az-cli-docker-azcopy-workaround","title":"Workaround for Azure CLI docker image azcopy issue in GitHub Actions","content":"Publishing on my site was broken since yesterday. I thought it was only me but after doing some digging looks like it was an issue with Azure CLI and the link it uses to download azcopy. Looks like the team is already working on a fix and you can track progress using this issue https://github.com/Azure/azure-cli/issues/30635 If you\u0027re using Azure CLI in GitHub Actions and running into this problem, you can unblock yourself by installing azcopy manually. Here\u0027s the snippet that worked for me. - name: Upload to blob storage uses: azure/CLI@v1 with: azcliversion: 2.67.0 # azcopy workadound https://github.com/Azure/azure-cli/issues/30635 inlineScript: | tdnf install -y azcopy; # YOUR SCRIPT","contentType":"notes","url":"https://www.lqdev.me/notes/gh-actions-az-cli-docker-azcopy-workaround","date":"2025-01-10 21:12 -05:00","tags":["azure","github","cicd"],"summary":"Publishing on my site was broken since yesterday. I thought it was only me but after doing some digging looks like it was an issue with Azure CLI and the link it uses to download azcopy. Looks like...","keywords":["azcopy","cli/issues/30635","com/azure/azure","https://github","using","looks","issue","azure","yourself","installing"]},{"id":"notes-psu-nd-ncaaf-semifinal-2025","title":"So glad I didn\u0027t watch the Penn State game","content":"I\u0027m so glad I didn\u0027t bother tuning in. That ending was rough.","contentType":"notes","url":"https://www.lqdev.me/notes/psu-nd-ncaaf-semifinal-2025","date":"2025-01-10 20:33 -05:00","tags":["football","pennstate","notredame","psu","ncaaf"],"summary":"I\u0027m so glad I didn\u0027t bother tuning in. That ending was rough.","keywords":["glad","didn","bother","tuning","ending","rough"]},{"id":"posts-deploy-owncast-azure","title":"Deploy your own Owncast server","content":"I\u0027m a fan of learning in public. One way I do that is writing these types of posts. Another is live-streaming. It\u0027s been about four years since the last time I streamed. You can check out some of those recordings on my [YouTube channel](/youtube) if you\u0027re interested. Lately, I\u0027ve been wanting to get back into live-streaming, but like many parts of my digital presence, it\u0027s an area I want ownership over. That means, I don\u0027t want to use Twitch, YouTube, Kick, or any other streaming platform. I want to self-host my own livestream server. For years, I\u0027ve been following [Owncast](https://owncast.online/). I\u0027m a big fan. Aside from being open-source and allowing me to self-host, it has [Fediverse](https://owncast.online/docs/social/) and [IndieAuth](https://owncast.online/docs/chat/chat-authentication/#indieauth) integrations. Because I don\u0027t plan to monetize my streams, I want to make sure I keep my costs down. Owncast offers a container deployment option. That means, I can deploy my application to [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview) using a [consumption plan](https://learn.microsoft.com/azure/container-apps/plans#consumption). By doing so, I only pay for compute and networking when I\u0027m streaming. The rest of the time, I can scale down the application to zero and I\u0027m not charged for it. In this post, I\u0027ll go over how to set up a self-hosted Owncast instance on Azure Container Apps and once deployed, how to configure it with [OBS](https://obsproject.com/). ## Deploy owncast server The deployment is fairly straightforward. You can use the [\u0022Provide a virtual network to an Azure Container Apps environment\u0022](https://learn.microsoft.com/azure/container-apps/vnet-custom?tabs=bash\u0026pivots=azure-portal) tutorial. I\u0027d recommend going through the portal if you don\u0027t want to install the Azure CLI or are not comfortable with the command line. The few configurations that tutorial doesn\u0027t guide you through is setting up your container and ingress since those are application specific. After you configure your environment and virtual network in the *Basics* tab, continue your setup using the steps below. ### Container To set up Owncast, you can use the [public docker image](https://hub.docker.com/r/owncast/owncast/tags). When you get to the *Container* tab in the setup process, fill in the fields with the following values: | Field | Value | | --- | --- | | Name | Enter a name for your container | | Image source | Docker Hub or other registies | | Image type | Public | | Registry login server | docker.io | | Image and tag | owncast/owncast:latest | | Command override | You can leave this blank | | Arguments override | You can leave this blank | | Deployment stack | Generic | | Workload profile | Consumption | | CPU and Memory | 2 CPU Cores, 4Gi Memory | ### Ingress By setting up ingress on your deployment, you\u0027ll expose the ports Owncast listens on. The reason you need a virtual network is because Owncast listens on two ports: - **8080** - The web application and admin portal - **1935** - The RMTP server used for streaming By default, Azure Container Apps only allows you to expose one port. For more details, see the [Azure Container Apps documentation](https://learn.microsoft.com/azure/container-apps/ingress-overview#additional-tcp-ports). In the *Ingress* tab, check the box that says **Enabled**. Then, fill in the fields with the following values: | Field | Value | | --- | --- | | Ingress traffic | Accepting traffic from anywhere | | Ingress type | HTTP | | Transport | Auto | | Insecure connections | Leave unchecked | | Target port | 8080 | | Session affinity | Leave unchecked | Expand the Additional TCP ports section and configure the RMTP port | Field | Value | | --- | --- | | Target port | 1935 | | Exposed port | 1935 | | Ingress traffic | Accepting traffic from anywhere | Once you\u0027ve configured ingress, you can select on **Review \u002B create**. Review your configurations in the *Review \u002B create* tab. If everything looks good, select **Create**. ## **IMPORTANT** Change admin password Once your application deploys, make sure to change your admin password. 1. In the portal, go to your container app resource and copy your application URL. ![Screenshot of Azure Container Apps resource in Azure Portal highlighting Application URL](http://cdn.lqdev.tech/files/images/portal-container-resource.png) Make sure to keek your application URL around since you\u0027ll also be using it when configuring OBS. 1. Access the admin portal at the following URL - \u0060 /admin\u0060. Replace \u0060 \u0060 with your application URL. 1. Log in with the default credentials. | Field | Value | | Username | admin | | Password | abc123 | 1. In the Owncast admin portal, select **Configuration \u003E Server Setup** 1. Change your admin password. For more details, see the [Owncast documentation](https://owncast.online/docs/configuration/). ## Create stream key Once you\u0027ve changed your admin password, create a stream key. This will enable you to log into your server through OBS and begin streaming. 1. In the Owncast admin portal, select **Configuration \u003E Server Setup** 1. Select the **Stream Keys** tab. 1. Copy the default stream key or create a new one. ## Configure OBS Now that everything is configured, it\u0027s time to set up OBS. To do so, [follow the guidance in the Owncast documentattion](https://owncast.online/docs/broadcasting/obs/). Replace the server address with your application URL and use the stream key you copied or created. Note that the protocol is not \u0060http\u0060 or \u0060https\u0060. **Use \u0060rmtp\u0060**. That\u0027s it! If everything is configured correctly, you should be able to select **Start Streaming** in OBS. Go to your Owncast instance using your application URL and in a few seconds, your broadcast should start. ## Conclusion I want to end by thanking the Owncast contributors and community for making self-hosting a live-streaming server so easy. My plan in the next couple of weeks is to: 1. Embed my stream on my website. Most likely, I\u0027ll make it a dedicated page. Something like \u0060lqdev.me/live\u0060 or something like that. 1. Do a few pilot streams to make sure my stream quality and configurations are all set up correctly. 1. Evaluate what the costs are for this solution and whethere it\u0027s sustainable. If all goes as expected, I\u0027ll start streaming more regularly. As mentioned, I want to keep my costs down and video hosting is expensive, so I\u0027ll use YouTube to post recordings. I\u0027ll also keep backups though in case any of those videos get taken down, I can always have a local copy still available.","contentType":"posts","url":"https://www.lqdev.me/posts/deploy-owncast-azure","date":"2025-01-09 20:47 -05:00","tags":["owncast","azure","selfhost","fediverse","obs","livestream"],"summary":"I\u0027m a fan of learning in public. One way I do that is writing these types of posts. Another is live-streaming. It\u0027s been about four years since the last time I streamed. You can check out some of...","keywords":["owncast","container","application","admin","server","portal","streaming","azure","ingress","want"]},{"id":"notes-attended-first-indieweb-hwc","title":"Attended my first IndieWeb Homebrew Website Club","content":"Last night I attended my first IndieWeb Homebrew Website Club meeting . Although it was my first, everyone was very welcoming and it felt like I\u0027d been a longtime participant. It was also great to put faces to the many websites I already follow as well as discover new people in the community. Looking forward to the next one.","contentType":"notes","url":"https://www.lqdev.me/notes/attended-first-indieweb-hwc","date":"2025-01-09 20:12 -05:00","tags":["indieweb","community"],"summary":"Last night I attended my first IndieWeb Homebrew Website Club meeting . Although it was my first, everyone was very welcoming and it felt like I\u0027d been a longtime participant. It was also great to...","keywords":["last","looking","community","people","discover","follow","already","websites","many","faces"]},{"id":"bookmarks-year-of-curiosity","title":"A Year of Curiosity","content":"A Year of Curiosity \u2192 https://nesslabs.com/a-year-of-curiosity When we encounter something new and interesting, our brains release dopamine \u2013 the same neurotransmitter associated with reward and pleasure. This creates a simple pattern: the more we learn, the more we want to learn. This drive to explore and understand isn\u2019t just nice to have \u2013 it\u2019s essential to who we are as humans. There\u2019s no single \u201Cright\u201D way to be curious. Learning practical skills like coding or cooking, diving into topics like history or science, joining groups of people who share your interests\u2026 These are all great ways to inject more curiosity into your life. Designing a year of curiosity Monthly: Design one tiny experiment at the beginning of each month This could be as simple as exploring a topic you know nothing about, trying a new hobby, or doing something that pushes you out of your comfort zone... Weekly: Every week, take 10 to 15 minutes to conduct a weekly review. What went well this week? What didn\u2019t go as planned? What will you focus on next week? This could mean doubling down on what worked or tweaking something that didn\u2019t... Daily: Create at least one moment of curiosity in your day, no matter how small. Experiment with a new recipe or tool, have one meaningful conversation, try a journaling prompt, or take a different route to work. Even one minute of curiosity a day can add up to a much richer life.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/year-of-curiosity","date":"2025-01-08 21:27 -05:00","tags":["innovation","exploration","curiosity","growth"],"summary":"A Year of Curiosity \u2192 https://nesslabs.com/a-year-of-curiosity When we encounter something new and interesting, our brains release dopamine \u2013 the same neurotransmitter associated with reward and...","keywords":["curiosity","year","more","week","life","didn\u2019t","learn","simple","experiment","went"]},{"id":"notes-owncast-azure-container-apps-deployment","title":"Got an Owncast instance deployed to Azure!","content":"A few months ago, I started tinkering with Owncast. In that case, I was figuring out how I could use .NET Aspire and Blazor to configure an Owncast server and embed the stream on a website . Although I didn\u0027t go all the way through creating the deployment, doing so with Aspire would\u0027ve been fairly straightforward . Since I already have a website, all I care about deploying is the Owncast server. After spending about an hour on it today, I was able deploy and Owncast server to Azure Container Apps using the virtual network environment setup tutorial . The hardest part was figuring out that I needed to set up a virtual network in order to publicly expose multiple ports. Once the server was up and running, configuring OBS was relatively easy as well. I\u0027ll try to put together a more detailed writeup so I remember what I did for next time as well as help others who may be interested in self-hosting their own livestream server.","contentType":"notes","url":"https://www.lqdev.me/notes/owncast-azure-container-apps-deployment","date":"2025-01-07 19:52 -05:00","tags":["owncast","livestream","azure","selfhost","fediverse","obs","containers","aca"],"summary":"A few months ago, I started tinkering with Owncast. In that case, I was figuring out how I could use .NET Aspire and Blazor to configure an Owncast server and embed the stream on a website . Although...","keywords":["server","owncast","network","website","aspire","virtual","figuring","tutorial","hardest","part"]},{"id":"bookmarks-agents-huyen","title":"Agents","content":"Agents \u2192 https://huyenchip.com/2025/01/07/agents.html The unprecedented capabilities of foundation models have opened the door to agentic applications that were previously unimaginable. These new capabilities make it finally possible to develop autonomous, intelligent agents to act as our assistants, coworkers, and coaches. They can help us create a website, gather data, plan a trip, do market research, manage a customer account, automate data entry, prepare us for interviews, interview our candidates, negotiate a deal, etc. The possibilities seem endless, and the potential economic value of these agents is enormous. This section will start with an overview of agents and then continue with two aspects that determine the capabilities of an agent: tools and planning. Agents, with their new modes of operations, have new modes of failure. This section will end with a discussion on how to evaluate agents to catch these failures.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/agents-huyen","date":"2025-01-07 11:23 -05:00","tags":["ai","agents"],"summary":"Agents \u2192 https://huyenchip.com/2025/01/07/agents.html The unprecedented capabilities of foundation models have opened the door to agentic applications that were previously unimaginable. These new...","keywords":["agents","capabilities","data","modes","section","failure","entry","prepare","interviews","interview"]},{"id":"bookmarks-kaggle-agents-whitepaper","title":"Agents Whitepaper","content":"Agents Whitepaper \u2192 https://www.kaggle.com/whitepaper-agents Humans are fantastic at messy pattern recognition tasks. However, they often rely on tools - like books, Google Search, or a calculator - to supplement their prior knowledge before arriving at a conclusion. Just like humans, Generative AI models can be trained to use tools to access real-time information or suggest a real-world action. For example, a model can leverage a database retrieval tool to access specific information, like a customer\u0027s purchase history, so it can generate tailored shopping recommendations. Alternatively, based on a user\u0027s query, a model can make various API calls to send an email response to a colleague or complete a financial transaction on your behalf. To do so, the model must not only have access to a set of external tools, it needs the ability to plan and execute any task in a self- directed fashion. This combination of reasoning, logic, and access to external information that are all connected to a Generative AI model invokes the concept of an agent, or a program that extends beyond the standalone capabilities of a Generative AI model. This whitepaper dives into all these and associated aspects in more detail.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/kaggle-agents-whitepaper","date":"2025-01-06 21:50 -05:00","tags":["agents","ai","whitepaper"],"summary":"Agents Whitepaper \u2192 https://www.kaggle.com/whitepaper-agents Humans are fantastic at messy pattern recognition tasks. However, they often rely on tools - like books, Google Search, or a calculator -...","keywords":["model","access","generative","information","tools","real","external","agents","humans","whitepaper"]},{"id":"notes-2025-01-05-weekly-post-summary","title":"Week of January 5, 2025 - Post Summary","content":"Blogs Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod Responses Welcome to the Public Domain in 2025 LangChain State of AI 2024 Report","contentType":"notes","url":"https://www.lqdev.me/notes/2025-01-05-weekly-post-summary","date":"2025-01-05 21:08 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Blogs Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod Responses Welcome to the Public Domain in 2025 LangChain State of AI 2024 Report","keywords":["blogs","building","reading","habit","audiobooks","librivox","antennapod","responses","welcome","public"]},{"id":"posts-listen-audiobooks-librivox-antennapod","title":"Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod","content":"Last year, I was able to complete my goal of reading 12 books or roughly one book per month. I kept track of some of the books I read in my [library page](/reviews). I enjoyed most of the books I read. However, despite hitting my goal and averaging a book a month, I didn\u0027t build up a habit which is what I would\u0027ve preferred. Many of the books I read last year, I read them during long periods of uninterrupted time, such as cross-country flights. The topic of goal setting and measuring the right thing could be an entire post, but I\u0027ll leave that for another time. Taking the learnings from last year, this year, I\u0027ll try to focus on building a reading habit rather than reading a set number of books. While I\u0027d still like to finish at least one book per month, that\u0027s a metric I\u0027ll use to track general progress rather than making it the goal. Unless I\u0027m reading something like War and Peace, I\u0027d expect that by establishing a consistent cadence, I should be able to get through at least one book every month. Also, I\u0027m only accounting for uninterrupted reading time blocks. Using audiobooks, I can continue building my reading habit while working out or doing chores. Kind of like having TV playing in the background. There is no shortage of audiobooks out there. Audible is one of the easiest ways to access audiobooks and they have a large selection to choose from. Personally, I don\u0027t use Audible. I don\u0027t like paying for a separate subscription, getting credits to \u0022buy\u0022 books that I don\u0027t really own and can only listen through the Audible app. Instead, I use Spotify. I get [15 hours of access to audiobooks per month](https://support.spotify.com/us/article/audiobooks-access-plan/) as part of my subscription. While I still don\u0027t own the books and can only listen on the Spotify app, at least I\u0027m not paying a separate subscription. Also, the 15 hour limit at this time is more than enough for me. The library is another amazing option. No subscription fees and you get access to your local library\u0027s entire digital catalog. Using the [Libby app](https://www.overdrive.com/apps/libby), you can borrow books and start listening for free. Selection though may be an issue here since you are limited to what\u0027s available through your local library. Pro-tip, universities and employers sometimes have libraries, so as an alumnus or employee, you can expand the number of books available to you by registering multiple libraries on the Libby app. Regardless of which library you choose though, you still have to go through the Libby app. The dream setup for me is, having direct access to the audiobook files so I can listen to them whenever and wherever I\u0027d like. Similar to podcasts. Thanks to LibriVox and AntennaPod, that setup is possible. In case you\u0027re interested in setting up something similar, in this post, I\u0027ll briefly explain what LibriVox and AntennaPod are and show how you can use them to start listening to free audiobooks on your phone. ## What is LibriVox? [LibriVox](https://librivox.org/) is a website that provides a collection of free audiobooks from the public domain. Thanks to a vibrant community of volunteers, you can enjoy classics such as: - [The Divine Comedy](https://librivox.org/the-divine-comedy-by-dante-alighieri/) - [The Adventures of Sherlock Holmes](https://librivox.org/the-adventures-of-sherlock-holmes/) - [Discourses of Epictetus](https://librivox.org/discourses-of-epictetus-by-epictetus/) - [Roughing It](https://librivox.org/roughing-it/) - [And many others...](https://librivox.org/search?primary_key=0\u0026search_category=title\u0026search_page=1\u0026search_form=get_results\u0026search_order=catalog_date) It\u0027s important to note that the catalog is limited to public domain works. That means you won\u0027t have access to the latest self-help, business, or current events books. However, there are still thousands of books to keep you busy for an entire lifetime and the list is constantly growining since there are new works introduced into the public domain every year. Here\u0027s [a few notable literary works that entered the public domain in 2025](https://blog.archive.org/2025/01/01/welcome-to-the-public-domain-in-2025/) as well as the complete list of [literary works from 1929](https://archive.org/details/internetarchivebooks?tab=collection\u0026query=date%3A1929). ## What is Antennapod? [AntennaPod](https://antennapod.org/) is an open-source podcast player for Android. The app has a ton of great features, such as [Podcast Index](https://podcastindex.org/) support, subscribe via RSS directly, OPML import and export, chapters, adjustable playback speed, and for the past few years, a personalized yearly summary which is computed locally on your device [(here\u0027s my summary from 2024)](/notes/antennapod-echo-2024). You can get AntennaPod from the Play Store or [F-Droid](https://f-droid.org/en/packages/de.danoeh.antennapod/) ## Keep track of new releases LibriVox provides [several feeds](https://librivox.org/pages/librivox-feeds/). Since podcast distribution is primarly done through RSS and you can subscribe to podcasts directly using RSS feeds in AntennaPod, we can use the app to stay up to date with new audiobook releases. To subscribe to the LibriVox New Release feed: 1. Copy the LibriVox New Releases RSS feed URL to your clipboard: [https://librivox.org/rss/latest_releases](https://librivox.org/rss/latest_releases) 1. In AntennaPod, open up the hamburger menu and select **Add Podcast**. 1. In the *Add Podcast* dialog, select **Add podcast by RSS address**. ![AntennaPod Add Podcast Dialog](http://cdn.lqdev.tech/files/images/add-podcast-antennapod.png) 1. Paste the LibriVox New Releases RSS feed URL into the text box and select **Confirm**. At this point, you should see the feed listed in your AntennaPod subscriptions. By subscribing the LibriVox New Release feed, AntennaPod treats the feed as a podcast, and each new release entry or audiobook is like a podcast episode. For more information on subscribing to podcasts, see the [AntennaPod documentation](https://antennapod.org/documentation/getting-started/subscribe). ## Add books to Antennapod Subscribing to the new releases feed only notifies you when new books are added, you can\u0027t listen to or download the books. To do that, you\u0027ll need to subscribe to the RSS feed of the book you want to listen to in AntennaPod. ### From New Releases Feed 1. In AntennaPod\u0027s Subscription screen, select **LibriVox\u0027s New Releases**. 1. From the list of new releases, select the book you want to listen to. For example, \u0022Room of One\u0027s Own\u0022 by Virginia Woolf. ![AntennaPod New Releases Feed](http://cdn.lqdev.tech/files/images/antennapod-librivox-new-releases.png) 1. In the audiobook\u0027s screen, select **Visit Website**. This will take you to the LibriVox audiobook\u0027s page in the browser. ![AntennaPod Audiobook Screen](http://cdn.lqdev.tech/files/images/antennapod-room-of-ones-own.png) 1. In the LibriVox audiobook page, select the book\u0027s RSS Feed. ![A Room of One\u0027s Own by Virginia Woolf LibriVox book page](http://cdn.lqdev.tech/files/images/librivox-room-ones-own.png) 1. Copy the book\u0027s feed URL to your clipboard. 1. In AntennaPod, open up the hamburger menu and select **Add Podcast**. 1. In the *Add Podcast* dialog, select **Add podcast by RSS address**. 1. Paste the book\u0027s RSS feed URL into the text box and select **Confirm**. ### Any other books The process works the same for other books but you\u0027ll have to use LibriVox as the entrypoint to browse books. 1. In your browser, navigate to LibriVox. 1. Go to the audiobook page. For example, this is the page for [Walden](https://librivox.org/walden-by-henry-david-thoreau). 1. In the audiobook page, select the book\u0027s RSS Feed. For Walden, it\u0027s [https://librivox.org/rss/549](https://librivox.org/rss/549). 1. Copy the book\u0027s feed URL to your clipboard. 1. In AntennaPod, open up the hamburger menu and select **Add Podcast**. 1. In the *Add Podcast* dialog, select **Add podcast by RSS address**. 1. Paste the book\u0027s RSS feed URL into the text box and select **Confirm**. ## Conclusion Although AntennaPod wasn\u0027t designed for audiobook listening, thanks to the flexibility of RSS feeds and the thankless work of the LibriVox community, you can easily listen to audiobooks for free on your mobile device. I plan on testing out this setup and seeing whether it helps keep my reading habit on track.","contentType":"posts","url":"https://www.lqdev.me/posts/listen-audiobooks-librivox-antennapod","date":"2025-01-05 21:04 -05:00","tags":["book","audiobook","rss","antennapod","librivox","android","podcast","publicdomain","opensource"],"summary":"Last year, I was able to complete my goal of reading 12 books or roughly one book per month. I kept track of some of the books I read in my [library page](/reviews). I enjoyed most of the books I...","keywords":["antennapod","books","librivox","feed","select","book","https://librivox","releases","podcast","audiobook"]},{"id":"reshare-internet-archive-public-domain-2025","title":"Welcome to the Public Domain in 2025","content":"Welcome to the Public Domain in 2025 \u2192 https://blog.archive.org/2025/01/01/welcome-to-the-public-domain-in-2025/ On January 1, 2025, we celebrate published works from 1929 and published sound recordings from 1924 entering the public domain! The passage of these works into the public domain celebrates our shared cultural heritage. The ability to breathe new life into long forgotten works, remix the most popular and enduring works of the time, and to better circulate the oddities we find in thrift stores, attics, and on random pockets of the internet are now freely available for us all. While not at the same blockbuster level as 2024 with Steamboat Willie\u2019s passage into the public domain, works from 1929 still inhabit strong cultural significance today. The works of 1929 continue to capture the Lost Generation\u2019s voice, the rise of sound film, and the emerging modern moment of the 1920s.","contentType":"reshare","url":"https://www.lqdev.me/responses/internet-archive-public-domain-2025","date":"2025-01-03 15:53 -05:00","tags":["internetarchive","publicdomain"],"summary":"Welcome to the Public Domain in 2025 \u2192 https://blog.archive.org/2025/01/01/welcome-to-the-public-domain-in-2025/ On January 1, 2025, we celebrate published works from 1929 and published sound...","keywords":["works","public","domain","1929","from","passage","sound","published","2025","cultural"]},{"id":"reshare-langchain-state-of-ai-2024","title":"LangChain State of AI 2024 Report","content":"LangChain State of AI 2024 Report \u2192 https://blog.langchain.dev/langchain-state-of-ai-2024/ In 2024, developers leaned into complexity with multi-step agents, sharpened efficiency by doing more with fewer LLM calls, and added quality checks to their apps using methods of feedback and evaluation.","contentType":"reshare","url":"https://www.lqdev.me/responses/langchain-state-of-ai-2024","date":"2024-12-29 12:12 -05:00","tags":["ai","2024","report","langchain","llm"],"summary":"LangChain State of AI 2024 Report \u2192 https://blog.langchain.dev/langchain-state-of-ai-2024/ In 2024, developers leaned into complexity with multi-step agents, sharpened efficiency by doing more with...","keywords":["langchain","state","2024","methods","using","apps","checks","quality","added","calls"]},{"id":"reshare-microsoft-markitdown","title":"MarkItDown - Convert files to Markdown","content":"MarkItDown - Convert files to Markdown \u2192 https://github.com/microsoft/markitdown MarkItDown is a utility for converting various files to Markdown (e.g., for indexing, text analysis, etc). It supports: PDF PowerPoint Word Excel Images (EXIF metadata and OCR) Audio (EXIF metadata and speech transcription) HTML Text-based formats (CSV, JSON, XML) ZIP files (iterates over contents)","contentType":"reshare","url":"https://www.lqdev.me/responses/microsoft-markitdown","date":"2024-12-18 21:43 -05:00","tags":["markdown","etl","data","python","ai","microsoft"],"summary":"MarkItDown - Convert files to Markdown \u2192 https://github.com/microsoft/markitdown MarkItDown is a utility for converting various files to Markdown (e.g., for indexing, text analysis, etc). It...","keywords":["files","markitdown","markdown","text","metadata","exif","json","formats","based","html"]},{"id":"reshare-agentic-fediverse","title":"Agentic Fediverse","content":"Agentic Fediverse \u2192 https://github.com/muni-town/agentic-fediverse The \u0026quot;Agentic Fediverse\u0026quot; is the idea of a new kind of network federation, a complementary iteration on the concept of federation currently established with ActivityPub. It\u0027s an experiment and something that we in Muni Town are still trying to define more concretely. Tenets We are still working on defining the core tenets of the agentic fediverse, but here is what we have so far. An agentic fediverse is... A fediverse of agents; agent-centric, as opposed to server-centric. Local-first; solve local problems for local people. Accessible by default; inaccessibility is a bug. Systematically consensual; architected on the basis of informed consent.","contentType":"reshare","url":"https://www.lqdev.me/responses/agentic-fediverse","date":"2024-12-17 19:44 -05:00","tags":["fediverse","munitown","agents","agentic","activitypub","protocols","socialweb"],"summary":"Agentic Fediverse \u2192 https://github.com/muni-town/agentic-fediverse The \u0026quot;Agentic Fediverse\u0026quot; is the idea of a new kind of network federation, a complementary iteration on the concept of...","keywords":["fediverse","agentic","local","centric","federation","tenets","inaccessibility","defining","core","here"]},{"id":"reshare-hello-social-web","title":"A New Social Web - Hello, Social Web","content":"A New Social Web - Hello, Social Web \u2192 https://www.anew.social/hello-social-web/ We\u0027re A New Social, a new non-profit organization focused on building cross-protocol tools and services for the open social web. Our mission is to liberate people\u0027s networks from their platforms, enabling The Last Network Effect and leveling the playing field across the open social web. The first project we\u0027ll take on to accomplish this mission is Bridgy Fed , a service that enables users of ActivityPub-based platforms like Mastodon, ATProto-based platforms like Bluesky, and websites to interact and engage across ecosystems.","contentType":"reshare","url":"https://www.lqdev.me/responses/hello-social-web","date":"2024-12-17 19:39 -05:00","tags":["anewsocial","fediverse","socialmedia","protocols","atproto","activitypub","socialweb","personalweb"],"summary":"A New Social Web - Hello, Social Web \u2192 https://www.anew.social/hello-social-web/ We\u0027re A New Social, a new non-profit organization focused on building cross-protocol tools and services for the open...","keywords":["social","platforms","across","based","mission","open","leveling","playing","field","project"]},{"id":"reshare-streambuilder-tumblr-framework-building-feed","title":"StreamBuilder: our open-source framework for powering your dashboard","content":"StreamBuilder: our open-source framework for powering your dashboard \u2192 https://engineering.tumblr.com/post/722102563011493888/streambuilder-our-open-source-framework-for Interesting post. Whether building your feeds or plugging into existing platforms like Bluesky , this framework could serve as a good starting point. Today, we\u2019re abnormally jazzed to announce that we\u2019re open-sourcing the custom framework we built to power your dashboard on Tumblr. We call it StreamBuilder, and we\u2019ve been using it for many years. StreamBuilder has a lot going on. The primary architecture centers around \u201Cstreams\u201D of content: whether posts from a blog, a list of blogs you\u2019re following, posts using a specific tag, or posts relating to a search. These are separate kinds of streams, which can be mixed together, filtered based on certain criteria, ranked for relevancy or engagement likelihood, and more. So, what\u2019s included in the box? The full framework library of code that we use today, on Tumblr, to power almost every feed of content you see on the platform. A YAML syntax for composing streams of content, and how to filter, inject, and rank them. Abstractions for programmatically composing, filtering, ranking, injecting, and debugging streams. Abstractions for composing streams together\u2014such as with carousels, for streams-within-streams. An abstraction for cursor-based pagination for complex stream templates. Unit tests covering the public interface for the library and most of the underlying code. GitHub Repo","contentType":"reshare","url":"https://www.lqdev.me/responses/streambuilder-tumblr-framework-building-feed","date":"2024-12-17 19:03 -05:00","tags":["tumblr","algorithms","feed","socialmedia"],"summary":"StreamBuilder: our open-source framework for powering your dashboard \u2192 https://engineering.tumblr.com/post/722102563011493888/streambuilder-our-open-source-framework-for Interesting post. Whether...","keywords":["streams","framework","open","composing","tumblr","posts","abstractions","streambuilder","power","based"]},{"id":"posts-weird-inc-2025","title":"Weird, Inc. and 2025","content":"I don\u0027t remember how I came across Weird and their projects but I\u0027m glad I did. I appreciate their approach of using existing platforms, protocols, and practices from the yesterweb to put the personal website front and center. ## Weird, Inc First, the topic of [Weird the organization and their aspirations](https://blog.muni.town/weird-inc/). \u003E We have big aspirations for Weird. At minimum, a \u0027social network of personal websites\u0027 can be catalyzed by simply mapping and linking existing web spaces together, playing the role of social glue. On a grander scale there\u0027s potential for a pro-social reformation of the web as we know it, putting people\u0027s websites at the center of our sociodigital interactions. By aligning to the foundations of [Seth Godin\u0027s Modern Business Plan](https://seths.blog/2010/05/the-modern-business-plan/), they aim to focus on: 1. Truth - describe the world as it is 1. Assertions - how you\u2019re going to change things 1. Alternatives - what you\u0027ll do if assertions don\u0027t pan out. 1. People - who is on your team, who is going to join your team 1. Money - how much do you need and how will you spend it Below are some of the points that resonated with me. ### Truth \u003E Home ownership on the web, remarkably, is just as fraught with rent-seeking monopolists as our physical spaces. No one gets to own anything anymore. While they\u0027re mainly referring to the social web, this is is generally true across many areas of our digital spaces. \u003E Bluesky...is bringing into mainstream a nearly forgotten cornerstone of the Indie Web: Domain sovereignty \u003E \u003E In a reinvigorated social web where your domain-name is your universal address in the digital sphere, Weird is the virtual home to which that address is pointing \u003E \u003E \u0027Weird the data importer\u0027 is our wedge into the data fortresses keeping our digital identities captive. What will start as a Publish Elsewhere, Syndicate (to your) Own Site (PESOS) strategy makes way for a fully self-sovereign Publish (on your) Own Site, Syndicate Elsewhere (POSSE) end state. \u003E \u003E Imagine a WordPress-like website engine that aggregates all of your digital personas into a unified mosaic, completely under your control by combination of cloud backing and local storage. ### Alternatives If the assertions don\u0027t pan out, the alternatives that I think might be interesting: - Local-first knowledge base with cloud synchronization. [AnyType](https://anytype.io/) comes to mind. - Cloud \u002B local archiver for many other 3rd party accounts such as Bluesky, Mastodon, Discourse etc. This might make PESOS scenarios easier. Of course, this only works if the 3rd party services allow developer access to their APIs. ## Weird in 2025 Speaking specifically about [Weird the product and what the team plans of shipping in 2025](https://blog.muni.town/weird-2025/). \u003E Weird is best understood as a single-page website generator ### Q1 and Q2 \u003E \u0027Weird the data importer\u0027 is our wedge into the data fortresses keeping our digital identities captive. \u003E \u003E \u0027Weird: \u20B1ESOS-edition\u0027 aggregates all of your digital personas into a harmonious mosaic, made completely yours by combination of cloud backing and local-first storage. \u003E \u003E Setting up a POSSE pipeline however requires not just the right infrastructure...but also a big shift in workflow. \u003E \u003E Most people will probably want a combination of both. \u003E \u003E Hence our commitment to both models, starting with the least laborious one [PESOS]. Personally, [I prefer POSSE](/posts/rss-to-mastodon-posse-azure-logic-apps/), but it\u0027s great to see that Weird plans on meeting people where they are. ### Late 2025 \u003E Around half-way through 2025, we\u0027re hoping to shift some of our attention away from Weird as an individualistic publishing tool to Weird as a network engine. \u003E \u003E We think there\u0027s massive untapped potential in the web 1.0 concept of webrings, and we fully intend to bring them back. \u003E \u003E All of this culminates in one of the key motivations behind the Weird project: [A Network of Shared Purpose](https://github.com/muni-town/weird/issues/3). I\u0027m not sure yet what this looks like but making it easier to discover and build networks of personal websites is a good problem to try to solve. ## Conclusion I wish them luck and success in what they\u0027re aiming to accomplish. I\u0027m excited to see more projects like this sprouting to shake up the status-quo in our digital spaces.","contentType":"posts","url":"https://www.lqdev.me/posts/weird-inc-2025","date":"2024-12-17 18:41 -05:00","tags":["weird","cms","munitown","2025","indieweb","posse","pesos","personalweb","internet"],"summary":"I don\u0027t remember how I came across Weird and their projects but I\u0027m glad I did. I appreciate their approach of using existing platforms, protocols, and practices from the yesterweb to put the...","keywords":["weird","digital","social","2025","local","cloud","people","data","they","spaces"]},{"id":"bookmarks-openverse","title":"Openverse","content":"Openverse \u2192 https://openverse.org/ Openverse is a tool that allows openly licensed and public domain works to be discovered and used by everyone. Openverse searches across more than 800 million images and audio tracks from open APIs and the Common Crawl dataset. We aggregate works from multiple public repositories, and facilitate reuse through features like one-click attribution. Openverse is the successor to CC Search which was launched by Creative Commons in 2019","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/openverse","date":"2024-12-17 16:53 -05:00","tags":["openverse","opensource","creativecommons"],"summary":"Openverse \u2192 https://openverse.org/ Openverse is a tool that allows openly licensed and public domain works to be discovered and used by everyone. Openverse searches across more than 800 million...","keywords":["openverse","works","public","from","allows","crawl","dataset","aggregate","multiple","repositories"]},{"id":"reshare-state-of-the-word-2024-wordpress","title":"State of the Word 2024","content":"State of the Word 2024 \u2192 https://www.youtube.com/watch?v=KLybH5YvIPQ Cool retrospective and announcements from this WordPress\u0027 State of the Word 2024. Focused mode and templates look interesting.","contentType":"reshare","url":"https://www.lqdev.me/responses/state-of-the-word-2024-wordpress","date":"2024-12-17 16:50 -05:00","tags":["wordpress","conference","blogging","website","opensource"],"summary":"State of the Word 2024 \u2192 https://www.youtube.com/watch?v=KLybH5YvIPQ Cool retrospective and announcements from this WordPress\u0027 State of the Word 2024. Focused mode and templates look interesting.","keywords":["state","word","2024","templates","mode","focused","wordpress","from","announcements","retrospective"]},{"id":"reshare-tumblr-adding-reddit-like-communities","title":"Tumblr adds Reddit-like \u2018communities\u2019","content":"Tumblr adds Reddit-like \u2018communities\u2019 \u2192 https://www.theverge.com/2024/12/13/24320336/tumblr-communities-reddit-topics-groups Tumblr is introducing a new Community feature \u2014 in-app groups organized by topic or interest. Communities are similar to subreddits or Facebook groups and had previously been in beta. Given recent announcements like Tumblr replatforming on top of WordPress and WordPress adding support for ActivityPub , I could see communities becoming not only an alternative to Reddit, but also Lemmy in the Fediverse.","contentType":"reshare","url":"https://www.lqdev.me/responses/tumblr-adding-reddit-like-communities","date":"2024-12-16 22:00 -05:00","tags":["tumblr","reddit","communities","automattic","wordpress","activitypub","fediverse","lemmy"],"summary":"Tumblr adds Reddit-like \u2018communities\u2019 \u2192 https://www.theverge.com/2024/12/13/24320336/tumblr-communities-reddit-topics-groups Tumblr is introducing a new Community feature \u2014 in-app groups organized by...","keywords":["tumblr","reddit","communities","groups","wordpress","beta","given","recent","announcements","replatforming"]},{"id":"reshare-youtube-chat-app-ai-dotnet","title":"Build a YouTube chat app with .NET","content":"Build a YouTube chat app with .NET \u2192 https://jordanmatthiesen.me/programming/build-youtube-chatbot-dotnet/ Great post from jordanmatthiesen.me . Recently on a trip for a tech conference I created a YouTube chat app using .NET and AI. This is part of my exploration into creating a larger app for chatting about .NET AI development (leveraging docs, presentations, and sample code my team has been working on at Microsoft).","contentType":"reshare","url":"https://www.lqdev.me/responses/youtube-chat-app-ai-dotnet","date":"2024-12-16 21:22 -05:00","tags":["dotnet","ai","youtube","chat","microsoft.extensions.ai","openai"],"summary":"Build a YouTube chat app with .NET \u2192 https://jordanmatthiesen.me/programming/build-youtube-chatbot-dotnet/ Great post from jordanmatthiesen.me . Recently on a trip for a tech conference I created a...","keywords":["youtube","chat","build","team","code","sample","presentations","docs","leveraging","development"]},{"id":"notes-2024-12-15-weekly-post-summary","title":"Week of December 15, 2024 - Post Summary","content":"Blogs Starter Packs with OPML and RSS Notes Antennapod Echo 2024 Surrender to the quantum conscience Responses Sora is here","contentType":"notes","url":"https://www.lqdev.me/notes/2024-12-15-weekly-post-summary","date":"2024-12-15 13:26 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Blogs Starter Packs with OPML and RSS Notes Antennapod Echo 2024 Surrender to the quantum conscience Responses Sora is here","keywords":["blogs","starter","packs","opml","notes","antennapod","echo","2024","surrender","quantum"]},{"id":"notes-surrender-quantum-conscience","title":"Surrender to the quantum conscience","content":"I\u0027m trying.","contentType":"notes","url":"https://www.lqdev.me/notes/surrender-quantum-conscience","date":"2024-12-12 20:08 -05:00","tags":["quantum","conscience","message"],"summary":"I\u0027m trying.","keywords":["trying"]},{"id":"notes-antennapod-echo-2024","title":"Antennapod Echo 2024","content":"That\u0027s a lot of podcasts!","contentType":"notes","url":"https://www.lqdev.me/notes/antennapod-echo-2024","date":"2024-12-11 20:52 -05:00","tags":["podcasts","opensource","antennapod","stats"],"summary":"That\u0027s a lot of podcasts!","keywords":["podcasts"]},{"id":"reshare-sora-released","title":"Sora is here","content":"Sora is here \u2192 https://openai.com/index/sora-is-here/ Our video generation model is rolling out at sora.com\u2060 . Users can generate videos up to 1080p resolution, up to 20 sec long, and in widescreen, vertical or square aspect ratios. You can bring your own assets to extend, remix, and blend, or generate entirely new content from text. The version of Sora we are deploying has many limitations. It often generates unrealistic physics and struggles with complex actions over long durations. We\u2019re introducing our video generation technology now to give society time to explore its possibilities and co-develop norms and safeguards that ensure it\u2019s used responsibly as the field advances.","contentType":"reshare","url":"https://www.lqdev.me/responses/sora-released","date":"2024-12-09 21:34 -05:00","tags":["sora","openai","ai"],"summary":"Sora is here \u2192 https://openai.com/index/sora-is-here/ Our video generation model is rolling out at sora.com\u2060 . Users can generate videos up to 1080p resolution, up to 20 sec long, and in widescreen,...","keywords":["sora","video","generation","generate","long","limitations","often","generates","unrealistic","physics"]},{"id":"notes-2024-12-09-weekly-post-summary","title":"Week of December 09, 2024 - Post Summary","content":"Blogs Starter Packs with OPML and RSS Notes Spotify Wrapped 2024 Spotify Wrapped 2024 AI Generated Podcast Responses How I turned my blog into a social media hub","contentType":"notes","url":"https://www.lqdev.me/notes/2024-12-09-weekly-post-summary","date":"2024-12-08 19:45 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Blogs Starter Packs with OPML and RSS Notes Spotify Wrapped 2024 Spotify Wrapped 2024 AI Generated Podcast Responses How I turned my blog into a social media hub","keywords":["spotify","wrapped","2024","blogs","starter","packs","opml","notes","generated","podcast"]},{"id":"posts-rss-opml-starter-packs","title":"Starter Packs with OPML and RSS","content":"One of the things I like about Bluesky is the [Starter Pack feature](https://bsky.social/about/blog/06-26-2024-starter-packs). In a gist, a Starter Pack is a collection of feeds. Bluesky users can: - Create starter packs - Share starter packs - Subscribe to starter packs Unfortunately, Starter Packs are limited to Bluesky. Or are they? As mentioned, starter packs are a collection of feeds that others can create, share, and subscribe to. [Bluesky supports RSS](/responses/bluesky-rss-support/), which means you could organize the feeds using an OPML file that you can share with others and others can subscribe to. The benefits of this is, you can continue to keep up with activity on Bluesky from the feed reader of your choice without being required to have an account on Bluesky. More importantly, because RSS and OPML are open standards, you\u0027re not limited to building starter packs for Bluesky. You can create, share, and subscribe to starter packs for any platform that supports RSS. That includes blogs, podcasts, forums, YouTube, Mastodon, etc. [Manton seems to have something similar in mind](https://www.manton.org/2024/12/07/with-the-success.html) as a means of building on open standards that make it easy for Micro.blog to interop with various platforms. If you\u0027re interested in what that might look like in practice, check out my \u0022starter packs\u0022 which you can subscribe to using your RSS reader of choice and the provided OPML files. - [Blogs](/collections/blogroll) - [Podcasts](/collections/podroll) - [YouTube](/collections/youtube) - [Forums](/collections/forums) I\u0027m still working on similar collections for Mastodon and Bluesky but the same concept applies. Although these are just simple examples, it shows the importance of building on open standards and the open web. Doing so introduces more freedom for creators and communities. Here are other \u0022starter packs\u0022 you might consider subscribing to. - [People and Blogs](https://www.thingelstad.com/2024/12/03/people-blogs-list.html) - [Blogroll Club](https://blogroll.club/faq/) If this is interesting to you, [Feedland](https://feedland.org/) might be a project worth checking out.","contentType":"posts","url":"https://www.lqdev.me/posts/rss-opml-starter-packs","date":"2024-12-08 19:40 -05:00","tags":["rss","opml","indieweb","starterpacks","community","bluesky","socialmedia","openweb","protocols","openstandards","feedland"],"summary":"One of the things I like about Bluesky is the [Starter Pack feature](https://bsky.social/about/blog/06-26-2024-starter-packs). In a gist, a Starter Pack is a collection of feeds. Bluesky users can: -...","keywords":["starter","packs","bluesky","subscribe","open","blogs","share","standards","building","others"]},{"id":"reshare-how-i-turned-website-social-media-hub-newman","title":"How I turned my blog into a social media hub","content":"How I turned my blog into a social media hub \u2192 https://www.fastcompany.com/91235471/how-i-turned-my-blog-into-a-social-media-hub ...my blog has become a central hub for almost everything I write online, including nearly all my social media posts, links to my published work on other sites, and the occasional freestanding blog post. While this approach has a few trade-offs, I feel better about posting in the first place knowing that I\u2019m doing it on my own terms. There\u2019s also just something deeply satisfying about seeing a running feed of my own writing, on my own website, with my own design and presentation. A social network alone can\u2019t replicate that, but the POSSE approach offers the best of both worlds. I\u0027ve enjoyed seeing over the past few weeks, as people seek new online platforms, the personal website is being rediscovered.","contentType":"reshare","url":"https://www.lqdev.me/responses/how-i-turned-website-social-media-hub-newman","date":"2024-12-05 19:22 -05:00","tags":["indieweb","social","microblog","blogging","internet","web","socialmedia"],"summary":"How I turned my blog into a social media hub \u2192 https://www.fastcompany.com/91235471/how-i-turned-my-blog-into-a-social-media-hub ...my blog has become a central hub for almost everything I write...","keywords":["blog","social","media","turned","seeing","online","website","approach","presentation","design"]},{"id":"notes-spotify-wrapped-2024-podcast-notebook-lm","title":"Spotify Wrapped 2024 AI Generated Podcast","content":"While looking at Spotify Wrapped this year , I completely missed the AI-generated podcast that came along with it. As cool as this is, it should\u0027ve been opt-in . Although I would hope that when I clicked \u0026quot;I Agree\u0026quot; on the user agreement, the use of my data in this way is covered (for Spotify\u0027s sake), it\u0027s not clear or easy to find: What data was shared with Google\u0027s NotebookLM What Google will do with that data There\u0027s a reason I purposefully haven\u0027t linked Spotify to other accounts. I know there\u0027s probably hundreds of worse ways my data is being used I don\u0027t know about. This one though was hard to miss and a sad reminder that often you don\u0027t learn about how your data is being used until after the fact. Cases like this are not really a big deal. So what if you know I was going through a \u0026quot;Witchy Jam Band Phsychedelic Rock\u0026quot; phase in September? Where it does matter though is when it involves more personal and sensitive data. Again, because we don\u0027t realize it until after the fact, it\u0027s not until then that we learn what we\u0027re giving up when we make the simple, often uninformed, decision to select \u0026quot;I Agree\u0026quot;. --\u003E","contentType":"notes","url":"https://www.lqdev.me/notes/spotify-wrapped-2024-podcast-notebook-lm","date":"2024-12-05 09:02 -05:00","tags":["spotify","ai","spotifywrapped","google","notebooklm","privacy","podcast","generativeai","genai"],"summary":"While looking at Spotify Wrapped this year , I completely missed the AI-generated podcast that came along with it. As cool as this is, it should\u0027ve been opt-in . Although I would hope that when I...","keywords":["data","spotify","\u0026quot","until","agree\u0026quot","google","though","fact","used","often"]},{"id":"notes-spotify-wrapped-2024","title":"Spotify Wrapped 2024","content":"It\u0027s finally here! I can\u0027t say I\u0027m too surprised by the results. Compared to last year, I nearly doubled my listening minutes. Also, none of the artists from the previous year made it onto this list. Although, I\u0027m not sure how Buffalo Springfield worked its way into the list of top artists. I love how my listening habits from September are described, \u0026quot;Witchy Jam Band Psychedelic Rock\u0026quot;.","contentType":"notes","url":"https://www.lqdev.me/notes/spotify-wrapped-2024","date":"2024-12-04 08:39 -05:00","tags":["music","spotify","spotifywrapped","stats"],"summary":"It\u0027s finally here! I can\u0027t say I\u0027m too surprised by the results. Compared to last year, I nearly doubled my listening minutes. Also, none of the artists from the previous year made it onto this list....","keywords":["list","from","artists","year","listening","sure","buffalo","springfield","worked","love"]},{"id":"notes-running-linux-gui-apps-windows-wsl-amazing","title":"Running Linux GUI Apps on Windows (WSLg) is amazing!","content":"So you might\u0027ve heard of Windows Subsystem for Linux (WSL), but did you know you can also run Linux GUI apps with WSLg ? While trying to install the Element Desktop client on my Spandragon X Elite Windows device, I quickly realized you can\u0027t because of an issue with Electron . WSL is supported on ARM64 Windows devices. This means, it should also support GUI apps. I decided to try and install the Element Desktop client for Linux in WSL and this was the result!","contentType":"notes","url":"https://www.lqdev.me/notes/running-linux-gui-apps-windows-wsl-amazing","date":"2024-11-27 20:48 -05:00","tags":["wsl","linux","windows","gui","element","matrix"],"summary":"So you might\u0027ve heard of Windows Subsystem for Linux (WSL), but did you know you can also run Linux GUI apps with WSLg ? While trying to install the Element Desktop client on my Spandragon X Elite...","keywords":["windows","linux","also","apps","client","desktop","install","element","heard","electron"]},{"id":"notes-2024-11-25-weekly-post-summary","title":"Week of November 25, 2024 - Post Summary","content":"Notes Mastodon instance working again Week of November 18, 2024 - Post Summary Using my domain for discovery in Mastodon Mastodon instance now supports OpenGraph creator tag Responses Digital minimalism: A moral duty? Jimmy The Gambler Implement ActivityPub on a static site series by maho.dev","contentType":"notes","url":"https://www.lqdev.me/notes/2024-11-25-weekly-post-summary","date":"2024-11-24 19:41 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes Mastodon instance working again Week of November 18, 2024 - Post Summary Using my domain for discovery in Mastodon Mastodon instance now supports OpenGraph creator tag Responses Digital...","keywords":["mastodon","instance","notes","site","static","activitypub","implement","gambler","jimmy","duty"]},{"id":"reshare-jimmy-gambler-psu","title":"Jimmy The Gambler","content":"Jimmy The Gambler \u2192 https://www.yardbarker.com/college_football/articles/jimmy_the_gambler_made_all_the_difference_for_penn_state/s1_17286_41290206 \uD83D\uDE02 \uD83D\uDE02 \uD83D\uDE02 I don\u0027t think I\u0027ve ever heard James Franklin called a gambler. The game against Minnesota was too close for comfort, but a win is a win. In many ways, thanks to Jimmy, The Gambler.","contentType":"reshare","url":"https://www.lqdev.me/responses/jimmy-gambler-psu","date":"2024-11-24 19:36 -05:00","tags":["pennstate","ncaaf","collegefootball","psu"],"summary":"Jimmy The Gambler \u2192 https://www.yardbarker.com/college_football/articles/jimmy_the_gambler_made_all_the_difference_for_penn_state/s1_17286_41290206 \uD83D\uDE02 \uD83D\uDE02 \uD83D\uDE02 I don\u0027t think I\u0027ve ever heard James...","keywords":["gambler","jimmy","many","comfort","close","minnesota","against","game","called","franklin"]},{"id":"reshare-implement-activitypub-static-site-series-maho","title":"Implement ActivityPub on a static site series by maho.dev","content":"Implement ActivityPub on a static site series by maho.dev \u2192 https://maho.dev/2024/02/a-guide-to-implement-activitypub-in-a-static-site-or-any-website/ I\u0027ve been following along with this amazing series from Maho on implementing ActivityPub on static websites. As I think about what I want out of my website and the way I engage in the Fediverse, there\u0027s a lot of overlap. Today, what I mainly use my Mastodon instance for is: Having a Fediverse presence on a self-hosted Mastodon instance. Cross-posting posts on by website using the POSSE pattern . Although I have learned a ton from self-hosting my own Mastodon instance, neither of the points listed above require me self-hosting or even having an account on someone else\u0027s instance. Yesterday, I took the first step in linking my Fediverse presence to my domain . That fulfills my first requirement. I rarely post original content on Mastodon and for consumption, I already subscribe to accounts and tags via RSS . Therefore, the second requirement is one that can naturally occur without having to use a Mastodon instance as an intermediary. I can just post on my website and because my posts can show up in my outbox, there\u0027s no need to POSSE. I don\u0027t want my presence to be limited to Mastodon though, but since I plan on supporting media, reviews, and other types of posts, I expect my posts to be accessible across other platforms on the Fediverse like Pixelfed, Bookwyrm, and many others. Since my website and website features are built using .NET, Maho\u0027s guide simplifies my implementation. Because I\u0027m not using any of the existing static site generators and rolled my own , that means that there\u0027s going to be some effort and customizations required on my end. It would no different from my custom Webmentions implementation though. My hope is that I will save time I often spend maintaining the server as well as money since I no longer need to rent a server to host my instance. At the same time, I get to learn and contribute to building a more open, decentralized, and personal web.","contentType":"reshare","url":"https://www.lqdev.me/responses/implement-activitypub-static-site-series-maho","date":"2024-11-19 10:03 -05:00","tags":["acvititypub","fediverse","staticsite","blogging","indieweb","openweb","personalweb","dotnet","internet"],"summary":"Implement ActivityPub on a static site series by maho.dev \u2192 https://maho.dev/2024/02/a-guide-to-implement-activitypub-in-a-static-site-or-any-website/ I\u0027ve been following along with this amazing...","keywords":["mastodon","instance","website","static","fediverse","posts","activitypub","site","maho","since"]},{"id":"reshare-digital-minimalism-moral-duty-deep-questions","title":"Digital minimalism: A moral duty?","content":"Digital minimalism: A moral duty? \u2192 https://www.youtube.com/watch?v=KK9ktzbBDPE I saw a reference to this paper yesterday and added it to my Read-It-Later list. Here\u0027s a link to the original paper: https://philpapers.org/archive/AYLITA.pdf Chances are, I wasnt going to get to it until way later with the backlog of other things I want to read later. Lucky for me, that was the subject of the latest Deep Questions episode. Cal does a nice job breaking down the main arguments of the paper. The main takeaway for me was, technology is a great tool. However, when it robs you of your rational decision making abilities and pushes you to compulsive behaviors that move you away from the habits and practices that fuel your growth, it\u0027s a problem. For example, lets say you want to build a habit of reading or working out. Unfortunately, when you , time, attention, and energy that would\u0027ve otherwise been spent on those habits is now diverted towards the distraction. That is irrational and goes against your desire to grow, yet making the rational choice is where most of us struggle. Although the paper focuses on our digital lives, I can see other ways similar arguments can be applied. I\u0027ve also included the audio version as well: https://www.buzzsprout.com/1121972/episodes/16124495-ep-327-would-kant-use-tiktok","contentType":"reshare","url":"https://www.lqdev.me/responses/digital-minimalism-moral-duty-deep-questions","date":"2024-11-19 09:56 -05:00","tags":["digitalminimalism","postcast","deepquestions","philosophy","calnewport"],"summary":"Digital minimalism: A moral duty? \u2192 https://www.youtube.com/watch?v=KK9ktzbBDPE I saw a reference to this paper yesterday and added it to my Read-It-Later list. Here\u0027s a link to the original paper:...","keywords":["later","paper","digital","habits","rational","arguments","main","want","read","making"]},{"id":"notes-using-domain-mastodon-discovery","title":"Using my domain for discovery in Mastodon","content":"For a while, I\u0027ve known that I can use my domain and the WebFinger protocol to aid with discovery in the Fediverse, specifically Mastodon. I tried implementing this feature on my website a while back but was unsuccessful. Last night, I got an itch to try it again and got it working. Basically, what I was doing wrong the first time was, I was trying to use lqdev.me as the domain. Currently, my domain is not set up to use apex / naked domains. Therefore, it makes sense why that didn\u0027t work. Once I used www.lqdev.me , everything seemed to be working fine. The implementation isn\u0027t perfect, because technically you can use any username. Scott Hanselman does a nice job explaining that . The important part though is that my persence on the Fediverse is now directly linked to my domain, just like it is with Bluesky . This goes beyond the link verification Mastodon provides. As a result, it provides more flexibility and opportunities to shape my presence on the Fediverse. If you\u0027re interested in implementing it for yourself, there\u0027s a ton of guides, but here\u0027s the one I used: Use your own user @ domain for Mastodon discoverability . I don\u0027t like the www subdomain. So far I\u0027ve put up with it for a while but this might be the forcing function to finally getting the apex domain setup. In the meantime, you can search my account in Mastodon using @lqdev@www.lqdev.me .","contentType":"notes","url":"https://www.lqdev.me/notes/using-domain-mastodon-discovery","date":"2024-11-19 09:35 -05:00","tags":["mastodon","fediverse","indieweb","openweb","web","smallweb","cdn","internet"],"summary":"For a while, I\u0027ve known that I can use my domain and the WebFinger protocol to aid with discovery in the Fediverse, specifically Mastodon. I tried implementing this feature on my website a while back...","keywords":["domain","mastodon","while","fediverse","lqdev","apex","working","implementing","provides","persence"]},{"id":"notes-mastodon-instance-supports-opengraph-creator-tag","title":"Mastodon instance now supports OpenGraph creator tag","content":"In version 4.3, Mastodon introduced OpenGraph creator tags to link to an author\u0027s Fediverse account from within a post. I had implemented it on my website a while back. However, because my instance wasn\u0027t using a version that supported it, it didn\u0027t show up on my instance. Since upgrading to 4.3.1, I can now see my posts making use of the creator tag feature. As you can see from the original post, implementing it was relatively trivial. Another win for open protocols and standards.","contentType":"notes","url":"https://www.lqdev.me/notes/mastodon-instance-supports-opengraph-creator-tag","date":"2024-11-19 09:25 -05:00","tags":["mastodon","fediverse","opengraph","protocols","indieweb"],"summary":"In version 4.3, Mastodon introduced OpenGraph creator tags to link to an author\u0027s Fediverse account from within a post. I had implemented it on my website a while back. However, because my instance...","keywords":["version","post","from","creator","instance","tags","open","another","trivial","relatively"]},{"id":"notes-mastodon-working-again-v-4-3-1","title":"Mastodon instance working again","content":"After about a month of not being able to access my Mastodon instance, I\u0027m so excited to have it working again! Not sure what I did wrong with my upgrade to 4.3.0. Fortunately, the issues appears to have been on the frontend and the fix wasn\u0027t too involved. Upgrading to the latest 4.3.1 fixed the issue I ran into during my last upgrade. Thanks to my POSSE setup , I\u0027ve been able to continue posting to my website and syndicating to Mastodon. Unfortunately, since the frontend wasn\u0027t working, I was unable to access my notifications. Because the mobile apps make use of the web frontend to log in, I couldn\u0027t use the mobile app either.","contentType":"notes","url":"https://www.lqdev.me/notes/mastodon-working-again-v-4-3-1","date":"2024-11-18 17:34 -05:00","tags":["mastodon","fediverse","sysadmin","selfhost","socialmedia","social","microblog","indieweb","posse"],"summary":"After about a month of not being able to access my Mastodon instance, I\u0027m so excited to have it working again! Not sure what I did wrong with my upgrade to 4.3.0. Fortunately, the issues appears to...","keywords":["frontend","upgrade","able","access","mastodon","mobile","working","wasn","posse","setup"]},{"id":"notes-2024-11-18-weekly-post-summary","title":"Week of November 18, 2024 - Post Summary","content":"Notes Upgraded to .NET 9 Mike Tyson vs Jake Paul Responses Thundercat on Tyler\u0027s Chromakopia album? Day 1 of .NET Conf .NET Conf 2024 - Session Recordings Why YOU Should Make A Website","contentType":"notes","url":"https://www.lqdev.me/notes/2024-11-18-weekly-post-summary","date":"2024-11-17 19:56 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes Upgraded to .NET 9 Mike Tyson vs Jake Paul Responses Thundercat on Tyler\u0027s Chromakopia album? Day 1 of .NET Conf .NET Conf 2024 - Session Recordings Why YOU Should Make A Website","keywords":["conf","notes","upgraded","mike","tyson","jake","paul","responses","thundercat","tyler"]},{"id":"reshare-why-you-should-make-a-website-luvstarkei","title":"Why YOU Should Make A Website","content":"Why YOU Should Make A Website \u2192 https://youtu.be/uNlZ50b6wSs This video showed up in my feed yesterday. LuvstarKei makes some great points for building your own space on the web. With the most recent Twitter / X exodus, it\u0027s great to see folks moving to places built on more open protocols like AT (Bluesky) or ActivityPub (Fediverse). That said, those platforms are not your own, even in cases where you self-host. If tomorrow Threads, Bluesky, or Mastodon stop hosting your instance or stop development altogether, what happens to the content and your community? In the case of the Fediverse, many projects are being built in the open and there are other providers you could switch to. The switching costs though may not make it worth bothering to export and keep your content as is usually the case. That\u0027s not to say with a personal website you might not have similar issues (i.e. switching from WordPress to Ghost). However, HTML is HTML and you can rehost that anywhere on the internet. Everyone is looking for different things from the social platforms, and that\u0027s not to say they don\u0027t have their value. However, from an ownership, permanence, and creative freedom perspective, nothing beats having your own website.","contentType":"reshare","url":"https://www.lqdev.me/responses/why-you-should-make-a-website-luvstarkei","date":"2024-11-16 18:08 -05:00","tags":["website","indieweb","socialmedia","personalweb"],"summary":"Why YOU Should Make A Website \u2192 https://youtu.be/uNlZ50b6wSs This video showed up in my feed yesterday. LuvstarKei makes some great points for building your own space on the web. With the most recent...","keywords":["website","from","content","switching","stop","platforms","fediverse","bluesky","open","however"]},{"id":"notes-tyson-vs-paul","title":"Mike Tyson vs Jake Paul","content":"Amazing that Tyson at his age just did that. He earned that paycheck. That said, it was tough to watch and fortunately it didn\u0027t seem like he got hurt too bad. It looked like Jake was pulling his punches because that fight could\u0027ve ended in the 2nd / 3rd round. Overall, it\u0027s the best outcome possible. Jake can say he \u0026quot;beat\u0026quot; Mike Tyson, Tyson didn\u0027t get knocked out or hurt too bad, and they both earned a decent paycheck.","contentType":"notes","url":"https://www.lqdev.me/notes/tyson-vs-paul","date":"2024-11-16 10:36 -05:00","tags":["boxing","tyson","ironmike"],"summary":"Amazing that Tyson at his age just did that. He earned that paycheck. That said, it was tough to watch and fortunately it didn\u0027t seem like he got hurt too bad. It looked like Jake was pulling his...","keywords":["tyson","didn","earned","paycheck","jake","hurt","best","outcome","possible","amazing"]},{"id":"reshare-dotnet-conf-2024-session-playlist","title":".NET Conf 2024 - Session Recordings","content":".NET Conf 2024 - Session Recordings \u2192 https://www.youtube.com/playlist?list=PLdo4fOcmZ0oXeSG8BgCVru3zQtw_K4ANY The recordings are out for .NET Conf 2024 in case you missed any of the sessions. Here are the recordings from the sessions I participated in. Building AI Applications From Scratch: A Hands-On Guide for .NET Developers Building the Foundation: AI Fundamentals in .NET Also, there\u0027s a Premier Bonus playlist which has a ton of amazing bonus content.","contentType":"reshare","url":"https://www.lqdev.me/responses/dotnet-conf-2024-session-playlist","date":"2024-11-15 11:52 -05:00","tags":["dotnet","dotnetconf","net9","dotnet9","programming","dev"],"summary":".NET Conf 2024 - Session Recordings \u2192 https://www.youtube.com/playlist?list=PLdo4fOcmZ0oXeSG8BgCVru3zQtw_K4ANY The recordings are out for .NET Conf 2024 in case you missed any of the sessions. Here...","keywords":["recordings","conf","2024","from","bonus","sessions","building","hands","guide","developers"]},{"id":"notes-website-upgraded-dotnet-9","title":"Upgraded to .NET 9","content":"Obligatory upgrade post. Just upgraded the website to .NET 9. As with all the previous years, it involved no code changes, and all I had to do was updade the version I\u0027m using in my project file and GitHub Actions file.","contentType":"notes","url":"https://www.lqdev.me/notes/website-upgraded-dotnet-9","date":"2024-11-14 00:38 -05:00","tags":["dotnet","net9","githubactions","github","indieweb","upgrade"],"summary":"Obligatory upgrade post. Just upgraded the website to .NET 9. As with all the previous years, it involved no code changes, and all I had to do was updade the version I\u0027m using in my project file and...","keywords":["file","obligatory","project","using","version","updade","changes","github","code","years"]},{"id":"star-thundercat-chromakopia","title":"Thundercat on Tyler\u0027s Chromakopia album?","content":"Thundercat on Tyler\u0026#39;s Chromakopia album? \u2192 https://www.youtube.com/watch?v=KvR_QlXaAfU Love this new \u0026quot;making of\u0026quot; video. I was surprised to learn Thundercat is on the new album. I didn\u0027t catch which track it\u0027s on, but will be listening for it.","contentType":"star","url":"https://www.lqdev.me/responses/thundercat-chromakopia","date":"2024-11-13 02:11 -05:00","tags":["tylerthecreator","chromakopia","thundercat","music"],"summary":"Thundercat on Tyler\u0026#39;s Chromakopia album? \u2192 https://www.youtube.com/watch?v=KvR_QlXaAfU Love this new \u0026quot;making of\u0026quot; video. I was surprised to learn Thundercat is on the new album. I didn\u0027t...","keywords":["thundercat","album","which","catch","didn","learn","surprised","video","of\u0026quot","making"]},{"id":"reshare-dotnetconf-2024-day-1-success","title":"Day 1 of .NET Conf","content":"Day 1 of .NET Conf \u2192 https://www.youtube.com/watch?v=hM4ifrqF_lQ Day 1 of .NET Conf was a lot of fun. Tons of great sessions. Here\u0027s the recording of all the sessions. If you\u0027re interested in checking out my session with Jeremy, Building AI Applications From Scratch, here\u0027s a link to it. Tomorrow, I\u0027ll also get a chance to share the stage with Tarek and Tanner to talk AI Fundamentals .","contentType":"reshare","url":"https://www.lqdev.me/responses/dotnetconf-2024-day-1-success","date":"2024-11-13 01:47 -05:00","tags":["dotnet","dotnetconf","dotnet9","net9","conference","ai","aspire","microsoft"],"summary":"Day 1 of .NET Conf \u2192 https://www.youtube.com/watch?v=hM4ifrqF_lQ Day 1 of .NET Conf was a lot of fun. Tons of great sessions. Here\u0027s the recording of all the sessions. If you\u0027re interested in...","keywords":["conf","sessions","here","tanner","tarek","stage","share","chance","also","tomorrow"]},{"id":"notes-netconf-bound-2024","title":".NET Conf 2024 Bound","content":"It\u0027s that time of the year again. A new .NET release. Tons of great sessions at .NET Conf this year, which you can see in the agenda . I\u0027ll be taking part in two sessions: Building AI Applications from Scratch: A Hands-On Guide for .NET Developers Building the Foundation: AI Fundamentals in .NET","contentType":"notes","url":"https://www.lqdev.me/notes/netconf-bound-2024","date":"2024-11-10 14:29 -05:00","tags":["dotnet","dotnetconf","conference","ai","dotnet9","net9"],"summary":"It\u0027s that time of the year again. A new .NET release. Tons of great sessions at .NET Conf this year, which you can see in the agenda . I\u0027ll be taking part in two sessions: Building AI Applications...","keywords":["year","building","developers","guide","hands","scratch:","from","applications","sessions:","part"]},{"id":"reply-leo-fm-moves-to-microblog","title":"Leo.fm move to micro.blog","content":"Leo.fm move to micro.blog \u2192 https://leo.fm/2024/11/04/you-might-notice.html So cool to see you posting more on your own platform.","contentType":"reply","url":"https://www.lqdev.me/responses/leo-fm-moves-to-microblog","date":"2024-11-10 14:27 -05:00","tags":["indieweb","microblog","twit"],"summary":"Leo.fm move to micro.blog \u2192 https://leo.fm/2024/11/04/you-might-notice.html So cool to see you posting more on your own platform.","keywords":["micro","blog","https://leo","fm/2024/11/04/you","notice","html","cool","posting","more","platform"]},{"id":"notes-2024-11-10-weekly-post-summary","title":"Week of November 10, 2024 - Post Summary","content":"Started experimenting some more with posting photos this week. Notes Week of November 3, 2024 - Post Summary NYC Sunrise .NET Conf 2024 Bound Finally starting to feel like fall Responses Microsoft Research Focus: Week of October 28, 2024 Thinking LLMs: General Instruction Following with Thought Generation Microsoft Research: Introducing DRIFT Search In-Context LoRA for Diffusion Transformers Leo.fm move to micro.blog","contentType":"notes","url":"https://www.lqdev.me/notes/2024-11-10-weekly-post-summary","date":"2024-11-10 09:33 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Started experimenting some more with posting photos this week. Notes Week of November 3, 2024 - Post Summary NYC Sunrise .NET Conf 2024 Bound Finally starting to feel like fall Responses Microsoft...","keywords":["week","2024","microsoft","thinking","llms:","general","instruction","following","thought","generation"]},{"id":"notes-nyc-sunrise-fall-2024","title":"NYC Sunrise","content":"I was lucky to snap this picture the other day.","contentType":"notes","url":"https://www.lqdev.me/notes/nyc-sunrise-fall-2024","date":"2024-11-06 19:35 -05:00","tags":["photo","sunrise","nyc","city"],"summary":"I was lucky to snap this picture the other day.","keywords":["lucky","snap","picture"]},{"id":"reshare-introducing-drift-search","title":"Microsoft Research: Introducing DRIFT Search","content":"Microsoft Research: Introducing DRIFT Search \u2192 https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/ Introducing DRIFT Search: Combining global and local search methods to improve quality and efficiency DRIFT search (Dynamic Reasoning and Inference with Flexible Traversal)...builds upon Microsoft\u2019s GraphRAG technique, combining characteristics of both global and local search to generate detailed responses in a method that balances computational costs with quality outcomes. DRIFT Search: A step-by-step process Primer : When a user submits a query, DRIFT compares it to the top K most semantically relevant community reports. This generates an initial answer along with several follow-up questions, which act as a lighter version of global search. To do this, we expand the query using Hypothetical Document Embeddings (HyDE), to increase sensitivity (recall), embed the query, look up the query against all community reports, select the top K and then use the top K to try to answer the query. The aim is to leverage high-level abstractions to guide further exploration. Follow-Up : With the primer in place, DRIFT executes each follow-up using a local search variant. This yields additional intermediate answers and follow-up questions, creating a loop of refinement that continues until the search engine meets its termination criteria, which is currently configured for two iterations (further research will investigate reward functions to guide terminations). This phase represents a globally informed query refinement. Using global data structures, DRIFT navigates toward specific, relevant information within the knowledge graph even when the initial query diverges from the indexing persona. This follow-up process enables DRIFT to adjust its approach based on emerging information. Output Hierarchy : The final output is a hierarchy of questions and answers ranked on their relevance to the original query. This hierarchical structure can be customized to fit specific user needs. During benchmark testing, a naive map-reduce approach aggregated all intermediate answers, with each answer weighted equally.","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-drift-search","date":"2024-11-04 21:59 -05:00","tags":["ai","search","RAG","llm","microsoft","research","msr","graphrag"],"summary":"Microsoft Research: Introducing DRIFT Search \u2192 https://www.microsoft.com/en-us/research/blog/introducing-drift-search-combining-global-and-local-search-methods-to-improve-quality-and-efficiency/...","keywords":["drift","search","query","follow","global","local","answers","answer","questions","quality"]},{"id":"reshare-microsoft-research-focus-oct-28-2024","title":"Microsoft Research Focus: Week of October 28, 2024","content":"Microsoft Research Focus: Week of October 28, 2024 \u2192 https://www.microsoft.com/en-us/research/blog/research-focus-week-of-october-28-2024/ METAREFLECTION: Learning Instructions for Language Agents using Past Reflections Language agents are AI systems that can understand, reason and respond in natural language to complete various tasks. While the latest LLMs are capable enough to power reasonably good language agents, the closed-API model makes it hard to improve them when they perform sub-optimally. Recent studies have explored using techniques like self-reflection and prompt optimization to improve performance. Unfortunately, self-reflection can be used only during the agent\u2019s current run, while contemporary prompt optimization techniques are designed and tested to work on simple single-step agents. In a recent paper: METAREFLECTION: Learning Instructions for Language Agents using Past Reflections, researchers from Microsoft introduce a novel offline reinforcement learning technique that enhances the performance of language agents by augmenting a semantic memory based on experiential learnings from past trials. They demonstrate the efficacy of METAREFLECTION across multiple domains, including complex logical reasoning, biomedical semantic similarity, open world question answering, and vulnerability threat detection, in Infrastructure-as-Code, spanning different agent designs. METAREFLECTION boosts language agents\u2019 performance by 4% to 16.82% over the baseline agent implementations and performs on par with existing state-of-the-art prompt optimization techniques while requiring fewer LLM calls. Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping Generative AI applications rely on large, foundation models, particularly LLMs. LLMs often have tens to hundreds of billions of parameters, making them too large for a single graphics processing unit (GPU) to handle in terms of both memory and computation. Because of their size, training these models requires distributing the workload across hundreds or even thousands of GPUs. This can lead to significant communication overhead, a challenge that arises when data needs to be shared between different GPUs. In a recent paper: Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping, researchers from Microsoft introduce a system designed to enhance the efficiency of LLM training by reducing the time lost to communication between GPUs. Domino breaks down data dependencies in a single batch of training into smaller, independent pieces. These smaller pieces are processed in parallel, and communication between GPUs happens simultaneously with computation, minimizing delays. Test results comparing Domino to Megatron-LM show that Domino speeds up the training process by up to 1.3x on Nvidia DGX-H100 GPUs. OmniParser for pure vision-based GUI agent Large vision-language models (VLMs) such as GPT-4V and GPT-4o show promise in driving intelligent agent systems that operate within user interfaces (UI). However, VLMs\u2019 full potential remains underexplored in real-world applications, particularly when it comes to acting as general agents across diverse operating systems and applications with only vision input. One limiting factor is the absence of a robust technique for screen parsing which is capable of 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associating the intended action with the corresponding region on the screen. In a recent article: OmniParser for pure vision-based GUI agent, researchers from Microsoft present a compact screen parsing module that can convert UI screenshots into structured elements. OmniParser can be used with a variety of models to create agents capable of taking actions on UIs. When used with GPT-4V, OmniParser significantly improves the agent capability to generate precisely grounded actions for interface regions. OmniParser with GPT-4V agent achieved the best performance on the recently released WindowsAgentArena (opens in new tab) benchmark.","contentType":"reshare","url":"https://www.lqdev.me/responses/microsoft-research-focus-oct-28-2024","date":"2024-11-04 21:39 -05:00","tags":["ai","microsoft","research","msr","microsoftresearch"],"summary":"Microsoft Research Focus: Week of October 28, 2024 \u2192 https://www.microsoft.com/en-us/research/blog/research-focus-week-of-october-28-2024/ METAREFLECTION: Learning Instructions for Language Agents...","keywords":["agents","language","agent","training","microsoft","omniparser","gpus","communication","vision","from"]},{"id":"reshare-in-context-lora","title":"In-Context LoRA for Diffusion Transformers","content":"In-Context LoRA for Diffusion Transformers \u2192 https://arxiv.org/abs/2410.23775 Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20\u223C100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems. Repo: https://github.com/ali-vilab/In-Context-LoRA","contentType":"reshare","url":"https://www.lqdev.me/responses/in-context-lora","date":"2024-11-04 21:28 -05:00","tags":["ai","transformers","diffusers","imagegeneration","lora","finetuning"],"summary":"In-Context LoRA for Diffusion Transformers \u2192 https://arxiv.org/abs/2410.23775 Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation...","keywords":["context","task","lora","tuning","generation","image","images","dits","agnostic","pipeline"]},{"id":"reshare-thinking-llms-instruction-following-thought-generation","title":"Thinking LLMs: General Instruction Following with Thought Generation","content":"Thinking LLMs: General Instruction Following with Thought Generation \u2192 https://arxiv.org/abs/2410.10630 LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. We propose a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. We show that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning \u0026amp; problem-solving tasks.","contentType":"reshare","url":"https://www.lqdev.me/responses/thinking-llms-instruction-following-thought-generation","date":"2024-11-04 21:24 -05:00","tags":["ai","llm","research","meta","chainofthought","training","finetuning"],"summary":"Thinking LLMs: General Instruction Following with Thought Generation \u2192 https://arxiv.org/abs/2410.10630 LLMs are typically trained to answer user questions or follow instructions similarly to how...","keywords":["thinking","thought","instruction","reasoning","general","following","procedure","optimization","questions","llms"]},{"id":"notes-2024-11-03-weekly-post-summary","title":"Week of November 3, 2024 - Post Summary","content":"Fairly active week on the website. Blogs Digitize Analog Bookmarks using AI, .NET, and GitHub Models Notes Website Post Statistics - October 2024 Crate Finds - October 2024 Finally starting to feel like fall Only 365 more days until next halloween St. Chroma is a great intro Responses New Release: Spirit Box by Flying Lotus Chromakopia by Tyler The Creator Released NotebookLlama: An Open Source version of NotebookLM Haves and Choices Art the Clown Will Ring the Nasdaq Closing Bell in Times Square on Halloween!","contentType":"notes","url":"https://www.lqdev.me/notes/2024-11-03-weekly-post-summary","date":"2024-11-03 20:42 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Fairly active week on the website. Blogs Digitize Analog Bookmarks using AI, .NET, and GitHub Models Notes Website Post Statistics - October 2024 Crate Finds - October 2024 Finally starting to feel...","keywords":["halloween","website","2024","october","responses","release:","spirit","flying","lotus","chromakopia"]},{"id":"notes-finally-feels-like-fall","title":"Finally starting to feel like fall","content":"It\u0027s been unseasonably warm around these parts but this weekend it finally started to cool down. That meant hot cider and cider donuts straight from the source.","contentType":"notes","url":"https://www.lqdev.me/notes/finally-feels-like-fall","date":"2024-11-03 20:30 -05:00","tags":["fall","autumn","seasons"],"summary":"It\u0027s been unseasonably warm around these parts but this weekend it finally started to cool down. That meant hot cider and cider donuts straight from the source.","keywords":["cider","unseasonably","warm","around","parts","weekend","finally","started","cool","down"]},{"id":"notes-st-chroma-chromakopia","title":"St. Chroma is a great intro","content":"I love the album overall but St. Chroma is a great intro that sets up the rest of the album. Some of my favorite parts from the track You are the light. It\u0027s not on you, it\u0027s in you. Don\u0027t you ever in your motherfucking life dim your light for nobody. P said I could do it too, and boy, did I believe him. I built a path to freedom \u0027cause them words that he said. Give a fuck about tradition, stop impressin\u0027 the dead Don\u0027t you ever stop bein\u0027 who you are and dimmin\u0027 your light for none of these motherfuckers out here","contentType":"notes","url":"https://www.lqdev.me/notes/st-chroma-chromakopia","date":"2024-11-01 22:28 -05:00","tags":["chromakopia","tylerthecreator","music"],"summary":"I love the album overall but St. Chroma is a great intro that sets up the rest of the album. Some of my favorite parts from the track You are the light. It\u0027s not on you, it\u0027s in you. Don\u0027t you ever...","keywords":["light","said","album","ever","stop","freedom","cause","words","give","tradition"]},{"id":"reshare-haves-choices-brown-nielsen","title":"Haves and Choices","content":"Haves and Choices \u2192 https://notes.jim-nielsen.com/#2024-10-30T2141 The \u201Chave to\u201D narrative positions us as repositories of instructions made elsewhere, as if we were just programs following the code we\u2019ve been given... The \u201Cchoose to\u201D narrative has no illusions about our power and recognizes that we are small players in a bigger, and certainly unjust, world. But we are not machines. And maybe we don\u2019t like the choices available to us, maybe we wish there were others within reach. But once we accept that there are choices to make, we may notice where we have some room to maneuver, some space to play with, some opportunity or avenue or loophole we can exploit. Refusing your own agency time and again is like disconnecting from a power source\u2014the energy is still there, latent and ready, but the plug dangles inches from the outlet. ...accepting that you have some agency might hurt: you bump up against the systems that constrain your choices; you see more clearly how other people\u2019s choices limit (or expand) your own. But it keeps you connected to that source, that font of energy that is yours and no one else\u2019s. It keeps you hooked up to who you are, and to what you want. Interesting excerpts from Mandy Brown\u0027s \u0026quot;Haves and Choices\u0026quot; article to think about.","contentType":"reshare","url":"https://www.lqdev.me/responses/haves-choices-brown-nielsen","date":"2024-11-01 09:37 -05:00","tags":["life","thoughts"],"summary":"Haves and Choices \u2192 https://notes.jim-nielsen.com/#2024-10-30T2141 The \u201Chave to\u201D narrative positions us as repositories of instructions made elsewhere, as if we were just programs following the code...","keywords":["choices","from","haves","power","agency","narrative","energy","maybe","keeps","hurt:"]},{"id":"bookmarks-notebook-llama-oss-notebook-lm","title":"NotebookLlama: An Open Source version of NotebookLM","content":"NotebookLlama: An Open Source version of NotebookLM \u2192 https://github.com/meta-llama/llama-recipes/tree/main/recipes/quickstart/NotebookLlama This is cool! Already looking at how I can repurpose this sample for a project I have in mind.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/notebook-llama-oss-notebook-lm","date":"2024-11-01 09:32 -05:00","tags":["ai","sample","postcast","texttospeech","programming","sample","tutorial"],"summary":"NotebookLlama: An Open Source version of NotebookLM \u2192 https://github.com/meta-llama/llama-recipes/tree/main/recipes/quickstart/NotebookLlama This is cool! Already looking at how I can repurpose this...","keywords":["notebookllama:","open","source","version","notebooklm","https://github","com/meta","llama/llama","recipes/tree/main/recipes/quickstart/notebookllama","cool"]},{"id":"notes-crate-finds-october-2024","title":"Crate Finds - October 2024","content":"Lots of music released this month Chromakopia by Tyler The Creator Spirit Box by Flying Lotus Both amazing albums I\u0027ve now listed to multiple times from beginning to end. That said, the Crate Finds this month are light and can best be summarized as my favorite songs from Got A Story to Tell by Thee Sacred Souls . Tracks Ingo Swann by Flying Lotus Album: Ingo Swann Duration: 3:37 Listen on YouTube Backup: Listen on Spotify Lucid Girl by Thee Sacred Souls Album: Got a Story to Tell Duration: 3:24 Listen on YouTube Backup: Listen on Spotify Price I\u0027ll Pay by Thee Sacred Souls Album: Got a Story to Tell Duration: 2:59 Listen on YouTube Backup: Listen on Spotify Live For You by Thee Sacred Souls Album: Got a Story to Tell Duration: 3:30 Listen on YouTube Backup: Listen on Spotify On My Mind by Thee Sacred Souls Album: Got a Story to Tell Duration: 3:26 Listen on YouTube Backup: Listen on Spotify Waiting on the Right Time by Thee Sacred Souls Album: Got a Story to Tell Duration: 3:39 Listen on YouTube Backup: Listen on Spotify I\u0027m So Glad I Found You, Baby by Thee Sacred Souls Album: Got a Story to Tell Duration: 3:24 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-october-2024","date":"2024-11-01 09:26 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"Lots of music released this month Chromakopia by Tyler The Creator Spirit Box by Flying Lotus Both amazing albums I\u0027ve now listed to multiple times from beginning to end. That said, the Crate Finds...","keywords":["listen","spotify","thee","sacred","tell","story","album:","duration:","youtube","backup:"]},{"id":"notes-post-statistics-october-2024","title":"Website Post Statistics - October 2024","content":"It\u0027s been a while since I\u0027ve done one of these. One thing I\u0027m changing is the titles. For the other posts, the titles don\u0027t match up with the date for the metrics. For example the post for \u0026quot;July 2024\u0026quot; was actually talking about June metrics. Kind of confusing, so fixing that this month. This is my post Post Type Count Blogs 4 Notes 83 Responses 251 Overall the trend is nice. I\u0027m just under 1 response a day. Not like that\u0027s the goal, but in general I think it shows a steady stream of content I\u0027m posting on the website. With two recent blog posts published, that brings me to 5. The script ran just before I hit publish on the last blog which is why you only see four. Build your own self-hosted live streaming application with Owncast and .NET Aspire Digitize Analog Bookmarks using AI, .NET, and GitHub Models There\u0027s a few other ideas I\u0027m tinkering with, which might easily get me close to or past the 8 blog posts from last year.","contentType":"notes","url":"https://www.lqdev.me/notes/post-statistics-october-2024","date":"2024-11-01 09:25 -05:00","tags":["githubactions","cicd","blogging","stats","github"],"summary":"It\u0027s been a while since I\u0027ve done one of these. One thing I\u0027m changing is the titles. For the other posts, the titles don\u0027t match up with the date for the metrics. For example the post for \u0026quot;July...","keywords":["blog","posts","post","metrics","titles","which","last","self","build","four"]},{"id":"posts-digitize-analog-bookmarks-with-ai-dotnet","title":"Digitize Analog Bookmarks using AI, .NET, and GitHub Models","content":"## Introduction This past year I\u0027ve made more of an effort to read. I track books in my [/library](/library) page. Although I have a [Nova Air 2 E-Note device](https://shop.boox.com/products/novaair2), given the choice, I prefer physical books. Despite the conveniene of an electronic note-taking device, theres something about analog that I find hard to quit. As I read books, especially non-fiction, I annotate them in various ways. Eventually, those annotations make their way into my library page. Here\u0027s an example of those annotations for [Building A Second Brain by Thiago Forte](/reviews/building-a-second-brain). The process of transferring notes is manual and tedious. I don\u0027t always have the discipline to transfer them at periodic intervals and what ends up happening is, I get to the end of the book without transferring any notes. To make space for new books, I donate it or resell the ones I\u0027ve read. If I didn\u0027t take the time to transfer those notes, they\u0027re gone. [Slow Productivity](/reviews/slow-productivity-newport) is an example of that. I want to find a better system that\u0027s low-maintenance for keeping more of these notes and retaining knowledge I\u0027ve found valuable. Then it hit me, why not use AI? I know I could use OCR or even some of the AI models of yesteryear. The challenge is, those systems are error prone and given I don\u0027t always have the motivation to transfer notes manually, I have even less motivation to build and maintain such a system. However, vision models have advanced significantly and when paired with language models, the barrier to entry for reasoning over image data has drastically decreased. That\u0027s what led to this post. In this post, I\u0027ll show how you can use AI models like GPT-4o Mini to extract the passages I\u0027ve annotated in physical books from an image. I then format those passages in markdown to make them easy to directly copy and paste them onto the website. I know there\u0027s probably a ton of services that do this for you, but it\u0027s a lot more fun to build one from scratch. With that in mind, let\u0027s get started. You can find the source for the application in the [AIBookmarks GitHub repository](/github/AIBookmarks). Alternatively, I\u0027ve configured the repo to use GitHub CodeSpaces, so you can launch the application there as well. [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/lqdev/AIBookmarks) ## Configure access to GitHub Models I\u0027ll be using [GitHub Models](https://github.blog/news-insights/product-news/introducing-github-models/) as my AI model provider. GitHub Models provides developers with access to a catalog of AI models such as GPT-4o, Llama 3.2, Phi 3.5, and many others. Best of all, it\u0027s completely free, [though restrictions apply](https://docs.github.com/en/github-models/prototyping-with-ai-models#rate-limits). I chose GitHub Models for the following reasons: - Zero installation. Although I really like Ollama and some of the other local AI model providers, I didn\u0027t want to fill precious hard-drive space with AI models. - It\u0027s free! Since I\u0027m just prototyping, even with their limited capacity, it should be enough to prove out whether my scenario is feasible. - They provide access to multi-modal models such as GPT-4o, Llama 3.2 Vision, and Phi 3.5 Vision, which can reason over text and images, which is what I need. - I\u0027m one of the GPU poor. My [Lenovo Thinkpad X1](https://www.lenovo.com/us/en/p/laptops/thinkpad/thinkpadx1/x1-titanium-g1/22tp2x1x1t1?orgRef=https%253A%252F%252Fduckduckgo.com%252F) couldn\u0027t handle running one of the vision models. Getting set up with GitHub models is fairly easy. At minimum, it requires: 1. A GitHub Account. 1. A Personal Access Token. For more details, see the [GitHub documentation](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens). Once you have both of those, you can get started building your application. ## Create a client The sample application is a C# console application which targets .NET 9. However, the code shown here should work on the latest LTS version as well. For this solution, I use the Azure AI Inference implementation of [Microsoft.Extensions.AI](https://www.nuget.org/packages/Microsoft.Extensions.AI.AzureAIInference/). [Microsoft.Extensions.AI](https://devblogs.microsoft.com/dotnet/introducing-microsoft-extensions-ai-preview/), M.E.AI for short, is a set of abstractions that provides a common set of interfaces for interacting with AI models. Although we\u0027re using the Azure AI Inference client SDK implementation of M.E.AI, we can still use it to work with models from the GitHub Model catalog. \u0060\u0060\u0060csharp IChatClient client = new ChatCompletionsClient( endpoint: new Uri(\u0022https://models.inference.ai.azure.com\u0022), new AzureKeyCredential(Environment.GetEnvironmentVariable(\u0022GITHUB_TOKEN\u0022))) .AsChatClient(\u0022gpt-4o-mini\u0022); \u0060\u0060\u0060 ## Load your images In the repo, I have a set of sample images containing annotations. These are the images I\u0027ll send to the AI model for processing. The following is an sample from [The Creative Act by Rick Rubin](/reviews/creative-act-way-of-being-rubin). ![An image of a book with pencil markings](https://github.com/lqdev/AIBookmarks/raw/main/data/creative-act-1.jpg) 1. Load the files \u0060\u0060\u0060csharp var filePaths = Directory.GetFiles(\u0022data\u0022); \u0060\u0060\u0060 1. Create a collection to store extracted passages \u0060\u0060\u0060csharp var passages = new List (); \u0060\u0060\u0060 ## Process the images Once you have the images loaded, it\u0027s time to proces them. 1. Start by setting the system prompt. This will provide the initial guidance for the extraction task. \u0060\u0060\u0060csharp var systemPrompt = \u0022\u0022\u0022 You are an AI assistant that extracts underlined, highlighted, and marked passages from book page images. When passages have a natural continuation between pages, merge them and assign the page number where the first passage starts. \u0022\u0022\u0022; \u0060\u0060\u0060 1. Then, iterate over each of the images and process them. \u0060\u0060\u0060csharp foreach(var path in filePaths) { var file = await File.ReadAllBytesAsync(path); var messages = new List { new ChatMessage(ChatRole.System, systemPrompt), new ChatMessage(ChatRole.User, new AIContent[] { new ImageContent(file, \u0022image/jpeg\u0022), new TextContent(\u0022Extract the marked passages from the image\u0022), }) }; var response = await client.CompleteAsync \u003E(messages, options: new ChatOptions {Temperature = 0.1f}); passages.AddRange(response.Result); } \u0060\u0060\u0060 This code: 1. Loads the image as a \u0060byte[]\u0060. 1. Composes a set of messages that include: - The system prompt - The image - The user prompt instructing the model to extract the marked passages from the image 1. Sends the messages to the model for processing and returns a list of \u0060AIBookmark\u0060. An \u0060AIBookmark\u0060 is a custom class I\u0027ve defined as follows: \u0060\u0060\u0060csharp class AIBookmark { public string Text {get;set;} public int PageNumber {get;set;} } \u0060\u0060\u0060 Some of the more recent AI models support structured output, which enforce a schema on AI model outputs. You can take a look at the [OpenAI documentation](https://openai.com/index/introducing-structured-outputs-in-the-api/) for more details. It\u0027s important to note though that the functionality is not exclusive to OpenAI models. 1. Adds the extracted passages to the \u0060passages\u0060 collection. ## Format the results Once all of the files are processed by the AI model, additional processing is done to ensure that they\u0027re in the correct page order as well as formatted as markdown blockquotes. \u0060\u0060\u0060csharp var sortedPassages = passages .OrderBy(p =\u003E p.PageNumber) .Select(p =\u003E $\u0022\u003E {p.Text} (pg. {p.PageNumber})\u0022); \u0060\u0060\u0060 ## Display the results \u0060\u0060\u0060csharp foreach(var passage in sortedPassages) { Console.WriteLine(passage); Console.WriteLine(\u0022\u0022); } \u0060\u0060\u0060 For the images included in the repo\u0027s *data* directory, output might look like the following. \u0060\u0060\u0060markdown \u003E This isn\u2019t a matter of blind belief in yourself. It\u2019s a matter of experimental faith. (pg. 278) \u003E When we don\u2019t yet know where we\u2019re going, we don\u2019t wait. We move forward in the dark. If nothing we attempt yields progress, we rely on belief and will. We may take several steps backward in the sequence to move ahead. (pg. 278) \u003E If we try ten experiments and none of them work; we have a choice. We can take it personally, and think of ourselves as a failure and question our ability to solve the problem. (pg. 278) \u003E Staying in it means a commitment to remain open to what\u2019s around you. Paying attention and listening. Looking for connections and relationships in the outside world. Searching for beauty. Seeking stories. Noticing what you find interesting, what makes you lean forward. And knowing all of this is available to use next time you sit down to work, where the raw data gets put into form. (pg. 296) \u003E Just as a surfer can\u2019t control the waves, artists are at the mercy of the creative rhythms of nature. This is why it\u2019s of such great importance to remain aware and present at all times. Watching and waiting. (pg. 296) \u003E Maybe the best idea is the one you\u2019re going to come up with this evening. (pg. 297) \u0060\u0060\u0060 ## Improvements and next steps Putting the application together took me less than an hour so this is far from done. However, it does provide me with a starting point and offers validation that this could be a way to more easily capture the knowledge I\u0027m curating from physical books. Some improvements I can make here: 1. Update the system prompt with some samples to help guide the extraction. For example, in the output I shared, that last passage is not annotated. It just happens to be a mostly blank page with that quote in the center, therefore giving the illusion that the passage is important in some way. 1. Add additional information to the AIBookmark class like \u0060Index\u0060 so I can ensure order within a page is preserved. Right now page number is good enough, but I can\u0027t guarantee the correct order. An index property might help here. 1. Use a service with higher rate limits. The current rate limits wouldn\u0027t allow me to process a large number of images at once. Therefore, I\u0027d need to use a service with higher limits. Alternatively, I could make this a job that runs in the background which abides by the rate limits but I also don\u0027t have to spend money on. Given I\u0027m not using this for anything mission-critical, that\u0027d be an acceptable solution as well. 1. Refactor the solution so I can more easily swap between tasks. For example, sometimes I might want to use it with images from my bullet journal. Other times, I might want to use it with handwritten notes. Whatever the case may be, it\u0027d be good to not have to rewrite the prompts every time. Some ways I see myself using this project: 1. Periodically collect images of annotated pages and save them to cloud storage. 1. When I\u0027m done with the book, drop all the images in the *data* directory. 1. Further enrich data by condensing repetitive passages and extracting key concepts. 1. Storing this knowledge into some sort of knowledge store to make it actionable. ## Conclusion Just for fun, Tyler released all of the lyrics to his [latest album](/responses/chromakopia-tyler-the-creator-released) as [images on X](https://twitter.com/tylerthecreator/status/1852105825650708651/photo/1). With a few tweaks, I was able to repurpose this solution to extract the text from them and that worked relatively well. Just with this simple solution, there\u0027s a ton of other applications I can think of in my daily life to help bridge my analog and digital lives. What other use cases do you see yourself using something like this for? [Let me know](/contact). Happy coding!","contentType":"posts","url":"https://www.lqdev.me/posts/digitize-analog-bookmarks-with-ai-dotnet","date":"2024-10-31 21:41 -05:00","tags":["ai","dotnet","github","reading","bookmarks","analog","digital","reading","notes"],"summary":"## Introduction This past year I\u0027ve made more of an effort to read. I track books in my [/library](/library) page. Although I have a [Nova Air 2 E-Note...","keywords":["models","github","images","passages","from","more","model","page","\u0060\u0060\u0060csharp","image"]},{"id":"reshare-art-clown-nasdaq-bell-halloween","title":"Art the Clown Will Ring the Nasdaq Closing Bell in Times Square on Halloween!","content":"Art the Clown Will Ring the Nasdaq Closing Bell in Times Square on Halloween! \u2192 https://bloody-disgusting.com/the-further/3837684/terrifier-takeover-art-the-clown-will-ring-the-nasdaq-closing-bell-in-times-square-on-halloween/ With Terrifier 3 now playing in theaters and just passing $50 million worldwide, Art the Clown\u2019s domination of the Halloween season continues with a takeover of the Nasdaq MarketSite in Times Square! ...Art the Clown and star Lauren LaVera will ring the Nasdaq closing bell! This is so cool! Hopefully that\u0027s all he does.","contentType":"reshare","url":"https://www.lqdev.me/responses/art-clown-nasdaq-bell-halloween","date":"2024-10-31 09:05 -05:00","tags":["art","terrifier","halloween","movie","horror","newyork","ny","nasdaq"],"summary":"Art the Clown Will Ring the Nasdaq Closing Bell in Times Square on Halloween! \u2192 https://bloody-disgusting.com/the-further/3837684/terrifier-takeover-art-the-clown-will-ring-the-nasdaq-closing-bell-in-...","keywords":["nasdaq","clown","ring","closing","bell","times","square","halloween","takeover","domination"]},{"id":"notes-spooky-season-2024-over","title":"Only 365 more days until next halloween","content":"That\u0027s a wrap for spooky season. It was unseasonably warm so it hasn\u0027t really felt like fall.","contentType":"notes","url":"https://www.lqdev.me/notes/spooky-season-2024-over","date":"2024-10-31 08:29 -05:00","tags":["fall","spookyszn","halloween"],"summary":"That\u0027s a wrap for spooky season. It was unseasonably warm so it hasn\u0027t really felt like fall.","keywords":["wrap","spooky","season","unseasonably","warm","hasn","really","felt","fall"]},{"id":"star-spirit-box-flying-lotus","title":"New Release: Spirit Box by Flying Lotus","content":"New Release: Spirit Box by Flying Lotus \u2192 https://www.youtube.com/watch?v=E-BbWldXfJk\u0026amp;list=OLAK5uy_kQDB4vsLJy0bmjuYsCXKa-eMO-W33na2w He\u0027s back! A day after Tyler too. I already loved Garmonbozia but Ajhussi is my favorite track. Although on this album you hear some of the early FlyLo sound, it still feels new and fresh.","contentType":"star","url":"https://www.lqdev.me/responses/spirit-box-flying-lotus","date":"2024-10-30 08:45 -05:00","tags":["music","flyinglotus","spiritbox","newmusic","album"],"summary":"New Release: Spirit Box by Flying Lotus \u2192 https://www.youtube.com/watch?v=E-BbWldXfJk\u0026amp;list=OLAK5uy_kQDB4vsLJy0bmjuYsCXKa-eMO-W33na2w He\u0027s back! A day after Tyler too. I already loved Garmonbozia...","keywords":["release:","sound","flylo","early","hear","album","although","track","favorite","ajhussi"]},{"id":"star-chromakopia-tyler-the-creator-released","title":"Chromakopia by Tyler The Creator Released","content":"Chromakopia by Tyler The Creator Released \u2192 https://www.youtube.com/watch?v=hCcwCv3G1FQ\u0026amp;list=OLAK5uy_nt1Nw4wT6I7VlzNknxTiIz3hfED0ttO8Q I did a first pass earlier today and I generally enjoyed this new album from Tyler. Like many previous albums, it\u0027s highly introspective. I loved the instrumentals as well. Some first impressions: Track 10 is not a double track as it\u0027s generally been the case of his previous albums. \u0026quot;Like Him\u0026quot; adds a new contrasting dimension to Tyler\u0027s relationship with his father. I\u0027ll need some more time to digest it and read through the lyrics but overall, great album.","contentType":"star","url":"https://www.lqdev.me/responses/chromakopia-tyler-the-creator-released","date":"2024-10-28 19:59 -05:00","tags":["chromakopia","music","tylerthecreator","newmusic","album","rap","hiphop"],"summary":"Chromakopia by Tyler The Creator Released \u2192 https://www.youtube.com/watch?v=hCcwCv3G1FQ\u0026amp;list=OLAK5uy_nt1Nw4wT6I7VlzNknxTiIz3hfED0ttO8Q I did a first pass earlier today and I generally enjoyed...","keywords":["tyler","albums","previous","track","album","generally","chromakopia","double","case","\u0026quot"]},{"id":"reshare-rip-399-queen-tetons","title":"R.I.P 399 - Queen of the Tetons","content":"R.I.P 399 - Queen of the Tetons \u2192 https://apnews.com/article/famous-grizzly-399-killed-grand-teton-wyoming-3e13c4b5234926cbd799dbb3db1ffac8 Sad news from the Tetons this week. Grizzly 399 passed away after being struck by a vehicle. Grizzly No. 399 died Tuesday night on a highway in Snake River Canyon south of Jackson... At 28 years old, No. 399 was the oldest known reproducing female grizzly in the Yellowstone ecosystem. Each spring, wildlife enthusiasts eagerly awaited her emergence from her den to see how many cubs she had birthed over the winter \u2014 then quickly shared the news online. The grizzly lived through a time of strife over her species in the region...","contentType":"reshare","url":"https://www.lqdev.me/responses/rip-399-queen-tetons","date":"2024-10-25 21:59 -05:00","tags":["399","grandteton","grizzly","teton"],"summary":"R.I.P 399 - Queen of the Tetons \u2192 https://apnews.com/article/famous-grizzly-399-killed-grand-teton-wyoming-3e13c4b5234926cbd799dbb3db1ffac8 Sad news from the Tetons this week. Grizzly 399 passed away...","keywords":["grizzly","tetons","news","from","queen","reproducing","female","yellowstone","ecosystem","spring"]},{"id":"reshare-kamasi-washington-tiny-desk-concert-npr","title":"Tiny Desk Concerts - Kamasi Washington","content":"Tiny Desk Concerts - Kamasi Washington \u2192 https://www.youtube.com/watch?v=x8WTPgeVPjg I think this is Kamasi\u0027s first time on Tiny Desk. Great performance. I didn\u0027t realize Brandon Coleman was on keys. The encore, \u0026quot;Asha The First\u0026quot;, was really good.","contentType":"reshare","url":"https://www.lqdev.me/responses/kamasi-washington-tiny-desk-concert-npr","date":"2024-10-22 09:30 -05:00","tags":["kamasiwashington","music","tinydesk","npr","jazz","brainfeeder"],"summary":"Tiny Desk Concerts - Kamasi Washington \u2192 https://www.youtube.com/watch?v=x8WTPgeVPjg I think this is Kamasi\u0027s first time on Tiny Desk. Great performance. I didn\u0027t realize Brandon Coleman was on keys....","keywords":["tiny","desk","kamasi","first\u0026quot","asha","\u0026quot","encore","keys","coleman","brandon"]},{"id":"reshare-noid-tyler-the-creator-chromakopia","title":"Noid by Tyler The Creator","content":"Noid by Tyler The Creator \u2192 https://open.spotify.com/track/1tnZxHryc2wWtjUZC1LQw5 I\u0027m ready for this new album. It reminds me of Cherry Bomb in some ways. Love the instrumentals.","contentType":"reshare","url":"https://www.lqdev.me/responses/noid-tyler-the-creator-chromakopia","date":"2024-10-21 20:15 -05:00","tags":["music","chromakopia","tylerthecreator","noid","newmusic"],"summary":"Noid by Tyler The Creator \u2192 https://open.spotify.com/track/1tnZxHryc2wWtjUZC1LQw5 I\u0027m ready for this new album. It reminds me of Cherry Bomb in some ways. Love the instrumentals.","keywords":["noid","tyler","creator","https://open","spotify","com/track/1tnzxhryc2wwtjuzc1lqw5","ready","album","reminds","cherry"]},{"id":"posts-build-your-own-live-streaming-app-owncast-dotnet-aspire","title":"Build your own self-hosted live streaming application with Owncast and .NET Aspire","content":"Platforms come and go. As a result, I\u0027m a strong advocate for [owning your data](https://indieweb.org/own_your_data) (when possible). Self-hosting is a way for you to do that. Owning your data could mean self-hosting [your website](https://buttondown.com/ownyourweb/archive/issue-06/), [password manager](https://bitwarden.com/blog/host-your-own-open-source-password-manager/), [media server](https://jellyfin.org/docs/), or [social media](https://docs.joinmastodon.org/user/run-your-own/). However, self-hosting comes with its own challenges, primarily cost (time and money) and in cases where software isn\u0027t provided as an appliance on your hosting provider of choice, some technical knowledge may be required. As I got more into self-hosting applications such as my [Mastodon instance](/mastodon), I came across Owncast. During peak COVID lockdowns, like many others, live streaming is one of the ways I passed the time. While Twitch and YouTube got the job done, self-hosting was always in the back of my mind. It\u0027s been a while since I\u0027ve live-streamed, so I never really went through the process of evaluating the self-hosted route with Owncast. While browsing for something on YouTube the other day, I ran into some of my [old live-stream recordings](https://www.youtube.com/playlist?list=PLsdMoYmuvh9ZtgB8U7FECR_8wKMYXJNAm). This got me thinking again, how difficult would it be to put together my own live-streaming setup. This post is the result of that exploration. In this post, I\u0027ll modify a .NET Aspire Starter Application template and show how to set up a self-hosted live-streaming application using Owncast and .NET Aspire. You can find the source code in the [lqdev/BYOwncastAspire](/github/BYOwncastAspire) repository. ![BYOwnCastAspire Web Frontend](https://github.com/user-attachments/assets/bdd9f901-8d8f-45be-9e37-4dce8459e481) ## What is Owncast? The Owncast website describes the project as, \u0022...a free and open source live video and web chat server for use with existing popular broadcasting software.\u0022 ![Owncast admin server page](https://github.com/user-attachments/assets/769ec2d8-a3d1-4ec3-8271-c2de6e11ddd3) It goes on to further describe some of the reasons I like Owncast, which are: - **Self-hosting** - I\u0027m in complete control over the service and my data - **Open-source** - I can freely use and contribute to the project. Free in this case meaning both as in freedom and pizza. - **Builds on top of open standards like RMTP** - Software that supports the Real-Time Messaging Protocol (RMTP) like OBS can immediately be leveraged. - **Fediverse compatible** - Your content and network federated across [the Fediverse](https://joinfediverse.wiki/What_is_the_Fediverse%3F). To learn more, check out the [Owncast website](https://owncast.online/). ## What is .NET Aspire? The .NET Aspire documentation describes it as, \u0022...an opinionated, cloud ready stack for building observable, production ready, distributed applications.\u200B .NET Aspire is delivered through a collection of NuGet packages that handle specific cloud-native concerns.\u0022 Personally, the parts of .NET Aspire that matter to me are: - **App composition** - The .NET Aspire programming model makes it easy to define and compose a variety of resources such as .NET projects, containers, and much more in a single place. In many cases, for commonly used services, these resources are exposed in the form of [integrations](https://learn.microsoft.com/dotnet/aspire/fundamentals/integrations-overview). These integrations further simplify the composition of applications. - **Tooling (Dashboard)** - .NET Aspire provides a set of tools and templates. However, my favorite is the dashboard. The dashboard provides me with a single place to view my resources, their configurations, and logs. Although in this post, I don\u0027t cover deployment, there is also the provisioning component provided by .NET Aspire which in many cases can [simplify your application deployments](https://learn.microsoft.com/dotnet/aspire/deployment/overview). To learn more, check out the [.NET Aspire documentation](https://learn.microsoft.com/dotnet/aspire/get-started/aspire-overview). ## Build your application This application makes a few modifications to the [.NET Aspire Starter Application template](https://learn.microsoft.com/dotnet/aspire/fundamentals/setup-tooling?tabs=linux\u0026pivots=dotnet-cli#net-aspire-project-templates). The application consists of a few projects: - **BYOwncastAspire.AppHost** - This project is the entrypoint for .NET Aspire applications. This is where we\u0027ll configure the Owncast server as a container resource. - **BYOwncastAspire.Web** - Blazor web application. Although Owncast provides its own page where viewers can tune into your stream, by having a separate web application, preferably your own personal website, you can further enrich and customize how and where you publish content. - **BYOwncastAspire.ServiceDefaults** - This project contains default configurations for telemetry, health checks, resiliency, etc. No changes or updates required here. ### Configure Owncast There are many ways to host an Owncast server, with one of them being a container. In the context of Aspire which has built-in container support, you can easily add Owncast as a resource in your application. From the [Owncast documentation](https://owncast.online/quickstart/container/), once you\u0027ve pulled the Owncast container image, you can start it with the following command. \u0060\u0060\u0060bash docker run -v \u0060pwd\u0060/data:/app/data -p 8080:8080 -p 1935:1935 owncast/owncast:latest \u0060\u0060\u0060 This translates to the following in the *Program.cs* of the *BYOwncastAspire.AppHost* project. \u0060\u0060\u0060csharp var owncast = builder .AddContainer(name: \u0022owncast\u0022, image:\u0022owncast/owncast\u0022) .WithBindMount(\u0022./data\u0022,\u0022/app/data\u0022) .WithHttpEndpoint(port:8080,targetPort:8080,name:\u0022admin\u0022) .WithHttpEndpoint(port:1935,targetPort:1935,name:\u0022streaming\u0022) .WithExternalHttpEndpoints(); \u0060\u0060\u0060 This code: - Adds the owncast container image - Mounts a local *data* directory to the */app/data* directory in the container - Maps port \u00608080\u0060 for the Owncast admin server and \u00601935\u0060 for RMTP server. - Exposes the endpoints publicly ### Embed your stream Owncast provides the ability to [embed your video stream onto a website](https://owncast.online/docs/embed/). Although we don\u0027t need a frontend because one is already provided by Owncast, by embedding your stream on your website you can provide a single place for your viewers to consume your content. In this case, we can treat the *BYOwncastAspire.Web* project as our website. To embed your stream to the website, add the following code to your *Home.razor* page. \u0060\u0060\u0060csharp \u0060\u0060\u0060 In this case, we\u0027re pointing to the endpoint listening on port \u00608080\u0060 of our \u0060localhost\u0060. When you deploy the application, you\u0027d replace \u0060src\u0060 with your domain. ## Start your application That\u0027s all there is in terms of configuration. To start the application: 1. Open the terminal 2. Navigate to the *BYOwncastAspire.AppHost* project and run the following command. \u0060\u0060\u0060bash dotnet run \u0060\u0060\u0060 This will launch you into the .NET Aspire dashboard. At this point, you can further customize your Owncast server as well as the website. ![BYOwncastAspire .NET Aspire Dashboard Resource Page](https://github.com/user-attachments/assets/c20d84d8-925a-4f80-9058-622466cb08e9) ## What next? Now that you have your application running, in its current form, this application is only meant to serve as a sample of what you can do with Owncast and .NET Aspire. Some next steps might include: - [Change your admin password and customize your Owncast server](https://owncast.online/docs/configuration/) - [Configure object storage](https://owncast.online/docs/storage/) - [Set up your steaming software](https://owncast.online/docs/broadcasting/obs/) - [Deploy your application](https://learn.microsoft.com/dotnet/aspire/deployment/azure/aca-deployment) - [Extend the app\u0027s functionality by building custom plugins and extensions](https://owncast.online/thirdparty/) The plugins and extensions are particularly interesting to me because there may even be opportunities to experiment and insert AI capabilities at various layers such as moderation, translation, accessibility, show notes, chat summaries, etc. ## Conclusion In this post, I showed how you can combine the powerful live-steaming features of Owncast with the app orchestration and tooling in .NET Aspire to create rich self-hosted live-steaming applications. If you use this sample as a starting point for your own live-streaming or self-hosting explorations, [send me a message](/contact). I\u0027d love to hear about it.","contentType":"posts","url":"https://www.lqdev.me/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire","date":"2024-10-20 14:01 -05:00","tags":["owncast","dotnet","indieweb","fediverse","aspire","livestreaming","twitch","youtube","blazor"],"summary":"Platforms come and go. As a result, I\u0027m a strong advocate for [owning your data](https://indieweb.org/own_your_data) (when possible). Self-hosting is a way for you to do that. Owning your data could...","keywords":["owncast","aspire","application","self","server","live","website","project","https://owncast","hosting"]},{"id":"notes-2024-10-20-weekly-post-summary","title":"Week of October 20, 2024 - Post Summary","content":"Blogs Build your own self-hosted live streaming application with Owncast and .NET Aspire Responses You should be using an RSS reader Chromakopia - Tyler The Creator","contentType":"notes","url":"https://www.lqdev.me/notes/2024-10-20-weekly-post-summary","date":"2024-10-20 09:27 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Blogs Build your own self-hosted live streaming application with Owncast and .NET Aspire Responses You should be using an RSS reader Chromakopia - Tyler The Creator","keywords":["blogs","build","self","hosted","live","streaming","application","owncast","aspire","responses"]},{"id":"star-chromakopia-tyler-the-creator","title":"Chromakopia - Tyler The Creator","content":"Chromakopia - Tyler The Creator \u2192 https://www.youtube.com/watch?v=gkZ4dLMH-B8 2024 just keeps delivering. Most of the artists I follow have dropped an album this year. Now I\u0027m just waiting on FlyLo. Hopefully he releases his album by end of year. I don\u0027t know what to expect from Chromakopia, but I can\u0027t wait after watching this trailer.","contentType":"star","url":"https://www.lqdev.me/responses/chromakopia-tyler-the-creator","date":"2024-10-18 09:28 -05:00","tags":["music","tylerthecreator","chromakopia","newmusic"],"summary":"Chromakopia - Tyler The Creator \u2192 https://www.youtube.com/watch?v=gkZ4dLMH-B8 2024 just keeps delivering. Most of the artists I follow have dropped an album this year. Now I\u0027m just waiting on FlyLo....","keywords":["chromakopia","year","album","wait","from","expect","releases","hopefully","flylo","waiting"]},{"id":"reply-you-should-be-using-rss-reader-pluralistic","title":"You should be using an RSS reader","content":"You should be using an RSS reader \u2192 https://pluralistic.net/2024/10/16/keep-it-really-simple-stupid/#read-receipts-are-you-kidding-me-seriously-fuck-that-noise Yes, you should! ...the conduit through which I experience Molly\u0027s excellent work is totally enshittification-proof, and the more I use it, the easier it is for everyone to be less enshittified. This conduit is anti-lock-in, it works for nearly the whole internet. It is surveillance-resistant, far more accessible than the web or any mobile app interface. It is my secret super-power. It\u0027s RSS. ...But RSS isn\u0027t just good for the news! It\u0027s good for everything. Your RSS reader doesn\u0027t (necessarily) have an algorithm. By default, you\u0027ll get everything as it appears, in reverse-chronological order. Now, you sign up to so many feeds that you\u0027re feeling overwhelmed and you want an algorithm to prioritize posts \u2013 or recommend content. Lots of RSS readers have some kind of algorithm and recommendation system... But you control the algorithm, you control the recommendations. And if a new RSS reader pops up with an algorithm you\u0027re dying to try, you can export all the feeds you follow with a single click, which will generate an OPML file. Then, with one click, you can import that OPML file into any other RSS reader in existence and all your feeds will be seamlessly migrated there. You can delete your old account, or you can even use different readers for different purposes. RSS basically works like social media should work. Using RSS is a chance to visit a utopian future in which the platforms have no power, and all power is vested in publishers, who get to decide what to publish, and in readers, who have total control over what they read and how, without leaking any personal information through the simple act of reading. And here\u0027s the best part: every time you use RSS, you bring that world closer into being! Unlike those largely useless, performative boycotts of widely used platforms, switching to RSS doesn\u0027t require that you give anything up. Not only does switching to RSS let you continue to follow all the newsletters, webpages and social media accounts you\u0027re following now, it makes doing so better: more private, more accessible, and less enshittified. Using RSS to follow the stuff that matters to you will have an immediate, profoundly beneficial impact on your own digital life \u2013 and it will appreciably, irreversibly nudge the whole internet towards a better state. Great article by Cory Doctorow . I\u0027ve written at length on my website as to why I enjoy RSS . My use goes back to the days of Google Reader. Like many, once that was disbanded, my consumption pretty much shifted to social media for a few years. After growing dissatisfaction with the limitations and lack of control over my feed, I started my journey back to RSS readers eventually landing on NewsBlur and elfeed as my readers of choice. On this website, RSS plays an important role in a few areas. RSS is how I: Enable discovery of my own feeds Enable visitors to subscribe and get the latest content Syndicate posts to Mastodon . Enable feed discovery of websites I subscribe to through my: Blogroll Podroll (podcasts) YouTubeRoll? ForumRoll? So how can you get started? Learn about feeds . Check out the many getting started guides . Browse through blogrolls and feed catalogs Explore various feed readers to find the one that works best for you.","contentType":"reply","url":"https://www.lqdev.me/responses/you-should-be-using-rss-reader-pluralistic","date":"2024-10-18 08:54 -05:00","tags":["rss","pluralistic","indieweb","protocols","openweb","freeweb","internet","smallweb"],"summary":"You should be using an RSS reader \u2192 https://pluralistic.net/2024/10/16/keep-it-really-simple-stupid/#read-receipts-are-you-kidding-me-seriously-fuck-that-noise Yes, you should! ...the conduit through...","keywords":["readers","reader","algorithm","feeds","more","control","feed","using","enable","power"]},{"id":"notes-penn-state-usc-oct-2024","title":"Penn State (33) vs. USC (30)","content":"Wow! This game delivered. I haven\u0027t watched any games this season and I couldn\u0027t have picked a better one to tune into. It was an ugly game for Penn State and they got lucky on some calls but they found a way to get it done on the road.","contentType":"notes","url":"https://www.lqdev.me/notes/penn-state-usc-oct-2024","date":"2024-10-12 19:22 -05:00","tags":["pennstate","usc","football","ncaaf","ncaa","psu"],"summary":"Wow! This game delivered. I haven\u0027t watched any games this season and I couldn\u0027t have picked a better one to tune into. It was an ugly game for Penn State and they got lucky on some calls but they...","keywords":["game","they","found","calls","lucky","state","penn","ugly","done","tune"]},{"id":"notes-flylo-ingo-swann-single","title":"New Release: Ingo Swann by Flying Lotus","content":"New drop from FlyLo. This feels like a new album coming. So far the theme is mystical. The previous single, Garmonbozia , was a reference to Twin Peaks. Ingo Swann\u0027s website describes him as ...a pioneer of parapsychology and remote viewing, who worked with CIA and NASA, wrote books, and created art. The website invites visitors to, explore his research on NHEs, UFOs, UAPs, and the planets. Similarly, Wikipedia describes him as an American psychic, artist, and author, whose claims of clairvoyance were investigated as a part of the Central Intelligence Agency\u2019s Stargate Project. Swann is credited as the creator of the term \u201CRemote Viewing,\u0026quot; a term which refers to the use of extrasensory perception to perceive distant persons, places, or events. I\u0027m excited to discovery the mystery behind this new project from FlyLo.","contentType":"notes","url":"https://www.lqdev.me/notes/flylo-ingo-swann-single","date":"2024-10-10 21:13 -05:00","tags":["music","newrelease","flyinglotus","flylo","dance","cia","remoteviewing","ingoswann"],"summary":"New drop from FlyLo. This feels like a new album coming. So far the theme is mystical. The previous single, Garmonbozia , was a reference to Twin Peaks. Ingo Swann\u0027s website describes him as ...a...","keywords":["viewing","from","flylo","describes","website","swann","project","term","credited","stargate"]},{"id":"notes-got-a-story-to-tell-thee-sacred-souls","title":"New Release: Got a Story to Tell by Thee Sacred Souls","content":"New release from Thee Sacred Souls. I really enjoyed the beginning and the end. My favorite songs are: Lucid Girl Price I\u0027ll Pay Live for You On My Mind Waiting on the Right Time I\u0027m So Glad I Found You, Baby","contentType":"notes","url":"https://www.lqdev.me/notes/got-a-story-to-tell-thee-sacred-souls","date":"2024-10-10 21:10 -05:00","tags":["music","newrelease","theesacredsouls","soul","rnb","oldies"],"summary":"New release from Thee Sacred Souls. I really enjoyed the beginning and the end. My favorite songs are: Lucid Girl Price I\u0027ll Pay Live for You On My Mind Waiting on the Right Time I\u0027m So Glad I Found...","keywords":["release","glad","waiting","mind","live","price","girl","lucid","are:","songs"]},{"id":"notes-crate-finds-september-2024","title":"Crate Finds - September 2024","content":"This month had a mix of genres. Some of the ones I kept coming back to: Wharf Rat by Grateful Dead Deal by Grateful Dead God is Love by Marvin Gaye Save the Children by Marvin Gaye I listened to the album What\u0027s Going On a few times throughout the month . Tracks Mr. Soul by Buffalo Springfield Album: Buffalo Springfield Again Duration: 2:52 Listen on YouTube Backup: Listen on Spotify Little Bit Of Rain by Karen Dalton Album: It\u0027s So Hard To Tell Who\u0027s Going To Love You The Best Duration: 2:36 Listen on YouTube Backup: Listen on Spotify Sunlight by The Youngbloods Album: Elephant Mountain Duration: 3:07 Listen on YouTube Backup: Listen on Spotify The Paisley Window Pane by Wendy \u0026amp; Bonnie Album: Genesis Duration: 2:58 Listen on YouTube Backup: Listen on Spotify Homemade Ice Cream by Tony Joe White Album: Homemade Ice-Cream Duration: 3:12 Listen on YouTube Backup: Listen on Spotify Asleep in the Desert (Instrumental) by ZZ Top Album: Tejas Duration: 3:29 Listen on YouTube Backup: Listen on Spotify Glad and Sorry by Faces Album: Ooh La La Duration: 3:06 Listen on YouTube Backup: Listen on Spotify Great Day - 2020 Remaster by Paul McCartney, Linda McCartney Album: Flaming Pie (Archive Collection) Duration: 2:09 Listen on YouTube Backup: Listen on Spotify Sugaree by Jerry Garcia Album: Garcia Duration: 5:52 Listen on YouTube Backup: Listen on Spotify Hey, Hey, What Can I Do - Remaster by Led Zeppelin Album: Coda (Deluxe Edition) Duration: 3:56 Listen on YouTube Backup: Listen on Spotify Franklin\u0027s Tower - 2013 Remaster by Grateful Dead Album: Blues for Allah (2013 Remaster) Duration: 4:34 Listen on YouTube Backup: Listen on Spotify There\u0027s a New Day Coming by Menahan Street Band, Saundra Williams Album: There\u0027s a New Day Coming / Tommy Don\u0027t Duration: 2:23 Listen on YouTube Backup: Listen on Spotify Whatcha Gonna Do by The Valdons Album: Stop Wait A Minute Girl B/W Whatcha Gonna Do Duration: 2:04 Listen on YouTube Backup: Listen on Spotify Recuerdos de una Noche by Los Pasteles Verdes Album: Recuerdos de una Noche Duration: 2:59 Listen on YouTube Backup: Listen on Spotify Wharf Rat (Live at the Capitol Theatre, Port Chester, NY, 2/18/71) by Grateful Dead Album: American Beauty (50th Anniversary Deluxe Edition) Duration: 7:24 Listen on YouTube Backup: Listen on Spotify Flying on the Ground Is Wrong by Buffalo Springfield Album: Buffalo Springfield Duration: 2:43 Listen on YouTube Backup: Listen on Spotify Deal by Grateful Dead Album: Live Albums Collection Duration: 4:45 Listen on YouTube Backup: Listen on Spotify From The Start by Laufey Album: From The Start Duration: 2:49 Listen on YouTube Backup: Listen on Spotify Those Years Are Over by The Sha La Das Album: Love in the Wind Duration: 3:58 Listen on YouTube Backup: Listen on Spotify Good Thoughts, Bad Thoughts by Funkadelic Album: Standing On The Verge Of Getting It On Duration: 12:17 Listen on YouTube Backup: Listen on Spotify Saturday\u0027s Children (feat. Neal Casal) by Circles Around The Sun, Neal Casal Album: Interludes For The Dead (feat. Neal Casal) Duration: 9:25 Listen on YouTube Backup: Listen on Spotify Waiting on the Right Time by Thee Sacred Souls Album: Waiting on the Right Time Duration: 3:39 Listen on YouTube Backup: Listen on Spotify Within You Without You - Remastered 2009 by The Beatles Album: Sgt. Pepper\u0027s Lonely Hearts Club Band (Remastered) Duration: 5:04 Listen on YouTube Backup: Listen on Spotify La plus que lente, L. 121 (Arr. Roques) by Claude Debussy, Jean-Guihen Queyras, Alexandre Tharaud Album: Debussy \u0026amp; Poulenc: Cello Sonatas Duration: 4:34 Listen on YouTube Backup: Listen on Spotify Etude No. 2 by Philip Glass Album: Etudes for Piano Vol 1 No. 1-10 Duration: 4:55 Listen on YouTube Backup: Listen on Spotify Where Are We Going? by Marvin Gaye Album: Gold Duration: 3:55 Listen on YouTube Backup: Listen on Spotify God Is Love by Marvin Gaye Album: What\u0027s Going On Duration: 1:41 Listen on YouTube Backup: Listen on Spotify After The Dance by Marvin Gaye Album: I Want You Duration: 4:39 Listen on YouTube Backup: Listen on Spotify Come Live With Me Angel by Marvin Gaye Album: I Want You (Deluxe Edition) Duration: 6:28 Listen on YouTube Backup: Listen on Spotify Save The Children by Marvin Gaye Album: What\u0027s Going On Duration: 4:03 Listen on YouTube Backup: Listen on Spotify Didn\u0027t We by Peggy Lee Album: From The Vaults (Vol. 3) Duration: 2:48 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-september-2024","date":"2024-10-10 21:04 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"This month had a mix of genres. Some of the ones I kept coming back to: Wharf Rat by Grateful Dead Deal by Grateful Dead God is Love by Marvin Gaye Save the Children by Marvin Gaye I listened to the...","keywords":["listen","spotify","youtube","backup:","duration:","album:","gaye","marvin","dead","grateful"]},{"id":"bookmarks-defcon-media-server","title":"DEFCON Media Server","content":"DEFCON Media Server \u2192 https://media.defcon.org/ Videos, Slides, Presentations, and assets from DEFCON.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/defcon-media-server","date":"2024-10-10 20:41 -05:00","tags":["defcon","media","persentations","videos","slides"],"summary":"DEFCON Media Server \u2192 https://media.defcon.org/ Videos, Slides, Presentations, and assets from DEFCON.","keywords":["defcon","media","server","https://media","org/","videos","slides","presentations","assets","from"]},{"id":"star-fighting-for-our-web-white","title":"Fighting for our web - A talk from XOXO","content":"Fighting for our web - A talk from XOXO \u2192 https://www.citationneeded.news/fighting-for-our-web/ I haven\u0027t watched the entire talk, but here are some excerpts from the transcript that resonated with me. For a lot of people, the web feels worse than it used to. Much of our time is spent on a handful of giant social networks that do everything they can to keep people in their apps for as long as possible, even if it has detrimental effects on the people who are using them, or on the social networks themselves. Websites outside of these handful of social networks are harder and harder to even find, and partially because of that, they have a harder and harder time sustaining themselves. What is left for those of us who saw the web as an infinite canvas, a tool to reach those who we never could have dreamed of reaching before, a medium that could stretch the limits of what was even possible in an analog world? ...having said all this, would it surprise you to hear that now more than ever, I feel that same burning feeling of excitement around what\u2019s possible? That\u2019s because what really sucks about the web these days, what has us feeling despair and anger, has everything to do with the industry that has formed around the web, but not the web itself. The web is still just a substrate on which anything can be built. Most importantly, the web is the people who use it, not the companies that have established themselves around it. And the widespread disillusionment that we\u2019re seeing may actually be a good thing. More people than ever have realized that the utopian dreams of a web that could only bring about positive and wonderful things might have been misguided. With this knowledge comes power. The power to shape the web that we want to see, while fighting against the one that we don\u2019t. My experience in fighting this fight has helped to convince me of just how much power we, everyday normal people, have\u2014even when we\u2019re staring down massive platforms and billion-dollar companies. I ended up doing what I enjoy doing\u2014building something cool, mostly for the sake of building it, and writing down what I saw. ...this simple act of building something interesting and somewhat different had an impact that I could not have anticipated. It turned out that people were starving for it. And even though I was a nobody, it had a major impact. The lesson was clear. The tech industry, even with its billions of dollars, is not an indomitable force. Though they can and will ignore what people want from them, they cannot control what those people think. And the tide can turn against them. And it doesn\u2019t always take a job at The New York Times or a huge pre-established platform to become one of the voices speaking up, helping to turn that tide. Sometimes you just have to make something cool. And using the very same technology that enabled crypto guys to sell their scam tokens, or the boosterist journalists to publish obsequious descriptions of companies that were really selling vaporware, or the crypto community to spam social media with promotions for their NFTs, I was able to do something a little different to push back against that very same phenomenon. What they don\u2019t seem to realize is that in doing so, by reducing the web only to the types of expression that can happen within their cramped boxes\u2014where you can\u2019t write more than 280 characters, or you can\u2019t publish your cool JavaScript-based art project, or you can\u2019t say the things that you want to say without getting de-boosted by the engagement maximization machine, or you can\u2019t read what your friends are posting without the platform interjecting offensive troll posts or soulless AI-generated meme images\u2014they\u2019re creating a thirst for everything outside of those boxes. A thirst for what the web really is\u2014a medium, a conduit, a tool that is used by readers and artists and creators and explorers, not a gatekeeper that seems to be in an ever more adversarial relationship with everyone who uses it. The platforms I talk about are deeply entrenched to the point where many people barely use the web outside of them. But the platforms do not exist without the people, and there are a lot more of us than there are of them. ...it has never been easier to do cool things on the web. What used to be expensive and require a great deal of technical expertise is now becoming more and more accessible, both financially and technologically, by the day. More people than ever have access to the web, both in terms of access to devices, but also in terms of access to internet connectivity and software. What used to be the realm of the nerds and those with the financial wherewithal to purchase expensive home computers, connectivity, and software packages, is now home to people from all walks of life, who bring new ideas, perspectives, and experiences that we often forget were in short supply during what some of us think of as the \u201Cgood old days\u201D of the internet. And so, we\u2014all of us\u2014can build the projects we want to see, write the software that the big platforms won\u2019t, and create the services that people need. We can wire everything together, whether the platforms like it or not, to tear down the walls that they put up. We can modify the software, reverse engineer their systems, and wrestle back control of how we experience the web, even through these very platforms. We can share information and help people understand what is happening around them. We can teach others to build the things they want to build, and share with one another the important and meaningful work that is being done. We can build the web that we want to see, and we can return to that place where the web is a place of wonder, where all of us feel that same burning feeling of excitement as we push the web back towards the wonderful, beautiful, joyful place it ought to be.","contentType":"star","url":"https://www.lqdev.me/responses/fighting-for-our-web-white","date":"2024-10-10 20:20 -05:00","tags":["internet","indieweb","protocolsnotplatforms","humanweb","smallweb","web3.0"],"summary":"Fighting for our web - A talk from XOXO \u2192 https://www.citationneeded.news/fighting-for-our-web/ I haven\u0027t watched the entire talk, but here are some excerpts from the transcript that resonated with...","keywords":["people","more","they","platforms","want","even","those","everything","social","harder"]},{"id":"star-marvin-gaye-whats-going-on","title":"Marvin Gaye - What\u0027s Going On","content":"Marvin Gaye - What\u0026#39;s Going On \u2192 https://www.youtube.com/watch?v=o5TmORitlKk\u0026amp;list=OLAK5uy_krxU85sRSpvmZqiwHyVmYNhKBDVgvj-CE This is such a beautiful album. It seems I\u0027m not the only one that thinks so since it\u0027s rated as the #1 album of all time by Rolling Stone . The powerful messages in the lyrics still resonate to this day and the composition is such a perfect match for Marvin Gaye\u0027s voice. 50 years later it feels like this album has transcended space and time. Over the past few days, I\u0027ve listened to it from beginning to end at least 3 times. Of course there\u0027s the classics like What\u0027s Going On and Mercy Mercy Me that many people know. However, the ones that have quickly become my favorites are: Save The Children God Is Love","contentType":"star","url":"https://www.lqdev.me/responses/marvin-gaye-whats-going-on","date":"2024-09-25 20:19 -05:00","tags":["music","marvingaye","whatsgoingon","soul","rnb","album","war","nature","environment","conservation","love"],"summary":"Marvin Gaye - What\u0026#39;s Going On \u2192 https://www.youtube.com/watch?v=o5TmORitlKk\u0026amp;list=OLAK5uy_krxU85sRSpvmZqiwHyVmYNhKBDVgvj-CE This is such a beautiful album. It seems I\u0027m not the only one that...","keywords":["album","marvin","going","mercy","gaye","are:","transcended","space","past","days"]},{"id":"notes-2024-09-15-weekly-post-summary","title":"Week of September 15, 2024 - Post Summary","content":"Notes Crate Finds - August 2024 Leadership Ascension Track (LAT) Website Post Statistics - September 2024 Responses Listening to the Sun by Andre 3000 Cosmohedron by Duncan Hatch","contentType":"notes","url":"https://www.lqdev.me/notes/2024-09-15-weekly-post-summary","date":"2024-09-15 13:26 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes Crate Finds - August 2024 Leadership Ascension Track (LAT) Website Post Statistics - September 2024 Responses Listening to the Sun by Andre 3000 Cosmohedron by Duncan Hatch","keywords":["2024","notes","cosmohedron","3000","andre","listening","responses","september","statistics","post"]},{"id":"notes-post-statistics-september-2024","title":"Website Post Statistics - September 2024","content":"The most significant update from last month is that I have now passed the number of responses compared to last year. The top tag continues to be AI and The Verge is the top domain I link to. I\u0027m in the process of redesigning the website, so that\u0027s probably one of the things I\u0027ll be focusing the next few months.","contentType":"notes","url":"https://www.lqdev.me/notes/post-statistics-september-2024","date":"2024-09-15 09:19 -05:00","tags":["githubactions","cicd","blogging","stats","github"],"summary":"The most significant update from last month is that I have now passed the number of responses compared to last year. The top tag continues to be AI and The Verge is the top domain I link to. I\u0027m in...","keywords":["last","significant","focusing","things","probably","website","redesigning","process","link","domain"]},{"id":"notes-crate-finds-august-2024","title":"Crate Finds - August 2024","content":"A little late with this post, but here\u0027s what I was listening to in August. Some songs that were on repeat: Dark Star by Grateful Dead Ripple by Grateful Dead Garmonbozia by Flying Lotus Pretty Girl Why by Buffalo Springfield Sit with the Guru by Strawberry Alarm Clock Tracks Dark Star - Single Version by Grateful Dead Album: What a Long Strange Trip It\u0027s Been: The Best of the Grateful Dead Duration: 2:41 Listen on YouTube Backup: Listen on Spotify Sit With The Guru by Strawberry Alarm Clock Album: Wake Up...It\u0027s Tomorrow Duration: 3:02 Listen on YouTube Backup: Listen on Spotify A Message to Pretty by The Rising Storm Album: Calm Before... Duration: 3:28 Listen on YouTube Backup: Listen on Spotify L\u0027America by The Doors Album: L.A. Woman Duration: 4:34 Listen on YouTube Backup: Listen on Spotify Long Train Runnin\u0027 by The Doobie Brothers Album: The Captain and Me Duration: 3:27 Listen on YouTube Backup: Listen on Spotify Thorn Tree In The Garden by Derek \u0026amp; The Dominos Album: Layla And Other Assorted Love Songs (Remastered 2010) Duration: 2:50 Listen on YouTube Backup: Listen on Spotify Leticia Isabel by Los Moonlights Album: 15 De Los Moonlights Duration: 3:17 Listen on YouTube Backup: Listen on Spotify Are You Lonesome Like Me? by The Feminine Complex Album: Livin\u0027 Love Duration: 2:52 Listen on YouTube Backup: Listen on Spotify I\u2019ve Got Soul by The Royal Jesters Album: Royal Jesters: English Oldies Duration: 2:53 Listen on YouTube Backup: Listen on Spotify En el Cielo No Hay Cerveza (In Heaven There Is No Beer) by Flaco Jimenez Album: Squeeze Box King Duration: 2:34 Listen on YouTube Backup: Listen on Spotify Our Day Will Come by Sunny \u0026amp; The Sunliners Album: Young, Gifted \u0026amp; Brown Duration: 2:37 Listen on YouTube Backup: Listen on Spotify Sparkle In Your Eyes by Jensine Benitez, Thee Sacred Souls Album: Ilusi\u00F3n De Amor Duration: 3:25 Listen on YouTube Backup: Listen on Spotify It\u0027s Only Love by Joey Quinones, Thee Sinseers Album: It\u0027s Only Love Duration: 3:44 Listen on YouTube Backup: Listen on Spotify Western Sky by Dangermuffin Album: Songs for the Universe Duration: 2:26 Listen on YouTube Backup: Listen on Spotify Garmonbozia by Flying Lotus Album: Garmonbozia Duration: 3:33 Listen on YouTube Backup: Listen on Spotify Lost in Love by Robin Trower Album: Caravan to Midnight Duration: 4:26 Listen on YouTube Backup: Listen on Spotify Dirty Work by Steely Dan Album: Can\u0027t Buy A Thrill Duration: 3:10 Listen on YouTube Backup: Listen on Spotify Questions by Buffalo Springfield Album: Last Time Around Duration: 2:57 Listen on YouTube Backup: Listen on Spotify Get A Little by The Mothers Of Invention Album: Weasels Ripped My Flesh Duration: 2:33 Listen on YouTube Backup: Listen on Spotify Pretty Girl Why by Buffalo Springfield Album: Last Time Around Duration: 2:27 Listen on YouTube Backup: Listen on Spotify You Don\u0027t Have to Cry - 2005 Remaster by Crosby, Stills \u0026amp; Nash Album: Crosby, Stills \u0026amp; Nash Duration: 2:42 Listen on YouTube Backup: Listen on Spotify Rock \u0026amp; Roll Woman by Buffalo Springfield Album: Buffalo Springfield Again Duration: 2:45 Listen on YouTube Backup: Listen on Spotify Ripple - 2013 Remaster by Grateful Dead Album: American Beauty (2013 Remaster) Duration: 4:09 Listen on YouTube Backup: Listen on Spotify I Am a Child by Buffalo Springfield Album: Last Time Around Duration: 2:22 Listen on YouTube Backup: Listen on Spotify Tanto Arriba Como Abajo by Chicano Batman Album: Tanto Arriba Como Abajo Duration: 3:41 Listen on YouTube Backup: Listen on Spotify A Long Slow Little Wave / Citizen, an Activity by Loving Album: Loving Duration: 2:33 Listen on YouTube Backup: Listen on Spotify Dreamed by Lee Clarke, Daniel Suun Album: Dreamed Duration: 3:48 Listen on YouTube Backup: Listen on Spotify Ecstasy Cowgirl by Duster, Dirty Art Club Album: Anhedonia II b/w Ecstasy Cowgirl Duration: 1:57 Listen on YouTube Backup: Listen on Spotify Song of a Sinner by Top Drawer Album: Forge Your Own Chains: Heavy Psychedelic Ballads and Dirges 1968-1974 Duration: 8:38 Listen on YouTube Backup: Listen on Spotify Queen St. Gang by Arzachel Album: Arzachel Duration: 4:25 Listen on YouTube Backup: Listen on Spotify Holy Are You by The Electric Prunes Album: Release Of An Oath Duration: 4:09 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-august-2024","date":"2024-09-15 09:11 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"A little late with this post, but here\u0027s what I was listening to in August. Some songs that were on repeat: Dark Star by Grateful Dead Ripple by Grateful Dead Garmonbozia by Flying Lotus Pretty Girl...","keywords":["listen","spotify","backup:","duration:","album:","youtube","\u0026amp","springfield","buffalo","love"]},{"id":"notes-leadership-ascension-track-omegamart","title":"Leadership Ascension Track (LAT)","content":"Excerpts from the Omega Mart Leadership Ascension Track (LAT) Meeting the self Make a human pyramid with your high school guidance counselor and middle school crush. Renouncing the body Contribute 14% of your net worth to Drampcorp. Be nice to 77 dogs. Understanding nature Learn to suck venom out of a snake bite. Befriend a snake. Interview a rock and ghost write its memoir. Spend a week in the desert, living off the land. Harmonizing with the essence Buy two cakes. Decorate and dedicate one of the cakes to the other. Stack the cakes. Leave them both on a neighbor\u0027s doorstep in a ding-dong-ditch scenario. Swim to a whirlpool and dance within its rotating waves (hint: go with the flow). Moving beyond Approach a stranger in a coffee shop, have them show you their laptop search history, find the second-most recent country they\u0027ve searched and move there for 7 months. Come to peace with the infinite Use an infinity mirror to get ready for a night on the town. Manifesting the self Take a childhood photo to the top of a mountain. Bury the photo and create a new peak. One day a week, wake at dawn and meditate until sundown.","contentType":"notes","url":"https://www.lqdev.me/notes/leadership-ascension-track-omegamart","date":"2024-09-15 08:59 -05:00","tags":["meowwolf","omegamart","art","lasvegas","las","corporate","lat","leadershipascensiontrack","source","area15","leadership","fun","conspiracy"],"summary":"Excerpts from the Omega Mart Leadership Ascension Track (LAT) Meeting the self Make a human pyramid with your high school guidance counselor and middle school crush. Renouncing the body Contribute...","keywords":["cakes","snake","self","photo","school","week","excerpts","search","laptop","show"]},{"id":"star-andre-3000-listening-to-the-sun","title":"Listening to the Sun by Andre 3000","content":"Listening to the Sun by Andre 3000 \u2192 https://www.youtube.com/watch?v=2nOxSeBhQAA Simply amazing.","contentType":"star","url":"https://www.lqdev.me/responses/andre-3000-listening-to-the-sun","date":"2024-09-14 22:08 -05:00","tags":["andre3000","music","instrumental","experimental","ambient","jazz","creative","instrumental"],"summary":"Listening to the Sun by Andre 3000 \u2192 https://www.youtube.com/watch?v=2nOxSeBhQAA Simply amazing.","keywords":["listening","andre","3000","https://www","youtube","com/watch","v=2noxsebhqaa","simply","amazing"]},{"id":"star-cosmohedron-duncan-hatch","title":"Cosmohedron by Duncan Hatch","content":"Cosmohedron by Duncan Hatch \u2192 https://www.youtube.com/watch?v=3pxrECZYEAA I love the visuals on this video.","contentType":"star","url":"https://www.lqdev.me/responses/cosmohedron-duncan-hatch","date":"2024-09-14 22:06 -05:00","tags":["short","animation","psychidelic","duncanhatch","video","youtube"],"summary":"Cosmohedron by Duncan Hatch \u2192 https://www.youtube.com/watch?v=3pxrECZYEAA I love the visuals on this video.","keywords":["cosmohedron","duncan","hatch","https://www","youtube","com/watch","v=3pxreczyeaa","love","visuals","video"]},{"id":"star-shipping-wordpress-tumblr","title":"Shipping Tumblr and WordPress","content":"Shipping Tumblr and WordPress \u2192 https://automattic.com/2024/08/27/shipping-tumblr-and-wordpress/ Since Automattic acquired Tumblr we\u2019ve made it more efficient, grown its revenue, and worked to improve the platform. But there\u2019s one part of the plan that we haven\u2019t yet started, which is to run Tumblr on WordPress. I\u2019m pleased to say we\u2019re kicking off that project now! A little late, but I can\u0027t express how much I love this. We love Tumblr\u2019s streamlined posting experience and its current product direction. We\u2019re not changing that. We\u2019re talking about running Tumblr\u2019s backend on WordPress. You won\u2019t even notice a difference from the outside. Although it sounds outside the scope of the original plan, it would be amazing if the shared backend as well as the Tumblr publishing front-end made their way into the open-source version of WordPress. That would open the door to self-hosted personal websites that also make it significantly easier to post content in smaller chunks like a microblog. Pair that with ActivityPub integrations and you now have a connected web of individuals where the first place they post content to is their own website and for discovery / broader reach can also federate and post to other platforms that also support protocols like ActivityPub. Can\u0027t wait to see how this project develops. Do you yearn for the days when people owned their corner of the internet and expressed themselves in wild and wacky ways? Do you want to see an internet focused on creativity, art, and ideas instead of debating and dividing? Do you think content and data should be owned by authors and artists, instead of getting locked behind the closed platform of a mega-corporation? Do you want to build an internet where anyone with a story can tell it, and anyone with a product can sell it, regardless of income, gender, politics, language, or where they live in the world? The answer to all of those is YES.","contentType":"star","url":"https://www.lqdev.me/responses/shipping-wordpress-tumblr","date":"2024-09-05 22:15 -05:00","tags":["wordpress","tumblr","personalweb","blogging","indieweb","microblog","posse","smallweb","automattic"],"summary":"Shipping Tumblr and WordPress \u2192 https://automattic.com/2024/08/27/shipping-tumblr-and-wordpress/ Since Automattic acquired Tumblr we\u2019ve made it more efficient, grown its revenue, and worked to...","keywords":["tumblr","wordpress","content","post","internet","also","we\u2019re","backend","outside","instead"]},{"id":"reshare-devintersection-vegas-sept-2024","title":"Join me at DEVintersection in Las Vegas - September 10-12","content":"Join me at DEVintersection in Las Vegas - September 10-12 \u2192 https://devblogs.microsoft.com/dotnet/discover-dotnet-at-dev-intersection-las-vegas-2024/ I\u0027m excited to be at DEVintersection again this year where I\u0027ll get a chance to meet old and new friends. I have a few sessions where I\u0027ll be talking about some of my favorite things, .NET and AI. Introduction to AI in .NET Building AI Applications from Scratch: A Hands-On Guide for .NET Developers Ask The Experts - Starting out on your .NET \u0026amp; AI explorations Other sessions I recommend as well: Navigating the World of AI Models in .NET: From Local Development to the Cloud KEYNOTE - Practical Real-World AI for Developers Making Web Applications Intelligent and Performant with Redis Building AI Copilots: Integrate Semantic Kernel, Azure OpenAI, and Azure Cosmos DB with .NET Aspire Introduction to .NET Aspire There\u0027s so many more, but in the interest of not listing them all out, check out the schedule . See you there!","contentType":"reshare","url":"https://www.lqdev.me/responses/devintersection-vegas-sept-2024","date":"2024-09-04 20:10 -05:00","tags":["devintersection","conference","dotnet","ai","aspnet","LAS","lasvegas","desert"],"summary":"Join me at DEVintersection in Las Vegas - September 10-12 \u2192 https://devblogs.microsoft.com/dotnet/discover-dotnet-at-dev-intersection-las-vegas-2024/ I\u0027m excited to be at DEVintersection again this...","keywords":["sessions","developers","from","applications","building","introduction","azure","world","devintersection","vegas"]},{"id":"reshare-npr-tiny-desk-bob-weir-wolf-bros-mikaela-davis","title":"NPR Tiny Desk Concerts: Bob Weir, Wolf Bros, Mikaela Davis","content":"NPR Tiny Desk Concerts: Bob Weir, Wolf Bros, Mikaela Davis \u2192 https://www.youtube.com/watch?v=4l_gUwdPrNY I came across this gem over the weekend. Mikaela Davis on the harp just takes Bird Song and Ripple to the next level.","contentType":"reshare","url":"https://www.lqdev.me/responses/npr-tiny-desk-bob-weir-wolf-bros-mikaela-davis","date":"2024-08-26 19:27 -05:00","tags":["music","tinydesk","gratefuldead","bobweir","npr","wolfbros","mikaeladavis","concert","deadhead"],"summary":"NPR Tiny Desk Concerts: Bob Weir, Wolf Bros, Mikaela Davis \u2192 https://www.youtube.com/watch?v=4l_gUwdPrNY I came across this gem over the weekend. Mikaela Davis on the harp just takes Bird Song and...","keywords":["mikaela","davis","tiny","ripple","song","bird","takes","harp","weekend","across"]},{"id":"reshare-meta-bringing-llama-3-to-life","title":"Bringing Llama 3 to life","content":"Bringing Llama 3 to life \u2192 https://engineering.fb.com/2024/08/21/production-engineering/bringing-llama-3-to-life/ At AI Infra @ Scale 2024, Meta engineers discussed every step of how we built and brought Llama 3 to life, from data and training to inference.","contentType":"reshare","url":"https://www.lqdev.me/responses/meta-bringing-llama-3-to-life","date":"2024-08-22 16:21 -05:00","tags":["ai","meta","llama3","opensource","llm"],"summary":"Bringing Llama 3 to life \u2192 https://engineering.fb.com/2024/08/21/production-engineering/bringing-llama-3-to-life/ At AI Infra @ Scale 2024, Meta engineers discussed every step of how we built and...","keywords":["llama","life","bringing","data","from","brought","built","step","discussed","engineers"]},{"id":"reshare-transformers-music-recommendation-google","title":"Transformers in music recommendation","content":"Transformers in music recommendation \u2192 https://research.google/blog/transformers-in-music-recommendation/ We present a music recommendation ranking system that uses Transformer models to better understand the sequential nature of user actions based on the current user context.","contentType":"reshare","url":"https://www.lqdev.me/responses/transformers-music-recommendation-google","date":"2024-08-21 20:16 -05:00","tags":["google","ai","transformers","deeplearning","neuralneworks","recommendation","research","music"],"summary":"Transformers in music recommendation \u2192 https://research.google/blog/transformers-in-music-recommendation/ We present a music recommendation ranking system that uses Transformer models to better...","keywords":["music","recommendation","user","transformers","based","actions","nature","sequential","understand","better"]},{"id":"reshare-new-chicano-batman-tanto-arriba-como-abajo","title":"New Chicano Batman: Tanto Arriba Como Abajo","content":"New Chicano Batman: Tanto Arriba Como Abajo \u2192 https://open.spotify.com/track/518ruJoGWraifuVpTBKr5a?si=6e7c5ac2ff7c42f3 As above, so below. Some new Chicano Batman was a pleasant surprise this morning. Even better, an instrumental track. You can hear some of their older sound in this track, which I really enjoyed.","contentType":"reshare","url":"https://www.lqdev.me/responses/new-chicano-batman-tanto-arriba-como-abajo","date":"2024-08-20 20:24 -05:00","tags":["music","chicanobatman","instrumental","newmusic"],"summary":"New Chicano Batman: Tanto Arriba Como Abajo \u2192 https://open.spotify.com/track/518ruJoGWraifuVpTBKr5a?si=6e7c5ac2ff7c42f3 As above, so below. Some new Chicano Batman was a pleasant surprise this...","keywords":["chicano","track","which","sound","older","hear","instrumental","better","even","morning"]},{"id":"reshare-pain-and-sorrow-on-repeat","title":"Pain and sorrow on repeat","content":"Pain and sorrow on repeat \u2192 https://www.youtube.com/watch?v=CfB0Uqd2aRE I\u0027ve had this song on repeat since Friday . Also, I didn\u0027t know the name of the song is a Twin Peaks reference. Given his recent focus as a composer, I hope this means there\u0027s a bigger Twin Peaks project or collaboration in the works.","contentType":"reshare","url":"https://www.lqdev.me/responses/pain-and-sorrow-on-repeat","date":"2024-08-19 21:47 -05:00","tags":["music","flyinglotus","twinpeaks","davidlynch","flylo","tv"],"summary":"Pain and sorrow on repeat \u2192 https://www.youtube.com/watch?v=CfB0Uqd2aRE I\u0027ve had this song on repeat since Friday . Also, I didn\u0027t know the name of the song is a Twin Peaks reference. Given his...","keywords":["twin","repeat","song","peaks","project","bigger","means","hope","composer","focus"]},{"id":"reshare-thundercat-yo-gabba-gabba","title":"Thundercat \u002B Yo Gabba Gabba","content":"Thundercat \u002B Yo Gabba Gabba \u2192 https://x.com/DatDaDatty/status/1824251654813704281 Here\u0027s something I didn\u0027t know I needed in my life. Perfect way to start a Saturday morning.","contentType":"reshare","url":"https://www.lqdev.me/responses/thundercat-yo-gabba-gabba","date":"2024-08-17 11:06 -05:00","tags":["music","thundercat","yogabbagabba","fun"],"summary":"Thundercat \u002B Yo Gabba Gabba \u2192 https://x.com/DatDaDatty/status/1824251654813704281 Here\u0027s something I didn\u0027t know I needed in my life. Perfect way to start a Saturday morning.","keywords":["gabba","thundercat","https://x","com/datdadatty/status/1824251654813704281","here","didn","needed","life","perfect","start"]},{"id":"reply-organic-maps-removed-play-store","title":"Organic Maps removed from Google Play Store","content":"Organic Maps removed from Google Play Store \u2192 https://twitter.com/organicmapsapp/status/1824727403580596260 Last night Organic Maps was removed from the Play Store without any warnings or additional details due to \u0026quot;not meeting the requirements for the Family Program\u0026quot;. Compared to Google Maps and other maps apps rated for 3\u002B age, there are no ads or in-app purchases in Organic Maps. We have asked for an appeal. The app is still available on F-Droid . Much better place for getting apps in my opinion.","contentType":"reply","url":"https://www.lqdev.me/responses/organic-maps-removed-play-store","date":"2024-08-17 11:00 -05:00","tags":["organicmaps","google","play","fdroid","opensource","osm","openstreetmaps","maps","app","store"],"summary":"Organic Maps removed from Google Play Store \u2192 https://twitter.com/organicmapsapp/status/1824727403580596260 Last night Organic Maps was removed from the Play Store without any warnings or additional...","keywords":["maps","organic","removed","from","google","play","store","apps","rated","purchases"]},{"id":"reshare-new-flying-lotus-garmonbozia","title":"New Flying Lotus: Garmonbozia","content":"New Flying Lotus: Garmonbozia \u2192 https://flyinglotus.bandcamp.com/track/garmonbozia Good way to start a Friday. New drop from FlyLo. I like it. Garmonbozia by Flying Lotus","contentType":"reshare","url":"https://www.lqdev.me/responses/new-flying-lotus-garmonbozia","date":"2024-08-16 09:52 -05:00","tags":["music","flyinglotus","garmonbozia","newmusic","flylo","bandcamp"],"summary":"New Flying Lotus: Garmonbozia \u2192 https://flyinglotus.bandcamp.com/track/garmonbozia Good way to start a Friday. New drop from FlyLo. I like it. Garmonbozia by Flying Lotus","keywords":["flying","garmonbozia","lotus:","https://flyinglotus","bandcamp","com/track/garmonbozia","good","start","friday","drop"]},{"id":"bookmarks-transformer-explainer-interactive-learning-text-generative-models","title":"Transformer Explainer: Interactive Learning of Text-Generative Models","content":"Transformer Explainer: Interactive Learning of Text-Generative Models \u2192 https://arxiv.org/abs/2408.04619 Transformers have revolutionized machine learning, yet their inner workings remain opaque to many. We present Transformer Explainer, an interactive visualization tool designed for non-experts to learn about Transformers through the GPT-2 model. Our tool helps users understand complex Transformer concepts by integrating a model overview and enabling smooth transitions across abstraction levels of mathematical operations and model structures. It runs a live GPT-2 instance locally in the user\u0027s browser, empowering users to experiment with their own input and observe in real-time how the internal components and parameters of the Transformer work together to predict the next tokens. Our tool requires no installation or special hardware, broadening the public\u0027s education access to modern generative AI techniques. Our open-sourced tool is available at this https URL. A video demo is available at this https URL. Tool Video","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/transformer-explainer-interactive-learning-text-generative-models","date":"2024-08-14 21:51 -05:00","tags":["ai","transformer","neuralnetwork","learning","gpt"],"summary":"Transformer Explainer: Interactive Learning of Text-Generative Models \u2192 https://arxiv.org/abs/2408.04619 Transformers have revolutionized machine learning, yet their inner workings remain opaque to...","keywords":["tool","transformer","model","https","interactive","learning","available","generative","video","users"]},{"id":"bookmarks-hybridrag-knowledge-graphs-vector-rag-info-extraction","title":"HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction","content":"HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction \u2192 https://arxiv.org/abs/2408.04948 Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q\u0026amp;A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q\u0026amp;A format, and hence provide a natural set of pairs of ground-truth Q\u0026amp;As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/hybridrag-knowledge-graphs-vector-rag-info-extraction","date":"2024-08-14 21:49 -05:00","tags":["ai","rag","graph","database","knowledgegraph","kb","vector"],"summary":"HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction \u2192 https://arxiv.org/abs/2408.04948 Extraction and interpretation of intricate...","keywords":["retrieval","financial","generation","information","documents","vectorrag","both","q\u0026amp","extraction","techniques"]},{"id":"notes-ivy-cliq-2-camera-photo-printer","title":"Playing around with the Ivy Cliq\u002B 2","content":"I got my hands on an Ivy Cliq\u002B 2 earlier today and am really liking it. It\u0027s really cool that it\u0027s both a point-and-shoot camera as well as a photo printer. The camera comes in handy if you want to capture something in the moment and print it out immediately. The printer is great for pictures you\u0027ve taken on your phone or some other device. Even better, you can create collages, apply filters, and do all sorts of adjustments to images before printing. If there\u0027s one thing I didn\u0027t like is that in order to set it up, it needed access to my exact location. Trying approximate location wasn\u0027t enough. It connects over Bluetooth, so you\u0027d think that should be enough to pair. Not sure what it needs my EXACT location for.","contentType":"notes","url":"https://www.lqdev.me/notes/ivy-cliq-2-camera-photo-printer","date":"2024-08-13 22:29 -05:00","tags":["analog","camera","photos","journal","memory"],"summary":"I got my hands on an Ivy Cliq\u002B 2 earlier today and am really liking it. It\u0027s really cool that it\u0027s both a point-and-shoot camera as well as a photo printer. The camera comes in handy if you want to...","keywords":["location","printer","really","enough","camera","exact","apply","filters","sorts","adjustments"]},{"id":"reshare-us-govt-fcc-easier-unsubscribe","title":"The US government wants to make it easier for you to click the \u0027unsubscribe\u0027 button","content":"The US government wants to make it easier for you to click the \u0026#39;unsubscribe\u0026#39; button \u2192 https://apnews.com/article/consumer-protection-ftc-fcc-biden-250f6eece6e2665535019128e8fa38da Given my recent experience unsubscribing from content , there\u0027s definitely room for improvement.","contentType":"reshare","url":"https://www.lqdev.me/responses/us-govt-fcc-easier-unsubscribe","date":"2024-08-12 21:31 -05:00","tags":["regulation","fcc","subscriptions","uspol","government","us","darkpatterns"],"summary":"The US government wants to make it easier for you to click the \u0026#39;unsubscribe\u0026#39; button \u2192 https://apnews.com/article/consumer-protection-ftc-fcc-biden-250f6eece6e2665535019128e8fa38da Given my...","keywords":["government","definitely","content","from","unsubscribing","experience","recent","given","250f6eece6e2665535019128e8fa38da","room"]},{"id":"notes-2024-08-11-weekly-post-summary","title":"Week of August 11, 2024 - Post Summary","content":"Notes What is Kick? Responses Bluesky Welcomes Mike Masnick to Board of Directors Blogroll Club - A blog directory Jackson Hole\u0027s new Instagram filter warns you when you\u0027re dangerously close to wildlife Congratulations on ten years of Overcast CNET purchased for JUST $100 million GPT-4o System Card","contentType":"notes","url":"https://www.lqdev.me/notes/2024-08-11-weekly-post-summary","date":"2024-08-11 13:54 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes What is Kick? Responses Bluesky Welcomes Mike Masnick to Board of Directors Blogroll Club - A blog directory Jackson Hole\u0027s new Instagram filter warns you when you\u0027re dangerously close to...","keywords":["notes","million","$100","purchased","cnet","overcast","years","congratulations","wildlife","close"]},{"id":"bookmarks-gpt4o-system-card","title":"GPT-4o System Card","content":"GPT-4o System Card \u2192 https://openai.com/index/gpt-4o-system-card/ GPT-4o is an autoregressive omni model, which accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It\\u2019s trained end-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window)2 in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House3, we are sharing the GPT-4o System Card, which includes our Preparedness Framework(opens in a new window)5 evaluations. In this System Card, we provide a detailed look at GPT-4o\\u2019s capabilities, limitations, and safety evaluations across multiple categories, with a focus on speech-to-speech (voice)A while also evaluating text and image capabilities, and the measures we\\u2019ve taken to enhance safety and alignment. We also include third party assessments on general autonomous capabilities, as well as discussion of potential societal impacts of GPT-4o text and vision capabilities.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/gpt4o-system-card","date":"2024-08-08 20:16 -05:00","tags":["ai","openai","gpt-4o","documentation","llm"],"summary":"GPT-4o System Card \u2192 https://openai.com/index/gpt-4o-system-card/ GPT-4o is an autoregressive omni model, which accepts as input any combination of text, audio, image, and video and generates any...","keywords":["text","audio","system","capabilities","also","vision","card","image","which","milliseconds"]},{"id":"reshare-mike-masnick-bluesky-board-of-directors","title":"Bluesky Welcomes Mike Masnick to Board of Directors","content":"Bluesky Welcomes Mike Masnick to Board of Directors \u2192 https://bsky.social/about/blog/08-06-2024-board Nice! I remember reading Protocols, Not Platforms many years ago and felt inspired to seek out a better web.","contentType":"reshare","url":"https://www.lqdev.me/responses/mike-masnick-bluesky-board-of-directors","date":"2024-08-07 22:14 -05:00","tags":["bluesky","protocols","platforms","socialmedia","decentralization","distributedweb","indieweb","smallweb","humanweb","web"],"summary":"Bluesky Welcomes Mike Masnick to Board of Directors \u2192 https://bsky.social/about/blog/08-06-2024-board Nice! I remember reading Protocols, Not Platforms many years ago and felt inspired to seek out a...","keywords":["board","bluesky","inspired","felt","years","many","platforms","protocols","reading","remember"]},{"id":"reshare-jackson-hole-instagram-filter-wildlife","title":"Jackson Hole\u0027s new Instagram filter warns you when you\u0027re dangerously close to wildlife","content":"Jackson Hole\u0026#39;s new Instagram filter warns you when you\u0026#39;re dangerously close to wildlife \u2192 https://www.theverge.com/2024/8/7/24215490/jackson-hole-travel-tourism-board-instagram-filter-animals-national-park-safety","contentType":"reshare","url":"https://www.lqdev.me/responses/jackson-hole-instagram-filter-wildlife","date":"2024-08-07 21:59 -05:00","tags":["wildlife","wyoming","tourons","instagram","socialmedia","fun","jacksonhole","teton","nationalpark"],"summary":"Jackson Hole\u0026#39;s new Instagram filter warns you when you\u0026#39;re dangerously close to wildlife \u2192 https://www.theverge.com/2024/8/7/24215490/jackson-hole-travel-tourism-board-instagram-filter-animals-...","keywords":["instagram","filter","jackson","national","animals","board","tourism","travel","hole","com/2024/8/7/24215490/jackson"]},{"id":"reply-cnet-bought-for-just-100-million","title":"CNET purchased for JUST $100 million","content":"CNET purchased for JUST $100 million \u2192 https://www.theverge.com/2024/8/6/24214374/cnet-zeff-davis-acquisition-digital-media-100-million JUST $100 million. That\u0027s pocket change \uD83D\uDE43 Red Ventures paid $500 million for the tech property once valued at $1.8 billion. Given previous valuations though, I see why the headline was phrased that way.","contentType":"reply","url":"https://www.lqdev.me/responses/cnet-bought-for-just-100-million","date":"2024-08-06 09:06 -05:00","tags":["cnet","tech","acquisition","news"],"summary":"CNET purchased for JUST $100 million \u2192 https://www.theverge.com/2024/8/6/24214374/cnet-zeff-davis-acquisition-digital-media-100-million JUST $100 million. That\u0027s pocket change \uD83D\uDE43 Red Ventures paid...","keywords":["million","$100","cnet","though","valuations","previous","given","billion","valued","once"]},{"id":"notes-what-is-kick","title":"What is Kick?","content":"I\u0027m oficially old. I had to look up what Kick is. In case you also didn\u0027t know, I\u0027ll save you some time. Kick is a platform where you can watch and interact with live streamers from various categories, such as games, music, gambling and creative.","contentType":"notes","url":"https://www.lqdev.me/notes/what-is-kick","date":"2024-08-05 21:30 -05:00","tags":["livestream","video","app","web","internet"],"summary":"I\u0027m oficially old. I had to look up what Kick is. In case you also didn\u0027t know, I\u0027ll save you some time. Kick is a platform where you can watch and interact with live streamers from various...","keywords":["kick","oficially","music","games","categories","various","from","streamers","live","interact"]},{"id":"reshare-blogroll-club-blog-directory","title":"Blogroll Club - A blog directory","content":"Blogroll Club - A blog directory \u2192 https://notes.jeddacp.com/a-blog-directory/ A Blog Directory is a cool project from JC (Probably) and Lou Plummer . Check it out !","contentType":"reshare","url":"https://www.lqdev.me/responses/blogroll-club-blog-directory","date":"2024-08-05 20:53 -05:00","tags":["blog","rss","indieweb","blogroll","community","web","smallweb","personalweb"],"summary":"Blogroll Club - A blog directory \u2192 https://notes.jeddacp.com/a-blog-directory/ A Blog Directory is a cool project from JC (Probably) and Lou Plummer . Check it out !","keywords":["blog","directory","blogroll","club","https://notes","jeddacp","com/a","directory/","cool","project"]},{"id":"reshare-congratulations-overcast-ten-years","title":"Congratulations on ten years of Overcast","content":"Congratulations on ten years of Overcast \u2192 https://marco.org/2024/07/16/overcast-rewrite I no longer have an iPhone, but whenever anyone asks for a podcast app recommendation, Overcast is the first one I mention. Congrats on 10 years. Here\u0027s to many more.","contentType":"reshare","url":"https://www.lqdev.me/responses/congratulations-overcast-ten-years","date":"2024-08-04 17:07 -05:00","tags":["podcast","app","overcast","anniversary","milestone","apple","ios","rss"],"summary":"Congratulations on ten years of Overcast \u2192 https://marco.org/2024/07/16/overcast-rewrite I no longer have an iPhone, but whenever anyone asks for a podcast app recommendation, Overcast is the first...","keywords":["years","overcast","congratulations","here","congrats","mention","recommendation","podcast","asks","whenever"]},{"id":"notes-post-statistics-august-2024","title":"Website Post Statistics - August 2024","content":"Finished July with more regular posting. Only 4 posts away from passing last year\u0027s response count. As mentioned on another post , I\u0027ve been slowly working on transitioning authoring workflows to Emacs using org-capture templates. This month, I want to work on making capturing content from Nyxt more seamless, especially since responses are the types of posts I use most. The other thing I\u0027d like to start doing is consolidating the structure of my posts. Right now the YAML front-matter for the various posts types is significantly different. However, it doesn\u0027t have to be. For most posts, I need: Title Date published Data updated Tags Responses are a little different because I also need to include the link to the original content I\u0027m referencing in the post. Once that\u0027s done, at some point, I\u0027d also like to consolidate my URLs. /posts captures long-form blog posts /feed captures microblog posts and responses Since everything is a post, consolidating under one of them for all my posts and providing additional urls like /notes , /photos , /articles for the individual types of posts might make more sense.","contentType":"notes","url":"https://www.lqdev.me/notes/post-statistics-august-2024","date":"2024-08-01 20:19 -05:00","tags":["githubactions","cicd","blogging","stats","github"],"summary":"Finished July with more regular posting. Only 4 posts away from passing last year\u0027s response count. As mentioned on another post , I\u0027ve been slowly working on transitioning authoring workflows to...","keywords":["posts","types","more","post","responses","content","also","urls","captures","since"]},{"id":"notes-website-feeds-opml","title":"OPML for website feeds","content":"While thiking about implementing .well-known for RSS feeds on my site , I had another idea. Since that uses OPML anyways, I remembered recently doing something similar for my blogroll . The concept is the same, except instead of making my blogroll discoverable, I\u0027m doing it for my feeds. At the end of the day, a blogroll is a collection of feeds, so it should just work for my own feeds. The implementation ended up being: Create an OPML file for each of the feeds on by website . \u0026lt;opml version=\u0026quot;2.0\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Luis Quintanilla Feeds\u0026lt;/title\u0026gt; \u0026lt;ownerId\u0026gt;https://www.luisquintanilla.me\u0026lt;/ownerId\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;outline title=\u0026quot;Blog\u0026quot; text=\u0026quot;Blog\u0026quot; type=\u0026quot;rss\u0026quot; htmlUrl=\u0026quot;/posts/1\u0026quot; xmlUrl=\u0026quot;/blog.rss\u0026quot; /\u0026gt; \u0026lt;outline title=\u0026quot;Microblog\u0026quot; text=\u0026quot;Microblog\u0026quot; type=\u0026quot;rss\u0026quot; htmlUrl=\u0026quot;/feed\u0026quot; xmlUrl=\u0026quot;/microblog.rss\u0026quot; /\u0026gt; \u0026lt;outline title=\u0026quot;Responses\u0026quot; text=\u0026quot;Responses\u0026quot; type=\u0026quot;rss\u0026quot; htmlUrl=\u0026quot;/feed/responses\u0026quot; xmlUrl=\u0026quot;/responses.rss\u0026quot; /\u0026gt; \u0026lt;outline title=\u0026quot;Mastodon\u0026quot; text=\u0026quot;Mastodon\u0026quot; type=\u0026quot;rss\u0026quot; htmlUrl=\u0026quot;/mastodon\u0026quot; xmlUrl=\u0026quot;/mastodon.rss\u0026quot; /\u0026gt; \u0026lt;outline title=\u0026quot;Bluesky\u0026quot; text=\u0026quot;Bluesky\u0026quot; type=\u0026quot;rss\u0026quot; htmlUrl=\u0026quot;/bluesky\u0026quot; xmlUrl=\u0026quot;/bluesky.rss\u0026quot; /\u0026gt; \u0026lt;outline title=\u0026quot;YouTube\u0026quot; text=\u0026quot;YouTube\u0026quot; type=\u0026quot;rss\u0026quot; htmlUrl=\u0026quot;/youtube\u0026quot; xmlUrl=\u0026quot;/bluesky.rss\u0026quot; /\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/opml\u0026gt; Add a link tag to the head element of my website. \u0026lt;link rel=\u0026quot;feeds\u0026quot; type=\u0026quot;text/xml\u0026quot; title=\u0026quot;Luis Quintanilla\u0027s Feeds\u0026quot; href=\u0026quot;/feed/index.opml\u0026quot;\u0026gt;","contentType":"notes","url":"https://www.lqdev.me/notes/website-feeds-opml","date":"2024-08-01 20:08 -05:00","tags":["rss","opml","indieweb","feeds","internet","protocols","webstandards","standards","web"],"summary":"While thiking about implementing .well-known for RSS feeds on my site , I had another idea. Since that uses OPML anyways, I remembered recently doing something similar for my blogroll . The concept...","keywords":["rss\u0026quot","title=\u0026quot","type=\u0026quot","htmlurl=\u0026quot","/\u0026gt","text=\u0026quot","outline","xmlurl=\u0026quot","feeds","opml"]},{"id":"notes-crate-finds-july-2024","title":"Crate Finds - July 2024","content":"Grateful Dead on repeat this month. Songs I kept coming back to: Standing on the Moon Eyes of the World Fire on the Mountain A few Doors songs also got in there I hadn\u0027t listened to before like Blue Sunday. Tracks Fire on the Mountain - 2013 Remaster by Grateful Dead Album: Shakedown Street Duration: 3:49 Listen on YouTube Backup: Listen on Spotify Standing on the Moon - 2013 remaster by Grateful Dead Album: Built to Last (2013 Remaster) Duration: 5:23 Listen on YouTube Backup: Listen on Spotify Terrapin Station Medley - 2014 Remaster by Grateful Dead Album: Terrapin Station (2014 Remaster) Duration: 16:22 Listen on YouTube Backup: Listen on Spotify Eyes of the World - 2013 Remaster by Grateful Dead Album: Wake of the Flood (2013 Remaster) Duration: 5:20 Listen on YouTube Backup: Listen on Spotify Riverside by America Album: America Duration: 3:01 Listen on YouTube Backup: Listen on Spotify Only A Fool Would Say That by Steely Dan Album: Can\u0027t Buy A Thrill Duration: 2:55 Listen on YouTube Backup: Listen on Spotify The Spy by The Doors Album: Morrison Hotel Duration: 4:15 Listen on YouTube Backup: Listen on Spotify Stella Blue - 2013 Remaster by Grateful Dead Album: Wake of the Flood (2013 Remaster) Duration: 6:23 Listen on YouTube Backup: Listen on Spotify Indian Summer by The Doors Album: Morrison Hotel Duration: 2:34 Listen on YouTube Backup: Listen on Spotify Touch of Grey by Grateful Dead Album: The Very Best of the Grateful Dead Duration: 5:47 Listen on YouTube Backup: Listen on Spotify Are You Leaving for the Country by Karen Dalton Album: In My Own Time (50th Anniversary Edition) Duration: 3:14 Listen on YouTube Backup: Listen on Spotify Midwest Gospel Radio by Yonder Mountain String Band Album: Yonder Mountain String Band Duration: 2:32 Listen on YouTube Backup: Listen on Spotify Help on the Way / Slipknot! - 2013 Remaster by Grateful Dead Album: Blues for Allah (2013 Remaster) Duration: 7:21 Listen on YouTube Backup: Listen on Spotify Dark Star - Single Version; 2001 Remaster by Grateful Dead Album: Complete Studio Rarities Collection Duration: 2:43 Listen on YouTube Backup: Listen on Spotify Ram On - Remastered 2012 by Paul McCartney, Linda McCartney Album: Ram (Archive Collection) Duration: 2:29 Listen on YouTube Backup: Listen on Spotify Night By Night by Steely Dan Album: Pretzel Logic Duration: 3:38 Listen on YouTube Backup: Listen on Spotify Fire In The Hole by Steely Dan Album: Can\u0027t Buy A Thrill Duration: 3:29 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-july-2024","date":"2024-08-01 20:06 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"Grateful Dead on repeat this month. Songs I kept coming back to: Standing on the Moon Eyes of the World Fire on the Mountain A few Doors songs also got in there I hadn\u0027t listened to before like Blue...","keywords":["listen","spotify","duration:","album:","youtube","backup:","remaster","grateful","dead","2013"]},{"id":"reply-reddit-companies-pay-data-access","title":"Reddit says companies must pay for data access","content":"Reddit says companies must pay for data access \u2192 https://www.theverge.com/2024/7/31/24210565/reddit-microsoft-anthropic-perplexity-pay-ai-search I\u0027m all for compensation. Remind me again, how much are the communities that create the content and make Reddit what it is getting out of these licensing deals?","contentType":"reply","url":"https://www.lqdev.me/responses/reddit-companies-pay-data-access","date":"2024-07-31 16:23 -05:00","tags":["ai","reddit","data","licensing","microsoft","anthropic","perplexity"],"summary":"Reddit says companies must pay for data access \u2192 https://www.theverge.com/2024/7/31/24210565/reddit-microsoft-anthropic-perplexity-pay-ai-search I\u0027m all for compensation. Remind me again, how much...","keywords":["reddit","getting","content","create","communities","much","again","remind","compensation","search"]},{"id":"reply-well-known-feeds","title":".well-known feeds","content":".well-known feeds \u2192 https://kiko.io/post/My-well-known-feeds-and-thoughts-beyond/ This is a clever use of .well-known and OPML. Definitely something I want to experiment with and implement on my site even if it\u0027s not widely adopted.","contentType":"reply","url":"https://www.lqdev.me/responses/well-known-feeds","date":"2024-07-31 09:01 -05:00","tags":["standards","feeds","rss","web","indieweb","smallweb"],"summary":".well-known feeds \u2192 https://kiko.io/post/My-well-known-feeds-and-thoughts-beyond/ This is a clever use of .well-known and OPML. Definitely something I want to experiment with and implement on my site...","keywords":["known","feeds","https://kiko","io/post/my","thoughts","beyond/","clever","opml","definitely","want"]},{"id":"notes-subscription-dark-patterns","title":"Subscription dark patterns","content":"I recently took inventory of subscriptions I\u0027m no longer using. The ones that were especially tricky to spot were the annual subscriptions that don\u0027t show up on bank statements every month. These also happened to be ones that I hadn\u0027t used in years. Getting a list of them was the easy part. The fun didn\u0027t start until I actually went through the process of unsubscribing. For a few of them, cancelling was relatively straightforward. Others, it was an adventure. Best case, I was presented with several screens where the option to keep my subscription was front and center and the cancel button was in small print all the way at the bottom of the screen. In other cases, I had to send an e-mail to cancel. My favorite though was the one where I was able to log in and access member content. When I went to the account settings to try and cancel my subscription, they were conveniently having technical difficulties \uD83D\uDE43 Super frustrating experience all around, but happy to get it done so I get to keep more of my money.","contentType":"notes","url":"https://www.lqdev.me/notes/subscription-dark-patterns","date":"2024-07-30 20:37 -05:00","tags":["darkpattern","subscriptions","services"],"summary":"I recently took inventory of subscriptions I\u0027m no longer using. The ones that were especially tricky to spot were the annual subscriptions that don\u0027t show up on bank statements every month. These...","keywords":["cancel","keep","subscriptions","subscription","ones","went","recently","mail","send","cases"]},{"id":"bookmarks-longrope-extending-llm-context-2m-tokens","title":"LongROPE: Extending LLM Context Window Beyond 2 Million Tokens","content":"LongROPE: Extending LLM Context Window Beyond 2 Million Tokens \u2192 https://arxiv.org/abs/2402.13753 Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations. GitHub Repo","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/longrope-extending-llm-context-2m-tokens","date":"2024-07-30 14:16 -05:00","tags":["ai","llm","longrope","microsoft","research","msr"],"summary":"LongROPE: Extending LLM Context Window Beyond 2 Million Tokens \u2192 https://arxiv.org/abs/2402.13753 Large context window is a desirable feature in large language models (LLMs). However, due to high...","keywords":["context","window","fine","tuning","positional","longrope","tokens","extended","original","length"]},{"id":"star-tensors-from-scratch-blog-series","title":"Tensors from scratch series","content":"Tensors from scratch series \u2192 https://maharshi.bearblog.dev/tensors-from-scratch-part-2/ I\u0027ve been enjoying reading through this Tensors series. If you\u0027re interested, here\u0027s also the link to part 1 .","contentType":"star","url":"https://www.lqdev.me/responses/tensors-from-scratch-blog-series","date":"2024-07-29 22:26 -05:00","tags":["tensors","math","ai","c","machinelearning","ml","artificialintelligence"],"summary":"Tensors from scratch series \u2192 https://maharshi.bearblog.dev/tensors-from-scratch-part-2/ I\u0027ve been enjoying reading through this Tensors series. If you\u0027re interested, here\u0027s also the link to part 1 .","keywords":["tensors","from","scratch","series","part","https://maharshi","bearblog","dev/tensors","enjoying","reading"]},{"id":"reshare-meta-ai-segment-anything-model-2","title":"Meta AI\u0027s Segment Anything Model (SAM) 2","content":"Meta AI\u0026#39;s Segment Anything Model (SAM) 2 \u2192 https://ai.meta.com/blog/segment-anything-2/ Takeaways Following up on the success of the Meta Segment Anything Model (SAM) for images, we\u0027re releasing SAM 2, a unified model for real-time promptable object segmentation in images and videos that achieves state-of-the-art performance. In keeping with our approach to open science, we\u0027re sharing the code and model weights with a permissive Apache 2.0 license. We\u0027re also sharing the SA-V dataset, which includes approximately 51,000 real-world videos and more than 600,000 masklets (spatio-temporal masks). SAM 2 can segment any object in any video or image - even for objects and visual domains it has not seen previously, enabling a diverse range of use cases without custom adaptation. SAM 2 has many potential real-world applications. For example, the outputs of SAM 2 can be used with a generative video model to create new video effects and unlock new creative applications. SAM 2 could also aid in faster annotation tools for visual data to build better computer vision systems.","contentType":"reshare","url":"https://www.lqdev.me/responses/meta-ai-segment-anything-model-2","date":"2024-07-29 22:22 -05:00","tags":["meta","ai","computervision","sam2","segmentanythingmodel","aimodel","cv"],"summary":"Meta AI\u0026#39;s Segment Anything Model (SAM) 2 \u2192 https://ai.meta.com/blog/segment-anything-2/ Takeaways Following up on the success of the Meta Segment Anything Model (SAM) for images, we\u0027re releasing...","keywords":["model","real","meta","video","segment","anything","visual","world","also","sharing"]},{"id":"notes-org-capture-functions-file-target","title":"User-provided target file names in Org Capture","content":"I finally got my website note org-capture template working for new files. Originally, I only had it working on existing files using this snippet. (\u0026quot;wne\u0026quot; \u0026quot;Creates a note in an existing file\u0026quot; plain (file buffer-file-name) (file ,(file-name-concat website-template-dir \u0026quot;note.txt\u0026quot;))) This template uses the file name of the current buffer to select the insertion target. Getting it to work with a new file specified by the user, requires a small tweak. (\u0026quot;wnn\u0026quot; \u0026quot;Creates a note in a new file\u0026quot; plain (file (lambda () (file-name-concat website-note-dir (format \u0026quot;%s.md\u0026quot; (read-string \u0026quot;Enter file name: \u0026quot;))))) (file ,(file-name-concat website-template-dir \u0026quot;note.txt\u0026quot;))) At first, I thought I had to use the function target type since I wanted to use a function to capture the file name. That didn\u0027t work. I then realized, I could still keep the file target type. However, to fill in the file name, I could use a function which takes in user input. Now that I got this working, I\u0027ve also done the same for my reponse template.","contentType":"notes","url":"https://www.lqdev.me/notes/org-capture-functions-file-target","date":"2024-07-29 21:45 -05:00","tags":["emacs","orgmode","capture","org","website","templates","gnu"],"summary":"I finally got my website note org-capture template working for new files. Originally, I only had it working on existing files using this snippet. (\u0026quot;wne\u0026quot; \u0026quot;Creates a note in an existing...","keywords":["file","\u0026quot","note","template","website","target","working","concat","function","user"]},{"id":"notes-useful-emacs-commands-pt-1","title":"Note to Self: Useful Emacs Commands Pt. 1","content":"I still need to set up an org-capture template for snippets, so writing a note to myself to remember these commands I\u0027ll eventually want to come back to. Capture elfeed link In this custom function, elfeed-show-yank extracts the link element in an elfeed entry. org-capture then just invokes the org-capture template selection prompt. At this point, I can move forward with creating a response entry on the website and since the link to the entry I was viewing in elfeed is in the kill-ring, I can easily paste the URL while filling out the org-capture template. (defun capture-elfeed-entry () (elfeed-show-yank) (org-capture)) Org capture contexts I recently found out, I can add filters to org-capture templates based on the mode I\u0027m in Emacs. Here\u0027s an example: (setq org-capture-templates-contexts \u0027((\u0026quot;wrn\u0026quot; ((in-mode . \u0026quot;elfeed-show-mode\u0026quot;))))) org-capture-templates-contexts defines a set of conditions that defines which contexts certain org-capture templates appear under. For example, the org-capture template mapped to wrn will only be visible when org-capture is invoked from a buffer in elfeed-show-mode . More information can be found in the org-capture templates in context documentation","contentType":"notes","url":"https://www.lqdev.me/notes/useful-emacs-commands-pt-1","date":"2024-07-28 17:00 -05:00","tags":["emacs","lisp","elisp","programming","code","notetoself"],"summary":"I still need to set up an org-capture template for snippets, so writing a note to myself to remember these commands I\u0027ll eventually want to come back to. Capture elfeed link In this custom function,...","keywords":["capture","elfeed","templates","entry","template","contexts","show","link","mode","\u0026quot"]},{"id":"bookmarks-nyxt-emacs-hacks","title":"Nyxt Emacs Hacks","content":"Nyxt Emacs Hacks \u2192 https://nyxt.atlas.engineer/article/emacs-hacks.org Saving this guide for future reference as I set up my elfeed / Nyxt capture workflows for the website. Additional articles that might be helpful. Org capture in Nyxt: Taking Notes While Browsing","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/nyxt-emacs-hacks","date":"2024-07-28 16:53 -05:00","tags":["nyxt","emacs","orgmode","capture","templates","workflow","lisp"],"summary":"Nyxt Emacs Hacks \u2192 https://nyxt.atlas.engineer/article/emacs-hacks.org Saving this guide for future reference as I set up my elfeed / Nyxt capture workflows for the website. Additional articles that...","keywords":["nyxt","hacks","capture","notes","taking","nyxt:","helpful","articles","additional","website"]},{"id":"notes-2024-07-28-weekly-post-summary","title":"Week of July 28, 2024 - Post Summary","content":"Notes Week of July 21, 2024 - Post Summary Crate Finds - June 2024 Note to Self: Useful Emacs Commands Pt. 1 Responses Nyxt Emacs Hacks","contentType":"notes","url":"https://www.lqdev.me/notes/2024-07-28-weekly-post-summary","date":"2024-07-28 12:27 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes Week of July 21, 2024 - Post Summary Crate Finds - June 2024 Note to Self: Useful Emacs Commands Pt. 1 Responses Nyxt Emacs Hacks","keywords":["2024","emacs","notes","responses","commands","useful","self:","note","june","finds"]},{"id":"notes-crate-finds-june-2024","title":"Crate Finds - June 2024","content":"A few new songs. Besides listening to them a few times though, there aren\u0027t many I came back to throughout the month. I\u0027m excited for the new album from Thee Sacred Souls though. Tracks Wasted Words (feat. Thundercat) by KAYTRANADA, Thundercat Album: TIMELESS Duration: 1:27 Listen on YouTube Backup: Listen on Spotify Sweet Vibe by Rasputin\u0027s Stash Album: Hidden Stash Duration: 3:25 Listen on YouTube Backup: Listen on Spotify Love Heart Cheat Code by Hiatus Kaiyote Album: Love Heart Cheat Code Duration: 4:33 Listen on YouTube Backup: Listen on Spotify Mirage by Glass Beams Album: Mirage Duration: 4:50 Listen on YouTube Backup: Listen on Spotify Drinking from the Cup of Bob Knob by Surprise Chef Album: All News Is Good News Duration: 3:59 Listen on YouTube Backup: Listen on Spotify Necking by Scone Cash Players Album: Blast Furnace! Duration: 2:34 Listen on YouTube Backup: Listen on Spotify You Ready by Telemakus, Marcelo Soler, Tane Album: The New Heritage Duration: 3:04 Listen on YouTube Backup: Listen on Spotify Let\u0027s Dream Anyhow - Remastered by Piero Piccioni Album: Il Dio sotto la pelle - God Under the Skin (Original Motion Picture Soundtrack) Duration: 1:46 Listen on YouTube Backup: Listen on Spotify Coming Home by beabadoobee Album: Coming Home Duration: 2:16 Listen on YouTube Backup: Listen on Spotify Euphoric Avenue by Brijean Album: Euphoric Avenue Duration: 3:54 Listen on YouTube Backup: Listen on Spotify Coraz\u00F3n Fr\u00EDo by EKA Norte\u00F1a Album: Coraz\u00F3n Fr\u00EDo Duration: 3:32 Listen on YouTube Backup: Listen on Spotify PSICOD\u00C9LICO by Sucesi\u00F3n M Album: PSICOD\u00C9LICO Duration: 4:14 Listen on YouTube Backup: Listen on Spotify Lover Boy by Jason Joshua Album: La Voz De Oro Duration: 2:55 Listen on YouTube Backup: Listen on Spotify Oh Yes by The Expert Album: Excursions Duration: 1:54 Listen on YouTube Backup: Listen on Spotify Together by Misha Panfilov Album: Days As Echoes Duration: 6:34 Listen on YouTube Backup: Listen on Spotify Need 2 by Pinegrove Album: Everything So Far Duration: 3:11 Listen on YouTube Backup: Listen on Spotify La Ametralladora by El Halcon De La Sierra Album: Con Sus Amigos Vol.10 Duration: 2:32 Listen on YouTube Backup: Listen on Spotify Lucid Girl by Thee Sacred Souls Album: Lucid Girl Duration: 3:22 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-june-2024","date":"2024-07-21 18:29 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"A few new songs. Besides listening to them a few times though, there aren\u0027t many I came back to throughout the month. I\u0027m excited for the new album from Thee Sacred Souls though. Tracks Wasted Words...","keywords":["listen","spotify","backup:","youtube","duration:","album:","mirage","avenue","euphoric","psicod\u00E9lico"]},{"id":"notes-2024-07-21-weekly-post-summary","title":"Week of July 21, 2024 - Post Summary","content":"Notes Crate Finds - June 2024 Testing org-capture template for notes Website Post Statistics - July 2024 Responses Zombie Internet Test response using org capture templates","contentType":"notes","url":"https://www.lqdev.me/notes/2024-07-21-weekly-post-summary","date":"2024-07-21 13:50 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes Crate Finds - June 2024 Testing org-capture template for notes Website Post Statistics - July 2024 Responses Zombie Internet Test response using org capture templates","keywords":["notes","2024","capture","response","test","internet","zombie","responses","july","statistics"]},{"id":"notes-post-statistics-july-2024","title":"Website Post Statistics - July 2024","content":"Better late than never. Nothing too interesting this month. I haven\u0027t been posting as much. I think part of that is because I\u0027ve been creating DayOne posts more. I\u0027ve been relying on DayOne to share articles from my RSS reader. That\u0027s worked really well for bookmarking. As far as my authoring process, when working locally, I\u0027ve been using Emacs more. One reason for Emacs is capture templates . Technically, I already have something similar with VS Code Snippets . I like the richness of the Emacs experience more though. The future workflow I\u0027m thinking of involves two components: Elfeed Nyxt When reading articles using Elfeed, I can create a function that takes the information from the entry I\u0027m on and converts it into an entry on my site using capture templates. Similarly, in Nyxt while browsing the internet, I can create a similar function which uses capture templates to create website entries. Maybe I can get a long-form post out of that experiment. As far as numbers go, I\u0027m at 56 notes for the year which is 15 more than last year. I\u0027m closing in on passing last year\u0027s response count at 210. Most of the content shared continues to be mainly about AI. Top linked domain is still The Verge.","contentType":"notes","url":"https://www.lqdev.me/notes/post-statistics-july-2024","date":"2024-07-18 18:26 -05:00","tags":["githubactions","cicd","blogging","stats","github"],"summary":"Better late than never. Nothing too interesting this month. I haven\u0027t been posting as much. I think part of that is because I\u0027ve been creating DayOne posts more. I\u0027ve been relying on DayOne to share...","keywords":["more","year","capture","templates","emacs","using","create","entry","which","elfeed"]},{"id":"reply-facebook-zombie-internet-willison","title":"Zombie Internet","content":"Zombie Internet \u2192 https://simonwillison.net/2024/Jul/15/facebook-is-the-zombie-internet/#atom-everything In my experience, the supermajority of engagement on viral AI Facebook pages is just as artificially-generated as the content they publish. Whether it\u0027s a child transforming into a water bottle cyborg, a three-armed flight attendant rescuing Tiger Jesus from a muddy plane crash, or a hybrid human-monkey baby being stung to death by giant hornets, all tend to have copy\u002Bpasted captions, reactions \u0026amp; comments which usually make no sense in the observed context. I\u0027ve noticed similar patterns on YouTube. Sometime the comments include timestamp links which makes them seem more credible, but upon further inspection, it\u0027s all bot activity.","contentType":"reply","url":"https://www.lqdev.me/responses/facebook-zombie-internet-willison","date":"2024-07-16 11:48 -05:00","tags":["bots","web","comments","ai","facebook","internet","personalweb"],"summary":"Zombie Internet \u2192 https://simonwillison.net/2024/Jul/15/facebook-is-the-zombie-internet/#atom-everything In my experience, the supermajority of engagement on viral AI Facebook pages is just as...","keywords":["zombie","comments","which","monkey","baby","stung","death","giant","hornets","tend"]},{"id":"wiki-nix-upgrade-versions","title":"Upgrade NixOS versions","content":"Overview This guide provides general guidance on upgrading between NixOS versions Check which version you\u0027re currently running cat /etc/lsb_release Get list of channels This provides the URL used to download packages for nixos release nix-channel --list | grep nixos Add / Replace Software Channel To get on the latest version, you need to update the nixos channel to the latest version. You can find a list of versions in this repository . For example, if you wanted to upgrade to the latest 24.05 version, you\u0027d use the following command: nix-channel --add https://channels.nixos.org/nixos-24.05 nixos The general format is: nix-channel --add \u0026lt;CHANNEL_URL\u0026gt; nixos Rebuild your system Once you\u0027ve configured the channel for the latest version, switch to it just like you would when upgading sofware packages. nixos-rebuild switch --upgrade After the operation completes, you\u0027ll want to check which version is running as mentioned in previous instructions. Additional Resources Upgrading Nix OS","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/nix-upgrade-versions","date":"2024-07-14 14:49 -05:00","tags":["linux","sysadmin","nixos","nix","os"],"summary":"Overview This guide provides general guidance on upgrading between NixOS versions Check which version you\u0027re currently running cat /etc/lsb_release Get list of channels This provides the URL used to...","keywords":["nixos","channel","version","latest","list","switch","release","rebuild","running","check"]},{"id":"reshare-test-emacs-capture-response-org","title":"Test response using org capture templates","content":"Test response using org capture templates \u2192 https://orgmode.org/manual/Capture-templates.html Testing org-capture template generated response file","contentType":"reshare","url":"https://www.lqdev.me/responses/test-emacs-capture-response-org","date":"2024-07-14 13:47 -05:00","tags":["org","emacs","blogging","automation","orgmode","templates","capturetemplates","website","personalweb","capture"],"summary":"Test response using org capture templates \u2192 https://orgmode.org/manual/Capture-templates.html Testing org-capture template generated response file","keywords":["response","capture","templates","test","using","https://orgmode","org/manual/capture","html","testing","template"]},{"id":"notes-love-plain-text-org-mode-clocktable","title":"Clock Tables - Org Mode, Plain Text, and AI","content":"Org-mode appreciation post. I use plain text and org-mode for most things in my life, especially when it comes to task and life management. I won\u0027t rehash all the reasons Emacs and org-mode are amazing. There are tons of blog posts and videos out there that would do it more justice than I ever could. Over the last few years, Emacs has become my go-to text editor. Throughout all that time, I\u0027ve continued to find new features that delight. The most recent is clock table . I already use org-mode to track my to-dos and perform some sort of time-block planning by setting deadlines and scheduling tasks. Recently though, I\u0027ve been wanting a way to see all of the things I\u0027ve worked on over the past [ INSERT TIME PERIOD ]. More importantly, I\u0027d like to have time associated with them to see where my time has gone and evaluate whether I\u0027m spending time on the things I should be. I knew you could clock in and clock out on tasks. However, I didn\u0027t know you could easily build a customized report that automatically updates. That\u0027s when I came across clock tables. Now, I have a way of visualizing all of the things I worked on during a week or month, and as I\u0027m planning for the next week or month, I can adjust and reprioritize the things I\u0027m working on. I know there are enterprise offerings like the Viva suite from Microsoft which provides detailed reports on how you spend your time. What excites me about org-mode though is that it\u0027s plain text. The clock table report that gets generated is a plain text table which makes it portable and easy to access using any text editor of your choice. It works best with Emacs, but that\u0027s not a requirement. On their own, clock tables are amazing. However, given how well language models work on plain text, they could be used as context for your queries. Imagine giving a language model as input an org file which contains: A clock table A list of TODO tasks (with notes, priorities, deadlines, tags, properties, and other annotations) A high level list of goals you want to achieve Then, you could enter a prompt along the lines of: \u0026quot;Using the following clock-table and list of goals I want to achieve, provide me with recommendations of tasks I should work on for the next week. Ensure that they align with my goals, are top priority, and provide the highest return on my efforts\u0026quot;. Additionally, you might also provide your org-agenda view containing appointments and use the results from the first query as context for the following prompt: \u0026quot;Given the agenda view for the next week, schedule the top 3 tasks you recommended\u0026quot;. The result would be a list of TODO items containing schedule / active timestamps annotations which now show up on your org-agenda view. Today, almost every productivity application out there is working on building these kinds of AI features into their products. The difference is, in many cases, the formats used by those applications aren\u0027t plain text. This adds complexity to the data extraction and transformation pipelines required to get the data into the right format for a language model to process. With plain text, there\u0027s little to no transformations required. What\u0027s even better, I can extend org-mode and Emacs using elisp to do the things I just talked about. I\u0027m no elisp expert, so I asked Copilot to generate an elisp function that takes an org-mode clock table as input and generates a written text summary of it. Here are the results: (defun org-clock-summary-to-chatgpt () \u0026quot;Extracts clock table data and requests a summary from ChatGPT.\u0026quot; (interactive) (let* ((clock-table-data (org-clocktable-get-clocktable)) (summary-text (org-clocktable-format-summary clock-table-data)) (api-endpoint \u0026quot;https://api.openai.com/v1/engines/davinci/completions\u0026quot;) (api-key \u0026quot;YOUR_API_KEY\u0026quot;)) ; Replace with your actual API key ;; Construct the HTTP request and send it to ChatGPT (with-current-buffer (url-retrieve-synchronously api-endpoint \u0060((\u0026quot;text\u0026quot; . ,summary-text) (\u0026quot;api_key\u0026quot; . ,api-key))) (goto-char (point-min)) (search-forward \u0026quot;\\n\\n\u0026quot;) (let ((response-json (json-read))) (message \u0026quot;ChatGPT summary: %s\u0026quot; (cdr (assoc \u0027choices response-json))))))) All you elisp experts out there, let me know how it did. I know at least the API endpoint is wrong, but generally speaking, it seems to be doing the right thing. Such function could be extended to include org-agenda information, TODO items, and many other things that would use AI to augment the existing functionality of Emacs and org-mode to tailor them to my needs.","contentType":"notes","url":"https://www.lqdev.me/notes/love-plain-text-org-mode-clocktable","date":"2024-07-09 22:07 -05:00","tags":["emacs","orgmode","ai","plaintext","productivity","tools","technology","gnu","opensource","gtd","calendar","agenda","llm","openai"],"summary":"Org-mode appreciation post. I use plain text and org-mode for most things in my life, especially when it comes to task and life management. I won\u0027t rehash all the reasons Emacs and org-mode are...","keywords":["clock","text","\u0026quot","table","mode","things","plain","summary","data","emacs"]},{"id":"notes-stevenson-vs-harutyunyan","title":"Stevenson vs. Harutyunyan","content":"Scorecard (Unofficial) Rd 1 2 3 4 5 6 7 8 9 10 11 12 Stevenson 9 9 9 10 10 10 10 10 9 9 10 10 Harutyunyan 10 10 10 9 9 9 9 9 10 10 9 9 Shakur did enough not to lose. It was a boring fight, but he should\u0027ve done enough to be up on the scorecards.","contentType":"notes","url":"https://www.lqdev.me/notes/stevenson-vs-harutyunyan","date":"2024-07-06 23:37 -05:00","tags":["boxing","stevensonharutyunyan","toprank","stevenson","harutyunyan"],"summary":"Scorecard (Unofficial) Rd 1 2 3 4 5 6 7 8 9 10 11 12 Stevenson 9 9 9 10 10 10 10 10 9 9 10 10 Harutyunyan 10 10 10 9 9 9 9 9 10 10 9 9 Shakur did enough not to lose. It was a boring fight, but he...","keywords":["enough","scorecard","unofficial","stevenson","harutyunyan","shakur","lose","boring","fight","done"]},{"id":"star-home-cooked-software-barefoot-developers-appleton","title":"Home-Cooked Software and Barefoot Developers","content":"Home-Cooked Software and Barefoot Developers \u2192 https://maggieappleton.com/home-cooked-software The emerging golden age of home-cooked software, barefoot developers, and why the local-first community should help build it This is a talk I presented Local-first Conference in Berlin, May 2024. It\u0027s specifically directed at the local-first community, but its relevant to anyone involved in building software. For the last ~year I\u0027ve been keeping a close eye on how language models capabilities meaningfully change the speed, ease, and accessibility of software development. The slightly bold theory I put forward in this talk is that we\u0027re on a verge of a golden age of local, home-cooked software and a new kind of developer \u2013 what I\u0027ve called the barefoot developer.","contentType":"star","url":"https://www.lqdev.me/responses/home-cooked-software-barefoot-developers-appleton","date":"2024-07-06 22:02 -05:00","tags":["localfirst","smallweb","sofware","llm","programming","talk","indieweb"],"summary":"Home-Cooked Software and Barefoot Developers \u2192 https://maggieappleton.com/home-cooked-software The emerging golden age of home-cooked software, barefoot developers, and why the local-first community...","keywords":["software","cooked","local","home","barefoot","developers","developer","golden","community","talk"]},{"id":"notes-added-fediverse-og-creator-tag","title":"Just added Fediverse Open Graph meta tag to my website","content":"I learned from Matthias Ott that Mastodon created a new Open Graph meta tag which displays a direct link to the website owner\u0027s Fediverse (Mastodon, Pixelfed, Threads, etc...) profile as part of the URL preview card on the Mastodon web and mobile apps. I just added support for it on my website. All I had to do was add the following tag to my site. \u0026lt;meta property=\u0026quot;fediverse:creator\u0026quot; content=\u0026quot;@lqdev@toot.lqdev.tech\u0026quot;\u0026gt; I\u0027m not running the nightly version of Mastodon on my instance, but if anyone on an instance where this is already supported like mastodon.social can verify and let me know it\u0027s working, it\u0027s much appreciated.","contentType":"notes","url":"https://www.lqdev.me/notes/added-fediverse-og-creator-tag","date":"2024-07-02 21:41 -05:00","tags":["mastodon","fediverse","blogging","indieweb","blogs","news","journalism","openweb","distributedweb","smallweb","personalweb","opengraph","html"],"summary":"I learned from Matthias Ott that Mastodon created a new Open Graph meta tag which displays a direct link to the website owner\u0027s Fediverse (Mastodon, Pixelfed, Threads, etc...) profile as part of the...","keywords":["mastodon","meta","instance","website","learned","site","property=\u0026quot","fediverse:creator\u0026quot","content=\u0026quot","@lqdev@toot"]},{"id":"notes-teofimo-claggett-fight","title":"Teofimo vs. Claggett - June 29 2024","content":"Ring Walk Didn\u0027t see Claggett\u0027s but Lopez ring walk was a little cringe. Scorecard (Unofficial) Round 1 2 3 4 5 6 7 8 9 10 11 12 Lopez 10 10 10 10 10 10 ? 10 9 10 10 10 Claggett 9 9 9 9 9 9 ? 9 10 9 9 9 Rd 1 Claggett put more pressure but Lopez got in the better shots and looked in control even boxing off his back foot. Rd 2 Claggett still putting in a lot of pressure but Teofimo still looks in control. Rd 3 Teofimo still in control. Claggett has some damage to the face but still keeps going forward. Rd 4 Some nasty shots from Teofimo but Claggett still keeps coming forward. Rd 5 Solid back and forth Rd 6 Claggett is solid. Making Teofimo work and he\u0027s got some wear on his face. Rd 7 PC battery died \uD83D\uDE03 Rd 8 Big uppercuts from Teofimo but Claggett keeps coming forward. Rd 9 Claggett keeps coming forward and he landed some good shots. Rd 10 Teofimo stole the round at the end. Not like he needed to. He got the better shots. Sad to see because Claggett is putting the pressure but not doing as much damage. Rd 11 Much of the same. Claggett reguses to go away. Sadly, he hasn\u0027t done enough that if / when it goes the distance, he\u0027ll lose. He needs a knockout and it looks unlikely. Rd 12 Teofimo still looked fresh in this one. Claggett still kept the pressure though Verdict I\u0027d be surprised if Lopez didn\u0027t win. Claggett showed a lot of heart and made Teofimo uncomfortable. Sadly it wasn\u0027t enough though.","contentType":"notes","url":"https://www.lqdev.me/notes/teofimo-claggett-fight","date":"2024-06-29 23:48 -05:00","tags":["boxing","teofimoclaggett","teofimo","clagget","toprank"],"summary":"Ring Walk Didn\u0027t see Claggett\u0027s but Lopez ring walk was a little cringe. Scorecard (Unofficial) Round 1 2 3 4 5 6 7 8 9 10 11 12 Lopez 10 10 10 10 10 10 ? 10 9 10 10 10 Claggett 9 9 9 9 9 9 ? 9 10 9...","keywords":["claggett","teofimo","shots","pressure","keeps","forward","lopez","coming","control","from"]},{"id":"notes-ramirez-benitez-fight","title":"Nasty KO - Ramirez vs. Benitez","content":"Ramirez dominated most of the fight but that knockout was insane. Watching it live, it wasn\u0027t easy to see but in the replay, Ramirez exploded from the botttom and nearly took Benitez\u0027 head off. He wasn\u0027t out cold but he got his bell rung and just didn\u0027t make the count. Still, I hope he\u0027s okay. He took a beating the rest of the fight.","contentType":"notes","url":"https://www.lqdev.me/notes/ramirez-benitez-fight","date":"2024-06-29 23:23 -05:00","tags":["boxing","teofimoclaggett","featherweight","ko","fighting","teofimo","claggett"],"summary":"Ramirez dominated most of the fight but that knockout was insane. Watching it live, it wasn\u0027t easy to see but in the replay, Ramirez exploded from the botttom and nearly took Benitez\u0027 head off. He...","keywords":["ramirez","fight","wasn","took","okay","hope","count","didn","rung","bell"]},{"id":"reshare-meta-llm-compiler","title":"Meta Large Language Model Compiler: Foundation Models of Compiler Optimization","content":"Meta Large Language Model Compiler: Foundation Models of Compiler Optimization \u2192 https://huggingface.co/collections/facebook/llm-compiler-667c5b05557fe99a9edd25cb Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners. HuggingFace Collection Paper","contentType":"reshare","url":"https://www.lqdev.me/responses/meta-llm-compiler","date":"2024-06-27 22:39 -05:00","tags":["ai","compiler","meta","llm"],"summary":"Meta Large Language Model Compiler: Foundation Models of Compiler Optimization \u2192 https://huggingface.co/collections/facebook/llm-compiler-667c5b05557fe99a9edd25cb Large Language Models (LLMs) have...","keywords":["compiler","optimization","code","language","model","assembly","large","foundation","models","billion"]},{"id":"reply-deep-questions-debunking-genai-rss-blogroll","title":"Deep Questions - Debunking AI Model Capabilities / Distributed Webs of Trust","content":"Deep Questions - Debunking AI Model Capabilities / Distributed Webs of Trust \u2192 https://www.youtube.com/watch?v=OvlfCW3Ec1g Nice job by Cal debunking misconceptions about AI model capabilities. The segment highlights a few points I cover in my unpublished NoLM - Not Only Language Models blog post. Specifically the fact that Language Models on their own can\u0027t do much and need to be connected to data sources and other systems. Complex AI systems will be built with more specialized roles and leverage various components for their planning and execution. In the end though, models will require integration into existing systems. Those integrations need to be done by people, meaning humans are still in control of the AI-assisted system capabilities. Later in the podcast, Cal takes a question about distributed webs of trust. I agree with Cal\u0027s point of using existing open standards like RSS for content consumption. It\u0027s the reason you often hear the phrase, \u0026quot;or wherever you get your podcasts\u0026quot;. Assuming you have a program that can read an RSS feed, you can follow all types of content. On the topic of discovery, Cal makes the suggestion of using distributed webs of trust. Using domain names and linking as ways of discovering content. While blogrolls were not directly called out, it\u0027s one of the benefits a curated set of links provides.","contentType":"reply","url":"https://www.lqdev.me/responses/deep-questions-debunking-genai-rss-blogroll","date":"2024-06-24 20:40 -05:00","tags":["ai","deepquestions","rss","calnewport","podcast","blogroll","opml","social","socialmedia","distributedweb"],"summary":"Deep Questions - Debunking AI Model Capabilities / Distributed Webs of Trust \u2192 https://www.youtube.com/watch?v=OvlfCW3Ec1g Nice job by Cal debunking misconceptions about AI model capabilities. The...","keywords":["using","models","content","trust","webs","systems","capabilities","distributed","need","language"]},{"id":"star-ntietz-blogging-affirmations","title":"Affirmations for bloggers","content":"Affirmations for bloggers \u2192 https://ntietz.com/blog/blogging-affirmations/ The affirmations Here are the things I\u0027ve seen and learned. Each of these will be expanded in its own section. You have things to write about. Your perspective matters. You are good enough. Posts don\u0027t have to be novel. People will read it. Mistakes are okay! It\u0027s okay to ask for things. You can get started quickly. You can write on a schedule.","contentType":"star","url":"https://www.lqdev.me/responses/ntietz-blogging-affirmations","date":"2024-06-11 21:42 -05:00","tags":["blogging","writing","personalweb","socialweb","indieweb"],"summary":"Affirmations for bloggers \u2192 https://ntietz.com/blog/blogging-affirmations/ The affirmations Here are the things I\u0027ve seen and learned. Each of these will be expanded in its own section. You have...","keywords":["things","affirmations","okay","write","started","mistakes","read","people","novel","posts"]},{"id":"reply-npr-cassette-tapes-making-comeback","title":"The cassette tape is making a comeback","content":"The cassette tape is making a comeback \u2192 https://www.npr.org/2024/06/07/nx-s1-4976071/the-cassette-tape-is-making-a-comeback-thanks-to-a-family-run-company-in-missouri Despite the odds, cassette tapes are making a comeback. And one family-owned company in Springfield, Missouri is a leader in the revival. Hopefully this also means that players are also making a comeback because you need something to play them on. Having visited a museum recently full of old radio recordings on cassette tapes, I was sad to find out I couldn\u0027t listen to them because the tape player was broken. It\u0027s hard enough finding a decent MP3 player, I\u0027m sure it must be just as hard if not harder finding a tape player.","contentType":"reply","url":"https://www.lqdev.me/responses/npr-cassette-tapes-making-comeback","date":"2024-06-11 21:31 -05:00","tags":["cassette","tapes","retro","music","comeback"],"summary":"The cassette tape is making a comeback \u2192 https://www.npr.org/2024/06/07/nx-s1-4976071/the-cassette-tape-is-making-a-comeback-thanks-to-a-family-run-company-in-missouri Despite the odds, cassette...","keywords":["cassette","tape","making","comeback","player","finding","hard","family","company","missouri"]},{"id":"reshare-deep-dive-ownership-mojo","title":"Deep Dive into Ownership in Mojo","content":"Deep Dive into Ownership in Mojo \u2192 https://www.modular.com/blog/deep-dive-into-ownership-in-mojo In the second part of the ownership series in Mojo, we built on the mental model developed in the first part and provided practical examples to illustrate how ownership works in Mojo. We covered the different kinds of values (BValue, LValue, and RValue) and how they propagate through expressions. We also explained the function argument conventions (borrowed, inout, owned) and demonstrated how these conventions help manage memory safely and efficiently. We concluded with three fundamental rules: Rule 1 : Owned arguments take RValue on the caller side but are LValue on the callee side. Rule 2 : Owned arguments own the type if the transfer operator ^ is used; otherwise, they copy the type if it is Copyable. Rule 3 : Copy operations are optimized to move operations if the type is Copyable and Movable and isn\u2019t used anymore, reducing unnecessary overhead. Lastly, we emphasized that the main goals of ownership in Mojo are: Memory Safety : Enforcing exclusive ownership and proper lifetimes to prevent memory errors such as use-after-free and double-free. Performance Optimization : Converting unnecessary copy operations into move operations to reduce overhead and enhance performance. Ease of Use : Automating memory management through ownership rules and the transfer operator, simplifying development. Compile-Time Guarantees : Providing strong compile-time guarantees through type-checking and dataflow lifetime analysis, catching errors early in the development process.","contentType":"reshare","url":"https://www.lqdev.me/responses/deep-dive-ownership-mojo","date":"2024-06-11 21:20 -05:00","tags":["mojo","python","c","cpp","c\u002B\u002B","pl","programming","programminglanguage"],"summary":"Deep Dive into Ownership in Mojo \u2192 https://www.modular.com/blog/deep-dive-into-ownership-in-mojo In the second part of the ownership series in Mojo, we built on the mental model developed in the...","keywords":["ownership","mojo","memory","operations","type","owned","copy","rule","conventions","free"]},{"id":"star-eff-the-encryptids-bigfoot","title":"EFF Presents - The Encryptids - Bigfoot","content":"EFF Presents - The Encryptids - Bigfoot \u2192 https://www.eff.org/deeplinks/2024/05/bigfoot I\u2019m proud to share the first post in a series from our friends, The Encryptids \u2014the rarely-seen enigmas who inspire campfire lore. But this time, they\u2019re spilling secrets about how they survive this ever-digital world. We begin by checking in with the legendary Bigfoot de la Sasquatch... People say I\u0027m the most famous of The Encryptids, but sometimes I don\u0027t want the spotlight. They all want a piece of me: exes, ad trackers, scammers, even the government. A picture may be worth a thousand words, but my digital profile is worth cash (to skeezy data brokers). I can\u2019t hit a city block without being captured by doorbell cameras, CCTV, license plate readers, and a maze of street-level surveillance. It can make you want to give up on privacy altogether. Honey, no. Why should you have to hole up in some dank, busted forest for freedom and respect? You don\u2019t. Privacy isn\u0027t about hiding. It\u0027s about revealing what you want to who you want on your terms. It\u0027s your basic right to dignity.","contentType":"star","url":"https://www.lqdev.me/responses/eff-the-encryptids-bigfoot","date":"2024-06-11 21:12 -05:00","tags":["eff","privacy","digitalrights","campaign"],"summary":"EFF Presents - The Encryptids - Bigfoot \u2192 https://www.eff.org/deeplinks/2024/05/bigfoot I\u2019m proud to share the first post in a series from our friends, The Encryptids \u2014the rarely-seen enigmas who...","keywords":["want","encryptids","worth","digital","privacy","they","bigfoot","skeezy","data","brokers"]},{"id":"reshare-mapping-mind-large-language-model-anthropic","title":"Mapping the Mind of a Large Language Model","content":"Mapping the Mind of a Large Language Model \u2192 https://www.anthropic.com/research/mapping-mind-language-model Today we report a significant advance in understanding the inner workings of AI models. We have identified how millions of concepts are represented inside Claude Sonnet, one of our deployed large language models. This is the first ever detailed look inside a modern, production-grade large language model. This interpretability discovery could, in future, help us make AI models safer. Previously, we made some progress matching patterns of neuron activations, called features, to human-interpretable concepts. We used a technique called \u0026quot;dictionary learning\u0026quot;, borrowed from classical machine learning, which isolates patterns of neuron activations that recur across many different contexts. In turn, any internal state of the model can be represented in terms of a few active features instead of many active neurons. Just as every English word in a dictionary is made by combining letters, and every sentence is made by combining words, every feature in an AI model is made by combining neurons, and every internal state is made by combining features. In October 2023, we reported success applying dictionary learning to a very small \u0026quot;toy\u0026quot; language model and found coherent features corresponding to concepts like uppercase text, DNA sequences, surnames in citations, nouns in mathematics, or function arguments in Python code. We used the same scaling law philosophy that predicts the performance of larger models from smaller ones to tune our methods at an affordable scale before launching on Sonnet. We successfully extracted millions of features from the middle layer of Claude 3.0 Sonnet, (a member of our current, state-of-the-art model family, currently available on claude.ai), providing a rough conceptual map of its internal states halfway through its computation. This is the first ever detailed look inside a modern, production-grade large language model.","contentType":"reshare","url":"https://www.lqdev.me/responses/mapping-mind-large-language-model-anthropic","date":"2024-06-11 21:03 -05:00","tags":["ai","interpretability","anthropic","llm"],"summary":"Mapping the Mind of a Large Language Model \u2192 https://www.anthropic.com/research/mapping-mind-language-model Today we report a significant advance in understanding the inner workings of AI models. We...","keywords":["model","language","made","features","large","combining","models","state","sonnet","claude"]},{"id":"reshare-claudes-character-anthropic","title":"Claude\u0027s Character","content":"Claude\u0026#39;s Character \u2192 https://www.anthropic.com/research/claude-character Companies developing AI models generally train them to avoid saying harmful things and to avoid assisting with harmful tasks. The goal of this is to train models to behave in ways that are \u0026quot;harmless\u0026quot;. But when we think of the character of those we find genuinely admirable, we don\u2019t just think of harm avoidance. We think about those who are curious about the world, who strive to tell the truth without being unkind, and who are able to see many sides of an issue without becoming overconfident or overly cautious in their views. We think of those who are patient listeners, careful thinkers, witty conversationalists, and many other traits we associate with being a wise and well-rounded person. AI models are not, of course, people. But as they become more capable, we believe we can\u2014and should\u2014try to train them to behave well in this much richer sense. Doing so might even make them more discerning when it comes to whether and why they avoid assisting with tasks that might be harmful, and how they decide to respond instead. Claude 3 was the first model where we added \u0026quot;character training\u0026quot; to our alignment finetuning process: the part of training that occurs after initial model training, and the part that turns it from a predictive text model into an AI assistant. The goal of character training is to make Claude begin to have more nuanced, richer traits like curiosity, open-mindedness, and thoughtfulness. Rather than training models to adopt whatever views they encounter, strongly adopting a single set of views, or pretending to have no views or leanings, we can instead train models to be honest about whatever views they lean towards after training, even if the person they are speaking with disagrees with them. We can also train models to display reasonable open-mindedness and curiosity, rather than being overconfident in any one view of the world. In order to steer Claude\u2019s character and personality, we made a list of many character traits we wanted to encourage the model to have...We don\u2019t want Claude to treat its traits like rules from which it never deviates. We just want to nudge the model\u2019s general behavior to exemplify more of those traits. Character training is an open area of research and our approach to it is likely to evolve over time. It raises complex questions like whether AI models should have unique and coherent characters or should be more customizable, as well as what responsibilities we have when deciding which traits AI models should and shouldn\u2019t have.","contentType":"reshare","url":"https://www.lqdev.me/responses/claudes-character-anthropic","date":"2024-06-11 20:56 -05:00","tags":["anthropic","claude","alignment","ai"],"summary":"Claude\u0026#39;s Character \u2192 https://www.anthropic.com/research/claude-character Companies developing AI models generally train them to avoid saying harmful things and to avoid assisting with harmful...","keywords":["character","models","training","traits","they","train","views","more","model","those"]},{"id":"reshare-ultravox-multimodal-llm","title":"Ultravox - An open, fast, and extensible multimodal LLM","content":"Ultravox - An open, fast, and extensible multimodal LLM \u2192 https://github.com/fixie-ai/ultravox Ultravox is a new kind of multimodal LLM that can understand text as well as human speech, without the need for a separate Audio Speech Recognition (ASR) stage. Building on research like AudioLM, SeamlessM4T, Gazelle, SpeechGPT, and others, we\u0027ve extended Meta\u0027s Llama 3 model with a multimodal projector that converts audio directly into the high-dimensional space used by Llama 3. This direct coupling allows Ultravox to respond much more quickly than systems that combine separate ASR and LLM components. In the future this will also allow Ultravox to natively understand the paralinguistic cues of timing and emotion that are omnipresent in human speech. The current version of Ultravox (v0.1), when invoked with audio content, has a time-to-first-token (TTFT) of approximately 200ms, and a tokens-per-second rate of ~100, all using a Llama 3 8B backbone. While quite fast, we believe there is considerable room for improvement in these numbers. We look forward to working with LLM hosting providers to deliver state-of-the-art performance for Ultravox. Ultravox currently takes in audio and emits streaming text. As we evolve the model, we\u0027ll train it to be able to emit a stream of speech tokens that can then be converted directly into raw audio by an appropriate unit vocoder. We\u0027re interested in working with interested parties to build this functionality!","contentType":"reshare","url":"https://www.lqdev.me/responses/ultravox-multimodal-llm","date":"2024-06-11 20:51 -05:00","tags":["ai","multimodal","llm"],"summary":"Ultravox - An open, fast, and extensible multimodal LLM \u2192 https://github.com/fixie-ai/ultravox Ultravox is a new kind of multimodal LLM that can understand text as well as human speech, without the...","keywords":["ultravox","audio","speech","multimodal","llama","working","tokens","separate","fast","human"]},{"id":"reshare-apple-private-cloud-compute","title":"Apple - Private Cloud Compute","content":"Apple - Private Cloud Compute \u2192 https://security.apple.com/blog/private-cloud-compute/ We set out to build Private Cloud Compute with a set of core requirements: Stateless computation on personal user data. ...we want a strong form of stateless data processing where personal data leaves no trace in the PCC system. Enforceable guarantee. Security and privacy guarantees are strongest when they are entirely technically enforceable, which means it must be possible to constrain and analyze all the components that critically contribute to the guarantees of the overall Private Cloud Compute system. No privileged runtime access. Private Cloud Compute must not contain privileged interfaces that would enable Apple\u2019s site reliability staff to bypass PCC privacy guarantees, even when working to resolve an outage or other severe incident. Non-targetability. An attacker should not be able to attempt to compromise personal data that belongs to specific, targeted Private Cloud Compute users without attempting a broad compromise of the entire PCC system. Verifiable transparency. Security researchers need to be able to verify, with a high degree of confidence, that our privacy and security guarantees for Private Cloud Compute match our public promises.","contentType":"reshare","url":"https://www.lqdev.me/responses/apple-private-cloud-compute","date":"2024-06-11 20:42 -05:00","tags":["ai","pcc","privacy","cloud","privatecloudcompute","wwdc"],"summary":"Apple - Private Cloud Compute \u2192 https://security.apple.com/blog/private-cloud-compute/ We set out to build Private Cloud Compute with a set of core requirements: Stateless computation on personal...","keywords":["cloud","private","compute","guarantees","data","privacy","security","system","personal","apple"]},{"id":"reshare-introducing-apple-on-device-server-foundational-models","title":"Introducing Apple\u2019s On-Device and Server Foundation Models","content":"Introducing Apple\u2019s On-Device and Server Foundation Models \u2192 https://machinelearning.apple.com/research/introducing-apple-foundation-models Apple Intelligence is comprised of multiple highly-capable generative models that are specialized for our users\u2019 everyday tasks, and can adapt on the fly for their current activity. The foundation models built into Apple Intelligence have been fine-tuned for user experiences such as writing and refining text, prioritizing and summarizing notifications, creating playful images for conversations with family and friends, and taking in-app actions to simplify interactions across apps. In the following overview, we will detail how two of these models \u2014 a ~3 billion parameter on-device language model, and a larger server-based language model available with Private Cloud Compute and running on Apple silicon servers \u2014 have been built and adapted to perform specialized tasks efficiently, accurately, and responsibly. These two foundation models are part of a larger family of generative models created by Apple to support users and developers; this includes a coding model to build intelligence into Xcode, as well as a diffusion model to help users express themselves visually, for example, in the Messages app. Pre-training Our foundation models are trained on Apple\u0027s AXLearn framework, an open-source project we released in 2023. It builds on top of JAX and XLA, and allows us to train the models with high efficiency and scalability on various training hardware and cloud platforms, including TPUs and both cloud and on-premise GPUs. We used a combination of data parallelism, tensor parallelism, sequence parallelism, and Fully Sharded Data Parallel (FSDP) to scale training along multiple dimensions such as data, model, and sequence length. Post-training We find that data quality is essential to model success, so we utilize a hybrid data strategy in our training pipeline, incorporating both human-annotated and synthetic data, and conduct thorough data curation and filtering procedures. We have developed two novel algorithms in post-training: (1) a rejection sampling fine-tuning algorithm with teacher committee, and (2) a reinforcement learning from human feedback (RLHF) algorithm with mirror descent policy optimization and a leave-one-out advantage estimator. We find that these two algorithms lead to significant improvement in the model\u2019s instruction-following quality. Optimization Both the on-device and server models use grouped-query-attention. We use shared input and output vocab embedding tables to reduce memory requirements and inference cost. These shared embedding tensors are mapped without duplications. The on-device model uses a vocab size of 49K, while the server model uses a vocab size of 100K, which includes additional language and technical tokens. For on-device inference, we use low-bit palletization, a critical optimization technique that achieves the necessary memory, power, and performance requirements. To maintain model quality, we developed a new framework using LoRA adapters that incorporates a mixed 2-bit and 4-bit configuration strategy \u2014 averaging 3.5 bits-per-weight \u2014 to achieve the same accuracy as the uncompressed models. Additionally, we use an interactive model latency and power analysis tool, Talaria, to better guide the bit rate selection for each operation. We also utilize activation quantization and embedding quantization, and have developed an approach to enable efficient Key-Value (KV) cache update on our neural engines. Model Adaptation Our foundation models are fine-tuned for users\u2019 everyday activities, and can dynamically specialize themselves on-the-fly for the task at hand... We represent the values of the adapter parameters using 16 bits, and for the ~3 billion parameter on-device model, the parameters for a rank 16 adapter typically require 10s of megabytes. The adapter models can be dynamically loaded, temporarily cached in memory, and swapped \u2014 giving our foundation model the ability to specialize itself on the fly for the task at hand while efficiently managing memory and guaranteeing the operating system\u0027s responsiveness. Performance and Evaluation We compare our models with both open-source models (Phi-3, Gemma, Mistral, DBRX) and commercial models of comparable size (GPT-3.5-Turbo, GPT-4-Turbo)1. We find that our models are preferred by human graders over most comparable competitor models. On this benchmark, our on-device model, with ~3B parameters, outperforms larger models including Phi-3-mini, Mistral-7B, and Gemma-7B. Our server model compares favorably to DBRX-Instruct, Mixtral-8x22B, and GPT-3.5-Turbo while being highly efficient. To further evaluate our models, we use the Instruction-Following Eval (IFEval) benchmark to compare their instruction-following capabilities with models of comparable size. The results suggest that both our on-device and server model follow detailed instructions better than the open-source and commercial models of comparable size.","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-apple-on-device-server-foundational-models","date":"2024-06-11 20:26 -05:00","tags":["ai","apple","slm","edge","wwdc"],"summary":"Introducing Apple\u2019s On-Device and Server Foundation Models \u2192 https://machinelearning.apple.com/research/introducing-apple-foundation-models Apple Intelligence is comprised of multiple highly-capable...","keywords":["models","model","device","data","foundation","apple","server","size","both","training"]},{"id":"reshare-introducing-apple-intelligence","title":"Introducing Apple Intelligence","content":"Introducing Apple Intelligence \u2192 https://www.apple.com/newsroom/2024/06/introducing-apple-intelligence-for-iphone-ipad-and-mac/ Apple today introduced Apple Intelligence, the personal intelligence system for iPhone, iPad, and Mac that combines the power of generative models with personal context to deliver intelligence that\u2019s incredibly useful and relevant. Apple Intelligence unlocks new ways for users to enhance their writing and communicate more effectively. With brand-new systemwide Writing Tools built into iOS 18, iPadOS 18, and macOS Sequoia, users can rewrite, proofread, and summarize text nearly everywhere they write... Apple Intelligence powers exciting image creation capabilities to help users communicate and express themselves in new ways. With Image Playground, users can create fun images in seconds, choosing from three styles: Animation, Illustration, or Sketch...All images are created on device, giving users the freedom to experiment with as many images as they want. Powered by Apple Intelligence, Siri becomes more deeply integrated into the system experience. With richer language-understanding capabilities, Siri is more natural, more contextually relevant, and more personal, with the ability to simplify and accelerate everyday tasks. A cornerstone of Apple Intelligence is on-device processing, and many of the models that power it run entirely on device. To run more complex requests that require more processing power, Private Cloud Compute extends the privacy and security of Apple devices into the cloud to unlock even more intelligence. Apple is integrating ChatGPT access into experiences within iOS 18, iPadOS 18, and macOS Sequoia, allowing users to access its expertise \u2014 as well as its image- and document-understanding capabilities \u2014 without needing to jump between tools.","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-apple-intelligence","date":"2024-06-11 20:16 -05:00","tags":["ai","apple","appleintelligence"],"summary":"Introducing Apple Intelligence \u2192 https://www.apple.com/newsroom/2024/06/introducing-apple-intelligence-for-iphone-ipad-and-mac/ Apple today introduced Apple Intelligence, the personal intelligence...","keywords":["apple","intelligence","more","users","images","capabilities","image","power","personal","device"]},{"id":"reshare-verge-wwdc-2024-18-minute-recap-video","title":"The Verge - Apple WWDC 2024 keynote in 18 minutes","content":"The Verge - Apple WWDC 2024 keynote in 18 minutes \u2192 https://www.youtube.com/watch?v=sBXdyUA6A88 I always look forward to watching these recap videos from the Verge.","contentType":"reshare","url":"https://www.lqdev.me/responses/verge-wwdc-2024-18-minute-recap-video","date":"2024-06-11 20:05 -05:00","tags":["apple","wwdc","keynote","wwdc2024","ai","ios","ipad","iphone","watchos","ipados","macos","visionos"],"summary":"The Verge - Apple WWDC 2024 keynote in 18 minutes \u2192 https://www.youtube.com/watch?v=sBXdyUA6A88 I always look forward to watching these recap videos from the Verge.","keywords":["verge","recap","watching","forward","look","always","v=sbxdyua6a88","videos","com/watch","https://www"]},{"id":"reshare-karpathy-reproduce-gpt-2-124m-tutorial","title":"Andrej Karpathy - Let\u0027s reproduce GPT-2 (124M)","content":"Andrej Karpathy - Let\u0026#39;s reproduce GPT-2 (124M) \u2192 https://www.youtube.com/watch?v=l8pRSuU81PU","contentType":"reshare","url":"https://www.lqdev.me/responses/karpathy-reproduce-gpt-2-124m-tutorial","date":"2024-06-11 19:59 -05:00","tags":["ai","gpt","transformer","tutorial"],"summary":"Andrej Karpathy - Let\u0026#39;s reproduce GPT-2 (124M) \u2192 https://www.youtube.com/watch?v=l8pRSuU81PU","keywords":["andrej","karpathy","let\u0026#39","reproduce","124m","https://www","youtube","com/watch","v=l8prsuu81pu"]},{"id":"reshare-TinyAgent-function-calling-edge","title":"TinyAgent: Function Calling at the Edge","content":"TinyAgent: Function Calling at the Edge \u2192 https://bair.berkeley.edu/blog/2024/05/29/tiny-agent/ The ability of LLMs to execute commands through plain language (e.g. English) has enabled agentic systems that can complete a user query by orchestrating the right set of tools...recent multi-modal efforts such as the GPT-4o or Gemini-1.5 model, has expanded the realm of possibilities with AI agents. While this is quite exciting, the large model size and computational requirements of these models often requires their inference to be performed on the cloud. This can create several challenges for their widespread adoption. First and foremost, uploading data such as video, audio, or text documents to a third party vendor on the cloud, can result in privacy issues. Second, this requires cloud/Wi-Fi connectivity which is not always possible...latency could also be an issue as uploading large amounts of data to the cloud and waiting for the response could slow down response time, resulting in unacceptable time-to-solution. These challenges could be solved if we deploy the LLM models locally at the edge. ...current LLMs like GPT-4o or Gemini-1.5 are too large for local deployment. One contributing factor is that a lot of the model size ends up memorizing general information about the world into its parametric memory which may not be necessary for a specialized downstream application. ...this leads to an intriguing research question: Can a smaller language model with significantly less parametric memory emulate such emergent ability of these larger language models? Achieving this would significantly reduce the computational footprint of agentic systems and thus enable efficient and privacy-preserving edge deployment. Our study demonstrates that this is feasible for small language models through training with specialized, high-quality data that does not require recalling generic world knowledge. Such a system could particularly be useful for semantic systems where the AI agent\u2019s role is to understand the user query in natural language and, instead of responding with a ChatGPT-type question answer response, orchestrate the right set of tools and APIs to accomplish the user\u2019s command. For example, in a Siri-like application, a user may ask a language model to create a calendar invite with particular attendees. If a predefined script for creating calendar items already exists, the LLM simply needs to learn how to invoke this script with the correct input arguments (such as attendees\u2019 email addresses, event title, and time). This process does not require recalling/memorization of world knowledge from sources like Wikipedia, but rather requires reasoning and learning to call the right functions and to correctly orchestrate them. Our goal is to develop Small Language Models (SLM) that are capable of complex reasoning that could be deployed securely and privately at the edge. Here we will discuss the research directions that we are pursuing to that end. First, we discuss how we can enable small open-source models to perform accurate function calling, which is a key component of agentic systems. It turns out that off-the-shelf small models have very low function calling capabilities. We discuss how we address this by systematically curating high-quality data for function calling, using a specialized Mac assistant agent as our driving application. We then show that fine-tuning the model on this high quality curated dataset, can enable SLMs to even exceed GPT-4-Turbo\u2019s function calling performance. We then show that this could be further improved and made efficient through a new Tool RAG method. Finally, we show how the final models could be deployed efficiently at the edge with real time responses.","contentType":"reshare","url":"https://www.lqdev.me/responses/TinyAgent-function-calling-edge","date":"2024-06-11 19:43 -05:00","tags":["agent","ai","slm","function","edge","local","languagemodel","smalllanguagemodel","research"],"summary":"TinyAgent: Function Calling at the Edge \u2192 https://bair.berkeley.edu/blog/2024/05/29/tiny-agent/ The ability of LLMs to execute commands through plain language (e.g. English) has enabled agentic...","keywords":["models","language","model","function","calling","edge","systems","small","data","large"]},{"id":"reshare-tierra-whack-npr-tiny-desk-2024","title":"Tierra Whack - NPR Tiny Desk Concert","content":"Tierra Whack - NPR Tiny Desk Concert \u2192 https://www.youtube.com/watch?v=uRG6IWYVazM Great performance.","contentType":"reshare","url":"https://www.lqdev.me/responses/tierra-whack-npr-tiny-desk-2024","date":"2024-06-08 11:55 -05:00","tags":["music","tinydesk","tierrawhack","hiphop","rap","livemusic","performance","npr"],"summary":"Tierra Whack - NPR Tiny Desk Concert \u2192 https://www.youtube.com/watch?v=uRG6IWYVazM Great performance.","keywords":["tierra","whack","tiny","desk","concert","https://www","youtube","com/watch","v=urg6iwyvazm","performance"]},{"id":"notes-crate-finds-may-2024","title":"Crate Finds - May 2024","content":"Better late than never. Here\u0027s May\u0027s drop. Some of my favorites this month are: Journey of the Sorcerer by the Eagles Any of the songs by Piero Piccioni Any of the songs by Fishmans Tracks Yara by Charif Megarbane Album: Marzipan (Habibi Funk 023) [Bonus Edition] Duration: 2:25 Listen on YouTube Backup: Listen on Spotify J.R.D.A by Balla Et Ses Balladins Album: The Syliphone Years Duration: 3:32 Listen on YouTube Backup: Listen on Spotify SHOUT/OUT MENTAL ILLNESS by BrokinPaper Album: THE MASLOW EP Duration: 3:48 Listen on YouTube Backup: Listen on Spotify Red Planet by Gadgetor Album: Prey Duration: 3:50 Listen on YouTube Backup: Listen on Spotify toil by Space Banjo Album: Terraform Duration: 3:29 Listen on YouTube Backup: Listen on Spotify Journey of the Sorcerer - 2013 Remaster by Eagles Album: One of These Nights (2013 Remaster) Duration: 6:37 Listen on YouTube Backup: Listen on Spotify Outlaw Man - 2013 Remaster by Eagles Album: To the Limit: The Essential Collection Duration: 3:33 Listen on YouTube Backup: Listen on Spotify Midnight Flyer - 2013 Remaster by Eagles Album: To the Limit: The Essential Collection Duration: 3:58 Listen on YouTube Backup: Listen on Spotify All My Life by Fluorescent Smogg Album: Soul Spectrum Records, Vol. 1 Duration: 3:34 Listen on YouTube Backup: Listen on Spotify Il ricordo di Serena by Riz Ortolani Album: Confessione di un commissario di polizia al procuratore della Repubblica (Original Motion Picture Soundtrack) [Remastered] Duration: 2:41 Listen on YouTube Backup: Listen on Spotify Cowboys by Harrison Album: Birds, Bees, The Clouds \u0026amp; The Trees Duration: 1:44 Listen on YouTube Backup: Listen on Spotify Lune noire by Isolde Lasoen Album: Oh Dear Duration: 3:52 Listen on YouTube Backup: Listen on Spotify Space Garden by Auntie Leo \u0026amp; The Backstabbers Album: Space Garden Duration: 4:56 Listen on YouTube Backup: Listen on Spotify All News Is Good News by Surprise Chef Album: All News Is Good News Duration: 4:21 Listen on YouTube Backup: Listen on Spotify Easy Lovers by Piero Piccioni Album: Camille 2000 (Original Motion Picture Soundtrack) Duration: 3:45 Listen on YouTube Backup: Listen on Spotify Endless Love by Piero Piccioni Album: Colpo Rovente - Red Hot Shot (Original Motion Picture Soundtrack) Duration: 2:18 Listen on YouTube Backup: Listen on Spotify A Starless Night - Remastered by Piero Piccioni Album: Il Dio sotto la pelle - God Under the Skin (Original Motion Picture Soundtrack) Duration: 2:46 Listen on YouTube Backup: Listen on Spotify Sketches Of Brunswick East I by King Gizzard \u0026amp; The Lizard Wizard, Mild High Club Album: Sketches of Brunswick East Duration: 1:20 Listen on YouTube Backup: Listen on Spotify Towers by Via Mardot Album: Towers Duration: 2:10 Listen on YouTube Backup: Listen on Spotify Lost My Mind by Will Van Horn Album: Pedal Steel Guitar Duration: 3:49 Listen on YouTube Backup: Listen on Spotify \u30CA\u30A4\u30C8\u30AF\u30EB\u30FC\u30B8\u30F3\u30B0 by Fishmans Album: \u7A7A\u4E2D\u30AD\u30E3\u30F3\u30D7 Duration: 6:01 Listen on YouTube Backup: Listen on Spotify \u30D0\u30C3\u30AF\u30D3\u30FC\u30C8\u306B\u306E\u3063\u304B\u3063\u3066 by Fishmans Album: \u5B87\u5B99 \u65E5\u672C \u4E16\u7530\u8C37 Duration: 8:26 Listen on YouTube Backup: Listen on Spotify BABY BLUE by Fishmans Album: \u7A7A\u4E2D\u30AD\u30E3\u30F3\u30D7 Duration: 6:09 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-may-2024","date":"2024-06-06 22:37 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"Better late than never. Here\u0027s May\u0027s drop. Some of my favorites this month are: Journey of the Sorcerer by the Eagles Any of the songs by Piero Piccioni Any of the songs by Fishmans Tracks Yara by...","keywords":["listen","spotify","duration:","youtube","backup:","album:","original","news","motion","fishmans"]},{"id":"notes-post-statistics-june-2024","title":"Website Post Statistics - June 2024","content":"6 months in and these are the main takeaways. At 51 notes, I\u0027m 10 over last year\u0027s total notes. Responses are healthy. I\u0027m closing in on 200. I wasn\u0027t as active the month of May so I\u0027m still shy of last year\u0027s mark, but given it\u0027s still only halfway through the year, it seems like good progress so far. Most of my posts continue to be related to AI. While I can believe it, I also think I could be more specific and get more creative with my tags. Given the lead AI has though in terms of tag numbers, it\u0027ll be harder for those more fine-grained tags to come through. I won\u0027t use the word struggle, but I have paid less attention to long form posts. There\u0027s a few that I have planned for June, so that should help bump those numbers. Overall I\u0027m happy with the progress and use I\u0027m getting out of the website. Some things I\u0027d like to work on are simplifying my authoring process. Although github.dev and Codespaces can be convenient, it\u0027s a hassle. Authoring content locally reduces some of that friction. However, I\u0027m still bound to my laptop. As a Day One subscriber, I haven\u0027t maximized my subscription. This past week I\u0027ve started to use it more for bookmarking articles I\u0027m reading. Maybe that\u0027s a way that I can bookmark articles when on the go and when I\u0027m ready to reshare them on the website, I can just copy from there. Something else I can use Day One for are to start longer form blog or note drafts. Then, when I\u0027m ready to do more serious work and publish them, I can continue on my laptop.","contentType":"notes","url":"https://www.lqdev.me/notes/post-statistics-june-2024","date":"2024-06-06 22:26 -05:00","tags":["githubactions","cicd","blogging","stats","github"],"summary":"6 months in and these are the main takeaways. At 51 notes, I\u0027m 10 over last year\u0027s total notes. Responses are healthy. I\u0027m closing in on 200. I wasn\u0027t as active the month of May so I\u0027m still shy of...","keywords":["more","year","website","tags","those","laptop","articles","continue","posts","progress"]},{"id":"bookmarks-genai-handbook-learning-resource-brown","title":"Generative AI Handbook: A Roadmap for Learning Resources","content":"Generative AI Handbook: A Roadmap for Learning Resources \u2192 https://genai-handbook.github.io/ This document aims to serve as a handbook for learning the key concepts underlying modern artificial intelligence systems. Given the speed of recent development in AI, there really isn\u2019t a good textbook-style source for getting up-to-speed on the latest-and-greatest innovations in LLMs or other generative models, yet there is an abundance of great explainer resources (blog posts, videos, etc.) for these topics scattered across the internet. My goal is to organize the \u201Cbest\u201D of these resources into a textbook-style presentation, which can serve as a roadmap for filling in the prerequisites towards individual AI-related learning goals. My hope is that this will be a \u201Cliving document\u201D, to be updated as new innovations and paradigms inevitably emerge, and ideally also a document that can benefit from community input and contribution. This guide is aimed at those with a technical background of some kind, who are interested in diving into AI either out of curiosity or for a potential career. I\u2019ll assume that you have some experience with coding and high-school level math, but otherwise will provide pointers for filling in any other prerequisites.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/genai-handbook-learning-resource-brown","date":"2024-06-06 22:18 -05:00","tags":["generativeai","genai","handbook","learning","resource"],"summary":"Generative AI Handbook: A Roadmap for Learning Resources \u2192 https://genai-handbook.github.io/ This document aims to serve as a handbook for learning the key concepts underlying modern artificial...","keywords":["learning","resources","generative","filling","innovations","style","prerequisites","speed","serve","textbook"]},{"id":"reshare-omakub-ubuntu-developer-desktop","title":"Omakub - An Omakase Developer Setup for Ubuntu 24.04","content":"Omakub - An Omakase Developer Setup for Ubuntu 24.04 \u2192 https://omakub.org/ Cool project. Turn a fresh Ubuntu installation into a fully-configured, beautiful, and modern web development system by running a single command. Omakub is an opinionated take on what Linux can be at its best. Omakub includes a curated set of applications and tools that one might discover through hours of watching YouTube, reading blogs, or just stumbling around Linux internet. All so someone coming straight from a platform like Windows or the Mac can immediately start enjoying a ready-made system, without having to do any configuration and curation legwork at all.","contentType":"reshare","url":"https://www.lqdev.me/responses/omakub-ubuntu-developer-desktop","date":"2024-06-06 22:14 -05:00","tags":["linux","developer","ubuntu","desktop","37signals"],"summary":"Omakub - An Omakase Developer Setup for Ubuntu 24.04 \u2192 https://omakub.org/ Cool project. Turn a fresh Ubuntu installation into a fully-configured, beautiful, and modern web development system by...","keywords":["omakub","linux","ubuntu","system","hours","watching","youtube","reading","blogs","stumbling"]},{"id":"notes-2024-06-03-weekly-post-summary","title":"Week of June 03, 2024 - Post Summary","content":"Responses My Permadomain Micro.blog - Podcast hosting for $5 Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20 21 Years of WordPress","contentType":"notes","url":"https://www.lqdev.me/notes/2024-06-03-weekly-post-summary","date":"2024-06-02 20:22 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Responses My Permadomain Micro.blog - Podcast hosting for $5 Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20 21 Years of WordPress","keywords":["responses","permadomain","micro","blog","podcast","hosting","reproducing","124m","minutes","years"]},{"id":"reply-my-permadomain-rscottjones","title":"My Permadomain","content":"My Permadomain \u2192 https://rscottjones.com/my-permadomain/ I\u0027m guilty of owning way too many domains but this short-hand (lqdev.me) and other permadomain (luisquintanilla.me) are the ones I use mostly. A potential happy medium I\u0027ve seen is using subdomains (i.e. project.lqdev.me). Jim Nielsen has great examples of how he\u0027s doing that today. Though that\u0027s an older post, he also addressed it most recently in the post Domain Sins of My Youth . That somehow feels better than the URL you get when you use free hosting like GitHub Pages. As a bonus, it\u0027s already tied to your identity (if using your personal domain). Also, you save some money. There\u0027s a few ways I practice this today. In addition to the previously mentioned domains, I also own lqdev.tech. Services and projects I host live there. For example: Mastodon Instance (toot.lqdev.tech) Matrix server (matrix.lqdev.tech) Webmentions service (webmentions.lqdev.tech) So far it\u0027s been working well.","contentType":"reply","url":"https://www.lqdev.me/responses/my-permadomain-rscottjones","date":"2024-05-30 21:41 -05:00","tags":["domains","personalweb","permadomain","indieweb","projects","selfhosting","hosting","self-hosting"],"summary":"My Permadomain \u2192 https://rscottjones.com/my-permadomain/ I\u0027m guilty of owning way too many domains but this short-hand (lqdev.me) and other permadomain (luisquintanilla.me) are the ones I use mostly....","keywords":["lqdev","tech","also","today","post","using","domains","permadomain","domain","matrix"]},{"id":"star-microblog-podcast-hosting","title":"Micro.blog - Podcast hosting for $5","content":"Micro.blog - Podcast hosting for $5 \u2192 https://www.manton.org/2024/05/29/podcast-hosting-for.html This is cool! The Micro.blog premium plan has tons of great features as well. Six years ago, we launched our $10/month plan with podcast hosting. Since then we\u2019ve added several big features to the plan... Today, I want to bring the podcast feature to more people, so we\u2019re moving it down to the standard $5/month plan.","contentType":"star","url":"https://www.lqdev.me/responses/microblog-podcast-hosting","date":"2024-05-30 21:27 -05:00","tags":["microblog","podcast","indieweb","hosting","blog","blogging"],"summary":"Micro.blog - Podcast hosting for $5 \u2192 https://www.manton.org/2024/05/29/podcast-hosting-for.html This is cool! The Micro.blog premium plan has tons of great features as well. Six years ago, we...","keywords":["plan","podcast","hosting","micro","blog","features","several","today","want","bring"]},{"id":"reshare-repro-gpt-2-llm-c-90-min-20-dollars-karpathy","title":"Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20","content":"Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20 \u2192 https://github.com/karpathy/llm.c/discussions/481 ...the TLDR is that we\u0027re training a 12-layer GPT-2 (124M), from scratch, on 10B tokens of FineWeb, with max sequence length of 1024 tokens. The 124M model is the smallest model in the GPT-2 series released by OpenAI in 2019, and is actually quite accessible today, even for the GPU poor. With llm.c, which is quite efficient at up to ~60% model flops utilization, reproducing this model on one 8X A100 80GB SXM node takes ~90 minutes. For example, on Lambda this node goes for ~$14/hr, so the total cost of reproducing this model today is about $20. You can train the model with a single GPU too, it would just take proportionally longer (e.g. ~4-24 hours depending on the GPU).","contentType":"reshare","url":"https://www.lqdev.me/responses/repro-gpt-2-llm-c-90-min-20-dollars-karpathy","date":"2024-05-28 20:33 -05:00","tags":["ai","llm","gpt","gpt2","llmc","c","slm"],"summary":"Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20 \u2192 https://github.com/karpathy/llm.c/discussions/481 ...the TLDR is that we\u0027re training a 12-layer GPT-2 (124M), from scratch, on 10B tokens of...","keywords":["model","reproducing","124m","minutes","quite","tokens","today","node","which","efficient"]},{"id":"reshare-21-years-of-wordpress","title":"21 Years of WordPress","content":"21 Years of WordPress \u2192 https://ma.tt/2024/05/wp21/ 21 years since Mike and I did the first release of WordPress, forking Michel\u2019s work on b2/caf\u00E9log. I\u2019ve been thinking a lot about elements that made WordPress successful in its early years that we should keep in mind as we build this year and beyond. Here\u2019s 11 opinions: Simple things should be easy and intuitive, and complex things possible. ... Wikis are amazing, and our documentation should be wiki-easy to edit. ... It\u2019s important that we all do support, go to meetups and events, anything we can to stay close to regular end-users of what we make. Congrats to WordPress and the team on 21 years. I think the following are some good elements to keep building on.","contentType":"reshare","url":"https://www.lqdev.me/responses/21-years-of-wordpress","date":"2024-05-27 21:05 -05:00","tags":["wordpress","blogging","web","anniversary","opensource","openweb"],"summary":"21 Years of WordPress \u2192 https://ma.tt/2024/05/wp21/ 21 years since Mike and I did the first release of WordPress, forking Michel\u2019s work on b2/caf\u00E9log. I\u2019ve been thinking a lot about elements that...","keywords":["years","wordpress","easy","keep","elements","things","release","wikis","amazing","documentation"]},{"id":"notes-2024-05-26-weekly-post-summary","title":"Week of May 26, 2024 - Post Summary","content":"Notes False alarm - Link is in X bio X (Twitter) - No more link in bio? Blogroll discovery enabled Responses RIP Kabosu The IndieWeb\u0027s Next Stage The Daylight DC1 is a $729 attempt to build a calmer computer","contentType":"notes","url":"https://www.lqdev.me/notes/2024-05-26-weekly-post-summary","date":"2024-05-26 18:34 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes False alarm - Link is in X bio X (Twitter) - No more link in bio? Blogroll discovery enabled Responses RIP Kabosu The IndieWeb\u0027s Next Stage The Daylight DC1 is a $729 attempt to build a calmer...","keywords":["link","notes","build","attempt","$729","daylight","stage","next","indieweb","kabosu"]},{"id":"notes-false-alarm-link-in-bio-x-twitter","title":"False alarm - Link is in X bio","content":"I may have spoken too early in my post about X not displaying bio links correctly . Not sure if it was a glitch or what was going on, but I now see my domain correctly displayed as the bio link. All good. I\u0027d be interested in knowing whether others have experienced something similar.","contentType":"notes","url":"https://www.lqdev.me/notes/false-alarm-link-in-bio-x-twitter","date":"2024-05-26 17:00 -05:00","tags":["x","twitter","website"],"summary":"I may have spoken too early in my post about X not displaying bio links correctly . Not sure if it was a glitch or what was going on, but I now see my domain correctly displayed as the bio link. All...","keywords":["correctly","spoken","others","whether","knowing","interested","good","link","displayed","domain"]},{"id":"reshare-daylight-dc-1-tablet-computer","title":"The Daylight DC1 is a $729 attempt to build a calmer computer","content":"The Daylight DC1 is a $729 attempt to build a calmer computer \u2192 https://www.theverge.com/2024/5/23/24163225/daylight-dc1-tablet-livepaper Like many in the comments, I like the idea of this device and what it\u0027s aiming to do. The \u0026quot;LivePaper\u0026quot; display technology seems interesting. At $729 though, it seems a little steep. I paid a fraction of that for my Onyx Boox Nova Air 2 and I have most of the functionality of this device. As an e-reader, note-taking device, and tablet that can install any Android app, it works perfectly fine for my use cases and I\u0027m very happy with it. Still, I\u0027m interested in seeing whether this takes off or whether it ends up being another Rabbit R1 or Humane Ai Pin.","contentType":"reshare","url":"https://www.lqdev.me/responses/daylight-dc-1-tablet-computer","date":"2024-05-26 16:48 -05:00","tags":["eink","digitalminimalism","daylight","tablet","technology","gadgets"],"summary":"The Daylight DC1 is a $729 attempt to build a calmer computer \u2192 https://www.theverge.com/2024/5/23/24163225/daylight-dc1-tablet-livepaper Like many in the comments, I like the idea of this device and...","keywords":["device","$729","whether","seems","tablet","daylight","boox","nova","functionality","reader"]},{"id":"notes-no-link-in-bio-x-twitter","title":"X (Twitter) - No more link in bio?","content":"While making some updates to my X profile, I noticed the link displayed on my bio isn\u0027t the same one I configured in the form. Instead it\u0027s a t.co shortened link. I get I\u0027m just a free user on the platform but that seems like an annoying change. I haven\u0027t found much details about the change, but I\u0027d hope paying subscribers get the \u0026quot;perk\u0026quot; of having control over how the link on their bio displays.","contentType":"notes","url":"https://www.lqdev.me/notes/no-link-in-bio-x-twitter","date":"2024-05-26 15:48 -05:00","tags":["x","twitter","website"],"summary":"While making some updates to my X profile, I noticed the link displayed on my bio isn\u0027t the same one I configured in the form. Instead it\u0027s a t.co shortened link. I get I\u0027m just a free user on the...","keywords":["link","change","while","having","perk\u0026quot","\u0026quot","subscribers","paying","hope","details"]},{"id":"notes-blogroll-discovery-implemented","title":"Blogroll discovery enabled","content":"Just enabled discovery for my blogroll based on the OPML blogroll spec . While technically not part of the spec, I also have similar collections for podcasts and YouTube . I\u0027ve added those as well. \u0026lt;head\u0026gt; \u0026lt;!--...--\u0026gt; \u0026lt;link rel=\u0026quot;blogroll\u0026quot; type=\u0026quot;text/xml\u0026quot; title=\u0026quot;Luis Quintanilla\u0027s Blogroll\u0026quot; href=\u0026quot;/collections/blogroll//index.opml\u0026quot;\u0026gt; \u0026lt;link rel=\u0026quot;youtuberoll\u0026quot; type=\u0026quot;text/xml\u0026quot; title=\u0026quot;Luis Quintanilla\u0027s YouTube Roll\u0026quot; href=\u0026quot;/collections/youtube//index.opml\u0026quot;\u0026gt; \u0026lt;link rel=\u0026quot;podroll\u0026quot; type=\u0026quot;text/xml\u0026quot; title=\u0026quot;Luis Quintanilla\u0027s Podroll\u0026quot; href=\u0026quot;/collections/podroll//index.opml\u0026quot;\u0026gt; \u0026lt;!--...--\u0026gt; \u0026lt;/head\u0026gt; youtuberoll doesn\u0027t really roll off the tongue, but I\u0027m hesitant to call it videoroll or vidroll. My concerns mainly come from the fact that YouTube != video. Ideas welcome!","contentType":"notes","url":"https://www.lqdev.me/notes/blogroll-discovery-implemented","date":"2024-05-25 22:44 -05:00","tags":["blogroll","opml","rss","indieweb","openweb","socialweb","personalweb"],"summary":"Just enabled discovery for my blogroll based on the OPML blogroll spec . While technically not part of the spec, I also have similar collections for podcasts and YouTube . I\u0027ve added those as well....","keywords":["href=\u0026quot","opml\u0026quot","quintanilla","luis","title=\u0026quot","text/xml\u0026quot","type=\u0026quot","rel=\u0026quot","link","youtube"]},{"id":"reply-indieweb-next-steps-durnell","title":"The IndieWeb\u0027s Next Stage","content":"The IndieWeb\u0026#39;s Next Stage \u2192 https://tracydurnell.com/2024/05/17/indieweb-next-stage/ \uD83D\uDE4B Great post. One of the challenges I\u0027ve found, even when using appliances like those from hosting providers like Linode or tools like YunoHost is connecting them to your domain. Usually, that\u0027s specific to your domain name provider and usually a manual process. In cases where that\u0027s easy, you\u0027re often overcharged for the convenience. Compared to \u0026quot;free\u0026quot; social media and publishing websites, it makes using your own domain a less desirable option. Maybe advocating for a multi-staged approach like IndieWebify.Me could make the journey more approachable. More importantly, making it a journey, rather than a destination could help with meeting folks where they are and guide them closer towards their goals. In any case, the proposed list of goals seem like a great start towards helping people create their own place on the web.","contentType":"reply","url":"https://www.lqdev.me/responses/indieweb-next-steps-durnell","date":"2024-05-25 21:54 -05:00","tags":["indieweb","community","personalweb"],"summary":"The IndieWeb\u0026#39;s Next Stage \u2192 https://tracydurnell.com/2024/05/17/indieweb-next-stage/ \uD83D\uDE4B Great post. One of the challenges I\u0027ve found, even when using appliances like those from hosting providers...","keywords":["domain","more","journey","towards","using","usually","goals","next","staged","approach"]},{"id":"reshare-rip-kabosu-doge","title":"RIP Kabosu","content":"RIP Kabosu \u2192 https://www.theverge.com/2024/5/24/24163865/doge-meme-shiba-inu-kabosu-dead-crypto The face of one of the defining memes of the 2010s, the doge meme, died on Friday. Kabosu, the shiba inu with the knowing face that launched a million internet jokes, was 18 when she died","contentType":"reshare","url":"https://www.lqdev.me/responses/rip-kabosu-doge","date":"2024-05-25 19:55 -05:00","tags":["shiba","doge","meme","dog"],"summary":"RIP Kabosu \u2192 https://www.theverge.com/2024/5/24/24163865/doge-meme-shiba-inu-kabosu-dead-crypto The face of one of the defining memes of the 2010s, the doge meme, died on Friday. Kabosu, the shiba...","keywords":["kabosu","meme","shiba","died","face","million","launched","knowing","friday","doge"]},{"id":"star-badu-watts-rebillet","title":"Erykah Badu \u002B Reggie Watts \u002B Marc Rebillet","content":"Erykah Badu \u002B Reggie Watts \u002B Marc Rebillet \u2192 https://www.youtube.com/watch?v=AtR1yVmCCvw I never knew I needed this in my life.","contentType":"star","url":"https://www.lqdev.me/responses/badu-watts-rebillet","date":"2024-05-18 23:50 -05:00","tags":["badu","watts","rebillet","music"],"summary":"Erykah Badu \u002B Reggie Watts \u002B Marc Rebillet \u2192 https://www.youtube.com/watch?v=AtR1yVmCCvw I never knew I needed this in my life.","keywords":["erykah","badu","reggie","watts","marc","rebillet","https://www","youtube","com/watch","v=atr1yvmccvw"]},{"id":"notes-fury-usyk-1","title":"Fury vs. Usyk","content":"What a great fight. I think the judges did a great job. It was a close fight, but the knockdown won it for Usyk. The ref did a good job not stopping it in the 9th and giving Fury a chance to come back. I think Fury clowned around too much in the beginning and didn\u0027t hit the gas in the middle rounds letting Usyk come back in the later rounds. The clowning probably was masking because Usyk is most likely the best fighter he\u0027s fought in his career. Great job by both fighters coming back, Usyk in those middle rounds and Fury from the knockdock where it looked like he was done. Also, it\u0027s business and most likely both fighters carried each other rather than press the action because this leaves the door open for the rematch. I doubt Usyk does the trilogy with Anthony Joshua given the series is 2-0. The next fight for both will probably be the rematch.","contentType":"notes","url":"https://www.lqdev.me/notes/fury-usyk-1","date":"2024-05-18 20:09 -05:00","tags":["boxing","fury","usyk","heavyweight","championship"],"summary":"What a great fight. I think the judges did a great job. It was a close fight, but the knockdown won it for Usyk. The ref did a good job not stopping it in the 9th and giving Fury a chance to come...","keywords":["usyk","fight","back","fury","rounds","both","likely","middle","come","probably"]},{"id":"notes-post-statistics-may-2024","title":"Website Post Statistics - May 2024","content":"Still not mid-year and I\u0027m making progress on publishing to the website. Some notable observations this month: At 43 notes, I\u0027m already past the total number of notes posted in 2023. Weekly summary posts like Create Finds and Weekly Summary continue to contribute towards this number. Despite a slow last week of April with no posts, I was still able to manage adding nearly 40 new posts. At 184, I\u0027m closing in and expect to surpass last year\u0027s response posts within the next two months. I\u0027m still working through how to go about publishing longer-form content to the blog. I\u0027m still only at 3 posts. Given that I posted 8 last year, it\u0027s not an elusive target. Still, I haven\u0027t figured out what some good topics / experiments I\u0027m doing today would be interesting to expand upon in the blog. I\u0027m in no rush though. I\u0027m enjoying the shorter form posts both notes and responses.","contentType":"notes","url":"https://www.lqdev.me/notes/post-statistics-may-2024","date":"2024-05-05 20:31 -05:00","tags":["githubactions","cicd","blogging","stats","github"],"summary":"Still not mid-year and I\u0027m making progress on publishing to the website. Some notable observations this month: At 43 notes, I\u0027m already past the total number of notes posted in 2023. Weekly summary...","keywords":["posts","last","notes","year","number","summary","weekly","form","blog","publishing"]},{"id":"notes-crate-finds-april-2024","title":"Crate Finds - April 2024","content":"April\u0027s drop. This month, I kept coming back to: Pon Pon by Khruangbin This Feeling I Get by Doug Shorts Tracks Talk About It by Kate Bollinger Album: Talk About It (B-Side) Duration: 2:12 Listen on YouTube Backup: Listen on Spotify Luz Errante by Hielo Ardiente Album: Antologia Musical 20 Exitos! Duration: 3:04 Listen on YouTube Backup: Listen on Spotify Crei by Los Apaches De Usulutan Album: Grandes Hits Duration: 2:58 Listen on YouTube Backup: Listen on Spotify Los Dos by Los Mustangs Album: Cuatro Super Grupos Salvadorenos Duration: 2:52 Listen on YouTube Backup: Listen on Spotify Vivir de Suenos by Cesar Donald Album: El Salvador y Su Musica, Vol. 2 Duration: 2:56 Listen on YouTube Backup: Listen on Spotify Tan Lejos de Ti by Los Supersonicos Album: Epoca de Oro de el Salvador, Vol. 3 Duration: 2:58 Listen on YouTube Backup: Listen on Spotify Camino De Hormigas by The Lovers Album: Cuatro Super Grupos Salvadorenos Duration: 2:53 Listen on YouTube Backup: Listen on Spotify Bluebird by Alan Hawkshaw Album: Kpm 1000 Series: Happy Rainbows Duration: 2:20 Listen on YouTube Backup: Listen on Spotify Oye No Me Beses by Charanga Sensaci\u00F3n de Rolando Vald\u00E9s Album: Charanga Super Charanga Vol. 2 Duration: 3:15 Listen on YouTube Backup: Listen on Spotify Pon P\u00F3n by Khruangbin Album: A LA SALA Duration: 2:58 Listen on YouTube Backup: Listen on Spotify Dream State by Kamasi Washington, Andr\u00E9 3000 Album: Dream State Duration: 8:39 Listen on YouTube Backup: Listen on Spotify My Sweet Baby by The Mark-Keys Album: Dave Hamilton\u0027s Detroit Dancers, Vol. 2 Duration: 2:37 Listen on YouTube Backup: Listen on Spotify The Meaning of Love (feat. Gaspard Sicx) by Perfect Angels, Gaspard Sicx Album: Exit from the Ultra-World Duration: 3:08 Listen on YouTube Backup: Listen on Spotify The Girl Who Was Frightened of Ashtrays by Charlie Megira Album: The Abtomatic Miesterzinger Mambo Chic Duration: 3:50 Listen on YouTube Backup: Listen on Spotify This Feeling I Get by Doug Shorts Album: Casual Encounter Duration: 3:46 Listen on YouTube Backup: Listen on Spotify Incontro la contessa by Bruno Nicolai, Edda Dell\u0027Orso Album: Love Birds - Una strana voglia d\u0027amare (Original Motion Picture Soundtrack) Duration: 2:07 Listen on YouTube Backup: Listen on Spotify Desert Scene by Uncle Chris Album: Uncle Chris Duration: 7:20 Listen on YouTube Backup: Listen on Spotify Ghost Walk by The Budos Band Album: The Budos Band Duration: 2:10 Listen on YouTube Backup: Listen on Spotify Song With No Words by \u0421\u041E\u042E\u0417 Album: Force of the Wind Duration: 1:45 Listen on YouTube Backup: Listen on Spotify Sonho no C\u00E9u by Waltel Branco Album: Meu Balan\u00E7o Duration: 1:55 Listen on YouTube Backup: Listen on Spotify FRENCH BOSSA NOVA by Ladji Mouflet, aupinard, gracegrace Album: FRENCH BOSSA NOVA Duration: 3:50 Listen on YouTube Backup: Listen on Spotify Jammin by RUBII Album: Olive Green Duration: 2:49 Listen on YouTube Backup: Listen on Spotify Tsukikage No Nagisa by Miki Hirayama Album: Tokyo Riddim 1976-1985 Duration: 4:31 Listen on YouTube Backup: Listen on Spotify NO SIENTO MI SANGRE by Evlay, TAICHU Album: 333 Duration: 4:02 Listen on YouTube Backup: Listen on Spotify Ghost Detain by !tems Album: Time Swap EP Duration: 5:37 Listen on YouTube Backup: Listen on Spotify Dynamo by Troker Album: Imperfecto Duration: 4:17 Listen on YouTube Backup: Listen on Spotify El Efecto by Las Bodas Quimicas Album: Ley de Gravedad Duration: 3:50 Listen on YouTube Backup: Listen on Spotify Perd\u00F3name by Jay de la Cueva Album: Perd\u00F3name Duration: 3:36 Listen on YouTube Backup: Listen on Spotify En voiture by Forever Pavot Album: Simple Comme Sylvain (Bande originale du film de Monia Chokri) Duration: 1:15 Listen on YouTube Backup: Listen on Spotify Cantar das Kandakinhas by Pedro Ricardo Album: Soprem Bons Ventos Duration: 2:05 Listen on YouTube Backup: Listen on Spotify Sunset by Resavoir Album: Resavoir Duration: 2:57 Listen on YouTube Backup: Listen on Spotify Gostar do Mundo - Alternate by Sessa Album: Estrela Acesa: Demos \u0026amp; Alternates Duration: 2:30 Listen on YouTube Backup: Listen on Spotify Dlareme by Don Glori Album: Welcome Duration: 6:07 Listen on YouTube Backup: Listen on Spotify It\u0027s Alright by Chuck Senrick Album: Dreamin\u0027 Duration: 1:21 Listen on YouTube Backup: Listen on Spotify Arpeggio by Oh Shu Album: Pulchra Ondo Duration: 1:29 Listen on YouTube Backup: Listen on Spotify La belle affaire by Forever Pavot Album: La pantoufle Duration: 2:18 Listen on YouTube Backup: Listen on Spotify City Of Refuge by Eddie Suzuki Album: City Of Refuge Duration: 3:36 Listen on YouTube Backup: Listen on Spotify Can\u0027t We Smile? by Johnny \u0026quot;Hammond\u0026quot; Smith Album: Gears Duration: 4:35 Listen on YouTube Backup: Listen on Spotify Cuidado by Ki!, Charlotte Schultz Album: Yong-Gwanglo Part One Duration: 4:57 Listen on YouTube Backup: Listen on Spotify LIFE IMITATES ART by CUPIDS OPPRESSION Album: LIFE IMITATES ART Duration: 2:00 Listen on YouTube Backup: Listen on Spotify Life by Life Album: Call Me Old Fashioned Duration: 2:19 Listen on YouTube Backup: Listen on Spotify Bloom by Benja, Glitter Party Album: Bloom Duration: 2:54 Listen on YouTube Backup: Listen on Spotify Unemployed by Tierra Whack Album: Unemployed Duration: 2:48 Listen on YouTube Backup: Listen on Spotify Get Lit by Kamasi Washington, George Clinton, D Smoke Album: Get Lit Duration: 3:26 Listen on YouTube Backup: Listen on Spotify Llegaste por Mi by Jesus Guerrero Album: Llegaste por Mi Duration: 3:20 Listen on YouTube Backup: Listen on Spotify Coyote, You\u0027re My Star by Dana and Alden Album: Coyote, You\u0027re My Star Duration: 2:05 Listen on YouTube Backup: Listen on Spotify I Am in Love by Jennifer Lara Album: Studio One Presents Jennifer Lara Duration: 5:18 Listen on YouTube Backup: Listen on Spotify Make Me Happy by Alton Ellis Album: Legend Duration: 2:44 Listen on YouTube Backup: Listen on Spotify Til You\u2019re Gone by Jr Thomas \u0026amp; The Volcanos Album: Rockstone Duration: 2:58 Listen on YouTube Backup: Listen on Spotify Gotta Find a Way by The Frightnrs Album: Nothing More to Say Duration: 3:49 Listen on YouTube Backup: Listen on Spotify Midnight by Willie Lindo Album: Breezing Duration: 3:25 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-april-2024","date":"2024-05-05 20:20 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"April\u0027s drop. This month, I kept coming back to: Pon Pon by Khruangbin This Feeling I Get by Doug Shorts Tracks Talk About It by Kate Bollinger Album: Talk About It (B-Side) Duration: 2:12 Listen on...","keywords":["listen","spotify","youtube","duration:","album:","backup:","2:58","life","super","love"]},{"id":"notes-2024-05-06-weekly-post-summary","title":"Week of May 06, 2024 - Post Summary","content":"Notes Crate Finds - April 2024 Week of April 29, 2024 - Post Summary","contentType":"notes","url":"https://www.lqdev.me/notes/2024-05-06-weekly-post-summary","date":"2024-05-05 19:24 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes Crate Finds - April 2024 Week of April 29, 2024 - Post Summary","keywords":["april","2024","notes","crate","finds","week","post","summary"]},{"id":"reply-one-year-retrospect-lightphone-2-verge","title":"One year with the Light Phone 2","content":"One year with the Light Phone 2 \u2192 https://www.theverge.com/24140675/light-phone-2-one-year-retrospect Between this article and The Dumbphone Boom is Real , I\u0027m seeing more publications on the subject. Far from a comeback, but still cool to see.","contentType":"reply","url":"https://www.lqdev.me/responses/one-year-retrospect-lightphone-2-verge","date":"2024-04-28 20:43 -05:00","tags":["dumbphone","retro","technology","lightphone","minimalism","digitalminimalism"],"summary":"One year with the Light Phone 2 \u2192 https://www.theverge.com/24140675/light-phone-2-one-year-retrospect Between this article and The Dumbphone Boom is Real , I\u0027m seeing more publications on the...","keywords":["year","phone","from","subject","publications","more","seeing","real","boom","dumbphone"]},{"id":"reshare-new-kamasi-washington-album-fearless-movement","title":"New Album: Kamasi Washington - Fearless Movement","content":"New Album: Kamasi Washington - Fearless Movement \u2192 https://kamasiwashington.ffm.to/fearlessmovement 2024 just keeps getting better in terms of new music releases. Kamasi Washington has a new album coming out this week called Fearless Movement. The most recent single, Dream State, with Andre 3000 is great.","contentType":"reshare","url":"https://www.lqdev.me/responses/new-kamasi-washington-album-fearless-movement","date":"2024-04-28 20:34 -05:00","tags":["newmusic","kamasiwashington","jazz","album","music"],"summary":"New Album: Kamasi Washington - Fearless Movement \u2192 https://kamasiwashington.ffm.to/fearlessmovement 2024 just keeps getting better in terms of new music releases. Kamasi Washington has a new album...","keywords":["kamasi","washington","fearless","movement","album:","state","dream","single","recent","called"]},{"id":"notes-2024-04-29-weekly-post-summary","title":"Week of April 29, 2024 - Post Summary","content":"Notes GitHub Actions Publishing Workflow Fixed Responses \u0027PBS Retro\u0027 is coming to Roku as a FAST channel Google Penzai The Dumbphone Boom Is Real Introducing Phi-3 Remembering the 90\u0027s Video Store New Album: Kamasi Washington - Fearless Movement One year with the Light Phone 2 SAMMO: A general-purpose framework for prompt optimization The Internet Used To Be Fun Calculus Made Easy Ghost is federating over ActivityPub Introducing Snowflake Arctic Grow Your Own Services We Need To Rewild The Internet","contentType":"notes","url":"https://www.lqdev.me/notes/2024-04-29-weekly-post-summary","date":"2024-04-28 19:55 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes GitHub Actions Publishing Workflow Fixed Responses \u0027PBS Retro\u0027 is coming to Roku as a FAST channel Google Penzai The Dumbphone Boom Is Real Introducing Phi-3 Remembering the 90\u0027s Video Store...","keywords":["introducing","internet","notes","light","phone","sammo:","general","purpose","framework","prompt"]},{"id":"reshare-dumbphone-boom-real","title":"The Dumbphone Boom Is Real","content":"The Dumbphone Boom Is Real \u2192 https://www.newyorker.com/culture/infinite-scroll/the-dumbphone-boom-is-real Will Stults spent too much time on his iPhone, doom-scrolling the site formerly known as Twitter and tweeting angrily at Elon Musk as if the billionaire would actually notice. Stults\u2019s partner, Daisy Krigbaum, was addicted to Pinterest and YouTube, bingeing videos on her iPhone before going to sleep. Two years ago, they both tried Apple\u2019s Screen Time restriction tool and found it too easy to disable, so the pair decided to trade out their iPhones for more low-tech devices. They\u2019d heard about so-called dumbphones, which lacked the kinds of bells and whistles\u2014a high-resolution screen, an app store, a video camera\u2014that made smartphones so addictive. But they found the process of acquiring one hard to navigate. \u201CThe information on it was kind of disparate and hard to get to. A lot of people who know the most about dumbphones spend the least time online,\u201D Krigbaum said. A certain irony presented itself: figuring out a way to be less online required aggressive online digging. The growing dumbphone fervor may be motivated, in part, by the discourse around child safety online. Parents are increasingly confronted with evidence that sites like Instagram and TikTok intentionally try to hook their children. Using those sites can increase teens\u2019 anxiety and lower their self-esteem, according to some studies, and smartphones make it so that kids are logged on constantly. Why should this situation be any healthier for adults? After almost two decades with iPhones, the public seems to be experiencing a collective ennui with digital life. So many hours of each day are lived through our portable, glowing screens, but the Internet isn\u2019t even fun anymore. We lack the self-control to wean ourselves off, so we crave devices that actively prevent us from getting sucked into them. That means opting out of the prevailing technology and into what Cal Newport, a contributing writer for The New Yorker, has called a more considered \u201Cdigital minimalism.\u201D While dumbphones aren\u0027t a cure-all for unhealthy technology habits, as a dumbphone user, I can relate to the frustrations that come from the lack of device availability and support. Even when new devices hit the market, they tend to be targeted towards non-US markets.","contentType":"reshare","url":"https://www.lqdev.me/responses/dumbphone-boom-real","date":"2024-04-25 23:38 -05:00","tags":["dumbphone","technology","culture","digitalminimalism","minimalism"],"summary":"The Dumbphone Boom Is Real \u2192 https://www.newyorker.com/culture/infinite-scroll/the-dumbphone-boom-is-real Will Stults spent too much time on his iPhone, doom-scrolling the site formerly known as...","keywords":["dumbphone","online","devices","they","dumbphones","more","iphones","even","found","lack"]},{"id":"reshare-snowflake-arctic-enterprise-ai-llm","title":"Introducing Snowflake Arctic","content":"Introducing Snowflake Arctic \u2192 https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/ Today, the Snowflake AI Research Team is thrilled to introduce Snowflake Arctic, a top-tier enterprise-focused LLM that pushes the frontiers of cost-effective training and openness. Arctic is efficiently intelligent and truly open. Efficiently Intelligent: Arctic excels at enterprise tasks such as SQL generation, coding and instruction following benchmarks even when compared to open source models trained with significantly higher compute budgets. In fact, it sets a new baseline for cost-effective training to enable Snowflake customers to create high-quality custom models for their enterprise needs at a low cost. Truly Open: Apache 2.0 license provides ungated access to weights and code. In addition, we are also open sourcing all of our data recipes and research insights. Snowflake Arctic is available from Hugging Face, NVIDIA API catalog and Replicate today or via your model garden or catalog of choice, including Snowflake Cortex, Amazon Web Services (AWS), Microsoft Azure, Lamini, Perplexity and Together over the coming days.","contentType":"reshare","url":"https://www.lqdev.me/responses/snowflake-arctic-enterprise-ai-llm","date":"2024-04-25 23:34 -05:00","tags":["snowflake","ai","llm","enterprise"],"summary":"Introducing Snowflake Arctic \u2192 https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/ Today, the Snowflake AI Research Team is thrilled to introduce Snowflake...","keywords":["snowflake","arctic","open","enterprise","cost","models","research","today","effective","training"]},{"id":"star-pbs-retro-roku-channel","title":"\u2018PBS Retro\u2019 is coming to Roku as a FAST channel","content":"\u2018PBS Retro\u2019 is coming to Roku as a FAST channel \u2192 https://www.theverge.com/2024/4/24/24139057/pbs-retro-free-roku-channel-fast-streaming PBS is making child edutainment classics like Zoboomafoo, Mister Rogers\u2019 Neighborhood, and Reading Rainbow available for free on a new \u2018PBS Retro\u2019 channel on Roku. This is cool! Although I\u0027m kind of bummed the stuff I grew up watching is now considered \u0026quot;retro\u0026quot;.","contentType":"star","url":"https://www.lqdev.me/responses/pbs-retro-roku-channel","date":"2024-04-25 23:31 -05:00","tags":["pbs","roku","tv","fast","retro","education"],"summary":"\u2018PBS Retro\u2019 is coming to Roku as a FAST channel \u2192 https://www.theverge.com/2024/4/24/24139057/pbs-retro-free-roku-channel-fast-streaming PBS is making child edutainment classics like Zoboomafoo,...","keywords":["roku","channel","\u2018pbs","retro\u2019","fast","free","rainbow","available","cool","although"]},{"id":"reshare-rewild-the-internet-farrell-berjon","title":"We Need To Rewild The Internet","content":"We Need To Rewild The Internet \u2192 https://www.noemamag.com/we-need-to-rewild-the-internet/ The internet has become an extractive and fragile monoculture. But we can revitalize it using lessons learned by ecologists. Our online spaces are not ecosystems, though tech firms love that word. They\u2019re plantations; highly concentrated and controlled environments... We all know this. We see it each time we reach for our phones. But what most people have missed is how this concentration reaches deep into the internet\u2019s infrastructure \u2014 the pipes and protocols, cables and networks, search engines and browsers. These structures determine how we build and use the internet, now and in the future. They\u2019ve concentrated into a series of near-planetary duopolies. Two kinds of everything may be enough to fill a fictional ark and repopulate a ruined world, but can\u2019t run an open, global \u201Cnetwork of networks\u201D where everyone has the same chance to innovate and compete. The internet made the tech giants possible. Their services have scaled globally, via its open, interoperable core. But for the past decade, they\u2019ve also worked to enclose the varied, competing and often open-source or collectively provided services the internet is built on into their proprietary domains. Although this improves their operational efficiency, it also ensures that the flourishing conditions of their own emergence aren\u2019t repeated by potential competitors. For tech giants, the long period of open internet evolution is over. Their internet is not an ecosystem. It\u2019s a zoo. Up close, internet concentration seems too intricate to untangle; from far away, it seems too difficult to deal with. But what if we thought of the internet not as a doomsday \u201Chyperobject,\u201D but as a damaged and struggling ecosystem facing destruction? What if we looked at it not with helpless horror at the eldritch encroachment of its current controllers, but with compassion, constructiveness and hope? Rewilding \u201Caims to restore healthy ecosystems by creating wild, biodiverse spaces,\u201D according to the International Union for Conservation of Nature. More ambitious and risk-tolerant than traditional conservation, it targets entire ecosystems to make space for complex food webs and the emergence of unexpected interspecies relations. It\u2019s less interested in saving specific endangered species. Individual species are just ecosystem components, and focusing on components loses sight of the whole. Ecosystems flourish through multiple points of contact between their many elements, just like computer networks. And like in computer networks, ecosystem interactions are multifaceted and generative. Whatever we do, the internet isn\u2019t returning to old-school then-common interfaces like FTP and Gopher, or organizations operating their own mail servers again instead of off-the-shelf solutions like G-Suite. But some of what we need is already here, especially on the web. Look at the resurgence of RSS feeds, email newsletters and blogs, as we discover (yet again) that relying on one app to host global conversations creates a single point of failure and control. New systems are growing, like the Fediverse with its federated islands, or Bluesky with algorithmic choice and composable moderation. We don\u2019t know what the future holds. Our job is to keep open as much opportunity as we can, trusting that those who come later will use it. Instead of setting purity tests for which kind of internet is most like the original, we can test changes against the values of the original design. Do new standards protect the network\u2019s \u201Cgenerality,\u201D i.e. its ability to support multiple uses, or is functionality limited to optimize efficiency for the biggest tech firms? ...our internet took off because it was designed as a general-purpose network, built to connect anyone. Our internet was built to be complex and unbiddable, to do things we cannot yet imagine. Internet infrastructure is a degraded ecosystem, but it\u2019s also a built environment, like a city. Its unpredictability makes it generative, worthwhile and deeply human. We need to stop thinking of internet infrastructure as too hard to fix. It\u2019s the underlying system we use for nearly everything we do. Rewilding the internet connects and grows what people are doing across regulation, standards-setting and new ways of organizing and building infrastructure, to tell a shared story of where we want to go. It\u2019s a shared vision with many strategies. The instruments we need to shift away from extractive technological monocultures are at hand or ready to be built.","contentType":"reshare","url":"https://www.lqdev.me/responses/rewild-the-internet-farrell-berjon","date":"2024-04-25 23:11 -05:00","tags":["internet","technology","openweb","personalweb","online"],"summary":"We Need To Rewild The Internet \u2192 https://www.noemamag.com/we-need-to-rewild-the-internet/ The internet has become an extractive and fragile monoculture. But we can revitalize it using lessons learned...","keywords":["internet","need","it\u2019s","ecosystem","built","open","infrastructure","tech","ecosystems","networks"]},{"id":"bookmarks-calculus-made-easy","title":"Calculus Made Easy","content":"Calculus Made Easy \u2192 https://calculusmadeeasy.org/ Calculus Made Easy is a book on calculus originally published in 1910 by Silvanus P. Thompson, considered a classic and elegant introduction to the subject. Project Gutenberg PDF","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/calculus-made-easy","date":"2024-04-25 23:01 -05:00","tags":["calculus","book","math"],"summary":"Calculus Made Easy \u2192 https://calculusmadeeasy.org/ Calculus Made Easy is a book on calculus originally published in 1910 by Silvanus P. Thompson, considered a classic and elegant introduction to the...","keywords":["calculus","made","easy","subject","introduction","elegant","classic","considered","thompson","1910"]},{"id":"bookmarks-internet-used-to-be-fun-kwon","title":"The Internet Used To Be Fun","content":"The Internet Used To Be Fun \u2192 https://projects.kwon.nyc/internet-is-fun/ I\u2019ve been meaning to write some kind of Important Thinkpiece\u2122 on the glory days of the early internet, but every time I sit down to do it, I find another, better piece that someone else has already written. So for now, here\u2019s a collection of articles that to some degree answer the question \u201CWhy have a personal website?\u201D with \u201CBecause it\u2019s fun, and the internet used to be fun.\u201D This is a great catalog of posts about the personal web. Courtesy of Rachel Kwon","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/internet-used-to-be-fun-kwon","date":"2024-04-25 22:56 -05:00","tags":["internet","indieweb","personalweb","online","blogging","blogs"],"summary":"The Internet Used To Be Fun \u2192 https://projects.kwon.nyc/internet-is-fun/ I\u2019ve been meaning to write some kind of Important Thinkpiece\u2122 on the glory days of the early internet, but every time I sit...","keywords":["internet","used","kwon","personal","already","written","here\u2019s","collection","articles","degree"]},{"id":"bookmarks-grow-your-own-services","title":"Grow Your Own Services","content":"Grow Your Own Services \u2192 https://growyourown.services/ This is a site encouraging non-technical people and organisations to create their own online services such as websites, social networks, personal clouds, instant messaging etc.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/grow-your-own-services","date":"2024-04-25 22:53 -05:00","tags":["selfhosting","indieweb","openweb","internet","community","cloud","messaging","website","socialnetworks","websites","online"],"summary":"Grow Your Own Services \u2192 https://growyourown.services/ This is a site encouraging non-technical people and organisations to create their own online services such as websites, social networks,...","keywords":["services","grow","clouds","personal","networks","social","websites","online","create","organisations"]},{"id":"reshare-sammo-prompt-optimization-framework","title":"SAMMO: A general-purpose framework for prompt optimization","content":"SAMMO: A general-purpose framework for prompt optimization \u2192 https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/ Large language models (LLMs) have revolutionized a wide range of tasks and applications that were previously reliant on manually crafted machine learning (ML) solutions, streamlining through automation. However, despite these advances, a notable challenge persists: the need for extensive prompt engineering to adapt these models to new tasks. New generations of language models like GPT-4 and Mixtral 8x7B advance the capability to process long input texts. This progress enables the use of longer inputs, providing richer context and detailed instructions to language models. A common technique that uses this enhanced capacity is the Retrieval Augmented Generation (RAG) approach. RAG dynamically incorporates information into the prompt based on the specific input example. To address these challenges, we developed the Structure-Aware Multi-objective Metaprompt Optimization (SAMMO) framework. SAMMO is a new open-source tool that streamlines the optimization of prompts, particularly those that combine different types of structural information like in the RAG example above. It can make structural changes, such as removing entire components or replacing them with different ones. These features enable AI practitioners and researchers to efficiently refine their prompts with little manual effort. Central to SAMMO\u2019s innovation is its approach to treating prompts not just as static text inputs but as dynamic, programmable entities\u2014metaprompts. SAMMO represents these metaprompts as function graphs, where individual components and substructures can be modified to optimize performance, similar to the optimization process that occurs during traditional program compilation. The following key features contribute to SAMMO\u2019s effectiveness: Structured optimization: Unlike current methods that focus on text-level changes, SAMMO focuses on optimizing the structure of metaprompts. This granular approach facilitates precise modifications and enables the straightforward integration of domain knowledge, for instance, through rewrite operations targeting specific stylistic objectives. Multi-objective search: SAMMO\u2019s flexibility enables it to simultaneously address multiple objectives, such as improving accuracy and computational efficiency. Our paper illustrates how SAMMO can be used to compress prompts without compromising their accuracy. General purpose application: SAMMO has proven to deliver significant performance improvements across a variety of tasks, including instruction tuning, RAG, and prompt compression.","contentType":"reshare","url":"https://www.lqdev.me/responses/sammo-prompt-optimization-framework","date":"2024-04-25 22:48 -05:00","tags":["ai","microsoft","research","prompt","optimization","sammo"],"summary":"SAMMO: A general-purpose framework for prompt optimization \u2192 https://www.microsoft.com/en-us/research/blog/sammo-a-general-purpose-framework-for-prompt-optimization/ Large language models (LLMs) have...","keywords":["sammo","prompt","prompts","models","optimization","tasks","sammo\u2019s","language","general","approach"]},{"id":"reply-remembering-90s-video-store-plummer","title":"Remembering the 90\u0027s Video Store","content":"Remembering the 90\u0026#39;s Video Store \u2192 https://amerpie.lol/2024/04/21/in-the-prestreaming.html Reading this post and attached image brings back so many memories. I remember in the last few years of Blockbuster and early days of Netflix, for about $20/month you could rent unlimited movies (I think it was up to three at a time). If you binge watched them or weren\u0027t happy with the choices you made, you could just drive down to your local store, return them, and grab a new set of movies. Initially you had a return period, but towards the end, there were none so you basically got to keep some movies as long as you liked. Good times.","contentType":"reply","url":"https://www.lqdev.me/responses/remembering-90s-video-store-plummer","date":"2024-04-25 22:31 -05:00","tags":["nostalgia","90s","videos","blockbuster"],"summary":"Remembering the 90\u0026#39;s Video Store \u2192 https://amerpie.lol/2024/04/21/in-the-prestreaming.html Reading this post and attached image brings back so many memories. I remember in the last few years of...","keywords":["movies","return","store","three","binge","watched","weren","happy","choices","made"]},{"id":"star-ghost-activitypub-integration-announcement","title":"Ghost is federating over ActivityPub","content":"Ghost is federating over ActivityPub \u2192 https://activitypub.ghost.org/ In 2024, Ghost is adopting ActivityPub and connecting with other federated platforms across the web. This means that, soon, Ghost publishers will be able to follow, like and interact with one another in the same way that you would normally do on a social network \u2014 but on your own website. The difference, of course, is that you\u2019ll also be able to follow, like, and interact with users on Mastodon, Threads, Flipboard, Buttondown, WriteFreely, Tumblr, WordPress, PeerTube, Pixelfed... or any other platform that has adopted ActivityPub, too. You don\u2019t need to limit yourself to following people who happen to use the same platform as you. For the past few years the choice has been difficult. Either participate in closed networks at the mercy of algorithms, or set up an independent website at the expense of your growth. Email gave us private messaging technology that isn\u2019t owned by a single company. ActivityPub is doing the same for social technology. The open web is coming back, and with it returns diversity. You can both publish independently and grow faster than ever before with followers from all over the world \u0026amp; the web. I can\u0027t express how much I love this. Personally I don\u0027t use Ghost, but given platforms like WordPress and now Ghost are adding support for ActivityPub, it empowers people to build their own platforms. That said, this still doesn\u0027t address the challenges of building your own website, which as mentioned in the post are one of the appealing aspects of current closed networks. Still though, there is a vast number of creators, businesses, and company websites or blogs that can benefit from this today. When paired with RSS, it gives people choice and autonomy in how they create and consume content as I mentioned in a previous post, Rediscovering the RSS protocol","contentType":"star","url":"https://www.lqdev.me/responses/ghost-activitypub-integration-announcement","date":"2024-04-25 22:11 -05:00","tags":["ghost","openweb","activitypub","fediverse","blogging","websites","indieweb"],"summary":"Ghost is federating over ActivityPub \u2192 https://activitypub.ghost.org/ In 2024, Ghost is adopting ActivityPub and connecting with other federated platforms across the web. This means that, soon, Ghost...","keywords":["ghost","activitypub","people","platforms","website","same","technology","from","networks","closed"]},{"id":"reshare-google-jax-penzai","title":"Google Penzai","content":"Google Penzai \u2192 https://github.com/google-deepmind/penzai A JAX research toolkit for building, editing, and visualizing neural networks. Penzai is a JAX library for writing models as legible, functional pytree data structures, along with tools for visualizing, modifying, and analyzing them. Penzai focuses on making it easy to do stuff with models after they have been trained, making it a great choice for research involving reverse-engineering or ablating model components, inspecting and probing internal activations, performing model surgery, debugging architectures, and more. (But if you just want to build and train a model, you can do that too!)","contentType":"reshare","url":"https://www.lqdev.me/responses/google-jax-penzai","date":"2024-04-25 22:06 -05:00","tags":["google","jax","ai","visualization"],"summary":"Google Penzai \u2192 https://github.com/google-deepmind/penzai A JAX research toolkit for building, editing, and visualizing neural networks. Penzai is a JAX library for writing models as legible,...","keywords":["penzai","model","making","research","visualizing","models","they","trained","choice","involving"]},{"id":"notes-gha-publishing-workflow-fixed","title":"GitHub Actions Publishing Workflow Fixed","content":"The website had been out of comission for a few days because my credentials had expired. I could still add posts and make updates. However, none of them were published to Azure. Figuring out the right incantations wasn\u0027t straightforward, so saving the fix for future reference.","contentType":"notes","url":"https://www.lqdev.me/notes/gha-publishing-workflow-fixed","date":"2024-04-23 22:51 -05:00","tags":["github","blogging","azure","githubactions","cicd"],"summary":"The website had been out of comission for a few days because my credentials had expired. I could still add posts and make updates. However, none of them were published to Azure. Figuring out the...","keywords":["website","saving","straightforward","wasn","incantations","figuring","azure","published","none","however"]},{"id":"reshare-introducing-phi-3","title":"Introducing Phi-3","content":"Introducing Phi-3 \u2192 https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/ Starting today, Phi-3-mini, a 3.8B language model is available on Microsoft Azure AI Studio, Hugging Face, and Ollama. Phi-3-mini is available in two context-length variants\u20144K and 128K tokens. It is the first model in its class to support a context window of up to 128K tokens, with little impact on quality. It is instruction-tuned, meaning that it\u2019s trained to follow different types of instructions reflecting how people normally communicate. This ensures the model is ready to use out-of-the-box. It is available on Azure AI to take advantage of the deploy-eval-finetune toolchain, and is available on Ollama for developers to run locally on their laptops. It has been optimized for ONNX Runtime with support for Windows DirectML along with cross-platform support across graphics processing unit (GPU), CPU, and even mobile hardware. It is also available as an NVIDIA NIM microservice with a standard API interface that can be deployed anywhere. And has been optimized for NVIDIA GPUs. In the coming weeks, additional models will be added to Phi-3 family to offer customers even more flexibility across the quality-cost curve. Phi-3-small (7B) and Phi-3-medium (14B) will be available in the Azure AI model catalog and other model gardens shortly.","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-phi-3","date":"2024-04-23 22:47 -05:00","tags":["microsoft","phi3","ai","slm","llm","genai"],"summary":"Introducing Phi-3 \u2192 https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/ Starting today, Phi-3-mini, a 3.8B language model is available on Microsoft Azure AI...","keywords":["available","model","azure","support","nvidia","tokens","128k","context","ollama","even"]},{"id":"reshare-introducing-llama-3","title":"Introducing Llama 3","content":"Introducing Llama 3 \u2192 https://llama.meta.com/llama3/ Build the future of AI with Meta Llama 3 Now available with both 8B and 70B pretrained and instruction-tuned versions to support a wide range of applications Llama 3 models take data and scale to new heights. It\u2019s been trained on our two recently announced custom-built 24K GPU clusters on over 15T token of data \u2013 a training dataset 7x larger than that used for Llama 2, including 4x more code. This results in the most capable Llama model yet, which supports a 8K context length that doubles the capacity of Llama 2.","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-llama-3","date":"2024-04-18 21:01 -05:00","tags":["meta","ai","llama3","llama","llm"],"summary":"Introducing Llama 3 \u2192 https://llama.meta.com/llama3/ Build the future of AI with Meta Llama 3 Now available with both 8B and 70B pretrained and instruction-tuned versions to support a wide range of...","keywords":["llama","meta","data","introducing","built","clusters","token","training","dataset","larger"]},{"id":"reshare-radio-bilingue-rock-alterlatino","title":"Rock AlterLatino from Radio Bilingue","content":"Rock AlterLatino from Radio Bilingue \u2192 http://https://https://radiobilingue.org/rb-programas/alterlatino/ While listening to KHOL earlier today, they were rebroadcasting a recording of A Todo Pulmon , a radio show from Radio Bilingue in Fresno, CA. Good stuff.","contentType":"reshare","url":"https://www.lqdev.me/responses/radio-bilingue-rock-alterlatino","date":"2024-04-14 15:18 -05:00","tags":["radio","music","rock","alternative","fresno","radiobilingue","khol"],"summary":"Rock AlterLatino from Radio Bilingue \u2192 http://https://https://radiobilingue.org/rb-programas/alterlatino/ While listening to KHOL earlier today, they were rebroadcasting a recording of A Todo Pulmon...","keywords":["radio","from","bilingue","rock","fresno","show","pulmon","todo","recording","rebroadcasting"]},{"id":"notes-2024-04-14-weekly-post-summary","title":"Week of April 14, 2024 - Post Summary","content":"Notes These models are too damn big! Enjoying El Sonido from KEXP New podcast (Better Offline) added to blogroll Week of April 07, 2024 - Post Summary Responses LLM training in simple, raw C/CUDA RecurrentGemma - Open weights language model from Google DeepMind, based on Griffin. Beeper was just acquired by Automattic Hello OLMo: A truly open LLM Molly White\u0027s Blogroll ARAGOG: Advanced RAG Output Grading Rock AlterLatino from Radio Bilingue Far Out Guides Proton and Standard Notes are joining forces DE-COP: Detecting Copyrighted Content in Language Models Training Data Using LLM to select the right SQL Query from candidates Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models","contentType":"notes","url":"https://www.lqdev.me/notes/2024-04-14-weekly-post-summary","date":"2024-04-14 14:23 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes These models are too damn big! Enjoying El Sonido from KEXP New podcast (Better Offline) added to blogroll Week of April 07, 2024 - Post Summary Responses LLM training in simple, raw C/CUDA...","keywords":["from","models","language","notes","open","blogroll","training","alterlatino","radio","forces"]},{"id":"reshare-proton-acquires-standard-notes","title":"Proton and Standard Notes are joining forces ","content":"Proton and Standard Notes are joining forces \u2192 https://proton.me/blog/proton-standard-notes-join-forces ...today, we\u2019re happy to announce that Standard Notes will also join us to advance our shared mission. Both Proton and Standard Notes share a strong commitment to our communities, so Standard Notes will remain open source, freely available, and fully supported. Prices are not changing, and if you have a current subscription to Standard Notes, it will continue to be honored. Proton aspires to do the right thing and be a responsible home for open-source projects, and just as we did with SimpleLogin, we are committed to preserving what makes Standard Notes special and much loved. In the coming months, we hope to find ways to make Standard Notes more easily accessible to the Proton community. This way, in addition to protecting your email, calendar, files, passwords, and online activity, you can also protect your notes. This is another exciting acquisition! I mainly use org-mode in Emacs for note taking. However, I love the ecosystem Proton is building with their security and privacy focused set of collaborative software offerings.","contentType":"reshare","url":"https://www.lqdev.me/responses/proton-acquires-standard-notes","date":"2024-04-10 22:35 -05:00","tags":["proton","standardnotes","notetaking","security","privacy","email","notes"],"summary":"Proton and Standard Notes are joining forces \u2192 https://proton.me/blog/proton-standard-notes-join-forces ...today, we\u2019re happy to announce that Standard Notes will also join us to advance our shared...","keywords":["notes","standard","proton","open","source","join","also","forces","months","coming"]},{"id":"bookmarks-griffin-mix-linear-recurrence-local-attention-language-models","title":"Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models","content":"Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models \u2192 https://arxiv.org/abs/2402.19427 Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/griffin-mix-linear-recurrence-local-attention-language-models","date":"2024-04-10 22:02 -05:00","tags":["griffin","ai","research","architecture","rnn","attention","transformers","llm"],"summary":"Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models \u2192 https://arxiv.org/abs/2402.19427 Recurrent neural networks (RNNs) have fast inference and scale...","keywords":["griffin","training","models","recurrences","during","gated","linear","scale","local","attention"]},{"id":"bookmarks-decop-detecting-copyright-llm-training-data","title":"DE-COP: Detecting Copyrighted Content in Language Models Training Data","content":"DE-COP: Detecting Copyrighted Content in Language Models Training Data \u2192 https://arxiv.org/abs/2402.09910 How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP\u0027s core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model\u0027s training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give \u2248 4% accuracy. Our code and datasets are available at this https URL Repo","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/decop-detecting-copyright-llm-training-data","date":"2024-04-10 22:00 -05:00","tags":["ai","copyright","research","llm","data"],"summary":"DE-COP: Detecting Copyrighted Content in Language Models Training Data \u2192 https://arxiv.org/abs/2402.09910 How can we detect if copyrighted content was used in the training process of a language...","keywords":["training","copyrighted","content","language","models","prior","model","available","books","method"]},{"id":"bookmarks-llm-text-to-sql-right-candidates","title":"Using LLM to select the right SQL Query from candidates","content":"Using LLM to select the right SQL Query from candidates \u2192 https://arxiv.org/abs/2401.02115 Text-to-SQL models can generate a list of candidate SQL queries, and the best query is often in the candidate list, but not at the top of the list. An effective re-rank method can select the right SQL query from the candidate list and improve the model\u0027s performance. Previous studies on code generation automatically generate test cases and use them to re-rank candidate codes. However, automatic test case generation for text-to-SQL is an understudied field. We propose an automatic test case generation method that first generates a database and then uses LLMs to predict the ground truth, which is the expected execution results of the ground truth SQL query on this database. To reduce the difficulty for LLMs to predict, we conduct experiments to search for ways to generate easy databases for LLMs and design easy-to-understand prompts. Based on our test case generation method, we propose a re-rank method to select the right SQL query from the candidate list. Given a candidate list, our method can generate test cases and re-rank the candidate list according to their pass numbers on these test cases and their generation probabilities. The experiment results on the validation dataset of Spider show that the performance of some state-of-the-art models can get a 3.6% improvement after applying our re-rank method.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/llm-text-to-sql-right-candidates","date":"2024-04-10 21:59 -05:00","tags":["ai","research","sql","llm"],"summary":"Using LLM to select the right SQL Query from candidates \u2192 https://arxiv.org/abs/2401.02115 Text-to-SQL models can generate a list of candidate SQL queries, and the best query is often in the...","keywords":["list","candidate","test","method","query","generation","rank","generate","select","from"]},{"id":"reshare-google-deepmind-recurrent-gemma","title":"RecurrentGemma - Open weights language model from Google DeepMind, based on Griffin.","content":"RecurrentGemma - Open weights language model from Google DeepMind, based on Griffin. \u2192 https://github.com/google-deepmind/recurrentgemma RecurrentGemma is a family of open-weights Language Models by Google DeepMind, based on the novel Griffin architecture. This architecture achieves fast inference when generating long sequences by replacing global attention with a mixture of local attention and linear recurrences. This repository contains the model implementation and examples for sampling and fine-tuning. We recommend most users adopt the Flax implementation, which is highly optimized. We also provide an un-optimized PyTorch implementation for reference.","contentType":"reshare","url":"https://www.lqdev.me/responses/google-deepmind-recurrent-gemma","date":"2024-04-10 21:57 -05:00","tags":["ai","gemma","google","llm","opensource","slm","griffin","neuralnetwork"],"summary":"RecurrentGemma - Open weights language model from Google DeepMind, based on Griffin. \u2192 https://github.com/google-deepmind/recurrentgemma RecurrentGemma is a family of open-weights Language Models by...","keywords":["implementation","recurrentgemma","optimized","griffin","architecture","deepmind","based","attention","model","language"]},{"id":"reshare-allen-ai-olmo-llm","title":"Hello OLMo: A truly open LLM","content":"Hello OLMo: A truly open LLM \u2192 https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e7359222 Today, The Allen Institute for AI (AI2) has released OLMo 7B, a truly open, state-of-the-art large language model released alongside the pre-training data and training code. This empowers researchers and developers to use the best and open models to advance the science of language models collectively. OLMo-7B on HuggingFace","contentType":"reshare","url":"https://www.lqdev.me/responses/allen-ai-olmo-llm","date":"2024-04-09 22:17 -05:00","tags":["allenai","llm","opensource"],"summary":"Hello OLMo: A truly open LLM \u2192 https://blog.allenai.org/hello-olmo-a-truly-open-llm-43f7e7359222 Today, The Allen Institute for AI (AI2) has released OLMo 7B, a truly open, state-of-the-art large...","keywords":["open","truly","olmo","training","models","released","language","hello","data","code"]},{"id":"reshare-llm-c-karpathy","title":"LLM training in simple, raw C/CUDA ","content":"LLM training in simple, raw C/CUDA \u2192 https://github.com/karpathy/llm.c LLM training in simple, pure C/CUDA. There is no need for 245MB of PyTorch or 107MB of cPython. For example, training GPT-2 (CPU, fp32) is ~1,000 lines of clean code in a single file. It compiles and runs instantly, and exactly matches the PyTorch reference implementation. I chose GPT-2 as the first working example because it is the grand-daddy of LLMs, the first time the modern stack was put together.","contentType":"reshare","url":"https://www.lqdev.me/responses/llm-c-karpathy","date":"2024-04-09 22:15 -05:00","tags":["llm","gpt","c","programming","learning","tutorial"],"summary":"LLM training in simple, raw C/CUDA \u2192 https://github.com/karpathy/llm.c LLM training in simple, pure C/CUDA. There is no need for 245MB of PyTorch or 107MB of cPython. For example, training GPT-2...","keywords":["training","example","pytorch","simple","c/cuda","modern","llms","daddy","grand","working"]},{"id":"reshare-beeper-acquired-automattic","title":"Beeper was just acquired by Automattic","content":"Beeper was just acquired by Automattic \u2192 https://www.theverge.com/2024/4/9/24124179/beeper-app-automattic-acquisition-matrix-messaging Beeper, the upstart messaging app that attempts to corral all your messaging services into one inbox, is being acquired by Automattic, the giant that runs Wordpress.com, Tumblr, and a number of other hugely popular web properties This is exciting especially given some of the recent developments in the EU. What\u0027s most interesting to me is how Beeper leverages open protocols like Matrix and for bridging capabilities where possible to provide secure messaging. With more people moving to smaller spaces to communicate with their communities, being able to do so in a single place without everyone being on the same platform like in the early days of the internet is a welcome development. Additional coverage from the Beeper blog . What we\u2019re announcing today\u2026 No more waitlist \u2013 Beeper is now available to everyone! Beeper has been acquired by Automattic Our new Android app is out of beta We\u2019re renaming Beeper Cloud \u2192 Beeper (sorry for the confusion) and Matt Mullenweg\u0027s blog Today the announcement went out that we\u2019re combining the best technology from Beeper and Texts to create a great private, secure, and open source messaging client for people to have control of their communications. We\u2019re going to use the Beeper brand, because it\u2019s fun. This is not unlike how browsers have evolved, where solid tech and encryption on top of an open ecosystem has created untold value for humanity. A lot of people are asking about iMessage on Android\u2026 I have zero interest in fighting with Apple, I think instead it\u2019s best to focus on messaging networks that want more engagement from power-user clients. This is an area I\u2019m excited to work on when I return from my sabbatical next month.","contentType":"reshare","url":"https://www.lqdev.me/responses/beeper-acquired-automattic","date":"2024-04-09 22:06 -05:00","tags":["beeper","messaging","automattic","matrix"],"summary":"Beeper was just acquired by Automattic \u2192 https://www.theverge.com/2024/4/9/24124179/beeper-app-automattic-acquisition-matrix-messaging Beeper, the upstart messaging app that attempts to corral all...","keywords":["beeper","messaging","automattic","we\u2019re","from","acquired","people","open","more","it\u2019s"]},{"id":"reshare-aragog-advanced-rag-output-grading","title":"ARAGOG: Advanced RAG Output Grading","content":"ARAGOG: Advanced RAG Output Grading \u2192 https://arxiv.org/abs/2404.01037 Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods\u0027 impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary Index as a competent retrieval approach. All resources related to this research are publicly accessible for further investigation through our GitHub repository ARAGOG (this https URL). We welcome the community to further this exploratory study in RAG systems. GitHub repo","contentType":"reshare","url":"https://www.lqdev.me/responses/aragog-advanced-rag-output-grading","date":"2024-04-09 00:32 -05:00","tags":["rag","ai","research","llm","knowledge","retrieval","retrievalaugmentedgeneration"],"summary":"ARAGOG: Advanced RAG Output Grading \u2192 https://arxiv.org/abs/2404.01037 Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs....","keywords":["retrieval","precision","study","answer","further","similarity","github","comparisons","document","rerank"]},{"id":"notes-slm-too-big","title":"These models are too damn big!","content":"While the size of these smaller language models is significantly less than the trillion parameter models like GPT, they still take up a lot of storage space. Playing around with Mistral 7B Instruct v0.2 , the safetensor files containing the weights take up roughly 15GB of space. I\u0027m thinking of playing around with blobfuse to mount a storage container to my local file system. That way, I\u0027m only caching and accessing the models I\u0027m working with at any given time.","contentType":"notes","url":"https://www.lqdev.me/notes/slm-too-big","date":"2024-04-08 23:43 -05:00","tags":["ai","slm","huggingface","llm","opensource"],"summary":"While the size of these smaller language models is significantly less than the trillion parameter models like GPT, they still take up a lot of storage space. Playing around with Mistral 7B Instruct...","keywords":["models","around","playing","space","storage","while","15gb","thinking","blobfuse","mount"]},{"id":"notes-enjoying-kexp-el-sonido","title":"Enjoying El Sonido from KEXP","content":"I just found El Sonido from KEXP . Albina Cabrera is a great host. I\u0027ve already found a few new songs and artists to listen to I otherwise wouldn\u0027t have been aware of or exposed to.","contentType":"notes","url":"https://www.lqdev.me/notes/enjoying-kexp-el-sonido","date":"2024-04-08 23:36 -05:00","tags":["radio","music","kexp","shows"],"summary":"I just found El Sonido from KEXP . Albina Cabrera is a great host. I\u0027ve already found a few new songs and artists to listen to I otherwise wouldn\u0027t have been aware of or exposed to.","keywords":["found","sonido","from","kexp","albina","cabrera","host","already","songs","artists"]},{"id":"bookmarks-far-out-guides-hiking-app","title":"Far Out Guides","content":"Far Out Guides \u2192 https://faroutguides.com/ Outdoor nagivation app for long-distance trails","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/far-out-guides-hiking-app","date":"2024-04-07 19:38 -05:00","tags":["outdoors","app","hiking","technology"],"summary":"Far Out Guides \u2192 https://faroutguides.com/ Outdoor nagivation app for long-distance trails","keywords":["guides","https://faroutguides","com/","outdoor","nagivation","long","distance","trails"]},{"id":"bookmarks-blogroll-molly-white","title":"Molly White\u0027s Blogroll","content":"Molly White\u0026#39;s Blogroll \u2192 https://www.mollywhite.net/blogroll/ Bookmarking for reference. I\u0027m already subscribed to many of these websites and publications. However, there\u0027s several new ones I found that I think will eventually make their rotation into my blogroll .","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/blogroll-molly-white","date":"2024-04-07 17:50 -05:00","tags":["blogroll","indieweb","rss","blogging","web","internet","smallweb","community"],"summary":"Molly White\u0026#39;s Blogroll \u2192 https://www.mollywhite.net/blogroll/ Bookmarking for reference. I\u0027m already subscribed to many of these websites and publications. However, there\u0027s several new ones I...","keywords":["blogroll","molly","found","ones","several","however","publications","websites","many","subscribed"]},{"id":"notes-better-offline-podroll","title":"New podcast (Better Offline) added to blogroll","content":"While doing chores today, I put on one of the episodes from Ed Zitron\u0027s Better Offline podcast . This resulted me in me basically binging the rest of the episodes on the backlog. Also, it\u0027s now made its way into the regular rotation with other podcasts and the podroll .","contentType":"notes","url":"https://www.lqdev.me/notes/better-offline-podroll","date":"2024-04-07 16:50 -05:00","tags":["podcast","podroll","betteroffline"],"summary":"While doing chores today, I put on one of the episodes from Ed Zitron\u0027s Better Offline podcast . This resulted me in me basically binging the rest of the episodes on the backlog. Also, it\u0027s now made...","keywords":["episodes","while","rotation","regular","made","also","backlog","rest","binging","basically"]},{"id":"notes-2024-04-07-weekly-post-summary","title":"Week of April 07, 2024 - Post Summary","content":"Notes New Album - Notebook Fantasy from Chicano Batman New podcast added to blogroll Website Post Statistics - April 2024 Week of April 1, 2024 - Post Summary Thoughts on World Wide Whack Crate Finds - March 2024 Responses Large Language Models Are Zero-Shot Time Series Forecasters Google Podcasts is gone \u2014 and so is my faith in Google IPEX-LLM Introducing Stable Audio 2.0 Start using ChatGPT instantly Introducing Command R\u002B: A Scalable LLM Built for Business OpenAI - Introducing improvements to the fine-tuning API and expanding our custom models program Announcing DBRX: A new standard for efficient open source LLMs ReALM: Reference Resolution As Language Modeling T-Mobile Sidekick\u2019s Jump button made mobile multitasking easy The Matrix is coming back for a fifth movie","contentType":"notes","url":"https://www.lqdev.me/notes/2024-04-07-weekly-post-summary","date":"2024-04-07 16:01 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes New Album - Notebook Fantasy from Chicano Batman New podcast added to blogroll Website Post Statistics - April 2024 Week of April 1, 2024 - Post Summary Thoughts on World Wide Whack Crate Finds...","keywords":["introducing","2024","mobile","language","models","google","april","post","program","custom"]},{"id":"reshare-openai-introducing-fine-tuning-improvements","title":"OpenAI - Introducing improvements to the fine-tuning API and expanding our custom models program","content":"OpenAI - Introducing improvements to the fine-tuning API and expanding our custom models program \u2192 https://openai.com/blog/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-models-program New fine-tuning API features Today, we\u2019re introducing new features to give developers even more control over their fine-tuning jobs, including: Epoch-based Checkpoint Creation: Automatically produce one full fine-tuned model checkpoint during each training epoch, which reduces the need for subsequent retraining, especially in the cases of overfitting Comparative Playground: A new side-by-side Playground UI for comparing model quality and performance, allowing human evaluation of the outputs of multiple models or fine-tune snapshots against a single prompt Third-party Integration: Support for integrations with third-party platforms (starting with Weights and Biases this week) to let developers share detailed fine-tuning data to the rest of their stack Comprehensive Validation Metrics: The ability to compute metrics like loss and accuracy over the entire validation dataset instead of a sampled batch, providing better insight on model quality Hyperparameter Configuration: The ability to configure available hyperparameters from the Dashboard (rather than only through the API or SDK) Fine-Tuning Dashboard Improvements: Including the ability to configure hyperparameters, view more detailed training metrics, and rerun jobs from previous configurations Expanding our Custom Models Program Assisted Fine-Tuning Today, we are formally announcing our assisted fine-tuning offering as part of the Custom Model program. Assisted fine-tuning is a collaborative effort with our technical teams to leverage techniques beyond the fine-tuning API, such as additional hyperparameters and various parameter efficient fine-tuning (PEFT) methods at a larger scale. It\u2019s particularly helpful for organizations that need support setting up efficient training data pipelines, evaluation systems, and bespoke parameters and methods to maximize model performance for their use case or task. Custom-Trained Model In some cases, organizations need to train a purpose-built model from scratch that understands their business, industry, or domain. Fully custom-trained models imbue new knowledge from a specific domain by modifying key steps of the model training process using novel mid-training and post-training techniques. Organizations that see success with a fully custom-trained model often have large quantities of proprietary data\u2014millions of examples or billions of tokens\u2014that they want to use to teach the model new knowledge or complex, unique behaviors for highly specific use cases.","contentType":"reshare","url":"https://www.lqdev.me/responses/openai-introducing-fine-tuning-improvements","date":"2024-04-04 22:58 -05:00","tags":["openai","finetuning","llm"],"summary":"OpenAI - Introducing improvements to the fine-tuning API and expanding our custom models program \u2192 https://openai.com/blog/introducing-improvements-to-the-fine-tuning-api-and-expanding-our-custom-mode...","keywords":["fine","tuning","model","custom","training","models","program","from","assisted","need"]},{"id":"reshare-introducing-cohere-comandr-plus","title":"Introducing Command R\u002B: A Scalable LLM Built for Business","content":"Introducing Command R\u002B: A Scalable LLM Built for Business \u2192 https://txt.cohere.com/command-r-plus-microsoft-azure/ Command R\u002B is a state-of-the-art RAG-optimized model designed to tackle enterprise-grade workloads, and is available first on Microsoft Azure Command R\u002B, like our recently launched Command R model, features a 128k-token context window and is designed to offer best-in-class: Advanced Retrieval Augmented Generation (RAG) with citation to reduce hallucinations Multilingual coverage in 10 key languages to support global business operations Tool Use to automate sophisticated business processes","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-cohere-comandr-plus","date":"2024-04-04 22:46 -05:00","tags":["cohere","llm","comandr","azure","comandrplus"],"summary":"Introducing Command R\u002B: A Scalable LLM Built for Business \u2192 https://txt.cohere.com/command-r-plus-microsoft-azure/ Command R\u002B is a state-of-the-art RAG-optimized model designed to tackle...","keywords":["command","business","microsoft","model","designed","introducing","offer","best","class:","advanced"]},{"id":"bookmarks-llm-time-series-forecasting","title":"Large Language Models Are Zero-Shot Time Series Forecasters","content":"Large Language Models Are Zero-Shot Time Series Forecasters \u2192 https://github.com/ngruver/llmtime By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF. Github Repo","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/llm-time-series-forecasting","date":"2024-04-04 11:07 -05:00","tags":["forecasting","llm","ai","research"],"summary":"Large Language Models Are Zero-Shot Time Series Forecasters \u2192 https://github.com/ngruver/llmtime By encoding time series as a string of numerical digits, we can frame time series forecasting as...","keywords":["series","models","llms","performance","large","text","show","data","numerical","find"]},{"id":"reshare-intel-ipex-llm","title":"IPEX-LLM","content":"IPEX-LLM \u2192 https://github.com/intel-analytics/ipex-llm IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency","contentType":"reshare","url":"https://www.lqdev.me/responses/intel-ipex-llm","date":"2024-04-04 11:05 -05:00","tags":["intel","llm","pytorch","cpu","gpu"],"summary":"IPEX-LLM \u2192 https://github.com/intel-analytics/ipex-llm IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very...","keywords":["ipex","https://github","com/intel","analytics/ipex","pytorch","library","running","intel","local","igpu"]},{"id":"bookmarks-apple-realm-research-paper","title":"ReALM: Reference Resolution As Language Modeling","content":"ReALM: Reference Resolution As Language Modeling \u2192 https://arxiv.org/abs/2403.20329 Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user\u0027s screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of references, with our smallest model obtaining absolute gains of over 5% for on-screen references. We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/apple-realm-research-paper","date":"2024-04-04 11:04 -05:00","tags":["ai","research","apple","gpt"],"summary":"ReALM: Reference Resolution As Language Modeling \u2192 https://arxiv.org/abs/2403.20329 Reference resolution is an important problem, one that is essential to understand and successfully handle context...","keywords":["entities","reference","resolution","screen","context","references","those","extremely","language","types"]},{"id":"reshare-introducing-stable-audio-2-0","title":"Introducing Stable Audio 2.0","content":"Introducing Stable Audio 2.0 \u2192 https://stability.ai/news/stable-audio-2-0 Stable Audio 2.0 sets a new standard in AI-generated audio, producing high-quality, full tracks with coherent musical structure up to three minutes in length at 44.1kHz stereo. The new model introduces audio-to-audio generation by allowing users to upload and transform samples using natural language prompts. Stable Audio 2.0 was exclusively trained on a licensed dataset from the AudioSparx music library, honoring opt-out requests and ensuring fair compensation for creators.","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-stable-audio-2-0","date":"2024-04-04 11:01 -05:00","tags":["stabilityai","ai","audio","nlp"],"summary":"Introducing Stable Audio 2.0 \u2192 https://stability.ai/news/stable-audio-2-0 Stable Audio 2.0 sets a new standard in AI-generated audio, producing high-quality, full tracks with coherent musical...","keywords":["audio","stable","introducing","upload","transform","samples","using","natural","language","prompts"]},{"id":"reply-matrix-five-announced","title":"The Matrix is coming back for a fifth movie","content":"The Matrix is coming back for a fifth movie \u2192 https://www.theverge.com/24120122/the-matrix-sequel-drew-goddard Deadline reports that The Martian writer Drew Goddard has been tapped to pen and direct another Matrix movie executive produced by Lana Wachowski. Currently, the new film has no title or projected premiere date, and there\u2019s been no announcement as to whether franchise stars like Keanu Reeves, Carrie-Anne Moss, Laurence Fishburne, Yahya Abdul-Mateen II, or Jessica Henwick will return. Not sure how to feel about this, but I\u0027ll end up watching anyway.","contentType":"reply","url":"https://www.lqdev.me/responses/matrix-five-announced","date":"2024-04-04 10:56 -05:00","tags":["matrix","movie","scifi","neo","theone"],"summary":"The Matrix is coming back for a fifth movie \u2192 https://www.theverge.com/24120122/the-matrix-sequel-drew-goddard Deadline reports that The Martian writer Drew Goddard has been tapped to pen and direct...","keywords":["matrix","movie","drew","goddard","there\u2019s","announcement","whether","franchise","stars","keanu"]},{"id":"reply-google-podcasts-gone-youtube-cant-replace-it-verge","title":"Google Podcasts is gone \u2014 and so is my faith in Google","content":"Google Podcasts is gone \u2014 and so is my faith in Google \u2192 https://www.theverge.com/2024/4/2/24118873/google-podcasts-shutdown-graveyard Good article. I felt this way when Google Reader and a few other services were shut down. That being said, this is kind of a good thing. Luckily, there are plenty of good podcast apps out there, like Pocket Casts, Overcast, Antennapod, and even Apple Podcasts. This line basically says it all. Podcasts, like blogging, continue to be an open ecosystem and where the saying,\u0026quot;wherever you get your podcasts\u0026quot;, is still going strong.","contentType":"reply","url":"https://www.lqdev.me/responses/google-podcasts-gone-youtube-cant-replace-it-verge","date":"2024-04-02 22:54 -05:00","tags":["google","podcasts","youtube","audio","opensource","rss"],"summary":"Google Podcasts is gone \u2014 and so is my faith in Google \u2192 https://www.theverge.com/2024/4/2/24118873/google-podcasts-shutdown-graveyard Good article. I felt this way when Google Reader and a few other...","keywords":["podcasts","google","good","casts","overcast","antennapod","even","apple","line","basically"]},{"id":"notes-world-wide-whack-tierra-whack-review","title":"Thoughts on World Wide Whack","content":"I saw this come across my feed a few days ago but didn\u0027t get around to listening to it until today. I really enjoyed it. Some of my favorite tracks are: Chanel Pit Imaginary Friends Shower Song","contentType":"notes","url":"https://www.lqdev.me/notes/world-wide-whack-tierra-whack-review","date":"2024-04-02 22:43 -05:00","tags":["music","newrelease","tierrawhack","worldwidewhack"],"summary":"I saw this come across my feed a few days ago but didn\u0027t get around to listening to it until today. I really enjoyed it. Some of my favorite tracks are: Chanel Pit Imaginary Friends Shower Song","keywords":["come","friends","imaginary","chanel","are:","tracks","favorite","enjoyed","shower","really"]},{"id":"notes-post-statistics-april-2024","title":"Website Post Statistics - April 2024","content":"My website stats script seems to be running as expected. Some highlights from last month: With 33 notes posted in the microblog, I\u0027m about 80% of the way to the number of total notes posted last year. These monthly summary posts, as well as my Crate Finds and Weekly Summary posts have somewhat contributed to this number but I like the pace I\u0027m setting. Not even halfway through the year and it looks promising. While most of my responses are reshares (93), there\u0027s still a broader distribution across bookmarks and other type of reponses posts. AI continue to be the main topic I share posts around. The Verge is still one of the main sources I reshare. Personally, the redesign has contributed to the way I consume content from The Verge but also, the topics covered align with many of the things I\u0027m interested in. I have yet to make progress on the long-form posts, but there are some experiments I\u0027ve been tinkering with that will eventually make it on there.","contentType":"notes","url":"https://www.lqdev.me/notes/post-statistics-april-2024","date":"2024-04-01 23:20 -05:00","tags":["githubactions","cicd","blogging","stats","github"],"summary":"My website stats script seems to be running as expected. Some highlights from last month: With 33 notes posted in the microblog, I\u0027m about 80% of the way to the number of total notes posted last...","keywords":["posts","summary","contributed","year","number","posted","verge","last","notes","main"]},{"id":"notes-crate-finds-march-2024","title":"Crate Finds - March 2024","content":"Similar to February, I didn\u0027t spend too much time listening this month so the list is somewhat short. I got a chance to listen to a few songs off Chicano Batman\u0027s new album, Noteboook Fantasy though. Here\u0027s the drop for March 2024: Tracks Mi Sol by Los Tranquilos Album: Todo Tranquis Duration: 3:11 Listen on YouTube Backup: Listen on Spotify Brillantes Con Carb\u00F3n by Trio Los Jaibos Album: Trio Los Jaibos Duration: 2:15 Listen on YouTube Backup: Listen on Spotify Legendary - Chopped Not Slopped by DJ Drama, Tyler, The Creator Album: I\u0027M REALLY LIKE THAT (Chopped Not Slopped) Duration: 3:15 Listen on YouTube Backup: Listen on Spotify Natascha - Main Theme by Peter Thomas Sound Orchester Album: Van De Velde: Die Vollkommene Ehe \u0026amp; Das Leben Zu Zweit Duration: 2:02 Listen on YouTube Backup: Listen on Spotify A Faithful Spirit by Polyphonic Music Library Album: A Faithful Spirit Duration: 1:45 Listen on YouTube Backup: Listen on Spotify Quit Your Job, Runaway! by Junior Mesa Album: Quit Your Job, Runaway! Duration: 2:46 Listen on YouTube Backup: Listen on Spotify A Whole Pot Of Jelly (For A Little Slice Of Toast) by Pete Wingfield Album: Breakfast Special Duration: 2:59 Listen on YouTube Backup: Listen on Spotify Heroine by Azamiah Album: In Phases Duration: 5:02 Listen on YouTube Backup: Listen on Spotify Y o Y by Born At Midnite Album: Y o Y Duration: 2:23 Listen on YouTube Backup: Listen on Spotify Minute by Paul Cherry Album: Flavour Duration: 2:24 Listen on YouTube Backup: Listen on Spotify Mermaid by Alan Hawkshaw, Brian Bennett Album: Kpm 1000 Series: Synthesis Duration: 3:56 Listen on YouTube Backup: Listen on Spotify Gonna Make Time by The Frightnrs Album: Nothing More to Say Duration: 3:34 Listen on YouTube Backup: Listen on Spotify Nunca by Y La Bamba Album: Lucha Duration: 4:22 Listen on YouTube Backup: Listen on Spotify Ropa Vieja by Family Time Album: The Golden Years Duration: 2:23 Listen on YouTube Backup: Listen on Spotify Pon P\u00F3n by Khruangbin Album: Pon P\u00F3n Duration: 2:58 Listen on YouTube Backup: Listen on Spotify Sinseerly Yours by Thee Sinseers, Joey Quinones Album: Sinseerly Yours Duration: 2:53 Listen on YouTube Backup: Listen on Spotify Facing East by 79.5 Album: Predictions Duration: 3:50 Listen on YouTube Backup: Listen on Spotify Mexican Dream by Piero Piccioni Album: Colpo Rovente - Red Hot Shot (Original Motion Picture Soundtrack) Duration: 2:01 Listen on YouTube Backup: Listen on Spotify Um Rosto de Mulher by Garoto Album: Projeto Memoria Brasileira : Viva Garoto Duration: 1:02 Listen on YouTube Backup: Listen on Spotify Floating Features by La Luz Album: Floating Features Duration: 2:13 Listen on YouTube Backup: Listen on Spotify If I Got It (Your Love Brought It) by Aaron Frazer Album: Introducing... Duration: 2:54 Listen on YouTube Backup: Listen on Spotify Got Caught in Amsterdam by Arc De Soleil Album: The Thief in Marrakesh Got Caught in Amsterdam Trying to Escape Duration: 2:50 Listen on YouTube Backup: Listen on Spotify The Tingling Tartness of Sprite - Instrumental by Raymond Scott Album: The Jingle Workshop: Midcentury Musical Miniatures 1951-1965 Duration: 1:02 Listen on YouTube Backup: Listen on Spotify French Waltz by Leon Ware Album: Musical Massage (Expanded Edition) Duration: 1:51 Listen on YouTube Backup: Listen on Spotify Forever Yours by Brent Faiyaz Album: Larger Than Life Duration: 1:36 Listen on YouTube Backup: Listen on Spotify Ripple - 2013 Remaster by Grateful Dead Album: American Beauty (2013 Remaster) Duration: 4:09 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-march-2024","date":"2024-04-01 23:17 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"Similar to February, I didn\u0027t spend too much time listening this month so the list is somewhat short. I got a chance to listen to a few songs off Chicano Batman\u0027s new album, Noteboook Fantasy though....","keywords":["listen","spotify","youtube","duration:","backup:","album:","yours","caught","amsterdam","garoto"]},{"id":"reshare-start-using-chatgpt-seamlessly-no-signup","title":"Start using ChatGPT instantly","content":"Start using ChatGPT instantly \u2192 https://openai.com/blog/start-using-chatgpt-instantly We\u2019re making it easier for people to experience the benefits of AI without needing to sign up. We may use what you provide to ChatGPT to improve our models for everyone. If you\u2019d like, you can turn this off through your Settings - whether you create an account or not. We\u2019ve also introduced additional content safeguards for this experience, such as blocking prompts and generations in a wider range of categories.","contentType":"reshare","url":"https://www.lqdev.me/responses/start-using-chatgpt-seamlessly-no-signup","date":"2024-04-01 20:44 -05:00","tags":["openai","chatgpt","ai","llm"],"summary":"Start using ChatGPT instantly \u2192 https://openai.com/blog/start-using-chatgpt-instantly We\u2019re making it easier for people to experience the benefits of AI without needing to sign up. We may use what...","keywords":["chatgpt","using","instantly","experience","start","create","account","we\u2019ve","also","introduced"]},{"id":"reshare-tmobile-sidekick-jump-button-mobile-productivity","title":"T-Mobile Sidekick\u2019s Jump button made mobile multitasking easy","content":"T-Mobile Sidekick\u2019s Jump button made mobile multitasking easy \u2192 https://www.theverge.com/24115039/danger-hiptop-t-mobile-sidekick-jump-button Bring back the Sidekick! Ayaneo Slide is probably the closest to this today. Would love to see a smaller version of it running on Windows on ARM-based Snapdragon processors . Before the iPhone, before Android, before webOS, a revolutionary soap bar of a phone made it incredibly easy to get shit done. The Danger Hiptop, better known as the T-Mobile Sidekick, made the internet portable and affordable like no phone before.","contentType":"reshare","url":"https://www.lqdev.me/responses/tmobile-sidekick-jump-button-mobile-productivity","date":"2024-04-01 20:27 -05:00","tags":["tmobile","nostalgia","sidekick","tmobile","phones","smartphones","productivity","mobile","cellphones","handhelds"],"summary":"T-Mobile Sidekick\u2019s Jump button made mobile multitasking easy \u2192 https://www.theverge.com/24115039/danger-hiptop-t-mobile-sidekick-jump-button Bring back the Sidekick! Ayaneo Slide is probably the...","keywords":["mobile","made","sidekick","button","easy","hiptop","jump","phone","processors","iphone"]},{"id":"reshare-announcing-dbrx-llm","title":"Announcing DBRX: A new standard for efficient open source LLMs","content":"Announcing DBRX: A new standard for efficient open source LLMs \u2192 https://www.databricks.com/blog/announcing-dbrx-new-standard-efficient-open-source-customizable-llms Today, we are excited to advance our mission by open sourcing DBRX, a general purpose large language model (LLM) built by our Mosaic Research team that outperforms all established open source models on standard benchmarks. We believe that pushing the boundary of open source models enables generative AI for all enterprises that is customizable and transparent. We are excited about DBRX for three distinct reasons. First, it handily beats open source models, such as, LLaMA2-70B, Mixtral, and Grok-1 on language understanding, programming, math, and logic... Second, DBRX beats GPT-3.5 on most benchmarks... Third, DBRX is a Mixture-of-Experts (MoE) model built on the MegaBlocks research and open source project, making the model extremely fast in terms of tokens/second.","contentType":"reshare","url":"https://www.lqdev.me/responses/announcing-dbrx-llm","date":"2024-03-31 20:26 -05:00","tags":["databricks","ai","llm","opensource"],"summary":"Announcing DBRX: A new standard for efficient open source LLMs \u2192 https://www.databricks.com/blog/announcing-dbrx-new-standard-efficient-open-source-customizable-llms Today, we are excited to advance...","keywords":["open","source","dbrx","standard","models","model","benchmarks","language","research","excited"]},{"id":"notes-notebook-fantasy-chicano-batman-released","title":"New Album - Notebook Fantasy from Chicano Batman","content":"I have to admit, I wasn\u0027t the biggest fan of the singles released before the album. Though I have to say, there\u0027s been a few times Era Primavera has been stuck in my head. Very catchy. I haven\u0027t listened to the whole thing, but so far I really like the tracks Notebook Fantasy, Parallels, and Beautiful Daughter.","contentType":"notes","url":"https://www.lqdev.me/notes/notebook-fantasy-chicano-batman-released","date":"2024-03-31 20:19 -05:00","tags":["music","chicanobatman","newrelease"],"summary":"I have to admit, I wasn\u0027t the biggest fan of the singles released before the album. Though I have to say, there\u0027s been a few times Era Primavera has been stuck in my head. Very catchy. I haven\u0027t...","keywords":["admit","parallels","fantasy","notebook","tracks","really","thing","whole","listened","haven"]},{"id":"notes-2024-04-01-weekly-post-summary","title":"Week of April 1, 2024 - Post Summary","content":"Notes New Album - Notebook Fantasy from Chicano Batman Responses Announcing DBRX: A new standard for efficient open source LLMs","contentType":"notes","url":"https://www.lqdev.me/notes/2024-04-01-weekly-post-summary","date":"2024-03-31 19:30 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes New Album - Notebook Fantasy from Chicano Batman Responses Announcing DBRX: A new standard for efficient open source LLMs","keywords":["notes","album","notebook","fantasy","from","chicano","batman","responses","announcing","dbrx:"]},{"id":"reshare-one-step-diffusion-distribution-matching-distillation","title":"One-step Diffusion with Distribution Matching Distillation","content":"One-step Diffusion with Distribution Matching Distillation \u2192 https://tianweiy.github.io/dmd/ Our one-step generator achieves comparable image quality with StableDiffusion v1.5 while being 30x faster. Diffusion models are known to approximate the score function of the distribution they are trained on. In other words, an unrealistic synthetic image can be directed toward higher probability density region through the denoising process (see SDS). Our core idea is training two diffusion models to estimate not only the score function of the target real distribution, but also that of the fake distribution. We construct a gradient update to our generator as the difference between the two scores, essentially nudging the generated images toward higher realism as well as lower fakeness (see VSD). Our method is similar to GANs in that a critic is jointly trained with the generator to minimize a divergence between the real and fake distributions, but differs in that our training does not play an adversarial game that may cause training instability, and our critic can fully leverage the weights of a pretrained diffusion model. Combined with a simple regression loss to match the output of the multi-step diffusion model, our method outperforms all published few-step diffusion approaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shot COCO-30k, comparable to Stable Diffusion but orders of magnitude faster. Utilizing FP16 inference, our model generates images at 20 FPS on modern hardware. Paper","contentType":"reshare","url":"https://www.lqdev.me/responses/one-step-diffusion-distribution-matching-distillation","date":"2024-03-26 21:44 -05:00","tags":["ai","genai","diffusion","dmd","stablediffusion","dalle"],"summary":"One-step Diffusion with Distribution Matching Distillation \u2192 https://tianweiy.github.io/dmd/ Our one-step generator achieves comparable image quality with StableDiffusion v1.5 while being 30x faster....","keywords":["diffusion","step","distribution","model","generator","training","method","images","fake","real"]},{"id":"star-404-media-full-text-rss","title":"404 Media Now Has a Full Text RSS Feed","content":"404 Media Now Has a Full Text RSS Feed \u2192 https://www.404media.co/404-media-now-has-a-full-text-rss-feed/ We paid for the development of full text RSS feeds for Ghost-based publishers. Now we can offer them to our paid subscribers, and other Ghost sites can use the service too. Our friends Anil Dash and Ernie Smith have recently written passionately and persuasively about the importance of RSS to the open web, and about how a technology that turns 25 years old this month remains both subversive and quite versatile. RSS-based distribution underpins a podcasting ecosystem that has allowed for shows to be distributed not just on Apple Podcasts but on Spotify, Google Podcasts, Pocket Casts, Overcast, and whatever other podcast player you might want to listen on. \u201CBeing able to say, \u2018wherever you get your podcasts\u2019 is a radical statement,\u201D Dash wrote. \u201CBecause what it represents is the triumph of exactly the kind of technology that\u0027s supposed to be impossible: open, empowering tech that\u0027s not owned by any one company, that can\u0027t be controlled by any one company, and that allows people to have ownership over their work and their relationship with their audience.\u201D RSS has empowered podcasters, but that it needs a \u201Ccreator economy rethink\u201D for text.","contentType":"star","url":"https://www.lqdev.me/responses/404-media-full-text-rss","date":"2024-03-26 21:16 -05:00","tags":["rss","404media","ghost","404","media","protocols","opensource","technology"],"summary":"404 Media Now Has a Full Text RSS Feed \u2192 https://www.404media.co/404-media-now-has-a-full-text-rss-feed/ We paid for the development of full text RSS feeds for Ghost-based publishers. Now we can...","keywords":["text","full","media","technology","open","dash","company","based","ghost","podcasts"]},{"id":"notes-introducing-100-sways-project","title":"Introducing the 100 Sways Project","content":"Introducing the 100 Sways project. Over the next few months, I plan on creating artifacts like blog posts, newsletters, and websites using Microsoft Sway . Bookmark the site 100sways.lqdev.tech and follow along! Note: If you can\u0027t access the website, you can use the direct link (https://sway.cloud.microsoft/aC19McbR3xcNo6s8) . What is the 100 Sways project? This project is an attempt to show how you might use Sway as a publishing platform for content like newsletters, blog posts, and websites. This project was inspired by James\u0027 100 things you can do on your personal website post . Motivations Learn more about Microsoft Sway Show how you can make the most use out of apps and services already included with your Microsoft Account Show how easy it is to create and consume web content using Sway","contentType":"notes","url":"https://www.lqdev.me/notes/introducing-100-sways-project","date":"2024-03-24 16:33 -05:00","tags":["microsoft","sway","indieweb","blog","website","microblog","blogging","content","m365","office","internet","web","microsoftoffice","microsoft365"],"summary":"Introducing the 100 Sways project. Over the next few months, I plan on creating artifacts like blog posts, newsletters, and websites using Microsoft Sway . Bookmark the site 100sways.lqdev.tech and...","keywords":["sway","project","show","microsoft","using","websites","newsletters","blog","posts","website"]},{"id":"notes-2024-03-24-weekly-post-summary","title":"Week of March 24, 2024 - Post Summary","content":"Notes New Forums Page Week of March 18, 2024 - Post Summary New Era of Work - Windows / Surface Event Blog (March 21, 2024) Looking for Discord server / community recommendations Use AI to generate a blogroll others can subscribe to Responses Building Meta\u2019s GenAI Infrastructure Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators Diffusion models from scratch, from a new theoretical perspective Spreadsheets are all you need Introducing Stable Video 3D: Quality Novel View Synthesis and 3D Generation from Single Images Mamba: Linear-Time Sequence Modeling with Selective State Spaces Nvidia reveals Blackwell B200 GPU Demystifying Embedding Spaces using Large Language Models Quanto: a PyTorch quantization toolkit LaVague - Large Action Model framework Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs Algorithms for Modern Hardware Ollama now supports AMD graphics cards Diffusion Models From Scratch Machine Learning for Games Course - HuggingFace Stability CEO Resigns The Tokenizer Playground Releasing Common Corpus: the largest public domain dataset for training LLMs Grok-1 OpenAI Transformer Debugger The Online Local Chronicle MM1: Methods, Analysis \u0026amp; Insights from Multimodal LLM Pre-training","contentType":"notes","url":"https://www.lqdev.me/notes/2024-03-24-weekly-post-summary","date":"2024-03-24 13:05 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes New Forums Page Week of March 18, 2024 - Post Summary New Era of Work - Windows / Surface Event Blog (March 21, 2024) Looking for Discord server / community recommendations Use AI to generate a...","keywords":["from","models","large","spaces","march","2024","algorithms","scratch","diffusion","training"]},{"id":"notes-new-forums-page","title":"New Forums Page","content":"Following the patterns of a blogroll and podroll, I decided to post a forums page to list various forums and communities I find interesting. I\u0027ve also created an OPML file for it so you can subscribe to them in your RSS reader of choice. Discord is good for communities. For example on servers like Shudder there\u0027s watch parties and HuggingFace reading club gets together to discuss some of the latest papers. For events, this works really well because everyone can participate in real-time. However, I find that outside of events or general chat, it\u0027s difficult to follow the message stream. I like the forum format with topics / threads much better. If you have any forum recommendations, feel free to send them my way .","contentType":"notes","url":"https://www.lqdev.me/notes/new-forums-page","date":"2024-03-23 12:38 -05:00","tags":["forums","community","rss","web","openweb","internet","indieweb"],"summary":"Following the patterns of a blogroll and podroll, I decided to post a forums page to list various forums and communities I find interesting. I\u0027ve also created an OPML file for it so you can subscribe...","keywords":["events","forums","forum","communities","find","following","latest","papers","works","really"]},{"id":"reshare-stability-ai-ceo-resigns","title":"Stability CEO Resigns","content":"Stability CEO Resigns \u2192 https://stability.ai/news/stabilityai-announcement Earlier today, Emad Mostaque resigned from his role as CEO of Stability AI and from his position on the Board of Directors of the company to pursue decentralized AI.","contentType":"reshare","url":"https://www.lqdev.me/responses/stability-ai-ceo-resigns","date":"2024-03-23 11:49 -05:00","tags":["stability","ai","news"],"summary":"Stability CEO Resigns \u2192 https://stability.ai/news/stabilityai-announcement Earlier today, Emad Mostaque resigned from his role as CEO of Stability AI and from his position on the Board of Directors...","keywords":["stability","from","company","directors","board","position","role","resigned","mostaque","emad"]},{"id":"notes-ai-generated-blogroll-opml","title":"Use AI to generate a blogroll others can subscribe to","content":"I\u0027ve seen several posts about blogrolls recently across my feed. My own needs some updating, but you can find it here . I also created one for podcasts which I call a podroll. I\u0027ve also seen posts about making the indie web easier. With both of those in mind, that\u0027s how this post came to be. Let\u0027s say that you have a blogroll on your website which might just be a list of links to the respective RSS feeds. If someone wanted to subscribe to the feeds on your blogroll, they could just copy the links and add them to their RSS reader. However, as your blogroll grows, doing this in bulk can be tedious. One way you can make it easier is using OPML files. Many RSS readers support importing feeds using OPML files. You could go through the process of figuring out what OPML is, how you need to format the file, and then do the work of creating and populating the file. Or...you could use AI. In this example, I opened up Copilot, provided it a list of links to RSS feeds and asked it to generate an OPML file. You can take the outputs generated using AI, save them to a file, and add it to your website. Now people who want to subscribe to the feeds on your blogroll can just load the OPML file into their RSS reader and all the feeds will be automatically added.","contentType":"notes","url":"https://www.lqdev.me/notes/ai-generated-blogroll-opml","date":"2024-03-21 22:01 -05:00","tags":["blogroll","indieweb","ai","copilot","rss","openweb"],"summary":"I\u0027ve seen several posts about blogrolls recently across my feed. My own needs some updating, but you can find it here . I also created one for podcasts which I call a podroll. I\u0027ve also seen posts...","keywords":["feeds","opml","file","blogroll","links","using","seen","reader","files","subscribe"]},{"id":"reshare-mamba-linear-time-sequence-selective-state-spaces","title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces","content":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces \u2192 https://arxiv.org/abs/2312.00752 Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers\u0027 computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\u00D7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.","contentType":"reshare","url":"https://www.lqdev.me/responses/mamba-linear-time-sequence-selective-state-spaces","date":"2024-03-21 20:44 -05:00","tags":["ai","models","mamba","llm","compute"],"summary":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces \u2192 https://arxiv.org/abs/2312.00752 Foundation models, now powering most of the exciting applications in deep learning, are almost...","keywords":["attention","sequence","transformers","mamba","models","length","language","modalities","state","model"]},{"id":"notes-surface-event-mar-21-24-event-blog","title":"New Era of Work - Windows / Surface Event Blog (March 21, 2024)","content":"12:03 Copilot is making people more productive. 12:06 Copilot for Microsoft 365 integrated into Windows. Demo: After coming back from vacation, you want to catch up. Ask copilot about latest project updates Ask copilot for summary of meeting you missed which generates a summary of the discussion and action items. Copilot then can help you tackle the action items like drafting and sending e-mails. 12:09 Demo: Using Copilot to configure Windows settings 12:13 Demo: Using Windows App to manage Windows 365 cloud PCs 12:21 Surface Pro 10 and Surface Laptop 6 announced for business. Comes with latest Intel processors, Neural Processing Units (NPUs), and Copilot. 12:39 The demo showing a .NET development workflow on the Surface Laptop 6 was great. Usually that same workflow longer on my Lenovo P4.","contentType":"notes","url":"https://www.lqdev.me/notes/surface-event-mar-21-24-event-blog","date":"2024-03-21 12:02 -05:00","tags":["windows","microsoft","surface","copilot","ai","pc","hardware"],"summary":"12:03 Copilot is making people more productive. 12:06 Copilot for Microsoft 365 integrated into Windows. Demo: After coming back from vacation, you want to catch up. Ask copilot about latest project...","keywords":["copilot","windows","surface","demo:","using","laptop","latest","action","summary","workflow"]},{"id":"bookmarks-common-corpus-llm-training-dataset","title":"Releasing Common Corpus: the largest public domain dataset for training LLMs","content":"Releasing Common Corpus: the largest public domain dataset for training LLMs \u2192 https://huggingface.co/blog/Pclanglais/common-corpus We announce today the release of Common Corpus on HuggingFace: Common Corpus is the largest public domain dataset released for training LLMs. Common Corpus includes 500 billion words from a wide diversity of cultural heritage initiatives. Common Corpus is multilingual and the largest corpus to date in English, French, Dutch, Spanish, German and Italian. Common Corpus shows it is possible to train fully open LLMs on sources without copyright concerns.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/common-corpus-llm-training-dataset","date":"2024-03-20 22:12 -05:00","tags":["ai","data","llm","huggingface","nlp"],"summary":"Releasing Common Corpus: the largest public domain dataset for training LLMs \u2192 https://huggingface.co/blog/Pclanglais/common-corpus We announce today the release of Common Corpus on HuggingFace:...","keywords":["corpus","common","largest","llms","public","domain","dataset","training","date","english"]},{"id":"notes-looking-for-discord-communities","title":"Looking for Discord server / community recommendations","content":"I finally decided to dust off my Discord. So far, I\u0027ve joined several communities like: HuggingFace LangChain LlamaIndex Racket Virtual ML.NET Shudder ClubTwit ...and many others I\u0027ve noticed that server / community discovery needs improvement, so I\u0027m asking for your recommendations! General topics I\u0027m interested in: AI Machine Learning Open Source Software Development .NET F# Open Web IndieWeb Fediverse Lisp Tech Personal Finance Podcasts Books Travel Retro Tech Retro Gaming E-mail me your recommendations at contact@lqdev.me or send me a Webmention! Thanks in advance.","contentType":"notes","url":"https://www.lqdev.me/notes/looking-for-discord-communities","date":"2024-03-19 22:53 -05:00","tags":["discord","community","recommendations","lazyweb"],"summary":"I finally decided to dust off my Discord. So far, I\u0027ve joined several communities like: HuggingFace LangChain LlamaIndex Racket Virtual ML.NET Shudder ClubTwit ...and many others I\u0027ve noticed that...","keywords":["tech","open","retro","recommendations","mail","interested","machine","learning","webmention","source"]},{"id":"bookmarks-machine-learning-for-games-course-huggingface","title":"Machine Learning for Games Course - HuggingFace","content":"Machine Learning for Games Course - HuggingFace \u2192 https://huggingface.co/learn/ml-games-course/unit0/introduction Welcome to the course that will teach you the most fascinating topic in game development: how to use powerful AI tools and models to create unique game experiences. New AI models are revolutionizing the Game Industry in two impactful ways: On how we make games: Generate textures using AI Using AI voice actors for the voices. How we create gameplay: Crafting smart Non-Playable Characters (NPCs) using large language models. This course will teach you: How to integrate AI models for innovative gameplay, featuring intelligent NPCs. How to use AI tools to help your game development pipeline.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/machine-learning-for-games-course-huggingface","date":"2024-03-19 22:35 -05:00","tags":["machinelearning","games","huggingface","course","ai","ml"],"summary":"Machine Learning for Games Course - HuggingFace \u2192 https://huggingface.co/learn/ml-games-course/unit0/introduction Welcome to the course that will teach you the most fascinating topic in game...","keywords":["game","models","using","course","games","teach","create","npcs","tools","actors"]},{"id":"reply-online-local-chronicle-searls","title":"The Online Local Chronicle","content":"The Online Local Chronicle \u2192 https://doc.searls.com/2024/03/19/the-online-local-chronicle/ In the same way that every little place in America used to have a printed newspaper, every little place in America could have an online local chronicle. Broadly speaking, an online local chronicle is a collection of facts organized mostly in chronological order. The \u201Cpages\u201D of the chronicle can be thought of as subsets of a community\u2019s universal timeline of events. These online local chronicles could become the backbone of local news operations. Nice project. Unfortunately, it\u0027s rare you get local news. I like publications / websites like Hoboken Girl and Block Club Chicago . I wish there were more of them in more cities and towns. I know they\u0027re there in some forms like Facebook Groups. Even better, it\u0027d be great to have the websites for these publications be the main source of truth that then syndicated their content to the various platforms out there.","contentType":"reply","url":"https://www.lqdev.me/responses/online-local-chronicle-searls","date":"2024-03-19 22:17 -05:00","tags":["community","news","local","knowledgebase","information","web","opensource"],"summary":"The Online Local Chronicle \u2192 https://doc.searls.com/2024/03/19/the-online-local-chronicle/ In the same way that every little place in America used to have a printed newspaper, every little place in...","keywords":["local","online","chronicle","news","america","little","websites","place","publications","more"]},{"id":"reshare-quanto-pytorch-quantization-toolkit","title":"Quanto: a PyTorch quantization toolkit ","content":"Quanto: a PyTorch quantization toolkit \u2192 https://huggingface.co/blog/quanto-introduction Quantization is a technique to reduce the computational and memory costs of evaluating Deep Learning Models by representing their weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32). Today, we are excited to introduce quanto, a versatile pytorch quantization toolkit, that provides several unique features: available in eager mode (works with non-traceable models) quantized models can be placed on any device (including CUDA and MPS), automatically inserts quantization and dequantization stubs, automatically inserts quantized functional operations, automatically inserts quantized modules (see below the list of supported modules), provides a seamless workflow for a float model, going from a dynamic to a static quantized model, supports quantized model serialization as a state_dict, supports not only int8 weights, but also int2 and int4, supports not only int8 activations, but also float8.","contentType":"reshare","url":"https://www.lqdev.me/responses/quanto-pytorch-quantization-toolkit","date":"2024-03-19 22:00 -05:00","tags":["huggingface","quantization","pytorch","tools","ai"],"summary":"Quanto: a PyTorch quantization toolkit \u2192 https://huggingface.co/blog/quanto-introduction Quantization is a technique to reduce the computational and memory costs of evaluating Deep Learning Models by...","keywords":["quantized","quantization","int8","models","model","inserts","automatically","supports","modules","provides"]},{"id":"reshare-demistifying-embedding-spaces-llms","title":"Demystifying Embedding Spaces using Large Language Models","content":"Demystifying Embedding Spaces using Large Language Models \u2192 https://arxiv.org/abs/2310.04475 Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing Large Language Models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decoding user preferences in recommender systems. Our work couples the immense information potential of embeddings with the interpretative power of LLMs.","contentType":"reshare","url":"https://www.lqdev.me/responses/demistifying-embedding-spaces-llms","date":"2024-03-19 21:39 -05:00","tags":["ai","llm","embedding","interpretability"],"summary":"Demystifying Embedding Spaces using Large Language Models \u2192 https://arxiv.org/abs/2310.04475 Embeddings have become a pivotal means to represent complex, multi-faceted information about entities,...","keywords":["embeddings","llms","vectors","tasks","useful","entities","information","complex","interpretation","large"]},{"id":"reshare-tokenizer-playground","title":"The Tokenizer Playground","content":"The Tokenizer Playground \u2192 https://huggingface.co/spaces/Xenova/the-tokenizer-playground Experiment with different tokenizers (running locally in your browser). I really love doing this thing","contentType":"reshare","url":"https://www.lqdev.me/responses/tokenizer-playground","date":"2024-03-18 22:35 -05:00","tags":["ai","tokenizer","huggingface","playground","aiplayground"],"summary":"The Tokenizer Playground \u2192 https://huggingface.co/spaces/Xenova/the-tokenizer-playground Experiment with different tokenizers (running locally in your browser). I really love doing this thing","keywords":["tokenizer","playground","https://huggingface","co/spaces/xenova/the","experiment","different","tokenizers","running","locally","browser"]},{"id":"reshare-introducing-stable-video-3d","title":"Introducing Stable Video 3D: Quality Novel View Synthesis and 3D Generation from Single Images","content":"Introducing Stable Video 3D: Quality Novel View Synthesis and 3D Generation from Single Images \u2192 https://stability.ai/news/introducing-stable-video-3d Today we are releasing Stable Video 3D (SV3D), a generative model based on Stable Video Diffusion, advancing the field of 3D technology and delivering greatly improved quality and view-consistency. This release features two variants: SV3D_u and SV3D_p. SV3D_u generates orbital videos based on single image inputs without camera conditioning. SV3D_p extends the capability by accommodating both single images and orbital views, allowing for the creation of 3D video along specified camera paths. Stable Video 3D can be used now for commercial purposes with a Stability AI Membership. For non-commercial use, you can download the model weights on Hugging Face and view our research paper here.","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-stable-video-3d","date":"2024-03-18 21:29 -05:00","tags":["stabilityai","ai","video","3d","computervision"],"summary":"Introducing Stable Video 3D: Quality Novel View Synthesis and 3D Generation from Single Images \u2192 https://stability.ai/news/introducing-stable-video-3d Today we are releasing Stable Video 3D (SV3D), a...","keywords":["video","stable","sv3d","view","single","commercial","based","model","camera","images"]},{"id":"reshare-nvidia-blackwell-b200-gpu","title":"Nvidia reveals Blackwell B200 GPU","content":"Nvidia reveals Blackwell B200 GPU \u2192 https://www.theverge.com/2024/3/18/24105157/nvidia-blackwell-gpu-b200-ai Nvidia reveals Blackwell B200 GPU, the \u2018world\u2019s most powerful chip\u2019 for AI \u2018Built to democratize trillion-parameter AI.\u2019 Nvidia says the new B200 GPU offers up to 20 petaflops of FP4 horsepower from its 208 billion transistors. Also, it says, a GB200 that combines two of those GPUs with a single Grace CPU can offer 30 times the performance for LLM inference workloads while also potentially being substantially more efficient. It \u201Creduces cost and energy consumption by up to 25x\u201D over an H100, says Nvidia.","contentType":"reshare","url":"https://www.lqdev.me/responses/nvidia-blackwell-b200-gpu","date":"2024-03-18 21:09 -05:00","tags":["ai","nvidia","gpu","hardware","pchardware","pc"],"summary":"Nvidia reveals Blackwell B200 GPU \u2192 https://www.theverge.com/2024/3/18/24105157/nvidia-blackwell-gpu-b200-ai Nvidia reveals Blackwell B200 GPU, the \u2018world\u2019s most powerful chip\u2019 for AI \u2018Built to...","keywords":["nvidia","b200","says","blackwell","also","reveals","grace","offer","times","performance"]},{"id":"bookmarks-mm1-multimodal-llm-pretraining","title":"MM1: Methods, Analysis \u0026 Insights from Multimodal LLM Pre-training","content":"MM1: Methods, Analysis \u0026amp; Insights from Multimodal LLM Pre-training \u2192 https://arxiv.org/abs/2403.09611 In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, consisting of both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/mm1-multimodal-llm-pretraining","date":"2024-03-18 21:02 -05:00","tags":["ai","mm1","llm","multimodal","apple"],"summary":"MM1: Methods, Analysis \u0026amp; Insights from Multimodal LLM Pre-training \u2192 https://arxiv.org/abs/2403.09611 In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In...","keywords":["image","training","multimodal","models","language","large","data","choices","careful","encoder"]},{"id":"reshare-enhace-rag-application-accuracy-knowledge-graphs","title":"Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs","content":"Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs \u2192 https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/ A practical guide to constructing and retrieving information from knowledge graphs in RAG applications with Neo4j and LangChain Graph retrieval augmented generation (Graph RAG) is gaining momentum and emerging as a powerful addition to traditional vector search retrieval methods. This approach leverages the structured nature of graph databases, which organize data as nodes and relationships, to enhance the depth and contextuality of retrieved information.","contentType":"reshare","url":"https://www.lqdev.me/responses/enhace-rag-application-accuracy-knowledge-graphs","date":"2024-03-17 21:29 -05:00","tags":["knowledgegraph","rag","genai","langchain","generativeai","retrievalaugmentedgeneration","patterns","applicationpatterns"],"summary":"Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs \u2192 https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge...","keywords":["graph","constructing","knowledge","based","accuracy","leveraging","graphs","information","langchain","retrieval"]},{"id":"reshare-lavague-large-action-model","title":"LaVague - Large Action Model framework","content":"LaVague - Large Action Model framework \u2192 https://github.com/lavague-ai/LaVague Redefining internet surfing by transforming natural language instructions into seamless browser interactions.","contentType":"reshare","url":"https://www.lqdev.me/responses/lavague-large-action-model","date":"2024-03-17 21:22 -05:00","tags":["ai","genai","lavague","selenium","web","lam","largeactionmodel"],"summary":"LaVague - Large Action Model framework \u2192 https://github.com/lavague-ai/LaVague Redefining internet surfing by transforming natural language instructions into seamless browser interactions.","keywords":["lavague","seamless","instructions","language","natural","transforming","surfing","internet","redefining","ai/lavague"]},{"id":"bookmarks-algorithms-modern-hardware-book","title":"Algorithms for Modern Hardware","content":"Algorithms for Modern Hardware \u2192 https://en.algorithmica.org/hpc/ This is an upcoming high performance computing book titled \u201CAlgorithms for Modern Hardware\u201D by Sergey Slotin. Its intended audience is everyone from performance engineers and practical algorithm researchers to undergraduate computer science students who have just finished an advanced algorithms course and want to learn more practical ways to speed up a program than by going from O(nlog\u2061n)O(nlogn) to O(nlog\u2061log\u2061n)O(nloglogn).","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/algorithms-modern-hardware-book","date":"2024-03-17 21:20 -05:00","tags":["book","algorithms","hardware"],"summary":"Algorithms for Modern Hardware \u2192 https://en.algorithmica.org/hpc/ This is an upcoming high performance computing book titled \u201CAlgorithms for Modern Hardware\u201D by Sergey Slotin. Its intended audience...","keywords":["algorithms","from","performance","practical","modern","going","undergraduate","computer","science","students"]},{"id":"reshare-spreadsheets-are-all-you-need","title":"Spreadsheets are all you need","content":"Spreadsheets are all you need \u2192 https://spreadsheets-are-all-you-need.ai/index.html A low-code way to learn AI - Learn how AI works from a real LLM implemented entirely in Excel","contentType":"reshare","url":"https://www.lqdev.me/responses/spreadsheets-are-all-you-need","date":"2024-03-17 21:18 -05:00","tags":["excel","tutorial","ai","genai","tokenizers","transformers"],"summary":"Spreadsheets are all you need \u2192 https://spreadsheets-are-all-you-need.ai/index.html A low-code way to learn AI - Learn how AI works from a real LLM implemented entirely in Excel","keywords":["need","learn","spreadsheets","https://spreadsheets","ai/index","html","code","works","from","real"]},{"id":"reshare-building-meta-genai-infrastructure","title":"Building Meta\u2019s GenAI Infrastructure","content":"Building Meta\u2019s GenAI Infrastructure \u2192 https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/ Marking a major investment in Meta\u2019s AI future, we are announcing two 24k GPU clusters. We are sharing details on the hardware, network, storage, design, performance, and software that help us extract high throughput and reliability for various AI workloads. We use this cluster design for Llama 3 training. We are strongly committed to open compute and open source. We built these clusters on top of Grand Teton, OpenRack, and PyTorch and continue to push open innovation across the industry. This announcement is one step in our ambitious infrastructure roadmap. By the end of 2024, we\u2019re aiming to continue to grow our infrastructure build-out that will include 350,000 NVIDIA H100 GPUs as part of a portfolio that will feature compute power equivalent to nearly 600,000 H100s.","contentType":"reshare","url":"https://www.lqdev.me/responses/building-meta-genai-infrastructure","date":"2024-03-17 21:14 -05:00","tags":["meta","ai","infrastraucture","generativeai","genai","facebook"],"summary":"Building Meta\u2019s GenAI Infrastructure \u2192 https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/ Marking a major investment in Meta\u2019s AI future, we are...","keywords":["open","infrastructure","clusters","continue","compute","design","genai","meta\u2019s","grand","teton"]},{"id":"reshare-openai-transformer-debugger","title":"OpenAI Transformer Debugger","content":"OpenAI Transformer Debugger \u2192 https://github.com/openai/transformer-debugger Transformer Debugger (TDB) is a tool developed by OpenAI\u0027s Superalignment team with the goal of supporting investigations into specific behaviors of small language models. The tool combines automated interpretability techniques with sparse autoencoders. TDB enables rapid exploration before needing to write code, with the ability to intervene in the forward pass and see how it affects a particular behavior. It can be used to answer questions like, \u0026quot;Why does the model output token A instead of token B for this prompt?\u0026quot; or \u0026quot;Why does attention head H attend to token T for this prompt?\u0026quot; It does so by identifying specific components (neurons, attention heads, autoencoder latents) that contribute to the behavior, showing automatically generated explanations of what causes those components to activate most strongly, and tracing connections between components to help discover circuits.","contentType":"reshare","url":"https://www.lqdev.me/responses/openai-transformer-debugger","date":"2024-03-17 21:05 -05:00","tags":["openai","ai","transformer","debugger","tools"],"summary":"OpenAI Transformer Debugger \u2192 https://github.com/openai/transformer-debugger Transformer Debugger (TDB) is a tool developed by OpenAI\u0027s Superalignment team with the goal of supporting investigations...","keywords":["\u0026quot","debugger","does","token","components","openai","prompt","attention","specific","behavior"]},{"id":"bookmarks-diffusion-models-from-scratch-duan","title":"Diffusion Models From Scratch","content":"Diffusion Models From Scratch \u2192 https://www.tonyduan.com/diffusion/index.html Here, we\u0027ll cover the derivations from scratch to provide a rigorous understanding of the core ideas behind diffusion. What assumptions are we making? What properties arise as a result? A reference [codebase] is written from scratch, which provides minimalist re-production of the MNIST example below. It clocks in at under 500 lines of code. Each page takes up to an hour to read thoroughly. Approximately a lecture each.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/diffusion-models-from-scratch-duan","date":"2024-03-17 21:03 -05:00","tags":["diffusion","ai","concepts","models"],"summary":"Diffusion Models From Scratch \u2192 https://www.tonyduan.com/diffusion/index.html Here, we\u0027ll cover the derivations from scratch to provide a rigorous understanding of the core ideas behind diffusion....","keywords":["from","scratch","diffusion","written","which","provides","minimalist","production","mnist","example"]},{"id":"bookmarks-diffusion-models-from-scratch-yuan","title":"Diffusion models from scratch, from a new theoretical perspective","content":"Diffusion models from scratch, from a new theoretical perspective \u2192 https://www.chenyang.co/diffusion.html This tutorial aims to introduce diffusion models from an optimization perspective as introduced in our paper (joint work with Frank Permenter). It will go over both theory and code, using the theory to explain how to implement diffusion models from scratch. By the end of the tutorial, you will learn how to implement training and sampling code for a toy dataset, which will also work for larger datasets and models.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/diffusion-models-from-scratch-yuan","date":"2024-03-17 21:02 -05:00","tags":["diffusion","ai","concepts","models"],"summary":"Diffusion models from scratch, from a new theoretical perspective \u2192 https://www.chenyang.co/diffusion.html This tutorial aims to introduce diffusion models from an optimization perspective as...","keywords":["models","from","diffusion","scratch","code","perspective","theory","implement","tutorial","using"]},{"id":"bookmarks-fast-inner-product-dnn-accelerators","title":"Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators","content":"Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators \u2192 https://arxiv.org/abs/2311.12224 We introduce a new algorithm called the Free-pipeline Fast Inner Product (FFIP) and its hardware architecture that improve an under-explored fast inner-product algorithm (FIP) proposed by Winograd in 1968. Unlike the unrelated Winograd minimal filtering algorithms for convolutional layers, FIP is applicable to all machine learning (ML) model layers that can mainly decompose to matrix multiplication, including fully-connected, convolutional, recurrent, and attention/transformer layers. We implement FIP for the first time in an ML accelerator then present our FFIP algorithm and generalized architecture which inherently improve FIP\u0027s clock frequency and, as a consequence, throughput for a similar hardware cost. Finally, we contribute ML-specific optimizations for the FIP and FFIP algorithms and architectures. We show that FFIP can be seamlessly incorporated into traditional fixed-point systolic array ML accelerators to achieve the same throughput with half the number of multiply-accumulate (MAC) units, or it can double the maximum systolic array size that can fit onto devices with a fixed hardware budget. Our FFIP implementation for non-sparse ML models with 8 to 16-bit fixed-point inputs achieves higher throughput and compute efficiency than the best-in-class prior solutions on the same type of compute platform. GitHub","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/fast-inner-product-dnn-accelerators","date":"2024-03-17 20:47 -05:00","tags":["ai","math","acceleration","neuralnetworks","dnn"],"summary":"Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators \u2192 https://arxiv.org/abs/2311.12224 We introduce a new algorithm called the Free-pipeline Fast Inner Product (FFIP)...","keywords":["ffip","fast","fixed","layers","hardware","algorithm","throughput","product","inner","algorithms"]},{"id":"reshare-grok-1-open-release","title":"Grok-1","content":"Grok-1 \u2192 https://github.com/xai-org/grok-1 This repository contains JAX example code for loading and running the Grok-1 open-weights model.","contentType":"reshare","url":"https://www.lqdev.me/responses/grok-1-open-release","date":"2024-03-17 20:45 -05:00","tags":["grok","ai","x","github","opensource","moe"],"summary":"Grok-1 \u2192 https://github.com/xai-org/grok-1 This repository contains JAX example code for loading and running the Grok-1 open-weights model.","keywords":["grok","https://github","com/xai","org/grok","repository","contains","example","code","loading","running"]},{"id":"notes-2024-03-18-weekly-post-summary","title":"Week of March 18, 2024 - Post Summary","content":"Notes When should I check in stranger\u0027s luggage? Responses Building Meta\u2019s GenAI Infrastructure Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators Diffusion models from scratch, from a new theoretical perspective Spreadsheets are all you need Introducing the Proton Mail desktop app What I learned from looking at 900 most popular open source AI tools LaVague - Large Action Model framework Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs Algorithms for Modern Hardware Ollama now supports AMD graphics cards Diffusion Models From Scratch Apple Vision Pro Perspectives - Hugo Barra Grok-1 OpenAI Transformer Debugger Bluesky\u2019s Stackable Approach to Moderation","contentType":"notes","url":"https://www.lqdev.me/notes/2024-03-18-weekly-post-summary","date":"2024-03-17 20:43 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Notes When should I check in stranger\u0027s luggage? Responses Building Meta\u2019s GenAI Infrastructure Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators Diffusion models...","keywords":["from","diffusion","scratch","algorithms","models","modern","graphs","knowledge","leveraging","constructing"]},{"id":"reshare-ollama-supports-amd-gpus","title":"Ollama now supports AMD graphics cards","content":"Ollama now supports AMD graphics cards \u2192 https://ollama.com/blog/amd-preview Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated by AMD graphics cards on Ollama for Linux and Windows.","contentType":"reshare","url":"https://www.lqdev.me/responses/ollama-supports-amd-gpus","date":"2024-03-17 20:43 -05:00","tags":["ollama","gpu","amd","ai","llm","opensource"],"summary":"Ollama now supports AMD graphics cards \u2192 https://ollama.com/blog/amd-preview Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated...","keywords":["ollama","graphics","cards","supports","preview","windows","linux","https://ollama","com/blog/amd","features"]},{"id":"reshare-apple-vision-pro-barra","title":"Apple Vision Pro Perspectives - Hugo Barra","content":"Apple Vision Pro Perspectives - Hugo Barra \u2192 https://hugo.blog/2024/03/11/vision-pro/ Friends and colleagues have been asking me to share my perspective on the Apple Vision Pro as a product. This started as blog post and became an essay before too long, so I\u2019ve structured my writing in multiple sections each with a clear lead to make it a bit easier to digest \u2014 peppered with my own \u2018takes\u2019. I\u2019ve tried to stick to original thoughts for the most part and link to what others have said where applicable. Some of the topics I touch on: Why I believe Vision Pro may be an over-engineered \u201Cdevkit\u201D The genius \u0026amp; audacity behind some of Apple\u2019s hardware decisions Gaze \u0026amp; pinch is an incredible UI superpower and major industry ah-ha moment Why the Vision Pro software/content story is so dull and unimaginative Why most people won\u2019t use Vision Pro for watching TV/movies Apple\u2019s bet in immersive video is a total game-changer for Live Sports Why I returned my Vision Pro\u2026 and my Top 10 wishlist to reconsider Apple\u2019s VR debut is the best thing that ever happened to Oculus/Meta My unsolicited product advice to Meta for Quest Pro 2 and beyond","contentType":"reshare","url":"https://www.lqdev.me/responses/apple-vision-pro-barra","date":"2024-03-15 21:12 -05:00","tags":["apple","visionpro","vr","ar"],"summary":"Apple Vision Pro Perspectives - Hugo Barra \u2192 https://hugo.blog/2024/03/11/vision-pro/ Friends and colleagues have been asking me to share my perspective on the Apple Vision Pro as a product. This...","keywords":["vision","apple\u2019s","apple","i\u2019ve","product","\u0026amp","happened","people","unimaginative","dull"]},{"id":"reshare-bluesky-stackable-approach-moderation","title":"Bluesky\u2019s Stackable Approach to Moderation","content":"Bluesky\u2019s Stackable Approach to Moderation \u2192 https://bsky.social/about/blog/03-12-2024-stackable-moderation Bluesky was created to put users and communities in control of their social spaces online. The first generation of social media platforms connected the world, but ended up consolidating power in the hands of a few corporations and their leaders. Our online experience doesn\u2019t have to depend on billionaires unilaterally making decisions over what we see. On an open social network like Bluesky, you can shape your experience for yourself. Today, we\u2019re excited to announce that we\u2019re open-sourcing Ozone , our collaborative moderation tool. With Ozone, individuals and teams can work together to review and label content across the network. Later this week, we\u2019re opening up the ability for you to run your own independent moderation services, seamlessly integrated into the Bluesky app. This means that you\u0027ll be able to create and subscribe to additional moderation services on top of what Bluesky requires, giving you unprecedented control over your social media experience.","contentType":"reshare","url":"https://www.lqdev.me/responses/bluesky-stackable-approach-moderation","date":"2024-03-15 20:53 -05:00","tags":["moderation","community","bluesky","atprotocol","socialmedia","internet","web"],"summary":"Bluesky\u2019s Stackable Approach to Moderation \u2192 https://bsky.social/about/blog/03-12-2024-stackable-moderation Bluesky was created to put users and communities in control of their social spaces online....","keywords":["moderation","bluesky","social","we\u2019re","experience","network","ozone","media","services","online"]},{"id":"reshare-introducing-protonmail-desktop-app","title":"Introducing the Proton Mail desktop app ","content":"Introducing the Proton Mail desktop app \u2192 https://proton.me/blog/proton-mail-desktop-app Today, we\u2019re excited to broaden the horizons of secure communication by launching the Proton Mail desktop app. Anyone can now use the new Proton Mail desktop app for Windows and macOS, with a Linux version in beta. With the new Proton Mail desktop apps, you get a dedicated email experience, allowing you to enjoy all the productivity innovations of our web app, allowing you to go through your emails and events faster without the potential distractions that pop up anytime you open your browser. And, of course, your privacy remains protected at all times.","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-protonmail-desktop-app","date":"2024-03-15 20:06 -05:00","tags":["email","proton","desktop","app","privacy","security"],"summary":"Introducing the Proton Mail desktop app \u2192 https://proton.me/blog/proton-mail-desktop-app Today, we\u2019re excited to broaden the horizons of secure communication by launching the Proton Mail desktop app....","keywords":["mail","desktop","proton","allowing","introducing","enjoy","productivity","innovations","emails","events"]},{"id":"reshare-learned-from-looking-900-oss-ai-tools-huyen","title":"What I learned from looking at 900 most popular open source AI tools","content":"What I learned from looking at 900 most popular open source AI tools \u2192 https://huyenchip.com//2024/03/14/ai-oss.html So many cool ideas are being developed by the community. Here are some of my favorites. Batch inference optimization: FlexGen, llama.cpp Faster decoder with techniques such as Medusa, LookaheadDecoding Model merging: mergekit Constrained sampling: outlines, guidance, SGLang Seemingly niche tools that solve one problem really well, such as einops and safetensors.","contentType":"reshare","url":"https://www.lqdev.me/responses/learned-from-looking-900-oss-ai-tools-huyen","date":"2024-03-14 22:13 -05:00","tags":["ai","tools","analysis","openai","github"],"summary":"What I learned from looking at 900 most popular open source AI tools \u2192 https://huyenchip.com//2024/03/14/ai-oss.html So many cool ideas are being developed by the community. Here are some of my...","keywords":["tools","learned","decoder","techniques","medusa","lookaheaddecoding","model","merging:","mergekit","constrained"]},{"id":"notes-checking-in-strangers-luggage","title":"When should I check in stranger\u0027s luggage?","content":"I was sitting at the airport waiting to board my flight and over the PA system, the usual \u0026quot;see something, say something\u0026quot; security announcements were going off. Although I\u0027ve heard these many times before, this time for some reason there\u0027s one that caught my attention and got me thinking. I don\u0027t remember the exact words, but basically the gist of it said something like, \u0026quot;If anyone asks you to check in baggage other than your own, do not accept it\u0026quot;. I barely even want to deal with my own bags. Why would I want to deal with someone else\u0027s bag? I\u0027ve been trying to visualize scenarios where: Someone would come up to you and ask you to check in THEIR luggage You would accept Maybe in scenarios where you\u0027re traveling with friends or family? In the case of family, maybe someone doesn\u0027t have a carry-on and if there\u0027s an extra bag that\u0027s small enough to be considered carry-on they\u0027ll \u0026quot;claim\u0026quot; it as their carry-on to not pay the extra fees. Still though, that\u0027s not checking it in. I\u0027m just having a hard time seeing it. Putting this out there ahead of time - no, stranger, I will not check in your bag.","contentType":"notes","url":"https://www.lqdev.me/notes/checking-in-strangers-luggage","date":"2024-03-11 17:57 -05:00","tags":["question","airport","security","fun"],"summary":"I was sitting at the airport waiting to board my flight and over the PA system, the usual \u0026quot;see something, say something\u0026quot; security announcements were going off. Although I\u0027ve heard these...","keywords":["carry","someone","check","\u0026quot","want","extra","maybe","deal","scenarios","accept"]},{"id":"notes-2024-03-10-weekly-post-summary","title":"Week of March 10, 2024 - Post Summary","content":"Blogs Configure Ollama on Dev Containers and VS Code Getting started with Ollama on Windows Notes Crate Finds - February 2024 Responses Microsoft to end its Android apps on Windows 11 subsystem in 2025 The surprising connection between after-hours work and decreased productivity Stable Diffusion 3: Research Paper Wix\u2019s new AI chatbot builds websites in seconds based on prompts Apple Podcasts now includes transcripts 5 reasons why desktop Linux is finally growing in popularity Levels of Complexity: RAG Applications Training great LLMs entirely from ground up in the wilderness as a startup Gemma PyTorch Guest host Hank Green makes Nilay Patel explain why websites have a future Inflection-2.5: meet the world\u0027s best personal AI The missing graph datatype already exists. It was invented in the \u002770s Today\u2019s smart homes: the hopes and the realities Introducing the next generation of Claude You can now train a 70b language model at home","contentType":"notes","url":"https://www.lqdev.me/notes/2024-03-10-weekly-post-summary","date":"2024-03-10 14:26 -05:00","tags":["weeklysummary","blogging","website","indieweb"],"summary":"Blogs Configure Ollama on Dev Containers and VS Code Getting started with Ollama on Windows Notes Crate Finds - February 2024 Responses Microsoft to end its Android apps on Windows 11 subsystem in...","keywords":["ollama","windows","websites","blogs","explain","patel","nilay","makes","green","hank"]},{"id":"reshare-answer-ai-train-70b-llm-at-home","title":"You can now train a 70b language model at home","content":"You can now train a 70b language model at home \u2192 https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html Today, we\u2019re releasing Answer.AI\u2019s first project: a fully open source system that, for the first time, can efficiently train a 70b large language model on a regular desktop computer with two or more standard gaming GPUs (RTX 3090 or 4090). This system, which combines FSDP and QLoRA, is the result of a collaboration between Answer.AI, Tim Dettmers (U Washington), and Hugging Face\u2019s Titus von Koeller and Sourab Mangrulkar.","contentType":"reshare","url":"https://www.lqdev.me/responses/answer-ai-train-70b-llm-at-home","date":"2024-03-08 12:13 -05:00","tags":["ai","llm","qlora","gpu","nvidia","finetuning"],"summary":"You can now train a 70b language model at home \u2192 https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html Today, we\u2019re releasing Answer.AI\u2019s first project: a fully open source system that, for the...","keywords":["answer","train","language","model","system","fsdp","qlora","gaming","gpus","3090"]},{"id":"reshare-levels-of-complexity-rag-applications","title":"Levels of Complexity: RAG Applications","content":"Levels of Complexity: RAG Applications \u2192 https://jxnl.github.io/blog/writing/2024/02/28/levels-of-complexity-rag-applications/ This post comprehensive guide to understanding and implementing RAG applications across different levels of complexity. Whether you\u0027re a beginner eager to learn the basics or an experienced developer looking to deepen your expertise, you\u0027ll find valuable insights and practical knowledge to help you on your journey. Let\u0027s embark on this exciting exploration together and unlock the full potential of RAG applications.","contentType":"reshare","url":"https://www.lqdev.me/responses/levels-of-complexity-rag-applications","date":"2024-03-07 22:08 -05:00","tags":["rag","ai","llm","architecture","app"],"summary":"Levels of Complexity: RAG Applications \u2192 https://jxnl.github.io/blog/writing/2024/02/28/levels-of-complexity-rag-applications/ This post comprehensive guide to understanding and implementing RAG...","keywords":["applications","levels","complexity","looking","deepen","expertise","find","valuable","insights","practical"]},{"id":"reshare-year-of-linux-desktop-4-percent","title":"5 reasons why desktop Linux is finally growing in popularity","content":"5 reasons why desktop Linux is finally growing in popularity \u2192 https://www.zdnet.com/article/5-reasons-why-desktop-linux-is-finally-growing-in-popularity/ StatCounter reported that desktop Linux reached over 4% market share for the first time. Why is Linux finally growing? While Windows is the king of the hill with 72.13% and MacOS comes in a distant second at 15.46%, it\u0027s clear that Linux is making progress. Microsoft isn\u0027t that interested in Windows Linux gaming, thanks to Steam, is also growing Users are finally figuring out that some Linux distros are easy to use Finding and installing Linux desktop software is easier than ever The Linux desktop is growing in popularity in India","contentType":"reshare","url":"https://www.lqdev.me/responses/year-of-linux-desktop-4-percent","date":"2024-03-07 21:27 -05:00","tags":["linux","pc","desktop","desktop"],"summary":"5 reasons why desktop Linux is finally growing in popularity \u2192 https://www.zdnet.com/article/5-reasons-why-desktop-linux-is-finally-growing-in-popularity/ StatCounter reported that desktop Linux...","keywords":["linux","desktop","growing","finally","reasons","popularity","windows","microsoft","interested","gaming"]},{"id":"reshare-inflection-2-5","title":"Inflection-2.5: meet the world\u0027s best personal AI","content":"Inflection-2.5: meet the world\u0026#39;s best personal AI \u2192 https://inflection.ai/inflection-2-5 At Inflection, our mission is to create a personal AI for everyone. Last May, we released Pi\u2014a personal AI, designed to be empathetic, helpful, and safe. In November we announced a new major foundation model, Inflection-2, the second best LLM in the world at the time. Now we are adding IQ to Pi\u2019s exceptional EQ. We are launching Inflection-2.5, our upgraded in-house model that is competitive with all the world\u0027s leading LLMs like GPT-4 and Gemini. It couples raw capability with our signature personality and unique empathetic fine-tuning. Inflection-2.5 is available to all Pi\u0027s users today, at pi.ai, on iOS, on Android, or our new desktop app. We achieved this milestone with incredible efficiency: Inflection-2.5 approaches GPT-4\u2019s performance, but used only 40% of the amount of compute for training.","contentType":"reshare","url":"https://www.lqdev.me/responses/inflection-2-5","date":"2024-03-07 21:22 -05:00","tags":["ai","llm","inflection"],"summary":"Inflection-2.5: meet the world\u0026#39;s best personal AI \u2192 https://inflection.ai/inflection-2-5 At Inflection, our mission is to create a personal AI for everyone. Last May, we released Pi\u2014a personal...","keywords":["inflection","personal","world","model","empathetic","best","performance","llms","gemini","couples"]},{"id":"reshare-surprising-connection-after-hours-work-productivity-slack","title":"The surprising connection between after-hours work and decreased productivity","content":"The surprising connection between after-hours work and decreased productivity \u2192 https://slack.com/blog/news/the-surprising-connection-between-after-hours-work-and-decreased-productivity Quick take: How do you spend your time at work and what is it costing you? Slack\u2019s Workforce Index, based on survey responses from more than 10,000 desk workers around the globe, uncovers new findings on how to structure the workday to maximize productivity and strengthen employee well-being and satisfaction. Key learnings include: Employees who log off at the end of the workday register 20% higher productivity scores than those who feel obligated to work after hours. Making time for breaks during the workday improves employee productivity and well-being, and yet half of all desk workers say they rarely or never take breaks. On average, desk workers say that the ideal amount of focus time is around four hours a day, and more than two hours a day in meetings is the tipping point at which a majority of workers feel overburdened by meetings. Three out of every four desk workers report working in the 3 to 6pm timeframe, but of those, only one in four consider these hours highly productive.","contentType":"reshare","url":"https://www.lqdev.me/responses/surprising-connection-after-hours-work-productivity-slack","date":"2024-03-07 21:14 -05:00","tags":["slack","productivity","work"],"summary":"The surprising connection between after-hours work and decreased productivity \u2192 https://slack.com/blog/news/the-surprising-connection-between-after-hours-work-and-decreased-productivity Quick take:...","keywords":["hours","productivity","workers","desk","workday","four","surprising","breaks","employee","around"]},{"id":"reshare-training-llms-ground-up-wilderness-startup","title":"Training great LLMs entirely from ground up in the wilderness as a startup","content":"Training great LLMs entirely from ground up in the wilderness as a startup \u2192 https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness Given that we\u2019ve successfully trained pretty strong multimodal language models at Reka, many people have been particularly curious about the experiences of building infrastructure and training large language \u0026amp; multimodal models from scratch from a completely clean slate. I complain a lot about external (outside Google) infrastructure and code on my social media, leading people to really be curious about what are the things I miss and what I hate/love in the wilderness. So here\u2019s a post (finally). This blogpost sheds light on the challenges and lessons learned. Figuring out things in the wilderness was an interesting experience. It was unfortunately not painless. Compute scarcity and also unreliable compute providers made things significantly harder than expected but we\u2019re glad we pulled through with brute technical strength. All in all, this is only a small part of the story of how we started a company, raised some money, bought some chips and matched Gemini pro/GPT 3.5 and outperformed many others in less than a year having to build everything from scratch.","contentType":"reshare","url":"https://www.lqdev.me/responses/training-llms-ground-up-wilderness-startup","date":"2024-03-07 21:10 -05:00","tags":["llm","ai","startup","compute","gpu","training"],"summary":"Training great LLMs entirely from ground up in the wilderness as a startup \u2192 https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness Given that we\u2019ve successfully...","keywords":["from","wilderness","things","training","language","multimodal","people","scratch","curious","models"]},{"id":"reshare-datalog-the-missing-graph-data-type-already-exists","title":"The missing graph datatype already exists. It was invented in the \u002770s","content":"The missing graph datatype already exists. It was invented in the \u0026#39;70s \u2192 https://tylerhou.com/posts/datalog-go-brrr/ The datatype for a graph is a relation, and graph algorithms are queries on the relation. But modern languages need better support for the relational model. This post is a response to/inspired by The Hunt for the Missing Data Type (HN) by Hillel Wayne . I suggest reading his article first. I claim the reason why it is so difficult to support graphs in languages nowadays is because the imperative/structured programming model of modern programming languages is ill-suited for graph algorithms. As Wayne correctly points out, the core problem is that when you write a graph algorithm in an imperative language like Python or Rust, you have to choose some explicit representation for the graph. Then, your traversal algorithm is dependent on the representation you chose. If you find out later that your representation is no longer efficient, it is a lot of work to adapt your algorithms for a new representation. So what if we just, like, didn\u2019t do this? We already have a declarative programming language where expressing graph algorithms is extremely natural\u2014Datalog, whose semantics are based on* the relational algebra, which was developed in the 1970s. Wonderful! Except for the \u201Cwriting Datalog\u201D part. If Datalog is so great, why hasn\u2019t it seen more adoption? The short answer is that Datalog is relatively esoteric outside of academia and some industry applications and, as a result, is not a great language from a \u201Csoftware engineering\u201D perspective. It is hard for programmers accustomed to imperative code to write Datalog programs, and large Datalog programs can be hard to write and understand.","contentType":"reshare","url":"https://www.lqdev.me/responses/datalog-the-missing-graph-data-type-already-exists","date":"2024-03-06 21:39 -05:00","tags":["datalog","programming","graphs","datatypes","programminglanguages","software","database","algorithms"],"summary":"The missing graph datatype already exists. It was invented in the \u0026#39;70s \u2192 https://tylerhou.com/posts/datalog-go-brrr/ The datatype for a graph is a relation, and graph algorithms are queries on...","keywords":["graph","representation","datalog","algorithms","language","write","programming","languages","programs","hard"]},{"id":"posts-install-ollama-vscode-devcontainer","title":"Configure Ollama on Dev Containers and VS Code","content":"The source for my website is [hosted on GitHub](/github/luisquintanilla.me). As a result, I use VS Code as the text editor to author my posts. Typically, I use [Codespaces or github.dev](/colophon) to quickly draft and publish articles to simplify the process. As part of my authoring and publishing posts, there\u0027s a few things I do like: 1. Coming up with a title 2. Creating relevant tags to help with discovery and connecting related information on my website. 3. For long-form blog posts, I also include a description. This is generally relatively easy to do. However, sometimes I spend too much time coming up with something that truly summarizes and condenses the information in the post. It\u0027d be nice if I had an assistant to help me brainstorm and workshop some of these items. In comes AI. Now, I could use something like [Copilot](https://code.visualstudio.com/docs/copilot/overview) which would work wonderfully and easily plug into my workflow. However, my website is a labor of love and I don\u0027t make any money from it. In many instances, I\u0027ve designed [various components to be as low-cost as possible](/posts/receive-webmentions-fsharp-az-functions-fsadvent). Recently, I created a post which showed [how to get started with Ollama on Windows](/posts/getting-started-ollama-windows). In this post, I\u0027ll show how you can do the same for your Dev Container environments. ## Install Ollama Assuming you already have a [Dev Container configuration file](https://code.visualstudio.com/docs/devcontainers/create-dev-container), add the following line to it: \u0060\u0060\u0060json \u0022postCreateCommand\u0022: \u0022curl -fsSL https://ollama.com/install.sh | sh\u0022 \u0060\u0060\u0060 This command triggers when the container environment is created. This will install Ollama in your development environment. For more details, see the [Ollama download instructions](https://ollama.com/download/linux). ## Start Ollama Now that Ollama is installed, it\u0027s time to start the service so you can get models and use them. To start the Ollama service on your Dev Conatiner environment, add the following line to your Dev Container configuration file. \u0060\u0060\u0060json \u0022postStartCommand\u0022: \u0022ollama serve\u0022 \u0060\u0060\u0060 This command will run \u0060ollama serve\u0060 when the Dev Container environment starts. ## Pull a model To use Ollama, you\u0027re going to need a model. In my case, I went with [Phi-2](https://ollama.com/library/phi) because it\u0027s lightweight and space on Codespaces is limited. To get the model: 1. Open the terminal 1. Enter the following command: \u0060\u0060\u0060bash ollama pull phi \u0060\u0060\u0060 After a few minutes, the model is downloaded and ready to use. 1. Enter the following command to ensure that your model is now available \u0060\u0060\u0060bash ollama list \u0060\u0060\u0060 The result should look like the following: \u0060\u0060\u0060text NAME ID SIZE MODIFIED phi:latest e2fd6321a5fe 1.6 GB 6 seconds ago \u0060\u0060\u0060 For more details, see the [Ollama model library](https://ollama.com/library). ## Conclusion In this post, I showed how you can configure Ollama with Dev Containers to use AI models locally in your projects. In subsequent posts, I\u0027ll show how once the service is running, I use it as part of my authoring and publishing posts. You can see a preview of it in my [scripts directory](https://github.com/lqdev/luisquintanilla.me/blob/main/Scripts/ai.fsx). Happy coding!","contentType":"posts","url":"https://www.lqdev.me/posts/install-ollama-vscode-devcontainer","date":"2024-03-06 13:50 -05:00","tags":["ollama","vscode","devcontainer","ai","llm","opensource","development"],"summary":"The source for my website is [hosted on GitHub](/github/luisquintanilla.me). As a result, I use VS Code as the text editor to author my posts. Typically, I use [Codespaces or github.dev](/colophon)...","keywords":["ollama","container","model","posts","post","https://ollama","environment","following","service","website"]},{"id":"reshare-apple-podcasts-transcript-17-4","title":"Apple Podcasts now includes transcripts","content":"Apple Podcasts now includes transcripts \u2192 https://www.theverge.com/2024/3/5/24091555/apple-podcasts-transcripts-ios-17-4-update Apple Podcasts will auto-generate transcripts for podcasts beginning today, thanks to the 17.4 update for iPhones and iPads. Transcripts will automatically appear for new podcast episodes shortly after their publication, while Apple will transcribe podcast back catalogs over time. The podcast transcripts are searchable, allowing users to type in a specific word or phrase and skip to that part of an episode. Users can find transcripts for individual podcast episodes on the bottom-left corner of the \u201CNow Playing\u201D screen. Podcasters who don\u2019t want to use Apple\u2019s automated transcription can opt to upload their own transcripts via RSS tags or in Apple Podcasts Connect for premium episodes, or they can download and edit Apple\u2019s transcript before reuploading. This is cool and great for accessibility. I recently chose to read the latest episode of Decoder instead of listening to it. One of the advantages this also provided was that I could reference direct quotes in my post from the episode. I could see Apple taking this further by making it easier to generate show notes / descriptions based on the episode using AI.","contentType":"reshare","url":"https://www.lqdev.me/responses/apple-podcasts-transcript-17-4","date":"2024-03-05 20:52 -05:00","tags":["iphone","apple","podcasts","ios","ipados"],"summary":"Apple Podcasts now includes transcripts \u2192 https://www.theverge.com/2024/3/5/24091555/apple-podcasts-transcripts-ios-17-4-update Apple Podcasts will auto-generate transcripts for podcasts beginning...","keywords":["transcripts","apple","podcasts","episode","podcast","episodes","users","generate","apple\u2019s","update"]},{"id":"reshare-windows-subsystem-for-android-end-of-support-2025","title":"Microsoft to end its Android apps on Windows 11 subsystem in 2025","content":"Microsoft to end its Android apps on Windows 11 subsystem in 2025 \u2192 https://www.theverge.com/2024/3/5/24091370/microsoft-windows-11-android-apps-end-of-support Microsoft is ending support for its Android subsystem in Windows 11 next year. The software giant first announced it was bringing Android apps to Windows 11 with Amazon\u2019s Appstore nearly three years ago, but this Windows Subsystem for Android will now be deprecated starting March 5th, 2025. That\u0027s unfortunate considering the new lineup of ARM-based PCs expected later this year. It would\u0027ve been nice to have a mobile PC with 5G support that could run mobile apps for scenarios where there are no web / native PC apps.","contentType":"reshare","url":"https://www.lqdev.me/responses/windows-subsystem-for-android-end-of-support-2025","date":"2024-03-05 20:47 -05:00","tags":["windows","android","wsa","pc"],"summary":"Microsoft to end its Android apps on Windows 11 subsystem in 2025 \u2192 https://www.theverge.com/2024/3/5/24091370/microsoft-windows-11-android-apps-end-of-support Microsoft is ending support for its...","keywords":["android","apps","windows","support","subsystem","year","microsoft","2025","mobile","nice"]},{"id":"reshare-stable-diffusion-3-research-paper","title":"Stable Diffusion 3: Research Paper","content":"Stable Diffusion 3: Research Paper \u2192 https://stability.ai/news/stable-diffusion-3-research-paper Key Takeaways: Today, we\u2019re publishing our research paper that dives into the underlying technology powering Stable Diffusion 3. Stable Diffusion 3 outperforms state-of-the-art text-to-image generation systems such as DALL\u00B7E 3, Midjourney v6, and Ideogram v1 in typography and prompt adherence, based on human preference evaluations. Our new Multimodal Diffusion Transformer (MMDiT) architecture uses separate sets of weights for image and language representations, which improves text understanding and spelling capabilities compared to previous versions of SD3.","contentType":"reshare","url":"https://www.lqdev.me/responses/stable-diffusion-3-research-paper","date":"2024-03-05 12:55 -05:00","tags":["stabilityai","stablediffusion","ai","generativeai","genai","images"],"summary":"Stable Diffusion 3: Research Paper \u2192 https://stability.ai/news/stable-diffusion-3-research-paper Key Takeaways: Today, we\u2019re publishing our research paper that dives into the underlying technology...","keywords":["diffusion","stable","research","paper","image","text","preference","evaluations","multimodal","transformer"]},{"id":"posts-getting-started-ollama-windows","title":"Getting started with Ollama on Windows","content":"Recently [Ollama announced support for Windows](/responses/ollama-windows-preview) in preview. In doing so, people who want to use AI models like Llama, Phi, and many others can do so locally on their PC. In this post, I\u0027ll go over how you can get started with Ollama on Windows. ## Install Ollama The first thing you\u0027ll want to do is install Ollama. You can do so by [downloading the installer from the website](https://ollama.com/download/windows) and following the installation prompts. ## Get a model Once you\u0027ve installed Ollama, it\u0027s time to get a model. 1. Open PowerShell 1. Run the following command \u0060\u0060\u0060powershell ollama pull llama2 \u0060\u0060\u0060 In this case, I\u0027m using llama2. However, you can choose another model. You could even download many models at once and switch between them. For a full list of supported models, see the [Ollama model documentation](https://ollama.com/library). ## Use the model Now that you have the model, it\u0027s time to use it. The easiest way to use the model is using the REST API. When you install Ollama, it starts up a server to host your model. One other neat thing is, the REST API is [OpenAI API compatible](https://ollama.com/blog/openai-compatibility). 1. Open PowerShell 1. Send the following request: \u0060\u0060\u0060powershell (Invoke-WebRequest -method POST -Body \u0027{\u0022model\u0022:\u0022llama2\u0022, \u0022prompt\u0022:\u0022Why is the sky blue?\u0022, \u0022stream\u0022: false}\u0027 -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json \u0060\u0060\u0060 This command will issue an HTTP POST request to the server listening on port 11434. The main things to highlight in the body: - *model*: The model you\u0027ll use. Make sure this is one of the models you pulled. - *prompt*: The input to the model - *stream*: Whether to stream responses back to the client For more details on the REST API, see the [Ollama REST API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md). ## Conclusion In this post, I went over how you can quickly install Ollama to start using generative AI models like Llama and Phi locally on your Windows PC. If you use Mac or Linux, you can perform similar steps as those outlined in this guide to get started on those operating systems. Happy coding!","contentType":"posts","url":"https://www.lqdev.me/posts/getting-started-ollama-windows","date":"2024-03-05 10:32 -05:00","tags":["ai","ollama","windows","llm","opensource","llama","openai","generativeai","genai"],"summary":"Recently [Ollama announced support for Windows](/responses/ollama-windows-preview) in preview. In doing so, people who want to use AI models like Llama, Phi, and many others can do so locally on...","keywords":["model","ollama","models","install","post","rest","windows","following","using","llama2"]},{"id":"reshare-wix-website-builder-ai-chatbot","title":"Wix\u2019s new AI chatbot builds websites in seconds based on prompts","content":"Wix\u2019s new AI chatbot builds websites in seconds based on prompts \u2192 https://www.theverge.com/2024/3/4/24090095/wix-ai-website-generator-chatbot You can now build a website, images and all, using only prompts in Wix\u2019s new AI website builder. Creating a website is free, but you\u2019ll have to upgrade to one of Wix\u2019s premium plans if you want to do things like accept payments or don\u2019t want to be limited to using a Wix domain name. You\u2019d probably need to delve into Wix\u2019s advanced editing features and know things about actual web development for that. But it was very easy to use the basic AI generator to create something that looks close to a legitimate site to start with, making it much easier to get to a basic starting point.","contentType":"reshare","url":"https://www.lqdev.me/responses/wix-website-builder-ai-chatbot","date":"2024-03-04 23:15 -05:00","tags":["wix","ai","websites","internet","web"],"summary":"Wix\u2019s new AI chatbot builds websites in seconds based on prompts \u2192 https://www.theverge.com/2024/3/4/24090095/wix-ai-website-generator-chatbot You can now build a website, images and all, using only...","keywords":["wix\u2019s","website","chatbot","things","want","prompts","generator","using","basic","need"]},{"id":"reshare-todays-smart-homes-hopes-realities-verge","title":"Today\u2019s smart homes: the hopes and the realities","content":"Today\u2019s smart homes: the hopes and the realities \u2192 https://www.theverge.com/24080426/smart-home-tech-matter-pets-kitchen-hubs-how-to The Verge team and others share their experiences of how smart technologies affect their lives \u2014 how it can often help and sometimes frustrate. In these articles, we\u2019ve concentrated on how our own experiences, and the experiences of others, have affected how we regard smart home tech. We\u2019ve got personal accounts by one reporter who decided to put together a brand-new smart home and another whose brother moved into a home haunted by the ghosts of someone else\u2019s smart tech. Several of our staffers wax enthusiastically about their favorite devices and automations. A writer describes how smart tech makes his home more accessible. Our smart home reviewer tells how she uses technology to keep her varied pets (and she has a lot of them) happy and healthy. We talk to people who use smart devices to help them care for their parents \u2014 and more.","contentType":"reshare","url":"https://www.lqdev.me/responses/todays-smart-homes-hopes-realities-verge","date":"2024-03-04 23:13 -05:00","tags":["iot","verge","smarthome","technology"],"summary":"Today\u2019s smart homes: the hopes and the realities \u2192 https://www.theverge.com/24080426/smart-home-tech-matter-pets-kitchen-hubs-how-to The Verge team and others share their experiences of how smart...","keywords":["smart","home","tech","experiences","help","more","others","devices","we\u2019ve","pets"]},{"id":"reshare-decoder-nilay-patel-why-websites-are-the-future","title":"Guest host Hank Green makes Nilay Patel explain why websites have a future","content":"Guest host Hank Green makes Nilay Patel explain why websites have a future \u2192 https://www.theverge.com/24087834/hank-green-decoder-podcast-google-youtube-web-media-platforms-distribution-future ...the last platform on the web of any scale or influence is Google Search. And so, over time, webpages have become dramatically optimized for Google Search. And that means the kinds of things people write about, the containers that we write in, are mostly designed to be optimized for Google Search. They\u2019re not designed for, \u201CI need to just quickly tell you about this and move on.\u201D Our little insight was, \u201CWell, what if we just don\u2019t do that? What if we only write for the people who come directly to our website instead of the people who find our articles through Search or Google Discover or whatever other Google platforms are in the world?\u201D And so we just made these little blog posts, and the idea was, if you just come to our website one more time a day because there\u2019s one more thing to look at that you\u2019ll like, we will be fine. more and more people are starting to realize, \u201COh, we should just make the websites more valuable. ...if you start writing for other people, which is the heart of what a blog post really is: it\u2019s you trying to entertain yourself and trying to entertain just a handful of other people, you\u2019re going to go really much farther than trying to satisfy the robot. Why am I writing in the text box that pays money to Elon and Mark [Zuckerberg] and not my text box? Why do we all work for free? Look, we want to talk about the platform era and media. Why do we all work for free? ...It\u2019s very confusing, and there are a lot of reasons. If you just sit back and think about why, there are a million reasons why. One, the software is nicer to use than most CMSes. You just pick one. Name a company that makes a CMS. They\u2019re like, \u201CIs this as fun to use as Twitter?\u201D And the answer is no. Flatly no. Even the one we have now for quick posts is not as fun to use as Twitter was in its heyday. Will this immediately bring me the dopamine hit of immediate feedback? No. [When redesigning the website]...the first instinct was, \u201CLet\u2019s at least make it easier to publish. Let\u2019s at least remove the barriers to entry to getting on the website, and then we can do comments, and then we can think about how we can distribute in different ways.\u201D So that is working. My team is happier. We did not know that the Twitter thing would happen, but the Twitter thing happened, and our desire to publish in the boxes we controlled went up as a group. And then, on top of it, our audience saw that we were having fun. And once you are having fun anywhere on the internet, people sort of gravitate to you. So traffic has gone up. The distribution actually just creates the work or creates the pressures that force all the work to be the same. And I think over time that\u2019s what drives the audiences away. So there\u2019s a real change in how these platforms work, where, over time, they just become more and more of the same thing and the creators become more and more the same. And that\u2019s a little exhausting. And every place where you see open distribution, you see a huge variety of creators and content. Podcasts have basically open distribution. Like podcast or distributor RSS feeds, that means people kind of own their distribution, there\u2019s a vast array of podcast creators. There\u2019s a vast array of podcast formats. They don\u2019t all sound like the beginning of YouTube videos or whatever. And I hate to keep picking on YouTube; you can pick any algorithmic platform, and it\u2019s the same. TikTokers are more the same than different. Podcasters are more different than the same. The web is distributed largely through websites and through RSS. There\u2019s a huge variety of websites and the way websites look. But then you see the algorithmic search pressure push web design kind of all under the same box. Newsletters distributed by email: open distribution. The newsletter economy is full of a huge variety of creators doing a huge variety of things. They\u2019re more different than the same. So all I see with the fediverse is, \u201COh, this is going to open social distribution up a little bit.\u201D It\u2019s going to allow us to control our distribution networks. It\u2019s going to say, \u201CI\u2019m not on Twitter, but people on Twitter can follow my website, and I can go promote that follow anywhere I want in different ways and build an audience outside of the pressures of the algorithm.\u201D To me, just that, that ability to try, is 1 percent better. If you\u2019re me and you run a big website and you are thinking, \u201CHow can I redistribute this website, how can I reach people more directly?\u201D my brain is lit up. You should be able to follow me at TheVerge.com and see all my quick posts in your Threads account when Threads federates.","contentType":"reshare","url":"https://www.lqdev.me/responses/decoder-nilay-patel-why-websites-are-the-future","date":"2024-03-04 22:45 -05:00","tags":["websites","internet","podcast","decoder","rss","platforms","blogs","technology"],"summary":"Guest host Hank Green makes Nilay Patel explain why websites have a future \u2192 https://www.theverge.com/24087834/hank-green-decoder-podcast-google-youtube-web-media-platforms-distribution-future ...the...","keywords":["more","people","same","distribution","website","twitter","google","there\u2019s","it\u2019s","different"]},{"id":"reshare-gemma-pytorch","title":"Gemma PyTorch","content":"Gemma PyTorch \u2192 https://github.com/google/gemma_pytorch Gemma is a family of lightweight, state-of-the art open models built from research and technology used to create Google Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. This is the official PyTorch implementation of Gemma models. We provide model and inference implementations using both PyTorch and PyTorch/XLA, and support running inference on CPU, GPU and TPU.","contentType":"reshare","url":"https://www.lqdev.me/responses/gemma-pytorch","date":"2024-03-04 21:29 -05:00","tags":["google","gemma","pytorch","ai","llm"],"summary":"Gemma PyTorch \u2192 https://github.com/google/gemma_pytorch Gemma is a family of lightweight, state-of-the art open models built from research and technology used to create Google Gemini models. They are...","keywords":["pytorch","models","gemma","text","variants","inference","open","weights","trained","instruction"]},{"id":"reshare-announcing-claude-3-model-family","title":"Introducing the next generation of Claude","content":"Introducing the next generation of Claude \u2192 https://www.anthropic.com/news/claude-3-family Today, we\u0027re announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus. Each successive model offers increasingly powerful performance, allowing users to select the optimal balance of intelligence, speed, and cost for their specific application. Claude 3 Opus is our most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Opus shows us the outer limits of what\u2019s possible with generative AI. Claude 3 Sonnet strikes the ideal balance between intelligence and speed\u2014particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments. Claude 3 Haiku is our fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with unmatched speed. Users will be able to build seamless AI experiences that mimic human interactions.","contentType":"reshare","url":"https://www.lqdev.me/responses/announcing-claude-3-model-family","date":"2024-03-04 21:25 -05:00","tags":["ai","anthropic","claude","llm"],"summary":"Introducing the next generation of Claude \u2192 https://www.anthropic.com/news/claude-3-family Today, we\u0027re announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of...","keywords":["claude","model","opus","performance","family","users","tasks","balance","intelligence","speed"]},{"id":"notes-crate-finds-february-2024","title":"Crate Finds - February 2024","content":"Short month and not as much time spent listening to music. As a result, this month\u0027s list is a short one. Here\u0027s the drop for February 2024: Tracks I Heard a Sigh by Cortex Album: Inedit 79 Duration: 3:29 Listen on YouTube Backup: Listen on Spotify Pienso en Ti by levitation room Album: Pienso en Ti Duration: 2:34 Listen on YouTube Backup: Listen on Spotify Bad Company by Yazmin Lacey Album: Voice Notes Duration: 3:38 Listen on YouTube Backup: Listen on Spotify \u5F71\u306B\u306A\u3063\u3066 by Yumi Matsutoya Album: \u60B2\u3057\u3044\u307B\u3069\u304A\u5929\u6C17 Duration: 5:32 Listen on YouTube Backup: Listen on Spotify The Loving Touch by Keith Mansfield Album: Kpm 1000 Series: Voices in Harmony Duration: 2:58 Listen on YouTube Backup: Listen on Spotify Grow Forever by Lizette \u0026amp; Quevin Album: Grow Forever b/w Now It\u2019s Your Turn To Sing Duration: 3:13 Listen on YouTube Backup: Listen on Spotify Good to Be Back Home by Charles Bradley, Menahan Street Band Album: Changes Duration: 3:01 Listen on YouTube Backup: Listen on Spotify Five of Cups by Holy Wave Album: Five of Cups Duration: 5:16 Listen on YouTube Backup: Listen on Spotify Right Now by Marcus Johnson Album: Quarantine Sessions Duration: 6:45 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-february-2024","date":"2024-03-03 00:09 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"Short month and not as much time spent listening to music. As a result, this month\u0027s list is a short one. Here\u0027s the drop for February 2024: Tracks I Heard a Sigh by Cortex Album: Inedit 79 Duration:...","keywords":["listen","spotify","youtube","backup:","duration:","album:","grow","forever","pienso","five"]},{"id":"notes-agency-gibson-review","title":"Book Review - Agency","content":"Just finished reading Agency by William Gibson. I was not a big fan. I posted my review with more details. Agency Review","contentType":"notes","url":"https://www.lqdev.me/notes/agency-gibson-review","date":"2024-03-02 23:44 -05:00","tags":["book","review","agency","scifi","future","nanotech","ai","gibson"],"summary":"Just finished reading Agency by William Gibson. I was not a big fan. I posted my review with more details. Agency Review","keywords":["agency","review","finished","reading","william","gibson","posted","more","details"]},{"id":"notes-post-statistics-march-2024","title":"Website Post Statistics - March 2024","content":"Another successful run of my monthly script to track post statistics . Some highlights: Over 100 responses through the month of February. That\u0027s basically halfway through the total number of responses I posted in 2023. Almost 20 notes. Similar to responses, I\u0027m halfway through the total number of notes posted last year. I\u0027ve never done 100 days to offload . Unintentionally though, between notes and responses, I\u0027m well past it. When it comes to more long-form posts, I haven\u0027t posted as many. Still only at 1 for the year. However, there\u0027s a few projects I\u0027m looking to undertake that involve some of my publishing practices and AI. That should result in a healthy number of longer blog posts.","contentType":"notes","url":"https://www.lqdev.me/notes/post-statistics-march-2024","date":"2024-03-02 15:57 -05:00","tags":["githubactions","cicd","blogging","stats","github"],"summary":"Another successful run of my monthly script to track post statistics . Some highlights: Over 100 responses through the month of February. That\u0027s basically halfway through the total number of...","keywords":["responses","notes","posted","number","halfway","year","posts","total","comes","more"]},{"id":"bookmarks-ente-private-media-cloud-storage","title":"Ente - Private cloud storage for photos, videos, etc.","content":"Ente - Private cloud storage for photos, videos, etc. \u2192 https://ente.io/ Store, share, and rediscover your memories with absolute privacy","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/ente-private-media-cloud-storage","date":"2024-03-02 15:49 -05:00","tags":["e2ee","photos","cloud","storage","product"],"summary":"Ente - Private cloud storage for photos, videos, etc. \u2192 https://ente.io/ Store, share, and rediscover your memories with absolute privacy","keywords":["ente","private","cloud","storage","photos","videos","https://ente","store","share","rediscover"]},{"id":"reshare-gguf-long-way-around","title":"GGUF, the long way around","content":"GGUF, the long way around \u2192 https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/ We\u2019ve been on a whirlwind adventure to build up our intuition of how machine learning models work, what artifacts they produce, how the machine learning artifact storage story has changed over the past couple years, and finally ended up in GGUF\u2019s documentation to better understand the log that is presented to us when we perform local inference on artifacts in GGUF.","contentType":"reshare","url":"https://www.lqdev.me/responses/gguf-long-way-around","date":"2024-02-29 21:49 -05:00","tags":["ai","genai","gguf","llm","opensource"],"summary":"GGUF, the long way around \u2192 https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/ We\u2019ve been on a whirlwind adventure to build up our intuition of how machine learning models work, what...","keywords":["gguf","long","artifacts","learning","machine","past","couple","years","finally","ended"]},{"id":"reshare-predictive-human-preference-model-ranking-routing","title":"Predictive Human Preference: From Model Ranking to Model Routing","content":"Predictive Human Preference: From Model Ranking to Model Routing \u2192 https://huyenchip.com//2024/02/28/predictive-human-preference.html Human preference has emerged to be both the Northstar and a powerful tool for AI model development. Human preference guides post-training techniques including RLHF and DPO. Human preference is also used to rank AI models, as used by LMSYS\u2019s Chatbot Arena. Chatbot Arena aims to determine which model is generally preferred. I wanted to see if it\u2019s possible to do predictive human preference: determine which model is preferred for each query. This post first discusses the correctness of Chatbot Arena, which will then be used as a baseline to evaluate the correctness of preference predictions. It then discusses how to build a preference predictor and the initial results.","contentType":"reshare","url":"https://www.lqdev.me/responses/predictive-human-preference-model-ranking-routing","date":"2024-02-29 10:43 -05:00","tags":["ai","llm","evaluation","ml"],"summary":"Predictive Human Preference: From Model Ranking to Model Routing \u2192 https://huyenchip.com//2024/02/28/predictive-human-preference.html Human preference has emerged to be both the Northstar and a...","keywords":["human","preference","model","used","which","arena","chatbot","then","correctness","discusses"]},{"id":"notes-era-primavera-chicano-batman","title":"New Single - Era Primavera (Chicano Batman)","content":"New song from Chicano Batman\u0027s latest album, Notebook Fantasy .","contentType":"notes","url":"https://www.lqdev.me/notes/era-primavera-chicano-batman","date":"2024-02-28 22:23 -05:00","tags":["music","newmusic","chicanobatman"],"summary":"New song from Chicano Batman\u0027s latest album, Notebook Fantasy .","keywords":["song","from","chicano","batman","latest","album","notebook","fantasy"]},{"id":"reshare-era-1-bit-llms","title":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits","content":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits \u2192 https://arxiv.org/abs/2402.17764 Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.","contentType":"reshare","url":"https://www.lqdev.me/responses/era-1-bit-llms","date":"2024-02-28 22:03 -05:00","tags":["llm","ai","bitnet","research"],"summary":"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits \u2192 https://arxiv.org/abs/2402.17764 Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models...","keywords":["llms","more","large","language","models","terms","performance","training","both","bitnet"]},{"id":"reshare-tubi-redesign-down-the-rabbit-hole","title":"Tubi\u2019s new redesign wants to push you down the rabbit hole","content":"Tubi\u2019s new redesign wants to push you down the rabbit hole \u2192 https://www.theverge.com/2024/2/28/24085869/tubi-redesign-shows-movies-turple Tubi says its new \u2018turple\u2019-forward brand identity is all about encouraging viewers to fall down rabbit holes to find exciting shows and movies to watch. I\u0027ve often found good content to watch on Tubi . Sure, they\u0027re not the latest blockbusters but there are decades worth of movies out there. Just like today, not all movies that come out are good but there\u0027s still tons of great content. Sometimes, there\u0027s even overlooked gems .","contentType":"reshare","url":"https://www.lqdev.me/responses/tubi-redesign-down-the-rabbit-hole","date":"2024-02-28 21:51 -05:00","tags":["tv","movies","tubi","streaming"],"summary":"Tubi\u2019s new redesign wants to push you down the rabbit hole \u2192 https://www.theverge.com/2024/2/28/24085869/tubi-redesign-shows-movies-turple Tubi says its new \u2018turple\u2019-forward brand identity is all...","keywords":["movies","redesign","down","rabbit","good","content","shows","watch","tubi","often"]},{"id":"bookmarks-hypertext-gardens-delightful-vistas","title":"Hypertext Gardens: Delightful Vistas","content":"Hypertext Gardens: Delightful Vistas \u2192 http://www.eastgate.com/garden/Enter.html The time, care, and expense devoted to creating and promoting a hypertext are lost if readers arrive, glance around, and click elsewhere. How can the craft of hypertext invite readers to stay, to explore, and to reflect?","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/hypertext-gardens-delightful-vistas","date":"2024-02-28 21:44 -05:00","tags":["openweb","digitalgarden","indieweb"],"summary":"Hypertext Gardens: Delightful Vistas \u2192 http://www.eastgate.com/garden/Enter.html The time, care, and expense devoted to creating and promoting a hypertext are lost if readers arrive, glance around,...","keywords":["hypertext","readers","stay","invite","craft","elsewhere","click","around","glance","arrive"]},{"id":"star-celebs-how-to-use-phone","title":"What a bunch of A-list celebs taught me about how to use my phone","content":"What a bunch of A-list celebs taught me about how to use my phone \u2192 https://www.theverge.com/24084772/celebrities-no-phone-bieber-sheeran-cruise-cera-ipad ...[phone-free celebs are] not trying to disconnect from everyone, but they are trying to get away from that feeling of being tapped constantly on the shoulder by all the calls, texts, and emails. So many celebrities ditch their phone, disconnect from their social media, log off entirely. A few years ago, Ed Sheeran shared a strategy...He hasn\u2019t had a phone since 2015...Being phoneless hadn\u2019t cut his contact to the world, Sheeran said, just reduced it \u2014 and that was the point. \u201CI have friends email and people email, and every few days I\u2019ll sit down and open up my laptop, and I\u2019ll answer 10 emails at a time,\u201D he said. \u201CI\u2019ll send them off, and I close my laptop, and then that\u2019ll be it. And then I\u2019ll go back to living my life, and I don\u2019t feel overwhelmed by it.\u201D Read and watch enough celebrity interviews, and the lesson becomes obvious: that the most powerful and connected device in your life shouldn\u2019t be within arm\u2019s reach at all times. All that does is invite distraction and makes it too easy to disengage from your life every time you get bored or sad or curious even for a second. It sounds a little like I\u2019m advocating for the return of the \u201990s, when the computer was a giant box that lived in a central room of your home and the only way to use it was to go to it. And to some extent, I am! I\u2019m increasingly convinced that my primary computer should be a device I use on purpose \u2014 that I sit down at, operate, and then extract myself from until the next time. Whether it\u2019s a laptop on a desk or an iPad on your nightstand, your computer should be a place as much as it is a device. And when you\u2019re not in that place, you\u2019re somewhere else. The computer doesn\u2019t come along. Over the last few weeks, as an experiment, I\u2019ve moved as many apps as possible \u2014 the obviously distracting social media stuff but also anything I can live without on a minute-to-minute basis \u2014 off my phone and onto my tablet and my laptop... So far, it\u2019s been great. I\u2019m realizing how much of a crutch my phone really has become: I would open up TikTok just to keep me company on the walk to the kitchen or scroll through Threads while I waited for the microwave to finish. Now, I\u2019m not sure I\u2019m doing any less of those things in aggregate, but at least I\u2019m doing them on purpose. I\u2019ve turned time-wasting into a deliberate activity \u2014 I sit in my scrolling chair and scroll away, then I get up, and the scrolling stays put. And best of all, when I leave the house, there\u2019s nothing to scroll at all. There has always been talk in tech about removing friction: the obsessive corporate desire to make everything easier, faster, fewer clicks, fewer chances for you to decide not to click that ad or buy that thing or like that post or upload that photo...It should be a little harder for someone to distract me while I\u2019m eating dinner with my wife or hanging out with my kid. It\u2019s not about ditching technology, just about doing technology on purpose.","contentType":"star","url":"https://www.lqdev.me/responses/celebs-how-to-use-phone","date":"2024-02-28 10:24 -05:00","tags":["technology","communication","phones","smartphones","dumbphones","email","celebrities"],"summary":"What a bunch of A-list celebs taught me about how to use my phone \u2192 https://www.theverge.com/24084772/celebrities-no-phone-bieber-sheeran-cruise-cera-ipad ...[phone-free celebs are] not trying to...","keywords":["phone","from","then","laptop","computer","life","doing","scroll","device","it\u2019s"]},{"id":"reply-automattic-ai-data-tumblr-wordpress","title":"Tumblr and WordPress to Sell Users\u2019 Data to Train AI Tools","content":"Tumblr and WordPress to Sell Users\u2019 Data to Train AI Tools \u2192 https://www.404media.co/tumblr-and-wordpress-to-sell-users-data-to-train-ai-tools/ Tumblr and WordPress.com are preparing to sell user data to Midjourney and OpenAI, according to a source with internal knowledge about the deals and internal documentation referring to the deals The internal documentation details a messy and controversial process within Tumblr itself. One internal post made by Cyle Gage, a product manager at Tumblr, states that a query made to prepare data for OpenAI and Midjourney compiled a huge number of user posts that it wasn\u2019t supposed to. It is not clear from Gage\u2019s post whether this data has already been sent to OpenAI and Midjourney, or whether Gage was detailing a process for scrubbing the data before it was to be sent. I generally enjoy what Automattic does for the web as a whole. However, if these claims are true, it\u0027s unfortunate. I believe there\u0027s a way to opt-out, but I\u0027d love to learn more before jumping to conclusions. That said, WordPress (.com not .org) and Tumblr are platforms just like Reddit, Twitter, and the Meta set of offerings. I\u0027m sure somewhere in their Terms of Service, there\u0027s some clauses around their ownserhip of the data you published on their platforms and just like it\u0027s sold to data brokers and advertisers, they can also sell it to companies training AI models. To counter these types of moves from platforms, I wish it were as easy as saying \u0026quot;build your own platform\u0026quot; . Doing so can be as \u0026quot;simple\u0026quot; as setting up a website using your own domain . Unfortunately, today, it\u0027s still not as easy to do and one of the products / companies that help you do that is WordPress. I think it\u0027s important though to note the distinction there between WordPress the company and WordPress the technology. Another piece that complicates building your own site is, there\u0027s still other ways for companies training AI models to use data that\u0027s publicly available on the internet. These are the arguments that are currently being litigated in several legal cases. Maybe there are opportunities to explore a robots.txt for AI . AI models need high quality data that\u0027s representative and as close as possible to the real world in order to improve. There is a role here for synthetic data . High quality synthetic data is behind groundbreaking models like Microsoft\u0027s Phi . Instincts tell me that synthetic data can only go so far though and real data is still needed. In that case, as an AI consumer who makes use of these AI models, but don\u0027t want to contribute my data, do I have a responsibility to contribute my data to improve the systems that I use? Piracy aside, in some ways it reminds me of torrenting. You usually run into scenarios where there are multiple people downloading a file. However, there\u0027s only a handful of seeders who once obtained, make the file available for others to download. There are also additional considerations such as how people are compensated for contributing their data to these systems. It\u0027s important to note that this is not a new problem and people had been thinking about this though in different contexts. Maybe it\u0027s time to reconsider ideas like inverse privacy and data dignity . There are no clear answers here and there are a lot of things to consider. However, it\u0027s comforting that as a society we\u0027re having these conversations.","contentType":"reply","url":"https://www.lqdev.me/responses/automattic-ai-data-tumblr-wordpress","date":"2024-02-27 21:09 -05:00","tags":["tumblr","automattic","wordpress","ai"],"summary":"Tumblr and WordPress to Sell Users\u2019 Data to Train AI Tools \u2192 https://www.404media.co/tumblr-and-wordpress-to-sell-users-data-to-train-ai-tools/ Tumblr and WordPress.com are preparing to sell user...","keywords":["data","wordpress","tumblr","models","sell","internal","though","companies","however","synthetic"]},{"id":"reply-copilot-android-update-mourn-cortana-death","title":"The latest Microsoft Copilot update on Android makes me mourn the death of Cortana","content":"The latest Microsoft Copilot update on Android makes me mourn the death of Cortana \u2192 https://www.windowscentral.com/software-apps/the-latest-microsoft-copilot-update-on-android-makes-me-mourn-the-death-of-cortana Microsoft Copilot will soon be able to be your default assistant app on Android. It\u0027s a shame that Cortana never worked out for Microsoft. If things had lined up differently, we may have seen Copilot gain access to smart devices and commands like Gemini has with Google Assistant (though that setup isn\u0027t perfect). While Copilot has a place on a computer, I think an assistant on your smartphone needs to be able to do more day-to-day tasks. So many things were ahead of their time. I just want a Windows Mobile PC (Windows Phone?) with an LLM-backed Cortana .","contentType":"reply","url":"https://www.lqdev.me/responses/copilot-android-update-mourn-cortana-death","date":"2024-02-27 21:04 -05:00","tags":["cortana","copilot","ai","microsoft","virtualassistant","assistant","chatbot"],"summary":"The latest Microsoft Copilot update on Android makes me mourn the death of Cortana \u2192 https://www.windowscentral.com/software-apps/the-latest-microsoft-copilot-update-on-android-makes-me-mourn-the-deat...","keywords":["copilot","microsoft","cortana","android","assistant","latest","things","windows","able","mourn"]},{"id":"reshare-verge-physical-media-week","title":"Physical media week - Exploring the importance of discs and cartridges in an increasingly digital age","content":"Physical media week - Exploring the importance of discs and cartridges in an increasingly digital age \u2192 https://www.theverge.com/entertainment/24054458/physical-media-preservation-discs-cartridges-digital The bright promise of streaming and digital stores has given way to a darker reality: we rarely have ownership over the art we love, and much is getting lost in the process. Only a fraction of movies released over the last century are available on streaming services, while a staggering 90 percent of classic video games are considered \u201Ccritically endangered\u201D by archivists. As these platforms continue to dominate the media landscape, a whole lot of cultural history is being abandoned. In this special issue, The Verge will explore how physical media factors into this and its importance in keeping art alive and accessible. That could mean boutique publishers releasing beautiful special editions of games and movies, foundations dedicated to preserving the physical history of video games, or musicians releasing their latest albums on floppy discs. We\u2019ll also be looking at some cautionary tales in the shift to subscription services and offering tips on building bookshelf-worthy collections. Cartridges and discs have been hurtling toward obsolescence \u2014 but it turns out, they may be more important than ever.","contentType":"reshare","url":"https://www.lqdev.me/responses/verge-physical-media-week","date":"2024-02-27 12:21 -05:00","tags":["media","verge","disc","dvd","cd","floppy","tech","technology"],"summary":"Physical media week - Exploring the importance of discs and cartridges in an increasingly digital age \u2192...","keywords":["media","discs","physical","games","cartridges","digital","video","releasing","services","movies"]},{"id":"reshare-jre-wherever-you-get-your-podcasts","title":"JRE - Now available wherever you get your podcasts","content":"JRE - Now available wherever you get your podcasts \u2192 https://joeroganexp.libsyn.com/rss Since Joe Rogan went exclusive with Spotify, I maybe listened to a handful of episodes. The main reason for it is, I don\u0027t use Spotify to listen to podcasts. Periodically, I\u0027d scroll through the feed to see if he had any intereting guests on. As part of his new contract, the podcast is now available on other platforms. That means you can listen wherever you get your podcasts .","contentType":"reshare","url":"https://www.lqdev.me/responses/jre-wherever-you-get-your-podcasts","date":"2024-02-26 18:29 -05:00","tags":["rss","podcast","protocols","jre","joerogan","audio","spotify"],"summary":"JRE - Now available wherever you get your podcasts \u2192 https://joeroganexp.libsyn.com/rss Since Joe Rogan went exclusive with Spotify, I maybe listened to a handful of episodes. The main reason for it...","keywords":["podcasts","available","wherever","spotify","listen","podcast","contract","part","guests","intereting"]},{"id":"reply-streaming-fatigue-collecting-dvds","title":"My streaming fatigue got so bad I started collecting DVDs","content":"My streaming fatigue got so bad I started collecting DVDs \u2192 https://www.theverge.com/24044151/streaming-subscription-prices-dvd-collection After spending years reassuring myself that I don\u2019t need physical copies of movies because of streaming, DVDs have officially reentered my life. Walmart...Thrift stores, flea markets, the library, and even my local mall\u2019s FYE have also become places I frequent to get my hands on oft-ignored discs. It makes sense to subscribe to all these services if you\u2019re into the exclusive content on each one and have the patience to sift through their massive libraries. However, all I\u2019ve been watching lately is the junk on Discovery Plus, simply because I\u2019m too tired to find anything else \u2014 especially when the extremely specific shows and movies I want to watch keep switching services or just aren\u2019t available. One of the most devastating examples of this was when both The Office and Parks and Recreation moved from Netflix to Peacock, disrupting the casual binge-watching sessions that I would default to when I was done with work. Within the past year, nearly every streaming service has raised its prices, including Netflix, Disney Plus, Hulu, Paramount Plus, Discovery Plus, and Apple TV Plus. I\u2019m not saying DVDs are flawless: there\u2019s a reason no one wants them anymore! Despite this, it\u2019s still nice to have something that you physically own and don\u2019t even need an internet connection to use. So when Best Buy confirmed it would stop selling DVDs this year and rumors emerged that Walmart would do the same, I was pretty disappointed. I can\u2019t imagine Walmart without its bin of DVDs, nor can I even see Best Buy without its already-shrunken selection of movies. It\u2019s 2024, and I\u2019m not ready to say goodbye to DVDs \u2014 in fact, I\u2019m just getting started. Great article from Emma. Personally, I\u0027ve been doing the same. Just a few weekends ago, I got something like 8-10 DVDs for ~$25 at my local thrift store. That haul included 3 seasons of The Sopranos. With streaming services taking back control of their content and putting it on their own platforms, I don\u0027t want to have to keep signing up for a new service just to watch the shows and movies I enjoy. Also, that\u0027s assuming you can find the content to begin with (i.e. Westworld). Parks and Recreation, The Office, and Breaking Bad were some of the first shows I started collecting and have slowly been building up my collection. To save on space, I\u0027ve ditched the cases and have the DVDs organized in a CD case. I\u0027ve not only limited myself to DVDs but have also started collecting CDs as well. Whenever I want variety, I just use one of the free streaming services like: Freevee Pluto TV Tubi YouTube Yes, there are ads but at least I\u0027m not paying for it and I know that\u0027s part of the deal. There are a ton of good older (and sometimes original) TV shows and movies on those platforms to keep me entertained. The most recent ones being Stargate and Vampire\u0027s Kiss .","contentType":"reply","url":"https://www.lqdev.me/responses/streaming-fatigue-collecting-dvds","date":"2024-02-26 15:44 -05:00","tags":["streaming","media","dvds","movies","tv","physicalmedia"],"summary":"My streaming fatigue got so bad I started collecting DVDs \u2192 https://www.theverge.com/24044151/streaming-subscription-prices-dvd-collection After spending years reassuring myself that I don\u2019t need...","keywords":["dvds","streaming","movies","plus","started","services","shows","also","content","walmart"]},{"id":"reshare-announcing-mistral-large","title":"Announcing Mistral Large","content":"Announcing Mistral Large \u2192 https://mistral.ai/news/mistral-large/ Mistral Large is our flagship model, with top-tier reasoning capacities. It is also available on Azure. Mistral Large comes with new capabilities and strengths: It is natively fluent in English, French, Spanish, German, and Italian, with a nuanced understanding of grammar and cultural context. Its 32K tokens context window allows precise information recall from large documents. Its precise instruction-following enables developers to design their moderation policies \u2013 we used it to set up the system-level moderation of le Chat. It is natively capable of function calling. This, along with constrained output mode, implemented on la Plateforme, enables application development and tech stack modernisation at scale. At Mistral, our mission is to make frontier AI ubiquitous. This is why we\u2019re announcing today that we\u2019re bringing our open and commercial models to Azure.","contentType":"reshare","url":"https://www.lqdev.me/responses/announcing-mistral-large","date":"2024-02-26 15:39 -05:00","tags":["ai","mistral","llm","opensource","azure"],"summary":"Announcing Mistral Large \u2192 https://mistral.ai/news/mistral-large/ Mistral Large is our flagship model, with top-tier reasoning capacities. It is also available on Azure. Mistral Large comes with new...","keywords":["mistral","large","announcing","moderation","precise","context","natively","azure","enables","we\u2019re"]},{"id":"notes-may-ninth-khruangbin","title":"New Single - May Ninth (Khruangbin)","content":"Another banger ahead of the new album, A La Sala .","contentType":"notes","url":"https://www.lqdev.me/notes/may-ninth-khruangbin","date":"2024-02-25 16:36 -05:00","tags":["music","newmusic","khruangbin"],"summary":"Another banger ahead of the new album, A La Sala .","keywords":["another","banger","ahead","album","sala"]},{"id":"notes-newsblur-maintenace-mode-022024","title":"Now where will I get my RSS fix?","content":"Good thing I have elfeed as my backup so I can use that in the meantime.","contentType":"notes","url":"https://www.lqdev.me/notes/newsblur-maintenace-mode-022024","date":"2024-02-25 14:18 -05:00","tags":["newsblur","rss","maintenance","emacs","elfeed","news"],"summary":"Good thing I have elfeed as my backup so I can use that in the meantime.","keywords":["good","thing","elfeed","backup","meantime"]},{"id":"notes-vampires-kiss-movie","title":"Vampire\u0027s Kiss (1988)","content":"I have no idea what I just watched, but I liked it. I thought Nicolas Cage making weird movies was a relatively recent thing, but I guess it goes way back to the 80s. I kept getting thrown off by his accent, which I only noticed about a quarter into the movie and even later into the movie sometimes it wasn\u0027t there. His descent and transformation into \u0026quot;Nosferatu\u0026quot; was interesting to watch. Mostly funny, but sometimes just weird and disturbing. If you\u0027re interested, it\u0027s currently streaming for free on Tubi","contentType":"notes","url":"https://www.lqdev.me/notes/vampires-kiss-movie","date":"2024-02-24 22:04 -05:00","tags":["movie","80s","horror","vampire"],"summary":"I have no idea what I just watched, but I liked it. I thought Nicolas Cage making weird movies was a relatively recent thing, but I guess it goes way back to the 80s. I kept getting thrown off by his...","keywords":["sometimes","weird","movie","idea","even","later","wasn","descent","transformation","\u0026quot"]},{"id":"reshare-direct-influence-twin-peaks-zelda","title":"The direct influence of Twin Peaks on Zelda","content":"The direct influence of Twin Peaks on Zelda \u2192 https://www.theverge.com/24078662/twin-peaks-zelda-links-awakening-influence In a 2010 interview, Link\u2019s Awakening director Takashi Tezuka revealed the inspiration for this memorably bizarre cast of characters. \u201CAt the time, Twin Peaks was rather popular. The drama was all about a small number of characters in a small town,\u201D Tezuka said. \u201CSo I wanted to make something like that, while it would be small enough in scope to easily understand, it would have deep and distinctive characteristics.\u201D ... [Mark] Frost reveals in an interview with The Verge, he actually spoke with Nintendo about the Zelda franchise. \u201CI don\u2019t want to overstate it. It was a single conversation. But it was fun,\u201D he tells me. \u201CThey were talking to me about a Twin Peaks game, and they mentioned Zelda at the time,\u201D says Frost. \u201CThey said, \u2018One of the things we love about your show is how there\u2019s all sorts of sideways associations that can drive the story forward.\u2019 They asked me about that as they were thinking about expanding the Zelda universe.\u201D Though he\u2019d never played a Zelda game, Frost had enough experience with fantasy storytelling that he had some suggestions. \u201CI\u2019d played lots of Dungeons \u0026amp; Dragons when I was young, so I was familiar with the kind of story they were thinking about,\u201D he says. \u201CI think I said, \u2018Don\u2019t be afraid to use dreamlike, Jungian symbolism. Things can connect thematically without having to connect concretely.\u2019 It was things like that that I was urging them [to consider].\u201D","contentType":"reshare","url":"https://www.lqdev.me/responses/direct-influence-twin-peaks-zelda","date":"2024-02-24 12:14 -05:00","tags":["twinpeaks","nintendo","zelda","tv","culture","gaming"],"summary":"The direct influence of Twin Peaks on Zelda \u2192 https://www.theverge.com/24078662/twin-peaks-zelda-links-awakening-influence In a 2010 interview, Link\u2019s Awakening director Takashi Tezuka revealed the...","keywords":["zelda","peaks","they","said","twin","things","frost","small","game","\u201Cthey"]},{"id":"reshare-gpt-500-lines-sql","title":"GPT in 500 lines of SQL","content":"GPT in 500 lines of SQL \u2192 https://explainextended.com/2023/12/31/happy-new-year-15/ GitHub Repo","contentType":"reshare","url":"https://www.lqdev.me/responses/gpt-500-lines-sql","date":"2024-02-24 12:06 -05:00","tags":["sql","gpt","ai","llm"],"summary":"GPT in 500 lines of SQL \u2192 https://explainextended.com/2023/12/31/happy-new-year-15/ GitHub Repo","keywords":["lines","https://explainextended","com/2023/12/31/happy","year","github","repo"]},{"id":"star-ali-frazier-I","title":"Ali vs. Frazier I","content":"Ali vs. Frazier I \u2192 https://www.youtube.com/watch?v=eIm2eK5uuVA Such a good fight. Frazier had no defense but just kept coming forward.","contentType":"star","url":"https://www.lqdev.me/responses/ali-frazier-I","date":"2024-02-24 00:19 -05:00","tags":["boxing","ali","frazier"],"summary":"Ali vs. Frazier I \u2192 https://www.youtube.com/watch?v=eIm2eK5uuVA Such a good fight. Frazier had no defense but just kept coming forward.","keywords":["frazier","https://www","youtube","com/watch","v=eim2ek5uuva","good","fight","defense","kept","coming"]},{"id":"reply-mcdonalds-ai-drive-thru-nvidia","title":"Jim Cramer says McDonald\u2019s embracing AI at drive-thrus is good news for Nvidia","content":"Jim Cramer says McDonald\u2019s embracing AI at drive-thrus is good news for Nvidia \u2192 https://www.cnbc.com/2024/02/23/jim-cramer-mcdonalds-use-of-ai-at-drive-thrus-is-good-news-for-nvidia.html I wish they\u0027d use AI to keep the ice cream machines from breaking instead.","contentType":"reply","url":"https://www.lqdev.me/responses/mcdonalds-ai-drive-thru-nvidia","date":"2024-02-24 00:02 -05:00","tags":["mcdonalds","ai","nvidia"],"summary":"Jim Cramer says McDonald\u2019s embracing AI at drive-thrus is good news for Nvidia \u2192 https://www.cnbc.com/2024/02/23/jim-cramer-mcdonalds-use-of-ai-at-drive-thrus-is-good-news-for-nvidia.html I wish...","keywords":["cramer","drive","thrus","good","news","nvidia","from","machines","cream","keep"]},{"id":"star-salami-rose-joe-louis-live-bandcamp-2023","title":"Salami Rose Joe Louis Live Band at Bandcamp - May 2023","content":"Salami Rose Joe Louis Live Band at Bandcamp - May 2023 \u2192 https://www.youtube.com/watch?v=VrnEQ3TqZGE Great performance. I remember the first time I listened to Salami was when she opened for Flying Lotus and she was amazing.","contentType":"star","url":"https://www.lqdev.me/responses/salami-rose-joe-louis-live-bandcamp-2023","date":"2024-02-22 22:38 -05:00","tags":["brainfeeder","salamirosejoelouis","music","bandcamp"],"summary":"Salami Rose Joe Louis Live Band at Bandcamp - May 2023 \u2192 https://www.youtube.com/watch?v=VrnEQ3TqZGE Great performance. I remember the first time I listened to Salami was when she opened for Flying...","keywords":["salami","flying","opened","listened","remember","performance","v=vrneq3tqzge","com/watch","youtube","https://www"]},{"id":"reshare-bluesky-open-social-web","title":"Bluesky: An Open Social Web","content":"Bluesky: An Open Social Web \u2192 https://bsky.social/about/blog/02-22-2024-open-social-web Today, we\u2019re excited to announce that the Bluesky network is federating and opening up in a way that allows you to host your own data. What does this mean? Your data, such as your posts, likes, and follows, needs to be stored somewhere. With traditional social media, your data is stored by the social media company whose services you\u0027ve signed up for. If you ever want to stop using that company\u0027s services, you can do that\u2014but you would have to leave that social network and lose your existing connections. It doesn\u0027t have to be this way! An alternative model is how the internet itself works. Anyone can put up a website on the internet We think social media should work the same way. When you register on Bluesky, by default we\u0027ll suggest that Bluesky will store your data. But if you\u0027d like to let another company store it, or even store it yourself, you can do that. You\u0027ll also be able to change your mind at any point, moving your data to another provider without losing any of your existing posts, likes, or follows. From your followers\u0027 perspective, your profile is always available at your handle\u2014no matter where your information is actually stored, or how many times it has been moved. I don\u0027t spend a lot of time on Bluesky, but I love what they\u0027re doing. They now: Made their service generally available . Have over 5 million users . Support RSS Now you can self-host your data. I\u0027m excited. The other piece of this that\u0027s interesting is their feature which enables you to use your domain as a custom handle . Not only is your identity portable, but also your data. I\u0027d be interested to see how this works in practice given you can already do some of this on the Fediverse on platforms like Mastodon. ... identity component to me is crucial. That\u0027s one of the challenges with Mastodon today. While you can move instances, your identity changes and while most of your data comes with you, there\u0027s tihngs that still dont transfer over. The other part that I\u0027d be interested in seeing is whether or not they can be efficient in the storage of federated data. One of the challenges with Mastodon is, your server quickly fills up because of data from other instances (when you federate). It\u0027s gotten better , but this is an area I spend most of my time when maintaining my own self-hosted instance. I\u0027m excited to tinker and self-host my own data. Maybe I\u0027ll also syndicate to Bluesky just like I do with Mastodon today . In the meantime, you can find me on Bluesky @lqdev.me .","contentType":"reshare","url":"https://www.lqdev.me/responses/bluesky-open-social-web","date":"2024-02-22 21:03 -05:00","tags":["bluesky","socialmedia","web","internet","community"],"summary":"Bluesky: An Open Social Web \u2192 https://bsky.social/about/blog/02-22-2024-open-social-web Today, we\u2019re excited to announce that the Bluesky network is federating and opening up in a way that allows you...","keywords":["data","social","bluesky","mastodon","store","also","company","media","they","stored"]},{"id":"reshare-stable-diffusion-3-early-preview","title":"Stable Diffusion 3 - Early Preview","content":"Stable Diffusion 3 - Early Preview \u2192 https://stability.ai/news/stable-diffusion-3 Announcing Stable Diffusion 3 in early preview, our most capable text-to-image model with greatly improved performance in multi-subject prompts, image quality, and spelling abilities. The Stable Diffusion 3 suite of models currently range from 800M to 8B parameters. Stable Diffusion 3 combines a diffusion transformer architecture and flow matching.","contentType":"reshare","url":"https://www.lqdev.me/responses/stable-diffusion-3-early-preview","date":"2024-02-22 20:59 -05:00","tags":["stabilityai","ai","stablediffusion","imagegeneration","ml","image","genai","generativeai"],"summary":"Stable Diffusion 3 - Early Preview \u2192 https://stability.ai/news/stable-diffusion-3 Announcing Stable Diffusion 3 in early preview, our most capable text-to-image model with greatly improved...","keywords":["diffusion","stable","early","preview","image","suite","models","currently","range","from"]},{"id":"bookmarks-ai-study-guide-azure-ml-edition","title":"The AI Study Guide: Azure Machine Learning Edition","content":"The AI Study Guide: Azure Machine Learning Edition \u2192 https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/the-ai-study-guide-azure-machine-learning-edition/ba-p/4063656 The AI Study Guide: Discover Machine Learning with these free Azure resources Welcome to the February edition of the Azure AI Study Guide. Every month I\u2019ll be spilling the tea on the best and newest tools for skilling up on Azure AI. This month we\u2019re putting on our thinking caps to investigate Azure Machine Learning (ML). I\u2019ll give you a quick breakdown of what it is, then we\u2019ll explore a four-week roadmap of our top FREE resources for you to continue your AI learning journey! And as a bonus, stay tuned to the end to see what makes machine learning and generative AI a dynamic duo.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/ai-study-guide-azure-ml-edition","date":"2024-02-21 15:18 -05:00","tags":["azure","tutorial","machinelearning","ml","ai","azureml"],"summary":"The AI Study Guide: Azure Machine Learning Edition \u2192 https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/the-ai-study-guide-azure-machine-learning-edition/ba-p/4063656 The AI Study...","keywords":["azure","learning","machine","study","month","i\u2019ll","guide","free","resources","edition"]},{"id":"star-on-blogs-searls","title":"On Blogs","content":"On Blogs \u2192 https://doc.searls.com/2024/02/21/on-blogs/ Thoughts I jotted down on Mastodon: Blogs are newsletters that don\u2019t require subscriptions. Blogrolls are lists of blogs. Both require the lowest possible cognitive and economic overhead. That\u2019s why they are coming back. I know, they never left. But you get my point.","contentType":"star","url":"https://www.lqdev.me/responses/on-blogs-searls","date":"2024-02-21 15:14 -05:00","tags":["blogs","writing","newsletters","blogroll"],"summary":"On Blogs \u2192 https://doc.searls.com/2024/02/21/on-blogs/ Thoughts I jotted down on Mastodon: Blogs are newsletters that don\u2019t require subscriptions. Blogrolls are lists of blogs. Both require the...","keywords":["blogs","they","require","never","back","coming","that\u2019s","overhead","economic","cognitive"]},{"id":"reply-windows-12-mobile-concept","title":"Windows 12 Mobile - Concept","content":"Windows 12 Mobile - Concept \u2192 https://www.windowscentral.com/phones/windows-phone/what-would-microsofts-windows-phone-look-like-in-2024-its-like-a-micro-pc-running-windows-12-in-your-pocket Cool concept. The likelihood of it happening is low, but there\u0027s a lot of really great opportunities here especially with the new wave or ARM PCs coming . I don\u0027t know what the device form factor looks like, but I wouldn\u0027t mind carrying around a pocket PC - a true mobile computer. Already with the Windows Store, you have access to tons of apps. For the apps that aren\u0027t in the Store, there\u0027s the browser. That seemed to be good enough for Apple\u0027s Vision Pro . Taking it a step further would the app gap matter as much if you have Copilot as your concierge orchestrating tasks for you using the various services? Better yet, what if these services had their own assistants / GPTs Copilot could talk to and coordinate on your behalf? At some point, I might just use OpenAI\u0027s Sora model to live vicariously through an AI-generated video depicting this alternate reality where Windows Phone exists...","contentType":"reply","url":"https://www.lqdev.me/responses/windows-12-mobile-concept","date":"2024-02-21 14:59 -05:00","tags":["windows","mobile","windowsphone","pc"],"summary":"Windows 12 Mobile - Concept \u2192 https://www.windowscentral.com/phones/windows-phone/what-would-microsofts-windows-phone-look-like-in-2024-its-like-a-micro-pc-running-windows-12-in-your-pocket Cool...","keywords":["windows","services","copilot","store","pocket","phone","apps","concept","mobile","microsofts"]},{"id":"reshare-gemini-pro-video-willison","title":"The killer app of Gemini Pro 1.5 is video","content":"The killer app of Gemini Pro 1.5 is video \u2192 https://simonwillison.net/2024/Feb/21/gemini-pro-video/ I\u2019ve been playing with Gemini Pro 1.5 for a few days, and I think the most exciting feature isn\u2019t so much the token count... it\u2019s the ability to use video as an input. The ability to extract structured content from text is already one of the most exciting use-cases for LLMs. GPT-4 Video and LLaVA expanded that to images. And now Gemini Pro 1.5 expands that to video. The ability to analyze video like this feels SO powerful. Being able to take a 20 second video of a bookshelf and get back a JSON array of those books is just the first thing I thought to try. ...as always with modern AI, there are still plenty of challenges to overcome...But this really does feel like another one of those glimpses of a future that\u2019s suddenly far closer then I expected it to be.","contentType":"reshare","url":"https://www.lqdev.me/responses/gemini-pro-video-willison","date":"2024-02-21 14:45 -05:00","tags":["google","ai","gemini"],"summary":"The killer app of Gemini Pro 1.5 is video \u2192 https://simonwillison.net/2024/Feb/21/gemini-pro-video/ I\u2019ve been playing with Gemini Pro 1.5 for a few days, and I think the most exciting feature isn\u2019t...","keywords":["video","ability","gemini","those","exciting","always","thought","thing","books","array"]},{"id":"reshare-huggingchat","title":"HuggingChat","content":"HuggingChat \u2192 https://huggingface.co/chat/assistants The goal of this app is to showcase that it is now possible to build an open source alternative to ChatGPT. GitHub repo Text Generation Inference","contentType":"reshare","url":"https://www.lqdev.me/responses/huggingchat","date":"2024-02-21 14:38 -05:00","tags":["huggingface","assistants","ai","chat","opensource"],"summary":"HuggingChat \u2192 https://huggingface.co/chat/assistants The goal of this app is to showcase that it is now possible to build an open source alternative to ChatGPT. GitHub repo Text Generation Inference","keywords":["huggingchat","https://huggingface","co/chat/assistants","goal","showcase","possible","build","open","source","alternative"]},{"id":"reshare-gemma-google-ai-models","title":"Gemma: Introducing new state-of-the-art open models","content":"Gemma: Introducing new state-of-the-art open models \u2192 https://blog.google/technology/developers/gemma-open-models/ Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. Developed by Google DeepMind and other teams across Google, Gemma is inspired by Gemini, and the name reflects the Latin gemma, meaning \u201Cprecious stone.\u201D Accompanying our model weights, we\u2019re also releasing tools to support developer innovation, foster collaboration, and guide responsible use of Gemma models. HuggingFace Gemma Release","contentType":"reshare","url":"https://www.lqdev.me/responses/gemma-google-ai-models","date":"2024-02-21 14:30 -05:00","tags":["ai","opensource","google","model","llm"],"summary":"Gemma: Introducing new state-of-the-art open models \u2192 https://blog.google/technology/developers/gemma-open-models/ Gemma is a family of lightweight, state-of-the-art open models built from the same...","keywords":["gemma","models","open","state","google","gemini","gemma:","\u201Cprecious","stone","accompanying"]},{"id":"reshare-localfirstfm-podcast","title":"Podcast - localfirst.fm","content":"Podcast - localfirst.fm \u2192 https://www.localfirst.fm/ A podcast about local-first software development","contentType":"reshare","url":"https://www.lqdev.me/responses/localfirstfm-podcast","date":"2024-02-20 21:38 -05:00","tags":["development","software","podcast","tech","podcast","localfirst","local"],"summary":"Podcast - localfirst.fm \u2192 https://www.localfirst.fm/ A podcast about local-first software development","keywords":["podcast","localfirst","https://www","local","software","development"]},{"id":"reshare-signal-usernames","title":"Keep your phone number private with Signal usernames","content":"Keep your phone number private with Signal usernames \u2192 https://signal.org/blog/phone-number-privacy-usernames/ Signal\u2019s mission and sole focus is private communication. For years, Signal has kept your messages private, your profile information (like your name and profile photo) private, your contacts private, and your groups private \u2013 among much else. Now we\u2019re taking that one step further, by making your phone number on Signal more private. Here\u2019s how: New default: Your phone number will no longer be visible to everyone in Signal... Connect without sharing your phone number... Control who can find you on Signal by phone number... Right now, these options are in beta, and will be rolling out to everyone in the coming weeks.","contentType":"reshare","url":"https://www.lqdev.me/responses/signal-usernames","date":"2024-02-20 17:01 -05:00","tags":["signal","privacy","security","osint","messaging"],"summary":"Keep your phone number private with Signal usernames \u2192 https://signal.org/blog/phone-number-privacy-usernames/ Signal\u2019s mission and sole focus is private communication. For years, Signal has kept...","keywords":["private","number","phone","signal","everyone","profile","step","further","making","more"]},{"id":"reshare-cosmopedia-ai-synthetic-dataset","title":"Cosmopedia v0.1","content":"Cosmopedia v0.1 \u2192 https://huggingface.co/datasets/HuggingFaceTB/cosmopedia Cosmopedia is a dataset of synthetic textbooks, blogposts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1.The dataset contains over 30 million files and 25 billion tokens, making it the largest open synthetic dataset to date. It covers a variety of topics; we tried to map world knowledge present in Web datasets like RefinedWeb and RedPajama, and generate synthetic content that covers them. This is the v0.1 of Cosmopedia, with ample room for improvement and topics to be more comprehensively covered. We hope this dataset will help the community\u0027s research efforts in the increasingly intriguing domain of synthetic data. This work is inspired by the great work of Phi1.5.","contentType":"reshare","url":"https://www.lqdev.me/responses/cosmopedia-ai-synthetic-dataset","date":"2024-02-20 16:50 -05:00","tags":["cosmopedia","dataset","huggingface","mixtral","ai","genai","llm"],"summary":"Cosmopedia v0.1 \u2192 https://huggingface.co/datasets/HuggingFaceTB/cosmopedia Cosmopedia is a dataset of synthetic textbooks, blogposts, stories, posts and WikiHow articles generated by...","keywords":["synthetic","dataset","cosmopedia","covers","topics","textbooks","datasets","refinedweb","redpajama","generate"]},{"id":"reshare-swift-mlx-ml-research","title":"MLX Swift - On-device ML research with MLX and Swift","content":"MLX Swift - On-device ML research with MLX and Swift \u2192 https://www.swift.org/blog/mlx-swift/ The Swift programming language has a lot of potential to be used for machine learning research because it combines the ease of use and high-level syntax of a language like Python with the speed of a compiled language like C\u002B\u002B. MLX is an array framework for machine learning research on Apple silicon. MLX is intended for research and not for production deployment of models in apps. MLX Swift expands MLX to the Swift language, making experimentation on Apple silicon easier for ML researchers.","contentType":"reshare","url":"https://www.lqdev.me/responses/swift-mlx-ml-research","date":"2024-02-20 16:25 -05:00","tags":["apple","mlx","swift","opensource","ml","ai"],"summary":"MLX Swift - On-device ML research with MLX and Swift \u2192 https://www.swift.org/blog/mlx-swift/ The Swift programming language has a lot of potential to be used for machine learning research because it...","keywords":["swift","research","language","apple","silicon","learning","machine","framework","intended","deployment"]},{"id":"reshare-ollama-windows-preview","title":"Ollama - Windows Preview","content":"Ollama - Windows Preview \u2192 https://ollama.com/blog/windows-preview Ollama is now available on Windows in preview, making it possible to pull, run and create large language models in a new native Windows experience. Ollama on Windows includes built-in GPU acceleration, access to the full model library, and the Ollama API including OpenAI compatibility. Download (https://ollama.com/download/windows)","contentType":"reshare","url":"https://www.lqdev.me/responses/ollama-windows-preview","date":"2024-02-19 16:13 -05:00","tags":["ollama","windows","llm","opensource","localmodels","ml","ai"],"summary":"Ollama - Windows Preview \u2192 https://ollama.com/blog/windows-preview Ollama is now available on Windows in preview, making it possible to pull, run and create large language models in a new native...","keywords":["ollama","windows","preview","https://ollama","compatibility","openai","including","library","model","full"]},{"id":"star-staticrypt-protected-posts-static-site-jayless","title":"StatiCrypt: Protected Posts on a Static Site","content":"StatiCrypt: Protected Posts on a Static Site \u2192 https://www.jayeless.net/2024/02/staticrypt.html ...I was longing for a way to do friends-only blog posts on the open web, today I came across Stati\u00ADCrypt , an open-source utility that lets you encrypt static HTML pages behind a password.","contentType":"star","url":"https://www.lqdev.me/responses/staticrypt-protected-posts-static-site-jayless","date":"2024-02-17 12:27 -05:00","tags":["staticsite","web","blogs","community","openweb","security","privacy"],"summary":"StatiCrypt: Protected Posts on a Static Site \u2192 https://www.jayeless.net/2024/02/staticrypt.html ...I was longing for a way to do friends-only blog posts on the open web, today I came across...","keywords":["posts","static","html","open","staticrypt:","pages","encrypt","lets","utility","source"]},{"id":"bookmarks-open-source-ai-cookbook-hf","title":"HuggingFace - Open Source AI Cookbook","content":"HuggingFace - Open Source AI Cookbook \u2192 https://huggingface.co/learn/cookbook/index The Open-Source AI Cookbook is a collection of notebooks illustrating practical aspects of building AI applications and solving various machine learning tasks using open-source tools and models.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/open-source-ai-cookbook-hf","date":"2024-02-16 22:36 -05:00","tags":["ai","tutorial","huggingface","opensource","ml"],"summary":"HuggingFace - Open Source AI Cookbook \u2192 https://huggingface.co/learn/cookbook/index The Open-Source AI Cookbook is a collection of notebooks illustrating practical aspects of building AI applications...","keywords":["open","source","cookbook","huggingface","using","tasks","learning","machine","various","solving"]},{"id":"reshare-observable-2-0","title":"Observable 2.0 - Announcing Observable Framework","content":"Observable 2.0 - Announcing Observable Framework \u2192 https://observablehq.com/blog/observable-2-0 Today we\u2019re launching Observable 2.0 with a bold new vision: an open-source static site generator for building fast, beautiful data apps, dashboards, and reports. Our mission is to help teams communicate more effectively with data. Effective presentation of data is critical for deep insight, nuanced understanding, and informed decisions. Observable notebooks are great for ephemeral, ad hoc data exploration. But notebooks aren\u2019t well-suited for polished dashboards and apps.","contentType":"reshare","url":"https://www.lqdev.me/responses/observable-2-0","date":"2024-02-15 20:48 -05:00","tags":["data","analytics","observable"],"summary":"Observable 2.0 - Announcing Observable Framework \u2192 https://observablehq.com/blog/observable-2-0 Today we\u2019re launching Observable 2.0 with a bold new vision: an open-source static site generator for...","keywords":["observable","data","apps","notebooks","dashboards","teams","communicate","more","effectively","effective"]},{"id":"reshare-v-jepa-meta","title":"V-JEPA: The next step toward Yann LeCun\u2019s vision of advanced machine intelligence (AMI)","content":"V-JEPA: The next step toward Yann LeCun\u2019s vision of advanced machine intelligence (AMI) \u2192 https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/ Today, we\u2019re publicly releasing the Video Joint Embedding Predictive Architecture (V-JEPA) model, a crucial step in advancing machine intelligence with a more grounded understanding of the world. This early example of a physical world model excels at detecting and understanding highly detailed interactions between objects. In the spirit of responsible open science, we\u2019re releasing this model under a Creative Commons NonCommercial license for researchers to further explore.","contentType":"reshare","url":"https://www.lqdev.me/responses/v-jepa-meta","date":"2024-02-15 20:20 -05:00","tags":["meta","ai","ami","ml","opensource","multimodal"],"summary":"V-JEPA: The next step toward Yann LeCun\u2019s vision of advanced machine intelligence (AMI) \u2192 https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/ Today,...","keywords":["model","video","understanding","step","embedding","yann","joint","world","machine","intelligence"]},{"id":"bookmarks-magic-dev","title":"Magic.dev","content":"Magic.dev \u2192 https://magic.dev/ Magic is working on frontier-scale code models to build a coworker, not just a copilot.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/magic-dev","date":"2024-02-15 20:18 -05:00","tags":["ai","copilot","magic","code","agi"],"summary":"Magic.dev \u2192 https://magic.dev/ Magic is working on frontier-scale code models to build a coworker, not just a copilot.","keywords":["magic","https://magic","dev/","working","frontier","scale","code","models","build","coworker"]},{"id":"reshare-openai-sora","title":"OpenAI Sora - Creating video from text","content":"OpenAI Sora - Creating video from text \u2192 https://openai.com/sora Sora is an AI model that can create realistic and imaginative scenes from text instructions. Sora can generate videos up to a minute long while maintaining visual quality and adherence to the user\u2019s prompt.","contentType":"reshare","url":"https://www.lqdev.me/responses/openai-sora","date":"2024-02-15 20:15 -05:00","tags":["openai","multimodal","ai","texttovideo"],"summary":"OpenAI Sora - Creating video from text \u2192 https://openai.com/sora Sora is an AI model that can create realistic and imaginative scenes from text instructions. Sora can generate videos up to a minute...","keywords":["sora","from","text","openai","adherence","quality","visual","maintaining","while","long"]},{"id":"reshare-gemini-1-5-google","title":"Introducing Gemini 1.5","content":"Introducing Gemini 1.5 \u2192 https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/?ref=ellipsismx.com Today, we\u2019re announcing our next-generation model: Gemini 1.5. Gemini 1.5 delivers dramatically enhanced performance. It represents a step change in our approach, building upon research and engineering innovations across nearly every part of our foundation model development and infrastructure. This includes making Gemini 1.5 more efficient to train and serve, with a new Mixture-of-Experts (MoE) architecture. Gemini 1.5 Pro comes with a standard 128,000 token context window. But starting today, a limited group of developers and enterprise customers can try it with a context window of up to 1 million tokens via AI Studio and Vertex AI in private preview.","contentType":"reshare","url":"https://www.lqdev.me/responses/gemini-1-5-google","date":"2024-02-15 20:11 -05:00","tags":["ai","gemini","lm","google"],"summary":"Introducing Gemini 1.5 \u2192 https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/?ref=ellipsismx.com Today, we\u2019re announcing our next-generation model: Gemini 1.5. Gemini...","keywords":["gemini","context","next","generation","model","window","today","standard","comes","architecture"]},{"id":"reshare-club-twit-shows-available-to-all","title":"Club TWiT Shows Now Open to Everyone!","content":"Club TWiT Shows Now Open to Everyone! \u2192 https://twit.tv/posts/inside-twit/club-shows-now-open-everyone We are thrilled to announce that our Club TWiT shows are now available to everyone in audio form. That\u0027s right, you can now listen to your favorite shows anytime, anywhere, and it\u0027s all starting as early as the end of this week.","contentType":"reshare","url":"https://www.lqdev.me/responses/club-twit-shows-available-to-all","date":"2024-02-14 22:37 -05:00","tags":["twit","podcast","tech","community"],"summary":"Club TWiT Shows Now Open to Everyone! \u2192 https://twit.tv/posts/inside-twit/club-shows-now-open-everyone We are thrilled to announce that our Club TWiT shows are now available to everyone in audio...","keywords":["shows","everyone","club","twit","open","starting","anywhere","anytime","favorite","listen"]},{"id":"bookmarks-twit-lesser-known-rss-feeds","title":"TWiT\u0027s Lesser Known RSS Feeds","content":"TWiT\u0026#39;s Lesser Known RSS Feeds \u2192 https://twit.tv/posts/inside-twit/twits-lesser-known-rss-feeds Subscribed! Many people are unaware that TWiT also has RSS feeds designed for news aggregators like Feedly, NetNewsWire, Mozilla Thunderbird, and Akregator. These feeds are not meant for podcast apps but are specifically designed for news aggregators. You can copy any of the RSS feed links below into your RSS feed reader of choice and get updates on the latest TWiT blog posts, articles, or podcasts as soon as they are published.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/twit-lesser-known-rss-feeds","date":"2024-02-14 22:35 -05:00","tags":["twit","rss","tech","news","community"],"summary":"TWiT\u0026#39;s Lesser Known RSS Feeds \u2192 https://twit.tv/posts/inside-twit/twits-lesser-known-rss-feeds Subscribed! Many people are unaware that TWiT also has RSS feeds designed for news aggregators like...","keywords":["feeds","twit","lesser","known","feed","aggregators","news","designed","copy","links"]},{"id":"star-we-love-rss-ott","title":"We \u2764\uFE0F RSS","content":"We \u2764\uFE0F RSS \u2192 https://matthiasott.com/notes/we-love-rss What makes RSS so powerful is that it is an open format. RSS is one of the reasons the blogosphere grew so rapidly and it is the reason why podcasting exploded: because this open format allowed everyone to participate by simply publishing a feed anywhere on the web, without being restricted by platform requirements, closed APIs, and paywalls. And this superpower is also why RSS is having a renaissance today: it allows everyone to subscribe to, share, syndicate, and cross-post content on the open web. And it also enables creative automations using tools like Zapier, IFTTT, Huggin, or n8n. There is no denying that RSS is having a moment again. Not only because it allows us all to improve the discoverability of our work and explore online content in a personalized and deliberate way, but also because it remains one of the most powerful and influential technologies of the open web. RSS already is the cornerstone of many open technology systems like podcasting, which can\u2019t be owned and controlled by any one company. As Anil Dash notes, this alone is radical, because it is the triumph of exactly the kind of technology that\u0027s supposed to be impossible: open and empowering tech that allows people to have ownership over their work and their relationship with their audience.","contentType":"star","url":"https://www.lqdev.me/responses/we-love-rss-ott","date":"2024-02-14 22:27 -05:00","tags":["rss","web","internet","oss","openweb","indieweb"],"summary":"We \u2764\uFE0F RSS \u2192 https://matthiasott.com/notes/we-love-rss What makes RSS so powerful is that it is an open format. RSS is one of the reasons the blogosphere grew so rapidly and it is the reason why...","keywords":["open","also","allows","podcasting","having","powerful","format","content","technology","everyone"]},{"id":"reshare-verge-ai-robots-txt","title":"The text file that runs the internet","content":"The text file that runs the internet \u2192 https://www.theverge.com/24067997/robots-txt-ai-text-file-web-crawlers-spiders For three decades, a tiny text file has kept the internet from chaos. This text file has no particular legal or technical authority, and it\u2019s not even particularly complicated. It represents a handshake deal between some of the earliest pioneers of the internet to respect each other\u2019s wishes and build the internet in a way that benefitted everybody. It\u2019s a mini constitution for the internet, written in code. It\u2019s called robots.txt and is usually located at yourwebsite.com/robots.txt. That file allows anyone who runs a website \u2014 big or small, cooking blog or multinational corporation \u2014 to tell the web who\u2019s allowed in and who isn\u2019t. Which search engines can index your site? What archival projects can grab a version of your page and save it? Can competitors keep tabs on your pages for their own files? You get to decide and declare that to the web. It\u2019s not a perfect system, but it works. Used to, anyway. For decades, the main focus of robots.txt was on search engines; you\u2019d let them scrape your site and in exchange they\u2019d promise to send people back to you. Now AI has changed the equation: companies around the web are using your site and its data to build massive sets of training data, in order to build models and products that may not acknowledge your existence at all. The robots.txt file governs a give and take; AI feels to many like all take and no give. But there\u2019s now so much money in AI, and the technological state of the art is changing so fast that many site owners can\u2019t keep up. And the fundamental agreement behind robots.txt, and the web as a whole \u2014 which for so long amounted to \u201Ceverybody just be cool\u201D \u2014 may not be able to keep up either.","contentType":"reshare","url":"https://www.lqdev.me/responses/verge-ai-robots-txt","date":"2024-02-14 20:47 -05:00","tags":["ai","internet","web"],"summary":"The text file that runs the internet \u2192 https://www.theverge.com/24067997/robots-txt-ai-text-file-web-crawlers-spiders For three decades, a tiny text file has kept the internet from chaos. This text...","keywords":["file","internet","text","site","robots","it\u2019s","build","keep","many","which"]},{"id":"reshare-introducing-sudo-windows","title":"Introducing Sudo for Windows","content":"Introducing Sudo for Windows \u2192 https://devblogs.microsoft.com/commandline/introducing-sudo-for-windows/ Sudo for Windows is a new way for users to run elevated commands directly from an unelevated console session. It is an ergonomic and familiar solution for users who want to elevate a command without having to first open a new elevated console. We are also excited to announce that we are open-sourcing this project here on GitHub! GitHub Repo","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-sudo-windows","date":"2024-02-13 21:51 -05:00","tags":["windows","sudo","system"],"summary":"Introducing Sudo for Windows \u2192 https://devblogs.microsoft.com/commandline/introducing-sudo-for-windows/ Sudo for Windows is a new way for users to run elevated commands directly from an unelevated...","keywords":["sudo","elevated","windows","github","console","users","open","command","having","also"]},{"id":"bookmarks-ms-research-graphrag","title":"GraphRAG: Unlocking LLM discovery on narrative private data ","content":"GraphRAG: Unlocking LLM discovery on narrative private data \u2192 https://www.microsoft.com/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/ Perhaps the greatest challenge \u2013 and opportunity \u2013 of LLMs is extending their powerful capabilities to solve problems beyond the data on which they have been trained, and to achieve comparable results with data the LLM has never seen. This opens new possibilities in data investigation, such as identifying themes and semantic concepts with context and grounding on datasets. In this post, we introduce GraphRAG, created by Microsoft Research, as a significant advance in enhancing the capability of LLMs.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/ms-research-graphrag","date":"2024-02-13 21:50 -05:00","tags":["research","ai","rag","llm","data","microsoft"],"summary":"GraphRAG: Unlocking LLM discovery on narrative private data \u2192 https://www.microsoft.com/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/ Perhaps the greatest challenge \u2013 and...","keywords":["data","unlocking","discovery","narrative","private","microsoft","llms","graphrag:","opens","possibilities"]},{"id":"bookmarks-every-noise-at-once","title":"Every Noise at Once","content":"Every Noise at Once \u2192 https://everynoise.com/engenremap.html Every Noise at Once is an ongoing attempt at an algorithmically-generated, readability-adjusted scatter-plot of the musical genre-space, based on data tracked and analyzed for 6,291 genre-shaped distinctions by Spotify as of 2023-11-19. The calibration is fuzzy, but in general down is more organic, up is more mechanical and electric; left is denser and more atmospheric, right is spikier and bouncier.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/every-noise-at-once","date":"2024-02-13 21:19 -05:00","tags":["music","genres","discovery","spotify"],"summary":"Every Noise at Once \u2192 https://everynoise.com/engenremap.html Every Noise at Once is an ongoing attempt at an algorithmically-generated, readability-adjusted scatter-plot of the musical genre-space,...","keywords":["more","noise","once","genre","distinctions","spotify","2023","calibration","fuzzy","general"]},{"id":"reshare-nvidia-chat-rtx","title":"NVIDIA Chat with RTX","content":"NVIDIA Chat with RTX \u2192 https://www.nvidia.com/ai-on-rtx/chat-with-rtx-generative-ai/ Chat With RTX is a demo app that lets you personalize a GPT large language model (LLM) connected to your own content\u2014docs, notes, videos, or other data. Leveraging retrieval-augmented generation (RAG), TensorRT-LLM, and RTX acceleration, you can query a custom chatbot to quickly get contextually relevant answers. And because it all runs locally on your Windows RTX PC or workstation, you\u2019ll get fast and secure results.","contentType":"reshare","url":"https://www.lqdev.me/responses/nvidia-chat-rtx","date":"2024-02-13 21:08 -05:00","tags":["nvidia","chat","ai","rag","chatbot","gpu","llm"],"summary":"NVIDIA Chat with RTX \u2192 https://www.nvidia.com/ai-on-rtx/chat-with-rtx-generative-ai/ Chat With RTX is a demo app that lets you personalize a GPT large language model (LLM) connected to your own...","keywords":["nvidia","chat","tensorrt","acceleration","query","custom","chatbot","quickly","contextually","relevant"]},{"id":"reshare-stability-ai-stable-cascade","title":"Stable Cascade","content":"Stable Cascade \u2192 https://github.com/Stability-AI/StableCascade Stable Cascade consists of three models: Stage A, Stage B and Stage C, representing a cascade for generating images, hence the name \u0026quot;Stable Cascade\u0026quot;. Stage A \u0026amp; B are used to compress images, similarly to what the job of the VAE is in Stable Diffusion. However, as mentioned before, with this setup a much higher compression of images can be achieved. Furthermore, Stage C is responsible for generating the small 24 x 24 latents given a text prompt. The following picture shows this visually. Note that Stage A is a VAE and both Stage B \u0026amp; C are diffusion models. For this release, we are providing two checkpoints for Stage C, two for Stage B and one for Stage A. Stage C comes with a 1 billion and 3.6 billion parameter version, but we highly recommend using the 3.6 billion version, as most work was put into its finetuning. The two versions for Stage B amount to 700 million and 1.5 billion parameters. Both achieve great results, however the 1.5 billion excels at reconstructing small and fine details. Therefore, you will achieve the best results if you use the larger variant of each. Lastly, Stage A contains 20 million parameters and is fixed due to its small size.","contentType":"reshare","url":"https://www.lqdev.me/responses/stability-ai-stable-cascade","date":"2024-02-13 21:02 -05:00","tags":["stabilityai","ai","stablediffusion"],"summary":"Stable Cascade \u2192 https://github.com/Stability-AI/StableCascade Stable Cascade consists of three models: Stage A, Stage B and Stage C, representing a cascade for generating images, hence the name...","keywords":["stage","billion","stable","cascade","small","images","version","million","however","diffusion"]},{"id":"reshare-openai-chatgpt-memory","title":"Memory and new controls for ChatGPT","content":"Memory and new controls for ChatGPT \u2192 https://openai.com/blog/memory-and-new-controls-for-chatgpt We\u2019re testing memory with ChatGPT. Remembering things you discuss across all chats saves you from having to repeat information and makes future conversations more helpful. You\u0027re in control of ChatGPT\u0027s memory. You can explicitly tell it to remember something, ask it what it remembers, and tell it to forget conversationally or through settings. You can also turn it off entirely. We are rolling out to a small portion of ChatGPT free and Plus users this week to learn how useful it is. We will share plans for broader roll out soon.","contentType":"reshare","url":"https://www.lqdev.me/responses/openai-chatgpt-memory","date":"2024-02-13 21:00 -05:00","tags":["openai","chatgpt","memory","ai","llm","gpt"],"summary":"Memory and new controls for ChatGPT \u2192 https://openai.com/blog/memory-and-new-controls-for-chatgpt We\u2019re testing memory with ChatGPT. Remembering things you discuss across all chats saves you from...","keywords":["chatgpt","memory","tell","controls","broader","remembers","forget","conversationally","settings","also"]},{"id":"bookmarks-sql-for-data-scientists-100-queries","title":"SQL for Data Scientists in 100 Queries","content":"SQL for Data Scientists in 100 Queries \u2192 https://gvwilson.github.io/sql-tutorial/ notes and working examples that instructors can use to perform a lesson","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/sql-for-data-scientists-100-queries","date":"2024-02-06 20:07 -05:00","tags":["sql","datascience","tutorial"],"summary":"SQL for Data Scientists in 100 Queries \u2192 https://gvwilson.github.io/sql-tutorial/ notes and working examples that instructors can use to perform a lesson","keywords":["data","scientists","queries","https://gvwilson","github","io/sql","tutorial/","notes","working","examples"]},{"id":"reshare-how-whatsapp-plans-interoperate-other-messaging-services","title":"Here\u2019s how WhatsApp plans to interoperate with other messaging apps","content":"Here\u2019s how WhatsApp plans to interoperate with other messaging apps \u2192 https://www.theverge.com/2024/2/6/24063705/whatsapp-interoperability-plans-eu-dma WhatsApp, like many other major tech platforms, will have to make some significant changes to comply with the European Union\u2019s Digital Markets Act (DMA). One of those changes is interoperability with other messaging platforms... The shift toward interoperability will first include text messages, images, voice messages, videos, and files sent from one person to another. In theory, this would allow users to chat with people on WhatsApp through third-party apps, like iMessage, Telegram, Google Messages, and Signal, and vice versa. As noted by Wired, WhatsApp wants the messaging services it connects with to use the same Signal Protocol to encrypt messages. Meta is also open to apps using alternate encryption protocols so long as companies can prove \u201Cthey reach the security standards that WhatsApp outlines in its guidance.\u201D The third-party services will also have to sign a contract with Meta before they plug into WhatsApp, with more details about the agreement coming in March.","contentType":"reshare","url":"https://www.lqdev.me/responses/how-whatsapp-plans-interoperate-other-messaging-services","date":"2024-02-06 20:03 -05:00","tags":["whatsapp","messaging","interoperability","signal"],"summary":"Here\u2019s how WhatsApp plans to interoperate with other messaging apps \u2192 https://www.theverge.com/2024/2/6/24063705/whatsapp-interoperability-plans-eu-dma WhatsApp, like many other major tech platforms,...","keywords":["whatsapp","messages","messaging","apps","interoperability","third","party","services","meta","also"]},{"id":"reshare-nomic-embed-open-embedding-model","title":"Introducing Nomic Embed: A Truly Open Embedding Model","content":"Introducing Nomic Embed: A Truly Open Embedding Model \u2192 https://blog.nomic.ai/posts/nomic-embed-text-v1 We\u0027re excited to announce the release of Nomic Embed, the first Open source Open data Open training code Fully reproducible and auditable","contentType":"reshare","url":"https://www.lqdev.me/responses/nomic-embed-open-embedding-model","date":"2024-02-03 20:37 -05:00","tags":["embedding","ai","opensource","ml","model"],"summary":"Introducing Nomic Embed: A Truly Open Embedding Model \u2192 https://blog.nomic.ai/posts/nomic-embed-text-v1 We\u0027re excited to announce the release of Nomic Embed, the first Open source Open data Open...","keywords":["open","nomic","embed","introducing","fully","code","training","data","source","release"]},{"id":"notes-post-statistics-february-2024","title":"Website Post Statistics - February 2024","content":"Success! At the end of last year, I created an automated script to tally some basic stats about my posts on this site. Last month, due to user error, it triggered but failed to run susccessfully . This month though, it ran as expected! Some highlights: I\u0027m posting more notes (short micro-blog style content). Through January, I\u0027m already over 30% of total notes posted last year which is what I\u0027m working towards. I\u0027ve already posted 50 response posts through January. This is already over 20% of total responses published last year (223)! I\u0027m resharing more than bookmarking. Still figuring that one out but I think this is a good start.","contentType":"notes","url":"https://www.lqdev.me/notes/post-statistics-february-2024","date":"2024-02-03 12:29 -05:00","tags":["githubactions","cicd","blogging","stats"],"summary":"Success! At the end of last year, I created an automated script to tally some basic stats about my posts on this site. Last month, due to user error, it triggered but failed to run susccessfully ....","keywords":["last","year","already","more","january","total","posted","posts","month","notes"]},{"id":"notes-down-the-weird-web","title":"Down the weird web","content":"Not sure how these videos ended up showing up on my feed but I\u0027m glad they did. They\u0027re weird, creepy, and fun all rolled into one. I\u0027m surprised that they have such low views / subscribers. Check them out on YouTube. The Archive in Between Alternate Bazaar Art The Archive in Between Alternate Bazaar Art","contentType":"notes","url":"https://www.lqdev.me/notes/down-the-weird-web","date":"2024-02-03 12:15 -05:00","tags":["alternatebazaarart","archiveinbetween","art","ai","digital","web","internet","video","fiction"],"summary":"Not sure how these videos ended up showing up on my feed but I\u0027m glad they did. They\u0027re weird, creepy, and fun all rolled into one. I\u0027m surprised that they have such low views / subscribers. Check...","keywords":["they","bazaar","archive","alternate","youtube","check","subscribers","views","surprised","creepy"]},{"id":"bookmarks-langchain-opengpts","title":"LangChain - OpenGPTs","content":"LangChain - OpenGPTs \u2192 https://blog.langchain.dev/opengpts/ A little over two months ago, on the heels of OpenAI dev day, we launched OpenGPTs: a take on what an open-source GPT store may look like. It was powered by an early version of LangGraph - an extension of LangChain aimed at building agents as graphs. At the time, we did not highlight this new package much, as we had not publicly launched it and were still figuring out the interface. We finally got around to launching LangGraph two weeks ago, and over the past weekend we updated OpenGPTs to fully use LangGraph (as well as added some new features). We figure now is as good of time as any to do a technical deep-dive on OpenGPTs and what powers it. In this blog, we will talk about: MessageGraph: A particular type of graph that OpenGPTs runs on Cognitive architectures: What the 3 different types of cognitive architectures OpenGPTs supports are, and how they differ Persistence: How persistence is baked in OpenGPTs via LangGraph checkpoints. Configuration: How we use LangChain primitives to configure all these different bots. New models: what new models we support New tools: what new tools we support astream_events: How we are using this new method to stream tokens and intermediate steps","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/langchain-opengpts","date":"2024-01-31 21:30 -05:00","tags":["langchain","gpt","ai","opengpt","langgraph","messagegraph"],"summary":"LangChain - OpenGPTs \u2192 https://blog.langchain.dev/opengpts/ A little over two months ago, on the heels of OpenAI dev day, we launched OpenGPTs: a take on what an open-source GPT store may look like....","keywords":["opengpts","langchain","langgraph","different","launched","cognitive","support","supports","architectures","types"]},{"id":"reshare-unredacted-magazine-issue-006","title":"UNREDACTED Magazine Issue 006","content":"UNREDACTED Magazine Issue 006 \u2192 https://inteltechniques.com/blog/2024/01/27/unredacted-magazine-issue-006/ https://inteltechniques.com/magazine.html https://inteltechniques.com/issues/006.pdf","contentType":"reshare","url":"https://www.lqdev.me/responses/unredacted-magazine-issue-006","date":"2024-01-31 21:24 -05:00","tags":["osint","magazine","unredacted","privacy","security"],"summary":"UNREDACTED Magazine Issue 006 \u2192 https://inteltechniques.com/blog/2024/01/27/unredacted-magazine-issue-006/ https://inteltechniques.com/magazine.html https://inteltechniques.com/issues/006.pdf","keywords":["https://inteltechniques","magazine","issue","unredacted","com/blog/2024/01/27/unredacted","006/","com/magazine","html","com/issues/006"]},{"id":"posts-setting-up-rust-dev-env-devcontainers-vscode","title":"Setting up your Rust development environment using Dev Containers","content":"Setting up a new development environment that\u0027s configured with all the SDKs and tools you need to get started working can be a multi-step process that is error-prone. Especially when you don\u0027t know whether you want to commit to a specific technology or don\u0027t have too much space on your device, you might not want to install everything on your device. Additionally, cleaning up and removing everything you installed isn\u0027t always 100%. This is where development environments like Dev Containers can come in handy. ## What are Dev Containers A development container (Dev Container) lets you use a container as a full development environment. Because your development environment is in a container, the environment is isolated. The builds are reproducible and disposable. This means you can quickly stand up a new clean development environments and dispose of them just as easily. For more details, see the [Dev Containers website](https://containers.dev/). If you have Docker installed, you can run them locally. Otherwise, you can run Dev Containers using GitHub Codespaces ## What are GitHub Codespaces A codespace is a development environment that\u0027s hosted in the cloud. For more details, see the [GitHub Codespaces website](https://github.com/features/codespaces). ## Set up Dev Container for Rust Development 1. Create a new directory. 1. Add a Dev Container configuration file called *.devcontainer.json* that contains the following content: \u0060\u0060\u0060json { \u0022name\u0022: \u0022Rust Development Environment\u0022, \u0022image\u0022: \u0022mcr.microsoft.com/devcontainers/rust:latest\u0022, \u0022customizations\u0022: { \u0022vscode\u0022: { \u0022extensions\u0022: [ \u0022rust-lang.rust-analyzer\u0022 ] } } } \u0060\u0060\u0060 - **name**: A human friendly name to identify your development environment. It\u0027s not as important when you only have one environment but if you have different environments, it can be useful for differentiating between the various configurations. - **image**: For the most customization, you can provide the path to a Docker file. However, to make things easier, there are a set of prebuilt container images. In this case, I\u0027m using the [Rust image](https://github.com/devcontainers/images/tree/main/src/rust). - **customizations.vscode.extensions**: A list of Visual Studio Code extensions that can help you with your development. In this case, it\u0027s the [*rust-analyzer*](https://marketplace.visualstudio.com/items?itemName=rust-lang.rust-analyzer). At minimum, this is all you need to get started. ## Start your environment [Visual Studio Code provides good support for Dev Containers](https://code.visualstudio.com/docs/devcontainers/tutorial). 1. Install [VS Code](https://code.visualstudio.com/download) 1. Open your directory where you configured your Dev Container in Visual Studio Code 1. Install the [Dev Containers VS Code extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers). 1. Open the command palette in VS Code. In the menu bar, select **View \u003E Command Palette.** 1. Enter the following command **\u003EDev Containers: Open Dev Container**. This will start building your dev container. Wait for it to start. ## Check your environment Once your Dev Container starts, check that \u0060rustc\u0060 is installed. 1. Open the terminal. 1. Run \u0060rustc -h\u0060. A help message should output to the console for the Rust compiler. ## Build your app For simplicity, I\u0027ve used the [Hello World sample from the Rust By Example website](https://doc.rust-lang.org/rust-by-example/hello.html). 1. Create a new file called *hello.rs*. 1. Add the following code: \u0060\u0060\u0060rust fn main () { println!(\u0022Hello World!\u0022); } \u0060\u0060\u0060 1. Compile your program \u0060\u0060\u0060bash rustc hello.rs \u0060\u0060\u0060 This will generate an executable called *hello* 1. Run your app \u0060\u0060\u0060bash ./hello \u0060\u0060\u0060 The application should print out the string \u0022Hello World\u0022 to the console. That\u0027s it! You now have a Rust development environment to get started learning and building apps. ## Conclusion Dev Containers make it easy for you to configure full-featured development environments. When paired with Visual Studio Code and GitHub Codespaces, they can help you focus on learning and building applications rather than setting up your environment. You can find a [final version of the configured environment and app on GitHub](https://github.com/lqdev/rust-codespace-sandbox). Happy coding! ## Additional Resources - [Rust in Visual Studio Code](https://code.visualstudio.com/docs/languages/rust) - [Rust by Example](https://doc.rust-lang.org/rust-by-example/index.html) - [Developing Inside a Container](https://code.visualstudio.com/docs/devcontainers/containers)","contentType":"posts","url":"https://www.lqdev.me/posts/setting-up-rust-dev-env-devcontainers-vscode","date":"2024-01-31 21:05 -05:00","tags":["rust","vscode","devcontainers","codespaces","programming","technology","docker","containers","github"],"summary":"Setting up a new development environment that\u0027s configured with all the SDKs and tools you need to get started working can be a multi-step process that is error-prone. Especially when you don\u0027t know...","keywords":["development","environment","rust","container","containers","code","visualstudio","github","visual","studio"]},{"id":"notes-crate-finds-january-2024","title":"Crate Finds - January 2024","content":"January 2024 drop of Crate Finds. Ones I kept coming back to: Where Are You by 54 Ultra Language of Love by Jason Joshua A Love International by Khruangbin Dreameater by Healing Gems La Isla En El Sol, Pt. 2 by Real Cumbia Activa RCA Portrait of a Time by Peter Cat Recording Co. Tracks Beyond Spiritual (feat. Wiz Khalifa, Big K.R.I.T. \u0026amp; Curren$y) by Smoke DZA, Flying Lotus, Wiz Khalifa, Big K.R.I.T., Curren$y Album: Flying Objects (Extended Version) Duration: 5:41 Listen on YouTube Backup: Listen on Spotify Lost in Your Eyes by Tommy James \u0026amp; The Shondells Album: Gettin\u0027 Together Duration: 2:14 Listen on YouTube Backup: Listen on Spotify My Life\u0027s All Right by Shira Small Album: The Line Of Time And The Plane Of Now Duration: 2:36 Listen on YouTube Backup: Listen on Spotify Ozan Koukle - Remastered by Lafayette Afro Rock Band, Ice Album: Afro Funk Explosion Duration: 3:15 Listen on YouTube Backup: Listen on Spotify Where Are You by 54 Ultra Album: Where Are You Duration: 2:22 Listen on YouTube Backup: Listen on Spotify Retrato Pra Iaia by Los Hermanos Album: Mega Hits - Los Hermanos Duration: 3:57 Listen on YouTube Backup: Listen on Spotify I Wish You Love (Part 1) by Joe Bataan Album: Saint Latin\u0027S Day Massacre Duration: 3:32 Listen on YouTube Backup: Listen on Spotify Language Of Love by Jason Joshua Album: La Voz De Oro Duration: 3:09 Listen on YouTube Backup: Listen on Spotify Interlude by Psychic TV Album: Force the Hand of Chance (Expanded Edition) Duration: 1:00 Listen on YouTube Backup: Listen on Spotify Some Time Alone, Alone by Melody\u0027s Echo Chamber Album: Melody\u0027s Echo Chamber Duration: 3:46 Listen on YouTube Backup: Listen on Spotify Looking for a Place by The Spyres Album: Looking for a Place Duration: 2:21 Listen on YouTube Backup: Listen on Spotify Cosca by Conway the Machine, Wun Two Album: Cosca Duration: 1:56 Listen on YouTube Backup: Listen on Spotify Spiritual by Smoke DZA, Flying Lotus Album: Flying Objects (Extended Version) Duration: 3:16 Listen on YouTube Backup: Listen on Spotify Painted Houses (feat. Conway the Machine) by Smoke DZA, Flying Lotus, Conway the Machine Album: Flying Objects (Extended Version) Duration: 3:03 Listen on YouTube Backup: Listen on Spotify Harlem World 97 (feat. Estelle) by Smoke DZA, Flying Lotus, Estelle Album: Flying Objects (Extended Version) Duration: 2:17 Listen on YouTube Backup: Listen on Spotify Let Me Prove My Love to You by The Main Ingredient Album: Shame on the World Duration: 3:39 Listen on YouTube Backup: Listen on Spotify Wana by nour Album: Wana Duration: 3:39 Listen on YouTube Backup: Listen on Spotify Matte Black UFO by Marigold Album: Under Green Light Duration: 1:38 Listen on YouTube Backup: Listen on Spotify Changing Light by Ironsides Album: Changing Light Duration: 4:46 Listen on YouTube Backup: Listen on Spotify Making up and Breaking Up (And Making up and Breaking up over Again) by Sharon Jones \u0026amp; The Dap-Kings Album: Give the People What They Want Duration: 2:23 Listen on YouTube Backup: Listen on Spotify soundgood by B. Cool-Aid, Pink Siifu, Ahwlee, Liv.e, Jimetta Rose, V.C.R Album: Leather Blvd. Duration: 2:43 Listen on YouTube Backup: Listen on Spotify A Love International by Khruangbin Album: A Love International Duration: 4:15 Listen on YouTube Backup: Listen on Spotify redrum by 21 Savage Album: american dream Duration: 4:30 Listen on YouTube Backup: Listen on Spotify Love of Mine by Bardo Album: Love of Mine Duration: 3:18 Listen on YouTube Backup: Listen on Spotify Walking Through Walls by Chola Orange Album: Chola Orange Duration: 4:06 Listen on YouTube Backup: Listen on Spotify The Ballad of Hamato Yoshi by Chola Orange Album: Tokyo Sunset Duration: 4:56 Listen on YouTube Backup: Listen on Spotify Tijuana Mushroom (Driving on LSD) by Healing Gems Album: Sacred Monkey Duration: 3:33 Listen on YouTube Backup: Listen on Spotify Shintaro by Fake Jazz Album: Shintaro Duration: 3:31 Listen on YouTube Backup: Listen on Spotify Como Fuego by Reyna Tropical Album: Como Fuego Duration: 3:04 Listen on YouTube Backup: Listen on Spotify Brillo M\u00E1s Que El Oro (La Bala Apunt\u00E1ndome) by Combo Chimbita Album: Ahomale Duration: 3:56 Listen on YouTube Backup: Listen on Spotify La Ciencia de la Paz by Los Ni\u00F1os Telepaticos Album: Estallados Duration: 3:41 Listen on YouTube Backup: Listen on Spotify Ojo por Ojo by Sociedad para el Cuidado de Uno Mismo, Manuela Ocampo Album: Nada Es Sencillo \u00BFPor Qu\u00E9 Todo Es Tan Raro? Duration: 3:44 Listen on YouTube Backup: Listen on Spotify El Martes Ser\u00E1 un Nuevo D\u00EDa (S2WS) by w.y.k Album: Diciembre Es un Domingo Duration: 6:01 Listen on YouTube Backup: Listen on Spotify Ni\u00F1a Muerte by Cachicamo Album: Esbozos del Llano Duration: 9:33 Listen on YouTube Backup: Listen on Spotify Canto de Trabajo by Cachicamo Album: Esbozos del Llano Duration: 10:25 Listen on YouTube Backup: Listen on Spotify Dreameater by Healing Gems Album: Dreameater Duration: 4:13 Listen on YouTube Backup: Listen on Spotify Anillo de Matrimonio by Ramona, \u00C9 Arenas Album: Anillo de Matrimonio Duration: 2:10 Listen on YouTube Backup: Listen on Spotify La Isla En El Sol, Pt. 2 by Real Cumbia Activa RCA Album: Ram\u00E9: La Isla del Sol Duration: 2:54 Listen on YouTube Backup: Listen on Spotify Memory Box by Peter Cat Recording Co. Album: Bismillah Duration: 8:03 Listen on YouTube Backup: Listen on Spotify Masterpiece by SAULT Album: 5 Duration: 5:44 Listen on YouTube Backup: Listen on Spotify Morning by Azymuth Album: Azymuth Duration: 3:48 Listen on YouTube Backup: Listen on Spotify Make a Smile for Me by Bill Withers Album: \u0027Justments Duration: 3:15 Listen on YouTube Backup: Listen on Spotify We Have Love by Amnesty Album: Free Your Mind Duration: 4:32 Listen on YouTube Backup: Listen on Spotify Portrait of a Time by Peter Cat Recording Co. Album: Portrait of a Time: 2010-2016 Duration: 5:52 Listen on YouTube Backup: Listen on Spotify Ciao by Georgia Anne Muldrow Album: Overload Duration: 0:59 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-january-2024","date":"2024-01-31 19:53 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"January 2024 drop of Crate Finds. Ones I kept coming back to: Where Are You by 54 Ultra Language of Love by Jason Joshua A Love International by Khruangbin Dreameater by Healing Gems La Isla En El...","keywords":["listen","spotify","duration:","album:","youtube","backup:","love","flying","smoke","extended"]},{"id":"reshare-thee-sacred-souls-npr-tiny-desk-concerts","title":"Tiny Desk Premiere: Thee Sacred Souls","content":"Tiny Desk Premiere: Thee Sacred Souls \u2192 https://www.npr.org/2024/01/26/1226810515/tiny-desk-concert-thee-sacred-souls San Diego-based trio Thee Sacred Souls made its mark at the Tiny Desk with satin vocals and vintage melodies. Paying homage to southern California Latino culture meeting American soul roots, the group\u0027s sweet fusion melodies brought history and love into the space.","contentType":"reshare","url":"https://www.lqdev.me/responses/thee-sacred-souls-npr-tiny-desk-concerts","date":"2024-01-31 19:50 -05:00","tags":["theesacredsouls","music","concert","npr","tinydeskconcert"],"summary":"Tiny Desk Premiere: Thee Sacred Souls \u2192 https://www.npr.org/2024/01/26/1226810515/tiny-desk-concert-thee-sacred-souls San Diego-based trio Thee Sacred Souls made its mark at the Tiny Desk with satin...","keywords":["desk","souls","thee","sacred","tiny","melodies","https://www","history","brought","fusion"]},{"id":"bookmarks-eagle-7b-rkwv","title":"Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100\u002B Languages (RWKV-v5)","content":"Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100\u002B Languages (RWKV-v5) \u2192 https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers Eagle 7B is a 7.52B parameter model that: Built on the RWKV-v5 architecture (a linear transformer with 10-100x\u002B lower inference cost) Ranks as the world\u2019s greenest 7B model (per token) Trained on 1.1 Trillion Tokens across 100\u002B languages Outperforms all 7B class models in multi-lingual benchmarks Approaches Falcon (1.5T), LLaMA2 (2T), Mistral (\u0026gt;2T?) level of performance in English evals Trade blows with MPT-7B (1T) in English evals All while being an \u201CAttention-Free Transformer\u201D Is a foundation model, with a very small instruct tune - further fine-tuning is required for various use cases! We are releasing RWKV-v5 Eagle 7B, licensed as Apache 2.0 license, under the Linux Foundation , and can be used personally or commercially without restrictions Download from HuggingFace","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/eagle-7b-rkwv","date":"2024-01-29 20:27 -05:00","tags":["ai","llm","rwkv","deeplearning","neuralnetwork"],"summary":"Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100\u002B Languages (RWKV-v5) \u2192 https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers Eagle 7B is a 7.52B parameter model that:...","keywords":["rwkv","eagle","model","foundation","evals","languages","100\u002B","english","tokens","trillion"]},{"id":"bookmarks-get-started-cuda-python-programmers","title":"Getting Started With CUDA for Python Programmers","content":"Getting Started With CUDA for Python Programmers \u2192 https://www.youtube.com/watch?v=nOxKexn3iBo In this comprehensive video tutorial, Jeremy Howard from answer.ai demystifies the process of programming NVIDIA GPUs using CUDA, and simplifies the perceived complexities of CUDA programming. Jeremy emphasizes the accessibility of CUDA, especially when combined with PyTorch\u0027s capabilities, allowing for programming directly in notebooks rather than traditional compilers and terminals. To make CUDA more approachable to Python programmers, Jeremy shows step by step how to start with Python implementations, and then convert them largely automatically to CUDA. This approach, he argues, simplifies debugging and development. The tutorial is structured in a hands-on manner, encouraging viewers to follow along in a Colab notebook. Jeremy uses practical examples, starting with converting an RGB image to grayscale using CUDA, demonstrating the process step-by-step. He further explains the memory layout in GPUs, emphasizing the differences from CPU memory structures, and introduces key CUDA concepts like streaming multi-processors and CUDA cores. Jeremy then delves into more advanced topics, such as matrix multiplication, a critical operation in deep learning. He demonstrates how to implement matrix multiplication in Python first and then translates it to CUDA, highlighting the significant performance gains achievable with GPU programming. The tutorial also covers CUDA\u0027s intricacies, such as shared memory, thread blocks, and optimizing CUDA kernels. The tutorial also includes a section on setting up the CUDA environment on various systems using Conda, making it accessible for a wide range of users. This is lecture 3 of the \u0026quot;CUDA Mode\u0026quot; series (but you don\u0027t need to watch the others first). The notebook is available in the lecture3 folder here: https://github.com/cuda-mode/lecture2 ...","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/get-started-cuda-python-programmers","date":"2024-01-29 20:23 -05:00","tags":["gpu","cuda","python","programming"],"summary":"Getting Started With CUDA for Python Programmers \u2192 https://www.youtube.com/watch?v=nOxKexn3iBo In this comprehensive video tutorial, Jeremy Howard from answer.ai demystifies the process of...","keywords":["cuda","jeremy","python","step","programming","tutorial","using","memory","then","process"]},{"id":"reshare-python-js-ollama-libraries","title":"Ollama - Python \u0026 JavaScript Libraries","content":"Ollama - Python \u0026amp; JavaScript Libraries \u2192 https://ollama.ai/blog/python-javascript-libraries The initial versions of the Ollama Python and JavaScript libraries are now available: Ollama Python Library Ollama JavaScript Library Both libraries make it possible to integrate new and existing apps with Ollama in a few lines of code, and share the features and feel of the Ollama REST API.","contentType":"reshare","url":"https://www.lqdev.me/responses/python-js-ollama-libraries","date":"2024-01-25 21:26 -05:00","tags":["ollama","ai","llama","python","javascript","opensource"],"summary":"Ollama - Python \u0026amp; JavaScript Libraries \u2192 https://ollama.ai/blog/python-javascript-libraries The initial versions of the Ollama Python and JavaScript libraries are now available: Ollama Python...","keywords":["ollama","libraries","javascript","python","library","https://ollama","features","share","code","lines"]},{"id":"reshare-google-cloud-huggingface-deal","title":"Google\u2019s Hugging Face deal puts \u2018supercomputer\u2019 power behind open-source AI","content":"Google\u2019s Hugging Face deal puts \u2018supercomputer\u2019 power behind open-source AI \u2192 https://www.theverge.com/2024/1/25/24050445/google-cloud-hugging-face-ai-developer-access Google Cloud\u2019s new partnership with AI model repository Hugging Face is letting developers build, train, and deploy AI models without needing to pay for a Google Cloud subscription. Now, outside developers using Hugging Face\u2019s platform will have \u201Ccost-effective\u201D access to Google\u2019s tensor processing units (TPU) and GPU supercomputers, which will include thousands of Nvidia\u2019s in-demand and export-restricted H100s. Google said that Hugging Face users can begin using the AI app-building platform Vertex AI and the Kubernetes engine that helps train and fine-tune models \u201Cin the first half of 2024.\u201D Press Release","contentType":"reshare","url":"https://www.lqdev.me/responses/google-cloud-huggingface-deal","date":"2024-01-25 21:23 -05:00","tags":["google","huggingface","ai","cloud","llm","ml","developers"],"summary":"Google\u2019s Hugging Face deal puts \u2018supercomputer\u2019 power behind open-source AI \u2192 https://www.theverge.com/2024/1/25/24050445/google-cloud-hugging-face-ai-developer-access Google Cloud\u2019s new partnership...","keywords":["hugging","face","google","google\u2019s","platform","models","train","developers","access","cloud"]},{"id":"reshare-new-openai-text-embedding-models-3","title":"New OpenAI embedding models and API updates","content":"New OpenAI embedding models and API updates \u2192 https://openai.com/blog/new-embedding-models-and-api-updates We are launching a new generation of embedding models, new GPT-4 Turbo and moderation models, new API usage management tools, and soon, lower pricing on GPT-3.5 Turbo.","contentType":"reshare","url":"https://www.lqdev.me/responses/new-openai-text-embedding-models-3","date":"2024-01-25 21:20 -05:00","tags":["openai","llm","embedding","openai","gpt"],"summary":"New OpenAI embedding models and API updates \u2192 https://openai.com/blog/new-embedding-models-and-api-updates We are launching a new generation of embedding models, new GPT-4 Turbo and moderation...","keywords":["models","embedding","updates","turbo","openai","https://openai","com/blog/new","launching","generation","moderation"]},{"id":"notes-notebook-fantasy-chicano-batman","title":"Notebook Fantasy - New Chicano Batman out March 29, 2024","content":"New Chicano Batman album! Notebook Fantasy . It\u0027s unfortunate Gabriel isn\u0027t part of this project but looking forward to the new sound tour.","contentType":"notes","url":"https://www.lqdev.me/notes/notebook-fantasy-chicano-batman","date":"2024-01-24 20:23 -05:00","tags":["music","chicanobatman","newmusic","album","newalbum"],"summary":"New Chicano Batman album! Notebook Fantasy . It\u0027s unfortunate Gabriel isn\u0027t part of this project but looking forward to the new sound tour.","keywords":["chicano","batman","album","notebook","fantasy","unfortunate","gabriel","part","project","looking"]},{"id":"reshare-fosdem-2024-schedule","title":"FOSDEM 2024 Schedule","content":"FOSDEM 2024 Schedule \u2192 https://fosdem.org/2024/schedule/events/ Lots of great sessions. I\u0027m looking forward to the sessions on the following topics: Matrix AI Nix / NixOS Software Defined Radio (SDR) \u0026amp; Amateur Radio Modern Email Collaboration \u0026amp; Content Management","contentType":"reshare","url":"https://www.lqdev.me/responses/fosdem-2024-schedule","date":"2024-01-23 22:19 -05:00","tags":["fosdem","opensource","foss","floss","ai","matrix","nixos"],"summary":"FOSDEM 2024 Schedule \u2192 https://fosdem.org/2024/schedule/events/ Lots of great sessions. I\u0027m looking forward to the sessions on the following topics: Matrix AI Nix / NixOS Software Defined Radio (SDR)...","keywords":["\u0026amp","sessions","radio","fosdem","collaboration","email","modern","amateur","defined","software"]},{"id":"bookmarks-openai-microscope","title":"OpenAI Microscope","content":"OpenAI Microscope \u2192 https://openai.com/research/microscope We\u2019re introducing OpenAI Microscope, a collection of visualizations of every significant layer and neuron of eight vision \u201Cmodel organisms\u201D which are often studied in interpretability. Microscope makes it easier to analyze the features that form inside these neural networks, and we hope it will help the research community as we move towards understanding these complicated systems.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/openai-microscope","date":"2024-01-23 22:15 -05:00","tags":["openai","interpretability","ai","generativeai","visualization"],"summary":"OpenAI Microscope \u2192 https://openai.com/research/microscope We\u2019re introducing OpenAI Microscope, a collection of visualizations of every significant layer and neuron of eight vision \u201Cmodel organisms\u201D...","keywords":["microscope","openai","easier","analyze","features","form","inside","neural","networks","hope"]},{"id":"reshare-unredacted-magazine-status-jan-2024","title":"UNREDACTED Magazine Status","content":"UNREDACTED Magazine Status \u2192 https://inteltechniques.com/blog/2024/01/05/unredacted-magazine-status/ ...the magazine is not \u0027dead\u0027. Much like the podcast, it is simply on a hiatus. Many people falsely report online that the podcast and magazine are officially never coming back, which is contradictory to my previous post . The reason there have been no issues of the magazine is simply a lack of submissions. The magazine is a community-driven product. Without the community driving it, it will go nowhere. If you would like to submit an article, please email it to staff@unredactedmagazine.com. Sponsors are lined up to pay the costs and keep the content free, but there lies other problems. We received constant complaints about having sponsors. Most readers demanded free content without ads, which is unrealistic. We have considered a small fee per issue, but the credit card fraud which comes with that is an even bigger issue. What is the solution? I do not know yet. If the articles pour in, I will figure it out.","contentType":"reshare","url":"https://www.lqdev.me/responses/unredacted-magazine-status-jan-2024","date":"2024-01-23 21:34 -05:00","tags":["magazine","unredacted","osint"],"summary":"UNREDACTED Magazine Status \u2192 https://inteltechniques.com/blog/2024/01/05/unredacted-magazine-status/ ...the magazine is not \u0027dead\u0027. Much like the podcast, it is simply on a hiatus. Many people...","keywords":["magazine","which","simply","sponsors","community","podcast","content","free","issue","constant"]},{"id":"reply-graham-microsoft-dead","title":"Microsoft is Dead (2007)","content":"Microsoft is Dead (2007) \u2192 https://paulgraham.com/microsoft.html Interesting points.","contentType":"reply","url":"https://www.lqdev.me/responses/graham-microsoft-dead","date":"2024-01-23 21:12 -05:00","tags":["google","microsoft","web"],"summary":"Microsoft is Dead (2007) \u2192 https://paulgraham.com/microsoft.html Interesting points.","keywords":["microsoft","dead","2007","https://paulgraham","com/microsoft","html","interesting","points"]},{"id":"reshare-nightshade-ai-model-poisoning-tool","title":"NightShade","content":"NightShade \u2192 https://nightshade.cs.uchicago.edu/ Nightshade, a tool that turns any image into a data sample that is unsuitable for model training. More precisely, Nightshade transforms images into \u0026quot;poison\u0026quot; samples, so that models training on them without consent will see their models learn unpredictable behaviors that deviate from expected norms, e.g. a prompt that asks for an image of a cow flying in space might instead get an image of a handbag floating in space. What is NightShade?","contentType":"reshare","url":"https://www.lqdev.me/responses/nightshade-ai-model-poisoning-tool","date":"2024-01-23 20:57 -05:00","tags":["research","tools","ai","generativeai","llm","computervision","cv"],"summary":"NightShade \u2192 https://nightshade.cs.uchicago.edu/ Nightshade, a tool that turns any image into a data sample that is unsuitable for model training. More precisely, Nightshade transforms images into...","keywords":["nightshade","image","space","training","models","consent","learn","unpredictable","behaviors","deviate"]},{"id":"reshare-introducing-shared-journals-day-one","title":"Introducing Shared Journals for Day One","content":"Introducing Shared Journals for Day One \u2192 https://dayoneapp.com/blog/introducing-shared-journals/ Shared Journals are a private space for your closest friends and family to shared life updates and memories. Shared Journals introduce a new dimension to journaling, offering a unique way to share your personal stories and experiences with up to 30 selected individuals, while keeping your individual entries private and secure. Awesome! When I was writing the post Private Social Media yesterday, I wasn\u0027t aware that these had already launched. I knew they were in beta but it\u0027s great to see they\u0027re now generally available. I\u0027ll have to give them a try.","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-shared-journals-day-one","date":"2024-01-23 20:26 -05:00","tags":["dayone","automattic","journal"],"summary":"Introducing Shared Journals for Day One \u2192 https://dayoneapp.com/blog/introducing-shared-journals/ Shared Journals are a private space for your closest friends and family to shared life updates and...","keywords":["shared","journals","private","they","introducing","keeping","individual","entries","secure","awesome"]},{"id":"reshare-introducing-stable-lm-2-1-6B","title":"Introducing Stable LM 2 1.6B","content":"Introducing Stable LM 2 1.6B \u2192 https://stability.ai/news/introducing-stable-lm-2 Stable LM 2 1.6B is a state-of-the-art 1.6 billion parameter small language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch. This model\u0027s compact size and speed lower hardware barriers, allowing more developers to participate in the generative AI ecosystem. In addition to the pre-trained and instruction-tuned version, we release the last checkpoint before the pre-training cooldown. We include optimizer states to facilitate developers in fine-tuning and experimentation. Data details will be provided in the upcoming technical report. Stable LM 2 1.6B can be used now both commercially and non-commercially with a Stability AI Membership \u0026amp; you can test the model on Hugging Face .","contentType":"reshare","url":"https://www.lqdev.me/responses/introducing-stable-lm-2-1-6B","date":"2024-01-23 09:36 -05:00","tags":["ai","stabilityai","slm","llm","smalllanguagemodel"],"summary":"Introducing Stable LM 2 1.6B \u2192 https://stability.ai/news/introducing-stable-lm-2 Stable LM 2 1.6B is a state-of-the-art 1.6 billion parameter small language model trained on multilingual data in...","keywords":["stable","model","developers","commercially","data","trained","introducing","states","optimizer","include"]},{"id":"notes-private-social-media","title":"Private social media?","content":"Brain dumping a bunch of half-formed thoughts. Over the past few days, I\u0027ve been thinking about different forms of sharing content. With the shifts in social media towards smaller, more private communities , is it possible to get the best of feeds for sharing updates and information but in a smaller more intimate way? Here\u0027s a few of the options I came up with: Journals Examples: Day One Shared Journals Pros Made for an audience of one, but other can collaborate on it. Depending on the interface, looking back at specific days and events that took place is easy to look back on. May be able to include different forms of media (i.e. audio, video, photos) Cons No individual profiles May require everyone to use the same app / service E-mail Examples: Newsletters, Mailing lists Pros Almost everyone has an e-mail address. Built on standard protocols. Producers can use any app they\u0027d like to compose e-mails. Consumers can use any app they\u0027d like to read e-mails. Could use any app / client / service-provider they\u0027d like. (Federated by default) Cons Content rendering may differ based on app. Could add to the noise already present in your e-mail inbox. No individual accounts / feeds. Messaging Apps Examples: Discord, Matrix, IRC, Slack, Signal Pros Sharing could be done via SMS (though this would limit the content you share). You could form various sub-groups to sub Cons May require everyone to use the same app / service. May not have a way to view individual profiles or feeds. Because conversations are not often grouped by topics, it might be difficult to find conversations, especially older conversations. Private blogs Examples: Private WordPress Blog , Haven Pros Produces can use any tool / service to author and publish posts. Consumers can access via any browser of their choice. Cons May have to host your own service. Consumers may have to create accounts and passwords for each of the private blogs they follow. Fediverse Examples: Mastodon, PixelFed, PeerTube Pros Familar social media interface. More availability of apps and services that form part of the Fediverse make it easier for producers and consumers to use what makes the most sense for them. Cons Limited to the format supported by the app / service (i.e. Pixelfed is for images / video, PeerTube for video, etc.). May require you to host your own service. P2P Social Media Examples: ScuttleButt Pros Could be made for an audience of one but shared with other peers. Decentralized by default. Access individual feeds. Cons Similar to the Fediverse, being exposed to new protocols and technical terms may make onboarding and adoption challenging. May require you to host your own service. Protocols haven\u0027t been widely adopted so users are limited in the apps / services they can use for this. Forums Examples: NixOS Forum , Lemmy Pros Could create threads and conversations on various topics Easy to search and tag content Cons Someone would have to host a server Might not work well for small groups RSS feeds This is a variation on the private blog. Except instead of having a website that displays the posts, you just have an RSS feed. Examples: RSS-Only Club Pros Producers can use any software they\u0027d like to create and publish their RSS feed Consumers can use any feed reader to access and read content Everyone could have their own feed, so effectively they\u0027d be like profiles Cons You have to figure out how to produce and host the RSS feed Can\u0027t really control who sees this since the feed is public. Maybe private feeds could help here. Limited interaction. Users can\u0027t comment or reply to items in a feed. Note that this is not an exhaustive list of pros and cons, just a few that I thought of. I persoanlly would skew towards e-mail and messaging apps. The main reason at least with e-mail is that everyone is familiar with it and most likely won\u0027t have to create separate accounts on various silos to communicate and engage with others. With apps like DeltaChat you could actualy combine them and get the best of both. What are your thoughts?","contentType":"notes","url":"https://www.lqdev.me/notes/private-social-media","date":"2024-01-22 21:40 -05:00","tags":["indieweb","website","community","private","privacy","blog","protocols","rss","forums","journal","e-mail","fediverse","messaging"],"summary":"Brain dumping a bunch of half-formed thoughts. Over the past few days, I\u0027ve been thinking about different forms of sharing content. With the shifts in social media towards smaller, more private...","keywords":["cons","pros","service","examples:","they","feed","private","feeds","mail","everyone"]},{"id":"reshare-top-five-rss-feed-readers-verge","title":"The top five RSS readers for keeping up with your news feeds","content":"The top five RSS readers for keeping up with your news feeds \u2192 https://www.theverge.com/24036427/rss-feed-reader-best RSS readers allow you to collect the articles of specific sources in one app, making it a lot easier to find the content you\u2019re interested in without crawling through a lot of noise. Whatever RSS feed reader you choose, it\u2019s worth it to try at least one or two. This way, you can keep up with news from your favorite sources without depending on the chaos that is your email account or the random opinions from TikTok. Great overview of the various RSS feed readers out there.","contentType":"reshare","url":"https://www.lqdev.me/responses/top-five-rss-feed-readers-verge","date":"2024-01-22 20:49 -05:00","tags":["rss","news","feeds","indieweb","protocols"],"summary":"The top five RSS readers for keeping up with your news feeds \u2192 https://www.theverge.com/24036427/rss-feed-reader-best RSS readers allow you to collect the articles of specific sources in one app,...","keywords":["readers","feed","news","sources","reader","from","five","choose","it\u2019s","worth"]},{"id":"notes-a-la-sala-khruangbin","title":"A La Sala by Khruangbin out April 5, 2024","content":"The first half of 2024 will be good for new music. Chicano Batman is releasing something in the next few days. Last night I found out Khruangbin released a new single, Love International, and has a new album coming out called A La Sala on April 5th . I can\u0027t wait!","contentType":"notes","url":"https://www.lqdev.me/notes/a-la-sala-khruangbin","date":"2024-01-20 21:07 -05:00","tags":["music","newmusic","khruangbin","indie","album"],"summary":"The first half of 2024 will be good for new music. Chicano Batman is releasing something in the next few days. Last night I found out Khruangbin released a new single, Love International, and has a...","keywords":["half","sala","called","coming","album","international","love","single","released","khruangbin"]},{"id":"reply-vision-pros-most-important-app-safari","title":"The Vision Pro\u2019s first killer app is the web, whether Apple likes it or not","content":"The Vision Pro\u2019s first killer app is the web, whether Apple likes it or not \u2192 https://www.theverge.com/2024/1/20/24044343/apple-vision-pro-safari-killer-app it\u2019s increasingly clear that the early success of the Vision Pro, and much of the answer to the question of what this headset is actually for, will come from a single app: Safari. That\u2019s right, friends. Web browsers are back. ...at least at first, the open web is Apple\u2019s best chance to make its headset a winner. Because at least so far, it seems developers are not exactly jumping to build new apps for Apple\u2019s new platform. Some of the high-profile companies that have announced they\u2019re not yet building apps for the Vision Pro and its visionOS platform \u2014 Netflix, Spotify, YouTube, and others \u2014 are the very same ones that have loudly taken issue with how Apple runs the App Store. But what if you don\u2019t need the App Store to reach Apple users anymore? All this corporate infighting has the potential to completely change the way we use our devices, starting with the Vision Pro. ...we\u2019ve all stopped opening websites and started tapping app icons, but the age of the URL might be coming back. If you believe the open web is a good thing, and that developers should spend more time on their web apps and less on their native ones, this is a big win for the future of the internet. The problem is, it\u2019s happening after nearly two decades of mobile platforms systematically downgrading and ignoring their browsing experience...Mobile platforms treat browsers like webpage viewers, not app platforms, and it shows. There are some reasons for hope, though...the company appears to be still invested in making Safari work. Safari for visionOS will also come with some platform-specific features: you\u2019ll be able to open multiple windows at the same time and move them all around in virtual space. With a good browser and powerful PWAs, many users might mostly not notice the difference between opening the Spotify app and going to Spotify.com. That\u2019s a win for the whole web. here\u2019s the real question for Apple: which is more important, getting the Vision Pro off to a good start or protecting the sanctity of its App Store control at all costs? As Apple tries to create a platform shift to face computers, I\u2019m not sure it can have it both ways. Great article by David Pierce . As part of my website stats, I should probably start also counting authors I reference since many of the articles from the Verge I\u0027ve previously linked to are written by David. As someone who accesses services - \u0026quot;apps\u0026quot; - primarily through the web browser on desktop , this is exciting to see. While native apps have their advantages, the types of cross-platform connected experiences that can be delivered through the browser can\u0027t be ignored. First-class support for browsers in various platforms can only make these experiences even better. With more folks building their own platforms on the web on top of open standards that have been around for decades, I\u0027m excited for the future of the web.","contentType":"reply","url":"https://www.lqdev.me/responses/vision-pros-most-important-app-safari","date":"2024-01-20 13:29 -05:00","tags":["apple","safari","visionpro","web","browser","vr","xr","mr","ar","indieweb","openweb"],"summary":"The Vision Pro\u2019s first killer app is the web, whether Apple likes it or not \u2192 https://www.theverge.com/2024/1/20/24044343/apple-vision-pro-safari-killer-app it\u2019s increasingly clear that the early...","keywords":["vision","platform","platforms","apple","open","safari","apps","store","good","spotify"]},{"id":"notes-mindcage-movie","title":"Mindcage","content":"I didn\u0027t know Martin Lawrence was still making movies. I knew about Bad Boys but that was almost 4 years ago. Just watched Mindcage , one of his latest ones. It was alright. The story is similar to Silence of the Lambs. Detectives seeking help from a manipulative serial killer to catch another killer on the loose. The twist at the end was interesting and not what I was expecting.","contentType":"notes","url":"https://www.lqdev.me/notes/mindcage-movie","date":"2024-01-18 21:42 -05:00","tags":["movie","thriller","mystery"],"summary":"I didn\u0027t know Martin Lawrence was still making movies. I knew about Bad Boys but that was almost 4 years ago. Just watched Mindcage , one of his latest ones. It was alright. The story is similar to...","keywords":["killer","didn","twist","loose","another","catch","serial","manipulative","from","help"]},{"id":"notes-subscribed-to-1042-feeds-newsblur","title":"Subscribed to 1042 feeds on NewsBlur","content":"I use NewsBlur as my feed reader. In it, I subscribe to: newsletters personal blogs sports blogs news publications GitHub releases Mastodon feeds and tags Bluesky feeds StackOverflow tags subreddits podcasts severe weather alerts YouTube channels PeerTube instances and channels Lemmy instances and sublemmys? (I don\u0027t know the Lemmy lingo yet) Forums Basically, if it has an RSS feed, I subscribe to it there. Today I noticed I\u0027m subscribed over 1000 feeds (1042 to be exact)! Over 300 of them are personal blogs. This has taken a few years to curate. I remember my reader being empty when I first started. As I\u0027ve read newsletters and personal blogs , I\u0027ve followed things they\u0027ve linked to which is where I find even more feeds to subscribe to, especially when they\u0027re easy to find . There\u0027s some pruning that needs to happen at this point whether that\u0027s because of broken links or I just don\u0027t read those publications often. However, I find myself usually checking in on personal blogs and newsletters on a daily basis so I think many of those will stay.","contentType":"notes","url":"https://www.lqdev.me/notes/subscribed-to-1042-feeds-newsblur","date":"2024-01-17 21:06 -05:00","tags":["rss","newsblur","indieweb","feeds"],"summary":"I use NewsBlur as my feed reader. In it, I subscribe to: newsletters personal blogs sports blogs news publications GitHub releases Mastodon feeds and tags Bluesky feeds StackOverflow tags subreddits...","keywords":["blogs","personal","feeds","find","subscribe","newsletters","instances","tags","lemmy","they"]},{"id":"notes-simplifying-feed-publishing-workflow","title":"I need to optimize my response publishing workflow","content":"I like to use my response feed to bookmark and reshare interesting content I find online. Typically my workflow looks like the following: flowchart TD A[\u0026quot;Scroll through feed reader\u0026quot;] --\u0026gt; B[\u0026quot;Open articles I want to read in a new tab\u0026quot;] B --\u0026gt; C[\u0026quot;Read articles\u0026quot;] C --\u0026gt; D[\u0026quot;Create a response file to publish on my website\u0026quot;] D --\u0026gt; E[\u0026quot;Capture interesting information from the article in the response file\u0026quot;] E --\u0026gt; F[\u0026quot;Publish the response\u0026quot;] The bottleneck I usually find is in the middle steps of creating a response file and capturing the relevant content. This usually happens as I\u0027m reading the article. For a single file, it\u0027s not a big deal. Doing it for several can be time consuming, especially with the number of feeds I subscribe to . There\u0027s a few things I think could help here: Create one response file and do a daily link dump. Create a browser extension or system where I can: Highlight text or content on a page Right-click or use a keyboard shortcut to create an entry for the content I highlighted Create an entry in a database somewhere which captures the URL of the page I\u0027m looking at. I enter a few more details like the file name I want to use and response type When I\u0027m done with my session, I can publish. This will use the database content to compose the content for my response and create a PR The first option is simple but I don\u0027t like it because although most of the content I come across is tech related, I prefer having individual articles I can link to with their own tags to make it easier to build relationships. The second option sounds complex, but it\u0027s basically a web highlighting tool. I know there\u0027s a few out there already so maybe worth checking out. I\u0027d prefer to build my own to tailor it specifically to what I want just like my website generator. However, it would mean that I\u0027d have to spend some time thinking about how to implement it. It\u0027d be a fun project though. Maybe worthwhile experimenting with building an AI agent to help me use the content I captured to nicely format the response.","contentType":"notes","url":"https://www.lqdev.me/notes/simplifying-feed-publishing-workflow","date":"2024-01-17 20:47 -05:00","tags":["workflow","publishing","blog","personalknowledgemanagement","pkm"],"summary":"I like to use my response feed to bookmark and reshare interesting content I find online. Typically my workflow looks like the following: flowchart TD A[\u0026quot;Scroll through feed reader\u0026quot;] --\u0026gt;...","keywords":["response","content","create","\u0026quot","file","publish","want","prefer","read","usually"]},{"id":"reshare-how-discord-serves-15-million-users-using-one-server","title":"How Discord Serves 15-Million Users on One Server","content":"How Discord Serves 15-Million Users on One Server \u2192 https://blog.bytebytego.com/p/how-discord-serves-15-million-users In early summer 2022, the Discord operations team noticed unusually high activity on their dashboards. They thought it was a bot attack, but it was legitimate traffic from MidJourney - a new, fast-growing community for generating AI images from text prompts. To use MidJourney, you need a Discord account. Most MidJourney users join one main Discord server. This server grew so quickly that it soon hit Discord\u2019s old limit of around 1 million users per server. This is the story of how the Discord team creatively solved this challenge. Discord\u2019s real-time messaging backend is built with Elixir. Elixir runs on the BEAM virtual machine. BEAM was created for Erlang - a language optimized for large real-time systems requiring rock-solid reliability and uptime. A key capability BEAM provides is extremely lightweight parallel processes. This enables a single server to efficiently run tens or hundreds of thousands of processes concurrently. Elixir brings friendlier, Ruby-inspired syntax to the battle-tested foundation of BEAM. Combined they make it much easier to program massively scalable, fault-tolerant systems. So by leveraging BEAM\u0027s lightweight processes, the Elixir code powering Discord can \u0026quot;fan out\u0026quot; messages to hundreds of thousands of users around the world concurrently. However, limits emerge as communities grow larger.","contentType":"reshare","url":"https://www.lqdev.me/responses/how-discord-serves-15-million-users-using-one-server","date":"2024-01-17 20:40 -05:00","tags":["beam","erlang","elixir","discord","scale","enterprise","chat","communities"],"summary":"How Discord Serves 15-Million Users on One Server \u2192 https://blog.bytebytego.com/p/how-discord-serves-15-million-users In early summer 2022, the Discord operations team noticed unusually high activity...","keywords":["discord","beam","users","server","elixir","midjourney","processes","million","real","from"]},{"id":"bookmarks-willow-protocol","title":"Willow Protocol Specifications","content":"Willow Protocol Specifications \u2192 https://willowprotocol.org/ A protocol for peer-to-peer data stores. The best parts? Fine-grained permissions, a keen approach to privacy, destructive edits, and a dainty bandwidth and memory footprint.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/willow-protocol","date":"2024-01-17 20:38 -05:00","tags":["willow","protocol","p2p","privacy","data"],"summary":"Willow Protocol Specifications \u2192 https://willowprotocol.org/ A protocol for peer-to-peer data stores. The best parts? Fine-grained permissions, a keen approach to privacy, destructive edits, and a...","keywords":["protocol","peer","willow","bandwidth","dainty","edits","destructive","privacy","approach","keen"]},{"id":"bookmarks-stable-code-3b-stability","title":"Stable Code 3B - Coding on the Edge","content":"Stable Code 3B - Coding on the Edge \u2192 https://stability.ai/news/stable-code-2024-llm-code-completion-release Stable Code 3B is a 3 billion parameter Large Language Model (LLM), allowing accurate and responsive code completion at a level on par with models such as CodeLLaMA 7b that are 2.5x larger. Operates offline even without a GPU on common laptops such as a MacBook Air.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/stable-code-3b-stability","date":"2024-01-17 20:36 -05:00","tags":["ai","stabilityai\u0022","code","software","softwaredevelopment","llm"],"summary":"Stable Code 3B - Coding on the Edge \u2192 https://stability.ai/news/stable-code-2024-llm-code-completion-release Stable Code 3B is a 3 billion parameter Large Language Model (LLM), allowing accurate and...","keywords":["code","stable","completion","common","even","offline","operates","larger","codellama","models"]},{"id":"bookmarks-sampling-text-generation-huyen","title":"Sampling for Text Generation","content":"Sampling for Text Generation \u2192 https://huyenchip.com//2024/01/16/sampling.html ML models are probabilistic. Imagine that you want to know what\u2019s the best cuisine in the world. If you ask someone this question twice, a minute apart, their answers both times should be the same. If you ask a model the same question twice, its answer can change. This probabilistic nature makes AI great for creative tasks. However, this probabilistic nature also causes inconsistency and hallucinations. It\u2019s fatal for tasks that depend on factuality. Recently, I went over 3 months\u2019 worth of customer support requests of an AI startup I advise and found that \u2155 of the questions are because users don\u2019t understand or don\u2019t know how to work with this probabilistic nature. To understand why AI\u2019s responses are probabilistic, we need to understand how models generate responses, a process known as sampling (or decoding). This post consists of 3 parts. 1. Sampling: sampling strategies and sampling variables including temperature, top-k, and top-p. 2. Test time sampling: sampling multiple outputs to help improve a model\u2019s performance. 3. Structured outputs: how to get models to generate outputs in a certain format.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/sampling-text-generation-huyen","date":"2024-01-17 20:32 -05:00","tags":["ai","generativeai","statistics","llm"],"summary":"Sampling for Text Generation \u2192 https://huyenchip.com//2024/01/16/sampling.html ML models are probabilistic. Imagine that you want to know what\u2019s the best cuisine in the world. If you ask someone this...","keywords":["sampling","probabilistic","nature","models","understand","tasks","generate","outputs","don\u2019t","same"]},{"id":"reshare-podcast-youtube-era","title":"Podcasting is in its YouTube era","content":"Podcasting is in its YouTube era \u2192 https://www.theverge.com/2024/1/9/24032155/youtube-podcast-rss-spotify-apple-audacy-bankruptcy Today, YouTube at very long last debuts RSS integration. This means more hosts saying \u0026quot;...or wherever you get your podcasts.\u0026quot; \uD83D\uDE42 Support Post","contentType":"reshare","url":"https://www.lqdev.me/responses/podcast-youtube-era","date":"2024-01-17 20:24 -05:00","tags":["rss","podcasts","youtube"],"summary":"Podcasting is in its YouTube era \u2192 https://www.theverge.com/2024/1/9/24032155/youtube-podcast-rss-spotify-apple-audacy-bankruptcy Today, YouTube at very long last debuts RSS integration. This means...","keywords":["youtube","\u0026quot","podcasting","podcasts","wherever","saying","hosts","more","means","integration"]},{"id":"reshare-facebook-users-monitored-thousands-companies-markup","title":"Each Facebook User is Monitored by Thousands of Companies","content":"Each Facebook User is Monitored by Thousands of Companies \u2192 https://themarkup.org/privacy/2024/01/17/each-facebook-user-is-monitored-by-thousands-of-companies-study-indicates By now most internet users know their online activity is constantly tracked. But what is the scale of this surveillance? Judging from data collected by Facebook and newly described in a unique study by non-profit consumer watchdog Consumer Reports, it\u2019s massive, and examining the data may leave you with more questions than answers. Using a panel of 709 volunteers who shared archives of their Facebook data, Consumer Reports found that a total of 186,892 companies sent data about them to the social network. On average, each participant in the study had their data sent to Facebook by 2,230 companies. That number varied significantly, with some panelists\u2019 data listing over 7,000 companies providing their data. What Exactly Does This Data Contain? The data examined by Consumer Reports in this study comes from two types of collection: events and custom audiences. Both categories include information about what people do outside of Meta\u2019s platforms. Custom audiences allow advertisers to upload customer lists to Meta, often including identifiers like email addresses and mobile advertising IDs... The other category of data collection, \u201Cevents,\u201D describes interactions that the user had with a brand, which can occur outside of Meta\u2019s apps and in the real world. Events can include visiting a page on a company\u2019s website, leveling up in a game, visiting a physical store, or purchasing a product... How Can I See My Data? Facebook users can browse through the list of companies that have sent their data to Facebook by going to: [https://accountscenter.facebook.com/info_and_permissions](https://accountscenter.facebook.com/info_and_permissions)","contentType":"reshare","url":"https://www.lqdev.me/responses/facebook-users-monitored-thousands-companies-markup","date":"2024-01-17 20:09 -05:00","tags":["socialmedia","privacy","facebook","meta","surveillance"],"summary":"Each Facebook User is Monitored by Thousands of Companies \u2192 https://themarkup.org/privacy/2024/01/17/each-facebook-user-is-monitored-by-thousands-of-companies-study-indicates By now most internet...","keywords":["data","facebook","companies","consumer","study","user","reports","sent","com/info","events"]},{"id":"reshare-talking-open-source-llms-oxide-friends-willison","title":"Talking about Open Source LLMs on Oxide and Friends","content":"Talking about Open Source LLMs on Oxide and Friends \u2192 https://simonwillison.net/2024/Jan/17/oxide-and-friends/ I recorded an episode of the Oxide and Friends podcast on Monday, talking with Bryan Cantrill and Adam Leventhal about Open Source LLMs. Too important for a small group to control... This technology is clearly extremely important to the future of all sorts of things that we want to do. I am totally on board with it. There are people who will tell you that it\u2019s all hype and bluster. I\u2019m over that. This stuff\u2019s real. It\u2019s really useful. It is far too important for a small group of companies to completely control this technology. That would be genuinely disastrous. And I was very nervous that was going to happen, back when it was just OpenAI and Anthropic that had the only models that were any good, that was really nerve-wracking. Today I\u2019m not afraid of that at all, because there are dozens of organizations now that have managed to create one of these things... On LLMs for learning... One of the most exciting things for me about this technology is that it\u2019s a teaching assistant that is always available to you. You know that thing where you\u2019re learning\u2014especially in a classroom environment\u2014and you miss one little detail and you start falling further and further behind everyone else because there was this one little thing you didn\u2019t quite catch, and you don\u2019t want to ask stupid questions? You can ask stupid questions of ChatGPT anytime you like and it can help guide you through to the right answer. That\u2019s kind of a revelation.","contentType":"reshare","url":"https://www.lqdev.me/responses/talking-open-source-llms-oxide-friends-willison","date":"2024-01-17 20:03 -05:00","tags":["ai","podcast","opensource","llm"],"summary":"Talking about Open Source LLMs on Oxide and Friends \u2192 https://simonwillison.net/2024/Jan/17/oxide-and-friends/ I recorded an episode of the Oxide and Friends podcast on Monday, talking with Bryan...","keywords":["things","llms","it\u2019s","technology","important","thing","really","little","further","want"]},{"id":"bookmarks-singsong-ai-music-accompaniment","title":"SingSong - Generating musical accompaniments from singing","content":"SingSong - Generating musical accompaniments from singing \u2192 https://arxiv.org/abs/2301.12662 We present SingSong, a system that generates instrumental music to accompany input vocals, potentially offering musicians and non-musicians alike an intuitive new way to create music featuring their own voice. To accomplish this, we build on recent developments in musical source separation and audio generation. Specifically, we apply a state-of-the-art source separation algorithm to a large corpus of music audio to produce aligned pairs of vocals and instrumental sources. Then, we adapt AudioLM (Borsos et al., 2022) -- a state-of-the-art approach for unconditional audio generation -- to be suitable for conditional \u0026quot;audio-to-audio\u0026quot; generation tasks, and train it on the source-separated (vocal, instrumental) pairs. In a pairwise comparison with the same vocal inputs, listeners expressed a significant preference for instrumentals generated by SingSong compared to those from a strong retrieval baseline. Sound examples at this https URL AI can now help you create a backing track to all the songs you make up about your pets.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/singsong-ai-music-accompaniment","date":"2024-01-17 19:39 -05:00","tags":["ai","generativeai","music"],"summary":"SingSong - Generating musical accompaniments from singing \u2192 https://arxiv.org/abs/2301.12662 We present SingSong, a system that generates instrumental music to accompany input vocals, potentially...","keywords":["audio","singsong","generation","source","music","instrumental","pairs","state","vocal","separation"]},{"id":"reshare-notion-cron-calendar-app","title":"Notion\u2019s new calendar app is designed to keep your meetings organized","content":"Notion\u2019s new calendar app is designed to keep your meetings organized \u2192 https://www.theverge.com/2024/1/17/24041330/notion-calendar-app After acquiring Cron in 2022, Notion is bringing the calendar app fully into its all-in-one workspace. The big new feature coming with the rebranding is Notion integration. If you or your company uses Notion, you\u2019ll be able to create or link Notion documents inside a calendar invite. If you have a database filled with due dates, you can add that as a calendar to Notion Calendar. It sounds like a much better way to handle agendas and notes than sending them around before and after a meeting or hunting for them in your Slack. Putting everything in the calendar event is a good move. This is one of the reasons I like org-mode in Emacs. Being able to annotate documents with timestamps and deadlines that show up and you can organize inside the Agenda view is so powerful. The integrations and learning curve is steeper compared to a tool like Notion but I find it simple and powerful enough for GTD-style workflows, I\u0027d have a hard time moving. I have yet to use AnyType, so maybe after trying that , I choose to shift some of my workflows there.","contentType":"reshare","url":"https://www.lqdev.me/responses/notion-cron-calendar-app","date":"2024-01-17 19:33 -05:00","tags":["calendar","notion","productivity","gtd","software","app","pkm"],"summary":"Notion\u2019s new calendar app is designed to keep your meetings organized \u2192 https://www.theverge.com/2024/1/17/24041330/notion-calendar-app After acquiring Cron in 2022, Notion is bringing the calendar...","keywords":["calendar","notion","powerful","able","documents","inside","workflows","show","deadlines","timestamps"]},{"id":"notes-reading-list-january-2024","title":"Building up the reading backlog","content":"Lately I\u0027ve been using Spotify to listen to audiobooks since it\u0027s included as part of their Premium tier. That\u0027s how I started listening to Making It In America . What I didn\u0027t know was that there\u0027s a limit to the number of hours you can listen in a given month. I only got as far as chapter 2 and now I have to wait 7 days for my listening hours to be replenished for the month. As a result, I started building the reading backlog at the local library which is now at an unreasonable 164 books that I probably won\u0027t get to. It got that way because I added many of the Best of 2023 recommendations from my local library to my wishlist. I haven\u0027t added many of those to my library page because I don\u0027t want to go through the process of manually adding them. I should look into whether there\u0027s an automated way of getting that information via APIs or batch download. Last night I started reading How To Sell A Haunted House and I\u0027m enjoying it so far. The next one on deck is A City on Mars .","contentType":"notes","url":"https://www.lqdev.me/notes/reading-list-january-2024","date":"2024-01-16 20:36 -05:00","tags":["books","reading","library"],"summary":"Lately I\u0027ve been using Spotify to listen to audiobooks since it\u0027s included as part of their Premium tier. That\u0027s how I started listening to Making It In America . What I didn\u0027t know was that there\u0027s...","keywords":["started","library","added","many","listen","hours","local","month","listening","reading"]},{"id":"reshare-openmentions","title":"OpenMentions","content":"OpenMentions \u2192 https://openmentions.com/ OpenMentions is a project designed to use Webmentions and ActivityPub for topical content discovery. The site is organised along the lines of a hierarchy of topics going from broad to fine. This we call OpenTopic \u2013 the idea being that many sites could host the full collection of topics so that the loss of any one site is not the loss of all topics. The intention is that this site should own nothing and that topic hierarchies are organic and discoverable.","contentType":"reshare","url":"https://www.lqdev.me/responses/openmentions","date":"2024-01-16 20:21 -05:00","tags":["webmentions","indieweb","activitypub","online"],"summary":"OpenMentions \u2192 https://openmentions.com/ OpenMentions is a project designed to use Webmentions and ActivityPub for topical content discovery. The site is organised along the lines of a hierarchy of...","keywords":["topics","site","openmentions","loss","call","opentopic","idea","many","sites","host"]},{"id":"reshare-lefoverlocals-llm-responses-leaked-gpu-memory","title":"LeftoverLocals: Listening to LLM responses through leaked GPU local memory","content":"LeftoverLocals: Listening to LLM responses through leaked GPU local memory \u2192 https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/ We are disclosing LeftoverLocals: a vulnerability that allows recovery of data from GPU local memory created by another process on Apple, Qualcomm, AMD, and Imagination GPUs. LeftoverLocals impacts the security posture of GPU applications as a whole, with particular significance to LLMs and ML models run on impacted GPU platforms. By recovering local memory\u2014an optimized GPU memory region\u2014we were able to build a PoC where an attacker can listen into another user\u2019s interactive LLM session (e.g., llama.cpp) across process or container boundaries","contentType":"reshare","url":"https://www.lqdev.me/responses/lefoverlocals-llm-responses-leaked-gpu-memory","date":"2024-01-16 20:16 -05:00","tags":["security","llm","ai"],"summary":"LeftoverLocals: Listening to LLM responses through leaked GPU local memory \u2192 https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/ We are...","keywords":["local","memory","leftoverlocals:","listening","responses","leaked","process","another","significance","llms"]},{"id":"reshare-easy-ways-improve-privacy-security-2024","title":"Easy Ways to Improve Your Privacy \u0026 Security in 2024","content":"Easy Ways to Improve Your Privacy \u0026amp; Security in 2024 \u2192 https://blog.thenewoil.org/easy-ways-to-improve-your-privacy-and-security-in-2024 Every year, I like to remind everyone to go back to the basics. For those who are new to privacy and security and may be trying to create some new, positive habits, this serves as a great entry point. For veteran privacy enthusiasts, the basics form our foundation for more advanced techniques later, making it imperative to ensure we cover all those bases. So in that spirit, let\u2019s all pause \u2013 wherever we are in our privacy journeys \u2013 to do a quick check and make sure we\u2019ve got the basics covered. If you\u2019re one of those new people I mentioned, welcome! But also know that this post is packed with information, so try not to get overwhelmed. Maybe bookmark this post and do one thing per day or something like that. Strong Passwords... Multi-Factor Authentication (MFA)... Regular Software Updates... Secure Your Wi-Fi Network... Be Cautious with Communications... Review App Permissions... Review Your Account Settings... Secure Browsing Habits... Device Security... Review Financial Statements... Educate Yourself...","contentType":"reshare","url":"https://www.lqdev.me/responses/easy-ways-improve-privacy-security-2024","date":"2024-01-15 16:54 -05:00","tags":["privacy","security"],"summary":"Easy Ways to Improve Your Privacy \u0026amp; Security in 2024 \u2192 https://blog.thenewoil.org/easy-ways-to-improve-your-privacy-and-security-in-2024 Every year, I like to remind everyone to go back to the...","keywords":["privacy","security","basics","review","those","ways","improve","2024","secure","habits"]},{"id":"bookmarks-ai-for-economists-prompts","title":"AI for economists - prompts and resources","content":"AI for economists - prompts and resources \u2192 https://sites.google.com/view/lastunen/ai-for-economists This page contains example prompts and responses intended to showcase how generative AI, namely LLMs like GPT-4, can benefit economists. Example prompts are shown from six domains: ideation and feedback; writing; background research; coding; data analysis; and mathematical derivations. The framework as well as some of the prompts and related notes come from Korinek, A. 2023. \u201CGenerative AI for Economic Research: Use Cases and Implications for Economists\u201C , Journal of Economic Literature, 61 (4): 1281\u20131317. Each application area includes 1-3 prompts and responses from an LLM, often from the field of development economics, along with brief notes. The prompts will be updated periodically.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/ai-for-economists-prompts","date":"2024-01-15 16:49 -05:00","tags":["ai","economy","promptengineering","llm","gpt"],"summary":"AI for economists - prompts and resources \u2192 https://sites.google.com/view/lastunen/ai-for-economists This page contains example prompts and responses intended to showcase how generative AI, namely...","keywords":["prompts","from","economists","notes","economic","example","responses","come","korinek","2023"]},{"id":"reply-hands-on-clicks-iphone","title":"Hands-on - Clicks for iPhone","content":"Hands-on - Clicks for iPhone \u2192 https://9to5mac.com/2024/01/12/clicks-iphone-hands-on/ Smartphones and physical keyboards aren\u2019t a combination we think of often, but Clicks for iPhone is trying to bring that back with a new keyboard case that\u2019s extremely good. Wishful thinking on my end but I\u0027d buy a Blackberry-like device. Not a \u0026quot;smartphone\u0026quot;, but an internet-connected device with all the phone capabilities that has a physical keyboard.","contentType":"reply","url":"https://www.lqdev.me/responses/hands-on-clicks-iphone","date":"2024-01-15 16:45 -05:00","tags":["smarphone","keyboard","iphone","featurephone"],"summary":"Hands-on - Clicks for iPhone \u2192 https://9to5mac.com/2024/01/12/clicks-iphone-hands-on/ Smartphones and physical keyboards aren\u2019t a combination we think of often, but Clicks for iPhone is trying to...","keywords":["iphone","hands","device","physical","keyboard","clicks","smartphones","connected","internet","smartphone\u0026quot"]},{"id":"reshare-goblin-tumblr-fediverse","title":"Goblin - Tumblr meets the Fediverse","content":"Goblin - Tumblr meets the Fediverse \u2192 https://goblin.band/ Goblin band is an attempt to replicate the colletive creative energy that happens on tumblr and take it to the fediverse Repo : https://github.com/johnHackworth/goblin","contentType":"reshare","url":"https://www.lqdev.me/responses/goblin-tumblr-fediverse","date":"2024-01-15 16:14 -05:00","tags":["tumblr","fediverse","activitypub","automattic","firefish","socialmedia","opensource","community","decentralization","federation"],"summary":"Goblin - Tumblr meets the Fediverse \u2192 https://goblin.band/ Goblin band is an attempt to replicate the colletive creative energy that happens on tumblr and take it to the fediverse Repo :...","keywords":["goblin","tumblr","fediverse","meets","https://goblin","band/","band","attempt","replicate","colletive"]},{"id":"bookmarks-clarkesworld-sci-fi-fantasy-magazine","title":"Clarkesworld - Sci-fi and Fantasy Magazine","content":"Clarkesworld - Sci-fi and Fantasy Magazine \u2192 https://clarkesworldmagazine.com/ Clarkesworld is a monthly science fiction and fantasy magazine first published in October 2006. Each issue contains interviews, thought-provoking articles, and between six and eight works of original fiction.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/clarkesworld-sci-fi-fantasy-magazine","date":"2024-01-15 15:00 -05:00","tags":["magazine","scifi","fantasy","online","fiction","nonfiction","art"],"summary":"Clarkesworld - Sci-fi and Fantasy Magazine \u2192 https://clarkesworldmagazine.com/ Clarkesworld is a monthly science fiction and fantasy magazine first published in October 2006. Each issue contains...","keywords":["clarkesworld","fantasy","magazine","fiction","eight","articles","provoking","thought","interviews","contains"]},{"id":"reshare-chicano-batman-fly-01-2024","title":"Fly - New Chicano Batman out 1/23","content":"Fly - New Chicano Batman out 1/23 \u2192 https://twitter.com/ChicanoBatman/status/1746940313678299500 Starting off the year right with new Chicano Batman ! \u201CFly\u201D is yours 1/23 \uD83D\uDD4A And this is just the beginning!","contentType":"reshare","url":"https://www.lqdev.me/responses/chicano-batman-fly-01-2024","date":"2024-01-15 12:36 -05:00","tags":["music","chicanobatman"],"summary":"Fly - New Chicano Batman out 1/23 \u2192 https://twitter.com/ChicanoBatman/status/1746940313678299500 Starting off the year right with new Chicano Batman ! \u201CFly\u201D is yours 1/23 \uD83D\uDD4A And this is just the...","keywords":["chicano","batman","1/23","https://twitter","com/chicanobatman/status/1746940313678299500","starting","year","\u201Cfly\u201D","yours","beginning"]},{"id":"reshare-floss-weekly-now-on-hackaday","title":"FLOSS Weekly is now on Hackaday!","content":"FLOSS Weekly is now on Hackaday! \u2192 https://hackaday.com/2023/12/20/floss-weekly-episode-762-spilling-the-tea/ We\u2019re excited to announce that Hackaday is the new home of FLOSS Weekly, a long-running podcast about free, libre, and open-source software! The TWiT network hosted the podcast for an incredible seventeen years, but due to some changes on their end, they recently had to wind things down. They were gracious enough to let us pick up the torch, with Jonathan Bennett now taking over hosting duties. That didn\u0027t take long. Last month I learned FLOSS Weekly was ending on the TWiT network . It\u0027s great to see it has found a new home in Hackaday! Time to update the RSS feeds and podroll .","contentType":"reshare","url":"https://www.lqdev.me/responses/floss-weekly-now-on-hackaday","date":"2024-01-15 12:15 -05:00","tags":["podcast","floss","hackaday","opensource","technology","internet"],"summary":"FLOSS Weekly is now on Hackaday! \u2192 https://hackaday.com/2023/12/20/floss-weekly-episode-762-spilling-the-tea/ We\u2019re excited to announce that Hackaday is the new home of FLOSS Weekly, a long-running...","keywords":["weekly","floss","hackaday","network","twit","podcast","long","home","they","didn"]},{"id":"notes-gravatar-digital-profile-beyond-blog-comments","title":"Gravatar - Your digital profile beyond blog comments","content":"I had known about Gravatar since its early days. I was under the (incorrect) impression that it could only be used to comment on blogs. I thought when you created an account, you added your name, e-mail, and image as part of your profile. Then, when you wanted to comment on blogs, instead of creating individual accounts or profiles, you could just reuse your Gravatar profile. That\u0027s partially true. However, I\u0027m happy to report I was wrong and there\u0027s so much more. While commenting on Matt Mullenweg\u0027s Birthday Gift post , I didn\u0027t use a Gravatar profile. This prompted Matt to kindly respond via e-mail and suggest I check out Gravatar. I put it off for a few days. Eventually, I decided to check it out and I\u0027m glad I did. Thanks Matt ! The signup process was simple. All I had to do was enter my e-mail address. This immediately sent me a confirmation e-mail which I then used to proceed with onboarding. Here\u0027s where things got interesting. You can add all the things I was already aware of (name, e-mail, photo). However, you can also do much more than that! You can add your own verified accounts, links, contact information, and payments. As of the time of this writing these options include: Accounts WordPress Instagram X / Twitter TikTok Tumblr Mastodon GitHub Twitch Fediverse Stackoverflow Calendly Vimeo TripIt Foursquare Contact info E-mail Phone number (home, work, mobile) Contact form Calendar Payments PayPal Patron Venmo Crypto Bitcoin Litecoin Dogecoin Ethereum XRP Cardano Custom In addition to those integrations, you can also provide as many of your own custom links as you\u0027d like. In my case, I\u0027ve chosen to link to my website and RSS feeds . The options are endless though. I don\u0027t have a Linktree profile so I can\u0027t objectively say how Gravatar compares to those offerings. At their core though, Gravatar and Linktree seem to address similar problems. If you\u0027re not yet committed to having your own place on the internet (website/blog) or if you\u0027re looking for a way to organize your digital identity in single place, Gravatar is an excellent option. I\u0027d encourage you to check it out. You can find my Gravatar profile at lqdev.me/gravatar !","contentType":"notes","url":"https://www.lqdev.me/notes/gravatar-digital-profile-beyond-blog-comments","date":"2024-01-15 11:18 -05:00","tags":["profile","online","gravatar","website","web","comments","internet","automattic","wordpress","identity","linktree","indieweb","socialmedia","blog","bloggin"],"summary":"I had known about Gravatar since its early days. I was under the (incorrect) impression that it could only be used to comment on blogs. I thought when you created an account, you added your name,...","keywords":["gravatar","mail","profile","contact","check","matt","accounts","custom","much","linktree"]},{"id":"bookmarks-every-newsletter","title":"Every - Daily Newsletter","content":"Every - Daily Newsletter \u2192 https://every.to/ Every is a daily newsletter founded in 2020. Every day, we publish a long-form essay to make you smarter about technology, productivity, and AI.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/every-newsletter","date":"2024-01-15 11:08 -05:00","tags":["newsletter","technology","productivity","ai","essay","internet"],"summary":"Every - Daily Newsletter \u2192 https://every.to/ Every is a daily newsletter founded in 2020. Every day, we publish a long-form essay to make you smarter about technology, productivity, and AI.","keywords":["daily","newsletter","https://every","founded","2020","publish","long","form","essay","smarter"]},{"id":"reshare-migrating-substack-self-hosted-ghost-white","title":"Migrating from Substack to self-hosted Ghost","content":"Migrating from Substack to self-hosted Ghost \u2192 https://citationneeded.news/substack-to-self-hosted-ghost/ I have found myself with a roughly $103/mo setup... However, more important to me than the exact price is the degree of control I have over my own not-a-platform... I realize that this is...a lot. If you are a newsletter writer looking to flee the Substack ship, please don\u0027t let this discourage you. Love seeing this! Whether through self-hosted or hosted options, I hope more people get to experience the benefits of owning their own platform.","contentType":"reshare","url":"https://www.lqdev.me/responses/migrating-substack-self-hosted-ghost-white","date":"2024-01-14 16:11 -05:00","tags":["substack","selfhost","indieweb","ghost"],"summary":"Migrating from Substack to self-hosted Ghost \u2192 https://citationneeded.news/substack-to-self-hosted-ghost/ I have found myself with a roughly $103/mo setup... However, more important to me than the...","keywords":["hosted","self","platform","substack","more","newsletter","writer","looking","flee","ship"]},{"id":"bookmarks-cool-uris-dont-change","title":"Cool URIs don\u0027t change","content":"Cool URIs don\u0026#39;t change \u2192 https://www.w3.org/Provider/Style/URI Keeping URIs so that they will still be around in 2, 20 or 200 or even 2000 years is clearly not as simple as it sounds. However, all over the Web, webmasters are making decisions which will make it really difficult for themselves in the future. Often, this is because they are using tools whose task is seen as to present the best site in the moment, and no one has evaluated what will happen to the links when things change. The message here is, however, that many, many things can change and your URIs can and should stay the same. They only can if you think about how you design them.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/cool-uris-dont-change","date":"2024-01-13 10:23 -05:00","tags":["url","http","standard","protocol","web","internet"],"summary":"Cool URIs don\u0026#39;t change \u2192 https://www.w3.org/Provider/Style/URI Keeping URIs so that they will still be around in 2, 20 or 200 or even 2000 years is clearly not as simple as it sounds. However,...","keywords":["uris","change","they","many","things","however","cool","tools","whose","task"]},{"id":"bookmarks-where-is-all-fediverse","title":"Where is all of the Fediverse?","content":"Where is all of the Fediverse? \u2192 https://blog.benjojo.co.uk/post/who-hosts-the-fediverse-instances Here we can see that Fastly and Cloudflare make up over 50% of the entire fediverse network. ...for the population using Cloudflare, a fair number of them (30%) appear to be hosting the instances behind a home broadband connection. ...the German hosting provider Hetzner hosts over 51% of the entire network!","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/where-is-all-fediverse","date":"2024-01-13 10:15 -05:00","tags":["fediverse","analytics","web"],"summary":"Where is all of the Fediverse? \u2192 https://blog.benjojo.co.uk/post/who-hosts-the-fediverse-instances Here we can see that Fastly and Cloudflare make up over 50% of the entire fediverse network. ...for...","keywords":["fediverse","hosts","instances","cloudflare","entire","network","hosting","german","connection","broadband"]},{"id":"reshare-perplexity-ai-pivots-open-source","title":"More than an OpenAI Wrapper: Perplexity Pivots to Open Source","content":"More than an OpenAI Wrapper: Perplexity Pivots to Open Source \u2192 https://thenewstack.io/more-than-an-openai-wrapper-perplexity-pivots-to-open-source/ ...Perplexity has become a surprisingly strong player in a market otherwise dominated by OpenAI, Microsoft, Google and Meta. At its core, Perplexity is a search engine. ...over the past year, Perplexity has evolved rapidly. It now has its own search index and has built its own LLMs based on open source models. They\u2019ve also begun to combine their proprietary technology products. At the end of November, Perplexity announced two new \u201Conline LLMs\u201D \u2014 LLMs combined with a search index \u2014 called pplx-7b-online and pplx-70b-online. They were built on top of the open source models mistral-7b and llama2-70b. Using open source models has been critical for the growth of Perplexity. ...the default Perplexity model still relies on GPT 3.5 (and a dash of LLaMA-2). But the intention is to move away from that long-standing reliance on OpenAI for its base model.","contentType":"reshare","url":"https://www.lqdev.me/responses/perplexity-ai-pivots-open-source","date":"2024-01-13 09:43 -05:00","tags":["ai","perplexity","opensource","search","llm"],"summary":"More than an OpenAI Wrapper: Perplexity Pivots to Open Source \u2192 https://thenewstack.io/more-than-an-openai-wrapper-perplexity-pivots-to-open-source/ ...Perplexity has become a surprisingly strong...","keywords":["perplexity","open","openai","source","models","search","online","pplx","model","index"]},{"id":"bookmarks-marimo-notebook","title":"Marimo","content":"Marimo \u2192 https://marimo.io/ marimo is an open-source reactive notebook for Python \u2014 reproducible, git-friendly, executable as a script, and shareable as an app.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/marimo-notebook","date":"2024-01-13 09:38 -05:00","tags":["notebook","programming","literateprogramming"],"summary":"Marimo \u2192 https://marimo.io/ marimo is an open-source reactive notebook for Python \u2014 reproducible, git-friendly, executable as a script, and shareable as an app.","keywords":["marimo","https://marimo","open","source","reactive","notebook","python","reproducible","friendly","executable"]},{"id":"bookmarks-indieweb-assimilation-shellshark","title":"IndieWeb assimilation","content":"IndieWeb assimilation \u2192 https://shellsharks.com/indieweb Principle Mechanics This section does not provide exhaustive coverage of how to implement IndieWeb functionality. Instead, I simply summarize five core primitives which I feel comprise an IndieWeb site. For a more official gauge on where a site scores within the IndieWeb spectrum, consider leveraging IndieMark! Hosting: You need a place to host your site and store your content. There are a lot of great options out there. Ideally, choose one that allows you the ability to make some under-the-hood changes and does not limit your content portability. Syndication: Share your content with the world! There are two preferred methods for syndication, PESOS and POSSE. This resource does a great job explaining both! For more examples of how this is done, check this and this out. RSS is a great starting point for helping others subscribe to new content on your site. Writing: Though your site could simply serve as a more static point/identity on the web, with little to no \u201Ccontent\u201D being regularly added, I recommend writing! Interactivity: One of the more advanced concepts within the IndieWeb world, the ability to bake in native comments, replies, likes, etc is a greay way to build community. This interactivity helps mitigate reliance on centralized social networks for communication within Indie communities. One example of IndieWeb interactivity is Webmentions. Identity: Make it unique, make it fun, make it yours. The corporate web is sterile and suffocating. Let\u2019s bring back the whimsy of the old web.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/indieweb-assimilation-shellshark","date":"2024-01-13 09:05 -05:00","tags":["indieweb","guide","catalog"],"summary":"IndieWeb assimilation \u2192 https://shellsharks.com/indieweb Principle Mechanics This section does not provide exhaustive coverage of how to implement IndieWeb functionality. Instead, I simply summarize...","keywords":["indieweb","site","more","content","within","does","simply","world","interactivity","ability"]},{"id":"bookmarks-ai-pipelines-sped-up-again-alexirpan","title":"My AI Timelines Have Sped Up (Again)","content":"My AI Timelines Have Sped Up (Again) \u2192 https://www.alexirpan.com/2024/01/10/ai-timelines-2024.html ...computers are useful, ML models are useful, and even if models fail to scale, people will want to fit GPT-4 sized models on their phone. It seems reasonable to assume the competing factions will figure something out. Data seems like the harder question. (Or at least the one I feel qualified talking about.) We have already crossed the event horizon of trying to train on everything on the Internet. It\u2019s increasingly difficult for labs to differentiate themselves on publicly available data. Differentiation is instead coming from non-public high-quality data to augment public low-quality data. All the scaling laws have followed power laws so far, including dataset size. Getting more data by hand doesn\u2019t seem good enough to cross to the next thresholds. We need better means to get good data. A long time ago, when OpenAI still did RL in games / simulation, they were very into self-play. You run agents against copies of themselves, score their interactions, and update the models towards interactions with higher reward. Given enough time, they learn complex strategies through competition. I think it\u2019s possible we\u2019re at the start of a world where self-play or self-play-like ideas work to improve LLM capabilities. Drawing an analogy, the environment is the dialogue, actions are text generated from an LLM, and the reward is from whatever reward model you have. Instead of using ground truth data, our models may be at a point where they can generate data that\u2019s good enough to train on.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/ai-pipelines-sped-up-again-alexirpan","date":"2024-01-11 10:07 -05:00","tags":["ai","predictions","agi","data","llm","technology","ml","computervision"],"summary":"My AI Timelines Have Sped Up (Again) \u2192 https://www.alexirpan.com/2024/01/10/ai-timelines-2024.html ...computers are useful, ML models are useful, and even if models fail to scale, people will want to...","keywords":["data","models","good","enough","reward","play","self","they","from","timelines"]},{"id":"bookmarks-introducing-gpt-store","title":"Introducing the GPT Store","content":"Introducing the GPT Store \u2192 https://openai.com/blog/introducing-the-gpt-store It\u2019s been two months since we announced GPTs, and users have already created over 3 million custom versions of ChatGPT. Many builders have shared their GPTs for others to use. Today, we\u0027re starting to roll out the GPT Store to ChatGPT Plus, Team and Enterprise users so you can find useful and popular GPTs.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/introducing-gpt-store","date":"2024-01-11 10:01 -05:00","tags":["openai","gpt","ai"],"summary":"Introducing the GPT Store \u2192 https://openai.com/blog/introducing-the-gpt-store It\u2019s been two months since we announced GPTs, and users have already created over 3 million custom versions of ChatGPT....","keywords":["store","gpts","users","chatgpt","introducing","find","enterprise","team","plus","roll"]},{"id":"bookmarks-python-3-13-gets-jit","title":"Python 3.13 gets a JIT","content":"Python 3.13 gets a JIT \u2192 https://tonybaloney.github.io/posts/python-gets-a-jit.html In late December 2023 (Christmas Day to be precise), CPython core developer Brandt Bucher submitted a little pull-request to the Python 3.13 branch adding a JIT compiler.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/python-3-13-gets-jit","date":"2024-01-09 09:53 -05:00","tags":["python","programming","compiler"],"summary":"Python 3.13 gets a JIT \u2192 https://tonybaloney.github.io/posts/python-gets-a-jit.html In late December 2023 (Christmas Day to be precise), CPython core developer Brandt Bucher submitted a little...","keywords":["python","gets","branch","request","pull","little","submitted","bucher","brandt","developer"]},{"id":"reply-replacing-apps-websites-kingett","title":"Replacing apps with websites","content":"Replacing apps with websites \u2192 https://robertkingett.com/posts/6421/ I woke up today...and decided I was going to delete all the apps off my iPhone. If I didn\u2019t need it to do a specialized function? Gone, poof. I decided to just use websites and bookmarks instead. The websites worked just fine. ...websites are just badly designed these days, especially with using a screen reader on a mobile device. I can\u2019t quite describe it, because mobile web browsers aren\u2019t really well designed either, so it makes the website worse because it doesn\u2019t even render all elements as smoothly as on desktop. Even though there are problems, I\u2019m honestly glad I deleted almost all my apps off my phone and started to pin websites to my home screen more. For one thing, it cuts down on notifications. it\u2019s super freeing to not get a random notification because you didn\u2019t open the app in a day, so the app pings you to say hey I\u2019m still here please pay attention to me, I feel lonely, and nobody will give me animals to snuggle with. With pinned websites, my phone is faster and my home screen is much more organized as well. I did this a few years ago. Websites work just fine for almost everything I need to do. The main benefits I noticed: Less distractions More real estate when viewing websites on laptop / desktop Less apps to drain the battery when running on the background More restricted access / permissions (i.e. websites don\u0027t need unrestricted access to my contacts or other information on my phone) Better PWA support on mobile platforms would go a long way here to better balance between apps and websites. In the meantime though, pinning websites and websites overall work just fine for most things I need to do day-to-day.","contentType":"reply","url":"https://www.lqdev.me/responses/replacing-apps-websites-kingett","date":"2024-01-09 09:37 -05:00","tags":["apps","websites","internet","mobile"],"summary":"Replacing apps with websites \u2192 https://robertkingett.com/posts/6421/ I woke up today...and decided I was going to delete all the apps off my iPhone. If I didn\u2019t need it to do a specialized function?...","keywords":["websites","apps","more","need","mobile","screen","fine","phone","home","less"]},{"id":"bookmarks-tiobe-index-january-2024","title":"TIOBE Index for January 2024","content":"TIOBE Index for January 2024 \u2192 https://www.tiobe.com/tiobe-index/ For the first time in the history of the TIOBE index, C# has won the programming language of the year award. Congratulations! C# has been a top 10 player for more than 2 decades and now that it is catching up with the big 4 languages, it won the well-deserved award by being the language with the biggest uptick in one year (\u002B1.43%). Exciting to see F# almost break into the Top 20 at number 22 with 0.77%.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/tiobe-index-january-2024","date":"2024-01-07 22:52 -05:00","tags":["programming","dotnet","technology","fsharp","csharp","programminglanguages"],"summary":"TIOBE Index for January 2024 \u2192 https://www.tiobe.com/tiobe-index/ For the first time in the history of the TIOBE index, C# has won the programming language of the year award. Congratulations! C# has...","keywords":["tiobe","index","language","year","award","almost","exciting","uptick","biggest","deserved"]},{"id":"reply-blog-posts-can-be-any-length-matt","title":"A blog post can be any length","content":"A blog post can be any length \u2192 https://ma.tt/2024/01/birthday-gift/","contentType":"reply","url":"https://www.lqdev.me/responses/blog-posts-can-be-any-length-matt","date":"2024-01-07 22:37 -05:00","tags":["writing","blog","web","internet"],"summary":"A blog post can be any length \u2192 https://ma.tt/2024/01/birthday-gift/","keywords":["blog","post","length","https://ma","tt/2024/01/birthday","gift/"]},{"id":"notes-new-rss-feed-links","title":"New RSS links","content":"Based on the post, own your RSS links , I decided to set up redirects for all my RSS feeds which was very easy to do using Azure CDN. You can find the updated links in my feed page . If you\u0027re currently subscribed to any of those feeds, you can keep using the old URLs but I\u0027d recommend updating your feed reader with the latest URLs.","contentType":"notes","url":"https://www.lqdev.me/notes/new-rss-feed-links","date":"2024-01-07 19:47 -05:00","tags":["rss","indieweb","identity","social","posts"],"summary":"Based on the post, own your RSS links , I decided to set up redirects for all my RSS feeds which was very easy to do using Azure CDN. You can find the updated links in my feed page . If you\u0027re...","keywords":["links","urls","feeds","using","feed","based","updating","recommend","keep","those"]},{"id":"reply-own-your-rss-links","title":"Own Your RSS Links","content":"Own Your RSS Links \u2192 https://heydingus.net/blog/2024/1/please-own-your-rss-links ...owning the address where your audience finds you is important. It allows you to be mobile, nimble, and without attached strings. It helps you show off all the things and places you want folks to see because you can put all these URLs on your /feeds page. It\u2019s user-friendly in more ways than one (pretty cool how you can make all those URLs human-readable, huh?). ...it means your audience never has to think about how they\u2019re going to get your stuff. This is a great idea. I\u0027ve written before about owning your links . Today, that\u0027s how I expose many of my links in the contact page . For example, to access my Mastodon profile, instead of going to the actual URL, you can just visit lqdev.me/mastodon which redirects to the actual URL. If tomorrow I choose to change where and how my Mastodon presence is hosted, the URL doesn\u0027t have to change. However, I haven\u0027t done the same for my RSS links. Recently I\u0027ve been thinking about restructuring my website, specifically my microblog feed which includes notes and responses. Today, the RSS urls are coupled to the folder structure on my website which is subject to change and isn\u0027t flexible. By setting up more user-friendly and stable RSS urls through redirection, that wouldn\u0027t be an issue and readers wouldn\u0027t have to change the RSS URL they use.","contentType":"reply","url":"https://www.lqdev.me/responses/own-your-rss-links","date":"2024-01-07 15:48 -05:00","tags":["rss","indieweb"],"summary":"Own Your RSS Links \u2192 https://heydingus.net/blog/2024/1/please-own-your-rss-links ...owning the address where your audience finds you is important. It allows you to be mobile, nimble, and without...","keywords":["links","change","urls","which","today","going","more","friendly","mastodon","page"]},{"id":"notes-mind-body-soul-huberman","title":"Aligning mind, body, and soul","content":"In the past two weeks I\u0027ve been listening to a few episodes from the Huberman Lab podcast . In many of the episodes, the host, Dr. Andrew Huberman, brings on guests to talk about topics like nutrition, fertility, motivation, finding your purpose, and many others. It\u0027s interesting though that despite the large diversity of topics and guests, at their core, many of the conversations ended up coming back to mind, body, and soul. More specifically, nurturing those various aspects of our lives. This includes but is not limited to, fueling your body with the right foods, building community, being active, learning new things, exposing yourself to new experiences, etc.","contentType":"notes","url":"https://www.lqdev.me/notes/mind-body-soul-huberman","date":"2024-01-07 15:18 -05:00","tags":["podcast","lifestyle","wellness","health"],"summary":"In the past two weeks I\u0027ve been listening to a few episodes from the Huberman Lab podcast . In many of the episodes, the host, Dr. Andrew Huberman, brings on guests to talk about topics like...","keywords":["many","topics","body","episodes","huberman","guests","mind","soul","more","specifically"]},{"id":"reshare-favorite-photos-2023-adventuretaco","title":"AdventureTaco - Favorite Photos 2023 Edition","content":"AdventureTaco - Favorite Photos 2023 Edition \u2192 https://adventuretaco.com/favorite-photos-2023-edition/","contentType":"reshare","url":"https://www.lqdev.me/responses/favorite-photos-2023-adventuretaco","date":"2024-01-06 21:33 -05:00","tags":["landscapes","photography","overlanding","adventure","photos","desert","mountain"],"summary":"AdventureTaco - Favorite Photos 2023 Edition \u2192 https://adventuretaco.com/favorite-photos-2023-edition/","keywords":["photos","2023","adventuretaco","favorite","edition","https://adventuretaco","com/favorite","edition/"]},{"id":"notes-anytype-local-only","title":"Time to try Anytype","content":"Currently I\u0027m using org-mode in Emacs for note-taking, journaling, habit-tracking, task management, any many other things. I\u0027ve heard good things about Notion and Obsidian but haven\u0027t made the switch because I like the flexibility of Emacs. I learned about Anytype a few months ago and it looked like it might be a good replacement. I\u0027d been holding out because back then it didn\u0027t have local-only mode. In their latest release, they added local-only mode so maybe it\u0027s finally time to try it. First though, I have to figure out if it works on NixOS.","contentType":"notes","url":"https://www.lqdev.me/notes/anytype-local-only","date":"2024-01-06 11:45 -05:00","tags":["pkm","personalknowledgemanagement","anytype","knowledge"],"summary":"Currently I\u0027m using org-mode in Emacs for note-taking, journaling, habit-tracking, task management, any many other things. I\u0027ve heard good things about Notion and Obsidian but haven\u0027t made the switch...","keywords":["mode","emacs","good","things","local","currently","looked","replacement","holding","back"]},{"id":"notes-post-statistics-gh-actions-working-almost","title":"GitHub Actions workflow for post metrics almost works","content":"A few days ago, I posted about a script I wrote. The script computes computes post metrics on my website . That\u0027s what I used to author my (We)blogging Rewind 2023 posts. Because I want the script to run at periodic intervals, I automated the process using GitHub Actions. The script is scheduled to run on the first of every month and I\u0027m happy to report that it works! Kind of. The GitHub Action was successfully triggered at the defined interval. Unfortunately, the script failed due to a user error which I\u0027ve since fixed . Come February 1st, everything should work as expected though, so I\u0027m happy about that.","contentType":"notes","url":"https://www.lqdev.me/notes/post-statistics-gh-actions-working-almost","date":"2024-01-03 00:26 -05:00","tags":["github","cicd","blogging","automation","githubactions","fsharp","cron"],"summary":"A few days ago, I posted about a script I wrote. The script computes computes post metrics on my website . That\u0027s what I used to author my (We)blogging Rewind 2023 posts. Because I want the script to...","keywords":["script","happy","github","computes","works","kind","action","successfully","triggered","defined"]},{"id":"reshare-i-want-you-to-blog-mullenweg","title":"Birthday Gift - I want you to blog","content":"Birthday Gift - I want you to blog \u2192 https://ma.tt/2024/01/birthday-gift/ the gift I most want for my 40th is something everyone can do. I want you to blog. Publish a post. About anything! It can be long or short, a photo or a video, maybe a quote or a link to something you found interesting. Don\u2019t sweat it. Just blog. Share something you created, or amplify something you enjoyed. It doesn\u2019t take much. The act of publishing will be a gift for you and me. That\u2019s it! No wrapping paper or bows. Just blogs and blogs and blogs, each unique and beautiful in its own way.","contentType":"reshare","url":"https://www.lqdev.me/responses/i-want-you-to-blog-mullenweg","date":"2024-01-03 00:16 -05:00","tags":["blogging","wordpress","writing","community"],"summary":"Birthday Gift - I want you to blog \u2192 https://ma.tt/2024/01/birthday-gift/ the gift I most want for my 40th is something everyone can do. I want you to blog. Publish a post. About anything! It can be...","keywords":["gift","want","blog","blogs","birthday","sweat","share","created","amplify","enjoyed"]},{"id":"reshare-blogging-pump-up-volume","title":"The Year For Blogging To Pump Up The Volume","content":"The Year For Blogging To Pump Up The Volume \u2192 https://bix.blog/2024/01/01/the-year-for-blogging-to-pump-up-the-volume/ There\u2019s been a lot of pontificating lately that the web is ripe for a blogging renaissance, wishing for it to be true. Much of it from people who don\u2019t seem to notice that\u2019s it\u2019s already begun. Maybe they don\u2019t anymore know quite where to look. Maybe the sorts of blogging they\u2019re seeing isn\u2019t what they mean. (To the blognoscenti, do things like \u201Cwordvomits\u201D count?) If you haven\u2019t seen it, either, that\u2019s okay. All you have to do is choose to be a part of it. There\u2019s never been a better time: those who managed to monopolize our attentions and keep too many of us chattering for a few hundred characters at a time to the benefit of advertisers are losing their relevance. I\u2019m not one for making personal resolutions, but let me suggest one on behalf of the blogosphere: this is the year we pump up the volume.","contentType":"reshare","url":"https://www.lqdev.me/responses/blogging-pump-up-volume","date":"2024-01-02 18:36 -05:00","tags":["blogging","community","writing","movie","radio"],"summary":"The Year For Blogging To Pump Up The Volume \u2192 https://bix.blog/2024/01/01/the-year-for-blogging-to-pump-up-the-volume/ There\u2019s been a lot of pontificating lately that the web is ripe for a blogging...","keywords":["blogging","year","pump","they","maybe","that\u2019s","there\u2019s","don\u2019t","volume","making"]},{"id":"notes-orange-bowl-2023","title":"Orange Bowl 2023 - the playoff game that never was","content":"So basically this Orange Bowl game between Georgia and Florida State is a look at what the College Football Playoff could\u0027ve been, had both teams been part of the playoff. I know teams in general were affected by the transfer portal, but it\u0027s not even close. It\u0027s barely halftime but the game is basically over unless Florida State finds a way to score 6 touchdowns in the next 30 minutes.","contentType":"notes","url":"https://www.lqdev.me/notes/orange-bowl-2023","date":"2023-12-30 17:51 -05:00","tags":["football","cfb","orangebowl","fsu","uga","collegefootball"],"summary":"So basically this Orange Bowl game between Georgia and Florida State is a look at what the College Football Playoff could\u0027ve been, had both teams been part of the playoff. I know teams in general...","keywords":["basically","game","florida","state","playoff","teams","touchdowns","score","finds","unless"]},{"id":"notes-2024-year-of-gemini","title":"2024 - The year of Gemini?","content":"If 2023 was the year of AI and the Fediverse, is 2024 the year of Gemini? I\u0027ve been seeing a few posts about Gemini cross my feed reader which is more than I\u0027m used to seeing. These include: The Gopher Revival Is Upon Us Poor Man\u0027s Web Gemini is weird in a fun way. Since content is text-based, using terminal clients like Elpher and Amfora make it easy to browse pages (capsules) and blogs (gemlogs).","contentType":"notes","url":"https://www.lqdev.me/notes/2024-year-of-gemini","date":"2023-12-30 00:34 -05:00","tags":["gemini","protocols","smallweb","openweb","web","internet","gopher"],"summary":"If 2023 was the year of AI and the Fediverse, is 2024 the year of Gemini? I\u0027ve been seeing a few posts about Gemini cross my feed reader which is more than I\u0027m used to seeing. These include: The...","keywords":["gemini","year","seeing","2023","content","text","based","using","terminal","clients"]},{"id":"reply-blogs-rss-decentralized-social-media-lemmer-webber","title":"Blogs \u002B RSS - best decentralized social media we ever had","content":"Blogs \u002B RSS - best decentralized social media we ever had \u2192 https://octodon.social/@cwebber/111647596861000656 Bring back self-hosted blogs, reinstall a feed reader, make your feed icon prominent on your blog. Blogs \u002B Atom/RSS is the best decentralized social media system we\u0027ve ever had! And yes I am saying that as co-author of ActivityPub: self hosted blogs is the best decentralized social networking we\u0027ve had \uD83D\uDCAF \uD83D\uDCAF \uD83D\uDCAF \uD83D\uDCAF \uD83D\uDCAF \uD83D\uDCAF","contentType":"reply","url":"https://www.lqdev.me/responses/blogs-rss-decentralized-social-media-lemmer-webber","date":"2023-12-30 00:30 -05:00","tags":["rss","blog","decentralized","socialmedia","blogging","mastodon","fediverse","indieweb","atom","feed"],"summary":"Blogs \u002B RSS - best decentralized social media we ever had \u2192 https://octodon.social/@cwebber/111647596861000656 Bring back self-hosted blogs, reinstall a feed reader, make your feed icon prominent on...","keywords":["blogs","best","decentralized","social","feed","self","hosted","ever","media","social/@cwebber/111647596861000656"]},{"id":"reply-what-farm-animals-think-orwell","title":"What are farm animals thinking?","content":"What are farm animals thinking? \u2192 https://www.science.org/content/article/not-dumb-creatures-livestock-surprise-scientists-their-complex-emotional-minds I think I\u0027ve read about this somewhere before... \u201CAll animals are equal, but some animals are more equal than others.\u201D \u2015 George Orwell, Animal Farm \u201CFour legs good, two legs bad.\u201D \u2015 George Orwell, Animal Farm","contentType":"reply","url":"https://www.lqdev.me/responses/what-farm-animals-think-orwell","date":"2023-12-27 14:57 -05:00","tags":["science","animals","farm","orwell","animallfarm","meme"],"summary":"What are farm animals thinking? \u2192 https://www.science.org/content/article/not-dumb-creatures-livestock-surprise-scientists-their-complex-emotional-minds I think I\u0027ve read about this somewhere...","keywords":["farm","animals","animal","orwell","george","equal","legs","\u201Cfour","others","more"]},{"id":"notes-app-defaults-2023","title":"App Defaults 2023","content":"Late to the App Defaults train but here\u0027s what I use as of December 2023. For the latest list and Every Day Carry (EDC) list, see the Uses page . Operating System : NixOS (Desktop) / Android (Mobile) E-Mail : Thunderbird (Desktop) / Outlook (Mobile) Feed Reader : NewsBlur Podcasts : AntennaPod Music : Spotify Notes : Emacs (Org-Mode) Tasks : Emacs (Org-Mode) / Microsoft To-Do Office : LibreOffice Browser : Firefox Messaging : Element / Signal (Molly) Maps : OSMAnd~ Text Editor : Visual Studio Code E-Book Management : Calibre Reading : Libby Video : mpv / VLC Authenticator : Aegis","contentType":"notes","url":"https://www.lqdev.me/notes/app-defaults-2023","date":"2023-12-25 22:28 -05:00","tags":["apps","appdefaults","edc"],"summary":"Late to the App Defaults train but here\u0027s what I use as of December 2023. For the latest list and Every Day Carry (EDC) list, see the Uses page . Operating System : NixOS (Desktop) / Android (Mobile)...","keywords":["desktop","mobile","mode","list","emacs","management","tasks","microsoft","office","libreoffice"]},{"id":"bookmarks-apple-ml-ferret","title":"Ferret: Refer and Ground Anything Anywhere at Any Granularity","content":"Ferret: Refer and Ground Anything Anywhere at Any Granularity \u2192 https://arxiv.org/abs/2310.07704 We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination. Code and data will be available at this https URL Code","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/apple-ml-ferret","date":"2023-12-25 20:12 -05:00","tags":["ai","ml","llm","mllm","largelanguagemodel","multimodal","multimodallargelanguagemodel","apple","opensource"],"summary":"Ferret: Refer and Ground Anything Anywhere at Any Granularity \u2192 https://arxiv.org/abs/2310.07704 We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial...","keywords":["region","ferret","spatial","grounding","image","referring","model","features","data","also"]},{"id":"reply-2023-goals-accomplishments-purple-life","title":"A Purple Life - 2023 Goals and Accomplishments","content":"A Purple Life - 2023 Goals and Accomplishments \u2192 https://apurplelife.com/2023/12/19/2023-goals-accomplishments/ My posts have been...including more reviews of fancy travel hacked flights, tours and slow travel locations. Possibly as a result of this shift in topic \u2013 or possibly simply because blogging seems to be on its way out according to a few of my blogging peers \u2013 my comments section has been quieter lately. I talked about this in one of my monthly recaps with the spin that I didn\u2019t realize I had come to rely on getting at least one comment per post to know that people (and not just bots \uD83D\uDE42 ) were reading my words and they weren\u2019t floating into an abyss. I didn\u2019t want to be reliant on external validation when I had written this blog without it being public for years, and hadn\u2019t realized I had come to rely on anything but the joy I get from writing it. So I was trying to grow after realizing that not receiving comments on multiple posts in a row bothered me for some reason. I\u2019m going to do my best to not rely on that kind of feedback going forward and will continue to blog for the main reason I always have: for myself \uD83D\uDE42 . I also followed a reader suggestion to add a \u201CLike\u201D button at the bottom of my posts (it\u2019s after the \u201CShare This\u201D section and before the \u201CRelated\u201D articles section) because readers said they don\u2019t necessarily have something they want to comment, but that a Like button would help show there is still a human reading. All fair \uD83D\uDE42 . \uD83D\uDE4B\u200D\u2642\uFE0F there\u0027s still a human reading your posts on this end \uD83D\uDE42 Also, today I learned there\u0027s a Guineafowl Pufferfish.","contentType":"reply","url":"https://www.lqdev.me/responses/2023-goals-accomplishments-purple-life","date":"2023-12-24 18:05 -05:00","tags":["blogging","review","goals"],"summary":"A Purple Life - 2023 Goals and Accomplishments \u2192 https://apurplelife.com/2023/12/19/2023-goals-accomplishments/ My posts have been...including more reviews of fancy travel hacked flights, tours and...","keywords":["posts","section","rely","they","reading","possibly","blogging","going","reason","blog"]},{"id":"posts-rss-to-mastodon-posse-azure-logic-apps","title":"POSSE to Mastodon using RSS and Azure Logic Apps","content":"## Introduction [Over the past year, I\u0027ve made this website the main place where I post content](/notes/weblogging-rewind-2023/). The easiest way to [subscribe to content on my website](/feed) is through the various RSS feeds. However, I have accounts on other platforms like X (formerly Twitter), Bluesky, and Mastodon where I\u0027d still like to repost my content to. Since the [changes to the Twitter API](https://techcrunch.com/2023/03/29/twitter-announces-new-api-with-only-free-basic-and-enterprise-levels/), the only place I cross-post to is Mastodon. The main reason behind it is, as of now, it\u0027s one of the few platforms that allows me to automate post creation via its REST APIs without restrictions. A large part of that is I self-host my own Mastodon instance but I assume there aren\u0027t as many restrictions when using other instances like mastodon.social. The way I automate posting is by setting up workflows using Azure Logic Apps. These workflows subscribe to my various RSS feeds and whenever a new post is published, they make an HTTP request to my Mastodon instance to create a new post. I\u0027ve been doing this for some time but never got around to documenting it. This blog post goes into more details about how to set up these workflows. ## What is POSSE [POSSE](https://indieweb.org/POSSE) is short for \u0022Post on your Own Site Syndicate Elsewhere\u0022. It\u0027s one of the patterns adopted within IndiWeb communities and projects. The main idea is, your website or a website you own or have administrative rights to becomes the main platform where you publish your content. Effectively, it\u0027s the cannonical version of your content. Once your content is on your website, you can optionally choose to distribute it on other platforms. ## What is RSS [RSS](https://en.wikipedia.org/wiki/RSS) is short for Really Simple Syndication. Accorting to Wikipedia, this protocol \u0022allows users and applications to access updates to websites in a standardized, computer-readable format\u0022. ## What is Mastodon [Mastodon](https://joinmastodon.org/) is an [open-source](https://github.com/mastodon/mastodon) decentralized microblogging platform built on the [ActivityPub](https://activitypub.rocks/) protocol and part of the larger collective of federated systems known as the [Fediverse](https://fediverse.info/). ## What are Azure Logic Apps If you\u0027ve used workflow automation systems like [IFTTT](https://ifttt.com/), then you generally know what Azure Logic Apps are. A more formal definition from the Azure documentation - \u0022Azure Logic Apps is a cloud platform where you can create and run automated workflows with little to no code. By using the visual designer and selecting from prebuilt operations, you can quickly build a workflow that integrates and manages your apps, data, services, and systems.\u0022 For more details, see the [Azure Logic Apps documentation](https://learn.microsoft.com/azure/logic-apps/logic-apps-overview). ## Prerequisites Since you\u0027ll be creating Logic App Resources on Azure, you\u0027ll need an [Azure account](https://aka.ms/free). ## Create Consumption Azure Logic App Resource There\u0027s various ways to create an Azure Logic App Resource but the easiest one for this relatively simple workflow is using the Azure Portal. For more details, see the [create a consumption logic app resource documentation](https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#create-a-consumption-logic-app-resource). For the most part you can leave the defaults as is. Since the intended use for this workflow is personal and I don\u0027t need enterprise features, I chose to create my logic app using the consumption plan. When prompted to choose **Plan Type**, select **Consumption**. ## The Workflow The workflow is relatively simple. Whenever a new item is posted to an RSS feed, make an HTTP POST request to the Mastodon API \u0060\u0060\u0060mermaid flowchart TD A[\u0022RSS\u0022] --\u003E B[\u0022HTTP (Mastodon API)\u0022] \u0060\u0060\u0060 Once your Logic App resource deploys to Azure, create a new logic app using the Blank Template. For more details, see the [select a blank template documentation](https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#select-the-blank-template). This will launch you into the Logic app designer UI where you can begin to configure your workflow. ### RSS trigger The first thing you\u0027ll want to do is set up the trigger that initiates the workflow. The trigger in this case will be new posts on an RSS feed. In the Logic app designer, search for *RSS* and add it as a trigger. Once the RSS trigger is added to your workflow, configure it as follows: - **The RSS feed URL**: The feed you want want to get posts from. In this case, I\u0027m using my blog posts [feed](/posts/feed.xml). **TIP: The RSS feed can be ANY feed. So if you wanted to subscribe and repost items from your favorite blog or website, you can as well. Just note though that if you don\u0027t own the content it might not be something the author wants you doing and for websites that update multiple times a day, it might produce a lot of noise in your feed. Other than that though, subscribe and post away!** - **How often do you want to check for items?**: The frequency at which you want to poll the RSS feed and check for new posts. In my case, I don\u0027t post very often but I do want my posts to be published on Mastodon within an hour of publishing. Therefore, I chose 1 hour as the update frequency. ### HTTP action Now that your trigger is configured, it\u0027s time to do someting with the latest posts. In this case, since I want to create a new post on Mastodon, I can do so via their REST API. #### Get Mastodon credentials One thing that you\u0027ll need to publish posts to Mastodon is an application token. The easiest way to get one is using the Web UI. To get your app credentials through the web UI: 1. In the Mastodon Web UI, select **Preferences**. 1. In the preferences page, select **Development**. 1. In the development preferences tab, select **New application**. 1. In the new application page: - Provide a name for your application - Choose the **write:statuses** scope checkbox. For details on required scopes to post statuses, see the [Post a new status REST API documentation](https://docs.joinmastodon.org/methods/statuses/#create) - Select **Submit** If successful, this will create credentials and a token you can use to send authenticated requests to the Mastodon API. #### Configure HTTP action Now that you have your credentials, you can configure your HTTP action in the Logic Apps workflow. In the Logic App designer: 1. Select **\u002B New Step**. 1. Search for *HTTP* and add it as an action. 1. Once the HTTP action is added, configure it as follows: - **Method**: The HTTP Method. To create statuses, choose **POST** from the dropdown. - **URI**: The endpoint to make the requests to. For mastodon, it\u0027s \u0060https:// /api/v1/statuses\u0060. Make sure to replace \u0060 \u0060 with your Mastodon instance. In my case, it\u0027s \u0060toot.lqdev.tech\u0060 since that\u0027s where I host my mastodon instance. - **Headers**: HTTP Headers to use. In this case, set the following: | Key | Value | | --- | --- | | Content-Type | application/x-www-form-urlencoded | - **Queries**: URL Query Parameters to add to the request. This is where you\u0027ll set your access token. | Key | Value | | --- | --- | | access_token | \u0060[YOUR-ACCESS-TOKEN]\u0060 | Make sure to replace \u0060[YOUR-ACCESS-TOKEN]\u0060 with the token credential generated in the Mastodon Web UI. - **Body**: The content to be published in your Mastodon post. This is entirely up to you though at minumum, you\u0027ll add the following to the text feed. \u0060status= \u0060. \u0060 \u0060 is what will be displayed in the Mastodon post. One of the nice things about Azure Logic Apps is, properties from previous steps are available to you in subsequent steps. Since our trigger is an RSS feed, we can get access to the feed and item properties of our feed in the HTTP action. If all you wanted was to post the tile and URL, you can do that using the **Feed title** and **Primary Feed Link** properties. For more details, see the [RSS connector documentation](https://learn.microsoft.com/connectors/rss/). ## Save and run That\u0027s it! Now you just need to select **Save** in the Logic app designer page. Once it\u0027s saved, click **Run trigger** which will kick off your trigger. If you have anything recent to publish and everything is configured correctly, it should show up in your Mastodon feed. ## Conclusion By publishing content on your own website first, you\u0027re in full control of your content. Regardless of which platforms come and go, you won\u0027t have to adjust to those changes because your content is not locked in to those platforms. However, that doesn\u0027t mean you can\u0027t also publish your content there. Using protocols like RSS make it easy to subscribe to updates on your website. Using REST APIs provided by the respective platforms, you can automate publishing these updates. To further automate and simplify this process, you can use services like Azure Logic Apps to make publishing to all places easy.","contentType":"posts","url":"https://www.lqdev.me/posts/rss-to-mastodon-posse-azure-logic-apps","date":"2023-12-24 16:40 -05:00","tags":["azure","mastodon","indieweb","logicapps","fediverse","posse","internet","web","blogging","blog","automation","programming","rss"],"summary":"## Introduction [Over the past year, I\u0027ve made this website the main place where I post content](/notes/weblogging-rewind-2023/). The easiest way to [subscribe to content on my website](/feed) is...","keywords":["mastodon","logic","post","feed","azure","content","create","using","workflow","http"]},{"id":"notes-one-more-post-2023","title":"One more blog post for 2023?","content":"Update from yesterday . 2023 is still not over. I think I can squeeze out at least one more post for the year \uD83D\uDE42. Blog post counts Year Count 2023 7 2022 7 Note counts Year Count 2023 37 2022 36","contentType":"notes","url":"https://www.lqdev.me/notes/one-more-post-2023","date":"2023-12-24 14:31 -05:00","tags":["weblogging","quantifiedself","indieweb","stats","analytics","blog","blogging","internet","opensource","community","selfhost","fsharp","spotify","antennapod","microblog","analytics","pkm","personalknowledgemanagement"],"summary":"Update from yesterday . 2023 is still not over. I think I can squeeze out at least one more post for the year \uD83D\uDE42. Blog post counts Year Count 2023 7 2022 7 Note counts Year Count 2023 37 2022 36","keywords":["2023","year","post","counts","count","2022","update","from","yesterday","squeeze"]},{"id":"posts-website-metrics-github-actions","title":"Computing website metrics using GitHub Actions","content":"## Introduction I recently posted my [(We)blogging Rewind](/notes/weblogging-rewind-2023/) and [(We)blogging Rewind Continued](/notes/weblogging-rewind-2023-continued) for 2023. In these posts, I discuss some analysis I did of my posting behaviors over the past year on this website. I don\u0027t use platforms like Google Analytics to track visitors. I used to self-host a [GoatCounter](https://www.goatcounter.com/) instance but personally I don\u0027t really care about traffic so I got rid of that. There are some standard reports I get from my [CDN provider on Azure](https://learn.microsoft.com/azure/cdn/cdn-advanced-http-reports), but again I don\u0027t really care about those metrics. What I do care about though is my output and understanding what I\u0027m publishing and the topics that were important to me at any given time. In the case of those blog posts, it was for the year 2023. Given that I had already done the analysis and had written the script, I thought, why not automate it and run it on a more regular basis to have monthly summaries. Since my blog and scripts are already on GitHub, it makes sense to create a GitHub Action workflow. In this post, I discuss in more details what my post analytics script does and how I configured my workflow in GitHub Actions to run the script on the first of every month. ## The script This script loads the various posts on my website and computes aggregate metrics based on post types and their metadata. You can find the full script at [stats.fsx](/resources/snippets/lqdev-me-website-post-metrics). ### Loading files The following are convenience functions which I use as part of my website build process. In general these functions: - Load the individual post files - Parse the content and YAML metadata \u0060\u0060\u0060fsharp let posts = loadPosts() let notes = loadFeed () let responses = loadReponses () \u0060\u0060\u0060 Instead of building new custom functions, I can repurpose them and apply additional transformations to compute aggregate statistics. ### Computing aggregate statistics Once the posts are loaded, I apply transformations on the collections to compute aggregate metrics: #### Annual post counts The following are annual aggreagates of blog posts, notes, and responses. ##### Blog posts Takes the blog post collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order. \u0060\u0060\u0060fsharp let postCountsByYear = posts |\u003E Array.countBy (fun (x:Post) -\u003E DateTime.Parse(x.Metadata.Date) |\u003E _.Year) |\u003E Array.sortByDescending fst \u0060\u0060\u0060 ##### Notes Takes the note collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order. \u0060\u0060\u0060fsharp let noteCountsByYear = notes |\u003E Array.countBy (fun (x:Post) -\u003E DateTime.Parse(x.Metadata.Date) |\u003E _.Year) |\u003E Array.sortByDescending fst \u0060\u0060\u0060 ##### Responses Takes the response collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order. \u0060\u0060\u0060fsharp let responseCountsByYear = responses |\u003E Array.countBy (fun (x:Response) -\u003E DateTime.Parse(x.Metadata.DatePublished) |\u003E _.Year) |\u003E Array.sortByDescending fst \u0060\u0060\u0060 #### Response counts by type Takes the response collection, parses the published date, filters it for the current year, and computes counts by the post type (reply, bookmark, reshare, star). \u0060\u0060\u0060fsharp let responsesByType = responses |\u003E Array.filter(fun x -\u003E (DateTime.Parse(x.Metadata.DatePublished) |\u003E _.Year) = DateTime.UtcNow.Year) |\u003E Array.countBy(fun x -\u003E x.Metadata.ResponseType) |\u003E Array.sortByDescending(snd) \u0060\u0060\u0060 #### Tag counts (responses) Takes the response collection, parses the published date, filters it for the current year, and computes counts by the tag name, and sorts in descending order using the count. \u0060\u0060\u0060fsharp let responsesByTag = responses |\u003E Array.filter(fun x -\u003E (DateTime.Parse(x.Metadata.DatePublished) |\u003E _.Year) = DateTime.UtcNow.Year) |\u003E Array.collect(fun x -\u003E match x.Metadata.Tags with | null -\u003E [|\u0022untagged\u0022|] | [||] -\u003E [|\u0022untagged\u0022|] | _ -\u003E x.Metadata.Tags ) |\u003E Array.countBy(fun x -\u003E x) |\u003E Array.sortByDescending(snd) \u0060\u0060\u0060 #### Domain counts (responses) Takes the response collection, parses the published date, filters it for the current year, and computes counts by the target URL host name, and sorts it in descending order using the count. \u0060\u0060\u0060fsharp let responsesByDomain = responses |\u003E Array.filter(fun x -\u003E (DateTime.Parse(x.Metadata.DatePublished) |\u003E _.Year) = DateTime.UtcNow.Year) |\u003E Array.countBy(fun x -\u003E Uri(x.Metadata.TargetUrl).Host) |\u003E Array.sortByDescending(snd) \u0060\u0060\u0060 ### Displaying counts Since the \u0060countBy\u0060 function is the one used to compute the counts, this produces a tuple. The tuple though could be \u0060string\u0060 or \u0060int\u0060. Therefore, I set the collection of entry counts to use a generic \u0060\u0027a\u0060 for the first item in the tuple. I\u0027m also able to control using \u0060n\u0060 whether to display the entire collection by using \u0060-1\u0060 as input or a limit when \u0060n \u003E= 0\u0060. \u0060\u0060\u0060fsharp let printEntryCounts (title:string) (entryCounts:(\u0027a * int) array) (n:int) = printfn $\u0022{title}\u0022 match n with | n when n = -1 -\u003E entryCounts |\u003E Array.iter(fun x -\u003E printfn $\u0022{fst x} {snd x}\u0022) |\u003E fun _ -\u003E printfn $\u0022\u0022 | n when n \u003E= 0 -\u003E entryCounts |\u003E Array.take n |\u003E Array.iter(fun x -\u003E printfn $\u0022{fst x} {snd x}\u0022) |\u003E fun _ -\u003E printfn $\u0022\u0022 \u0060\u0060\u0060 The result of running this script produces the following results: \u0060\u0060\u0060text Blogs 2023 5 2022 7 Notes 2023 34 2022 36 Responses 2023 216 2022 146 Response Types bookmark 151 reshare 48 reply 10 star 7 Response Tags ai 104 llm 42 untagged 41 opensource 31 internet 17 Domains github.com 15 huggingface.co 11 arxiv.org 10 openai.com 6 www.theverge.com 4 \u0060\u0060\u0060 ## The workflow file The workflow file is a GitHub Actions workflow which you can find in my [website repo](https://github.com/lqdev/luisquintanilla.me/blob/main/.github/workflows/stats.yml). ### Triggers I don\u0027t really want the script to run every time I publish my website. Instead, I just want to have these aggregate values computed on a monthly basis. Optionally though, I\u0027d like to be able to run ad-hoc reports and trigger this job manually. The triggers in my workflow file look like the following: \u0060\u0060\u0060yaml schedule: - cron: \u002730 0 1 * *\u0027 workflow_dispatch: \u0060\u0060\u0060 Using cron job syntax, I use the \u0060schedule\u0060 trigger to configure the script to run at 12:30 AM on the 1st day of every month. The \u0060workflow_dispatch\u0060 trigger is there so I can manually trigger this job. ## Steps The steps in the workflow file are the following: - Check out the repo \u0060\u0060\u0060yaml - uses: actions/checkout@v2 \u0060\u0060\u0060 - Install the .NET 8 SDK \u0060\u0060\u0060yaml - name: Setup .NET SDK 8.x uses: actions/setup-dotnet@v1.9.0 with: dotnet-version: \u00278.0.x\u0027 \u0060\u0060\u0060 - Restore dependencies \u0060\u0060\u0060yaml - name: Install dependencies run: dotnet restore \u0060\u0060\u0060 - Build the project \u0060\u0060\u0060yaml - name: Build project run: dotnet build --no-restore \u0060\u0060\u0060 - Run the script and display metrics \u0060\u0060\u0060yaml - name: Display Post Metrics run: dotnet fsi Scripts/stats.fsx \u0060\u0060\u0060 ## Conclusion If you have scripts that you run on a repo on a fairly regular basis, consider using GitHub Actions to automate the execution of these scripts. Happy coding!","contentType":"posts","url":"https://www.lqdev.me/posts/website-metrics-github-actions","date":"2023-12-24 13:24 -05:00","tags":["github","cicd","website","blog","blogging","web","internet","fsharp","dotnet"],"summary":"## Introduction I recently posted my [(We)blogging Rewind](/notes/weblogging-rewind-2023/) and [(We)blogging Rewind Continued](/notes/weblogging-rewind-2023-continued) for 2023. In these posts, I...","keywords":["array","year","counts","metadata","script","responses","post","datetime","workflow","\u0060\u0060\u0060fsharp"]},{"id":"notes-weblogging-rewind-2023-continued","title":"(We)blogging Rewind 2023 Continued","content":"Yesterday I posted aggregate statistics about this website in the post (We)blogging Rewind 2023 . I thought it\u0027d be cool to see which domains I referenced most in my responses. Here\u0027s the breakdown of the top 5. Domain Count github.com 15 huggingface.co 11 arxiv.org 10 openai.com 6 www.theverge.com 4 No surprise here given that AI was one of the top tags in my responses. Updated script is here stats.fsx .","contentType":"notes","url":"https://www.lqdev.me/notes/weblogging-rewind-2023-continued","date":"2023-12-24 09:39 -05:00","tags":["weblogging","quantifiedself","indieweb","stats","analytics","blog","blogging","internet","opensource","community","selfhost","fsharp","spotify","antennapod","microblog","analytics","pkm","personalknowledgemanagement"],"summary":"Yesterday I posted aggregate statistics about this website in the post (We)blogging Rewind 2023 . I thought it\u0027d be cool to see which domains I referenced most in my responses. Here\u0027s the breakdown...","keywords":["here","responses","updated","tags","given","surprise","theverge","openai","arxiv","huggingface"]},{"id":"notes-srs-distribution-las-vegas-bowl-2023","title":"Utah vs Northwestern SRS Distribution Las Vegas Bowl Thread","content":"21:40 This Utah defense are the real MVPs. Utah hasn\u0027t been able to put together a drive all game. So far I think I\u0027ve seen 3-4 turnovers. The fact that Northwestern is ONLY up by 7 is shocking. 21:47 Make it make sense. Something must\u0027ve happened. Maybe injuries or something. This season they: Beat Florida Beat USC Had a close game against Washington which is now in the playoffs Yet they can\u0027t put together a drive. Something doesn\u0027t add up. 21:54 Okay. I think I got it. Utah like many of the Pac-12 teams is a passing team. Northwestern is in the top 25 passing defenses . So I guess this is the toughest defense they\u0027ve faced? 22:01 7-7! 22:16 Armchair quarterbacking, but at 44% 4th down conversions, I don\u0027t think you go for it on 4th and 2 in the middle of the field. No matter how well your defense is playing and going against a backup quarterback. 22:25 And there it is. Starting QB came back into the game and had a short field to work with against a tired defense. Offense is going to have to dig deep for Utah. 22:33 That\u0027s the game. Bad execution on 4th down again.","contentType":"notes","url":"https://www.lqdev.me/notes/srs-distribution-las-vegas-bowl-2023","date":"2023-12-23 21:39 -05:00","tags":["football","utah","northwestern","cfb","collegefootball"],"summary":"21:40 This Utah defense are the real MVPs. Utah hasn\u0027t been able to put together a drive all game. So far I think I\u0027ve seen 3-4 turnovers. The fact that Northwestern is ONLY up by 7 is shocking....","keywords":["utah","defense","game","against","going","passing","down","northwestern","they","beat"]},{"id":"reshare-bluesky-rss-support","title":"Bluesky now supports RSS","content":"Bluesky now supports RSS \u2192 https://bsky.app/profile/bsky.app/post/3kh5rjl6bgu2i I\u0027m not as active on there, but feel free to subscribe to my Bluesky feed wherever you subscribe to feeds. Also, if you have any feed recommendations, let me know . \uD83D\uDCE2 1.60 is rolling out now (3/5) RSS feeds for profiles! Access your posts via RSS by pasting your profile link into your RSS feed reader and it will automatically be discovered.","contentType":"reshare","url":"https://www.lqdev.me/responses/bluesky-rss-support","date":"2023-12-23 20:59 -05:00","tags":["bluesky","rss","socialmedia"],"summary":"Bluesky now supports RSS \u2192 https://bsky.app/profile/bsky.app/post/3kh5rjl6bgu2i I\u0027m not as active on there, but feel free to subscribe to my Bluesky feed wherever you subscribe to feeds. Also, if you...","keywords":["feed","bluesky","subscribe","feeds","app/post/3kh5rjl6bgu2i","reader","link","profile","pasting","posts"]},{"id":"reply-mcgregor-ronaldo-zune-windows-phone-meme","title":"Conor McGregor pitching Zune and Windows Phone to Cristiano Ronaldo","content":"Conor McGregor pitching Zune and Windows Phone to Cristiano Ronaldo \u2192 https://twitter.com/HappyPunch/status/1738661281236672833 What was probably being said according to Copilot \uD83D\uDE02 Cristiano, you need to listen to me, mate. You\u0027re wasting your time and money on those Apple products. They\u0027re overpriced, overrated, and outdated. They\u0027re nothing compared to the real champions of technology: Windows Phone and Zune. Trust me, I know what I\u0027m talking about. I\u0027ve used them both, and they\u0027re the best phone and MP3 player ever made. Let me tell you why. First of all, they had a sleek design, a smooth interface, and a unique tile system that was way ahead of its time. They were also compatible with Microsoft\u0027s ecosystem, which gave you access to Office, Xbox, and Bing. And don\u0027t get me started on the Zune Pass, which let you stream and download unlimited music for a low monthly fee. You can\u0027t beat that, mate. Second of all, they had no apps, no games, no support, and no market share. Wait, what? No, no, no, that\u0027s not a bad thing. That\u0027s a good thing. That means they were exclusive, elite, and original. They were not for the masses, they were for the class. They were not for the sheep, they were for the lions. They were not for the followers, they were for the leaders. They were not for the weak, they were for the strong. They were not for you, they were for me. And they can be for you too, if you have the courage, the vision, and the taste to switch to Windows Phone and Zune. I\u0027m telling you, Cristiano, you\u0027re missing out on the best phone and MP3 player ever made. And I\u0027m willing to prove it to you. I\u0027ll bet you a million euros that if you try them for a week, you\u0027ll never go back to your Apple products. What do you say? Are you man enough to take the challenge? Or are you too scared to face the truth? Come on, Cristiano, don\u0027t be a coward. Don\u0027t be a fool. Don\u0027t be a loser. Be a winner. Be a legend. Be a Windows Phone and Zune user.","contentType":"reply","url":"https://www.lqdev.me/responses/mcgregor-ronaldo-zune-windows-phone-meme","date":"2023-12-23 20:23 -05:00","tags":["windowsphone","zune","ai","copilot","meme"],"summary":"Conor McGregor pitching Zune and Windows Phone to Cristiano Ronaldo \u2192 https://twitter.com/HappyPunch/status/1738661281236672833 What was probably being said according to Copilot \uD83D\uDE02 Cristiano, you...","keywords":["they","phone","zune","windows","cristiano","mate","ever","made","products","apple"]},{"id":"notes-genai-spotify-clips-instrumental","title":"What about instrumentals? AI Generated Spotify Clips Addendum","content":"In my recent post, Using Generative AI to produce Spotify Clips I listed a bunch of challenges but realized I missed a big one. While listening to IJ by Sam Gendel , I realized, what do you do about instrumental songs, solos, or songs with few lyrics? Some sort of audio-to-video technique? Maybe using metadata like title, genre, beats-per-minute (BPM), spectrogram, and musical notes would work? Or generating an image and animating it or its background to add depth similar to how in the VideoPoet blog post , the image of the Mona Lisa is animated. It would be a fun experiment.","contentType":"notes","url":"https://www.lqdev.me/notes/genai-spotify-clips-instrumental","date":"2023-12-23 19:32 -05:00","tags":["ai","genai","music","instrumental","spotify","generativeai","llm"],"summary":"In my recent post, Using Generative AI to produce Spotify Clips I listed a bunch of challenges but realized I missed a big one. While listening to IJ by Sam Gendel , I realized, what do you do about...","keywords":["post","using","songs","realized","image","recent","genre","beats","minute","spectrogram"]},{"id":"bookmarks-videopoet-llm-zero-shot-video-generation","title":"VideoPoet: A large language model for zero-shot video generation","content":"VideoPoet: A large language model for zero-shot video generation \u2192 https://sites.research.google/videopoet/ VideoPoet is a simple modeling method that can convert any autoregressive language model or large language model (LLM) into a high-quality video generator. It contains a few simple components: A pre-trained MAGVIT V2 video tokenizer and a SoundStream audio tokenizer transform images, video, and audio clips with variable lengths into a sequence of discrete codes in a unified vocabulary. These codes are compatible with text-based language models, facilitating an integration with other modalities, such as text. An autoregressive language model learns across video, image, audio, and text modalities to autoregressively predict the next video or audio token in the sequence. A mixture of multimodal generative learning objectives are introduced into the LLM training framework, including text-to-video, text-to-image, image-to-video, video frame continuation, video inpainting and outpainting, video stylization, and video-to-audio. Furthermore, such tasks can be composed together for additional zero-shot capabilities (e.g., text-to-audio). This simple recipe shows that language models can synthesize and edit videos with a high degree of temporal consistency. VideoPoet demonstrates state-of-the-art video generation, in particular in producing a wide range of large, interesting, and high-fidelity motions. The VideoPoet model supports generating videos in square orientation, or portrait to tailor generations towards short-form content, as well as supporting audio generation from a video input. Blog Post","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/videopoet-llm-zero-shot-video-generation","date":"2023-12-23 19:06 -05:00","tags":["ai","genai","videopoet","generativeai","google"],"summary":"VideoPoet: A large language model for zero-shot video generation \u2192 https://sites.research.google/videopoet/ VideoPoet is a simple modeling method that can convert any autoregressive language model or...","keywords":["video","audio","text","language","model","high","simple","image","videopoet","generation"]},{"id":"posts-generative-ai-spotify-clips","title":"Using Generative AI to produce Spotify Clips","content":"## Introduction While listening to music on Spotify last night, this thought hit me. It\u0027s been cooking for some time so now you get to read about it. Spotify and YouTube, if you build this into your platforms for songs or podcasts, [contact me](/contact) for info on where to send the check :joy:. As the title of the post suggests, this is a thought experiment on how you might use Generative AI to produce Spotify Clips. This is not limited to songs but could also be extended to audio-only podcasts. However, given the long-form nature of podcasts it might be extremely resource intensive to produce clips per episode. This post is not exactly a how-to guide, though there are some practical examples of how some of the steps in the workflow might work using [Microsoft Copilot (formerly Bing Chat)](https://copilot.microsoft.com/). ## Spotify Clips Let\u0027s start off by talking about what Spotify Clips are. Back in March of 2023, [Spotify introduced Clips to their platform](https://artists.spotify.com/blog/spotify-clips-get-started-short-form-video-stream-on). Spotify describes [Clips](https://clips.byspotify.com/) as \u0022short, under-30-second vertical videos that are uploaded directly to Spotify for Artists. They\u2019re designed to put you and your music front-and-center, so they can be attached to your artist profile, to a song, to an album, or to an upcoming release.\u0022. When paired with a song, you can think of them as short music videos. Music videos being the videos made to accompany songs that played on TV channels like Music Television (MTV) before Jersey Shore and Catfish. Music videos are still in production today and they generally serve the same purpose. They provide a visual representation of the story being told by the song. ## Generative AI for video An overwhelming majority of Generative AI models only generate text. With models like DALL-E and Stable Diffusion, you can also generate images. Other models to generate audio and video have started coming on the scene. Most recently, Google announced [VideoPoet](https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html), a large language model for zero-shot video generation. There\u0027s many interesting techniques that VideoPoet introduces but I\u0027ll keep it to one of the more simple scenarios and the technique used in this post, text-to-image. Given a text prompt, VideoPoet can generate by default, a short 2-second clip. You can use VideoPoet as well to extend a clip so you could go well beyond the default 2 seconds. That however is outside the scope of this post. With that in mind, let\u0027s talk about how we might use a Generative AI model and apply it to the context of Spotify Clips. ## Generative AI Spotify Clips workflow The following is a rough idea of how you might use song information like lyrics to \u0060\u0060\u0060mermaid flowchart TD A[\u0022Get song lyrics\u0022] --\u003E B[\u0022Generate scenes and script\u0022] B --\u003E C[\u0022Generate prompts for each scene\u0022] C --\u003E D[\u0022Generate video clips\u0022] D --\u003E E[\u0022(Optional) Edit clips\u0022] D --\u003E F[\u0022Stitch clips together\u0022] E--\u003EF \u0060\u0060\u0060 This is roughly what Google did in their VideoPoet samples. They \u0022...produced a short movie composed of many short clips generated by the model. For the script, we asked Bard to write a series of prompts to detail a short story about a traveling raccoon. We then generated video clips for each prompt, and stitched together all resulting clips\u0022. The resulting video looks like the following: [![An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities](http://img.youtube.com/vi/70wZKfx6Ylk/0.jpg)](https://www.youtube.com/watch?v=70wZKfx6Ylk \u0022An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities\u0022) One thing to note is that the videos generated are YouTube Shorts which are vertical as well. This makes it easier to produce videos in the format Spotify expects Clips to be in. ## AI Generated Spotify Clips in practice Now that we know the general process, what does it actually look like in practice? To help me with that, I used Microsoft Copilot. ### Generate scenes and script In the example shared by Google, they used a prompt to generate a script made up of a series of prompts. In the case of songs on Spotify, we can use that as context and the starting point for our script. From there, we can ask Copilot to compose scenes for the verses in the song. We can also make sure as these scenes are generated, that there is a smooth transition between them and the storytelling is consistent. Let\u0027s take a look at what this might look like using the Chicano Batman song \u0022A Hundred Dead and Loving Souls\u0022. Why that one? Just check out my [Spotify Wrapped](/notes/spotify-wrapped-2023). The general theme of that song is about the cycles of life experienced across millenia and generations. **Prompt** Given the following prompt: \u0060\u0060\u0060text Using the song information below as context, generate a script for a story that visually represents the song lyrics. Ensure that the script is broken down into individual scenes and the scenes are linear and cohesively transition. A Hundred Dead and Loving Souls I am the sand stuck under miles of rock watching the dry air sweep away my thoughts as we travel protected by shards of glass driving through the history of a million memories In a sea of a hundred dead and loving souls In a sea of a hundred dead and loving souls In a sea of a hundred dead and loving souls Climbing a thousand unclimbed trees I don\u0027t know I want to know how it would feel to be on top of one of these! I am the root quickly ascending up through the surrounding dirt quickly devoured by a hungry sun quickly devoured by a hungry son who is my father and we are alike we look alike we talk alike we are alike we are alike we are alike we look alike we talk alike Thats why Im hungry like the bellowing sky this is no lie as sure as light entering eye want to stay high everything\u0027s cool too cool so cool and I remember that everything falls So I take a look around me now, as I speak and I imagine a disinterested crowd talking aloud and I think of what I want most and the crowd disappears and a fixated gaze envelopes my wondering mind and then she recruits and takes me to this place I\u0027ve been there before and that\u0027s where I saw her face In a sea of a hundred dead and loving souls In a sea of a hundred dead and loving souls In a sea of a hundred dead and loving souls Climbing a thousand unclimbed trees I don\u0027t know I want to know how it would feel to be on top of one of these! Story: \u0060\u0060\u0060 **Response** The response looks like the following: \u0060\u0060\u0060text # Script for A Hundred Dead and Loving Souls ## Scene 1: The Sand - A close-up shot of a grain of sand under a microscope, showing its intricate structure and texture. - The camera zooms out to reveal that the sand is part of a vast desert landscape, with dunes and rocks stretching to the horizon. - A voice-over of the protagonist (a young man) narrates the first verse of the song, expressing his feelings of being trapped and forgotten under the weight of the world. - The camera pans to show a car driving on a dusty road, with the protagonist and his father inside. They are both wearing sunglasses and hats to protect themselves from the sun. - The car is filled with boxes and bags, indicating that they are moving to a new place. - The father is driving and listening to the radio, while the protagonist is looking out the window, bored and restless. - The radio plays a news report about a nuclear fusion experiment in South Korea, followed by a song by a popular band. - The father comments on the news, saying that it is a breakthrough for humanity and that he hopes to see it in his lifetime. - The protagonist does not respond, but thinks to himself that he does not care about anything anymore. ## Scene 2: The Sea - A montage of shots showing the protagonist and his father arriving at their new home, a small town by the sea. - The father is excited and optimistic, while the protagonist is indifferent and detached. - The father unpacks their belongings and sets up their new house, while the protagonist wanders around the town, exploring the beach, the pier, and the streets. - The protagonist sees a group of teenagers hanging out at a skate park, laughing and having fun. He feels out of place and lonely. - The protagonist walks to the edge of the pier, where he sees a sign that says \u0022No diving\u0022. - He looks down at the water, where he sees hundreds of dead fish floating on the surface, victims of a recent oil spill. - He hears the chorus of the song in his head, imagining himself diving into the sea of a hundred dead and loving souls, and climbing a thousand unclimbed trees. - He wonders what it would feel like to be free and alive, instead of being stuck and dead inside. ## Scene 3: The Root - A flashback scene, showing the protagonist as a child, playing in a garden with his father. - The father is a botanist, and he teaches his son about different plants and flowers, showing him how to care for them and appreciate their beauty. - The son is curious and eager, and he asks his father many questions about the natural world. - The father tells his son that he is a root, quickly ascending up through the surrounding dirt, and that he will grow into a strong and beautiful tree one day. - The son smiles and hugs his father, saying that he loves him and that he wants to be like him. - The father smiles and hugs his son back, saying that he loves him too and that he is proud of him. ## Scene 4: The Sun - A flash-forward scene, showing the protagonist as a young adult, working in a laboratory with his father. - The father is a nuclear physicist, and he is working on a project to harness the power of nuclear fusion, the same process that fuels the sun. - The son is his assistant, and he helps his father with the experiments and calculations, following his instructions and commands. - The father tells his son that he is a hungry sun, quickly devoured by a hungry son, and that he will make a great contribution to science and humanity one day. - The son nods and agrees, but he does not look happy or satisfied. He feels pressured and obligated to follow his father\u0027s footsteps, even though he does not share his passion or vision. - The son thinks to himself that he is hungry, like the bellowing sky, and that he wants to stay high, away from the reality and expectations that weigh him down. ## Scene 5: The Crowd - A present-day scene, showing the protagonist at a party, surrounded by a crowd of people. - The party is hosted by one of the teenagers that he saw at the skate park, who invited him out of pity and curiosity. - The party is loud and chaotic, with music, drinks, and drugs everywhere. - The protagonist feels uncomfortable and anxious, and he does not fit in with the crowd. He does not know anyone, and no one knows him. - He tries to make small talk with some of the guests, but they are either rude, bored, or high, and they do not pay attention to him. - He feels invisible and insignificant, and he wishes he could disappear. - He takes a look around him, as he speaks, and he imagines a disinterested crowd, talking aloud, and he thinks of what he wants most. ## Scene 6: The Gaze - A close-up shot of a pair of eyes, staring intently at the protagonist. - The eyes belong to a girl, who is also at the party, but who is different from the rest of the crowd. She is quiet and mysterious, and she has a captivating and mysterious aura. - The girl notices the protagonist, and she is intrigued by him. She sees something in him that no one else sees, something that she relates to and understands. - She walks up to him, and she introduces herself. She says her name is Luna, and that she likes his song. She says she heard him humming it to himself, and that she knows the lyrics by heart. - The protagonist is surprised and confused, and he asks her how she knows his song. He says he wrote it himself, and that he never shared it with anyone. - The girl smiles and says that she knows his song, because she wrote it too. She says she has a gift, or a curse, depending on how you look at it. She says she can hear the songs that people sing in their souls, and that she can sing them back to them. - The protagonist is shocked and amazed, and he asks her to prove it. He asks her to sing his song, or any song that he knows. - The girl nods and agrees, and she sings his song, word for word, note for note, perfectly. She sings it with emotion and expression, as if she wrote it herself. - The protagonist is speechless and mesmerized, and he feels a connection with the girl that he never felt with anyone before. He feels like she knows him, and he wants to know her. - The girl stops singing, and she looks at him with a fixated gaze, that envelopes his wondering mind. She says she likes him, and she asks him if he wants to go with her to a place that she knows. - The protagonist nods and agrees, and he follows her out of the party, leaving the crowd behind. ## Scene 7: The Place - A montage of shots showing the protagonist and the girl driving to a secluded spot in the woods, where they park their car and get out. - The spot is a clearing, where there is a large tree, with a rope swing hanging from one of its branches. - The tree is the only one of its kind in the area, and it stands out from the rest of the forest. It is tall and majestic, with green leaves and red flowers. - The girl tells the protagonist that this is her place, and that she comes here often to escape from the world. She says she loves this tree, and that she calls it Luna, after herself. - The protagonist tells the girl that this is his place too, and that he has been here before. He says he saw this tree in his dreams, and that he always wanted to climb it. - The girl smiles and says that they have more in common than they thought, and that they are meant to be together. She says she wants to show him something, and she asks him to follow her. - The girl climbs the tree, using the rope swing as a support. She is agile and graceful, and she reaches the top in no time. - The protagonist follows her, using the rope swing as well. He is clumsy and nervous, and he struggles to keep up with her. - The girl reaches the top of the tree, where she finds a small platform, made of wood and rope. She sits on the platform, and she looks at the view. - The view is breathtaking, and it shows the sea, the town, and the sky, all in one panorama. The sun is setting, and it casts a golden glow on everything. - The girl waves at the protagonist, and she invites him to join her. She says this is the best place to watch the sunset, and that she wants to share it with him. - The protagonist reaches the top of the tree, where he joins the girl on the platform. He sits next to her, and he looks at the view. He is awestruck, and he agrees that this is the best place to watch the sunset. - The girl leans in and kisses the protagonist, and he kisses her back. They embrace each other, and they watch the sunset together. - The chorus of the song plays in the background, as the scene fades to black. \u0060\u0060\u0060 Not too bad for a first pass. From here you might want to continue tweaking the different scenes but at least you have a starting point. ### Generate prompts for each scene Using the text-to-image technique, I first need to generate prompts to guide the model in the video generation process. In the case of this sample, you could take each of the bullet points in the scenes and generate a video for each of them. At the time of this writing, I didn\u0027t have access to a video generation model and I don\u0027t think VideoPoet is open to the public yet. Therefore, I had to slightly modify what I was generating. Instead of generating a video, I used Copilot to generate images using DALL-E. The concept is the same, except instead of generating videos, I\u0027m generating still images. I could\u0027ve applied the same concept of one image per bullet point. However, for brevity, I chose to summarize all of the bullet points in a scene into a single prompt. **Prompt** Given the following prompt: \u0060\u0060\u0060text Using storyboarding techniques, for each scene, generate a single prompt to use as input for DALL-E to generate an image. Since there are 7 scenes, there should be only 7 prompts. Distill as much of the concepts from each of the bullet points in each scene. \u0060\u0060\u0060 **Response** The results from that prompt are the following: \u0060\u0060\u0060text Scene 1: The Sand A grain of sand under a microscope and a car driving on a dusty road in a desert Scene 2: The Sea A boy standing on a pier looking at a sea of dead fish and a sign that says \u201CNo diving\u201D Scene 3: The Root A father and a son playing in a garden with different plants and flowers Scene 4: The Sun A father and a son working in a laboratory with a nuclear fusion reactor Scene 5: The Crowd A boy at a party surrounded by a crowd of people with music, drinks, and drugs Scene 6: The Gaze A girl with a pair of eyes that can hear the songs that people sing in their souls Scene 7: The Place A boy and a girl sitting on a platform on top of a tree with a rope swing and a view of the sea, the town, and the sunset \u0060\u0060\u0060 Again, not too bad for a first pass. The main points of the scene are captured. I can already foresee there are a few scenes like \u0022The Sand\u0022 which might be difficult to illustrate in a single image. ### Generate images (video clips) Now that we have individual prompts, it\u0027s time to generate some images. Again, I\u0027m doing this because I don\u0027t have access to a video generation model but the process would be generally the same. **Prompt** \u0060\u0060\u0060text Use the prompts for each of the scenes to generate images with DALL-E. Render them as pencil art. \u0060\u0060\u0060 **Response** For brevity, I\u0027ll only include some of the images generated: \u0022The Sand\u0022 ![AI genreated image of a grain of sand under a microscope and a car driving on a dusty road in a desert](http://cdn.lqdev.tech/files/images/sand-car-dirt-road.jpg) \u0022The Sea\u0022 ![AI generated image of a boy standing on a pier looking at a sea of dead fish and a sign that says \u201CNo diving\u201D](http://cdn.lqdev.tech/files/images/child-dock.jpg) \u0022The Root\u0022 ![AI generated image of a father and a son playing in a garden with different plants and flowers\u201D](http://cdn.lqdev.tech/files/images/father-son-playing.jpg) The pencil art part was something I thought would look nice to illustrate this instead of digital art or photorealism. That was a personal choice though. As we can see from the images, in general they\u0027re not too bad. When not condensed into a single image though and you have an image or video per frame in a scene, the generated content can be richer. ### Edit and stitch videos For the image generation example, this seems like a natural stopping point. However, with videos, you can imagine using AI to edit these videos. The VideoPoet post mentions how you might be able to do that. Part of that editing might also include lengthening the video clips. Once you\u0027re happy with your videos, you would then stitch them together to compose a larger clip. ## Improvements Tons! I used the simplest tools available at hand to validate whether my idea is feasible. ### Videos The most obvious improvement here would be to actually generate the videos. Whether that\u0027s using VideoPoet or another video generation model, the whole point is to end up with a video clip to upload to Spotify. ### Prompts The prompts I used were not complex at all. I tried to keep them as simple as possible and in many cases, I wasn\u0027t the one providing the prompts, Copilot generated them for me. I would try to use more complex prompts to further guide the model in the formats I expect the outputs to be in and give more details for the model to generate better results. ### Code All of this was done using Copilot in the browser. As a starting point to validate my ideas this was fine. However, this approach is not composable, testable, reproducible, or scalable. I\u0027d convert these steps to code in order to get all of the things Copilot doesn\u0027t give me. ### Timestamp sync One of the features Spotify offers is lyrics that are synced to the song. Similarly, being able to sync these generated clips to the song would be important so the visuals match the ## Challenges ### Consistency One challenge that someone brought up when I was explaining this idea was, how do you ensure that the visuals are consistent throughout all of the video clips. That is a great question I don\u0027t have a great answer for, though I have ideas. If there aren\u0027t many changes throughout scenes, you could just opt to keep lengthening the video clip. If there are flashbacks or different visuals between scenes, you might achieve consistency by using the image-to-video modality. Using a snapshot from a frame in a previous video clip, you might try to edit that snapshot to use in between transitions and then lengthen the video to produce the next scene. ### Power Consumption As fascinating as this is, is it worth the energy and cooling demands it requires when using large models? Maybe not in its current form, but maybe it makes sense when using smaller models or more power efficient compute. ### People For music videos, there\u0027s a human element to it both from the performers and the people who produce them. Again, the Generative AI is impressive and for someone like me without any artistic skills, it can be an empowering tool. Although I\u0027m tempted to see what an [AI Rickroll](https://www.youtube.com/watch?v=dQw4w9WgXcQ) might look like, I prefer the human version. :slightly_smiling_face: ## Conclusion In this post I discussed how you might use Generative AI models like VideoPoet to generate short videos for Spotify Clips. Since I don\u0027t have acess to VideoPoet or other video generation models at this time, I decided to show practical image generation examples instead using models like DALL-E to illustrate the general concepts. It\u0027s not wihtout its challenges and there\u0027s a ton of improvements I can think of but it was a fun experiment regardless. What else would you use these Generative AI models for? You can find an addendum to this post [here](/notes/genai-spotify-clips-instrumental).","contentType":"posts","url":"https://www.lqdev.me/posts/generative-ai-spotify-clips","date":"2023-12-23 18:30 -05:00","tags":["ai","video","chicanobatman","generativeai","genai","spotify","google","microsoft","bing","copilot","dalle","image","prompt","projectidea","music","podcasts","llm"],"summary":"## Introduction While listening to music on Spotify last night, this thought hit me. It\u0027s been cooking for some time so now you get to read about it. Spotify and YouTube, if you build this into your...","keywords":["scene","protagonist","video","clips","generate","song","father","spotify","girl","they"]},{"id":"notes-weblogging-rewind-2023","title":"(We)blogging Rewind 2023","content":"Inspired by Spotify Wrapped and Antennapod Echo , I decided to look back at my activity on this website for 2023. Here is the snippet I used to generate these metrics. Posts by Year Blog Blog posts are my more formal, long-form content. This year, I published less blog posts than last year. Year Count 2023 5 2022 7 Part of that includes me getting in my head about what these posts \u0026quot;should be\u0026quot;. Historically, they have been detailed technical posts related to a project I\u0027ve worked on like Accept Webmentions using F#, Azure Functions, and RSS . I haven\u0027t stopped experimenting and tinkering, but because they aren\u0027t end-to-end solutions, I don\u0027t think of them as \u0026quot;blog post worthy\u0026quot;. That doesn\u0027t mean though that learnings and growth haven\u0027t come from them. In more recent blog posts like AI like it\u0027s 1999 or 1899 and Quick thoughts about Snapdragon Summit 2023 , I\u0027ve started experimenting more with just posting. In 2024, I want to be more flexible in my posting and not worry too much about whether the blog posts are \u0026quot;good enough\u0026quot;. Microblog This site also contains a microblog feed, one for notes and another for responses . Notes Notes tend to be original content that although can include links to external content, they are short-form and informal versions of blog posts. This post is an example of a note. This year, I posted less notes than last year. Year Count 2023 33 2022 36 Usually, when these notes start to get long, I turn them into blog posts. Quick thoughts about Snapdragon Summit 2023 is an example of that. However, I haven\u0027t set a criteria for what makes a note \u0026quot;long\u0026quot;. This post for example I think is getting to the \u0026quot;blog post\u0026quot; length. I can\u0027t tell you why though, it\u0027s just \u0026quot;vibes\u0026quot;. One of the benefits of starting with notes types of posts is that the frame of mind is more informal which lets me just write and post without thinking too much about what the post \u0026quot;should be\u0026quot;. In 2024, I want to continue using notes as the starting point for these types of posts. Maybe later in the future, I just merge them both. Responses Responses are reactions to content created / published by someone else. These can include bookmarks, replies, likes (star), and reposts (reshare). This year, I posted significantly more responses than last year. Year Count 2023 216 2022 146 I\u0027m very happy with this, especially my bookmark use explained later. Not only am I happy because I\u0027m actually making use of my website and the work I put in previous years that enabled me to publish all types of content but also because these responses create a log of my interactions with the internet over the years I can look back at. Responses by Type Looking deeper into my responses and breaking them down by type, bookmarks take up the top spot with more than half of my responses being bookmarks. That makes sense since I\u0027m usually bookmarking articles, papers, and other content I come across on the internet. Type Count bookmark 151 reshare 48 reply 10 star 7 Let me start off with bookmarks. Bookmark style reponses on my website have slowly become my default capture system. Now, not everything I read and come across makes it onto my bookmarks. I\u0027m experimenting with the first pass which I might just turn into another post. Generally, the things that are interesting or thought provoking, I post. Bookmark posts generally include: Post title - Gives me a readable way to see what the post is about Source link - Provides a direct link to the source material Tags - Enable me to index and organize related topics Quotes / Notes - Provide highlights and things I thought were interesting about the post When all these pieces come together, they provide me with distilled bits of information that are organized in a way that enable quick retrieval and action when needed. Moving on to other posts. Since the post type is effectively just metadata, the line between reshare, star, and reply is often blurry. It\u0027s not as blurry with replies because I use that to draft replies / comments to other posts and content on the internet. My Tracy Durnell 20th anniversary of blogging post is an example of a reply. In these examples, the replies were directed at someone. However, for star and reshare, I\u0027m still trying to understand where they fit in my flow. Today other than visually expressing the post type, the role these post types serve is webmentions. However, I\u0027m not sure how many websites I create these post types for actually use webmentions. Maybe some analysis worth doing for 2025. Responses by Tag (Top 5) AI and LLMs took the top spots which I think in general aligns with what was top of mind for everyone in 2023. Tag Count ai 104 llm 42 untagged 41 opensource 31 internet 17 Not much commentary here. Tags have helped me organize related content on my website. I do think that I get too high level with my tags and could do a better job of being more specific. Ultimately, there\u0027s no limit to the number of tags I can use so why not be liberal with it. The only self-imposed limitation I\u0027ve placed on myself is the first 3 tags should be the most important because they\u0027re the ones that show up when my articles are automatically cross-posted to Mastodon. Conclusions Overall, I\u0027m happy with my activity and heavy use of my own website this year. I plan to make even more use of the bookmark type of posts in 2024 and plan on posting more whether that\u0027s as a note or blog post, just posting is something I want to work towards.","contentType":"notes","url":"https://www.lqdev.me/notes/weblogging-rewind-2023","date":"2023-12-23 02:28 -05:00","tags":["weblogging","quantifiedself","indieweb","stats","analytics","blog","blogging","internet","opensource","community","selfhost","fsharp","spotify","antennapod","microblog","analytics","pkm","personalknowledgemanagement"],"summary":"Inspired by Spotify Wrapped and Antennapod Echo , I decided to look back at my activity on this website for 2023. Here is the snippet I used to generate these metrics. Posts by Year Blog Blog posts...","keywords":["post","posts","year","more","blog","responses","content","notes","\u0026quot","2023"]},{"id":"reshare-classic-palmtop-concept","title":"Classic Palmtop","content":"Classic Palmtop \u2192 https://www.artstation.com/artwork/aoDzL0","contentType":"reshare","url":"https://www.lqdev.me/responses/classic-palmtop-concept","date":"2023-12-22 09:28 -05:00","tags":["concept","tech","cyberpunk"],"summary":"Classic Palmtop \u2192 https://www.artstation.com/artwork/aoDzL0","keywords":["classic","palmtop","https://www","artstation","com/artwork/aodzl0"]},{"id":"bookmarks-midjourney-v6-release","title":"Midjourney v6","content":"Midjourney v6 \u2192 https://mid-journey.ai/midjourney-v6-release/ The Dev Team gonna let the community test an alpha-version of Midjourney v6 model over the winter break, starting tonight, December 21st, 2023. What\u2019s new with the Midjourney v6 base model? Much more accurate prompt following as well as longer prompts, Improved coherence, and model knowledge, Improved image prompting and remix mode, Minor text drawing ability (you must write your text in \u201Cquotations\u201D and --style raw or lower --stylize values may help) /imagine a photo of the text \u0026quot;Hello World!\u0026quot; written with a marker on a sticky note --ar 16:9 --v 6 Improved upscalers, with both \u0027subtle\u2018 and \u0027creative\u2018 modes (increases resolution by 2x) (you\u2019ll see buttons for these under your images after clicking U1/U2/U3/U4)","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/midjourney-v6-release","date":"2023-12-22 08:57 -05:00","tags":["computervision","ai","midjourney"],"summary":"Midjourney v6 \u2192 https://mid-journey.ai/midjourney-v6-release/ The Dev Team gonna let the community test an alpha-version of Midjourney v6 model over the winter break, starting tonight, December 21st,...","keywords":["midjourney","model","improved","text","\u0026quot","\u201Cquotations\u201D","ability","style","lower","world"]},{"id":"bookmarks-langchain-state-of-ai-2023","title":"LangChain State of AI 2023","content":"LangChain State of AI 2023 \u2192 https://blog.langchain.dev/langchain-state-of-ai-2023/ What are people building? Retrieval has emerged as the dominant way to combine your data with LLMs. ...42% of complex queries involve retrieval ...about 17% of complex queries are part of an agent. Most used LLM Providers OpenAI has emerged as the leading LLM provider of 2023, and Azure (with more enterprise guarantees) has seized that momentum well. On the open source model side, we see Hugging Face (4th), Fireworks AI (6th), and Ollama (7th) emerge as the main ways users interact with those models. OSS Model Providers A lot of attention recently has been given to open source models, with more and more providers racing to host them at cheaper and cheaper costs. So how exactly are developers accessing these open source models? We see that the people are mainly running them locally, with options to do so like Hugging Face, LlamaCpp, Ollama, and GPT4All ranking high. Most used vector stores Vectorstores are emerging as the primary way to retrieve relevant context. ...local vectorstores are the most used, with Chroma, FAISS, Qdrant and DocArray all ranking in the top 5. Of the hosted offerings, Pinecone leads the pack as the only hosted vectorstore in the top 5. Weaviate follows next, showing that vector-native databases are currently more used than databases that add in vector functionality. Of databases that have added in vector functionality, we see Postgres (PGVector), Supabase, Neo4j, Redis, Azure Search, and Astra DB leading the pack. Most used embeddings OpenAI reigns supreme Open source providers are more used, with Hugging Face coming in 2nd most use On the hosted side, we see that Vertex AI actually beats out AzureOpenAI Top Advanced Retrieval Strategies the most common retrieval strategy we see is not a built-in one but rather a custom one. After that, we see more familiar names popping up: Self Query - which extracts metadata filters from user\u0027s questions Hybrid Search - mainly through provider specific integrations like Supabase and Pinecone Contextual Compression - which is postprocessing of base retrieval results Multi Query - transforming a single query into multiple, and then retrieving results for all TimeWeighted VectorStore - give more preference to recent documents How are people testing? 83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate. ...the majority of them use an LLM to evaluate the outputs. While some have expressed concern and hesitation around this, we are bullish on this as an approach and see that in practice it has emerged as the dominant way to test. ...nearly 40% of evaluators are custom evaluators. This is in line with the fact that we\u0027ve observed that evaluation is often really specific to the application being worked on, and there\u0027s no one-size-fits-all evaluator to rely on. What are people testing? ...most people are still primarily concerned with the correctness of their application (as opposed to toxicity, prompt leakage, or other guardrails ...low usage of Exact Matching as an evaluation technique [suggests] that judging correctness is often quite complex (you can\u0027t just compare the output exactly as is)","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/langchain-state-of-ai-2023","date":"2023-12-22 08:32 -05:00","tags":["ai","llm","opensource","report"],"summary":"LangChain State of AI 2023 \u2192 https://blog.langchain.dev/langchain-state-of-ai-2023/ What are people building? Retrieval has emerged as the dominant way to combine your data with LLMs. ...42% of...","keywords":["more","used","people","retrieval","open","vector","source","providers","complex","feedback"]},{"id":"bookmarks-eigensolutions-composability-antidote-overfitting-verou","title":"Eigensolutions: composability as the antidote to overfit ","content":"Eigensolutions: composability as the antidote to overfit \u2192 https://lea.verou.me/blog/2023/eigensolutions/ tl;dr: Overfitting happens when solutions don\u2019t generalize sufficiently and is a hallmark of poor design. Eigensolutions are the opposite: solutions that generalize so much they expose links between seemingly unrelated use cases. Designing eigensolutions takes a mindset shift from linear design to composability. The eigensolution is a solution that addresses several key use cases, that previously appeared unrelated. ...it takes a mindset shift, from the linear Use case \u2192 Idea \u2192 Solution process to composability. Rather than designing a solution to address only our driving use cases, step back and ask yourself: can we design a solution as a composition of smaller, more general features, that could be used together to address a broader set of use cases? Contrary to what you may expect, eigensolutions can actually be quite hard to push to stakeholders: Due to their generality, they often require significantly higher engineering effort to implement. Quick-wins are easier to sell: they ship faster and add value sooner. In my 11 years designing web technologies, I have seen many beautiful, elegant eigensolutions be vetoed due to implementation difficulties in favor of far more specific solutions \u2014 and often this was the right decision, it\u2019s all about the cost-benefit. Eigensolutions tend to be lower level primitives, which are more flexible, but can also involve higher friction to use than a solution that is tailored to a specific use case. Eigensolutions tend to be lower level primitives. They enable a broad set of use cases, but may not be the most learnable or efficient way to implement all of them, compared to a tailored solution. In other words, they make complex things possible, but do not necessarily make common things easy. Instead of implementing tailored solutions ad-hoc (risking overfitting), they can be implemented as shortcuts: higher level abstractions using the lower level primitive. Done well, shortcuts provide dual benefit: not only do they reduce friction for common cases, they also serve as teaching aids for the underlying lower level feature. This offers a very smooth ease-of-use to power curve: if users need to go further than what the shortcut provides, they can always fall back on the lower level primitive to do so. In an ideal world, lower level primitives and higher level abstractions would be designed and shipped together. However, engineering resources are typically limited, and it often makes sense to ship one before the other, so we can provide value sooner. This can happen in either direction: Lower level primitive first. Shortcuts to make common cases easy can ship at a later stage, and demos and documentation to showcase common \u201Crecipes\u201D can be used as a stopgap meanwhile. This prioritizes use case coverage over optimal UX, but it also allows collecting more data, which can inform the design of the shortcuts implemented. Higher level abstraction first, as an independent, ostensibly ad hoc feature. Then later, once the lower level primitive ships, it is used to \u201Cexplain\u201D the shortcut, and make it more powerful. This prioritizes optimal UX over use case coverage: we\u2019re not covering all use cases, but for the ones we are covering, we\u2019re offering a frictionless user experience. ...despite the name eigensolution, it\u2019s still all about the use cases: eigensolutions just expose links between use cases that may have been hard to detect, but seem obvious in retrospect...Requiring all use cases to precede any design work can be unnecessarily restrictive, as frequently solving a problem improves our understanding of the problem.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/eigensolutions-composability-antidote-overfitting-verou","date":"2023-12-21 22:38 -05:00","tags":["design","product"],"summary":"Eigensolutions: composability as the antidote to overfit \u2192 https://lea.verou.me/blog/2023/eigensolutions/ tl;dr: Overfitting happens when solutions don\u2019t generalize sufficiently and is a hallmark of...","keywords":["level","cases","they","lower","eigensolutions","solution","design","more","higher","case"]},{"id":"reply-proton-mail-vs-tutanota-encryption","title":"Proton Mail versus Tuta (Tutanota) encryption","content":"Proton Mail versus Tuta (Tutanota) encryption \u2192 https://proton.me/blog/proton-vs-tuta-encryption","contentType":"reply","url":"https://www.lqdev.me/responses/proton-mail-vs-tutanota-encryption","date":"2023-12-21 21:18 -05:00","tags":["protonmail","tutanota","encryption"],"summary":"Proton Mail versus Tuta (Tutanota) encryption \u2192 https://proton.me/blog/proton-vs-tuta-encryption","keywords":["tuta","encryption","proton","mail","versus","tutanota","https://proton","me/blog/proton"]},{"id":"reshare-phi-2-huggingface","title":"Phi-2 now on HuggingFace","content":"Phi-2 now on HuggingFace \u2192 https://huggingface.co/microsoft/phi-2 When Phi-2 initially released , it was on the Azure AI Studio Model Catalog. It\u0027s nice to see it\u0027s now in HuggingFace as well.","contentType":"reshare","url":"https://www.lqdev.me/responses/phi-2-huggingface","date":"2023-12-20 18:37 -05:00","tags":["opensource","ai","phi","languagemodel","smalllanguagemodel","huggingface","transformers","microsoft"],"summary":"Phi-2 now on HuggingFace \u2192 https://huggingface.co/microsoft/phi-2 When Phi-2 initially released , it was on the Azure AI Studio Model Catalog. It\u0027s nice to see it\u0027s now in HuggingFace as well.","keywords":["huggingface","https://huggingface","co/microsoft/phi","initially","released","azure","studio","model","catalog","nice"]},{"id":"bookmarks-llm-flash-inference-limited-memory","title":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory","content":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory \u2192 https://arxiv.org/abs/2312.11514 Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their intensive computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters on flash memory but bringing them on demand to DRAM. Our method involves constructing an inference cost model that harmonizes with the flash memory behavior, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this flash memory-informed framework, we introduce two principal techniques. First, \u0026quot;windowing\u0027\u0026quot; strategically reduces data transfer by reusing previously activated neurons, and second, \u0026quot;row-column bundling\u0026quot;, tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/llm-flash-inference-limited-memory","date":"2023-12-20 18:30 -05:00","tags":["llm","ai","hardware"],"summary":"LLM in a flash: Efficient Large Language Model Inference with Limited Memory \u2192 https://arxiv.org/abs/2312.11514 Large language models (LLMs) are central to modern natural language processing,...","keywords":["memory","flash","data","dram","inference","llms","\u0026quot","model","language","limited"]},{"id":"bookmarks-dismantle-technology-newport","title":"It\u2019s Time to Dismantle the Technopoly","content":"It\u2019s Time to Dismantle the Technopoly \u2192 https://www.newyorker.com/tech/annals-of-technology/its-time-to-dismantle-the-technopoly ...according to [Neil Postman], we no longer live in a technocratic era. We now inhabit what he calls technopoly. In this third technological age, Postman argues, the fight between invention and traditional values has been resolved, with the former emerging as the clear winner. The result is the \u201Csubmission of all forms of cultural life to the sovereignty of technique and technology.\u201D Innovation and increased efficiency become the unchallenged mechanisms of progress, while any doubts about the imperative to accommodate the shiny and new are marginalized. \u201CTechnopoly eliminates alternatives to itself in precisely the way Aldous Huxley outlined in Brave New World,\u201D Postman writes. \u201CIt does not make them illegal. It does not make them immoral. It does not even make them unpopular. It makes them invisible and therefore irrelevant.\u201D Technopoly, he concludes, \u201Cis totalitarian technocracy.\u201D What I didn\u2019t realize back in 2016, however, was that, although the grip of technopoly was strong, it was also soon to weaken. A major source of this destabilization was the Trump-Clinton election cycle...Where once they had seen platforms like Facebook as useful and in some sense mandatory, they started treating them more warily. This emerging resistance to the technopoly mind-set doesn\u2019t fall neatly onto a spectrum with techno-optimism at one end and techno-skepticism at the other. Instead, it occupies an orthogonal dimension we might call techno-selectionism. This is a perspective that accepts the idea that innovations can significantly improve our lives but also holds that we can build new things without having to accept every popular invention as inevitable. Techno-selectionists believe that we should continue to encourage and reward people who experiment with what comes next. But they also know that some experiments end up causing more bad than good. Techno-selectionists can be enthusiastic about artificial intelligence, say, while also taking a strong stance on settings where we should block its use. They can marvel at the benefits of the social Internet without surrendering their kids\u2019 mental lives to TikTok.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/dismantle-technology-newport","date":"2023-12-20 10:12 -05:00","tags":["technology","culture","media","internet"],"summary":"It\u2019s Time to Dismantle the Technopoly \u2192 https://www.newyorker.com/tech/annals-of-technology/its-time-to-dismantle-the-technopoly ...according to [Neil Postman], we no longer live in a technocratic...","keywords":["technopoly","techno","also","they","does","postman","while","more","invention","selectionists"]},{"id":"bookmarks-openai-prompting-guide","title":"OpenAI - Prompt engineering","content":"OpenAI - Prompt engineering \u2192 https://platform.openai.com/docs/guides/prompt-engineering This guide shares strategies and tactics for getting better results from large language models (sometimes referred to as GPT models) like GPT-4. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/openai-prompting-guide","date":"2023-12-19 19:37 -05:00","tags":["openai","llm","promptengineering","ai","guide"],"summary":"OpenAI - Prompt engineering \u2192 https://platform.openai.com/docs/guides/prompt-engineering This guide shares strategies and tactics for getting better results from large language models (sometimes...","keywords":["openai","engineering","methods","sometimes","models","experimentation","encourage","effect","greater","combination"]},{"id":"reshare-spritely-veilid-p2p-web","title":"Spritely and Veilid: Exciting Projects Building the Peer-to-Peer Web","content":"Spritely and Veilid: Exciting Projects Building the Peer-to-Peer Web \u2192 https://www.eff.org/deeplinks/2023/12/meet-spritely-and-veilid While there is a surge in federated social media sites, like Bluesky and Mastodon, some technologists are hoping to take things further than this model of decentralization with fully peer-to-peer applications. Two leading projects, Spritely and Veilid, hint at what this could look like. Spritely is a framework for building distributed apps that don\u2019t even have to know that they\u2019re distributed. The project is spearheaded by Christine Lemmer-Webber, who was one of the co-authors of the ActivityPub spec that drives the fediverse. She is taking the lessons learned from that work, combining them with security and privacy minded object capabilities models, and mixing it all up into a model for peer to peer computation that could pave the way for a generation of new decentralized tools. The Veilid project was released at DEFCON 31 in August and has a number of promising features that could lead to it being a fundamental tool in future decentralized systems. Described as a cross between TOR and Interplanetary File System (IPFS), Veilid is a framework and protocol that offers two complementary tools. The first is private routing, which, much like TOR, can construct an encrypted private tunnel over the public internet allowing two devices to communicate with each other without anyone else on the network knowing who is talking to whom...The second tool that Veilid offers is a Distributed Hash Table (DHT), which lets anyone look up a bit of data associated with a specific key, wherever that data lives on the network. Public interest in decentralized tools and services is growing, as people realize that there are downsides to centralized control over the platforms that connect us all. The past year has seen interest in networks like the fediverse and Bluesky explode and there\u2019s no reason to expect that to change. Projects like Spritely and Veilid are pushing the boundaries of how we might build apps and services in the future. The things that they are making possible may well form the foundation of social communication on the internet in the next decade, making our lives online more free, secure, and resilient. Additional Links Spritely Institute Website Veilid Website","contentType":"reshare","url":"https://www.lqdev.me/responses/spritely-veilid-p2p-web","date":"2023-12-19 11:47 -05:00","tags":["p2p","veilid","spritely","openweb","web","internet","decentralization"],"summary":"Spritely and Veilid: Exciting Projects Building the Peer-to-Peer Web \u2192 https://www.eff.org/deeplinks/2023/12/meet-spritely-and-veilid While there is a surge in federated social media sites, like...","keywords":["veilid","spritely","peer","distributed","projects","decentralized","tools","apps","project","fediverse"]},{"id":"reshare-2023-social-media-case-fediverse","title":"2023 in social media: the case for the fediverse","content":"2023 in social media: the case for the fediverse \u2192 https://www.theverge.com/23990974/social-media-2023-fediverse-mastodon-threads-activitypub Good article from David Pierce. I\u0027d add that many of these platforms (i.e. Mastodon, Lemmy, PeerTube, WordPress) have strong RSS support which offer another degree of freedom from opting out of signing up for any of the platforms but still being able to follow the people and topics you care about. Sure, the experience may not be as rich but it\u0027s yet another way for people to participate in the ecosystem. A new kind of social internet is currently forming. Right now it might still look like \u201CTwitter and Reddit, only different,\u201D but that\u2019s only the very beginning of what\u2019s to come. Hopefully. I\u2019m convinced we\u2019ll be better off with a hundred different apps for Snapchat or Instagram or X instead of just one... It doesn\u2019t make sense that we have a dozen usernames, a dozen profiles, a dozen sets of fans and friends. All that stuff should belong to me, and I should be able to access it and interact with it anywhere and everywhere. Decentralizing social media can sound like a sort of kumbaya anti-capitalist manifesto: \u201CIt\u2019s about openness and sharing, not capitalism, man!\u201D In practice it\u2019s the opposite: it\u2019s a truly free market approach to social networking. ...in a fediverse-dominated world, the way to win is not to achieve excellent lock-in and network effects. The only way to win is to build the best product. ...so far we\u2019re mostly in the \u201Cpopular app, but federated\u201D phase of this transition. Almost everything in the fediverse is a one-to-one competitor to an existing platform...Some of these apps are very good! But nearly all of them are differentiated only in that they\u2019re federated. Let\u2019s be super clear about this: the point of the fediverse is not that it\u2019s federated...Making the \u201CIt\u2019s federated!\u201D argument is like making the \u201CIt\u2019s better for privacy!\u201D argument: it makes you feel good, and at best it\u2019s a useful tiebreaker, but it doesn\u2019t actually matter. All that matters is the product. 2023 was the year \u201Cfediverse\u201D became a buzzword, 2024 will be the year it becomes an industry. (Hopefully one with a better name, but I\u2019ll get over that.) We\u2019ve spent too long living our lives online in someone else\u2019s spaces. What\u2019s next will belong to all of us. All that\u2019s left to do is start posting.","contentType":"reshare","url":"https://www.lqdev.me/responses/2023-social-media-case-fediverse","date":"2023-12-19 09:26 -05:00","tags":["fediverse","socialmedia","activitypub","internet","web","openweb","smallweb"],"summary":"2023 in social media: the case for the fediverse \u2192 https://www.theverge.com/23990974/social-media-2023-fediverse-mastodon-threads-activitypub Good article from David Pierce. I\u0027d add that many of...","keywords":["fediverse","it\u2019s","social","\u201Cit\u2019s","dozen","better","federated","good","2023","mastodon"]},{"id":"notes-growing-activitypub-adoption-december-2023","title":"Growing ActivityPub adoption","content":"Exciting developments in the ActivityPub space. A few months ago, the WordPress plugin was released. In the last few days, Threads recently started testing their ActivityPub integration . Not so new but new to me, I saw that Discourse, the forum software, also has an ActivityPub plugin . Flipboard is one of the latest names to express plans for ActivityPub integration. Not sure what the end result will look like when all these platforms are all interoping with each other but I\u0027m excited to see platforms building on open protocols.","contentType":"notes","url":"https://www.lqdev.me/notes/growing-activitypub-adoption-december-2023","date":"2023-12-18 20:28 -05:00","tags":["activitypub","fediverse","openweb","protocols","internet","web"],"summary":"Exciting developments in the ActivityPub space. A few months ago, the WordPress plugin was released. In the last few days, Threads recently started testing their ActivityPub integration . Not so new...","keywords":["activitypub","integration","platforms","plugin","exciting","latest","names","express","plans","result"]},{"id":"notes-crate-finds-fall-2023","title":"Crate Finds - Fall 2023","content":"Trying something new out I\u0027m calling Crate Finds. I\u0027m using it as a means to look back at either new music I came across or music I\u0027m listening to at various points in time. Kind of like Spotify Wrapped, but less stats and aggregates and more point-in-time snapshots without the granuliarity of scrobbler services like Last.fm. This is the first drop from Fall 2023. Tracks Look at My Soul by Adrian Quesada, Black Pumas, Kam Franklin Album: Look at My Soul: The Latin Shade of Texas Soul Duration: 3:28 Listen on YouTube Backup: Listen on Spotify Joey by Adrian Quesada, Paul and the Tall Trees Album: Look at My Soul: The Latin Shade of Texas Soul Duration: 4:02 Listen on YouTube Backup: Listen on Spotify 100 High Street by Earl Sweatshirt, The Alchemist Album: VOIR DIRE Duration: 1:34 Listen on YouTube Backup: Listen on Spotify Will I See You Again? by Thee Sacred Souls Album: Will I See You Again? Duration: 4:00 Listen on YouTube Backup: Listen on Spotify Change by Sugar Candy Mountain Album: 666 Duration: 1:50 Listen on YouTube Backup: Listen on Spotify Cactus Dream by Sugar Candy Mountain Album: Mystic Hits Duration: 1:17 Listen on YouTube Backup: Listen on Spotify It\u0027s Our Love by Thee Sacred Souls Album: Will I See You Again? Duration: 3:47 Listen on YouTube Backup: Listen on Spotify Can I Call You Rose? by Thee Sacred Souls Album: Thee Sacred Souls Duration: 3:10 Listen on YouTube Backup: Listen on Spotify What\u0027s His Name by Joey Quinones, Thee Sinseers Album: What\u0027s His Name Duration: 3:03 Listen on YouTube Backup: Listen on Spotify In The Meantime by The Altons Album: In The Meantime Duration: 5:32 Listen on YouTube Backup: Listen on Spotify A Rose Blooms in Chaos by Davishmar Album: A Rose Blooms in Chaos Duration: 1:56 Listen on YouTube Backup: Listen on Spotify Witches by Alice Phoebe Lou Album: Witches Duration: 3:15 Listen on YouTube Backup: Listen on Spotify Amor by Alta Elegancia, DannyLux Album: Amor Duration: 4:41 Listen on YouTube Backup: Listen on Spotify Jugaste y Sufr\u00ED by Eslabon Armado, DannyLux Album: Corta Venas Duration: 4:35 Listen on YouTube Backup: Listen on Spotify LAGUNAS by Peso Pluma, Jasiel Nu\u00F1ez Album: G\u00C9NESIS Duration: 3:51 Listen on YouTube Backup: Listen on Spotify Takis by \u00C9 Arenas Album: LA Duration: 3:11 Listen on YouTube Backup: Listen on Spotify Tell Me I\u0027m Dreaming by Los Yesterdays Album: Tell Me I\u0027m Dreaming Duration: 2:26 Listen on YouTube Backup: Listen on Spotify Hell N Back by Bakar Album: Hell N Back Duration: 3:33 Listen on YouTube Backup: Listen on Spotify Velocirapture (The Serpent Flew) by Particle Kid, Mickey Raphael, Sean Ono Lennon Album: Time Capsule Duration: 5:45 Listen on YouTube Backup: Listen on Spotify Propaganda by Salami Rose Joe Louis, Brijean Album: Akousmatikous / Propaganda Duration: 2:32 Listen on YouTube Backup: Listen on Spotify Akousmatikous by Salami Rose Joe Louis, Soccer96 Album: Akousmatikous / Propaganda Duration: 5:45 Listen on YouTube Backup: Listen on Spotify Smokestack Lightnin\u0027 by Howlin\u0027 Wolf Album: Moanin\u0027 In The Moonlight Duration: 3:08 Listen on YouTube Backup: Listen on Spotify San Tropez by Pink Floyd Album: Meddle Duration: 3:43 Listen on YouTube Backup: Listen on Spotify Running Away by Thee Sacred Souls Album: Running Away Duration: 3:15 Listen on YouTube Backup: Listen on Spotify NEON LIGHT by Bardo Album: EVERYWHERE REMINDS ME OF SPACE Duration: 3:58 Listen on YouTube Backup: Listen on Spotify Polaroid Weekend by Bardo Album: Polaroid Weekend Duration: 3:40 Listen on YouTube Backup: Listen on Spotify Rose Gold by Jason Joshua \u0026amp; The Beholders, Jason Joshua Album: Alegr\u00EDa Y Tristeza Duration: 5:10 Listen on YouTube Backup: Listen on Spotify Dragonfly by Dana and Alden Album: Quiet Music for Young People Duration: 5:57 Listen on YouTube Backup: Listen on Spotify IJ by Sam Gendel Album: IJ Duration: 2:30 Listen on YouTube Backup: Listen on Spotify Original Spotify Playlist: Listen on Spotify","contentType":"notes","url":"https://www.lqdev.me/notes/crate-finds-fall-2023","date":"2023-12-17 14:59 -05:00","tags":["cratefinds","music","playlist","spotify"],"summary":"Trying something new out I\u0027m calling Crate Finds. I\u0027m using it as a means to look back at either new music I came across or music I\u0027m listening to at various points in time. Kind of like Spotify...","keywords":["listen","spotify","backup:","album:","duration:","youtube","thee","rose","souls","sacred"]},{"id":"notes-turning-off-on-mastodon-edition","title":"Have you tried turning it off and on again? - Mastodon edition","content":"My Mastodon instance crashed this morning due to a combination of the usual running out of space issue and me being too lazy to remember to remove federated media regularly from my instance BEFORE it crashes. Now I know you can schedule this with CRON jobs but for some reason I\u0027ve never been able to configure that correctly nor have I been motivated to get to the bottom of it. Anyway, I went through the usual dance of starting up my services and using the CLI to clean up my instance . However, none of my commands worked because it couldn\u0027t connect to the database. A quick look at the status showed the database and all my services were running which led me to go down a troubleshooting rabbit hole with no end in sight and little guidance as to what might be the problem. Eventually, before gave up I thought, \u0026quot;my instance is down anyway, couldn\u0027t I just turn it off and on again?\u0026quot; I did exactly that and when it restarted I was able to continue with my cleanup tasks as normal. So as a reminder, if all else fails, try turning it off and on again.","contentType":"notes","url":"https://www.lqdev.me/notes/turning-off-on-mastodon-edition","date":"2023-12-15 15:53 -05:00","tags":["mastodon","sysadmin","selfhost","fediverse"],"summary":"My Mastodon instance crashed this morning due to a combination of the usual running out of space issue and me being too lazy to remember to remove federated media regularly from my instance BEFORE it...","keywords":["instance","couldn","database","usual","running","services","able","\u0026quot","anyway","again"]},{"id":"reshare-microblogging-telegram-jwz","title":"\u0022Microblogging\u0022 prior art","content":"\u0026quot;Microblogging\u0026quot; prior art \u2192 https://mastodon.social/@jwz/111583679963120813 Just published a blog post, AI like it\u0027s 1999 or 1899 , inspired by this post from jwz, among other things.","contentType":"reshare","url":"https://www.lqdev.me/responses/microblogging-telegram-jwz","date":"2023-12-15 11:18 -05:00","tags":["microblogging","telegram","meme"],"summary":"\u0026quot;Microblogging\u0026quot; prior art \u2192 https://mastodon.social/@jwz/111583679963120813 Just published a blog post, AI like it\u0027s 1999 or 1899 , inspired by this post from jwz, among other things.","keywords":["post","\u0026quot","microblogging\u0026quot","prior","https://mastodon","social/@jwz/111583679963120813","published","blog","1999","1899"]},{"id":"posts-ai-1999-1899","title":"AI like it\u0027s 1999 or 1899","content":"## Introduction In an earlier note, I talked about the natural progression of technology which goes from [scarcity to abundance](/notes/ai-abundance-scarcity-cycle-repeats-rss-enclosure-use-case) over time and AI is no different. That was in the context of media on the internet and paying per minute with early cell phones. One of the original challenges with AI was context length and token limits. It\u0027s becoming less relevant as context sizes continue to increase. However, it\u0027s not like you can fit the entire Harry Potter series either. So you still need to cleverly manage your context size especially when using documents and chat history as part of that context. Historically, people have used abbreviations to compress messages either because of cost or medium constraints. That\u0027s what this post explores in the context of AI. ## Compressing text like T9 Below is a conversation Microsoft Copilot (formerly Bing Chat) using some information from the article [From 1890: The First Text Messages](https://sundaymagazine.org/2010/08/10/from-1890-the-first-text-messages/): \u003E **Me** \u003E \u003E Summarize this text: \u003E \u003E \u003E \u003E I\u2019m trying something new today. Sometimes in my research I find an interesting old article that I wouldn\u2019t normally post because it\u2019s not from the Sunday Magazine section, or it\u2019s from further than 100 years ago so I\u2019ll never get to it. Instead of letting these go unused, I figure I\u2019ll occasionally post them midweek during what would otherwise be slow weeks. Since this weekend I only have three articles to post, it seems like a good week to try it. \u003E \u003E \u003E \u003E Telegraph operators on opposite sides of the country had some time to get to know each other when they weren\u2019t busy sending other people\u2019s messages. \u201CMetaphorically they shake hands cordially twice a day \u2014 when they begin work and when they end it. And when business is dull they hold long conversations, with hundreds of miles \u2014 perhaps thousands \u2014 separating them, as two friends might do over a dinner table.\u201D \u003E \u003E \u003E \u003E What really caught my eye, though, is that the abbreviations they used seem a lot like the abbreviations used in today\u2019s text messages. \u003E \u003E **Copilot** \u003E \u003E Here is a possible summary of the text: \u003E \u003E \u003E \u003E The author is trying a new feature of posting old articles from the Sunday Magazine section on midweek. The article is about how telegraph operators communicated with each other using abbreviations that are similar to today\u2019s text messages. The author finds this interesting and wonders if there are other similarities between old and new forms of communication. \u003E \u003E **Me** \u003E \u003E Now abbreviate it as if using a T9 keyboard for conciseness \u003E \u003E **Copilot** \u003E \u003E That sounds like a fun challenge. Here is a possible abbreviation of the summary: \u003E \u003E \u003E \u003E try new feat post old artcls midwk. artcl abt telgrph oprtrs chat w/ abbrvtns lk txt msgs. find intrstng n wndr othr smilrts btwn old n new comms. In this conversation, I first had Copilot summarize the original text. It\u0027s not a very long piece of text but it works for this example. You could imagine though how a longer piece of text would benefit even more from this summarization. Copilot does a good job of summarizing the text. However, what if I wanted to summarize it even more? In comes T9 (kind of). T9 is short for \u0022Text on 9 keys\u0022 and it was predictive text technology that made it easier to compose SMS text messages using only the 9 keys on the phone keypad. Before T9, it was cumbersome and time consuming to compose full sentences so you might abbreviate them. For example \u0022See you later\u0022 might be abbreviated as \u0022c u l8r\u0022. You can learn more in the [SMS language Wikipedia article](https://en.wikipedia.org/wiki/SMS_language). Not only would you abbreviate because it was tedious to compose messages one letter at a time, but also you were limited in the number of characters you could send in a single SMS. Somewhat related, in the early days of microblogging on Twitter, one of the ways you could compose and view tweets was through SMS. There was the cost aspects as well. You were charged per SMS so you wanted to send as much information in a single SMS as possible. Now SMS is only one example where this took place. Telegrams and telegraphs from yesteryear had similar constraints. Whether it was cost, size limits, or general UX challenges, people looked to abbreviations to draft their messages. Now that we\u0027re caught up on this history, that\u0027s what you\u0027re seeing in this response from Copilot. \u003E try new feat post old artcls midwk. artcl abt telgrph oprtrs chat w/ abbrvtns lk txt msgs. find intrstng n wndr othr smilrts btwn old n new comms. It is an abbreviated form of the summarized message enabling you to use even less tokens as part of the prompts you submit to AI. When looking at \u0022word\u0022 counts, they are roughly the following: | Text | Word Count | | --- | --- | | Original | 58 | | Summarized | 25 | | T9 | 18 | We can see that the abbreviated version is ~3x smaller than the original message. ## Conclusion Given that models like GPT and others have been trained on the entire internet, it has some knowledge of these types of abbreviations to compose messages. Now, is it scalable and would you want to use this in your production applications, I can\u0027t say. Probably not because the abbreviations are lossy. While in general you could read and get the gist of the message using the abbreviations, there are things lost in translation. Now maybe the LLM might do a better job at interpreting the abbreviations. It\u0027d be a fun experiment. \u0022n d mntm, il kp usng AI lk its 1999. c u l8r!\u0022 Translation: \u0022In the meantime, I\u0027ll keep using AI like it\u0027s 1999. See you later!\u0022 :slightly_smiling_face:","contentType":"posts","url":"https://www.lqdev.me/posts/ai-1999-1899","date":"2023-12-15 08:56 -05:00","tags":["ai","communication","sms","retro","internet"],"summary":"## Introduction In an earlier note, I talked about the natural progression of technology which goes from [scarcity to abundance](/notes/ai-abundance-scarcity-cycle-repeats-rss-enclosure-use-case)...","keywords":["text","abbreviations","from","messages","using","they","context","post","compose","original"]},{"id":"reply-floss-weekly-ends-twit-start-of-an-era","title":"FLOSS Weekly ended its run on the TWiT network","content":"FLOSS Weekly ended its run on the TWiT network \u2192 https://doc.searls.com/2023/12/14/start-of-an-era/ After 17 years and 761 episodes, FLOSS Weekly ended its run on the TWiT network yesterday. Nooo! So sad to hear that FLOSS Weekly is ending, especially after learning that The Privacy, Security, and OSINT Show with Michael Bazzell ended as well . At least there\u0027s some hope at the end of Doc\u0027s post which hints at it living on in some form. By the way, FLOSS Weekly has not slipped below the waves. I expect it will be picked up somewhere else on the Web, and wherever you get your podcasts. (I love that expression because it means podcasting isn\u2019t walled into some giant\u2019s garden.) When that happens, I\u2019ll point to it here. In any case, it was good while it lasted. Also, there\u0027s still Reality 2.0 where the guests and topics are just as interesting and entertaining.","contentType":"reply","url":"https://www.lqdev.me/responses/floss-weekly-ends-twit-start-of-an-era","date":"2023-12-14 21:10 -05:00","tags":["podcast","floss","twit","opensource"],"summary":"FLOSS Weekly ended its run on the TWiT network \u2192 https://doc.searls.com/2023/12/14/start-of-an-era/ After 17 years and 761 episodes, FLOSS Weekly ended its run on the TWiT network yesterday. Nooo! So...","keywords":["floss","weekly","ended","twit","network","isn\u2019t","podcasting","means","expression","love"]},{"id":"bookmarks-solo-mozilla-ai-website-builder","title":"Solo - an AI website builder for solopreneurs","content":"Solo - an AI website builder for solopreneurs \u2192 https://blog.mozilla.org/en/mozilla/introducing-solo-ai-website-builder/ Today we are excited to introduce a new Mozilla Innovation Project, Solo, an AI website builder for solopreneurs. If you scour Yelp, it appears a third of businesses lack a website. However, building a website not only provides you with a presence that you own and control but it is also good for business. Our survey data shows that the majority of solopreneurs rely upon their \u201Ctech buddy\u201D to help build their website. As a result, the websites become stale and harder to maintain as it relies on a call to their buddy. Others without a \u201Ctech buddy\u201D try popular website authoring tools and then abandon because it\u2019s simply too hard to author and curate content. Using AI to generate the content of your site and source your images, which a solopreneur can then revise into their own unique voice and style levels the playing field. Solo takes this a step further and can also scrape your existing business Yelp or other page so you have an online presence that is totally authentic to you.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/solo-mozilla-ai-website-builder","date":"2023-12-14 10:10 -05:00","tags":["ai","mozilla","web","internet"],"summary":"Solo - an AI website builder for solopreneurs \u2192 https://blog.mozilla.org/en/mozilla/introducing-solo-ai-website-builder/ Today we are excited to introduce a new Mozilla Innovation Project, Solo, an...","keywords":["website","solo","solopreneurs","then","buddy\u201D","\u201Ctech","content","business","also","yelp"]},{"id":"reply-farewell-privacy-security-osint-podcast","title":"Farewell to The Privacy, Security, and OSINT Show","content":"Farewell to The Privacy, Security, and OSINT Show \u2192 https://www.adamsdesk.com/posts/farewell-privacy-security-and-osint-show/ ...a farewell episode was released on November 20th, 2023 entitled \u201CMy Irish Exit\u201D. It was finally officially confirmed that [The Privacy, Security, and OSINT Show with Michael Bazzell]...has reached an end. That\u0027s unfortunate. I really enjoyed listening to this show and even had it listed in my podroll . The UNREDACTED magazine had great content as well.","contentType":"reply","url":"https://www.lqdev.me/responses/farewell-privacy-security-osint-podcast","date":"2023-12-13 20:00 -05:00","tags":["podcast","privacy","osint"],"summary":"Farewell to The Privacy, Security, and OSINT Show \u2192 https://www.adamsdesk.com/posts/farewell-privacy-security-and-osint-show/ ...a farewell episode was released on November 20th, 2023 entitled \u201CMy...","keywords":["privacy","security","osint","show","farewell","michael","bazzell","reached","unfortunate","really"]},{"id":"notes-til-firefish","title":"Today I Learned about Firefish","content":"TIL about Firefish . I like the look of it. It reminds me of Tumblr in many ways.","contentType":"notes","url":"https://www.lqdev.me/notes/til-firefish","date":"2023-12-13 19:36 -05:00","tags":["firefish","fediverse","socialmedia"],"summary":"TIL about Firefish . I like the look of it. It reminds me of Tumblr in many ways.","keywords":["firefish","look","reminds","tumblr","many","ways"]},{"id":"notes-antennapod-echo-2023","title":"Antennapod Echo (Wrapped) 2023","content":"This was a nice surprise to see when I opened up Antennapod the other day. Using my listening statistics, it was able to compile a yearly summary for me (locally). Looks like I have a lot of catching up to do on the backlog.","contentType":"notes","url":"https://www.lqdev.me/notes/antennapod-echo-2023","date":"2023-12-12 21:03 -05:00","tags":["podcasts","opensource","antennapod","stats"],"summary":"This was a nice surprise to see when I opened up Antennapod the other day. Using my listening statistics, it was able to compile a yearly summary for me (locally). Looks like I have a lot of catching...","keywords":["nice","surprise","opened","antennapod","using","listening","statistics","able","compile","yearly"]},{"id":"notes-spotify-wrapped-2023","title":"Spotify Wrapped 2023","content":"Kind of late, but here\u0027s my Spotify Wrapped for the year. No idea how Eslabon Armado snuck in there at the end. It\u0027d be cool if I was also able to get data from my MP3 player which is where I mainly listen to music. Though the artists wouldn\u0027t change much.","contentType":"notes","url":"https://www.lqdev.me/notes/spotify-wrapped-2023","date":"2023-12-12 21:03 -05:00","tags":["music","stats","spotifywrapped"],"summary":"Kind of late, but here\u0027s my Spotify Wrapped for the year. No idea how Eslabon Armado snuck in there at the end. It\u0027d be cool if I was also able to get data from my MP3 player which is where I mainly...","keywords":["kind","wouldn","artists","though","music","listen","mainly","which","player","from"]},{"id":"reshare-journaling-suggestions-day-one","title":"Introducing Journaling Suggestions for Day One","content":"Introducing Journaling Suggestions for Day One \u2192 https://dayoneapp.com/blog/introducing-journaling-suggestions/ See journaling recommendations inspired by your photos, locations, activities and more. Exclusively for iPhone. I\u0027ve been tinkering with Day One the past few months. When paired with their templates, this is a nice addition. Too bad it\u0027s iPhone exclusive. Hopefully it makes its way to Android at some point.","contentType":"reshare","url":"https://www.lqdev.me/responses/journaling-suggestions-day-one","date":"2023-12-12 20:49 -05:00","tags":["journaling","dayone"],"summary":"Introducing Journaling Suggestions for Day One \u2192 https://dayoneapp.com/blog/introducing-journaling-suggestions/ See journaling recommendations inspired by your photos, locations, activities and more....","keywords":["journaling","iphone","introducing","makes","hopefully","exclusive","addition","nice","templates","paired"]},{"id":"reshare-aoudaghost-aprin","title":"Aoudaghost, unveiling the lost economic hub of the Sahara","content":"Aoudaghost, unveiling the lost economic hub of the Sahara \u2192 https://wildmanlife.com/aoudaghost-economic-hub-of-the-sahara/ Since 2001, the site has been on the UNESCO World Heritage Tentative List. Today, Aoudaghost is in a state of complete abandonment. The remains of the once-thriving town are concentrated in the area most protected by the wind and sand, with several walls and fortifications yet to be fully englobed by the desert. From the adjacent cliff, the current state of Aoudaghost can be seen in its entirety, but only the mind can imagine the Aoudaghost that served as an economic and cultural hub for the Sahara","contentType":"reshare","url":"https://www.lqdev.me/responses/aoudaghost-aprin","date":"2023-12-12 19:45 -05:00","tags":["travel","sahara"],"summary":"Aoudaghost, unveiling the lost economic hub of the Sahara \u2192 https://wildmanlife.com/aoudaghost-economic-hub-of-the-sahara/ Since 2001, the site has been on the UNESCO World Heritage Tentative List....","keywords":["aoudaghost","economic","sahara","state","protected","wind","sand","several","walls","fortifications"]},{"id":"bookmarks-memorycache-personal-ai","title":"MemoryCache - Augmenting Local AI with Browser Data","content":"MemoryCache - Augmenting Local AI with Browser Data \u2192 https://future.mozilla.org/blog/introducing-memorycache/ MemoryCache, a Mozilla Innovation Project, is an early exploration project that augments an on-device, personal model with local files saved from the browser to reflect a more personalized and tailored experience through the lens of privacy and agency. Additional resources https://memorycache.ai/","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/memorycache-personal-ai","date":"2023-12-12 19:37 -05:00","tags":["ai","web","privacy","opensource"],"summary":"MemoryCache - Augmenting Local AI with Browser Data \u2192 https://future.mozilla.org/blog/introducing-memorycache/ MemoryCache, a Mozilla Innovation Project, is an early exploration project that augments...","keywords":["memorycache","local","browser","mozilla","project","from","reflect","more","personalized","experience"]},{"id":"bookmarks-mozilla-innovation-week-dec-2023","title":"Mozilla Innovation Week - Explore the Future of AI with Mozilla","content":"Mozilla Innovation Week - Explore the Future of AI with Mozilla \u2192 https://future.mozilla.org/innovation-week/ Mozilla\u2019s Innovation Week is a journey into the future of technology, where AI is not just a buzzword, but a reality we\u0027re actively shaping. Here, we\u0027re not just talking about innovation \u2013 we\u0027re living it through a series of AI-driven explorations. With that in mind, Innovation Week is more than a showcase. It\u0027s a platform for collaboration and inspiration. It\u0027s about bringing together ideas, people, and technology to pave the way for a more open and responsible future.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/mozilla-innovation-week-dec-2023","date":"2023-12-12 19:34 -05:00","tags":["ai","opensource","mozilla"],"summary":"Mozilla Innovation Week - Explore the Future of AI with Mozilla \u2192 https://future.mozilla.org/innovation-week/ Mozilla\u2019s Innovation Week is a journey into the future of technology, where AI is not...","keywords":["innovation","mozilla","week","future","more","technology","mind","showcase","platform","collaboration"]},{"id":"bookmarks-llamafile-llm-oneliners-tunney","title":"Bash One-Liners for LLMs","content":"Bash One-Liners for LLMs \u2192 https://justine.lol/oneliners/ I spent the last month working with Mozilla to launch an open source project called llamafile which is the new best way to run an LLM on your own computer. So far things have been going pretty smoothly. The project earned 5.6k stars on GitHub, 1073 upvotes on Hacker News, and received press coverage from Hackaday. Yesterday I cut a 0.3 release so let\u0027s see what it can do.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/llamafile-llm-oneliners-tunney","date":"2023-12-12 19:21 -05:00","tags":["llama","ai","llm","opensource"],"summary":"Bash One-Liners for LLMs \u2192 https://justine.lol/oneliners/ I spent the last month working with Mozilla to launch an open source project called llamafile which is the new best way to run an LLM on your...","keywords":["project","bash","pretty","smoothly","earned","stars","github","1073","upvotes","hacker"]},{"id":"bookmarks-mlx-mixtral-8x7b","title":"Mixtral 8x7B on Apple Silicon with MLX","content":"Mixtral 8x7B on Apple Silicon with MLX \u2192 https://github.com/ml-explore/mlx-examples/tree/main/mixtral Run the Mixtral1 8x7B mixture-of-experts (MoE) model in MLX on Apple silicon.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/mlx-mixtral-8x7b","date":"2023-12-12 19:19 -05:00","tags":["mlx","ai","frameworks"],"summary":"Mixtral 8x7B on Apple Silicon with MLX \u2192 https://github.com/ml-explore/mlx-examples/tree/main/mixtral Run the Mixtral1 8x7B mixture-of-experts (MoE) model in MLX on Apple silicon.","keywords":["8x7b","apple","silicon","mixtral","https://github","com/ml","explore/mlx","examples/tree/main/mixtral","mixtral1","mixture"]},{"id":"bookmarks-state-of-the-word-2023","title":"State of the Word 2023","content":"State of the Word 2023 \u2192 https://wordpress.org/state-of-the-word/ State of the Word is the annual keynote address delivered by the WordPress project\u2019s co-founder, Matt Mullenweg, celebrating the progress of the open source project and offering a glimpse into its future.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/state-of-the-word-2023","date":"2023-12-12 19:17 -05:00","tags":["wordpress","blogging","web","opensource","community"],"summary":"State of the Word 2023 \u2192 https://wordpress.org/state-of-the-word/ State of the Word is the annual keynote address delivered by the WordPress project\u2019s co-founder, Matt Mullenweg, celebrating the...","keywords":["state","word","offering","project","source","open","progress","celebrating","mullenweg","matt"]},{"id":"bookmarks-medprompt-promptbase-extending-power-prompting","title":"Steering at the Frontier: Extending the Power of Prompting ","content":"Steering at the Frontier: Extending the Power of Prompting \u2192 https://www.microsoft.com/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/ ...steering GPT-4 with a modified version of Medprompt achieves the highest score ever achieved on the complete MMLU. To achieve a new SoTA on MMLU, we extended Medprompt to Medprompt\u002B by adding a simpler prompting method and formulating a policy for deriving a final answer by integrating outputs from both the base Medprompt strategy and the simple prompts. The synthesis of a final answer is guided by a control strategy governed by GPT-4 and inferred confidences of candidate answers. While systematic prompt engineering can yield maximal performance, we continue to explore the out-of-the-box performance of frontier models with simple prompts. It\u2019s important to keep an eye on the native power of GPT-4 and how we can steer the model with zero- or few-shot prompting strategies.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/medprompt-promptbase-extending-power-prompting","date":"2023-12-12 19:14 -05:00","tags":["ai","promptengineering"],"summary":"Steering at the Frontier: Extending the Power of Prompting \u2192 https://www.microsoft.com/research/blog/steering-at-the-frontier-extending-the-power-of-prompting/ ...steering GPT-4 with a modified...","keywords":["power","prompting","medprompt","steering","prompts","answer","final","mmlu","simple","performance"]},{"id":"bookmarks-microsoft-promptbase","title":"promptbase","content":"promptbase \u2192 https://github.com/microsoft/promptbase promptbase is an evolving collection of resources, best practices, and example scripts for eliciting the best performance from foundation models like GPT-4.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/microsoft-promptbase","date":"2023-12-12 19:13 -05:00","tags":["ai","promptengineering"],"summary":"promptbase \u2192 https://github.com/microsoft/promptbase promptbase is an evolving collection of resources, best practices, and example scripts for eliciting the best performance from foundation models...","keywords":["promptbase","best","https://github","com/microsoft/promptbase","evolving","collection","resources","practices","example","scripts"]},{"id":"bookmarks-llm360-transparent-open-source-llm","title":"LLM360: Towards Fully Transparent Open-Source LLMs","content":"LLM360: Towards Fully Transparent Open-Source LLMs \u2192 https://arxiv.org/abs/2312.06550 The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at this https URL ). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future. Additional Resources https://www.llm360.ai/","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/llm360-transparent-open-source-llm","date":"2023-12-12 19:02 -05:00","tags":["ai","research"],"summary":"LLM360: Towards Fully Transparent Open-Source LLMs \u2192 https://arxiv.org/abs/2312.06550 The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides...","keywords":["llms","training","open","llm360","source","code","data","checkpoints","intermediate","choices"]},{"id":"bookmarks-microsoft-phi-2","title":"Phi-2: The surprising power of small language models","content":"Phi-2: The surprising power of small language models \u2192 https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/ We are now releasing Phi-2, a 2.7 billion-parameter language model that demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters. On complex benchmarks Phi-2 matches or outperforms models up to 25x larger, thanks to new innovations in model scaling and training data curation.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/microsoft-phi-2","date":"2023-12-12 19:00 -05:00","tags":["ai","slm","llm"],"summary":"Phi-2: The surprising power of small language models \u2192 https://www.microsoft.com/research/blog/phi-2-the-surprising-power-of-small-language-models/ We are now releasing Phi-2, a 2.7 billion-parameter...","keywords":["language","models","surprising","power","small","billion","model","less","parameters","complex"]},{"id":"bookmarks-answer-ai-rd-lab","title":"Answer.AI - A new old kind of R\u0026D lab","content":"Answer.AI - A new old kind of R\u0026amp;D lab \u2192 https://www.answer.ai/posts/2023-12-12-launch.html Jeremy Howard (founding CEO, previously co-founder of Kaggle and fast.ai) and Eric Ries (founding director, previously creator of Lean Startup and the Long-Term Stock Exchange) today launched Answer.AI, a new kind of AI R\u0026amp;D lab which creates practical end-user products based on foundational research breakthroughs. The creation of Answer.AI is supported by an investment of USD10m from Decibel VC. Answer.AI will be a fully-remote team of deep-tech generalists\u2014the world\u2019s very best, regardless of where they live, what school they went to, or any other meaningless surface feature.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/answer-ai-rd-lab","date":"2023-12-12 18:57 -05:00","tags":["ai","research","vc"],"summary":"Answer.AI - A new old kind of R\u0026amp;D lab \u2192 https://www.answer.ai/posts/2023-12-12-launch.html Jeremy Howard (founding CEO, previously co-founder of Kaggle and fast.ai) and Eric Ries (founding...","keywords":["answer","kind","r\u0026amp","they","previously","founding","decibel","from","usd10m","investment"]},{"id":"bookmarks-introducing-stable-lm-zephyr-3b","title":"Introducing Stable LM Zephyr 3B","content":"Introducing Stable LM Zephyr 3B \u2192 https://stability.ai/news/stablelm-zephyr-3b-stability-llm Stable LM Zephyr 3B is a 3 billion parameter Large Language Model (LLM), 60% smaller than 7B models, allowing accurate, and responsive output on a variety of devices without requiring high-end hardware.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/introducing-stable-lm-zephyr-3b","date":"2023-12-11 20:46 -05:00","tags":["llm","ai","edge"],"summary":"Introducing Stable LM Zephyr 3B \u2192 https://stability.ai/news/stablelm-zephyr-3b-stability-llm Stable LM Zephyr 3B is a 3 billion parameter Large Language Model (LLM), 60% smaller than 7B models,...","keywords":["zephyr","stable","introducing","requiring","devices","variety","output","responsive","accurate","allowing"]},{"id":"reshare-tumblr-still-working-fediverse-integration","title":"Tumblr\u2019s \u2018fediverse\u2019 integration is still being worked on","content":"Tumblr\u2019s \u2018fediverse\u2019 integration is still being worked on \u2192 https://techcrunch.com/2023/12/11/tumblrs-fediverse-integration-is-still-being-worked-on-says-owner-and-automattic-ceo-matt-mullenweg/ Despite delays, the plan to connect Tumblr\u2019s blogging site to the wider world of decentralized social media, also known as the \u201Cfediverse,\u201D is still on, it seems. ...Mullenweg explained that despite the re-org, which will see many Tumblr employees move to other projects at the end of the year, Automattic did switch someone over to Tumblr to work on the fediverse integration, which will continue in the new year. \u201CI remain a huge believer in open standards and user freedom, though I don\u2019t claim to have the truth on which particular standard is better or best, to serve our customers we will support everything we can in good faith to give users more freedom, choice, and avoid lock-in,\u201D [Matt Mullenweg] also said in his AMA. Mullunweg also noted that a larger effort to migrate Tumblr\u2019s half a billion blogs to WordPress on the backend is something he\u2019s also contemplating in the new year.","contentType":"reshare","url":"https://www.lqdev.me/responses/tumblr-still-working-fediverse-integration","date":"2023-12-11 20:29 -05:00","tags":["tumblr","socialmedia","fediverse"],"summary":"Tumblr\u2019s \u2018fediverse\u2019 integration is still being worked on \u2192 https://techcrunch.com/2023/12/11/tumblrs-fediverse-integration-is-still-being-worked-on-says-owner-and-automattic-ceo-matt-mullenweg/...","keywords":["also","tumblr\u2019s","year","integration","which","tumblr","mullenweg","despite","matt","freedom"]},{"id":"bookmarks-geometry-of-truth-data-explorer","title":"The Geometry of Truth: Dataexplorer","content":"The Geometry of Truth: Dataexplorer \u2192 https://saprmarks.github.io/geometry-of-truth/dataexplorer/ This page contains interactive charts for exploring how large language models represent truth. It accompanies the paper The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets by Samuel Marks and Max Tegmark. To produce these visualizations, we first extract LLaMA-13B representations of factual statements. These representations live in a 5120-dimensional space, far too high-dimensional for us to picture, so we use PCA to select the two directions of greatest variation for the data. This allows us to produce 2-dimensional pictures of 5120-dimensional data. See this footnote for more details. 1","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/geometry-of-truth-data-explorer","date":"2023-12-11 20:19 -05:00","tags":["ai","interpretability","llm"],"summary":"The Geometry of Truth: Dataexplorer \u2192 https://saprmarks.github.io/geometry-of-truth/dataexplorer/ This page contains interactive charts for exploring how large language models represent truth. It...","keywords":["dimensional","representations","geometry","produce","5120","large","language","truth:","data","greatest"]},{"id":"bookmarks-2023-state-of-ai-report","title":"State of AI Report - 2023","content":"State of AI Report - 2023 \u2192 https://docs.google.com/presentation/d/156WpBF_rGvf4Ecg19oM1fyR51g4FAmHV3Zs0WLukrLQ/edit?usp=sharing Key themes in the 2023 Report include: GPT-4 is the master of all it surveys (for now) , beating every other LLM on both classic benchmarks and exams designed to evaluate humans, validating the power of proprietary architectures and reinforcement learning from human feedback. Efforts are growing to try to clone or surpass proprietary performance , through smaller models, better datasets, and longer context. These could gain new urgency, amid concerns that human-generated data may only be able to sustain AI scaling trends for a few more years. LLMs and diffusion models continue to drive real-world breakthroughs , especially in the life sciences, with meaningful steps forward in both molecular biology and drug discovery. Compute is the new oil , with NVIDIA printing record earnings and startups wielding their GPUs as a competitive edge. As the US tightens its restrictions on trade restrictions on China and mobilizes its allies in the chip wars, NVIDIA, Intel, and AMD have started to sell export-control proof chips at scale. GenAI saves the VC world , as amid a slump in tech valuations, AI startups focused on generative AI applications (including video, text, and coding), raised over $18 billion from VC and corporate investors. The safety debate has exploded into the mainstream , prompting action from governments and regulators around the world. However, this flurry of activity conceals profound divisions within the AI community and a lack of concrete progress towards global governance, as governments around the world pursue conflicting approaches. Challenges mount in evaluating state of the art models , as standard LLMs often struggle with robustness. Considering the stakes, as \u201Cvibes-based\u201D approach isn\u2019t good enough. Additional resources State of AI Website","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/2023-state-of-ai-report","date":"2023-12-11 20:13 -05:00","tags":["ai"],"summary":"State of AI Report - 2023 \u2192 https://docs.google.com/presentation/d/156WpBF_rGvf4Ecg19oM1fyR51g4FAmHV3Zs0WLukrLQ/edit?usp=sharing Key themes in the 2023 Report include: GPT-4 is the master of all it...","keywords":["world","state","models","from","restrictions","startups","nvidia","llms","governments","amid"]},{"id":"bookmarks-onnxstream","title":"OnnxStream - Stable Diffusion XL 1.0 Base on a Raspberry Pi Zero 2","content":"OnnxStream - Stable Diffusion XL 1.0 Base on a Raspberry Pi Zero 2 \u2192 https://github.com/vitoplantamura/OnnxStream/ Generally major machine learning frameworks and libraries are focused on minimizing inference latency and/or maximizing throughput, all of which at the cost of RAM usage. So I decided to write a super small and hackable inference library specifically focused on minimizing memory consumption: OnnxStream. OnnxStream is based on the idea of decoupling the inference engine from the component responsible of providing the model weights, which is a class derived from WeightsProvider. A WeightsProvider specialization can implement any type of loading, caching and prefetching of the model parameters. For example a custom WeightsProvider can decide to download its data from an HTTP server directly, without loading or writing anything to disk (hence the word \u0026quot;Stream\u0026quot; in \u0026quot;OnnxStream\u0026quot;). Three default WeightsProviders are available: DiskNoCache, DiskPrefetch and Ram. OnnxStream can consume even 55x less memory than OnnxRuntime with only a 50% to 200% increase in latency (on CPU, with a good SSD, with reference to the SD 1.5\u0027s UNET - see the Performance section below).","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/onnxstream","date":"2023-12-11 20:11 -05:00","tags":["ai","onnx"],"summary":"OnnxStream - Stable Diffusion XL 1.0 Base on a Raspberry Pi Zero 2 \u2192 https://github.com/vitoplantamura/OnnxStream/ Generally major machine learning frameworks and libraries are focused on minimizing...","keywords":["onnxstream","weightsprovider","from","inference","loading","model","which","memory","latency","\u0026quot"]},{"id":"bookmarks-evaluating-llms-minefield","title":"Evaluating LLMs is a minefield","content":"Evaluating LLMs is a minefield \u2192 https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/ ...many things can go wrong when we are trying to evaluate LLMs\u2019 performance on a certain task or behavior in a certain scenario. It has big implications for reproducibility: both for research on LLMs and research that uses LLMs to answer a question in social science or any other field.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/evaluating-llms-minefield","date":"2023-12-11 20:08 -05:00","tags":["ai","llms","evaluation"],"summary":"Evaluating LLMs is a minefield \u2192 https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/ ...many things can go wrong when we are trying to evaluate LLMs\u2019 performance on a certain task...","keywords":["llms","research","certain","evaluating","social","question","answer","uses","both","reproducibility:"]},{"id":"bookmarks-vector-db-comparison-guide-2023","title":"Vector databases: A comparison and guide for 2023","content":"Vector databases: A comparison and guide for 2023 \u2192 https://benchmark.vectorview.ai/vectordbs.html Picking a vector database can be hard. Scalability, latency, costs, and even compliance hinge on this choice. For those navigating this terrain, I\u0027ve embarked on a journey to sieve through the noise and compare the leading vector databases of 2023. I\u2019ve included the following vector databases in the comparision: Pinecone, Weviate, Milvus, Qdrant, Chroma, Elasticsearch and PGvector. The data behind the comparision comes from ANN Benchmarks , the docs and internal benchmarks of each vector database and from digging in open source github repos.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/vector-db-comparison-guide-2023","date":"2023-12-11 20:05 -05:00","tags":["vector","databases"],"summary":"Vector databases: A comparison and guide for 2023 \u2192 https://benchmark.vectorview.ai/vectordbs.html Picking a vector database can be hard. Scalability, latency, costs, and even compliance hinge on...","keywords":["vector","2023","benchmarks","from","database","databases","i\u2019ve","included","following","comparision:"]},{"id":"bookmarks-memgpt-llm-operating-system","title":"MemGPT - Towards LLMs as Operating Systems","content":"MemGPT - Towards LLMs as Operating Systems \u2192 https://memgpt.ai/ Teach LLMs to manage their own memory for unbounded context! Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM\u0027s limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM\u0027s context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai. In MemGPT, a fixed-context LLM processor is augmented with a tiered memory system and a set of functions that allow it to manage its own memory. Main context is the (fixed-length) LLM input. MemGPT parses the LLM text ouputs at each processing cycle, and either yields control or executes a function call, which can be used to move data between main and external context. When the LLM generates a function call, it can request immediate return of execution to chain together functions. In the case of a yield, the LLM will not be run again until the next external event trigger (e.g. a user message or scheduled interrupt).","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/memgpt-llm-operating-system","date":"2023-12-11 19:58 -05:00","tags":["ai","os","llm"],"summary":"MemGPT - Towards LLMs as Operating Systems \u2192 https://memgpt.ai/ Teach LLMs to manage their own memory for unbounded context! Large language models (LLMs) have revolutionized AI, but are constrained...","keywords":["context","memory","memgpt","llms","limited","windows","manage","large","data","systems"]},{"id":"bookmarks-best-practices-rag-application-evaluation-databricks","title":"Best Practices for LLM Evaluation of RAG Applications","content":"Best Practices for LLM Evaluation of RAG Applications \u2192 https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG This blog represents the first in a series of investigations we\u2019re running at Databricks to provide learnings on LLM evaluation. Recently, the LLM community has been exploring the use of \u201CLLMs as a judge\u201D for automated evaluation with many using powerful LLMs such as GPT-4 to do the evaluation for their LLM outputs. Using the Few Shots prompt with GPT-4 didn\u2019t make an obvious difference in the consistency of results. Including few examples for GPT-3.5-turbo-16k significantly improves the consistency of the scores, and makes the result usable. ...evaluation results can\u2019t be transferred between use cases and we need to build use-case-specific benchmarks in order to properly evaluate how good a model can meet customer needs.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/best-practices-rag-application-evaluation-databricks","date":"2023-12-11 19:54 -05:00","tags":["ai","rag","evaluation","llm"],"summary":"Best Practices for LLM Evaluation of RAG Applications \u2192 https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG This blog represents the first in a series of investigations we\u2019re running at...","keywords":["evaluation","best","consistency","databricks","using","results","practices","examples","turbo","significantly"]},{"id":"bookmarks-mlflow-2-8-llm-as-judge-rag-evaluation","title":"MLflow 2.8 with LLM-as-a-judge metrics and Best Practices for LLM Evaluation of RAG Applications","content":"MLflow 2.8 with LLM-as-a-judge metrics and Best Practices for LLM Evaluation of RAG Applications \u2192 https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part?utm_source=twitter\u0026amp;utm_medium=organic-social LLM-as-a-judge is one promising tool in the suite of evaluation techniques necessary to measure the efficacy of LLM-based applications.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/mlflow-2-8-llm-as-judge-rag-evaluation","date":"2023-12-11 19:51 -05:00","tags":["mlflow","ai","llm","evaluation"],"summary":"MLflow 2.8 with LLM-as-a-judge metrics and Best Practices for LLM Evaluation of RAG Applications \u2192 https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluat...","keywords":["judge","evaluation","applications","mlflow","metrics","best","practices","measure","necessary","techniques"]},{"id":"bookmarks-metaclip-demistifying-clip-data","title":"Metadata-Curated Language-Image Pre-training (MetaCLIP) - Demystifying CLIP Data","content":"Metadata-Curated Language-Image Pre-training (MetaCLIP) - Demystifying CLIP Data \u2192 https://arxiv.org/abs/2309.16671 Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP\u0027s data by filtering with its model parameters. In this work, we intend to reveal CLIP\u0027s data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP\u0027s concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outperforms CLIP\u0027s data on multiple standard benchmarks. In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy, surpassing CLIP\u0027s 68.3% on ViT-B models. Scaling to 1B data, while maintaining the same training budget, attains 72.4%. Our observations hold across various model sizes, exemplified by ViT-H achieving 80.5%, without any bells-and-whistles. Curation code and training data distribution on metadata is made available at this https URL . Repository MetaCLIP","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/metaclip-demistifying-clip-data","date":"2023-12-11 19:41 -05:00","tags":["ai","opensource","clip"],"summary":"Metadata-Curated Language-Image Pre-training (MetaCLIP) - Demystifying CLIP Data \u2192 https://arxiv.org/abs/2309.16671 Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced...","keywords":["data","clip","training","metaclip","metadata","model","image","language","distribution","curated"]},{"id":"bookmarks-openagents-platform-language-agents","title":"OpenAgents: An Open Platform for Language Agents in the Wild","content":"OpenAgents: An Open Platform for Language Agents in the Wild \u2192 https://arxiv.org/abs/2310.10634 Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs). Current language agent frameworks aim to facilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level designs. We present OpenAgents, an open platform for using and hosting language agents in the wild of everyday life. OpenAgents includes three agents: (1) Data Agent for data analysis with Python/SQL and data tools; (2) Plugins Agent with 200\u002B daily API tools; (3) Web Agent for autonomous web browsing. OpenAgents enables general users to interact with agent functionalities through a web user interface optimized for swift responses and common failures while offering developers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate the challenges and opportunities, aspiring to set a foundation for future research and development of real-world language agents.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/openagents-platform-language-agents","date":"2023-12-11 19:39 -05:00","tags":["ai","llm","agents"],"summary":"OpenAgents: An Open Platform for Language Agents in the Wild \u2192 https://arxiv.org/abs/2310.10634 Language agents show potential in being capable of utilizing natural language for varied and intricate...","keywords":["language","agents","agent","data","openagents","open","platform","wild","tools","while"]},{"id":"bookmarks-foundational-model-transparency-index-stanford","title":"The Foundation Model Transparency Index","content":"The Foundation Model Transparency Index \u2192 https://crfm.stanford.edu/fmti/ A comprehensive assessment of the transparency of foundation model developers Context . Foundation models like GPT-4 and Llama 2 are used by millions of people. While the societal impact of these models is rising, transparency is on the decline. If this trend continues, foundation models could become just as opaque as social media platforms and other previous technologies, replicating their failure modes. Design . We introduce the Foundation Model Transparency Index to assess the transparency of foundation model developers. We design the Index around 100 transparency indicators, which codify transparency for foundation models, the resources required to build them, and their use in the AI supply chain. Execution . For the 2023 Index, we score 10 leading developers against our 100 indicators. This provides a snapshot of transparency across the AI ecosystem. All developers have significant room for improvement that we will aim to track in the future versions of the Index.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/foundational-model-transparency-index-stanford","date":"2023-12-11 19:37 -05:00","tags":["ai","opensource","transparency"],"summary":"The Foundation Model Transparency Index \u2192 https://crfm.stanford.edu/fmti/ A comprehensive assessment of the transparency of foundation model developers Context . Foundation models like GPT-4 and...","keywords":["transparency","foundation","index","model","developers","models","design","indicators","build","required"]},{"id":"bookmarks-new-kings-open-source-ai","title":"The New Kings of Open Source AI (Oct 2023 Recap)","content":"The New Kings of Open Source AI (Oct 2023 Recap) \u2192 https://www.latent.space/p/oct-2023 Mistral 7B, released at the tail end of Sept 2023, is both Apache 2.0 and smaller but better than Llama 2, and is now rumored to be raising $400m at $2.5b valuation from a16z.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/new-kings-open-source-ai","date":"2023-12-11 19:23 -05:00","tags":["ai","opensource","newsletter"],"summary":"The New Kings of Open Source AI (Oct 2023 Recap) \u2192 https://www.latent.space/p/oct-2023 Mistral 7B, released at the tail end of Sept 2023, is both Apache 2.0 and smaller but better than Llama 2, and...","keywords":["2023","kings","valuation","$400m","raising","rumored","llama","better","smaller","apache"]},{"id":"bookmarks-mixtral-of-experts","title":"Mixtral of experts","content":"Mixtral of experts \u2192 https://mistral.ai/news/mixtral-of-experts/ Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks. Mixtral has the following capabilities. It gracefully handles a context of 32k tokens. It handles English, French, Italian, German and Spanish. It shows strong performance in code generation. It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench. Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the \u201Cexperts\u201D) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/mixtral-of-experts","date":"2023-12-11 18:50 -05:00","tags":["ai","llm","opensource"],"summary":"Mixtral of experts \u2192 https://mistral.ai/news/mixtral-of-experts/ Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed...","keywords":["model","mixtral","parameters","token","experts","same","groups","output","benchmarks","outperforms"]},{"id":"bookmarks-future-rss-textcasting","title":"The Future of RSS is Textcasting","content":"The Future of RSS is Textcasting \u2192 https://kottke.org/23/11/the-future-of-rss-is-textcasting-1 Here\u2019s the philosophy: The goal is interop between social media apps and the features writers need. What we\u2019re doing: Moving documents between networked apps. We need a set of common features in order for it to work. The features are motivated by the needs of writers. Not by programmers or social media company execs. It\u2019s a proposal to build, using technologies we already have and understand very well, a very simple social media protocol that is completely agnostic about what editor you use to write your posts and what viewer you choose to read it. Writer/authors would have more control over styling, links, media enclosures, etc., and readers would have more control over how and where they consume it. It\u2019s decentralized social media, but without the need to peer through ActivityPub or anybody else\u2019s API and squeeze our toothpaste through its tubes. Additional resources Textcasting.org","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/future-rss-textcasting","date":"2023-12-11 18:40 -05:00","tags":["rss","openweb","internet","socialmedia"],"summary":"The Future of RSS is Textcasting \u2192 https://kottke.org/23/11/the-future-of-rss-is-textcasting-1 Here\u2019s the philosophy: The goal is interop between social media apps and the features writers need. What...","keywords":["media","social","textcasting","need","features","control","more","writers","apps","future"]},{"id":"bookmarks-omg-lol-internet-oasis-watson","title":"omg.lol: an oasis on the internet","content":"omg.lol: an oasis on the internet \u2192 https://blakewatson.com/journal/omg-lol-an-oasis-on-the-internet/ The main thing you are getting with omg.lol is one or more subdomains, which are referred to as addresses. Email forwarding : You get an email address, you@omg.lol, which you can forward to any email address. Web Page : This is your link-in-bio one-pager to do whatever you want with. By default this is where your main address (eg, you.omg.lol) points. It\u2019s the flagship feature of omg.lol. It comes with a markdown editor that has some fancy features baked into it. You get a selection of built-in themes but you also have the freedom to go wild with your own CSS. DNS : You have the ability to use your omg.lol subdomain however you wish by way of a friendly DNS panel. Now Page : This is a type of page you can use to let people know what\u2019s going on in your life. It\u2019s broader than a social media post but more immediately relevant than an about page. It comes with the same fancy markdown editor and you can optionally appear in omg.lol\u2019s Now Garden. Statuslog : This is a place to post statuses. It\u2019s really just a fun, silly alternative to other social media platforms but without follows and likes and such. These can cross-post to Mastodon if you want. Weblog : A full-fledge blogging platform. I\u2019m not aware of all its features but it\u2019s pretty powerful. It comes with fancy markdown support and has all the bloggy things you need like tags and RSS. A good example of a very custom blog on omg.lol is Apple Annie\u2019s Weblog. But it\u2019s worth noting you use it right out of the box without design customization if you want. Pastebin : It\u2019s just a pastebin for storing text snippets. Super simple and friendly like all of the omg.lol services. Pics : It\u2019s an image hosting service labeled as being \u201Csuper-beta\u201D as of the time of this writing. But it does what it says on the tin. You can host images there and they also show up on the some.pics image feed. PURLs : Persistent uniform resource locators. This is a URL redirection service. You get you.omg.lol/whatever and you.url.lol/whatever. You can use these the way you would use similar services and they come with a basic hit counter and way to preview the URL before following it. Switchboard : This is a powerful routing system that lets you point the variants of your address wherever you want, be it a destination on the omg.lol platform or an external website. Most omg.lol services have their own domain so you end up with a variety of options. Just as an example, you get a tilde address (ie, omg.lol/~you). Mine points to my tilde.club webpage. Keys : A place to store public keys\u2014SSH, PGP, etc. Proofs : A service for verifying ownership or control of a particular web property at a particular moment in time. For example, here is proof that I controlled blakewatson.com as of December 10, 2023. API access : Most, if not all, omg.lol services have an API you can use to interact with them. Total nerd freedom. \uD83E\uDD2F","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/omg-lol-internet-oasis-watson","date":"2023-12-11 08:51 -05:00","tags":["web","community"],"summary":"omg.lol: an oasis on the internet \u2192 https://blakewatson.com/journal/omg-lol-an-oasis-on-the-internet/ The main thing you are getting with omg.lol is one or more subdomains, which are referred to as...","keywords":["it\u2019s","address","page","services","want","markdown","fancy","email","service","example"]},{"id":"bookmarks-amazing-newsletters","title":"Amazing Newsletters","content":"Amazing Newsletters \u2192 https://amazingnewsletters.com/ Find the best newsletters to subscribe to!","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/amazing-newsletters","date":"2023-12-10 10:29 -05:00","tags":["newsletters"],"summary":"Amazing Newsletters \u2192 https://amazingnewsletters.com/ Find the best newsletters to subscribe to!","keywords":["newsletters","amazing","https://amazingnewsletters","com/","find","best","subscribe"]},{"id":"bookmarks-what-it-cost-to-say-yes-holiday","title":"To Everyone Who Asks For \u2018Just A Little\u2019 Of Your Time: Here\u2019s What It Costs To Say Yes","content":"To Everyone Who Asks For \u2018Just A Little\u2019 Of Your Time: Here\u2019s What It Costs To Say Yes \u2192 https://thoughtcatalog.com/ryan-holiday/2017/01/to-everyone-who-asks-for-just-a-little-of-your-time/ Makers...need to have large blocks of uninterrupted, unscheduled time to do what they do. To create and think. I keep a maker\u2019s schedule because I believe that anything else is anathema to deep work or creativity. Seneca writes that if all the geniuses in history were to get together, none would be able explain our baffling relationship with time. He says, No person would give up even an inch of their estate, and the slightest dispute with a neighbor can mean hell to pay; yet we easily let others encroach on our lives\u2014worse, we often pave the way for those who will take it over. No person hands out their money to passers-by, but to how many do each of us hand out our lives! We\u2019re tight-fisted with property and money, yet think too little of wasting time, the one thing about which we should all be the toughest misers. Time? Time is our most irreplaceable asset\u2014we cannot buy more of it. We cannot get a second of it back. We can only hope to waste as little as possible. Yet somehow we treat it as most renewable of all resources.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/what-it-cost-to-say-yes-holiday","date":"2023-12-09 10:08 -05:00","tags":["productivity","timemanagement"],"summary":"To Everyone Who Asks For \u2018Just A Little\u2019 Of Your Time: Here\u2019s What It Costs To Say Yes \u2192 https://thoughtcatalog.com/ryan-holiday/2017/01/to-everyone-who-asks-for-just-a-little-of-your-time/...","keywords":["little","money","cannot","everyone","asks","person","neighbor","many","passers","hands"]},{"id":"bookmarks-practical-ways-increase-product-velocity","title":"Practical Ways To Increase Product Velocity","content":"Practical Ways To Increase Product Velocity \u2192 https://staysaasy.com/management/2023/12/07/accelerating-product-velocity.html Remove Dependencies Create a culture that favors begging forgiveness (and reversing decisions quickly) rather than asking permission. Invest in infrastructure such as progressive / cancellable rollouts. Use asynchronous written docs to get people aligned (\u201Ccomment in this doc by Friday if you disagree with the plan\u201D) rather than meetings (\u201Cwe\u2019ll get approval at the next weekly review meeting\u201D). Demand Clear Narratives Unclear thinking is a reliable cause of slowness, and gets revealed under a microscope. Bonus points for documenting plans in writing. One of the largest advantages of a strong writing culture is that it forces much clearer narratives than meetings, powerpoint, or five Slack threads spread over 8 business days. Get Your Deployment and Incident Metrics In Shape No matter what your job function is, part of your role is ensuring that your engineering team has enough time to get their vital metrics in order. Especially if you\u2019re a product leader, it\u2019s essential that you resist the temptation to push relentlessly for more features and give your engineering counterparts the room to get fit. Find Trusted Engineering Guides ...it\u2019s especially important to build a strong relationship with all of your engineering partners, and especially these trusted guides.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/practical-ways-increase-product-velocity","date":"2023-12-09 10:03 -05:00","tags":["product","productivity"],"summary":"Practical Ways To Increase Product Velocity \u2192 https://staysaasy.com/management/2023/12/07/accelerating-product-velocity.html Remove Dependencies Create a culture that favors begging forgiveness (and...","keywords":["engineering","especially","product","narratives","culture","metrics","it\u2019s","strong","meetings","guides"]},{"id":"bookmarks-microsoft-satclip","title":"SatCLIP - A Global, General-Purpose Geographic Location Encoder","content":"SatCLIP - A Global, General-Purpose Geographic Location Encoder \u2192 https://github.com/microsoft/satclip SatCLIP trains location and image encoders via contrastive learning, by matching images to their corresponding locations. This is analogous to the CLIP approach, which matches images to their corresponding text. Through this process, the location encoder learns characteristics of a location, as represented by satellite imagery. For more details, check out our paper .","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/microsoft-satclip","date":"2023-12-08 07:40 -05:00","tags":["ai","opensource"],"summary":"SatCLIP - A Global, General-Purpose Geographic Location Encoder \u2192 https://github.com/microsoft/satclip SatCLIP trains location and image encoders via contrastive learning, by matching images to their...","keywords":["location","satclip","images","encoder","corresponding","purpose","details","more","imagery","satellite"]},{"id":"bookmarks-dos-deck","title":"DOS_deck","content":"DOS_deck \u2192 https://dosdeck.com/ DOS_deck is built upon the foundation of JS-DOS, which, in turn, relies on DOSBox. Together, they breathe new life into MS-DOS games by bringing them to your browser. However, there\u0027s a twist. Games from that era were designed for keyboard and mouse input, without established standards for interaction or control patterns. Here at DOS_deck, a tremendous effort was put into creating a seamless experience, enabling you to effortlessly navigate and play these games, ideally with the comfort of a controller in hand.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/dos-deck","date":"2023-12-08 07:36 -05:00","tags":["gaming","dos","retro","web"],"summary":"DOS_deck \u2192 https://dosdeck.com/ DOS_deck is built upon the foundation of JS-DOS, which, in turn, relies on DOSBox. Together, they breathe new life into MS-DOS games by bringing them to your browser....","keywords":["deck","games","established","standards","interaction","control","patterns","here","tremendous","input"]},{"id":"bookmarks-app-defaults-catalog","title":"Awesome App Defaults","content":"Awesome App Defaults \u2192 https://defaults.rknight.me/ Aggregated list of App Defaults blog posts inspired by Hemispheric Views 097 - Duel of the Defaults!","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/app-defaults-catalog","date":"2023-12-06 21:53 -05:00","tags":["community","blogging","apps"],"summary":"Awesome App Defaults \u2192 https://defaults.rknight.me/ Aggregated list of App Defaults blog posts inspired by Hemispheric Views 097 - Duel of the Defaults!","keywords":["defaults","awesome","https://defaults","rknight","aggregated","list","blog","posts","inspired","hemispheric"]},{"id":"bookmarks-rss-club","title":"RSS Club","content":"RSS Club \u2192 https://daverupert.com/rss-club/ RSS Club is a collection of blogs (personal and otherwise) committed to providing RSS-only content. It\u2019s like a newsletter delivered to your feed reader in order to celebrate the medium of RSS and breakaway from social media.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/rss-club","date":"2023-12-06 21:37 -05:00","tags":["rss","community"],"summary":"RSS Club \u2192 https://daverupert.com/rss-club/ RSS Club is a collection of blogs (personal and otherwise) committed to providing RSS-only content. It\u2019s like a newsletter delivered to your feed reader in...","keywords":["club","from","breakaway","medium","celebrate","order","reader","feed","delivered","newsletter"]},{"id":"notes-rss-only-blog","title":"RSS Only Blog","content":"I stumbled upon theunderground blog by Chris McLeod which is an interesting twist on RSS Club . Currently theunderground has a page that serves as the homepage and the RSS feed itself. I wonder whether it\u0027s possible to take it even further by using XSL(T) to style the RSS feed as shown in this Style Your RSS Feed post by Darek Kay . By styling the RSS feed, it can serve as the homepage as well. Using redirection, when people land on your domain, they would instead be presented with the styled RSS feed.","contentType":"notes","url":"https://www.lqdev.me/notes/rss-only-blog","date":"2023-12-06 21:20 -05:00","tags":["rss","blogs","social"],"summary":"I stumbled upon theunderground blog by Chris McLeod which is an interesting twist on RSS Club . Currently theunderground has a page that serves as the homepage and the RSS feed itself. I wonder...","keywords":["feed","using","theunderground","style","homepage","stumbled","shown","post","darek","styling"]},{"id":"bookmarks-claude-long-context-prompting","title":"Long context prompting for Claude 2.1","content":"Long context prompting for Claude 2.1 \u2192 https://www.anthropic.com/index/claude-2-1-prompting Claude 2.1 recalls information very well across its 200,000 token context window However, the model can be reluctant to answer questions based on an individual sentence in a document, especially if that sentence has been injected or is out of place A minor prompting edit removes this reluctance and results in excellent performance on these tasks What can users do if Claude is reluctant to respond to a long context retrieval question? We\u2019ve found that a minor prompt update produces very different outcomes in cases where Claude is capable of giving an answer, but is hesitant to do so. When running the same evaluation internally, adding just one sentence to the prompt resulted in near complete fidelity throughout Claude 2.1\u2019s 200K context window We achieved significantly better results on the same evaluation by adding the sentence \u201CHere is the most relevant sentence in the context:\u201D to the start of Claude\u2019s response. This was enough to raise Claude 2.1\u2019s score from 27% to 98% on the original evaluation.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/claude-long-context-prompting","date":"2023-12-06 21:07 -05:00","tags":["prompts","ai","llm"],"summary":"Long context prompting for Claude 2.1 \u2192 https://www.anthropic.com/index/claude-2-1-prompting Claude 2.1 recalls information very well across its 200,000 token context window However, the model can be...","keywords":["claude","sentence","context","prompting","evaluation","same","results","adding","minor","prompt"]},{"id":"star-blogging-where-its-at-again-mcleod","title":"Blogging is where it\u0027s at, again","content":"Blogging is where it\u0026#39;s at, again \u2192 https://chrismcleod.dev/blog/blogging-is-where-its-at-again/ the blog is the \u201Cnatural form\u201D of posting on the web: a site of your own, that you control[1] and set your own rules on content and discussion; where you can post whatever you like without worrying about \u201CThe Algorithm\u201D For better or for worse, social media opened up the web to a lot more people for a number of reasons...But deep down I feel having your own site is better. For the web, and for you: the writer and the reader. ...stumbling into such a trove of active blogs has enthused me about blogging as a medium again. It\u2019s sparked a thought that through a combination of increased blogging activity, declining platforms, and increasing adoption of open standards to glue everything together, that maybe \u2014 just maybe \u2014 we can swing the web back towards the blog again. Agree with many of the points. Also, TIL you could subscribe to OPML feeds.","contentType":"star","url":"https://www.lqdev.me/responses/blogging-where-its-at-again-mcleod","date":"2023-12-06 21:02 -05:00","tags":["blogging","socialmedia","rss","community"],"summary":"Blogging is where it\u0026#39;s at, again \u2192 https://chrismcleod.dev/blog/blogging-is-where-its-at-again/ the blog is the \u201Cnatural form\u201D of posting on the web: a site of your own, that you control[1] and...","keywords":["blogging","again","blog","maybe","site","better","declining","activity","increased","combination"]},{"id":"bookmarks-introducing-gemini-google","title":"Introducing Gemini","content":"Introducing Gemini \u2192 https://blog.google/technology/ai/google-gemini-ai/ Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research. It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, operate across and combine different types of information including text, code, audio, image and video. Gemini is also our most flexible model yet \u2014 able to efficiently run on everything from data centers to mobile devices. Its state-of-the-art capabilities will significantly enhance the way developers and enterprise customers build and scale with AI. We\u2019ve optimized Gemini 1.0, our first version, for three different sizes: Gemini Ultra \u2014 our largest and most capable model for highly complex tasks. Gemini Pro \u2014 our best model for scaling across a wide range of tasks. Gemini Nano \u2014 our most efficient model for on-device tasks. We designed Gemini to be natively multimodal, pre-trained from the start on different modalities. Then we fine-tuned it with additional multimodal data to further refine its effectiveness. This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models \u2014 and its capabilities are state of the art in nearly every domain. Our first version of Gemini can understand, explain and generate high-quality code in the world\u2019s most popular programming languages, like Python, Java, C\u002B\u002B, and Go. On TPUs, Gemini runs significantly faster than earlier, smaller and less-capable models. Starting today, Bard will use a fine-tuned version of Gemini Pro for more advanced reasoning, planning, understanding and more. We\u2019re also bringing Gemini to Pixel. Pixel 8 Pro is the first smartphone engineered to run Gemini Nano, which is powering new features like Summarize in the Recorder app and rolling out in Smart Reply in Gboard, starting with WhatsApp \u2014 with more messaging apps coming next year. Starting on December 13, developers and enterprise customers can access Gemini Pro via the Gemini API in Google AI Studio or Google Cloud Vertex AI.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/introducing-gemini-google","date":"2023-12-06 20:57 -05:00","tags":["ai","lmm"],"summary":"Introducing Gemini \u2192 https://blog.google/technology/ai/google-gemini-ai/ Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research....","keywords":["gemini","model","multimodal","from","google","different","more","tasks","version","understand"]},{"id":"bookmarks-introducing-llamafile","title":"Introducing llamafile","content":"Introducing llamafile \u2192 https://hacks.mozilla.org/2023/11/introducing-llamafile/ Today we\u2019re announcing the first release of llamafile and inviting the open source community to participate in this new project. llamafile lets you turn large language model (LLM) weights into executables. We achieved all this by combining two projects that we love: llama.cpp (a leading open source LLM chatbot framework) with Cosmopolitan Libc (an open source project that enables C programs to be compiled and run on a large number of platforms and architectures). It also required solving several interesting and juicy problems along the way, such as adding GPU and dlopen() support to Cosmopolitan;","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/introducing-llamafile","date":"2023-12-05 21:54 -05:00","tags":["ai","llm","opensource"],"summary":"Introducing llamafile \u2192 https://hacks.mozilla.org/2023/11/introducing-llamafile/ Today we\u2019re announcing the first release of llamafile and inviting the open source community to participate in this...","keywords":["llamafile","open","source","cosmopolitan","large","project","introducing","chatbot","framework","libc"]},{"id":"bookmarks-peertube-v6-released","title":"PeerTube v6 is out","content":"PeerTube v6 is out \u2192 https://framablog.org/2023/11/28/peertube-v6-is-out-and-powered-by-your-ideas/ The sixth major version is being released today and we are very proud ! Protect your videos with passwords ! Video storyboard : preview what\u2019s coming ! Upload a new version of your video ! Get chapters in your videos ! Stress tests, performance and config recommandations \u2026and there\u2019s always more !","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/peertube-v6-released","date":"2023-12-05 21:51 -05:00","tags":["fediverse","peertube","video","opensource"],"summary":"PeerTube v6 is out \u2192 https://framablog.org/2023/11/28/peertube-v6-is-out-and-powered-by-your-ideas/ The sixth major version is being released today and we are very proud ! Protect your videos with...","keywords":["video","version","videos","there\u2019s","\u2026and","recommandations","config","performance","tests","stress"]},{"id":"reshare-take-and-publish-notes-nielsen","title":"How I Take and Publish Notes","content":"How I Take and Publish Notes \u2192 https://blog.jim-nielsen.com/2023/how-i-take-and-publish-notes/ 99% of the time, this is how my note-taking process goes: I\u2019m catching up on my RSS feed (on my phone in the Reeder app) I read something that strikes me as interesting, novel, or insightful. I copy/paste it as an blockquote into a new, plain-text note in iA writer. I copy/paste the link of the article into iA writer. I finish reading the article and copy/paste anything else in the article that strikes me. I add my own comments in the note as they pop into my head. I move on to the next article in my RSS feed. Repeat. Kind of meta but somewhat similar process for me. To publish the different content found on my response feed , I: Go through articles on my RSS feed (NewsBlur on both desktop and mobile). Copy URL and block quotes from article and paste them somewhere. When I have time like now, I create a post like this one, usually in VS Code. If I don\u0027t have time though, I\u0027ve been experimenting with using a messaging app like Element and E-mail as a read-it-later service. At minimum, I create a message with the link and send it to myself for later review. Later on when I have time, I create the post with additional comments and content from the article. (Optional) Add some of my own comments. Publish the notes. Repeat.","contentType":"reshare","url":"https://www.lqdev.me/responses/take-and-publish-notes-nielsen","date":"2023-12-05 21:42 -05:00","tags":["blogging","notetaking"],"summary":"How I Take and Publish Notes \u2192 https://blog.jim-nielsen.com/2023/how-i-take-and-publish-notes/ 99% of the time, this is how my note-taking process goes: I\u2019m catching up on my RSS feed (on my phone in...","keywords":["article","publish","feed","create","copy/paste","note","comments","later","link","writer"]},{"id":"bookmarks-ai-mass-spying-schneier","title":"AI and Mass Spying","content":"AI and Mass Spying \u2192 https://www.schneier.com/blog/archives/2023/12/ai-and-mass-spying.html Surveillance facilitates social control, and spying will only make this worse. Governments around the world already use mass surveillance; they will engage in mass spying as well. Mass surveillance ushered in the era of personalized advertisements; mass spying will supercharge that industry...The tech monopolies that are currently keeping us all under constant surveillance won\u2019t be able to resist collecting and using all of that data. We could limit this capability. We could prohibit mass spying. We could pass strong data-privacy rules. But we haven\u2019t done anything to limit mass surveillance. Why would spying be any different?","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/ai-mass-spying-schneier","date":"2023-12-05 21:12 -05:00","tags":["ai","privacy","internet","surveillance"],"summary":"AI and Mass Spying \u2192 https://www.schneier.com/blog/archives/2023/12/ai-and-mass-spying.html Surveillance facilitates social control, and spying will only make this worse. Governments around the world...","keywords":["mass","spying","surveillance","data","limit","privacy","keeping","under","constant","won\u2019t"]},{"id":"bookmarks-ai-alliance","title":"AI Alliance Launches","content":"AI Alliance Launches \u2192 https://thealliance.ai/news AI Alliance Launches as an International Community of Leading Technology Developers, Researchers, and Adopters Collaborating Together to Advance Open, Safe, Responsible AI","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/ai-alliance","date":"2023-12-05 21:03 -05:00","tags":["ai","opensource"],"summary":"AI Alliance Launches \u2192 https://thealliance.ai/news AI Alliance Launches as an International Community of Leading Technology Developers, Researchers, and Adopters Collaborating Together to Advance...","keywords":["alliance","launches","open","advance","together","collaborating","adopters","researchers","developers","technology"]},{"id":"notes-ayaneo-slide","title":"Ayaneo Slide","content":"I NEED ONE! This gives me Sidekick / Sony Mylo vibes. The only reason I wouldn\u0027t get it is battery life. I don\u0027t care for the gaming capabilities. Throw in an ARM-based processor like the Snapdragon X Elite for all-day battery life, give me USB-C ports to attach a dongle to, and I\u0027m sold.","contentType":"notes","url":"https://www.lqdev.me/notes/ayaneo-slide","date":"2023-11-20 17:34 -05:00","tags":["pc","handheld"],"summary":"I NEED ONE! This gives me Sidekick / Sony Mylo vibes. The only reason I wouldn\u0027t get it is battery life. I don\u0027t care for the gaming capabilities. Throw in an ARM-based processor like the Snapdragon...","keywords":["battery","life","need","attach","ports","give","elite","snapdragon","processor","based"]},{"id":"notes-net8-website-update","title":"Website upgraded to .NET 8","content":"Obligatory yearly website update post. When I built this website, I started on .NET 5. 3 years later and although my code has changed to add new features, I haven\u0027t had to make any code changes to account for a new version of .NET. I love that I can get all the benefits of the new version without having to do any rewrites.","contentType":"notes","url":"https://www.lqdev.me/notes/net8-website-update","date":"2023-11-16 01:27 -05:00","tags":["dotnet","indieweb"],"summary":"Obligatory yearly website update post. When I built this website, I started on .NET 5. 3 years later and although my code has changed to add new features, I haven\u0027t had to make any code changes to...","keywords":["code","website","version","benefits","love","account","changes","haven","features","changed"]},{"id":"notes-covey-timeless-habits","title":"Timeless habits","content":"Following up on the theme of cycles and looking back at things from yesteryear explored in my post AI abundance after scarcity cycles , I came across another example. While looking through some of my old books, I came across Stephen Covey\u0027s \u0026quot;7 Habits of Highly Effective People\u0026quot;. I remember reading it over ten years ago. At the time, the habits and themes explored in the book had an impact on me. However, since that time I haven\u0027t looked at the book again. Today, I cracked it open and skimmed through the foreword. In it, Stephen talks about common human challenges such as: Fear and insecurity - This could mean a fear of the future, like losing your job. Lack of life balance - Put in the context of being always online, our lives continue to get more complex and demanding. Given recent talking points of AI coming for our jobs and the accelerated rate of technological innovation, you could pluck these challenges from the time they were written and they\u0027d still be relevant today. You could also place them 500 or 1000 years ago and they\u0027d also be relevant which made me think of the Lindy Effect . In short, it states that the longer a technology or idea continues to live, the higher the likelihood it will continue to exist into the future. I found it funny though that while flipping through the book and reading about these timeless challenges and principles, I came across the opposite of that. One of my bookmarks was a Borders membership card. If you don\u0027t know or remember what Borders was, in a gist, it was one of the large bookstores which succumbed to disruption from e-commerce (i.e. Amazon)","contentType":"notes","url":"https://www.lqdev.me/notes/covey-timeless-habits","date":"2023-11-12 14:05 -05:00","tags":["books","reading","sevenhabits"],"summary":"Following up on the theme of cycles and looking back at things from yesteryear explored in my post AI abundance after scarcity cycles , I came across another example. While looking through some of my...","keywords":["across","from","book","challenges","came","they","years","future","which","habits"]},{"id":"reshare-tyler-ted-talk","title":"Where this flower blooms","content":"Where this flower blooms \u2192 https://twitter.com/studiosipu/status/1723582194432794638/video/1 Preach \uD83D\uDE4C","contentType":"reshare","url":"https://www.lqdev.me/responses/tyler-ted-talk","date":"2023-11-12 11:53 -05:00","tags":["tedtalk","festival"],"summary":"Where this flower blooms \u2192 https://twitter.com/studiosipu/status/1723582194432794638/video/1 Preach \uD83D\uDE4C","keywords":["flower","blooms","https://twitter","com/studiosipu/status/1723582194432794638/video/1","preach"]},{"id":"notes-ai-abundance-scarcity-cycle-repeats-rss-enclosure-use-case","title":"AI abundance after scarcity cycles","content":"While writing the post RSS for new event notifications I mentioned how the flow of getting new event notifications through RSS might work: When a new event is added to the calendar, the RSS feed can be updated. The channel property has the link to the shared calendar you can subscribe to and the individual events have .ics files embedded in the enclosure . I actually didn\u0027t know if that was possible until I looked at the RSS enclosure syntax . Based on that page, as long as you provide the URL to the .ics file, length of the file, and the MIME type (text/calendar), you can link to your events inside enclosures. So it is possible. That\u0027s not the point of this post though. When I was looking at the references, I came across this post talking about the use case for RSS enclosures . I don\u0027t know when this post was published but given the problems mentioned with accessing high quality video, I assume it was in the days when the internet was on dial-up or DSL speeds. At that time, downloading webpages or e-mail was slow. Forget about downloading multimedia. As that post mentions, with enclosures, you could schedule your RSS client to download content like videos during off-peak hours so any content you\u0027d want to access would download overnight and be available to you in the morning. About 30 years later, in many places, even with a connection that has 1MB download speeds, downloading a podcast that\u0027s 100MB or a video that\u0027s 2GB is relatively fast. It might take a while, but compared to 30 years ago, it\u0027s \u0026quot;instant\u0026quot;. So at this point, it\u0027s not really technical challenge as much as it is an access challenge. Low bandwith speeds forced people to get creative in how they utilized their resources. The internet was not the only time this happened. About 30 years ago or so when cell phones were just becoming mainstream they too suffered from scarcity. Cell phones and plans were very expensive. You had limited minutes and when SMS became a thing, those too were limited. If you wanted to have hour long conversations with friends and loved ones, you\u0027d have to wait until off-peak hours so you could use your \u0026quot;free\u0026quot; minutes. Fast-forward 30 years later, cell phones and cell phone plans are relatively affordable. When you purchase these plans, you get access to unlimited minutes, messages, and data. So again, the technical limitations aren\u0027t as prevalent. It\u0027s mainly a matter of access and whether your cell phone provider offers good coverage in your area. This brings me to my point. We\u0027re seeing this cycle again with AI. Large Language Models today are expensive to run and I\u0027m not overindexing on the financial perspective. You pay per call, access to these models can be throttled at times, there\u0027s only so many GPUs you can get access to in a data center to run them, you pay the latency penalty by going through the network as well as the cost of inferencing, and you also have token limits. About a year into the release of ChatGPT, access to these models has been expanded, the models have gotten more capable ( GPT-4V and GPT4 with 128k tokens), it\u0027s become cheaper to run these models . At the same time, computing is becoming more efficient and smaller \u0026quot;open-source\u0026quot; models are becoming more capable. Today, we have to be creative with how we utilize our resources, but we\u0027re only a year in. If we extrapolate how these cycles play out though, in 30 years we\u0027ll forget how challenging it was to use and access early AI technologies and AI will become more abundant like the internet, cell phones, and other technologies before it.","contentType":"notes","url":"https://www.lqdev.me/notes/ai-abundance-scarcity-cycle-repeats-rss-enclosure-use-case","date":"2023-11-12 10:37 -05:00","tags":["ai","internet","history","rss","openai","llm","gpt","opensource"],"summary":"While writing the post RSS for new event notifications I mentioned how the flow of getting new event notifications through RSS might work: When a new event is added to the calendar, the RSS feed can...","keywords":["access","models","cell","post","years","more","phones","speeds","internet","downloading"]},{"id":"notes-rss-community-calendars-new-events","title":"RSS for new event notifications","content":"After my post on Shared calendars and RSS I kept thinking about what else you can get out of sharing a calendar using RSS. What I didn\u0027t see immediately is the main use case of RSS, though I did mention it as a drawback. The main drawbacks I see are that I can\u0027t get notifications or see it in my calendar. Actually, with RSS, I can get notifications when a new event has been added to the calendar. With iCal for example, when a new event is added to the calendar, it\u0027ll automatically be populated in my calendar client. However, I won\u0027t get a notification that a new event has been added. Conversely, with RSS, I won\u0027t see it in my calendar client but I will get a notification in my RSS reader. When paired together though you could use RSS to get notified when a new event is added to the calendar and also be able to view it in your calendar client since it\u0027ll automatically be populated (assuming you\u0027re already subscribed to the shared calendar). That means managing two feeds, but the more I think about it, the RSS format makes some sense. The shared calendar could be the source of truth. When a new event is added to the calendar, the RSS feed can be updated. The channel property has the link to the shared calendar you can subscribe to and the individual events have .ics files embedded in the enclosure . As a consumer, the new event notification will then appear in my RSS reader.","contentType":"notes","url":"https://www.lqdev.me/notes/rss-community-calendars-new-events","date":"2023-11-12 09:52 -05:00","tags":["rss","calendars","protocols","community"],"summary":"After my post on Shared calendars and RSS I kept thinking about what else you can get out of sharing a calendar using RSS. What I didn\u0027t see immediately is the main use case of RSS, though I did...","keywords":["calendar","event","added","shared","client","notification","reader","main","though","populated"]},{"id":"bookmarks-you-need-feeds","title":"You Need Feeds","content":"You Need Feeds \u2192 https://www.youneedfeeds.com/ YOU NEED FEEDS. A web feed is a special listing of the latest content from your favourite site. News, music, video and more - whatever is new, web feeds will show you. What\u0027s more, you can combine your favourite feeds using a feed reader application - and suddenly the whole web comes to you. You don\u0027t have to do the work of staying on top any more. You can now visit a single site, or use a single app, and see everything that\u0027s new and interesting. You choose the content. You\u0027re in control.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/you-need-feeds","date":"2023-11-11 17:49 -05:00","tags":["rss","feeds","guides"],"summary":"You Need Feeds \u2192 https://www.youneedfeeds.com/ YOU NEED FEEDS. A web feed is a special listing of the latest content from your favourite site. News, music, video and more - whatever is new, web feeds...","keywords":["feeds","more","single","site","favourite","content","need","feed","listing","interesting"]},{"id":"notes-rss-community-calendars","title":"Shared calendars and RSS","content":"I just came across a new use for RSS - shared calendars. Well, not exactly sharing calendars in RSS format. While I have seen calendars in RSS format as I\u0027ll mention later in this post, I think there\u0027s enough downsides that make this impractical. However, for discoverability, there\u0027s some RSS patterns that shared calendars could benefit from. While reading a recent post on Doc Searls\u0027 blog it got me thinking about using public calendars for sharing community events. By leveraging iCal or CalDav, this could be a good alternative to current solutions like Facebook Events or Meetup. It could also be a great way to foster interoperability. There\u0027s still the discoverability problem, but platforms like Meetup and Facebook Events could enable subscribing to shared public calendars. This sent me down a search to see whether others were using shared calendars, similar to The Big Calendar . It was nice to see that Penn State has shared calendars for their various sports. That shouldn\u0027t be surprising. As a university, they already share their academic calendars. The part that I found surprising though was that RSS was one of the options. Downloading the RSS feed enabled me to import it into my feed reader and view it there. I\u0027d prefer if instead of downloading the RSS feed as a file, they just provided the link so I could copy and paste it into my feed reader. However, it\u0027s still nice that just like podcasts and YouTube feeds, I can now also see Penn State athletic events in my feed. I\u0027m not sure what the benefit of subscribing to this feed via RSS as opposed to the calendar itself has. The main drawbacks I see are that I can\u0027t get notifications or see it in my calendar. I also don\u0027t know if changes and updates to events also propagate to the RSS feed. That\u0027s why I\u0027d prefer to just subscribe to the calendar directly. Going back to the broader topic of shared calendars and discoverability mentioned in the DataPress proposal for community calendars, you could imagine a solution similar to how websites can make their RSS feeds discoverable . As a calendar and community maintainer, you could post the link to your shared calendars on your website. As a community member, you\u0027d just need to point your calendar client to the respective website\u0027s domain (i.e. gopsusports.com) and your client would automatically import the shared calendars. This is how shared calendars work today. The difference is, you don\u0027t need the exact link to the calendar, you just need the website domain. The calendar client is what handles the discovery and import process. One important thing to highlight, doing something like this isn\u0027t exclusive to communities. You could imagine businesses being able to implement similar practices to communicate their business hours and availability. Platforms like Yelp or Google could then just import the public calendar for that business and display the business hours on their platform. To schedule your next oil change, imagine looking at your auto service shop\u0027s shared calendar and sending a calendar invite for one of their open slots. This is no different to solutions like Microsoft Bookings . The difference is, instead of using these platforms, you would do it from your preferred calendar client of choice. You could also see this working for individuals as well. If you need to coordinate something with other people, instead of the constant back and forth to find availability, you can just share your public calendar and they can find a time that works for them. Anyway, the point of this longish post is that by building on simple components like calendars, iCal, CalDAV, and RSS, communities, businesses, and individuals could communicate and collaborate more effectively without being constrained to any single platform.","contentType":"notes","url":"https://www.lqdev.me/notes/rss-community-calendars","date":"2023-11-11 13:34 -05:00","tags":["rss","calendars","protocols","community"],"summary":"I just came across a new use for RSS - shared calendars. Well, not exactly sharing calendars in RSS format. While I have seen calendars in RSS format as I\u0027ll mention later in this post, I think...","keywords":["calendars","calendar","shared","feed","also","events","need","client","import","community"]},{"id":"notes-apple-m3-scary-fast","title":"Apple M3 Announced","content":"Apple unveiled their M3 family of chips and new devices. First off, The Verge always does a great job covering and condensing these events into a short video. My initial impressions of the anouncement, this is why Apple wins. I\u0027m excited by the Snapdragon announcement last week. However, as I mentioned in my post , \u0026quot;the biggest miss from Qualcomm and Windows partners today was not having a lineup of devices ready for purchase\u0026quot;. That wasn\u0027t the case for Apple. Not only did they announce their new chips but they also announced devices which will be available as late as the end of November. I understand Apple owns the entire process so they can align their chip and device announcements. However, Qualcomm brought out many partners on stage. Unless I missed it, I don\u0027t remember seeing any PC announcements that would be ready for purchase within the next 2-3 months.","contentType":"notes","url":"https://www.lqdev.me/notes/apple-m3-scary-fast","date":"2023-10-31 09:00 -05:00","tags":["arm","apple","m3"],"summary":"Apple unveiled their M3 family of chips and new devices. First off, The Verge always does a great job covering and condensing these events into a short video. My initial impressions of the...","keywords":["apple","they","devices","partners","qualcomm","however","announcements","ready","chips","unless"]},{"id":"notes-snapdragon-summit-2023-addendum","title":"Quick Thoughts Snapdragon Summit 2023 Addendum","content":"Yesterday I wrote a post with some quick thoughts on why I\u0027m excited for the new Snapdragon X Elite set of chips. There\u0027s something I missed which came to mind this morning. If your Snapdragon X Elite powered computer is running Windows, and it also has calling and messaging capabilities like a phone, does that mean that you\u0027re now the proud owner of a \u0026quot;Windows Phone\u0026quot;? As a fan of Windows Phone, that sounds interesting. Now you might say, \u0026quot;Windows doesn\u0027t have all my apps\u0026quot;. Fair point but with Windows Subsystem for Android (WSA), maybe the app gap isn\u0027t much of a concern. More importantly, as I mentioned in the post, if the apps no longer become the main mode of interaction and instead are relegated to serve as background services to the AI interface, does it even matter? In the post, I also mentioned how I\u0027d prefer my mobile computer runs Linux (and no, I don\u0027t mean AOSP). Even that is not a strict limitation, since I can run Windows Subsystem for Linux (WSL) inside Windows. So technically I can still have Linux on my mobile computer even if it\u0027s running Windows. Let\u0027s see if 2024 becomes the Year of the Windows (or Linux) Phone.","contentType":"notes","url":"https://www.lqdev.me/notes/snapdragon-summit-2023-addendum","date":"2023-10-25 19:26 -05:00","tags":["windows","qualcomm","windowsphone","mobilecomputing","arm","hardware","5g","ai"],"summary":"Yesterday I wrote a post with some quick thoughts on why I\u0027m excited for the new Snapdragon X Elite set of chips. There\u0027s something I missed which came to mind this morning. If your Snapdragon X...","keywords":["windows","linux","even","post","phone","computer","mobile","mean","also","\u0026quot"]},{"id":"posts-quick-thoughts-snapdragon-summit-2023","title":"Quick thoughts about Snapdragon Summit 2023","content":"This started as a note where I was planning on posting a bullet point or two with my thoughts but kept on writing so I decided to turn it into a blog post. Here\u0027s a few of my thoughts after the Snapdragon Summit 2023 keynote. Today was the start of Snapdragon Summit 2023 and all I can say is, **WOW**. CNET has a [14 minute clip](https://www.youtube.com/watch?v=K7Q5iYHvgwo) that summarizes all the announcements. [![Qualcomm\u2019s Snapdragon AI Event: Everything Revealed in 14 Minutes](http://img.youtube.com/vi/K7Q5iYHvgwo/0.jpg)](https://www.youtube.com/watch?v=K7Q5iYHvgwo \u0022Qualcomm\u2019s Snapdragon AI Event: Everything Revealed in 14 Minutes\u0022) I was cautiously optimistic about what would be shown today but Qualcomm delivered. The main thing that I\u0027m excited about are their Snapdragon X Elite Oryon-based CPUs. As someone who does all of their computing on a laptop, I\u0027d love to have a PC that I can take with me on the go. Now you might think that\u0027s what your phone is for, and for many people that is definitely the case. However, I only use my phone for calls and messages. I find it much more enjoyable to have a larger screen and keyboard. Keeping my preferences in mind, being able to connect to your PC on the go is nothing new either. I can just use the hotspot on my phone or tether over USB in order to connect my PC to the internet. Also, SIM card slots in PCs have been a thing for a while and products like the [Surface Pro 9 with 5G](https://www.microsoft.com/d/surface-pro-9/93vkd8np4fvk?activetab=pivot:overviewtab) provide an even more integrated experience. However, in order to truly make this experience mobile, I need to be untethered. Just like a mobile phone, I need my device battery to last all-day while on the go so I don\u0027t have to carry my charger. To do that, either you need a device with a large battery or a low-power chip. Larger battery immediately takes the mobile out of mobile computing. I recently carried around my 16\u0022 work laptop for a while walking in the city and that was no fun. So then low-power chip it is. Problem solved. Not quite, because in order for it to be low-power, performance needs to be throttled. So for the longest time, it\u0027s been a constant struggle between power-efficiency and battery life. Apple was the first to solve this problem with their M1 line of chips. However, that meant this innovation was confined to the Apple ecosystem. It\u0027s not until now with the Oryon CPUs that Windows (and hopefully soon Linux too) can tap into these types of computing platforms that promise to deliver true mobile computing. This now opens up a world of possibilities when we think about mobile computing. Our phones keep getting [larger in size](https://www.theverge.com/23913658/best-small-phone-dead-iphone-mini-z-flip-pixel-8) that at this point it\u0027s harder to carry them in your pocket or use them with one hand. We\u0027ve seen a surge in interest in handheld gaming devices like the [SteamDeck](https://www.steamdeck.com/) which are full PCs. Their problem is they\u0027re using chips that are not power efficient, especially because of their requirements for gaming. What happens if you replace the current chips in those devices with Snapdragon X Elite? Now that it\u0027s high-performance and power efficient, does your handheld gaming device now become your handheld computing device? And if we\u0027re in handheld computing device territory, now all of a sudden it\u0027s bumping up against what smartphones do today. So at that point, do we need both a smartphone and a handheld computing device? If I was able to handle calls and messages from my PC then I wouldn\u0027t need a phone and I\u0027d much rather just carry a single handheld computing device. Considering that smartphones are general computing devices that are serverely locked down and restricted, I\u0027d much rather go for the general computing type of device like a PC because I have more freedom over what I can do with my PC. More importantly, because it\u0027s a full PC, it means when I need to dock down for serious work with a keyboard and multiple monitors, I can plug a USB-C Hub and connect all of my peripherals to the device. This was something projects like [Continuum](https://learn.microsoft.com/windows-hardware/design/device-experiences/continuum-phone) tried to achieve but were unsuccessful doing for various reasons. Because now your PC is now also your mobile device, you take a big problem with these type of experiences which is reconciling mobile and desktop UX patterns. What about apps? Not exactly sure what this looks like in the near-term but considering devices running Oryon CPUs will take some time to get to market, by using the AI capabilities you can think of a future where AI is the interface and the apps become background services for interactive with external systems. So no need for specific apps because you can express intent and complete tasks using natural language. What about form factors? The steamdeck might not be what folks are looking for when it comes to a mobile device. Foldables are seeing an uptick in adoption. Despite it\u0027s lack of commercial success, the Surface Duo was an excellent foldable form factor that was compact but functional for productivity. I think the Surface Duo was misunderstood because it was put into the box of mobile phone. As a mobile phone it didn\u0027t work as well. But as a mobile computer, it struck the right form factor that enabled you to be productive on the go while still being portable. Needed more screen real estate? Surface Neo or Courier would\u0027ve been nice devices to have. Let\u0027s go 3 years into the future. What happens when virtual and augmented reality headsets all of a sudden become as small as the [Ray-Ban Meta Smartglasses](https://www.ray-ban.com/usa/ray-ban-meta-smart-glasses)? Do we even need handheld PCs at that point if all we need is for the headset to display the screens we\u0027re looking at. Bring in IoT. If the items around us are also \u0022smart\u0022 devices, all of a sudden your environment becomes your compute platform. Bring AI into the mix and your AI assistant has access to your environment. I\u0027d say the biggest miss from Qualcomm and Windows partners today was not having a lineup of devices ready for purchase. That will come with time. Hopefully not too long after these announcements. I\u0027m sure that Apple which has an announcement coming up on October 30th will have a response. The most important thing though is that now there\u0027s competition and as we enter this new era of AI and TRUE mobile computing, it\u0027ll lead to innovation. The next things that need to happen is operating systems like Windows need to ensure their existing applications work on these platforms and Linux needs to enable support for these chipsets. I\u0027m so excited for the future of mobile computing and can\u0027t wait to be able to carry a device running Linux with always-on connectivity and all-day battery life in the form factor of a Surface Duo. An addendum to this post can be found [here](/notes/snapdragon-summit-2023-addendum).","contentType":"posts","url":"https://www.lqdev.me/posts/quick-thoughts-snapdragon-summit-2023","date":"2023-10-24 23:14 -05:00","tags":["arm","qualcomm","hardware","mobilecomputing","5g","ai"],"summary":"This started as a note where I was planning on posting a bullet point or two with my thoughts but kept on writing so I decided to turn it into a blog post. Here\u0027s a few of my thoughts after the...","keywords":["mobile","computing","device","need","phone","devices","handheld","power","https://www","snapdragon"]},{"id":"star-texts-joins-automattic","title":"Texts Joins Automattic","content":"Texts Joins Automattic \u2192 https://ma.tt/2023/10/texts-joins-automattic/ Using an all-in-one messaging app is a real game-changer for productivity and keeping up with things. This is obviously a tricky area to navigate, as in the past the networks have blocked third-party clients, but I think with the current anti-trust and regulatory environments this is actually something the big networks will appreciate: it maintains the same security as their clients, opens them up in a way consumers will love and is very user-centric, and because we\u2019re committed to supporting all their features it can actually increase engagement and usage of their platforms. I can relate to the feeling of wanting to have one inbox expressed in the video. Coincidentally, I\u0027ve been playing with Delta Chat and by building on top of e-mail, some of the issues with the siloed platforms are alleviated. Also, e-mail isn\u0027t dead and despite some of its shortcomings, it\u0027s still broadly used to sign up and sign into platforms.","contentType":"star","url":"https://www.lqdev.me/responses/texts-joins-automattic","date":"2023-10-24 17:13 -05:00","tags":["messaging","interop","decentralization","opensource"],"summary":"Texts Joins Automattic \u2192 https://ma.tt/2023/10/texts-joins-automattic/ Using an all-in-one messaging app is a real game-changer for productivity and keeping up with things. This is obviously a tricky...","keywords":["platforms","sign","clients","mail","actually","networks","joins","committed","features","wanting"]},{"id":"reshare-automattic-acquires-texts-messaging","title":"Automattic is acquiring Texts and betting big on the future of messaging","content":"Automattic is acquiring Texts and betting big on the future of messaging \u2192 https://www.theverge.com/2023/10/24/23928685/automattic-texts-acquisition-universal-messaging I\u0027m really liking the recent acquisitions from Automattic. I\u0027m just starting to use Day One and really enjoy it. Pocket Casts is a fantastic podcast app, though I prefer to use AntennaPod. WordPress is also starting to make it easy to plug into the Fediverse using your blog. I\u0027m excited for Texts and what that might offer in the current siloed messaging landscape. Automattic, the company that runs WordPress.com, Tumblr, Pocket Casts, and a number of other popular web properties, just made a different kind of acquisition: it\u2019s buying Texts, a universal messaging app, for $50 million. Texts is an app for all your messaging apps. You can use it to log in to WhatsApp, Instagram, LinkedIn, Signal, iMessage, and more and see and respond to all your messages in one place. ...Mullenweg says he\u2019s bullish on solutions like Matrix, which offers a decentralized and open-source messaging network, and other up-and-coming standards for messaging. He\u2019s already thinking about how Texts might gently nudge people toward more open protocols over time. Mullenweg and Automattic see a big future for messaging, as more online interaction shifts away from public-first social networks and toward things like group chats. Hardly anyone has figured out how to build a meaningful and sustainable business from chat, but Mullenweg thinks it\u2019s possible. And he thinks it starts with making your messaging a little less messy.","contentType":"reshare","url":"https://www.lqdev.me/responses/automattic-acquires-texts-messaging","date":"2023-10-24 09:16 -05:00","tags":["messaging","interop","decentralization","opensource"],"summary":"Automattic is acquiring Texts and betting big on the future of messaging \u2192 https://www.theverge.com/2023/10/24/23928685/automattic-texts-acquisition-universal-messaging I\u0027m really liking the recent...","keywords":["messaging","texts","automattic","mullenweg","more","from","wordpress","open","casts","pocket"]},{"id":"posts-generate-mecard-qr-code-fsharp","title":"Generate QR Codes for MeCard Data in F#","content":"## Introduction Today I learned about MeCard file formats for representing contact information while reading the post [Sharing contacts using QR Code without requiring a network connection](https://andregarzia.com/2023/10/sharing-contacts-using-qr-code.html) by Andre Garzia. Currently I host my [vCard](../vcard.vcf) on my site in a very similar way Andre describes in his post. I\u0027ve also encoded it in [QR Code format](http://cdn.lqdev.tech/files/images/contact/qr-vcard.svg) to make it easy to import and share. When I saw the MeCard format, I immediately liked how simple it was so I decided to see how difficult it\u0027d be to write some code to generate a QR code for my own contact data. ## What is MeCard? According to Wikipedia, MeCard is \u0022...a data file similar to vCard...It is largely compatible with most QR-readers for smartphones. It is an easy way to share a contact with the most used fields. Usually, devices can recognize it and treat it like a contact ready to import.\u0022 An example might look like: \u0060MECARD:N:Doe,John;TEL:13035551212;EMAIL:john.doe@example.com;;\u0060 ## Encoding text as a QR code using F# I\u0027d written a [snippet](/resources/snippets/qr-code-generator) a few years ago to help me generate the QR Codes for all accounts in [my contact page](/contact). The snippet is a F# Interactive script which uses [Net.Codecrete.QrCodeGenerator NuGet package](https://www.nuget.org/packages/Net.Codecrete.QrCodeGenerator) to encode text as a QR code and saves it to an SVG file. The core logic consists of the following code: \u0060\u0060\u0060fsharp let createQrCode (savePath:string) (target:string) = let qr = QrCode.EncodeText(target,QrCode.Ecc.High) let svgString = qr.ToSvgString(4) File.WriteAllText(savePath,svgString, Encoding.UTF8) \u0060\u0060\u0060 In this case \u0060target\u0060 is the text I want to encode and \u0060savePath\u0060 is the path of the SVG file I\u0027m saving the generated QR code in. Given MeCard with the following content: \u0060\u0060\u0060csharp MECARD:N:Doe,John;TEL:13035551212;EMAIL:john.doe@example.com;; \u0060\u0060\u0060 The QR Code generated might look as follows: ![MECARD QR CODE](http://cdn.lqdev.tech/files/images/mecard.svg) Scanning the QR Code with a smartphone would then immediately recognize it as a contact and prompt you to save the contact information to your device. ## Conclusion Since I already had the code snippet to do this available, creating a MeCard was straightforward. I\u0027ve since added my [MeCard](/mecard.txt) alongside by vCard. You can check out the corresponding [QR code](http://cdn.lqdev.tech/files/images/contact/qr-mecard.svg) in my [contact](/contact) page.","contentType":"posts","url":"https://www.lqdev.me/posts/generate-mecard-qr-code-fsharp","date":"2023-10-23 19:24 -05:00","tags":["dotnet","fsharp","qrcode"],"summary":"## Introduction Today I learned about MeCard file formats for representing contact information while reading the post [Sharing contacts using QR Code without requiring a network...","keywords":["code","mecard","contact","file","vcard","text","lqdev","using","http://cdn","snippet"]},{"id":"reshare-posse-better-way-to-post-social-networks","title":"POSSE: A Better Way to Post on Social Networks","content":"POSSE: A Better Way to Post on Social Networks \u2192 https://www.theverge.com/2023/10/23/23928550/posse-posting-activitypub-standard-twitter-tumblr-mastodon The platform era is ending. Rather than build new Twitters and Facebooks, we can create a stuff-posting system that works better for everybody. In a POSSE world, everybody owns a domain name, and everybody has a blog. (I\u2019m defining \u201Cblog\u201D pretty loosely here \u2014 just as a place on the internet where you post your stuff and others consume it.) But there are some big challenges to the idea...The most immediate question...is simply how to build a POSSE system that works. POSSE\u2019s problems start at the very beginning: it requires owning your own website, which means buying a domain and worrying about DNS records and figuring out web hosts, and by now, you\u2019ve already lost the vast majority of people who would rather just type a username and password into some free Meta platform...Even those willing and able to do the technical work can struggle to make POSSE work. When I ask Doctorow why he believed in POSSE, he describes the tension every poster feels on the modern internet. \u201CI wanted to find a way to stand up a new platform in this moment,\u201D he says, \u201Cwhere, with few exceptions, everyone gets their news and does their reading through the silos that then hold you to ransom. And I wanted to use those silos to bring in readers and to attract and engage with an audience, but I didn\u2019t want to become beholden to them.\u201D The best of both worlds is currently a lot of work. But the poster\u2019s paradise might not be so far away.","contentType":"reshare","url":"https://www.lqdev.me/responses/posse-better-way-to-post-social-networks","date":"2023-10-23 18:55 -05:00","tags":["indieweb","posse","socialnetworks","internet","community"],"summary":"POSSE: A Better Way to Post on Social Networks \u2192 https://www.theverge.com/2023/10/23/23928550/posse-posting-activitypub-standard-twitter-tumblr-mastodon The platform era is ending. Rather than build...","keywords":["posse","everybody","platform","works","better","post","stuff","internet","wanted","posting"]},{"id":"bookmarks-understanding-deep-learning-book","title":"Understanding Deep Learning","content":"Understanding Deep Learning \u2192 https://udlbook.github.io/udlbook/ The title of this book is \u201CUnderstanding Deep Learning\u201D to distinguish it from volumes that cover coding and other practical aspects. This text is primarily about the ideas that underlie deep learning. The first part of the book introduces deep learning models and discusses how to train them, measure their performance, and improve this performance. The next part considers architectures that are specialized to images, text, and graph data. These chapters require only introductory linear algebra, calculus, and probability and should be accessible to any second-year undergraduate in a quantitative discipline. Subsequent parts of the book tackle generative models and reinforcement learning. These chapters require more knowledge of probability and calculus and target more advanced students.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/understanding-deep-learning-book","date":"2023-10-14 21:35 -05:00","tags":["book","deeplearning","ai"],"summary":"Understanding Deep Learning \u2192 https://udlbook.github.io/udlbook/ The title of this book is \u201CUnderstanding Deep Learning\u201D to distinguish it from volumes that cover coding and other practical aspects....","keywords":["deep","learning","book","require","probability","calculus","part","models","text","performance"]},{"id":"bookmarks-dawn-lmms-early-explorations-gtp4v","title":"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)","content":"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) \u2192 https://arxiv.org/abs/2309.17421 Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V\u0027s capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V\u0027s unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V\u0027s unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI\u0027s innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution: this https URL","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/dawn-lmms-early-explorations-gtp4v","date":"2023-10-14 21:32 -05:00","tags":["ai","lmm"],"summary":"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) \u2192 https://arxiv.org/abs/2309.17421 Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as...","keywords":["multimodal","understanding","samples","model","models","lmms","visual","capabilities","genericity","ways"]},{"id":"bookmarks-chain-of-verification-prompting-technique","title":"Chain-of-Verification Reduces Hallucination in Large Language Models","content":"Chain-of-Verification Reduces Hallucination in Large Language Models \u2192 https://arxiv.org/abs/2309.11495 Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (COVE) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show COVE decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/chain-of-verification-prompting-technique","date":"2023-10-14 21:09 -05:00","tags":["ai","llm","promptengineering"],"summary":"Chain-of-Verification Reduces Hallucination in Large Language Models \u2192 https://arxiv.org/abs/2309.11495 Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved...","keywords":["verification","questions","language","models","chain","response","from","cove","answers","generation"]},{"id":"reshare-activitypub-plugin-available-hosted-wordpress","title":"Engage a Wider Audience With ActivityPub on WordPress.com","content":"Engage a Wider Audience With ActivityPub on WordPress.com \u2192 https://wordpress.com/blog/2023/10/11/activitypub/ Exciting times are here for all WordPress.com users! The revolutionary ActivityPub feature is now available across all WordPress.com plans, unlocking a world of engagement and interaction for your blog. Your blogs can now be part of the rapidly expanding fediverse, which enables you to connect with a broader audience and attract more followers. I can\u0027t believe I missed these news but so exciting!","contentType":"reshare","url":"https://www.lqdev.me/responses/activitypub-plugin-available-hosted-wordpress","date":"2023-10-14 20:57 -05:00","tags":["activitypub","mastodon","wordpress","blogging"],"summary":"Engage a Wider Audience With ActivityPub on WordPress.com \u2192 https://wordpress.com/blog/2023/10/11/activitypub/ Exciting times are here for all WordPress.com users! The revolutionary ActivityPub...","keywords":["wordpress","audience","activitypub","exciting","engage","part","rapidly","expanding","fediverse","which"]},{"id":"bookmarks-multimodality-large-multimodal-models-lmm-huyen","title":"Multimodality and Large Multimodal Models (LMMs)","content":"Multimodality and Large Multimodal Models (LMMs) \u2192 https://huyenchip.com/2023/10/10/multimodal.html This post covers multimodal systems in general, including LMMs. It consists of 3 parts. Part 1 covers the context for multimodality, including why multimodal, different data modalities, and types of multimodal tasks. Part 2 discusses the fundamentals of a multimodal system, using the examples of CLIP, which lays the foundation for many future multimodal systems, and Flamingo, whose impressive performance gave rise to LMMs. Part 3 discusses some active research areas for LMMs, including generating multimodal outputs and adapters for more efficient multimodal training, covering newer multimodal systems such as BLIP-2, LLaVA, LLaMA-Adapter V2, LAVIN, etc.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/multimodality-large-multimodal-models-lmm-huyen","date":"2023-10-14 18:51 -05:00","tags":["ai","lmm"],"summary":"Multimodality and Large Multimodal Models (LMMs) \u2192 https://huyenchip.com/2023/10/10/multimodal.html This post covers multimodal systems in general, including LMMs. It consists of 3 parts. Part 1...","keywords":["multimodal","lmms","including","systems","part","multimodality","covers","discusses","performance","impressive"]},{"id":"bookmarks-hf-text-embedding-inference","title":"HuggingFace: Text Embeddings Inference","content":"HuggingFace: Text Embeddings Inference \u2192 https://github.com/huggingface/text-embeddings-inference A blazing fast inference solution for text embeddings models.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/hf-text-embedding-inference","date":"2023-10-14 12:54 -05:00","tags":["ai","embeddings","text","inference","ml"],"summary":"HuggingFace: Text Embeddings Inference \u2192 https://github.com/huggingface/text-embeddings-inference A blazing fast inference solution for text embeddings models.","keywords":["embeddings","inference","text","huggingface:","https://github","com/huggingface/text","blazing","fast","solution","models"]},{"id":"reshare-radiooooo","title":"Radiooooo, The Musical Time Machine","content":"Radiooooo, The Musical Time Machine \u2192 https://radiooooo.com/ Radiooooo is a project born in 2013, dreamt up by a little family of friends, both djs and music lovers, who decided to share their record collections and the fruit of many years of research, for all to enjoy. \u00AB Sharing and discovering \u00BB , \u00AB curiosity and pleasure \u00BB these are the foundations of this musical time machine. Radiooooo is a collaborative website, whose goal is to open each and everyone\u2019s horizons through culture and beauty.","contentType":"reshare","url":"https://www.lqdev.me/responses/radiooooo","date":"2023-10-06 09:33 -05:00","tags":["music","radio"],"summary":"Radiooooo, The Musical Time Machine \u2192 https://radiooooo.com/ Radiooooo is a project born in 2013, dreamt up by a little family of friends, both djs and music lovers, who decided to share their record...","keywords":["radiooooo","musical","machine","research","enjoy","sharing","discovering","curiosity","pleasure","foundations"]},{"id":"bookmarks-scaffolded-llms-natural-language-computers","title":"Scaffolded LLMs as natural language computers","content":"Scaffolded LLMs as natural language computers \u2192 https://www.beren.io/2023-04-11-Scaffolded-LLMs-natural-language-computers/","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/scaffolded-llms-natural-language-computers","date":"2023-09-28 21:00 -05:00","tags":["ai","llm","cpu"],"summary":"Scaffolded LLMs as natural language computers \u2192 https://www.beren.io/2023-04-11-Scaffolded-LLMs-natural-language-computers/","keywords":["scaffolded","llms","natural","language","computers","https://www","beren","io/2023","computers/"]},{"id":"bookmarks-creating-first-confidential-gpus","title":"Creating the First Confidential GPUs","content":"Creating the First Confidential GPUs \u2192 https://queue.acm.org/detail.cfm?id=3623391 The team at NVIDIA brings confidentiality and integrity to user code and data for accelerated computing.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/creating-first-confidential-gpus","date":"2023-09-28 20:59 -05:00","tags":["ai","security","privacy","gpu"],"summary":"Creating the First Confidential GPUs \u2192 https://queue.acm.org/detail.cfm?id=3623391 The team at NVIDIA brings confidentiality and integrity to user code and data for accelerated computing.","keywords":["creating","confidential","gpus","https://queue","org/detail","id=3623391","team","nvidia","brings","confidentiality"]},{"id":"bookmarks-ai-attack-surface-map-v1","title":"The AI Attack Surface Map v1.0","content":"The AI Attack Surface Map v1.0 \u2192 https://danielmiessler.com/p/the-ai-attack-surface-map-v1-0/","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/ai-attack-surface-map-v1","date":"2023-09-28 20:45 -05:00","tags":["ai","security","cybersecurity"],"summary":"The AI Attack Surface Map v1.0 \u2192 https://danielmiessler.com/p/the-ai-attack-surface-map-v1-0/","keywords":["attack","surface","https://danielmiessler","com/p/the"]},{"id":"bookmarks-mistral-ai","title":"Mistral 7B Model","content":"Mistral 7B Model \u2192 https://mistral.ai/ Mistral-7B-v0.1 is a small, yet powerful model adaptable to many use-cases. Mistral 7B is better than Llama 2 13B on all benchmarks, has natural coding abilities, and 8k sequence length. It\u2019s released under Apache 2.0 licence. We made it easy to deploy on any cloud, and of course on your gaming GPU.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/mistral-ai","date":"2023-09-28 10:16 -05:00","tags":["ai","llm","opensource"],"summary":"Mistral 7B Model \u2192 https://mistral.ai/ Mistral-7B-v0.1 is a small, yet powerful model adaptable to many use-cases. Mistral 7B is better than Llama 2 13B on all benchmarks, has natural coding...","keywords":["mistral","model","cloud","deploy","easy","made","licence","apache","under","released"]},{"id":"bookmarks-meta-announces-ai-experiences-fb-whatsapp-ig","title":"Meta announces AI experiences in Facebook, Instagram, WhatsApp","content":"Meta announces AI experiences in Facebook, Instagram, WhatsApp \u2192 https://about.fb.com/news/2023/09/introducing-ai-powered-assistants-characters-and-creative-tools/ We\u2019re starting to roll out AI stickers across our apps, and soon you\u2019ll be able to edit your images or even co-create them with friends on Instagram using our new AI editing tools, restyle and backdrop. We\u2019re introducing Meta AI in beta, an advanced conversational assistant that\u2019s available on WhatsApp, Messenger, and Instagram, and is coming to Ray-Ban Meta smart glasses and Quest 3. Meta AI can give you real-time information and generate photorealistic images from your text prompts in seconds to share with friends. (Available in the US only) We\u2019re also launching 28 more AIs in beta, with unique interests and personalities. Some are played by cultural icons and influencers, including Snoop Dogg, Tom Brady, Kendall Jenner, and Naomi Osaka. Over time, we\u2019re making AIs for businesses and creators available, and releasing our AI studio for people and developers to build their own AIs. These new AI experiences also come with a new set of challenges for our industry. We\u2019re rolling out our new AIs slowly and have built in safeguards.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/meta-announces-ai-experiences-fb-whatsapp-ig","date":"2023-09-28 10:12 -05:00","tags":["meta","ai","facebook"],"summary":"Meta announces AI experiences in Facebook, Instagram, WhatsApp \u2192 https://about.fb.com/news/2023/09/introducing-ai-powered-assistants-characters-and-creative-tools/ We\u2019re starting to roll out AI...","keywords":["we\u2019re","meta","available","instagram","also","beta","friends","images","whatsapp","experiences"]},{"id":"bookmarks-raspberry-pi-5","title":"Introducing Raspberry Pi 5","content":"Introducing Raspberry Pi 5 \u2192 https://www.raspberrypi.com/products/raspberry-pi-5/ The everything computer. Optimised. With 2\u20133\u00D7 the speed of the previous generation, and featuring silicon designed in\u2011house for the best possible performance, we\u2019ve redefined the Raspberry Pi experience. Coming October 2023","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/raspberry-pi-5","date":"2023-09-28 10:10 -05:00","tags":["raspberrypi","selfhosting"],"summary":"Introducing Raspberry Pi 5 \u2192 https://www.raspberrypi.com/products/raspberry-pi-5/ The everything computer. Optimised. With 2\u20133\u00D7 the speed of the previous generation, and featuring silicon designed...","keywords":["raspberry","introducing","coming","experience","redefined","we\u2019ve","performance","possible","best","in\u2011house"]},{"id":"bookmarks-carton-ml-models","title":"Carton - Run any ML model from any programming language","content":"Carton - Run any ML model from any programming language \u2192 https://carton.run/ Carton makes it easy to run any ML model from any programming language.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/carton-ml-models","date":"2023-09-28 10:03 -05:00","tags":["ai","ml","mlops"],"summary":"Carton - Run any ML model from any programming language \u2192 https://carton.run/ Carton makes it easy to run any ML model from any programming language.","keywords":["carton","model","from","programming","language","https://carton","run/","makes","easy"]},{"id":"reshare-boox-palma-eink-phone","title":"BOOX Palma","content":"BOOX Palma \u2192 https://shop.boox.com/products/palma I really like my Boox e-reader but having a more pocketable device would be amazing. It\u0027s unforunate you can\u0027t also use it for handwritten notes but at this size it makes sense.","contentType":"reshare","url":"https://www.lqdev.me/responses/boox-palma-eink-phone","date":"2023-09-26 21:22 -05:00","tags":["eink","ereader","mp3","minimalism","simplicity"],"summary":"BOOX Palma \u2192 https://shop.boox.com/products/palma I really like my Boox e-reader but having a more pocketable device would be amazing. It\u0027s unforunate you can\u0027t also use it for handwritten notes but...","keywords":["boox","size","notes","handwritten","also","unforunate","amazing","device","pocketable","more"]},{"id":"bookmarks-flowiseai-visual-llm-flow","title":"FlowiseAI - Drag and drop for LLM flows","content":"FlowiseAI - Drag and drop for LLM flows \u2192 https://github.com/FlowiseAI/Flowise","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/flowiseai-visual-llm-flow","date":"2023-09-25 20:54 -05:00","tags":["ai","tools"],"summary":"FlowiseAI - Drag and drop for LLM flows \u2192 https://github.com/FlowiseAI/Flowise","keywords":["flowiseai","drag","drop","flows","https://github","com/flowiseai/flowise"]},{"id":"bookmarks-why-open-source-ai-will-win-varun","title":"Why Open Source AI Will Win","content":"Why Open Source AI Will Win \u2192 https://varunshenoy.substack.com/p/why-open-source-ai-will-win","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/why-open-source-ai-will-win-varun","date":"2023-09-25 20:48 -05:00","tags":["opensource","ai"],"summary":"Why Open Source AI Will Win \u2192 https://varunshenoy.substack.com/p/why-open-source-ai-will-win","keywords":["open","source","https://varunshenoy","substack","com/p/why"]},{"id":"bookmarks-distilling-step-x-step-outperforming-large-models-small-data","title":"Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes","content":"Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes \u2192 https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html In \u201CDistilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes\u201D , presented at ACL2023, we set out to tackle this trade-off between model size and training data collection cost. We introduce distilling step-by-step, a new simple mechanism that allows us to train smaller task-specific models with much less training data than required by standard fine-tuning or distillation approaches that outperform few-shot prompted LLMs\u2019 performance. We demonstrate that the distilling step-by-step mechanism enables a 770M parameter T5 model to outperform the few-shot prompted 540B PaLM model using only 80% of examples in a benchmark dataset, which demonstrates a more than 700x model size reduction with much less training data required by standard approaches.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/distilling-step-x-step-outperforming-large-models-small-data","date":"2023-09-25 20:45 -05:00","tags":["ai","llm","slm","optimization"],"summary":"Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes \u2192 https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html In...","keywords":["step","model","data","training","less","smaller","distilling","outperforming","models","language"]},{"id":"bookmarks-rewired-brain-six-weeks-bbc","title":"How I rewired my brain in six weeks","content":"How I rewired my brain in six weeks \u2192 https://www.bbc.com/future/article/20230912-how-i-hacked-my-brain There is growing evidence that simple, everyday changes to our lives can alter our brains and change how they work. Melissa Hogenboom put herself into a scanner to find out. I was surprised that something as simple as mindfulness can play such a crucial role in keeping our minds healthy. Research has shown that mindfulness is a simple but powerful way to enhance several cognitive functions. It can improve attention, relieve pain and reduce stress. Research has found that after only a few months of mindfulness training, certain depression and anxiety symptoms can ease \u2013 though as with any complex mental health problem, this may of course vary depending on individual circumstances.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/rewired-brain-six-weeks-bbc","date":"2023-09-25 20:42 -05:00","tags":["neuroscience","habits","mindfulness","cognition"],"summary":"How I rewired my brain in six weeks \u2192 https://www.bbc.com/future/article/20230912-how-i-hacked-my-brain There is growing evidence that simple, everyday changes to our lives can alter our brains and...","keywords":["simple","mindfulness","research","brain","several","cognitive","functions","improve","attention","relieve"]},{"id":"bookmarks-vim-llm-hacks-thacker","title":"vim \u002B llm = \uD83D\uDD25","content":"vim \u002B llm = \uD83D\uDD25 \u2192 http://josephthacker.com/ai/2023/09/18/vim-llm-hacks.html ... I learned that you could be inside vim, but manipulate the entire file as if you were piping the contents of the file into a command. The output of the command does in-line replacement of the entire file with those changes. That sounds confusing, but it just means you can be inside a vim file and do :%!grep test and it\u2019ll remove all lines that don\u2019t contain test, for example. This post is a simple showcase of taking that concept, but throwing an llm into the mix to add more dynamic functionality.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/vim-llm-hacks-thacker","date":"2023-09-25 20:39 -05:00","tags":["vi","linux","llm","ai","softwaredevelopment"],"summary":"vim \u002B llm = \uD83D\uDD25 \u2192 http://josephthacker.com/ai/2023/09/18/vim-llm-hacks.html ... I learned that you could be inside vim, but manipulate the entire file as if you were piping the contents of the file...","keywords":["file","inside","entire","test","command","http://josephthacker","it\u2019ll","remove","lines","don\u2019t"]},{"id":"bookmarks-next-gen-cpu-acceleration-avx-gen-ai","title":"Next-Gen CPU Acceleration: AVX For Generative AI","content":"Next-Gen CPU Acceleration: AVX For Generative AI \u2192 https://www.youtube.com/watch?v=bskEGP0r3hE The future is AVX10, so says Intel. Recently a document was released showcasing a post-AVX512 world, and to explain why this matters, I\u0027ve again invited the Chips And Cheese crew onto the channel. Chester and George answer my questions on AVX10 and why it matters! Visit http://www.chipsandcheese.com to learn more!","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/next-gen-cpu-acceleration-avx-gen-ai","date":"2023-09-25 20:34 -05:00","tags":["ai","hardware","hpc"],"summary":"Next-Gen CPU Acceleration: AVX For Generative AI \u2192 https://www.youtube.com/watch?v=bskEGP0r3hE The future is AVX10, so says Intel. Recently a document was released showcasing a post-AVX512 world, and...","keywords":["avx10","matters","again","invited","chips","cheese","crew","onto","channel","next"]},{"id":"reshare-fat-bear-week-2023","title":"PSA: Fat Bear Week Oct 4-10 2023","content":"PSA: Fat Bear Week Oct 4-10 2023 \u2192 https://www.nps.gov/katm/learn/fat-bear-week.htm Fat Bear Week - an annual celebration of success. All bears are winners but only one true champion will emerge. Held over the course of seven days and concluding on the Fat Bear Tuesday, people chose which bear to crown in this tournament style bracket where bears are pitted against each other for your vote.","contentType":"reshare","url":"https://www.lqdev.me/responses/fat-bear-week-2023","date":"2023-09-25 20:31 -05:00","tags":["nature","bears","nps"],"summary":"PSA: Fat Bear Week Oct 4-10 2023 \u2192 https://www.nps.gov/katm/learn/fat-bear-week.htm Fat Bear Week - an annual celebration of success. All bears are winners but only one true champion will emerge....","keywords":["bear","week","bears","psa:","pitted","bracket","style","tournament","crown","which"]},{"id":"bookmarks-chatgpt-voice-image-capabilities","title":"ChatGPT can now see, hear, and speak","content":"ChatGPT can now see, hear, and speak \u2192 https://openai.com/blog/chatgpt-can-now-see-hear-and-speak We are beginning to roll out new voice and image capabilities in ChatGPT. They offer a new, more intuitive type of interface by allowing you to have a voice conversation or show ChatGPT what you\u2019re talking about.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/chatgpt-voice-image-capabilities","date":"2023-09-25 10:12 -05:00","tags":["ai","chatgpt","openai","llm","machinelearning","ml"],"summary":"ChatGPT can now see, hear, and speak \u2192 https://openai.com/blog/chatgpt-can-now-see-hear-and-speak We are beginning to roll out new voice and image capabilities in ChatGPT. They offer a new, more...","keywords":["chatgpt","hear","speak","voice","show","conversation","allowing","interface","type","intuitive"]},{"id":"bookmarks-aws-anthropic-partnership","title":"Amazon and Anthropic announce strategic collaboration to advance generative AI ","content":"Amazon and Anthropic announce strategic collaboration to advance generative AI \u2192 https://www.aboutamazon.com/news/company-news/amazon-aws-anthropic-ai Anthropic selects AWS as its primary cloud provider and will train and deploy its future foundation models on AWS Trainium and Inferentia chips, taking advantage of AWS\u2019s high-performance, low-cost machine learning accelerators.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/aws-anthropic-partnership","date":"2023-09-25 10:11 -05:00","tags":["ai","cloud"],"summary":"Amazon and Anthropic announce strategic collaboration to advance generative AI \u2192 https://www.aboutamazon.com/news/company-news/amazon-aws-anthropic-ai Anthropic selects AWS as its primary cloud...","keywords":["anthropic","amazon","machine","cost","performance","high","aws\u2019s","advantage","taking","chips"]},{"id":"notes-upgraded-mastodon-4-2-0","title":"Upgraded to Mastodon 4.2.0","content":"Good thing I\u0027d upgraded to 4.1.7 just a few weeks ago, so getting to 4.2.0 was relatively easy. The wiki where I\u0027m tracking updates makes it easy to quickly remember the general process. I haven\u0027t tried all of the new features, but it\u0027s nice to see there\u0027s a new onboarding flow to the Web UI which should make it easier to get started for new folks.","contentType":"notes","url":"https://www.lqdev.me/notes/upgraded-mastodon-4-2-0","date":"2023-09-24 20:31 -05:00","tags":["mastodon","fediverse","sysadmin","socialmedia","decentralization","internet","web"],"summary":"Good thing I\u0027d upgraded to 4.1.7 just a few weeks ago, so getting to 4.2.0 was relatively easy. The wiki where I\u0027m tracking updates makes it easy to quickly remember the general process. I haven\u0027t...","keywords":["easy","good","easier","which","flow","onboarding","nice","features","tried","haven"]},{"id":"posts-down-the-slow-web-rabbit-hole","title":"Down the Slow / Quiet Web Rabbit Hole","content":"No real commentary on this post other than a web surf down the slow / quite web. \u0060\u0060\u0060mermaid flowchart TD A[\u0022My 20th anniversary of blogging!\u0022] --\u003E B[\u0022The Quiet Web\u0022] B --\u003E C[\u0022Asyncronous Conversations\u0022] C --\u003E D[\u0022The Slow Web\u0022] D --\u003E E[\u0022The Slow Web Movement - I DONE THIS\u0022] E --\u003E F[\u0022The Slow Web (plus: the modern experience of film-watching)\u0022] F --\u003E G[\u0022Minimal Gallery\u0022] G --\u003E H[\u0022My website is a shifting house next to a river of knowledge. What could yours be?\u0022] H --\u003E I[\u0022Quiet web\u0022] I --\u003E J[\u0022Jeremy Sarber\u0022] J --\u003E K[\u0022datagubbe\u0022] K --\u003E L[\u0022Writers and talkers and leaders, oh my!\u0022] L --\u003E M[\u0022Is Blogging Dead?\u0022] click A href \u0022https://tracydurnell.com/2023/09/23/my-20th-anniversary-of-blogging/\u0022 \u0022My 20th anniversary of blogging!\u0022 click B href \u0022https://briankoberlein.com/tech/quiet-web/\u0022 \u0022The Quiet Web\u0022 click C href \u0022https://manuelmoreale.com/asynchronous-conversations\u0022 \u0022Asyncronous Conversations\u0022 click D href \u0022https://www.jackcheng.com/the-slow-web/\u0022 \u0022The Slow Web\u0022 click E href \u0022https://blog.idonethis.com/the-slow-web-movement/?ref=jackcheng.com\u0022 \u0022The Slow Web Movement - I DONE THIS\u0022 click F href \u0022http://www.rebeccablood.net/archive/2010/06/a_slow_web.html\u0022 \u0022The Slow Web (plus: the modern experience of film-watching)\u0022 click G href \u0022https://minimal.gallery/\u0022 \u0022Minimal Gallery\u0022 click H href \u0022https://thecreativeindependent.com/essays/laurel-schwulst-my-website-is-a-shifting-house-next-to-a-river-of-knowledge-what-could-yours-be/\u0022 \u0022My website is a shifting house next to a river of knowledge. What could yours be?\u0022 click I href \u0022https://manuelmoreale.com/quiet-web\u0022 \u0022Quiet web\u0022 click J href \u0022https://jeremysarber.com/\u0022 \u0022Jeremy Sarber\u0022 click K href \u0022https://datagubbe.se/\u0022 \u0022datagubbe\u0022 click L href \u0022https://everythingchanges.us/blog/writers-and-talkers-and-leaders/\u0022 \u0022Writers and talkers and leaders, oh my!\u0022 click M href \u0022https://abeautifulmess.com/is-blogging-dead/\u0022 \u0022Is Blogging Dead?\u0022 \u0060\u0060\u0060","contentType":"posts","url":"https://www.lqdev.me/posts/down-the-slow-web-rabbit-hole","date":"2023-09-23 23:36 -05:00","tags":["blogs","indieweb","internet","slowweb","quietweb"],"summary":"No real commentary on this post other than a web surf down the slow / quite web. \u0060\u0060\u0060mermaid flowchart TD A[\u0022My 20th anniversary of blogging!\u0022] --\u003E B[\u0022The Quiet Web\u0022] B --\u003E C[\u0022Asyncronous...","keywords":["href","click","slow","blogging","quiet","next","river","knowledge","yours","conversations"]},{"id":"reply-tracy-durnell-20-blogging-anniversary","title":"Tracy Durnell 20th anniversary of blogging","content":"Tracy Durnell 20th anniversary of blogging \u2192 https://tracydurnell.com/2023/09/23/my-20th-anniversary-of-blogging/ Happy 20th anniversary! Also, thanks for the generous linking. Lots of new folks to read and subscribe to.","contentType":"reply","url":"https://www.lqdev.me/responses/tracy-durnell-20-blogging-anniversary","date":"2023-09-23 21:38 -05:00","tags":["blogs","blogging","indieweb"],"summary":"Tracy Durnell 20th anniversary of blogging \u2192 https://tracydurnell.com/2023/09/23/my-20th-anniversary-of-blogging/ Happy 20th anniversary! Also, thanks for the generous linking. Lots of new folks to...","keywords":["20th","anniversary","tracy","folks","lots","linking","generous","thanks","also","happy"]},{"id":"bookmarks-dall-e-3","title":"DALL\u00B7E 3","content":"DALL\u00B7E 3 \u2192 https://openai.com/dall-e-3 DALL\u00B7E 3 understands significantly more nuance and detail than our previous systems, allowing you to easily translate your ideas into exceptionally accurate images.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/dall-e-3","date":"2023-09-22 10:18 -05:00","tags":["dalle","ai","openai","images","llm","ml"],"summary":"DALL\u00B7E 3 \u2192 https://openai.com/dall-e-3 DALL\u00B7E 3 understands significantly more nuance and detail than our previous systems, allowing you to easily translate your ideas into exceptionally accurate...","keywords":["dall\u00B7e","exceptionally","ideas","translate","easily","allowing","systems","accurate","previous","nuance"]},{"id":"bookmarks-matrix-2-0","title":"Matrix 2.0: The Future of Matrix","content":"Matrix 2.0: The Future of Matrix \u2192 https://matrix.org/blog/2023/09/matrix-2-0/ TL;DR: If you want to play with a shiny new Matrix 2.0 client, head over to Element X .","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/matrix-2-0","date":"2023-09-22 10:10 -05:00","tags":["matrix","messaging","communication","opensource","protocols","decentralization","openweb","web","internet"],"summary":"Matrix 2.0: The Future of Matrix \u2192 https://matrix.org/blog/2023/09/matrix-2-0/ TL;DR: If you want to play with a shiny new Matrix 2.0 client, head over to Element X .","keywords":["matrix","future","https://matrix","org/blog/2023/09/matrix","want","play","shiny","client","head","element"]},{"id":"reshare-atlantic-privacy-guide-2023","title":"The Atlantic\u2019s Guide to Privacy","content":"The Atlantic\u2019s Guide to Privacy \u2192 https://www.theatlantic.com/technology/archive/2023/09/managing-digital-privacy-personal-information-online/675184/ Sponsored post, but it\u0027s still a good list with guidance and suggestions for common questions.","contentType":"reshare","url":"https://www.lqdev.me/responses/atlantic-privacy-guide-2023","date":"2023-09-17 13:03 -05:00","tags":["privacy","security","internet"],"summary":"The Atlantic\u2019s Guide to Privacy \u2192 https://www.theatlantic.com/technology/archive/2023/09/managing-digital-privacy-personal-information-online/675184/ Sponsored post, but it\u0027s still a good list with...","keywords":["privacy","atlantic\u2019s","suggestions","guidance","list","good","post","sponsored","online/675184/","information"]},{"id":"bookmarks-optimizing-llm-production-hf","title":"Optimizing your LLM in production ","content":"Optimizing your LLM in production \u2192 https://huggingface.co/blog/optimize-llm In this blog post, we will go over the most effective techniques at the time of writing this blog post to tackle these challenges for efficient LLM deployment: Lower Precision: Research has shown that operating at reduced numerical precision, namely 8-bit and 4-bit, can achieve computational advantages without a considerable decline in model performance. 2. Flash Attention: Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization. 3. Architectural Innovations: Considering that LLMs are always deployed in the same way during inference, namely autoregressive text generation with a long input context, specialized model architectures have been proposed that allow for more efficient inference. The most important advancement in model architectures hereby are Alibi, Rotary embeddings, Multi-Query Attention (MQA) and Grouped-Query-Attention (GQA).","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/optimizing-llm-production-hf","date":"2023-09-17 12:58 -05:00","tags":["ai","llm","production","engineering","software","mlops","aiops","opensource"],"summary":"Optimizing your LLM in production \u2192 https://huggingface.co/blog/optimize-llm In this blog post, we will go over the most effective techniques at the time of writing this blog post to tackle these...","keywords":["attention","model","efficient","more","architectures","flash","inference","namely","post","blog"]},{"id":"bookmarks-software-squared-jiang","title":"Software\u00B2","content":"Software\u00B2 \u2192 https://blog.minch.co/2022/11/15/software-squared.html A new generation of AIs that become increasingly general by producing their own training data We are currently at the cusp of transitioning from \u201Clearning from data\u201D to \u201Clearning what data to learn from\u201D as the central focus of AI research. If deep learning can be described as \u201CSoftware 2.0\u201D\u2014software that programs itself based on example inputs/output pairs, then this promising, data-centric paradigm, in which software effectively improves itself by searching for its own training data, can be described as a kind of \u201CSoftware\u00B2\u201D. This paradigm inherits the benefits of Software 2.0 while improving on its core, data-bound weaknesses: While deep learning (Software 2.0) requires the programmer to manually provide training data for each new task, Software\u00B2 recasts data as software that models or searches the world to produce its own, potentially unlimited, training tasks and data.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/software-squared-jiang","date":"2023-09-17 12:54 -05:00","tags":["ai","software","data"],"summary":"Software\u00B2 \u2192 https://blog.minch.co/2022/11/15/software-squared.html A new generation of AIs that become increasingly general by producing their own training data We are currently at the cusp of...","keywords":["data","software","training","software\u00B2","itself","described","learning","deep","while","paradigm"]},{"id":"bookmarks-point-llm-point-clouds","title":"PointLLM: Empowering Large Language Models to Understand Point Clouds","content":"PointLLM: Empowering Large Language Models to Understand Point Clouds \u2192 https://github.com/OpenRobotLab/PointLLM We introduce PointLLM, a multi-modal large language model capable of understanding colored point clouds of objects. It perceives object types, geometric structures, and appearance without concerns for ambiguous depth, occlusion, or viewpoint dependency. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy. To rigorously evaluate our model\u0027s perceptual abilities and its generalization capabilities, we establish two benchmarks: Generative 3D Object Classification and 3D Object Captioning, assessed through three different evaluation methods.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/point-llm-point-clouds","date":"2023-09-17 12:52 -05:00","tags":["ai","llm","generativeai","multimodal","research"],"summary":"PointLLM: Empowering Large Language Models to Understand Point Clouds \u2192 https://github.com/OpenRobotLab/PointLLM We introduce PointLLM, a multi-modal large language model capable of understanding...","keywords":["point","object","large","language","model","clouds","pointllm:","strategy","training","stage"]},{"id":"bookmarks-a16z-how-consumers-using-generative-ai","title":"How consumers are using Generative AI","content":"How consumers are using Generative AI \u2192 https://a16z.com/how-are-consumers-using-generative-ai/ 1. Most leading products are built from the \u201Cground up\u201D around generative AI Like ChatGPT, the majority of products on this list didn\u2019t exist a year ago\u201480% of these websites are new Of the 50 companies on the list, only 5 are products of, or acquisitions by, pre-existing big tech companies... Of the remaining list members, a whopping 48% are completely bootstrapped, with no outside funding, according to PitchBook data. 2. ChatGPT has a massive lead, for now\u2026 ChatGPT represents 60% of monthly traffic to the entire top 50 list, with an estimated 1.6 billion monthly visits and 200 million monthly users (as of June 2023). This makes ChatGPT the 24th most visited website globally. 3. LLM assistants (like ChatGPT) are dominant, but companionship and creative tools are on the rise General LLM chatbots represent 68% of total consumer traffic to the top 50 list. However, two other categories have started to drive significant usage in recent months\u2014AI companions (such as CharacterAI) and content generation tools (such as Midjourney and ElevenLabs). Within the broader content generation category, image generation is the top use case with 41% of traffic, followed by prosumer writing tools at 26%, and video generation at 8%. Another category worth mentioning? Model hubs. There are only 2 on the list, but they drive significant traffic\u2014Civitai (for images) and Hugging Face both rank in the top 10. This is especially impressive because consumers are typically visiting these sites to download models to run locally, so web traffic is likely an underestimate of actual usage. 4. Early \u201Cwinners\u201D have emerged, but most product categories are up for grabs Good news for builders: despite the surge in interest in generative AI, in many categories there is not yet a runway success. 5. Acquisition for top products is entirely organic\u2014and consumers are willing to pay! The majority of companies on this list have no paid marketing (at least, that SimilarWeb is able to attribute). There is significant free traffic \u201Cavailable\u201D via X, Reddit, Discord, and email, as well as word of mouth and referral growth. And consumers are willing to pay for GenAI. 90% of companies on the list are already monetizing, nearly all of them via a subscription model. The average product on the list makes $21/month (for users on monthly plans)\u2014yielding $252 annually. 6. Mobile apps are still emerging as a GenAI platform Consumer AI products have, thus far, been largely browser-first, rather than app-first. Even ChatGPT took 6 months to launch a mobile app! Why aren\u2019t more AI companies building on mobile? The browser is a natural starting place to reach the broadest base of consumers. Many AI companies have small teams and likely don\u2019t want to fragment their focus and resources across Web, iOS, and Android. Given that the average consumer now spends 36 minutes more per day on mobile than desktop (4.1 hours vs. 3.5 hours), we expect to see more mobile-first GenAI products emerge as the technology matures.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/a16z-how-consumers-using-generative-ai","date":"2023-09-17 12:40 -05:00","tags":["ai","generativeai","product","business","venturecapital","vc"],"summary":"How consumers are using Generative AI \u2192 https://a16z.com/how-are-consumers-using-generative-ai/ 1. Most leading products are built from the \u201Cground up\u201D around generative AI Like ChatGPT, the majority...","keywords":["list","companies","chatgpt","products","consumers","traffic","mobile","generation","generative","monthly"]},{"id":"reshare-wordpress-activitypub-plugin","title":"WordPress ActivityPub Plugin","content":"WordPress ActivityPub Plugin \u2192 https://wordpress.org/plugins/activitypub/ I can\u0027t believe I missed this announcement. This is great to see! Enter the fediverse with ActivityPub, broadcasting your blog to a wider audience! Attract followers, deliver updates, and receive comments from a diverse user base of ActivityPub-compliant platforms. With the ActivityPub plugin installed, your WordPress blog itself function as a federated profile, along with profiles for each author. For instance, if your website is example.com, then the blog-wide profile can be found at @example.com@example.com, and authors like Jane and Bob would have their individual profiles at @jane@example.com and @bobz@example.com, respectively.","contentType":"reshare","url":"https://www.lqdev.me/responses/wordpress-activitypub-plugin","date":"2023-09-17 12:32 -05:00","tags":["wordpress","activitypub","fediverse","openweb","decentralization","internet","blogging","opensource"],"summary":"WordPress ActivityPub Plugin \u2192 https://wordpress.org/plugins/activitypub/ I can\u0027t believe I missed this announcement. This is great to see! Enter the fediverse with ActivityPub, broadcasting your...","keywords":["activitypub","blog","wordpress","plugin","profile","profiles","itself","function","federated","along"]},{"id":"bookmarks-coqui-xtts-text-to-speech","title":"Coqui \uD83D\uDC38 XTTS","content":"Coqui \uD83D\uDC38 XTTS \u2192 https://huggingface.co/spaces/coqui/xtts XTTS is a Voice generation model that lets you clone voices into different languages by using just a quick 3-second audio clip. XTTS is built on previous research, like Tortoise, with additional architectural innovations and training to make cross-language voice cloning and multilingual speech generation possible.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/coqui-xtts-text-to-speech","date":"2023-09-17 12:25 -05:00","tags":["ai","generativeai","speech","huggingface","ml","tts","opensource"],"summary":"Coqui \uD83D\uDC38 XTTS \u2192 https://huggingface.co/spaces/coqui/xtts XTTS is a Voice generation model that lets you clone voices into different languages by using just a quick 3-second audio clip. XTTS is built...","keywords":["xtts","voice","generation","coqui","multilingual","cloning","language","cross","training","innovations"]},{"id":"bookmarks-wurstchen-fast-image-generation","title":"Introducing W\u00FCrstchen: Fast Diffusion for Image Generation ","content":"Introducing W\u00FCrstchen: Fast Diffusion for Image Generation \u2192 https://huggingface.co/blog/wuerstchen W\u00FCrstchen is a diffusion model, whose text-conditional component works in a highly compressed latent space of images. Why is this important? Compressing data can reduce computational costs for both training and inference by orders of magnitude. Training on 1024\u00D71024 images is way more expensive than training on 32\u00D732. Usually, other works make use of a relatively small compression, in the range of 4x - 8x spatial compression. W\u00FCrstchen takes this to an extreme. Through its novel design, it achieves a 42x spatial compression! This had never been seen before, because common methods fail to faithfully reconstruct detailed images after 16x spatial compression. W\u00FCrstchen employs a two-stage compression, what we call Stage A and Stage B. Stage A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found in the paper). Together Stage A and B are called the Decoder, because they decode the compressed images back into pixel space. A third model, Stage C, is learned in that highly compressed latent space. This training requires fractions of the compute used for current top-performing models, while also allowing cheaper and faster inference. We refer to Stage C as the Prior.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/wurstchen-fast-image-generation","date":"2023-09-17 12:15 -05:00","tags":["ai","generativeai","imagegeneration","huggingface","diffusion","ml"],"summary":"Introducing W\u00FCrstchen: Fast Diffusion for Image Generation \u2192 https://huggingface.co/blog/wuerstchen W\u00FCrstchen is a diffusion model, whose text-conditional component works in a highly compressed...","keywords":["stage","compression","images","training","diffusion","space","compressed","w\u00FCrstchen","spatial","works"]},{"id":"bookmarks-t2i-adapters-efficient-controllable-generation","title":"Efficient Controllable Generation for SDXL with T2I-Adapters ","content":"Efficient Controllable Generation for SDXL with T2I-Adapters \u2192 https://huggingface.co/blog/t2i-sdxl-adapters T2I-Adapter is an efficient plug-and-play model that provides extra guidance to pre-trained text-to-image models while freezing the original large text-to-image models. T2I-Adapter aligns internal knowledge in T2I models with external control signals. We can train various adapters according to different conditions and achieve rich control and editing effects. Over the past few weeks, the Diffusers team and the T2I-Adapter authors have been collaborating to bring the support of T2I-Adapters for Stable Diffusion XL (SDXL) in diffusers. In this blog post, we share our findings from training T2I-Adapters on SDXL from scratch, some appealing results, and, of course, the T2I-Adapter checkpoints on various conditionings (sketch, canny, lineart, depth, and openpose)!","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/t2i-adapters-efficient-controllable-generation","date":"2023-09-17 12:11 -05:00","tags":["ai","generativeai","imagegeneration","deeplearning","ml","huggingface"],"summary":"Efficient Controllable Generation for SDXL with T2I-Adapters \u2192 https://huggingface.co/blog/t2i-sdxl-adapters T2I-Adapter is an efficient plug-and-play model that provides extra guidance to...","keywords":["adapters","sdxl","adapter","models","diffusers","various","control","image","text","from"]},{"id":"bookmarks-falcon-180b-announcement","title":"Spread Your Wings: Falcon 180B is here ","content":"Spread Your Wings: Falcon 180B is here \u2192 https://huggingface.co/blog/falcon-180b Today, we\u0027re excited to welcome TII\u0027s Falcon 180B to HuggingFace! Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII\u0027s RefinedWeb dataset. This represents the longest single-epoch pretraining for an open model.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/falcon-180b-announcement","date":"2023-09-12 10:35 -05:00","tags":["ai","llm","opensource","huggingface"],"summary":"Spread Your Wings: Falcon 180B is here \u2192 https://huggingface.co/blog/falcon-180b Today, we\u0027re excited to welcome TII\u0027s Falcon 180B to HuggingFace! Falcon 180B sets a new state-of-the-art for open...","keywords":["180b","falcon","open","model","spread","billion","parameters","trained","massive","trillion"]},{"id":"bookmarks-visual-basic-history-retool","title":"The history and legacy of Visual Basic","content":"The history and legacy of Visual Basic \u2192 https://retool.com/visual-basic/ How Visual Basic became the world\u0027s most dominant programming environment, its sudden fall from grace, and why its influence is still shaping the future of software development.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/visual-basic-history-retool","date":"2023-09-12 10:33 -05:00","tags":["visualbasic","windows"],"summary":"The history and legacy of Visual Basic \u2192 https://retool.com/visual-basic/ How Visual Basic became the world\u0027s most dominant programming environment, its sudden fall from grace, and why its influence...","keywords":["visual","basic","history","future","shaping","influence","grace","from","fall","sudden"]},{"id":"bookmarks-mojo-available-local-download","title":"Modular: Mojo\uD83D\uDD25 - It\u2019s finally here!","content":"Modular: Mojo\uD83D\uDD25 - It\u2019s finally here! \u2192 https://www.modular.com/blog/mojo-its-finally-here Today, we\u2019re excited to announce the next big step in Mojo\u2019s evolution: Mojo is now available for local download \u2013 beginning with Linux systems, and adding Mac and Windows in coming releases.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/mojo-available-local-download","date":"2023-09-10 10:37 -05:00","tags":["ai","python","programminglanguages"],"summary":"Modular: Mojo\uD83D\uDD25 - It\u2019s finally here! \u2192 https://www.modular.com/blog/mojo-its-finally-here Today, we\u2019re excited to announce the next big step in Mojo\u2019s evolution: Mojo is now available for local...","keywords":["finally","here","modular:","windows","adding","systems","linux","beginning","download","local"]},{"id":"bookmarks-textualize-textual-web","title":"textual-web","content":"textual-web \u2192 https://github.com/Textualize/textual-web Textual Web publishes Textual apps and terminals on the web.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/textualize-textual-web","date":"2023-09-10 10:35 -05:00","tags":["tui","terminal","web"],"summary":"textual-web \u2192 https://github.com/Textualize/textual-web Textual Web publishes Textual apps and terminals on the web.","keywords":["textual","https://github","com/textualize/textual","publishes","apps","terminals"]},{"id":"notes-mastodon-upgrade-4-1-7","title":"Mastodon 4.1.7 feels snappy","content":"I didn\u0027t inspect the changelog between 4.1.0 and 4.1.7 in great detail, but the latest stable version feels snappy.","contentType":"notes","url":"https://www.lqdev.me/notes/mastodon-upgrade-4-1-7","date":"2023-09-09 16:44 -05:00","tags":["mastodon","fediverse"],"summary":"I didn\u0027t inspect the changelog between 4.1.0 and 4.1.7 in great detail, but the latest stable version feels snappy.","keywords":["didn","inspect","changelog","detail","latest","stable","version","feels","snappy"]},{"id":"bookmarks-dread-central-overlook-motel","title":"Dread Central - The Overlook Motel","content":"Dread Central - The Overlook Motel \u2192 https://www.dreadcentral.com/the-overlook-motel/ I\u0027m a fan of finding hidden gems and overlooked films, especially when it comes to horror. It\u0027s how I came across some of my favorites like Hell House and Terrifier. That\u0027s why I was excited to run into The Overlook Motel series from Dread Central which spotlights these kinds of films.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/dread-central-overlook-motel","date":"2023-09-08 22:30 -05:00","tags":["movies","horror"],"summary":"Dread Central - The Overlook Motel \u2192 https://www.dreadcentral.com/the-overlook-motel/ I\u0027m a fan of finding hidden gems and overlooked films, especially when it comes to horror. It\u0027s how I came across...","keywords":["overlook","dread","central","motel","films","which","from","series","excited","terrifier"]},{"id":"reshare-tailscale-partners-mullvad","title":"Tailscale has partnered with Mullvad","content":"Tailscale has partnered with Mullvad \u2192 https://mullvad.net/en/blog/2023/9/7/tailscale-has-partnered-with-mullvad/ Today we announce a partnership with Tailscale that allows you to use both in conjunction through the Tailscale app. This functionality is not available through the Mullvad VPN app. This partnership allows customers of Tailscale to make use of our WireGuard VPN servers as \u201Cexit nodes\u201D. This means that whilst connected to Tailscale, you can access your devices across Tailscale\u2019s mesh network, whilst still connecting outbound through Mullvad VPN WireGuard servers in any location.","contentType":"reshare","url":"https://www.lqdev.me/responses/tailscale-partners-mullvad","date":"2023-09-08 21:26 -05:00","tags":["privacy","vpn","security"],"summary":"Tailscale has partnered with Mullvad \u2192 https://mullvad.net/en/blog/2023/9/7/tailscale-has-partnered-with-mullvad/ Today we announce a partnership with Tailscale that allows you to use both in...","keywords":["tailscale","mullvad","whilst","servers","allows","partnership","wireguard","partnered","announce","connecting"]},{"id":"reshare-platos-cave-raising-rent","title":"Plato\u0027s Cave Regrets to Inform You It Will Be Raising Its Rent","content":"Plato\u0026#39;s Cave Regrets to Inform You It Will Be Raising Its Rent \u2192 https://www.jwz.org/blog/2023/09/platos-cave-regrets-to-inform-you-it-will-be-raising-its-rent/ If you are receiving this letter, it means you have been designated a tenant of the cave\u2014i.e., you are chained to the wall, you are forced to watch shadows for all eternity, you are projecting said shadow puppets, and/or you are a philosopher who was able to break free and understand the true shackles of reality (PhD candidates about to argue their thesis). We do not undertake this lightly. As the costs of maintaining a cave meant to trap you in your ignorance increases year after year, we want you to know, from the bottom of our hearts, that we, too, are suffering. We get that times are tough, and we hope you can extend that sympathy to us, the managers of your cave. Please rest assured that cave costs are increasing everywhere. We manage many other caves like that of Polyphemus the Cyclops and the childhood home of Zeus. So, trust us: we know caves. We hope you will continue to enjoy living in our cave. We believe you are a valued part of the Plato\u0027s Cave community. Credit, cash, or Venmo all work. Original Source: https://www.mcsweeneys.net/articles/platos-cave-regrets-to-inform-you-it-will-be-raising-its-rent","contentType":"reshare","url":"https://www.lqdev.me/responses/platos-cave-raising-rent","date":"2023-09-07 21:25 -05:00","tags":["simulation"],"summary":"Plato\u0026#39;s Cave Regrets to Inform You It Will Be Raising Its Rent \u2192 https://www.jwz.org/blog/2023/09/platos-cave-regrets-to-inform-you-it-will-be-raising-its-rent/ If you are receiving this letter,...","keywords":["cave","regrets","inform","raising","year","hope","costs","caves","https://www","rent"]},{"id":"reshare-generative-ai-dotnet-pt-2-powell","title":"Generative AI and .NET - Part 2 SDK","content":"Generative AI and .NET - Part 2 SDK \u2192 https://www.aaron-powell.com/posts/2023-09-04-generative-ai-and-dotnet---part-2-sdk/ It\u2019s time to have a look at how we can build the basics of an application using Azure OpenAI Services and the .NET SDK.","contentType":"reshare","url":"https://www.lqdev.me/responses/generative-ai-dotnet-pt-2-powell","date":"2023-09-06 09:29 -05:00","tags":["ai","dotnet","generativeai","llm","azure"],"summary":"Generative AI and .NET - Part 2 SDK \u2192 https://www.aaron-powell.com/posts/2023-09-04-generative-ai-and-dotnet---part-2-sdk/ It\u2019s time to have a look at how we can build the basics of an application...","keywords":["generative","part","azure","using","application","basics","build","look","it\u2019s","sdk/"]},{"id":"notes-cfb-week-1-2023-notes","title":"College Football Week 1 Notes","content":"College football is back! Penn State had a great win. Looking back at a post from last year , some of my thoughts around the upcoming class were right. Looking at the upcoming schedule it looks well spaced out and if they can stay healthy, they\u0027ll have a great season. Tough losses for LSU and Clemson. This year I\u0027m trying a new way to experience games: blogs \u002B audio streams. This past week, although I tuned into the games, I didn\u0027t watch any of them on TV. Instead, I tuned into the live audio streams on The Varsity Network and for live discusson, I tuned into the various team blogs / communities provided by SBNation (fun fact: I learned SB stands for Sports Blogs). For example, Black Shoe Diaries , the Penn State blog, had a live thread for each of the quarters where site writers and fans got to interact in real-time. Other team blogs had similar threads. Overall, I thought it was a fun experience, and I\u0027m most likely going to do it again this week. Games I\u0027m most looking forward to Week 2: Stanford vs USC Temple vs Rutgers Texas vs Alabama","contentType":"notes","url":"https://www.lqdev.me/notes/cfb-week-1-2023-notes","date":"2023-09-06 00:17 -05:00","tags":["sports","football"],"summary":"College football is back! Penn State had a great win. Looking back at a post from last year , some of my thoughts around the upcoming class were right. Looking at the upcoming schedule it looks well...","keywords":["blogs","week","looking","tuned","live","upcoming","audio","team","streams","they"]},{"id":"bookmarks-set-up-onion-service","title":"Set up Your Onion Service","content":"Set up Your Onion Service \u2192 https://community.torproject.org/onion-services/setup/ This guide shows you how to set up an Onion Service for your website. For the technical details of how the Onion Service protocol works, see our Onion Service protocol page.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/set-up-onion-service","date":"2023-09-06 00:14 -05:00","tags":["tor","internet","privacy","guide"],"summary":"Set up Your Onion Service \u2192 https://community.torproject.org/onion-services/setup/ This guide shows you how to set up an Onion Service for your website. For the technical details of how the Onion...","keywords":["onion","service","protocol","https://community","torproject","org/onion","services/setup/","guide","shows","website"]},{"id":"bookmarks-interoperability-can-save-open-web-doctorow","title":"Cory Doctorow: Interoperability Can Save the Open Web","content":"Cory Doctorow: Interoperability Can Save the Open Web \u2192 https://spectrum.ieee.org/doctorow-interoperability In his new book The Internet Con: How to Seize the Means of Computation , author Cory Doctorow presents a strong case for disrupting Big Tech. While the dominance of Internet platforms like Twitter, Facebook, Instagram, or Amazon is often taken for granted, Doctorow argues that these walled gardens are fenced in by legal structures, not feats of engineering. Doctorow proposes forcing interoperability\u2014any given platform\u2019s ability to interact with another\u2014as a way to break down those walls and to make the Internet freer and more democratic.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/interoperability-can-save-open-web-doctorow","date":"2023-09-06 00:09 -05:00","tags":["openweb","interoperability","internet","opensource"],"summary":"Cory Doctorow: Interoperability Can Save the Open Web \u2192 https://spectrum.ieee.org/doctorow-interoperability In his new book The Internet Con: How to Seize the Means of Computation , author Cory...","keywords":["internet","doctorow","cory","interoperability","granted","argues","walled","gardens","fenced","legal"]},{"id":"bookmarks-rethinking-trust-dm-ai-era","title":"Rethinking trust in direct messages in the AI era","content":"Rethinking trust in direct messages in the AI era \u2192 https://www.microsoft.com/en-us/research/blog/rethinking-trust-in-direct-messages-in-the-ai-era/ This blog post is a part of a series exploring our research in privacy, security, and cryptography. For the previous post, see https://www.microsoft.com/en-us/research/blog/research-trends-in-privacy-security-and-cryptography . While AI has the potential to massively increase productivity, this power can be used equally well for malicious purposes, for example, to automate the creation of sophisticated scam messages. In this post, we explore threats AI can pose for online communication ecosystems and outline a high-level approach to mitigating these threats.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/rethinking-trust-dm-ai-era","date":"2023-09-06 00:05 -05:00","tags":["security","ai","privacy","cryptography","communication","messaging","socialmedia","trust"],"summary":"Rethinking trust in direct messages in the AI era \u2192 https://www.microsoft.com/en-us/research/blog/rethinking-trust-in-direct-messages-in-the-ai-era/ This blog post is a part of a series exploring our...","keywords":["messages","post","threats","trust","direct","https://www","microsoft","com/en","security","privacy"]},{"id":"bookmarks-perplexity-interactive-llm-visualizations","title":"Perplexity: Interactive LLM visualization","content":"Perplexity: Interactive LLM visualization \u2192 https://perplexity.vercel.app/ I built this little tool to help me understand what it\u0027s like to be an autoregressive language model. For any given passage of text, it augments the original text with highlights and annotations that tell me how \u0026quot;surprising\u0026quot; each token is to the model, and which other tokens the model thought were most likely to occur in its place. Right now, the LM I\u0027m using is the smallest version of GPT-2, with 124M parameters.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/perplexity-interactive-llm-visualizations","date":"2023-09-06 00:02 -05:00","tags":["ai","llm","visualization","tools"],"summary":"Perplexity: Interactive LLM visualization \u2192 https://perplexity.vercel.app/ I built this little tool to help me understand what it\u0027s like to be an autoregressive language model. For any given passage...","keywords":["model","text","perplexity:","annotations","tell","\u0026quot","surprising\u0026quot","token","which","tokens"]},{"id":"bookmarks-can-llms-learn-single-example","title":"Can LLMs learn from a single example?","content":"Can LLMs learn from a single example? \u2192 https://www.fast.ai/posts/2023-09-04-learning-jumps/ Summary: recently while fine-tuning a large language model (LLM) on multiple-choice science exam questions, we observed some highly unusual training loss curves. In particular, it appeared the model was able to rapidly memorize examples from the dataset after seeing them just once. This astonishing feat contradicts most prior wisdom about neural network sample efficiency. Intrigued by this result, we conducted a series of experiments to validate and better understand this phenomenon. It\u2019s early days, but the experiments support the hypothesis that the models are able to rapidly remember inputs. This might mean we have to re-think how we train and use LLMs.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/can-llms-learn-single-example","date":"2023-09-05 23:59 -05:00","tags":["ai","llm","finetuning","fastai"],"summary":"Can LLMs learn from a single example? \u2192 https://www.fast.ai/posts/2023-09-04-learning-jumps/ Summary: recently while fine-tuning a large language model (LLM) on multiple-choice science exam...","keywords":["llms","experiments","able","model","rapidly","from","https://www","astonishing","feat","contradicts"]},{"id":"reply-doing-laundry-without-phone","title":"Doing Laundry on Campus Without a Phone","content":"Doing Laundry on Campus Without a Phone \u2192 https://naveenarun.wordpress.com/2023/08/31/doing-laundry-on-campus-without-a-phone/ This article reminds me of this gem \uD83D\uDE02","contentType":"reply","url":"https://www.lqdev.me/responses/doing-laundry-without-phone","date":"2023-09-02 18:45 -05:00","tags":["technology","tv"],"summary":"Doing Laundry on Campus Without a Phone \u2192 https://naveenarun.wordpress.com/2023/08/31/doing-laundry-on-campus-without-a-phone/ This article reminds me of this gem \uD83D\uDE02","keywords":["laundry","campus","doing","phone","https://naveenarun","wordpress","com/2023/08/31/doing","phone/","article","reminds"]},{"id":"bookmarks-teching-with-ai-openai","title":"Teaching with AI","content":"Teaching with AI \u2192 https://openai.com/blog/teaching-with-ai We\u2019re releasing a guide for teachers using ChatGPT in their classroom\u2014including suggested prompts, an explanation of how ChatGPT works and its limitations, the efficacy of AI detectors, and bias.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/teching-with-ai-openai","date":"2023-09-01 18:32 -05:00","tags":["ai","education","openai","guide"],"summary":"Teaching with AI \u2192 https://openai.com/blog/teaching-with-ai We\u2019re releasing a guide for teachers using ChatGPT in their classroom\u2014including suggested prompts, an explanation of how ChatGPT works and...","keywords":["chatgpt","efficacy","limitations","works","explanation","prompts","suggested","classroom\u2014including","teaching","using"]},{"id":"reply-pixelfed-solo-announcement","title":"Announcement - Solo by Pixelfed","content":"Announcement - Solo by Pixelfed \u2192 https://mastodon.social/@pixelfed/110989967683252426 Solo by Pixelfed We\u0027ve been secretly building a single user federated photo sharing server (based on Pixelfed), with minimal setup, and built-in import Solo is simple, download the code, drag your photos to the media directory and open your browser So simple, yet super smart, Solo won\u0027t get in your way, but it will impress Launching Oct 2023 Very excited about this!","contentType":"reply","url":"https://www.lqdev.me/responses/pixelfed-solo-announcement","date":"2023-09-01 10:55 -05:00","tags":["fediverse","pixelfed","photos","socialmedia"],"summary":"Announcement - Solo by Pixelfed \u2192 https://mastodon.social/@pixelfed/110989967683252426 Solo by Pixelfed We\u0027ve been secretly building a single user federated photo sharing server (based on Pixelfed),...","keywords":["solo","pixelfed","simple","announcement","download","code","drag","photos","media","open"]},{"id":"reshare-generative-ai-dotnet-pt-1-powell","title":"Generative AI and .NET - Part 1 Intro","content":"Generative AI and .NET - Part 1 Intro \u2192 https://www.aaron-powell.com/posts/2023-09-01-generative-ai-and-dotnet---part-1-intro/ over this series I\u2019m going to share my learnings on the APIs, SDKs, and the like. The goal here isn\u2019t to \u201Cbuild something\u201D but rather to share what I\u2019ve learnt, the mistakes I\u2019ve made, the things I\u2019ve found confusing, and the code I\u2019ve had to rewrite umpteen times because \u201Coh, that\u2019s a better way to do it\u201D.","contentType":"reshare","url":"https://www.lqdev.me/responses/generative-ai-dotnet-pt-1-powell","date":"2023-09-01 10:46 -05:00","tags":["ai","dotnet","generativeai","llm"],"summary":"Generative AI and .NET - Part 1 Intro \u2192 https://www.aaron-powell.com/posts/2023-09-01-generative-ai-and-dotnet---part-1-intro/ over this series I\u2019m going to share my learnings on the APIs, SDKs, and...","keywords":["i\u2019ve","generative","part","share","rather","learnt","mistakes","made","things","found"]},{"id":"bookmarks-web-llm-llama-2","title":"Llama 2 7B/13B are now available in Web LLM","content":"Llama 2 7B/13B are now available in Web LLM \u2192 https://webllm.mlc.ai/ Llama 2 7B/13B are now available in Web LLM! Llama 2 70B is also supported. This project brings large-language model and LLM-based chatbot to web browsers. Everything runs inside the browser with no server support and accelerated with WebGPU. This opens up a lot of fun opportunities to build AI assistants for everyone and enable privacy while enjoying GPU acceleration.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/web-llm-llama-2","date":"2023-08-31 18:57 -05:00","tags":["ai","webgpu","llama","llm"],"summary":"Llama 2 7B/13B are now available in Web LLM \u2192 https://webllm.mlc.ai/ Llama 2 7B/13B are now available in Web LLM! Llama 2 70B is also supported. This project brings large-language model and LLM-based...","keywords":["llama","7b/13b","available","while","privacy","enable","everyone","assistants","build","opportunities"]},{"id":"reshare-supporting-open-source-ai-community-az","title":"Supporting the Open Source AI Community","content":"Supporting the Open Source AI Community \u2192 https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/ We believe artificial intelligence has the power to save the world\u2014and that a thriving open source ecosystem is essential to building this future. To help close this resource gap, we\u2019re announcing today the a16z Open Source AI Grant program. We\u2019ll support a small group of open source developers through grant funding (not an investment or SAFE note), giving them the opportunity to continue their work without the pressure to generate financial returns.","contentType":"reshare","url":"https://www.lqdev.me/responses/supporting-open-source-ai-community-az","date":"2023-08-31 18:48 -05:00","tags":["ai","opensource","machinelearning","llm"],"summary":"Supporting the Open Source AI Community \u2192 https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/ We believe artificial intelligence has the power to save the world\u2014and that a thriving...","keywords":["open","source","grant","supporting","a16z","program","we\u2019ll","support","small","group"]},{"id":"reshare-podcast-wordpress-indieweb-reality2","title":"WordPress at 25 and the Indie Web","content":"WordPress at 25 and the Indie Web \u2192 https://www.reality2cast.com/151 Good discussion on the open web, self-hosting, radio, and the indie web.","contentType":"reshare","url":"https://www.lqdev.me/responses/podcast-wordpress-indieweb-reality2","date":"2023-08-30 21:02 -05:00","tags":["podcast","indieweb","wordpress","selfhost","radio","openweb","fediverse"],"summary":"WordPress at 25 and the Indie Web \u2192 https://www.reality2cast.com/151 Good discussion on the open web, self-hosting, radio, and the indie web.","keywords":["indie","wordpress","https://www","reality2cast","com/151","good","discussion","open","self","hosting"]},{"id":"bookmarks-making-llms-work-for-you-willison","title":"Making Large Language Models Work For You","content":"Making Large Language Models Work For You \u2192 https://simonwillison.net/2023/Aug/27/wordcamp-llms/ My goal today is to provide practical, actionable advice for getting the most out of Large Language Models\u2014both for personal productivity but also as a platform that you can use to build things that you couldn\u2019t build before.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/making-llms-work-for-you-willison","date":"2023-08-29 19:57 -05:00","tags":["ai","llm","wordpress","presentation"],"summary":"Making Large Language Models Work For You \u2192 https://simonwillison.net/2023/Aug/27/wordcamp-llms/ My goal today is to provide practical, actionable advice for getting the most out of Large Language...","keywords":["large","language","build","making","platform","also","productivity","personal","models\u2014both","getting"]},{"id":"reshare-no-new-twitter-newport","title":"We Don\u2019t Need a New Twitter","content":"We Don\u2019t Need a New Twitter \u2192 https://www.newyorker.com/culture/cultural-comment/we-dont-need-a-new-twitter If Meta can succeed in capturing some of this peak-Twitter magic, while avoiding late-stage Twitter\u2019s struggles, the company will perhaps even reclaim some of the cultural gravity that it squandered a decade ago when Facebook took its turn toward crazy-uncle irrelevance. But can Meta possibly succeed in building a saner, nicer Twitter? Breaking news can spread quickly, as can clips that are funny in an original or strange way\u2014but these innocuous trends feel serendipitous, like a rainbow spanning storm clouds. To reach the Twitter masses, conspiracy, demagoguery, and cancellation are much more likely to succeed. The result is a Faustian bargain for our networked era: trusting the wisdom of crowds to identify what\u2019s interesting can create an intensely compelling stream of shared content, but this content is likely to arrive drenched in rancor. The obvious way Meta can attempt to escape this bargain is by moving Threads away from retransmission-based curation and toward algorithmic ranking. This will give the company more control over which discussions are amplified, but, in doing so, they will also lose the human-powered selectivity that makes Twitter so engaging. If we look past this narrow discussion of Threads\u2019 challenges, however, a broader question arises: Why is it so important to create a better version of Twitter in the first place? Ignored amid the hand-wringing about the toxic turn taken by large-scale conversation platforms are the many smaller, less flashy sites and services that have long been supporting a more civilized form of digital interaction. \u201CThe Internet has become the ultimate narrowcasting vehicle: everyone from UFO buffs to New York Yankee fans has a Website (or dozen) to call his own,\u201D the journalist Richard Zoglin wrote in 1996. \u201CA dot-com in every pot.\u201D We\u2019ve gone from Zoglin\u2019s dot-com in every pot to the social-media age\u2019s vision of every pot being filled with slop from the same platforms .","contentType":"reshare","url":"https://www.lqdev.me/responses/no-new-twitter-newport","date":"2023-08-28 20:46 -05:00","tags":["twitter","socialmedia","indieweb","internet","culture"],"summary":"We Don\u2019t Need a New Twitter \u2192 https://www.newyorker.com/culture/cultural-comment/we-dont-need-a-new-twitter If Meta can succeed in capturing some of this peak-Twitter magic, while avoiding late-stage...","keywords":["twitter","from","meta","succeed","more","bargain","content","need","create","toward"]},{"id":"bookmarks-against-waldenponding-rao","title":"Against Waldenponding","content":"Against Waldenponding \u2192 https://studio.ribbonfarm.com/p/against-waldenponding Waldenponding (after Thoreau\u0027s Walden Pond experiment on which Walden is based). The crude caricature is \u0026quot;smash your smart phone and go live in a log cabin to reclaim your attention and your life from being hacked by evil social media platforms.\u0026quot;","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/against-waldenponding-rao","date":"2023-08-24 21:43 -05:00","tags":["internet","society","technology"],"summary":"Against Waldenponding \u2192 https://studio.ribbonfarm.com/p/against-waldenponding Waldenponding (after Thoreau\u0027s Walden Pond experiment on which Walden is based). The crude caricature is \u0026quot;smash your...","keywords":["waldenponding","\u0026quot","walden","social","evil","hacked","from","life","attention","reclaim"]},{"id":"bookmarks-increasing-surface-area-blogging-critchlow","title":"Increasing the surface area of blogging","content":"Increasing the surface area of blogging \u2192 https://tomcritchlow.com/2022/04/21/new-rss/ RSS is kind of an invisible technology. People call RSS dead because you can\u2019t see it. There\u2019s no feed, no login, no analytics. RSS feels subsurface. Come to think of it - all the interesting bits of blogging are invisible. The discussion has moved to Twitter, or discords, or DMs. Trackbacks aren\u2019t a thing anymore. So when you see someone blogging all you see is the blog post. The branching replies and conversations are either invisible or hard to track down. But I believe we\u2019re living in a golden age of RSS. Blogging is booming. My feed reader has 280 feeds in it. How do we increase the surface area of RSS and blogging? I think there\u2019s something quietly radical about making your feed reader open by default. It increases the surface area of RSS so others can discover content more easily. It makes blogging more visible. The nice thing about RSS and OPML is that\u2019s a very extensible spec. The file format is flexible, you can define your own schema and fields. This might open up new kinds of publishing.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/increasing-surface-area-blogging-critchlow","date":"2023-08-24 21:25 -05:00","tags":["rss","internet","blog","postcast","feeds","openweb","openstandards"],"summary":"Increasing the surface area of blogging \u2192 https://tomcritchlow.com/2022/04/21/new-rss/ RSS is kind of an invisible technology. People call RSS dead because you can\u2019t see it. There\u2019s no feed, no...","keywords":["blogging","surface","area","invisible","feed","reader","open","more","thing","there\u2019s"]},{"id":"bookmarks-about-feeds","title":"About Feeds","content":"About Feeds \u2192 https://aboutfeeds.com/ About Feeds is a free site from Matt Webb. I made this site because using web feeds for the first time is hard, and we can fix that.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/about-feeds","date":"2023-08-24 21:21 -05:00","tags":["rss","internet","blog","openweb","openstandards"],"summary":"About Feeds \u2192 https://aboutfeeds.com/ About Feeds is a free site from Matt Webb. I made this site because using web feeds for the first time is hard, and we can fix that.","keywords":["feeds","site","https://aboutfeeds","com/","free","from","matt","webb","made","using"]},{"id":"bookmarks-consciousness-big-suitcase-minsky","title":"Consciousness is a Big Suitcase","content":"Consciousness is a Big Suitcase \u2192 https://www.edge.org/conversation/marvin_minsky-consciousness-is-a-big-suitcase Marvin Minsky is the leading light of AI\u2014artificial intelligence, that is. He sees the brain as a myriad of structures. Scientists who, like Minsky, take the strong AI view believe that a computer model of the brain will be able to explain what we know of the brain\u0027s cognitive abilities. Minsky identifies consciousness with high-level, abstract thought, and believes that in principle machines can do everything a conscious human being can do.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/consciousness-big-suitcase-minsky","date":"2023-08-24 21:01 -05:00","tags":["ai","consciousness","computerscience"],"summary":"Consciousness is a Big Suitcase \u2192 https://www.edge.org/conversation/marvin_minsky-consciousness-is-a-big-suitcase Marvin Minsky is the leading light of AI\u2014artificial intelligence, that is. He sees...","keywords":["minsky","consciousness","brain","suitcase","able","explain","cognitive","abilities","identifies","high"]},{"id":"bookmarks-serenity-os","title":"SerenityOS","content":"SerenityOS \u2192 https://serenityos.org/ A graphical Unix-like operating system for desktop computers! SerenityOS is a love letter to \u002790s user interfaces with a custom Unix-like core. It flatters with sincerity by stealing beautiful ideas from various other systems. Roughly speaking, the goal is a marriage between the aesthetic of late-1990s productivity software and the power-user accessibility of late-2000s *nix. This is a system by us, for us, based on the things we like.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/serenity-os","date":"2023-08-24 18:57 -05:00","tags":["os","linux","90s","serenityos","retro"],"summary":"SerenityOS \u2192 https://serenityos.org/ A graphical Unix-like operating system for desktop computers! SerenityOS is a love letter to \u002790s user interfaces with a custom Unix-like core. It flatters with...","keywords":["serenityos","unix","system","user","late","systems","roughly","speaking","goal","marriage"]},{"id":"bookmarks-meta-code-llama","title":"Introducing Code Llama, a state-of-the-art large language model for coding","content":"Introducing Code Llama, a state-of-the-art large language model for coding \u2192 https://ai.meta.com/blog/code-llama-large-language-model-coding/ Takeaways Code Llama is a state-of-the-art LLM capable of generating code, and natural language about code, from both code and natural language prompts. Code Llama is free for research and commercial use. Code Llama is built on top of Llama 2 and is available in three models: Code Llama, the foundational code model; Codel Llama - Python specialized for Python; and Code Llama - Instruct, which is fine-tuned for understanding natural language instructions. In our own benchmark testing, Code Llama outperformed state-of-the-art publicly available LLMs on code tasks","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/meta-code-llama","date":"2023-08-24 12:43 -05:00","tags":["ai","llm","llama","code","opensource","meta"],"summary":"Introducing Code Llama, a state-of-the-art large language model for coding \u2192 https://ai.meta.com/blog/code-llama-large-language-model-coding/ Takeaways Code Llama is a state-of-the-art LLM capable of...","keywords":["code","llama","language","state","model","natural","available","large","python","foundational"]},{"id":"posts-first-steps-nixos","title":"First Steps with NixOS","content":"## Introduction For the longest time, when using Linux distributions, I chose those that were Debian-based. I still do for servers. However, on the dekstop I switched to [Manjaro](https://manjaro.org/), which is Arch-based, about two years ago. Two years is a long time without distro-hopping, so I was long overdue. I\u0027ve been following the [NixOS](https://nixos.org/) project for some time but since I was happy with my setup, I didn\u0027t consider making the switch. However, a month ago, I decided to [dip my toes](/notes/maybe-switching-nixos). I installed NixOS on a flash drive and started tinkering with it. In this post, I\u0027ll provide some of my initial impressions and things I find appealing about the OS. ## Declarative configuration One of the first things I did when I booted into my new NixOS installation was get familiar with the [configuration file](https://nixos.wiki/wiki/Overview_of_the_NixOS_Linux_distribution#Declarative_Configuration). The configuration file is authored using the [Nix programming language](https://nix.dev/tutorials/first-steps/nix-language) and it\u0027s the place where you configure your entire system, including services, packages, and desktop environments. The best way I can describe it is a Dockerfile for your desktop. If interested, you can find my [config file](/resources/snippets/nixos-configuration/) on the scripts section of this website. There\u0027s a few advantages to this approach: 1. **Configurations are stored in a single place** - The advantage to this is, I don\u0027t have to figure out where the configuration files for each of the components of my system are. I can manage everything in one place. A perfect example where the configuration file goes beyond what I expected was being able to include my bash configuration. Typically, I\u0027d have that configuration in my \u0060bashrc\u0060 file. With NixOS, I can just include that in the configuration file as follows: \u0060\u0060\u0060nix programs.bash = { shellAliases = { emacs=\u0022emacs -nw\u0022; }; }; \u0060\u0060\u0060 1. **Enables composition** - Because the configuration file is effectively a script, I can modularize the difference pieces. For example, if I wanted to split out my service and package configurations into separate files, I can do so and reference those individual files from my configuration file, separating concerns while still keeping my system configuration simple. 1. **Version control** - Because the configuration file is just like any other plain text file, it means I can check it into the version control system or cloud service of my choice and manage it that way. In doing so, not only can I roll back using the built-in NixOS mechanisms, but I have another layer of security in case I need to recover that file. Now, because I can define everything about my system in the configuration file, this means I can seamlessly rebuild my entire system using this single file in a reproducible manner. ## Reproducible builds I got to experience first-hand how the configuration file can simplify out-of-box-experiences and system configurations. Initially, I did not install NixOS on my main hard drive. All of my configuration and learning took place on a flash drive. Once I got my configuration to a place I was satisfied, it was time to take the plunge and reimage my PC. In the past, when I\u0027ve done something similar, I\u0027ve had to document everything I did when configuring my system. That\u0027s where posts like [Setting up a new Ubuntu PC](/posts/setting-up-new-ubuntu-pc/) come from. With NixOS, all I needed to do was replace my configuration file with the one I configured on the flash drive. Then, when I ran \u0060nixos-rebuild switch\u0060, my system was configured exactly like the one on the flash drive. ## Risk-free software evaluations This is something I haven\u0027t tried yet, but I could easily see it coming in handy. Sometimes I might need a piece of software to do one thing or maybe I want to see whether it\u0027ll solve a problem I\u0027m facing. In order to try it out and use it, I need to install it globally. This can cause changes to my system permanently that I don\u0027t want. With NixOS, you can create [ad-hoc shell environments](https://nix.dev/tutorials/first-steps/ad-hoc-shell-environments#ad-hoc-shell-environments). These environments make temporary changes to your system, so you can evaluate a piece of software or use it that one time. Then, when you exit the environment, the software is no longer installed in your system. ## Large package selection A [large package selection](https://search.nixos.org/packages) to choose from is something I got used to with Manjaro. Being an Arch-based distribution, I had access to the [Arch User Repository (AUR)](https://aur.archlinux.org/). Moving to NixOS, it\u0027s nice to know that in this area, NixOS offers great support. Additionally, NixOS has built-in support for [Flatpak](https://nixos.wiki/wiki/Flatpak) ## Still exploring There\u0027s still a few things I don\u0027t really get. For example, when installing the .NET SDK, I wanted to have side-by-side installs of the latest STS (7.0) and LTS (6.0). However, including the package names in my configuration file didn\u0027t work as expected. Instead I had to use the following convention. \u0060\u0060\u0060nix (with dotnetCorePackages; combinePackages [ dotnet-sdk dotnet-sdk_7 ]) \u0060\u0060\u0060 As I learn more, this will probably make more sense but for now it\u0027s a mystery. ## Conclusion Overall, I\u0027m happy with NixOS and I can see myself using it for the long-term. Hopefully longer than two years. I\u0027m not a gamer so I can\u0027t say how well it does in that front. For web browsing and software development though, I really like what NixOS has to offer. If some of the things I mentioned above sound interesting, I\u0027d encourage you to install it on a flash drive and start tinkering with it.","contentType":"posts","url":"https://www.lqdev.me/posts/first-steps-nixos","date":"2023-08-22 18:03 -05:00","tags":["nixos","linux","os","nix","sysadmin","floss"],"summary":"## Introduction For the longest time, when using Linux distributions, I chose those that were Debian-based. I still do for servers. However, on the dekstop I switched to...","keywords":["configuration","file","nixos","system","drive","using","flash","software","things","environments"]},{"id":"bookmarks-announcing-python-in-excel","title":"Announcing Python in Excel","content":"Announcing Python in Excel \u2192 https://techcommunity.microsoft.com/t5/excel-blog/announcing-python-in-excel-combining-the-power-of-python-and-the/ba-p/3893439 Today we\u2019re announcing a significant evolution in the analytical capabilities available within Excel by releasing a Public Preview of Python in Excel. Python in Excel makes it possible to natively combine Python and Excel analytics within the same workbook - with no setup required. With Python in Excel, you can type Python directly into a cell, the Python calculations run in the Microsoft Cloud, and your results are returned to the worksheet, including plots and visualizations.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/announcing-python-in-excel","date":"2023-08-22 17:38 -05:00","tags":["python","excel","dataanalytics","ai"],"summary":"Announcing Python in Excel \u2192 https://techcommunity.microsoft.com/t5/excel-blog/announcing-python-in-excel-combining-the-power-of-python-and-the/ba-p/3893439 Today we\u2019re announcing a significant...","keywords":["python","excel","announcing","microsoft","within","combine","analytics","same","workbook","setup"]},{"id":"bookmarks-llm-semantic-search-deeplearningai-course","title":"Large Language Models with Semantic Search","content":"Large Language Models with Semantic Search \u2192 https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/ Keyword search has been a common method for search for many years. But for content-rich websites like news media sites or online shopping platforms, the keyword search capability can be limiting. Incorporating large language models (LLMs) into your search can significantly enhance the user experience by allowing them to ask questions and find information in a much easier way. This course teaches the techniques needed to leverage LLMs into search. Throughout the lessons, you\u2019ll explore key concepts like dense retrieval, which elevates the relevance of retrieved information, leading to improved search results beyond traditional keyword search, and reranking, which injects the intelligence of LLMs into your search system, making it faster and more effective.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/llm-semantic-search-deeplearningai-course","date":"2023-08-16 22:43 -05:00","tags":["ai","llm","course","retrievalaugmentedgeneration"],"summary":"Large Language Models with Semantic Search \u2192 https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/ Keyword search has been a common method for search for many years. But...","keywords":["search","language","models","llms","keyword","large","which","information","semantic","intelligence"]},{"id":"bookmarks-patterns-for-building-llm-systems-products-yan","title":"Patterns for Building LLM-based Systems and Products","content":"Patterns for Building LLM-based Systems and Products \u2192 https://eugeneyan.com/writing/llm-patterns/ There are seven key patterns. They\u2019re also organized along the spectrum of improving performance vs. reducing cost/risk, and closer to the data vs. closer to the user. Evals: To measure performance RAG: To add recent, external knowledge Fine-tuning: To get better at specific tasks Caching: To reduce latency \u0026amp; cost Guardrails: To ensure output quality Defensive UX: To anticipate \u0026amp; manage errors gracefully Collect user feedback: To build our data flywheel Addendum: how to match these LLM patterns to potential problems","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/patterns-for-building-llm-systems-products-yan","date":"2023-08-16 22:20 -05:00","tags":["ai","llm","designpatterns"],"summary":"Patterns for Building LLM-based Systems and Products \u2192 https://eugeneyan.com/writing/llm-patterns/ There are seven key patterns. They\u2019re also organized along the spectrum of improving performance vs....","keywords":["patterns","\u0026amp","user","data","closer","performance","tasks","caching:","reduce","latency"]},{"id":"bookmarks-huggingface-candle","title":"HuggingFace Candle","content":"HuggingFace Candle \u2192 https://github.com/huggingface/candle Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) and ease of use.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/huggingface-candle","date":"2023-08-16 21:54 -05:00","tags":["ai","machinelearning","opensource","huggingface"],"summary":"HuggingFace Candle \u2192 https://github.com/huggingface/candle Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) and ease of use.","keywords":["candle","huggingface","https://github","com/huggingface/candle","minimalist","framework","rust","focus","performance","including"]},{"id":"bookmarks-open-challenges-llm-research-huyen","title":"Open challenges in LLM research","content":"Open challenges in LLM research \u2192 https://huyenchip.com/2023/08/16/llm-research-open-challenges.html Reduce and measure hallucinations Optimize context length and context construction Incorporate other data modalities Make LLMs faster and cheaper Design a new model architecture Develop GPU alternatives Make agents usable Improve learning from human preference Improve the efficiency of the chat interface Build LLMs for non-English languages","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/open-challenges-llm-research-huyen","date":"2023-08-16 21:47 -05:00","tags":["ai","llm","research"],"summary":"Open challenges in LLM research \u2192 https://huyenchip.com/2023/08/16/llm-research-open-challenges.html Reduce and measure hallucinations Optimize context length and context construction Incorporate...","keywords":["open","challenges","research","llms","improve","context","develop","alternatives","agents","usable"]},{"id":"bookmarks-jupyter-ai-notebooks","title":"Jupyter AI Brings Generative AI to Notebooks","content":"Jupyter AI Brings Generative AI to Notebooks \u2192 https://www.infoq.com/news/2023/08/jupyter-ai-notebooks/ The open-source Project Jupyter, used by millions for data science and machine learning, has released Jupyter AI, a free tool bringing powerful generative AI capabilities to Jupyter notebooks. https://jupyter-ai.readthedocs.io/en/latest/ Jupyter AI, which brings generative AI to Jupyter. Jupyter AI provides a user-friendly and powerful way to explore generative AI models in notebooks and improve your productivity in JupyterLab and the Jupyter Notebook. More specifically, Jupyter AI offers: An %%ai magic that turns the Jupyter notebook into a reproducible generative AI playground. This works anywhere the IPython kernel runs (JupyterLab, Jupyter Notebook, Google Colab, VSCode, etc.). A native chat UI in JupyterLab that enables you to work with generative AI as a conversational assistant. Support for a wide range of generative model providers and models (AI21, Anthropic, Cohere, Hugging Face, OpenAI, SageMaker, etc.).","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/jupyter-ai-notebooks","date":"2023-08-16 21:43 -05:00","tags":["ai","notebooks","development"],"summary":"Jupyter AI Brings Generative AI to Notebooks \u2192 https://www.infoq.com/news/2023/08/jupyter-ai-notebooks/ The open-source Project Jupyter, used by millions for data science and machine learning, has...","keywords":["jupyter","generative","notebook","jupyterlab","notebooks","brings","models","powerful","colab","google"]},{"id":"reshare-restrostrange-tv","title":"Retrostream TV","content":"Retrostream TV \u2192 https://retrostrange.com/ Interesting project. RetroStrange is part independent media experiment, part handmade exaltation of vintage weirdness and the public domain. It is produced by Noah Maher and Phil Nelson.","contentType":"reshare","url":"https://www.lqdev.me/responses/restrostrange-tv","date":"2023-07-23 23:51 -05:00","tags":["livestream"],"summary":"Retrostream TV \u2192 https://retrostrange.com/ Interesting project. RetroStrange is part independent media experiment, part handmade exaltation of vintage weirdness and the public domain. It is produced...","keywords":["part","retrostream","maher","noah","produced","domain","public","weirdness","vintage","exaltation"]},{"id":"notes-automate-website-updates","title":"Need to automate seasonal \u0027blue ckecks\u0027","content":"I really need to automate my website updates. I had forgotten to update the \u0026quot;checkmark\u0026quot; (for lack of better words) for the month of July. Usually, I try to have an icon show up as my \u0026quot;blue check\u0026quot; (Twitter inspired) that is relevant for each month. July is Roswell month mostly because I think I already used the sun emoji for one of the other summer months. I guess being in the US, the US flag would\u0027ve made sense here, but somehow aliens seemed more fun. Anyway, I need to find a good way to automate this so it\u0027s not one of those things that happen when I remember. Since I just changed to the alien emoji, it means I probably will only have it up for a week. August is Tux month because of Linux (again, sun and aliens were already taken). Recommendations are appreciated. Though I\u0027m excited for spooky season in September \u0026amp; October.","contentType":"notes","url":"https://www.lqdev.me/notes/automate-website-updates","date":"2023-07-23 16:23 -05:00","tags":["github","roswell","truthisoutthere","xfiles","automation"],"summary":"I really need to automate my website updates. I had forgotten to update the \u0026quot;checkmark\u0026quot; (for lack of better words) for the month of July. Usually, I try to have an icon show up as my...","keywords":["month","aliens","need","automate","already","july","\u0026quot","emoji","since","remember"]},{"id":"notes-maybe-switching-nixos","title":"Thinking of switching to NixOS","content":"Time to distro hop. Maybe. For the past two years I\u0027ve been happily using Manjaro as my main distribution. During that time I\u0027ve also kept an eye on the immutable distributions. One of the main ones I\u0027ve been paying attention to is NixOS . Like Arch, it\u0027s taken me some time to wrap my head around it. Now that it\u0027s gotten easier to set up through a graphical installer, I\u0027ll start out with a live install to see how I like it.","contentType":"notes","url":"https://www.lqdev.me/notes/maybe-switching-nixos","date":"2023-07-21 17:15 -05:00","tags":["linux","nixos"],"summary":"Time to distro hop. Maybe. For the past two years I\u0027ve been happily using Manjaro as my main distribution. During that time I\u0027ve also kept an eye on the immutable distributions. One of the main ones...","keywords":["main","distro","start","installer","graphical","easier","gotten","around","head","wrap"]},{"id":"reshare-wordpress-playground","title":"WordPress Playground","content":"WordPress Playground \u2192 https://developer.wordpress.org/playground/ Not a WordPress user but this is cool, especially this - \u0026quot;Build an entire site, save it, host it\u0026quot; WordPress Playground makes WordPress instantly accessible for users, learners, extenders, and contributors. You can: Try a block, a theme, or a plugin Build an entire site, save it, host it Test your plugin with many specific WordPress and PHP versions Embed a real, interactive WordPress site in your tutorial or course Showcase a plugin or theme on your website Preview pull requests from your repository \u2026or even run WordPress locally using the VisualStudio Code plugin or a CLI tool called wp-now","contentType":"reshare","url":"https://www.lqdev.me/responses/wordpress-playground","date":"2023-07-15 00:27 -05:00","tags":["indieweb","wordpress","openweb"],"summary":"WordPress Playground \u2192 https://developer.wordpress.org/playground/ Not a WordPress user but this is cool, especially this - \u0026quot;Build an entire site, save it, host it\u0026quot; WordPress Playground...","keywords":["wordpress","plugin","site","host","save","entire","build","theme","playground","especially"]},{"id":"bookmarks-tildeverse","title":"Tildeverse","content":"Tildeverse \u2192 https://tildeverse.org/ a loose association of like-minded tilde communities. tildes are pubnixes in the spirit of tilde.club , which was created in 2014 by paul ford . Public Access UNIX Systems (PAUS) are a type of server that provide various services to a multi-user community. They first began in the early 1980\u0027s and continue today. Early servers ran various flavors of UNIX, hence the name Public Access \u0026quot;UNIX\u0026quot; Systems, but later generations saw a large mix of Unix-variants and, of course, GNU/Linux. To recognize the many different operating systems online today, these systems are increasingly referred to generically as \u0026quot;pubnixes\u0026quot;. - Pubnix Hist","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/tildeverse","date":"2023-07-12 22:25 -05:00","tags":["linux","community","retrocomputing"],"summary":"Tildeverse \u2192 https://tildeverse.org/ a loose association of like-minded tilde communities. tildes are pubnixes in the spirit of tilde.club , which was created in 2014 by paul ford . Public Access...","keywords":["systems","unix","early","today","various","tilde","\u0026quot","access","public","continue"]},{"id":"bookmarks-hack-the-planet-notcurses-tuis","title":"Hacking the Planet (with Notcurses) - A Guide to TUIs and Character Graphics","content":"Hacking the Planet (with Notcurses) - A Guide to TUIs and Character Graphics \u2192 https://nick-black.com/htp-notcurses.pdf A TUI (text user interface) is a holistic model, view, and controller implemented using character graphics. TUIs, like WIMP3 GUIs, freely move the cursor around their rectilinear display, as opposed to line-oriented CLIs and their ineluctable marches through the scrolling region. Given the same interactive task \u2022 A TUI implementation is almost certainly a smaller memory and disk footprint than a GUI, \u2022 a good TUI implementation might introduce less latency, and \u2022 a properly-done TUI implementation can often be significantly more portable.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/hack-the-planet-notcurses-tuis","date":"2023-07-12 22:21 -05:00","tags":["linux","terminal","tui"],"summary":"Hacking the Planet (with Notcurses) - A Guide to TUIs and Character Graphics \u2192 https://nick-black.com/htp-notcurses.pdf A TUI (text user interface) is a holistic model, view, and controller...","keywords":["implementation","notcurses","tuis","character","graphics","hacking","marches","scrolling","region","given"]},{"id":"reshare-dam-slack-clone-bash","title":"A slack clone in 5 lines of bash","content":"A slack clone in 5 lines of bash \u2192 https://the-dam.org/docs/explanations/suc.html suc provides Slack, Mattermost, etc.\u2019s core features: Real-time, rich-text chat, File sharing, Fine-grained access control, Straightforward automation and integration with other tools, Data encryption in transit and optionally at rest, state-of-the-art user authentication. This paper shows how suc implements those features. suc stays small by leveraging the consistent and composable primitives offered by modern UNIX implementations","contentType":"reshare","url":"https://www.lqdev.me/responses/dam-slack-clone-bash","date":"2023-07-12 22:07 -05:00","tags":["linux","bash","communication"],"summary":"A slack clone in 5 lines of bash \u2192 https://the-dam.org/docs/explanations/suc.html suc provides Slack, Mattermost, etc.\u2019s core features: Real-time, rich-text chat, File sharing, Fine-grained access...","keywords":["slack","encryption","transit","optionally","rest","state","user","authentication","paper","shows"]},{"id":"bookmarks-interview-shopify-coo-career-path-meetings","title":"Kaz Nejatian (COO, Shopify): Why Shopify Elevated the Non-Manager Career Path and Ditched Meetings","content":"Kaz Nejatian (COO, Shopify): Why Shopify Elevated the Non-Manager Career Path and Ditched Meetings \u2192 https://creatoreconomy.so/p/kaz-coo-shopify-craft-and-no-meetings The difference between crafters and managers The difference is in what you spend time on. \u0026quot;Most people get satisfaction from building \u2014 from actually creating things.\u0026quot; As companies scale, optics start playing a larger role. People start spending more time on internal docs than actually talking to customers. How do you prevent this from happening at Shopify? In most product reviews, product managers spend way too much time preparing the perfect presentation for execs. At Shopify, our approach to product reviews is different. We want to see how the product actually works by playing with the demo or diving into the code. We want our PMs to be extremely user-focused, to take full ownership over problems, and to have a high tolerance for risk. If these attributes aren\u0027t present, product managers tend to become \u0026quot;keepers of strategy.\u201D You end up with smart, highly credentialed individuals spending all their time writing strategy memos to increase their team size so that they can write even more strategy memos. How Shopify rages against meetings In early 2023, Shopify initiated operation \u201CChaos Monkey\u201D to: Cancel all meetings with 3\u002B people Reinstate \u201Cno meeting Wednesdays\u201D Remove needless Slack channels Does Shopify have a strong writing culture to help people communicate without meetings? Yes, we try to make async decisions as much as we can. We do this in a few ways: One of our mantras is \u201CDo things, tell people.\u201D You\u2019ll see this plastered on our walls if you come to Shopify\u2019s office. We built an operating system called GSD (get shit done). This internal tool emphasizes frequent written updates, which are much easier to digest than constant meetings. A meeting is a bug that some other process didn\u2019t work out. We focus on the mission. We want to be the all-in-one commerce platform for people to start and grow businesses. We try to avoid getting distracted by other side quests. The main thing is to keep the main thing the main thing.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/interview-shopify-coo-career-path-meetings","date":"2023-07-12 20:20 -05:00","tags":["career","productivity","product"],"summary":"Kaz Nejatian (COO, Shopify): Why Shopify Elevated the Non-Manager Career Path and Ditched Meetings \u2192 https://creatoreconomy.so/p/kaz-coo-shopify-craft-and-no-meetings The difference between crafters...","keywords":["shopify","meetings","people","product","much","want","start","actually","from","thing"]},{"id":"bookmarks-x-ai-launch","title":"Announcing xAI ","content":"Announcing xAI \u2192 https://x.ai/ Today we announce the formation of xAI. The goal of xAI is to understand the true nature of the universe.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/x-ai-launch","date":"2023-07-12 20:15 -05:00","tags":["ai"],"summary":"Announcing xAI \u2192 https://x.ai/ Today we announce the formation of xAI. The goal of xAI is to understand the true nature of the universe.","keywords":["announcing","https://x","today","announce","formation","goal","understand","true","nature","universe"]},{"id":"bookmarks-keras-core-announcement","title":"Introducing Keras Core: Keras for TensorFlow, JAX, and PyTorch.","content":"Introducing Keras Core: Keras for TensorFlow, JAX, and PyTorch. \u2192 https://keras.io/keras_core/announcement/ We\u0027re excited to share with you a new library called Keras Core, a preview version of the future of Keras. In Fall 2023, this library will become Keras 3.0. Keras Core is a full rewrite of the Keras codebase that rebases it on top of a modular backend architecture. It makes it possible to run Keras workflows on top of arbitrary frameworks \u2014 starting with TensorFlow, JAX, and PyTorch. Keras Core is also a drop-in replacement for tf.keras, with near-full backwards compatibility with tf.keras code when using the TensorFlow backend. In the vast majority of cases you can just start importing it via import keras_core as keras in place of from tensorflow import keras and your existing code will run with no issue \u2014 and generally with slightly improved performance, thanks to XLA compilation.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/keras-core-announcement","date":"2023-07-11 16:22 -05:00","tags":["deeplearning","ai","keras","jax","tensorflow","pytorch"],"summary":"Introducing Keras Core: Keras for TensorFlow, JAX, and PyTorch. \u2192 https://keras.io/keras_core/announcement/ We\u0027re excited to share with you a new library called Keras Core, a preview version of the...","keywords":["keras","tensorflow","core","backend","pytorch","import","full","code","library","vast"]},{"id":"reshare-gpt-4-ga","title":"GPT-4 API general availability","content":"GPT-4 API general availability \u2192 https://openai.com/blog/gpt-4-api-general-availability GPT-4 API general availability and deprecation of older models in the Completions API. GPT-3.5 Turbo, DALL\u00B7E and Whisper APIs are also generally available, and we are releasing a deprecation plan for older models of the Completions API, which will retire at the beginning of 2024.","contentType":"reshare","url":"https://www.lqdev.me/responses/gpt-4-ga","date":"2023-07-06 22:08 -05:00","tags":["openai","gpt4","llm","ai"],"summary":"GPT-4 API general availability \u2192 https://openai.com/blog/gpt-4-api-general-availability GPT-4 API general availability and deprecation of older models in the Completions API. GPT-3.5 Turbo, DALL\u00B7E...","keywords":["general","availability","deprecation","older","models","completions","retire","which","plan","releasing"]},{"id":"bookmarks-512kb-club","title":"512KB Club","content":"512KB Club \u2192 https://512kb.club/ The 512KB Club is a collection of performance-focused web pages from across the Internet. To qualify your website must satisfy both of the following requirements: It must be an actual site that contains a reasonable amount of information, not just a couple of links on a page (more info here). Your total UNCOMPRESSED web resources must not exceed 512KB.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/512kb-club","date":"2023-06-29 10:56 -05:00","tags":["web","smallweb","community"],"summary":"512KB Club \u2192 https://512kb.club/ The 512KB Club is a collection of performance-focused web pages from across the Internet. To qualify your website must satisfy both of the following requirements: It...","keywords":["512kb","club","uncompressed","total","here","info","more","page","links","couple"]},{"id":"bookmarks-1mb-club","title":"1MB Club","content":"1MB Club \u2192 https://1mb.club/ 1MB Club is a growing collection of performance-focused web pages weighing less than 1 megabyte.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/1mb-club","date":"2023-06-29 10:53 -05:00","tags":["web","smallweb","community"],"summary":"1MB Club \u2192 https://1mb.club/ 1MB Club is a growing collection of performance-focused web pages weighing less than 1 megabyte.","keywords":["club","https://1mb","club/","growing","collection","performance","focused","pages","weighing","less"]},{"id":"reshare-protonpass-release","title":"Introducing Proton Pass","content":"Introducing Proton Pass \u2192 https://proton.me/blog/proton-pass-launch We\u2019re happy to announce the global launch of Proton Pass...a password manager, one of the most highly demanded services from the Proton community in our annual surveys since we first launched Proton Mail...","contentType":"reshare","url":"https://www.lqdev.me/responses/protonpass-release","date":"2023-06-29 10:32 -05:00","tags":["security"],"summary":"Introducing Proton Pass \u2192 https://proton.me/blog/proton-pass-launch We\u2019re happy to announce the global launch of Proton Pass...a password manager, one of the most highly demanded services from the...","keywords":["proton","pass","launch","introducing","since","surveys","annual","community","from","services"]},{"id":"reshare-fsharp-conf-2023","title":"fsharpConf 2023","content":"fsharpConf 2023 \u2192 http://fsharpconf.com/ I\u0027m really enjoying the sessions. Kudos to the team who put it together and presenters delivering great content. If you\u0027re interested, check out the stream at http://fsharpconf.com/ .","contentType":"reshare","url":"https://www.lqdev.me/responses/fsharp-conf-2023","date":"2023-06-26 13:02 -05:00","tags":["fsharp","fsharpConf","dotnet","programming","community","opensource"],"summary":"fsharpConf 2023 \u2192 http://fsharpconf.com/ I\u0027m really enjoying the sessions. Kudos to the team who put it together and presenters delivering great content. If you\u0027re interested, check out the stream at...","keywords":["http://fsharpconf","com/","fsharpconf","2023","really","enjoying","sessions","kudos","team","together"]},{"id":"reshare-book-8088-hand-386-dos-pcs","title":"LGR review of the Book 8088 and Hand 386 Retro DOS PCs","content":"LGR review of the Book 8088 and Hand 386 Retro DOS PCs \u2192 https://yewtu.be/watch?v=6bODiZ5bP84","contentType":"reshare","url":"https://www.lqdev.me/responses/book-8088-hand-386-dos-pcs","date":"2023-06-20 23:29 -05:00","tags":["retrocomputing","review","dos","pcs"],"summary":"LGR review of the Book 8088 and Hand 386 Retro DOS PCs \u2192 https://yewtu.be/watch?v=6bODiZ5bP84","keywords":["review","book","8088","hand","retro","https://yewtu","be/watch","v=6bodiz5bp84"]},{"id":"bookmarks-reclaim-open-2023-resources","title":"Reclaim Open 2023 Resources","content":"Reclaim Open 2023 Resources \u2192 https://reclaimopen.com/ List of resources from the Reclaim Open 2023 Conference. Website Videos KEYNOTE: We Have Never Been Social: Web 2.0 and What Went Wrong Digital Media Cookbook Minimal Computing (Minicomp) Wiki Minimal Definitions Book: From Counterculture to Cyberculture Article: Fragmented Future","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/reclaim-open-2023-resources","date":"2023-06-19 22:04 -05:00","tags":["reclaimopen","opensource","openweb","edtech"],"summary":"Reclaim Open 2023 Resources \u2192 https://reclaimopen.com/ List of resources from the Reclaim Open 2023 Conference. Website Videos KEYNOTE: We Have Never Been Social: Web 2.0 and What Went Wrong Digital...","keywords":["reclaim","open","2023","resources","from","minimal","article:","cyberculture","counterculture","book:"]},{"id":"notes-metablogging-life-after-email","title":"Blog instead of e-mail?","content":"I\u0027ve been thinking about shifting at least some of my communications from e-mails to blogs especially in the context of business communications. Blogs are pull rather than push, so I\u0027m not innundating other\u0027s inboxes. People who are interested in the projects or topics I\u0027m working on can subscribe and consume the content as they wish. Also it\u0027s a way to be more expressive since it\u0027s a medium where ideas can be connected to provide more relevant context. That doesn\u0027t mean chats and even e-mail couldn\u0027t be used for urgent communication. For async non-urgent communciation though, blogs might be a good alternative to e-mail. These two articles provide a good introduction into how individuals and companies have implemented similar practices: Metablogging Is There Life After E-mail? Yes, and It\u0027s Amazing","contentType":"notes","url":"https://www.lqdev.me/notes/metablogging-life-after-email","date":"2023-06-18 21:20 -05:00","tags":["blogging","email","communication"],"summary":"I\u0027ve been thinking about shifting at least some of my communications from e-mails to blogs especially in the context of business communications. Blogs are pull rather than push, so I\u0027m not...","keywords":["blogs","mail","context","provide","more","communications","good","urgent","used","couldn"]},{"id":"notes-matrix-synapse-needs-admin-ui","title":"Matrix Synapse REALLY needs an Admin UI","content":"Spent the day playing sysadmin and removed about 50GB worth of data from my Matrix Synapse instance. Like Mastodon, most of that data wasn\u0027t even mine. Most of it comes is remote data from other users in large rooms. I really like Matrix and the ability to host my own messaging server that provides end-to-end encryption as well as federation with other instances. However, not having a straight-forward way of managing my server and cleaning up resources is painful. Anyway, if anyone is interested, here are some resources that have helped me in this process. Matrix Synapse Server Administration","contentType":"notes","url":"https://www.lqdev.me/notes/matrix-synapse-needs-admin-ui","date":"2023-06-11 21:32 -05:00","tags":["matrix","selfhost","synapse","opensource","sysadmin","messaging"],"summary":"Spent the day playing sysadmin and removed about 50GB worth of data from my Matrix Synapse instance. Like Mastodon, most of that data wasn\u0027t even mine. Most of it comes is remote data from other...","keywords":["server","matrix","data","synapse","from","resources","anyone","encryption","federation","instances"]},{"id":"notes-bing-ai-phone-wallpaper","title":"New AI generated phone wallpaper","content":"I\u0027m really liking my new phone wallpaper. Courtesy of DALL-E. In case anyone is interested in the prompt - \u0026quot;pug in the utah desert on mushrooms opening his third eye and being beamed up by aliens\u0026quot;","contentType":"notes","url":"https://www.lqdev.me/notes/bing-ai-phone-wallpaper","date":"2023-06-07 23:02 -05:00","tags":["ai","dalle","dumbphone"],"summary":"I\u0027m really liking my new phone wallpaper. Courtesy of DALL-E. In case anyone is interested in the prompt - \u0026quot;pug in the utah desert on mushrooms opening his third eye and being beamed up by...","keywords":["really","third","opening","mushrooms","desert","utah","\u0026quot","prompt","interested","anyone"]},{"id":"bookmarks-orca-lm-progressive-learning-gpt-4","title":"Orca: Progressive Learning from Complex Explanation Traces of GPT-4","content":"Orca: Progressive Learning from Complex Explanation Traces of GPT-4 \u2192 https://arxiv.org/pdf/2306.02707.pdf Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model\u2019s capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca, a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100% in complex zero-shot reasoning benchmarks like Big- Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/orca-lm-progressive-learning-gpt-4","date":"2023-06-06 23:29 -05:00","tags":["ai","finetuning","gpt"],"summary":"Orca: Progressive Learning from Complex Explanation Traces of GPT-4 \u2192 https://arxiv.org/pdf/2306.02707.pdf Recent research has focused on enhancing the capability of smaller models through imitation...","keywords":["from","models","learning","step","orca","lfms","imitation","complex","reasoning","more"]},{"id":"bookmarks-gorilla-apis-llm","title":"Gorilla: Large Language Model Connected with Massive APIs","content":"Gorilla: Large Language Model Connected with Massive APIs \u2192 https://gorilla.cs.berkeley.edu/ Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today\u0027s state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model\u0027s ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Code","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/gorilla-apis-llm","date":"2023-06-03 22:37 -05:00","tags":["ai","opensource","llm"],"summary":"Gorilla: Large Language Model Connected with Massive APIs \u2192 https://gorilla.cs.berkeley.edu/ Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling...","keywords":["llms","model","gorilla","calls","demonstrates","changes","tools","potential","models","document"]},{"id":"notes-now-on-bluesky","title":"I\u0027m on Bluesky","content":"I\u0027m now on Bluesky. Follow me at @lqdev.me","contentType":"notes","url":"https://www.lqdev.me/notes/now-on-bluesky","date":"2023-06-02 15:50 -05:00","tags":["web","socialmedia","bluesky"],"summary":"I\u0027m now on Bluesky. Follow me at @lqdev.me","keywords":["bluesky","follow","@lqdev"]},{"id":"bookmarks-draggan","title":"Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold","content":"Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold \u2192 https://arxiv.org/abs/2305.10973 ...we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object\u0027s rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion. Code","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/draggan","date":"2023-06-01 22:52 -05:00","tags":["ai","computer-vision"],"summary":"Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold \u2192 https://arxiv.org/abs/2305.10973 ...we propose DragGAN, which consists of two main components: 1) a...","keywords":["point","image","draggan","manipulation","tracking","handle","manifold","based","position","generative"]},{"id":"bookmarks-hf-starcoder-llm","title":"StarCoder: A State-of-the-Art LLM for Code ","content":"StarCoder: A State-of-the-Art LLM for Code \u2192 https://huggingface.co/blog/starcoder StarCoder and StarCoderBase are Large Language Models for Code (Code LLMs) trained on permissively licensed data from GitHub, including from 80\u002B programming languages, Git commits, GitHub issues, and Jupyter notebooks. Similar to LLaMA, we trained a ~15B parameter model for 1 trillion tokens. We fine-tuned StarCoderBase model for 35B Python tokens, resulting in a new model that we call StarCoder.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/hf-starcoder-llm","date":"2023-06-01 22:50 -05:00","tags":["ai","code"],"summary":"StarCoder: A State-of-the-Art LLM for Code \u2192 https://huggingface.co/blog/starcoder StarCoder and StarCoderBase are Large Language Models for Code (Code LLMs) trained on permissively licensed data...","keywords":["model","code","github","from","starcoder","starcoderbase","tokens","trained","notebooks","similar"]},{"id":"bookmarks-microsoft-lora-llm","title":"LoRA: Low-Rank Adaptation of Large Language Models","content":"LoRA: Low-Rank Adaptation of Large Language Models \u2192 https://github.com/microsoft/LoRA LoRA reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights. This vastly reduces the storage requirement for large language models adapted to specific tasks and enables efficient task-switching during deployment all without introducing inference latency. LoRA also outperforms several other adaptation methods including adapter, prefix-tuning, and fine-tuning. Paper","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/microsoft-lora-llm","date":"2023-06-01 22:45 -05:00","tags":["ai","finetune","llm"],"summary":"LoRA: Low-Rank Adaptation of Large Language Models \u2192 https://github.com/microsoft/LoRA LoRA reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing...","keywords":["rank","adaptation","large","language","models","tuning","lora","reduces","lora:","efficient"]},{"id":"bookmarks-openai-shap-e","title":"Shap-E: Generating Conditional 3D Implicit Functions","content":"Shap-E: Generating Conditional 3D Implicit Functions \u2192 https://arxiv.org/abs/2305.02463 We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. Code","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/openai-shap-e","date":"2023-06-01 22:41 -05:00","tags":["ai","openai","3D"],"summary":"Shap-E: Generating Conditional 3D Implicit Functions \u2192 https://arxiv.org/abs/2305.02463 We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models...","keywords":["shap","conditional","implicit","train","generative","model","assets","encoder","point","parameters"]},{"id":"star-wordpress-paid-newsletter","title":"WordPress - Newsletter paid subscriptions","content":"WordPress - Newsletter paid subscriptions \u2192 https://wordpress.com/blog/2023/06/01/newsletters-paid-subscriptions/ ...we\u2019re introducing a big update \u2014 the ability to add paid subscriptions and premium content, whatever plan you\u2019re on. Including the Free plan. Paid subscriptions let your fans support your art, writing, or project directly.","contentType":"star","url":"https://www.lqdev.me/responses/wordpress-paid-newsletter","date":"2023-06-01 21:00 -05:00","tags":["web","blogging","wordpress","newsletters"],"summary":"WordPress - Newsletter paid subscriptions \u2192 https://wordpress.com/blog/2023/06/01/newsletters-paid-subscriptions/ ...we\u2019re introducing a big update \u2014 the ability to add paid subscriptions and premium...","keywords":["paid","subscriptions","plan","wordpress","writing","support","fans","free","including","you\u2019re"]},{"id":"notes-am-i-in-2006-newspapers-cds-mp3s","title":"Newspapers, CDs, and MP3 players - Am I in 2006?","content":"While flipping through a local (physical) newspaper, I came across an ad for a sale happening at a nearby record store. Having nothing planned, I decided to check it out and ended up with a nice haul of about 15 CDs and 2 DVDs. I fired up my portable CD/DVD drive to transfer my new purchases to my portable MP3 player . Once finished, I popped one of the DVDs into the drive, made popcorn, and settled in. If I didn\u0027t know any better, this could easily be a scene from 2006.","contentType":"notes","url":"https://www.lqdev.me/notes/am-i-in-2006-newspapers-cds-mp3s","date":"2023-05-29 19:50 -05:00","tags":["tech","media","retro","cds","dvds"],"summary":"While flipping through a local (physical) newspaper, I came across an ad for a sale happening at a nearby record store. Having nothing planned, I decided to check it out and ended up with a nice haul...","keywords":["dvds","portable","drive","cd/dvd","transfer","purchases","player","once","fired","finished"]},{"id":"bookmarks-meta-mms-tts-stt","title":"Massively Multilingual Speech (MMS)","content":"Massively Multilingual Speech (MMS) \u2192 https://ai.facebook.com/blog/multilingual-model-speech-recognition/ The Massively Multilingual Speech (MMS) project expands speech technology from about 100 languages to over 1,000 by building a single multilingual speech recognition model supporting over 1,100 languages (more than 10 times as many as before), language identification models able to identify over 4,000 languages (40 times more than before), pretrained models supporting over 1,400 languages, and text-to-speech models for over 1,100 languages. Our goal is to make it easier for people to access information and to use devices in their preferred language. Paper Code","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/meta-mms-tts-stt","date":"2023-05-22 23:43 -05:00","tags":["ai","nlp","speech-to-text","text-to-speech"],"summary":"Massively Multilingual Speech (MMS) \u2192 https://ai.facebook.com/blog/multilingual-model-speech-recognition/ The Massively Multilingual Speech (MMS) project expands speech technology from about 100...","keywords":["speech","languages","multilingual","models","massively","language","times","supporting","more","model"]},{"id":"reshare-implement-dns-weekend","title":"Implement DNS in a weekends","content":"Implement DNS in a weekends \u2192 https://implement-dns.wizardzines.com/","contentType":"reshare","url":"https://www.lqdev.me/responses/implement-dns-weekend","date":"2023-05-14 00:39 -05:00","tags":["internet","dns","www","zine"],"summary":"Implement DNS in a weekends \u2192 https://implement-dns.wizardzines.com/","keywords":["implement","weekends","https://implement","wizardzines","com/"]},{"id":"bookmarks-PaLM-2","title":"PaLM 2","content":"PaLM 2 \u2192 https://ai.google/discover/palm2 PaLM 2 is our next generation large language model that builds on Google\u2019s legacy of breakthrough research in machine learning and responsible AI. It excels at advanced reasoning tasks, including code and math, classification and question answering, translation and multilingual proficiency, and natural language generation better than our previous state-of-the-art LLMs, including PaLM. It can accomplish these tasks because of the way it was built \u2013 bringing together compute-optimal scaling, an improved dataset mixture, and model architecture improvements. PaLM 2 is grounded in Google\u2019s approach to building and deploying AI responsibly. It was evaluated rigorously for its potential harms and biases, capabilities and downstream uses in research and in-product applications. It\u2019s being used in other state-of-the-art models, like Med-PaLM 2 and Sec-PaLM, and is powering generative AI features and tools at Google, like Bard and the PaLM API. Technical Report","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/PaLM-2","date":"2023-05-11 20:39 -05:00","tags":["ai"],"summary":"PaLM 2 \u2192 https://ai.google/discover/palm2 PaLM 2 is our next generation large language model that builds on Google\u2019s legacy of breakthrough research in machine learning and responsible AI. It excels...","keywords":["palm","state","including","research","google\u2019s","model","tasks","generation","language","features"]},{"id":"bookmarks-huggingface-transformer-agents","title":"Transformers Agent","content":"Transformers Agent \u2192 https://huggingface.co/docs/transformers/en/transformers_agents Transformers Agent...provides a natural language API on top of transformers: we define a set of curated tools and design an agent to interpret natural language and to use these tools. It is extensible by design; we curated some relevant tools, but we\u2019ll show you how the system can be extended easily to use any tool developed by the community.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/huggingface-transformer-agents","date":"2023-05-11 20:35 -05:00","tags":["transformers","ai"],"summary":"Transformers Agent \u2192 https://huggingface.co/docs/transformers/en/transformers_agents Transformers Agent...provides a natural language API on top of transformers: we define a set of curated tools and...","keywords":["tools","agent","design","curated","language","natural","transformers","agents","tool","easily"]},{"id":"bookmarks-copilot-docs","title":"Copilot for Docs","content":"Copilot for Docs \u2192 https://githubnext.com/projects/copilot-for-docs Whether you\u2019re learning a new library or API or you\u2019ve been using it for years, it can feel like the documentation gets in your way more than it helps. Maybe the tutorials are too basic, or the reference manual is too sketchy, or the relevant information is split across multiple pages full of irrelevant details. We\u2019re exploring a way to get you the information you need, faster. By surfacing the most relevant content for questions with tailored summaries that help connect the dots, Copilot for docs saves developers from scouring reams of documentation.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/copilot-docs","date":"2023-05-11 20:32 -05:00","tags":["ai","documentation","developers"],"summary":"Copilot for Docs \u2192 https://githubnext.com/projects/copilot-for-docs Whether you\u2019re learning a new library or API or you\u2019ve been using it for years, it can feel like the documentation gets in your way...","keywords":["docs","copilot","relevant","documentation","information","you\u2019re","full","irrelevant","details","we\u2019re"]},{"id":"bookmarks-chatgpt-prompt-engineering-developers","title":"ChatGPT Prompt Engineering for Developers","content":"ChatGPT Prompt Engineering for Developers \u2192 https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/ In ChatGPT Prompt Engineering for Developers, you will learn how to use a large language model (LLM) to quickly build new and powerful applications. Using the OpenAI API, you\u2019ll be able to quickly build capabilities that learn to innovate and create value in ways that were cost-prohibitive, highly technical, or simply impossible before now.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/chatgpt-prompt-engineering-developers","date":"2023-05-11 20:29 -05:00","tags":["ai","gpt","promptengineering"],"summary":"ChatGPT Prompt Engineering for Developers \u2192 https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/ In ChatGPT Prompt Engineering for Developers, you will learn how to...","keywords":["prompt","engineering","chatgpt","developers","build","quickly","learn","capabilities","innovate","create"]},{"id":"bookmarks-imagebind","title":"ImageBind: One Embedding Space To Bind Them All","content":"ImageBind: One Embedding Space To Bind Them All \u2192 https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/ ...ImageBind, the first AI model capable of binding information from six modalities. The model learns a single embedding, or shared representation space, not just for text, image/video, and audio, but also for sensors that record depth (3D), thermal (infrared radiation), and inertial measurement units (IMU), which calculate motion and position. Paper Demo Code","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/imagebind","date":"2023-05-09 21:02 -05:00","tags":["ai","embeddings"],"summary":"ImageBind: One Embedding Space To Bind Them All \u2192 https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/ ...ImageBind, the first AI model capable of binding information from six...","keywords":["model","embedding","space","modalities","binding","record","depth","thermal","infrared","radiation"]},{"id":"bookmarks-large-language-models-explain-neural-network-neurons","title":"Language models can explain neurons in language models","content":"Language models can explain neurons in language models \u2192 https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html This paper applies automation to the problem of scaling an interpretability technique to all the neurons in a large language model. Our hope is that building on this approach of automating interpretability will enable us to comprehensively audit the safety of models before deployment. Our technique seeks to explain what patterns in text cause a neuron to activate. It consists of three steps: Explain the neuron\u0027s activations using GPT-4 Simulate activations using GPT-4, conditioning on the explanation Score the explanation by comparing the simulated and real activations","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/large-language-models-explain-neural-network-neurons","date":"2023-05-09 20:59 -05:00","tags":["ai","llm","deeplearning"],"summary":"Language models can explain neurons in language models \u2192 https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html This paper applies automation to the problem of scaling an...","keywords":["language","models","explain","activations","neurons","explanation","using","technique","interpretability","neuron"]},{"id":"reshare-generative-ai-developer-work","title":"How generative AI is changing the way developers work","content":"How generative AI is changing the way developers work \u2192 https://github.blog/2023-04-14-how-generative-ai-is-changing-the-way-developers-work/","contentType":"reshare","url":"https://www.lqdev.me/responses/generative-ai-developer-work","date":"2023-04-20 22:11 -05:00","tags":["ai","engineering","software"],"summary":"How generative AI is changing the way developers work \u2192 https://github.blog/2023-04-14-how-generative-ai-is-changing-the-way-developers-work/","keywords":["generative","changing","developers","https://github","blog/2023","work/"]},{"id":"bookmarks-peft-parameter-efficient-fine-tuning","title":"PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware","content":"PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware \u2192 https://huggingface.co/blog/peft ...as models get larger and larger, full fine-tuning becomes infeasible to train on consumer hardware. In addition, storing and deploying fine-tuned models independently for each downstream task becomes very expensive, because fine-tuned models are the same size as the original pretrained model. Parameter-Efficient Fine-tuning (PEFT) approaches are meant to address both problems! PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs. This also overcomes the issues of catastrophic forgetting, a behaviour observed during the full finetuning of LLMs. PEFT approaches have also shown to be better than fine-tuning in the low-data regimes and generalize better to out-of-domain scenarios. It can be applied to various modalities, e.g., image classification and stable diffusion dreambooth.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/peft-parameter-efficient-fine-tuning","date":"2023-04-20 21:32 -05:00","tags":["ai","finetuning"],"summary":"PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware \u2192 https://huggingface.co/blog/peft ...as models get larger and larger, full fine-tuning becomes infeasible to...","keywords":["fine","tuning","models","approaches","peft","better","parameters","also","model","pretrained"]},{"id":"bookmarks-wikipedia-embeddings","title":"Wikipedia embeddings dataset","content":"Wikipedia embeddings dataset \u2192 https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/wikipedia-embeddings","date":"2023-04-20 21:23 -05:00","tags":["ai","dataset","embeddings"],"summary":"Wikipedia embeddings dataset \u2192 https://huggingface.co/datasets/Cohere/wikipedia-22-12-simple-embeddings","keywords":["embeddings","wikipedia","dataset","https://huggingface","co/datasets/cohere/wikipedia","simple"]},{"id":"reshare-nielsen-offline-online-latency","title":"Offline Is Just Online With Extreme Latency","content":"Offline Is Just Online With Extreme Latency \u2192 https://blog.jim-nielsen.com/2023/offline-is-online-with-extreme-latency/ you can think of online/offline as part of the same continuum just different measurements of latency. There are gradations of latency when you\u2019re \u201Conline\u201D, and \u201Coffline\u201D is merely at the slowest end of that spectrum.","contentType":"reshare","url":"https://www.lqdev.me/responses/nielsen-offline-online-latency","date":"2023-04-20 11:00 -05:00","tags":["web","offline"],"summary":"Offline Is Just Online With Extreme Latency \u2192 https://blog.jim-nielsen.com/2023/offline-is-online-with-extreme-latency/ you can think of online/offline as part of the same continuum just different...","keywords":["latency","online","extreme","offline","merely","\u201Coffline\u201D","\u201Conline\u201D","you\u2019re","gradations","measurements"]},{"id":"bookmarks-llava-language-vision-assistant","title":"LLaVA: Large Language and Vision Assistant","content":"LLaVA: Large Language and Vision Assistant \u2192 https://llava-vl.github.io/ LLaVA represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA. Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks in the language domain, but the idea is less explored in the multimodal field. 1. Multimodal Instruct Data. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. 2. LLaVA Model. We introduce LLaVA (Large Language-and-Vision Assistant), an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding. 3. Performance. Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. 4. Open-source. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/llava-language-vision-assistant","date":"2023-04-18 21:34 -05:00","tags":["ai","language","vision"],"summary":"LLaVA: Large Language and Vision Assistant \u2192 https://llava-vl.github.io/ LLaVA represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for...","keywords":["language","multimodal","large","instruction","llava","vision","data","model","visual","following"]},{"id":"bookmarks-webgpu-api","title":"WebGPU API","content":"WebGPU API \u2192 https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API The WebGPU API enables web developers to use the underlying system\u0027s GPU (Graphics Processing Unit) to carry out high-performance computations and draw complex images that can be rendered in the browser.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/webgpu-api","date":"2023-04-15 22:57 -05:00","tags":["ai","web","openstandards"],"summary":"WebGPU API \u2192 https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API The WebGPU API enables web developers to use the underlying system\u0027s GPU (Graphics Processing Unit) to carry out...","keywords":["webgpu","images","complex","draw","computations","performance","high","carry","unit","rendered"]},{"id":"bookmarks-openai-consistency-models","title":"Consistency Models","content":"Consistency Models \u2192 https://github.com/openai/consistency_models Paper: https://arxiv.org/abs/2303.01469 Diffusion models have made significant breakthroughs in image, audio, and video generation, but they depend on an iterative generation process that causes slow sampling speed and caps their potential for real-time applications. To overcome this limitation, we propose consistency models, a new family of generative models that achieve high sample quality without adversarial training . They support fast one-step generation by design, while still allowing for few-step sampling to trade compute for sample quality. They also support zero-shot data editing , like image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either as a way to distill pre-trained diffusion models, or as standalone generative models. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step generation. For example, we achieve the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained as standalone generative models, consistency models also outperform single-step, non-adversarial generative models on standard benchmarks like CIFAR-10, ImageNet 64x64 and LSUN 256x256.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/openai-consistency-models","date":"2023-04-14 14:22 -05:00","tags":["openai","ai"],"summary":"Consistency Models \u2192 https://github.com/openai/consistency_models Paper: https://arxiv.org/abs/2303.01469 Diffusion models have made significant breakthroughs in image, audio, and video generation,...","keywords":["models","generation","step","consistency","they","generative","trained","diffusion","achieve","sample"]},{"id":"bookmarks-free-dolly-llm","title":"Free Dolly","content":"Free Dolly \u2192 https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/free-dolly-llm","date":"2023-04-12 23:06 -05:00","tags":["ai","llm","oss"],"summary":"Free Dolly \u2192 https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm","keywords":["free","dolly","https://www","databricks","com/blog/2023/04/12/dolly","open","commercially","viable","instruction","tuned"]},{"id":"bookmarks-llm-generative-agents","title":"Generative Agents: Interactive Simulacra of Human Behavior","content":"Generative Agents: Interactive Simulacra of Human Behavior \u2192 https://arxiv.org/pdf/2304.03442.pdf In this paper, we introduce generative agents\u2014computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent\u2019s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors... By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/llm-generative-agents","date":"2023-04-11 11:21 -05:00","tags":["ai","llm"],"summary":"Generative Agents: Interactive Simulacra of Human Behavior \u2192 https://arxiv.org/pdf/2304.03442.pdf In this paper, we introduce generative agents\u2014computational software agents that simulate believable...","keywords":["agents","generative","behavior","language","human","believable","they","interactive","using","natural"]},{"id":"bookmarks-koala-dialogue-model","title":"Koala: A Dialogue Model for Academic Research","content":"Koala: A Dialogue Model for Academic Research \u2192 https://bair.berkeley.edu/blog/2023/04/03/koala/ ...Koala, a chatbot trained by fine-tuning Meta\u2019s LLaMA on dialogue data gathered from the web. Many of the most capable LLMs require huge computational resources to train, and oftentimes use large and proprietary datasets. This suggests that in the future, highly capable LLMs will be largely controlled by a small number of organizations, and both users and researchers will pay to interact with these models without direct access to modify and improve them on their own. On the other hand, recent months have also seen the release of increasingly capable freely available or (partially) open-source models, such as LLaMA. These systems typically fall short of the most capable closed models, but their capabilities have been rapidly improving. This presents the community with an important question: will the future see increasingly more consolidation around a handful of closed-source models, or the growth of open models with smaller architectures that approach the performance of their larger but closed-source cousins? Our results suggest that learning from high-quality datasets can mitigate some of the shortcomings of smaller models, maybe even matching the capabilities of large closed-source models in the future.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/koala-dialogue-model","date":"2023-04-05 23:28 -05:00","tags":["ai","llm"],"summary":"Koala: A Dialogue Model for Academic Research \u2192 https://bair.berkeley.edu/blog/2023/04/03/koala/ ...Koala, a chatbot trained by fine-tuning Meta\u2019s LLaMA on dialogue data gathered from the web. Many...","keywords":["models","closed","capable","source","future","large","llama","capabilities","llms","open"]},{"id":"reshare-test-tagged-response","title":"Testing tagged responses","content":"Testing tagged responses \u2192 http://lqdev.me/tags My response","contentType":"reshare","url":"https://www.lqdev.me/responses/test-tagged-response","date":"2023-04-04 23:30 -05:00","tags":["webdev"],"summary":"Testing tagged responses \u2192 http://lqdev.me/tags My response","keywords":["testing","tagged","responses","http://lqdev","me/tags","response"]},{"id":"bookmarks-deep-learning-foundations-stable-diffusion","title":"From Deep Learning Foundations to Stable Diffusion","content":"From Deep Learning Foundations to Stable Diffusion \u2192 https://www.fast.ai/posts/part2-2023.html From Deep Learning Foundations to Stable Diffusion...is part 2 of Practical Deep Learning for Coders. In this course, containing over 30 hours of video content, we implement the astounding Stable Diffusion algorithm from scratch!","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/deep-learning-foundations-stable-diffusion","date":"2023-04-04 21:35 -05:00","tags":[],"summary":"From Deep Learning Foundations to Stable Diffusion \u2192 https://www.fast.ai/posts/part2-2023.html From Deep Learning Foundations to Stable Diffusion...is part 2 of Practical Deep Learning for Coders. In...","keywords":["from","deep","learning","stable","diffusion","foundations","astounding","implement","content","video"]},{"id":"bookmarks-schillace-laws-semantic-ai","title":"Schillace Laws of Semantic AI","content":"Schillace Laws of Semantic AI \u2192 https://learn.microsoft.com/semantic-kernel/howto/schillacelaws The \u0026quot;Schillace Laws\u0026quot; were formulated after working with a variety of Large Language Model (LLM) AI systems to date. Knowing them will accelerate your journey into this exciting space of reimagining the future of software engineering.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/schillace-laws-semantic-ai","date":"2023-03-29 21:01 -05:00","tags":[],"summary":"Schillace Laws of Semantic AI \u2192 https://learn.microsoft.com/semantic-kernel/howto/schillacelaws The \u0026quot;Schillace Laws\u0026quot; were formulated after working with a variety of Large Language Model...","keywords":["schillace","future","reimagining","space","exciting","journey","accelerate","knowing","date","systems"]},{"id":"reshare-darklang-gpt","title":"Darklang is going all-in on AI","content":"Darklang is going all-in on AI \u2192 https://blog.darklang.com/gpt/ On February 1, we stopped working on what we\u0027re now calling \u0026quot;darklang-classic\u0026quot;, and are fully heads down on building \u0026quot;darklang-gpt\u0026quot;, which is the same core Darklang but redesigned to have AI as the primary (or possibly only) way of writing code.","contentType":"reshare","url":"https://www.lqdev.me/responses/darklang-gpt","date":"2023-03-28 22:24 -05:00","tags":[],"summary":"Darklang is going all-in on AI \u2192 https://blog.darklang.com/gpt/ On February 1, we stopped working on what we\u0027re now calling \u0026quot;darklang-classic\u0026quot;, and are fully heads down on building...","keywords":["darklang","\u0026quot","possibly","primary","redesigned","core","same","which","gpt\u0026quot","building"]},{"id":"bookmarks-laion-openflamingo-vision-llm","title":"OpenFlamingo: An open-source framework for training vision-language models with in-context learning","content":"OpenFlamingo: An open-source framework for training vision-language models with in-context learning \u2192 https://laion.ai/blog/open-flamingo/ ...OpenFlamingo, an open-source reproduction of DeepMind\u0027s Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs).","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/laion-openflamingo-vision-llm","date":"2023-03-28 22:00 -05:00","tags":[],"summary":"OpenFlamingo: An open-source framework for training vision-language models with in-context learning \u2192 https://laion.ai/blog/open-flamingo/ ...OpenFlamingo, an open-source reproduction of DeepMind\u0027s...","keywords":["open","source","framework","training","models","openflamingo","openflamingo:","large","evaluation","enables"]},{"id":"bookmarks-cerebras-gpt","title":"Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models","content":"Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models \u2192 https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/ Cerebras open sources seven GPT-3 models from 111 million to 13 billion parameters. Trained using the Chinchilla formula, these models set new benchmarks for accuracy and compute efficiency. Today\u2019s release is designed to be used by and reproducible by anyone. All models, weights, and checkpoints are available on Hugging Face and GitHub under the Apache 2.0 license. Additionally, we provide detailed information on our training methods and performance results in our forthcoming paper. The Cerebras CS-2 systems used for training are also available on-demand via Cerebras Model Studio.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/cerebras-gpt","date":"2023-03-28 21:53 -05:00","tags":[],"summary":"Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models \u2192 https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/ Cerebras open sources...","keywords":["cerebras","models","open","compute","training","language","available","efficient","family","used"]},{"id":"bookmarks-prompt-engineering-weng","title":"Prompt Engineering","content":"Prompt Engineering \u2192 https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/prompt-engineering-weng","date":"2023-03-21 21:47 -05:00","tags":[],"summary":"Prompt Engineering \u2192 https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/ This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests,...","keywords":["prompt","engineering","models","github","alignment","goal","core","multimodality","generation","image"]},{"id":"reshare-openai-gpt-4","title":"GPT-4","content":"GPT-4 \u2192 https://openai.com/product/gpt-4 GPT-4 can accept images as inputs and generate captions, classifications, and analyses. GPT-4 is capable of handling over 25,000 words of text, allowing for use cases like long form content creation, extended conversations, and document search and analysis.","contentType":"reshare","url":"https://www.lqdev.me/responses/openai-gpt-4","date":"2023-03-14 13:59 -05:00","tags":[],"summary":"GPT-4 \u2192 https://openai.com/product/gpt-4 GPT-4 can accept images as inputs and generate captions, classifications, and analyses. GPT-4 is capable of handling over 25,000 words of text, allowing for...","keywords":["https://openai","document","conversations","extended","creation","content","form","long","cases","allowing"]},{"id":"bookmarks-alpaca-model-stanford","title":"Alpaca: A Strong Open-Source Instruction-Following Model","content":"Alpaca: A Strong Open-Source Instruction-Following Model \u2192 https://crfm.stanford.edu/2023/03/13/alpaca.html We are releasing our findings about an instruction-following language model, dubbed Alpaca, which is fine-tuned from Meta\u2019s LLaMA 7B model. We train the Alpaca model on 52K instruction-following demonstrations generated in the style of self-instruct using text-davinci-003. Alpaca shows many behaviors similar to OpenAI\u2019s text-davinci-003, but is also surprisingly small and easy/cheap to reproduce.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/alpaca-model-stanford","date":"2023-03-14 11:06 -05:00","tags":[],"summary":"Alpaca: A Strong Open-Source Instruction-Following Model \u2192 https://crfm.stanford.edu/2023/03/13/alpaca.html We are releasing our findings about an instruction-following language model, dubbed Alpaca,...","keywords":["model","instruction","following","alpaca","text","davinci","alpaca:","generated","style","self"]},{"id":"bookmarks-spqa-ai-based-architecture-replace-software","title":"SPQA: The AI-based Architecture That\u2019ll Replace Most Existing Software","content":"SPQA: The AI-based Architecture That\u2019ll Replace Most Existing Software \u2192 https://danielmiessler.com/blog/spqa-ai-architecture-replace-existing-software/ AI-based applications will be completely different than those we have today. The new architecture will be a far more elegant, four-component structure based around GPTs: State, Policy, Questions, and Action. A security program using SPQA Choose the base model \u2014 You start with the latest and greatest overall GPT model from OpenAI, Google, Meta, McKinsey, or whoever. Lots of companies will have one. Let\u2019s call it OpenAI\u2019s GPT-6. It already knows so incredibly much about security, biotech, project management, scheduling, meetings, budgets, incident response, and audit preparedness that you might be able to survive with it alone. But you need more personalized context. Train your custom model \u2014 Then you train your custom model which is based on your own data, which will stack on top of GPT-6. This is all the stuff in the STATE section above. It\u2019s your company\u2019s telemetry and context. Logs. Docs. Finances. Chats. Emails. Meeting transcripts. Everything. It\u2019s a small company and there are compression algorithms as part of the Custom Model Generation (CMG) product we use, so it\u2019s a total of 312TB of data. You train your custom model on that. Train your policy model \u2014 Now you train another model that\u2019s all about your company\u2019s desires. The mission, the goals, your anti-goals, your challenges, your strategies. This is the guidance that comes from humans that we\u2019re using to steer the ACTION part of the architecture. When we ask it to make stuff for us, and build out our plans, it\u2019ll do so using the guardrails captured here in the POLICY. Tell the system to take the following actions \u2014 Now the models are combined. We have GPT-6, stacked with our STATE model, also stacked with our POLICY model, and together they know us better than we know ourselves.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/spqa-ai-based-architecture-replace-software","date":"2023-03-14 10:52 -05:00","tags":[],"summary":"SPQA: The AI-based Architecture That\u2019ll Replace Most Existing Software \u2192 https://danielmiessler.com/blog/spqa-ai-architecture-replace-existing-software/ AI-based applications will be completely...","keywords":["model","train","based","architecture","policy","custom","using","state","it\u2019s","security"]},{"id":"bookmarks-large-language-models-stable-diffusion-moment","title":"Large language models are having their Stable Diffusion moment","content":"Large language models are having their Stable Diffusion moment \u2192 https://simonwillison.net/2023/Mar/11/llama/ The race is on to release the first fully open language model that gives people ChatGPT-like capabilities on their own devices.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/large-language-models-stable-diffusion-moment","date":"2023-03-14 10:43 -05:00","tags":[],"summary":"Large language models are having their Stable Diffusion moment \u2192 https://simonwillison.net/2023/Mar/11/llama/ The race is on to release the first fully open language model that gives people...","keywords":["language","large","chatgpt","people","gives","model","open","fully","release","race"]},{"id":"notes-mastodon-untitled-rss-items","title":"Downsides of untitled RSS items","content":"Although titles are optional for RSS feed items and it\u0027s a common practice for microblog platforms like Mastodon to exclude them, it unfortunately means some clients like Elfeed don\u0027t know how to render those posts.","contentType":"notes","url":"https://www.lqdev.me/notes/mastodon-untitled-rss-items","date":"2023-02-23 21:11 -05:00","tags":["mastodon","rss","emacs"],"summary":"Although titles are optional for RSS feed items and it\u0027s a common practice for microblog platforms like Mastodon to exclude them, it unfortunately means some clients like Elfeed don\u0027t know how to...","keywords":["although","render","elfeed","clients","means","unfortunately","exclude","mastodon","platforms","microblog"]},{"id":"reshare-access-bitwarden-vault-passwordless","title":"Access Your Bitwarden Vault Without a Password","content":"Access Your Bitwarden Vault Without a Password \u2192 https://bitwarden.com/blog/access-your-bitwarden-vault-without-a-password/ Bitwarden expands the Log in with device option that lets you use a second device to authenticate your Bitwarden vault login instead of using your Bitwarden password.","contentType":"reshare","url":"https://www.lqdev.me/responses/access-bitwarden-vault-passwordless","date":"2023-02-23 11:20 -05:00","tags":[],"summary":"Access Your Bitwarden Vault Without a Password \u2192 https://bitwarden.com/blog/access-your-bitwarden-vault-without-a-password/ Bitwarden expands the Log in with device option that lets you use a second...","keywords":["bitwarden","vault","password","device","access","https://bitwarden","com/blog/access","password/","expands","option"]},{"id":"bookmarks-introduction-data-centric-ai-mit","title":"Introduction to Data-Centric AI","content":"Introduction to Data-Centric AI \u2192 https://dcai.csail.mit.edu/ Data-Centric AI (DCAI) is an emerging science that studies techniques to improve datasets, which is often the best way to improve performance in practical ML applications. While good data scientists have long practiced this manually via ad hoc trial/error and intuition, DCAI considers the improvement of data as a systematic engineering discipline. This is the first-ever course on DCAI. This class covers algorithms to find and fix common issues in ML data and to construct better datasets, concentrating on data used in supervised learning tasks like classification. All material taught in this course is highly practical, focused on impactful aspects of real-world ML applications, rather than mathematical details of how particular models work. You can take this course to learn practical techniques not covered in most ML classes, which will help mitigate the \u201Cgarbage in, garbage out\u201D problem that plagues many real-world ML applications.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/introduction-data-centric-ai-mit","date":"2023-02-23 10:25 -05:00","tags":[],"summary":"Introduction to Data-Centric AI \u2192 https://dcai.csail.mit.edu/ Data-Centric AI (DCAI) is an emerging science that studies techniques to improve datasets, which is often the best way to improve...","keywords":["data","applications","practical","dcai","course","improve","which","datasets","real","techniques"]},{"id":"bookmarks-software-2-0-karpathy","title":"Software 2.0","content":"Software 2.0 \u2192 https://karpathy.medium.com/software-2-0-a64152b37c35 The \u201Cclassical stack\u201D of Software 1.0 is what we\u2019re all familiar with \u2014 it is written in languages such as Python, C\u002B\u002B, etc...Software 2.0 is written in much more abstract, human unfriendly language, such as the weights of a neural network. Benefits of Software 2.0 Computationally homogeneous Simple to bake into silicon Constant running time Constant memory use Highly portable Agile Modules can meld into an optimal whole It is better than you Limitations of Software 2.0 At the end of the optimization we\u2019re left with large networks that work well, but it\u2019s very hard to tell how. Across many applications areas, we\u2019ll be left with a choice of using a 90% accurate model we understand, or 99% accurate model we don\u2019t. The 2.0 stack can fail in unintuitive and embarrassing ways ,or worse, they can \u201Csilently fail\u201D ...the existence of adversarial examples and attacks highlights the unintuitive nature of this stack. Programming Software 2.0 If you recognize Software 2.0 as a new and emerging programming paradigm instead of simply treating neural networks as a pretty good classifier in the class of machine learning techniques, the extrapolations become more obvious, and it\u2019s clear that there is much more work to do. In particular, we\u2019ve built up a vast amount of tooling that assists humans in writing 1.0 code, such as powerful IDEs with features like syntax highlighting, debuggers, profilers, go to def, git integration, etc. In the 2.0 stack, the programming is done by accumulating, massaging and cleaning datasets. Who is going to develop the first Software 2.0 IDEs, which help with all of the workflows in accumulating, visualizing, cleaning, labeling, and sourcing datasets? Github is a very successful home for Software 1.0 code. Is there space for a Software 2.0 Github? In this case repositories are datasets and commits are made up of additions and edits of the labels. Traditional package managers and related serving infrastructure like pip, conda, docker, etc. help us more easily deploy and compose binaries. How do we effectively deploy, share, import and work with Software 2.0 binaries? What is the conda equivalent for neural networks?","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/software-2-0-karpathy","date":"2023-02-23 10:14 -05:00","tags":[],"summary":"Software 2.0 \u2192 https://karpathy.medium.com/software-2-0-a64152b37c35 The \u201Cclassical stack\u201D of Software 1.0 is what we\u2019re all familiar with \u2014 it is written in languages such as Python, C\u002B\u002B,...","keywords":["software","more","neural","programming","datasets","networks","stack","we\u2019re","constant","code"]},{"id":"bookmarks-blip-2-zero-shot-image-to-text-generation","title":"Zero-shot image-to-text generation with BLIP-2","content":"Zero-shot image-to-text generation with BLIP-2 \u2192 https://huggingface.co/blog/blip-2 This guide introduces BLIP-2 from Salesforce Research that enables a suite of state-of-the-art visual-language models that are now available in \uD83E\uDD17 Transformers. We\u0027ll show you how to use it for image captioning, prompted image captioning, visual question-answering, and chat-based prompting. BLIP-2 bridges the modality gap between vision and language models by adding a lightweight Querying Transformer (Q-Former) between an off-the-shelf frozen pre-trained image encoder and a frozen large language model. Q-Former is the only trainable part of BLIP-2; both the image encoder and language model remain frozen.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/blip-2-zero-shot-image-to-text-generation","date":"2023-02-15 14:04 -05:00","tags":[],"summary":"Zero-shot image-to-text generation with BLIP-2 \u2192 https://huggingface.co/blog/blip-2 This guide introduces BLIP-2 from Salesforce Research that enables a suite of state-of-the-art visual-language...","keywords":["image","blip","language","frozen","captioning","model","encoder","former","models","visual"]},{"id":"bookmarks-what-is-chatgpt-why-it-works-wolfram","title":"What Is ChatGPT Doing\u2026and Why Does It Work?","content":"What Is ChatGPT Doing\u2026and Why Does It Work? \u2192 https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/what-is-chatgpt-why-it-works-wolfram","date":"2023-02-14 23:30 -05:00","tags":[],"summary":"What Is ChatGPT Doing\u2026and Why Does It Work? \u2192 https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/","keywords":["chatgpt","does","doing\u2026and","https://writings","stephenwolfram","com/2023/02/what","doing","work/"]},{"id":"notes-mastodon-4-1-0-release","title":"Mastodon 4.1.0 released","content":"Mastodon v4.1.0 was released yesterday. This was a highly anticipated release for me since it introduced the option to remove old cached profile media which was taking up a significant portion of my disk space. After upgrading, I was able to cut my used disk space in half. If you\u0027re running your own instance or have the ability to contact your instance admin, I can\u0027t recommend upgrading to this version enough.","contentType":"notes","url":"https://www.lqdev.me/notes/mastodon-4-1-0-release","date":"2023-02-11 10:09 -05:00","tags":["mastodon","fediverse"],"summary":"Mastodon v4.1.0 was released yesterday. This was a highly anticipated release for me since it introduced the option to remove old cached profile media which was taking up a significant portion of my...","keywords":["instance","upgrading","space","disk","mastodon","recommend","admin","contact","ability","running"]},{"id":"reshare-attacking-machine-learning-systems","title":"Attacking Machine Learning Systems","content":"Attacking Machine Learning Systems \u2192 https://www.schneier.com/blog/archives/2023/02/attacking-machine-learning-systems.html At their core, modern ML systems have complex mathematical models that use training data to become competent at a task. And while there are new risks inherent in the ML model, all of that complexity still runs in software. Training data are still stored in memory somewhere. And all of that is on a computer, on a network, and attached to the Internet. Like everything else, these systems will be hacked through vulnerabilities in those more conventional parts of the system.","contentType":"reshare","url":"https://www.lqdev.me/responses/attacking-machine-learning-systems","date":"2023-02-07 10:54 -05:00","tags":[],"summary":"Attacking Machine Learning Systems \u2192 https://www.schneier.com/blog/archives/2023/02/attacking-machine-learning-systems.html At their core, modern ML systems have complex mathematical models that use...","keywords":["systems","machine","learning","data","training","attacking","stored","memory","somewhere","computer"]},{"id":"bookmarks-microsoft-lmops-github","title":"LMOps - Enabling AI capabilities with LLMs and Generative AI models","content":"LMOps - Enabling AI capabilities with LLMs and Generative AI models \u2192 https://github.com/microsoft/LMOps LMOps is a research initiative on fundamental research and technology for building AI products w/ foundation models, especially on the general technology for enabling AI capabilities w/ LLMs and Generative AI models. Better Prompts: Promptist, Extensible prompts Longer Context: Structured prompting, Length-Extrapolatable Transformers Knowledge Augmentation (TBA) Fundamentals","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/microsoft-lmops-github","date":"2023-02-07 09:49 -05:00","tags":[],"summary":"LMOps - Enabling AI capabilities with LLMs and Generative AI models \u2192 https://github.com/microsoft/LMOps LMOps is a research initiative on fundamental research and technology for building AI products...","keywords":["models","lmops","enabling","capabilities","llms","generative","research","technology","prompts","longer"]},{"id":"notes-skype-feature-request-default-e2ee","title":"Skype Feature Request - Default Encrypted Conversations","content":"As the feedback post suggests, Skype offers the option to have encrypted conversations (chat \u0026amp; calls) using the Signal protocol . However, it\u0027s not the detault. If you\u0027re using Skype and this is something you\u0027d like added, feel free to upvote the feedback request. https://feedbackportal.microsoft.com/feedback/idea/0b770e9e-bca0-ed11-a81b-002248519701","contentType":"notes","url":"https://www.lqdev.me/notes/skype-feature-request-default-e2ee","date":"2023-01-30 17:25 -05:00","tags":["communication","messsaging","privacy","skype"],"summary":"As the feedback post suggests, Skype offers the option to have encrypted conversations (chat \u0026amp; calls) using the Signal protocol . However, it\u0027s not the detault. If you\u0027re using Skype and this is...","keywords":["feedback","skype","using","ed11","bca0","com/feedback/idea/0b770e9e","microsoft","https://feedbackportal","request","upvote"]},{"id":"notes-ms-forms-embed-test","title":"Microsoft Forms Test Embedding","content":"Testing whether my site can handle embedding Microsoft Forms . Useful for polls across a variety of channels.","contentType":"notes","url":"https://www.lqdev.me/notes/ms-forms-embed-test","date":"2023-01-30 11:46 -05:00","tags":["forms","microsoft","polls","web","indieweb"],"summary":"Testing whether my site can handle embedding Microsoft Forms . Useful for polls across a variety of channels.","keywords":["testing","whether","site","handle","embedding","microsoft","forms","useful","polls","across"]},{"id":"reshare-skype-8-93-release","title":"Skype 8.93 Released","content":"Skype 8.93 Released \u2192 https://answers.microsoft.com/skype/forum/all/release-notes-for-skype-893/6407f3c4-4037-4e6b-b93f-1953408cf9f3 A few updates I found interesting. The Beauty of languages: You can now use your own voice during a translated call in Skype. Universal translator: During a Skype call, if a participant speaks different languages, Skype Translator will automatically detect the languages and translate it for you. Extra! Extra! Read all about it: Stay up-to-date with the latest news and trends. Hit me up sometime: You can easily add contacts in Skype on mobile using a unique QR code to get connected. I have a few thoughts on the Today feature and overall updates Skype has been receiving over the past few months but I\u0027ll leave those for another post.","contentType":"reshare","url":"https://www.lqdev.me/responses/skype-8-93-release","date":"2023-01-30 11:07 -05:00","tags":[],"summary":"Skype 8.93 Released \u2192 https://answers.microsoft.com/skype/forum/all/release-notes-for-skype-893/6407f3c4-4037-4e6b-b93f-1953408cf9f3 A few updates I found interesting. The Beauty of languages: You...","keywords":["skype","languages","call","during","updates","extra","overall","stay","those","date"]},{"id":"bookmarks-windows-central-install-manjaro-wsl-guide","title":"Install Manjaro WSL - Windows 10 \u0026 11","content":"Install Manjaro WSL - Windows 10 \u0026amp; 11 \u2192 https://www.windowscentral.com/how-install-manjaro-wsl-windows-10-and-11","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/windows-central-install-manjaro-wsl-guide","date":"2023-01-29 14:56 -05:00","tags":[],"summary":"Install Manjaro WSL - Windows 10 \u0026amp; 11 \u2192 https://www.windowscentral.com/how-install-manjaro-wsl-windows-10-and-11","keywords":["install","manjaro","windows","\u0026amp","https://www","windowscentral","com/how"]},{"id":"reply-webmentionfs-send-test","title":"WebmentionFs Send Test","content":"WebmentionFs Send Test \u2192 https://webmention.rocks/test/1 Testing WebmentionFs send integration. From Website.","contentType":"reply","url":"https://www.lqdev.me/responses/webmentionfs-send-test","date":"2023-01-28 11:24 -05:00","tags":[],"summary":"WebmentionFs Send Test \u2192 https://webmention.rocks/test/1 Testing WebmentionFs send integration. From Website.","keywords":["webmentionfs","send","test","https://webmention","rocks/test/1","testing","integration","from","website"]},{"id":"reshare-microsoft-edge-phoenix-browser","title":"Microsoft Edge \u0027Phoenix\u0027 is an internal reimagining of the Edge web browser with a new UI and more features","content":"Microsoft Edge \u0026#39;Phoenix\u0026#39; is an internal reimagining of the Edge web browser with a new UI and more features \u2192 https://www.windowscentral.com/software-apps/windows-11/microsoft-edge-phoenix-is-an-internal-reimagining-of-the-edge-web-browser-with-a-new-ui-and-more-features As someone who spends a significant amount of time in the browser, making Edge, more specifically the browser the platform for applications, this looks appealing. Especially when you consider Progressive Web App (PWA) support and integrations with the Windows Store.","contentType":"reshare","url":"https://www.lqdev.me/responses/microsoft-edge-phoenix-browser","date":"2023-01-26 11:26 -05:00","tags":[],"summary":"Microsoft Edge \u0026#39;Phoenix\u0026#39; is an internal reimagining of the Edge web browser with a new UI and more features \u2192...","keywords":["edge","browser","more","features","internal","reimagining","platform","applications","looks","appealing"]},{"id":"notes-i3-gaps-released","title":"i3-gaps Released","content":"Nice! i3-gaps was recently merged into the main i3 codebase some time ago and with the 4.22 release a few weeks ago, you can now use it with your existing i3 configuration. I\u0027ve started tinkering with it and I like the way it makes my desktop look.","contentType":"notes","url":"https://www.lqdev.me/notes/i3-gaps-released","date":"2023-01-26 10:46 -05:00","tags":["i3","windowmanager","linux","desktop"],"summary":"Nice! i3-gaps was recently merged into the main i3 codebase some time ago and with the 4.22 release a few weeks ago, you can now use it with your existing i3 configuration. I\u0027ve started tinkering...","keywords":["nice","gaps","recently","merged","main","codebase","release","weeks","existing","configuration"]},{"id":"notes-indieweb-wiki-dotnet-page","title":"New IndieWeb Wiki .NET Page","content":"Just created a new .NET page on the IndieWeb Wiki . If your personal website or blog is using .NET, feel free to add it there. Also, if you\u0027re building any services or libraries for the IndieWeb, add them as well. Looking to learn more about IndieWeb? https://indiewebify.me/ or the IndieWeb Wiki are great places to get started.","contentType":"notes","url":"https://www.lqdev.me/notes/indieweb-wiki-dotnet-page","date":"2023-01-25 21:14 -05:00","tags":["indieweb","wiki","dotnet","community"],"summary":"Just created a new .NET page on the IndieWeb Wiki . If your personal website or blog is using .NET, feel free to add it there. Also, if you\u0027re building any services or libraries for the IndieWeb, add...","keywords":["indieweb","wiki","created","https://indiewebify","more","learn","looking","libraries","services","building"]},{"id":"reply-hon-eternal-tumblr","title":"Eternal Tumblr","content":"Eternal Tumblr \u2192 https://newsletter.danhon.com/archive/s14e09-building-and-documentation-making-new/ Automattic, the current owner of tumblr, one of the biggest meme mines in the world, has said they\u2019re going to implement ActivityPub, which is the underlying protocol on which Mastodon operates. There are boring ways this could go, and then interesting ways this could go. Interesting points. I\u0027m looking forward to how this shakes out.","contentType":"reply","url":"https://www.lqdev.me/responses/hon-eternal-tumblr","date":"2023-01-25 19:26 -05:00","tags":[],"summary":"Eternal Tumblr \u2192 https://newsletter.danhon.com/archive/s14e09-building-and-documentation-making-new/ Automattic, the current owner of tumblr, one of the biggest meme mines in the world, has said...","keywords":["tumblr","interesting","which","ways","eternal","implement","activitypub","underlying","protocol","operates"]},{"id":"bookmarks-math-machine-learning-data-science-mooc","title":"Mathematics for Machine Learning and Data Science","content":"Mathematics for Machine Learning and Data Science \u2192 https://www.coursera.org/specializations/mathematics-for-machine-learning-and-data-science Master the Toolkit of AI and Machine Learning. Mathematics for Machine Learning and Data Science is a beginner-friendly specialization where you\u2019ll learn the fundamental mathematics toolkit of machine learning: calculus, linear algebra, statistics, and probability.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/math-machine-learning-data-science-mooc","date":"2023-01-25 17:02 -05:00","tags":[],"summary":"Mathematics for Machine Learning and Data Science \u2192 https://www.coursera.org/specializations/mathematics-for-machine-learning-and-data-science Master the Toolkit of AI and Machine Learning....","keywords":["machine","learning","mathematics","data","science","toolkit","algebra","linear","calculus","learning:"]},{"id":"star-nielsen-best-time-to-own-domain","title":"The Best Time to Own a Domain Was 20 Years Ago; The Second Best Time Is Today","content":"The Best Time to Own a Domain Was 20 Years Ago; The Second Best Time Is Today \u2192 https://blog.jim-nielsen.com/2023/best-time-to-own-a-domain/ if own your domain, create value there, and drive people to it, you\u2019re paying ~$10 a year to build unbounded value over the years \u2014 value you control. That is why owning a domain (and publishing your content there) is like planting a tree: it\u2019s value that starts small and grows. The best time to own a domain and publish your content there was 20 years ago. The second best time is today. I love a technology like rel=me which pushes the idea of domain ownership into broader and broader spheres of society \u2014 \u201CHow do I get that nice little green checkmark on my profile?\u201D It reinforces the value of owning your own domain (which you can verify elsewhere) and encourages citizens of the internet people to build value in their own little corners of the world wide web.","contentType":"star","url":"https://www.lqdev.me/responses/nielsen-best-time-to-own-domain","date":"2023-01-25 10:29 -05:00","tags":[],"summary":"The Best Time to Own a Domain Was 20 Years Ago; The Second Best Time Is Today \u2192 https://blog.jim-nielsen.com/2023/best-time-to-own-a-domain/ if own your domain, create value there, and drive people...","keywords":["domain","value","best","years","owning","which","content","people","build","little"]},{"id":"reshare-bitwarden-open-source-security-explained","title":"Bitwarden Open Source Security Explained","content":"Bitwarden Open Source Security Explained \u2192 https://bitwarden.com/blog/bitwarden-open-source-security-explained/ This article answers three common questions about how being open source strengthens Bitwarden security, transparency, and privacy.","contentType":"reshare","url":"https://www.lqdev.me/responses/bitwarden-open-source-security-explained","date":"2023-01-25 10:26 -05:00","tags":[],"summary":"Bitwarden Open Source Security Explained \u2192 https://bitwarden.com/blog/bitwarden-open-source-security-explained/ This article answers three common questions about how being open source strengthens...","keywords":["open","source","security","bitwarden","explained","https://bitwarden","com/blog/bitwarden","explained/","article","answers"]},{"id":"star-github-fsharp-bioproviders","title":"BioProviders - .NET library for accessing and manipulating bioinformatic datasets","content":"BioProviders - .NET library for accessing and manipulating bioinformatic datasets \u2192 https://github.com/fsprojects/BioProviders","contentType":"star","url":"https://www.lqdev.me/responses/github-fsharp-bioproviders","date":"2023-01-24 10:08 -05:00","tags":[],"summary":"BioProviders - .NET library for accessing and manipulating bioinformatic datasets \u2192 https://github.com/fsprojects/BioProviders","keywords":["bioproviders","library","accessing","manipulating","bioinformatic","datasets","https://github","com/fsprojects/bioproviders"]},{"id":"bookmarks-ml-remote-sensing-datasets","title":"Remote Sensing Datasets Repo","content":"Remote Sensing Datasets Repo \u2192 https://github.com/satellite-image-deep-learning/remote-sensing-datasets","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/ml-remote-sensing-datasets","date":"2023-01-24 10:01 -05:00","tags":[],"summary":"Remote Sensing Datasets Repo \u2192 https://github.com/satellite-image-deep-learning/remote-sensing-datasets","keywords":["sensing","datasets","remote","repo","https://github","com/satellite","image","deep","learning/remote"]},{"id":"bookmarks-implement-q-a-documentation-gpt-willison","title":"How to implement Q\u0026A against your documentation with GPT3, embeddings and Datasette","content":"How to implement Q\u0026amp;A against your documentation with GPT3, embeddings and Datasette \u2192 http://simonwillison.net/2023/Jan/13/semantic-search-answers/#atom-everything Here\u0027s how to do this: Run a text search (or a semantic search, described later) against your documentation to find content that looks like it could be relevant to the user\u0027s question. Grab extracts of that content and glue them all together into a blob of text. Construct a prompt consisting of that text followed by \u0026quot;Given the above content, answer the following question: \u0026quot; and the user\u0027s question Send the whole thing through the GPT-3 API and see what comes back","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/implement-q-a-documentation-gpt-willison","date":"2023-01-19 14:19 -05:00","tags":[],"summary":"How to implement Q\u0026amp;A against your documentation with GPT3, embeddings and Datasette \u2192 http://simonwillison.net/2023/Jan/13/semantic-search-answers/#atom-everything Here\u0027s how to do this: Run a...","keywords":["content","search","text","\u0026quot","user","against","documentation","question","glue","together"]},{"id":"bookmarks-visual-guide-ssh-tunnels","title":"A Visual Guide to SSH Tunnels","content":"A Visual Guide to SSH Tunnels \u2192 https://iximiuz.com/en/posts/ssh-tunnels/","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/visual-guide-ssh-tunnels","date":"2023-01-18 10:52 -05:00","tags":[],"summary":"A Visual Guide to SSH Tunnels \u2192 https://iximiuz.com/en/posts/ssh-tunnels/","keywords":["visual","guide","tunnels","https://iximiuz","com/en/posts/ssh","tunnels/"]},{"id":"reshare-karpathy-lets-build-gpt-from-scratch-video","title":"Karpathy - Let\u0027s build GPT: from scratch, in code, spelled out","content":"Karpathy - Let\u0026#39;s build GPT: from scratch, in code, spelled out \u2192 https://www.youtube.com/watch?v=kCc8FmEb1nY","contentType":"reshare","url":"https://www.lqdev.me/responses/karpathy-lets-build-gpt-from-scratch-video","date":"2023-01-18 10:45 -05:00","tags":[],"summary":"Karpathy - Let\u0026#39;s build GPT: from scratch, in code, spelled out \u2192 https://www.youtube.com/watch?v=kCc8FmEb1nY","keywords":["karpathy","let\u0026#39","build","gpt:","from","scratch","code","spelled","https://www","youtube"]},{"id":"bookmarks-coyier-big-list-dev-designer-blogs-opml-file","title":"A Big Pile of Personal Developer \u0026 Designer Blogs in an OPML File","content":"A Big Pile of Personal Developer \u0026amp; Designer Blogs in an OPML File \u2192 https://chriscoyier.net/2023/01/05/a-big-pile-of-personal-developer-designer-blogs-in-an-opml-file/ OPML File: https://chriscoyier.s3.amazonaws.com/personal-developer-blogs.xml","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/coyier-big-list-dev-designer-blogs-opml-file","date":"2023-01-11 15:38 -05:00","tags":[],"summary":"A Big Pile of Personal Developer \u0026amp; Designer Blogs in an OPML File \u2192 https://chriscoyier.net/2023/01/05/a-big-pile-of-personal-developer-designer-blogs-in-an-opml-file/ OPML File:...","keywords":["developer","blogs","opml","pile","personal","designer","https://chriscoyier","\u0026amp","file","net/2023/01/05/a"]},{"id":"star-subscribe-wherever-you-get-your-content-nielsen","title":"Subscribe Wherever You Get Your Content","content":"Subscribe Wherever You Get Your Content \u2192 https://blog.jim-nielsen.com/2023/subscribe-wherever-you-get-your-content/ Wouldn\u2019t it be amazing if a similar phrase could enter the larger public consciousness for blogs or even people who you follow online? Instead of: \u201CFollow me on Twitter.\u201D \u201CI\u2019m @realChuckieCheese on Instagram.\u201D \u201CSubscribe to my videos on YouTube.\u201D You\u2019d hear some popular influencer saying: \u201CFollow me wherever you follow people online.\u201D \u201CFind me and subscribe wherever you get your content.\u201D In a world like this, perhaps apex domains could become the currency of online handles : follow me @jim-nielsen.com .","contentType":"star","url":"https://www.lqdev.me/responses/subscribe-wherever-you-get-your-content-nielsen","date":"2023-01-11 10:43 -05:00","tags":[],"summary":"Subscribe Wherever You Get Your Content \u2192 https://blog.jim-nielsen.com/2023/subscribe-wherever-you-get-your-content/ Wouldn\u2019t it be amazing if a similar phrase could enter the larger public...","keywords":["wherever","online","follow","subscribe","people","\u201Cfollow","content","nielsen","content/","videos"]},{"id":"reply-building-website-like-1999-in-2022-localghost","title":"Building a website like it\u0027s 1999... in 2022","content":"Building a website like it\u0026#39;s 1999... in 2022 \u2192 https://localghost.dev/blog/building-a-website-like-it-s-1999-in-2022/ Now the only challenges for me are: Should the song on my website match my ringtone? Where do I place the flames GIF?","contentType":"reply","url":"https://www.lqdev.me/responses/building-website-like-1999-in-2022-localghost","date":"2023-01-11 10:18 -05:00","tags":[],"summary":"Building a website like it\u0026#39;s 1999... in 2022 \u2192 https://localghost.dev/blog/building-a-website-like-it-s-1999-in-2022/ Now the only challenges for me are: Should the song on my website match my...","keywords":["website","1999","building","it\u0026#39","2022","https://localghost","dev/blog/building","2022/","challenges","are:"]},{"id":"reshare-bring-back-blogs-directory","title":"Bring Back Blogging Directory","content":"Bring Back Blogging Directory \u2192 https://bringback.blog/ If you\u0027re looking for new folks follow, here\u0027s a list of blogs on a variety of topics.","contentType":"reshare","url":"https://www.lqdev.me/responses/bring-back-blogs-directory","date":"2023-01-04 21:03 -05:00","tags":[],"summary":"Bring Back Blogging Directory \u2192 https://bringback.blog/ If you\u0027re looking for new folks follow, here\u0027s a list of blogs on a variety of topics.","keywords":["bring","back","blogging","directory","https://bringback","blog/","looking","folks","follow","here"]},{"id":"reshare-ces-intel-entry-level-n-series-processor","title":"Intel launches new entry-level Core i3 N-series mobile CPUs","content":"Intel launches new entry-level Core i3 N-series mobile CPUs \u2192 https://www.windowscentral.com/hardware/computers-desktops/intel-core-i3-n-series-launch-ces-2023 The article suggests these processors are for the education market. Hopefully a few of the devices with these processors in a Surface Go form-factor are made generally available as well. I think the N200 series is the sweet spot in terms of performance and battery life with only 6W of max turbo power. The i3 which draws a max of 15W may be too much. At that point, some of the newly announced U-series processors might make more sense.","contentType":"reshare","url":"https://www.lqdev.me/responses/ces-intel-entry-level-n-series-processor","date":"2023-01-04 00:28 -05:00","tags":[],"summary":"Intel launches new entry-level Core i3 N-series mobile CPUs \u2192 https://www.windowscentral.com/hardware/computers-desktops/intel-core-i3-n-series-launch-ces-2023 The article suggests these processors...","keywords":["series","processors","core","intel","generally","available","n200","sweet","spot","terms"]},{"id":"reply-personal-websites-and-online-identifies-hd","title":"Personal Websites and Online Identities","content":"Personal Websites and Online Identities \u2192 https://hacdias.com/2022/12/26/personal-websites-and-online-identities Great post. For me, I\u0027ve found POSSE to Twitter and Mastodon using RSS and IFTTT-like services has been a relatively low-effort thing to do. The downside though is any follow-up conversations take place on those platforms. That hasn\u0027t been an issue for me yet though so until it is, I\u0027ll continue to use my relatively low-effort solution. I agree with you on likes / reposts. My current implementation of those posts is focused on Webmentions, which means it\u0027s virtually meaningless since few people know about let alone integrate Webmentions into their website. I would argue replies tend to fall into that same space as well. I have however found great use in bookmark posts to which I try and add some content from the source document to help me quickly identfy why I thought that post / website was interesting or relevant to a specific topic at that time.","contentType":"reply","url":"https://www.lqdev.me/responses/personal-websites-and-online-identifies-hd","date":"2023-01-03 23:14 -05:00","tags":[],"summary":"Personal Websites and Online Identities \u2192 https://hacdias.com/2022/12/26/personal-websites-and-online-identities Great post. For me, I\u0027ve found POSSE to Twitter and Mastodon using RSS and IFTTT-like...","keywords":["though","posts","website","those","webmentions","effort","relatively","which","identities","websites"]},{"id":"bookmarks-nanoGPT","title":"nanoGPT - The simplest, fastest repository for training/finetuning medium-sized GPTs.","content":"nanoGPT - The simplest, fastest repository for training/finetuning medium-sized GPTs. \u2192 https://github.com/karpathy/nanoGPT The simplest, fastest repository for training/finetuning medium-sized GPTs. It\u0027s a re-write of minGPT, which I think became too complicated, and which I am hesitant to now touch. Still under active development, currently working to reproduce GPT-2 on OpenWebText dataset. The code itself aims by design to be plain and readable: train.py is a ~300-line boilerplate training loop and model.py a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That\u0027s it.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/nanoGPT","date":"2023-01-03 22:35 -05:00","tags":[],"summary":"nanoGPT - The simplest, fastest repository for training/finetuning medium-sized GPTs. \u2192 https://github.com/karpathy/nanoGPT The simplest, fastest repository for training/finetuning medium-sized GPTs....","keywords":["which","simplest","fastest","repository","training/finetuning","medium","sized","gpts","~300","model"]},{"id":"reshare-petals-ml-model-inference","title":"Petals - Easy way to run 100B\u002B language models at home","content":"Petals - Easy way to run 100B\u002B language models at home \u2192 https://petals.ml/ Run 100B\u002B language models at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading","contentType":"reshare","url":"https://www.lqdev.me/responses/petals-ml-model-inference","date":"2023-01-03 22:34 -05:00","tags":[],"summary":"Petals - Easy way to run 100B\u002B language models at home \u2192 https://petals.ml/ Run 100B\u002B language models at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading","keywords":["100b\u002B","language","models","home","petals","easy","https://petals","bittorrent","style","fine"]},{"id":"star-bring-back-personal-blogging","title":"Bring Back Personal Blogging","content":"Bring Back Personal Blogging \u2192 https://www.theverge.com/23513418/bring-back-personal-blogging In the beginning, there were blogs, and they were the original social web. We built community. We found our people. We wrote personally. We wrote frequently. We self-policed, and we linked to each other so that newbies could discover new and good blogs. The biggest reason personal blogs need to make a comeback is a simple one: we should all be in control of our own platforms. People built entire communities around their favorite blogs, and it was a good thing. You could find your people, build your tribe, and discuss the things your collective found important. Buy that domain name. Carve your space out on the web. Tell your stories, build your community, and talk to your people. It doesn\u2019t have to be big. It doesn\u2019t have to be fancy. You don\u2019t have to reinvent the wheel. It doesn\u2019t need to duplicate any space that already exists on the web \u2014 in fact, it shouldn\u2019t. This is your creation. It\u2019s your expression. It should reflect you.","contentType":"star","url":"https://www.lqdev.me/responses/bring-back-personal-blogging","date":"2023-01-03 22:20 -05:00","tags":[],"summary":"Bring Back Personal Blogging \u2192 https://www.theverge.com/23513418/bring-back-personal-blogging In the beginning, there were blogs, and they were the original social web. We built community. We found...","keywords":["blogs","people","personal","doesn\u2019t","good","back","blogging","space","build","need"]},{"id":"reshare-openai-text-embedding-ada-002","title":"OpenAI Text Embedding Ada 002 Model","content":"OpenAI Text Embedding Ada 002 Model \u2192 https://openai.com/blog/new-and-improved-embedding-model/ The new model, text-embedding-ada-002 , replaces five separate models for text search, text similarity, and code search, and outperforms our previous most capable model, Davinci, at most tasks, while being priced 99.8% lower.","contentType":"reshare","url":"https://www.lqdev.me/responses/openai-text-embedding-ada-002","date":"2023-01-02 13:48 -05:00","tags":[],"summary":"OpenAI Text Embedding Ada 002 Model \u2192 https://openai.com/blog/new-and-improved-embedding-model/ The new model, text-embedding-ada-002 , replaces five separate models for text search, text similarity,...","keywords":["text","embedding","model","search","openai","while","tasks","davinci","capable","previous"]},{"id":"bookmarks-swyx-favorite-podcasts-2022","title":"SWYX - Fave Podcasts 2022","content":"SWYX - Fave Podcasts 2022 \u2192 https://www.swyx.io/fave-podcasts-2022/","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/swyx-favorite-podcasts-2022","date":"2023-01-02 12:44 -05:00","tags":[],"summary":"SWYX - Fave Podcasts 2022 \u2192 https://www.swyx.io/fave-podcasts-2022/","keywords":["swyx","podcasts","fave","2022","https://www","io/fave","2022/"]},{"id":"bookmarks-used-thinkpad-buyers-guide","title":"ThinkPad Buyer\u0027s Guide","content":"ThinkPad Buyer\u0026#39;s Guide \u2192 https://www.bobble.tech/free-stuff/used-thinkpad-buyers-guide","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/used-thinkpad-buyers-guide","date":"2022-12-29 16:44 -05:00","tags":[],"summary":"ThinkPad Buyer\u0026#39;s Guide \u2192 https://www.bobble.tech/free-stuff/used-thinkpad-buyers-guide","keywords":["thinkpad","guide","buyer\u0026#39","https://www","bobble","tech/free","stuff/used","buyers"]},{"id":"star-miessler-time-get-back-rss","title":"It\u2019s Time to Get Back Into RSS","content":"It\u2019s Time to Get Back Into RSS \u2192 https://danielmiessler.com/blog/its-time-to-get-back-into-rss/ We all mourned when Reader died and took RSS with it, but it\u0027s time to return to what made it great","contentType":"star","url":"https://www.lqdev.me/responses/miessler-time-get-back-rss","date":"2022-12-28 10:50 -05:00","tags":[],"summary":"It\u2019s Time to Get Back Into RSS \u2192 https://danielmiessler.com/blog/its-time-to-get-back-into-rss/ We all mourned when Reader died and took RSS with it, but it\u0027s time to return to what made it great","keywords":["back","it\u2019s","https://danielmiessler","com/blog/its","rss/","mourned","reader","died","took","return"]},{"id":"reshare-umbrel-personal-server-os-selfhosting","title":"Umbrel - Personal Server OS","content":"Umbrel - Personal Server OS \u2192 https://umbrel.com/ Really cool project. Don\u0027t really care for the crypto stuff but the rest looks very interesting.","contentType":"reshare","url":"https://www.lqdev.me/responses/umbrel-personal-server-os-selfhosting","date":"2022-12-28 10:27 -05:00","tags":[],"summary":"Umbrel - Personal Server OS \u2192 https://umbrel.com/ Really cool project. Don\u0027t really care for the crypto stuff but the rest looks very interesting.","keywords":["really","umbrel","personal","server","https://umbrel","com/","cool","project","care","crypto"]},{"id":"bookmarks-e2eml-transformers-from-scratch","title":"Transformers from Scratch","content":"Transformers from Scratch \u2192 https://e2eml.school/transformers.html","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/e2eml-transformers-from-scratch","date":"2022-12-28 10:03 -05:00","tags":[],"summary":"Transformers from Scratch \u2192 https://e2eml.school/transformers.html","keywords":["transformers","from","scratch","https://e2eml","school/transformers","html"]},{"id":"star-matrix-holiday-edition-2022","title":"Matrix Holiday Update","content":"Matrix Holiday Update \u2192 https://matrix.org/blog/2022/12/25/the-matrix-holiday-update-2022 These updates are exciting! Reddit appears to be building out new Chat functionality using Matrix Discourse is working on adding Matrix support Thunderbird launched Matrix support. Automattic is busy building Matrix plugins for Wordpress Not as exciting ...only a handful of these initiatives have resulted in funding reaching the core Matrix team. This is directly putting core Matrix development at risk. In short: folks love the amazing decentralised encrypted comms utopia of Matrix. But organisations also love that they can use it without having to pay anyone to develop or maintain it. This is completely unsustainable , and Element is now literally unable to fund the entirety of the Matrix Foundation on behalf of everyone else - and has had to lay off some of the folks working on the core team as a result. In the interim, if you are an organisation who\u2019s building on Matrix and you want the project to continue to flourish, please mail funding@matrix.org to discuss how you can support the foundations that you are depending on. I\u0027m looking forward to the future of the Matrix protocol, especially the P2P components .","contentType":"star","url":"https://www.lqdev.me/responses/matrix-holiday-edition-2022","date":"2022-12-27 21:34 -05:00","tags":[],"summary":"Matrix Holiday Update \u2192 https://matrix.org/blog/2022/12/25/the-matrix-holiday-update-2022 These updates are exciting! Reddit appears to be building out new Chat functionality using Matrix Discourse...","keywords":["matrix","core","building","support","holiday","update","team","exciting","love","folks"]},{"id":"bookmarks-appropedia-sustaibility-wiki","title":"Appropedia - Sustainability Wiki","content":"Appropedia - Sustainability Wiki \u2192 https://www.appropedia.org/Welcome_to_Appropedia","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/appropedia-sustaibility-wiki","date":"2022-12-27 14:54 -05:00","tags":[],"summary":"Appropedia - Sustainability Wiki \u2192 https://www.appropedia.org/Welcome_to_Appropedia","keywords":["appropedia","sustainability","wiki","https://www","org/welcome"]},{"id":"notes-mastodon-prune-profiles-media-vnext","title":"Removing avatars and headers in Mastodon","content":"The next version of Mastodon can\u0027t come soon enough. About 20GB of disk space on my instance are being taken up by media from remote users. And this is for a single user instance! I can\u0027t imagine how it must be on other instances with multiple accounts. A recent PR should make things much better though by allowing you to remove these files. The only thing you need to do is pass the --prune-profiles flag to the media remove CLI command. The PR is already merged so hopefully that means it\u0027s coming the next release.","contentType":"notes","url":"https://www.lqdev.me/notes/mastodon-prune-profiles-media-vnext","date":"2022-12-26 17:00 -05:00","tags":["mastodon","fediverse","selfhost","sysadmin"],"summary":"The next version of Mastodon can\u0027t come soon enough. About 20GB of disk space on my instance are being taken up by media from remote users. And this is for a single user instance! I can\u0027t imagine how...","keywords":["next","remove","media","instance","better","though","allowing","files","thing","need"]},{"id":"notes-upgraded-mastodon-v4-0-2","title":"Upgraded to Mastodon v4.0.2","content":"Upgrading my server to 4.0.2 wasn\u0027t bad, especially considering I was coming from a 3.x version. Also posting this to make sure that there have been no changes to the API and my website can still post to Mastodon on my behalf.","contentType":"notes","url":"https://www.lqdev.me/notes/upgraded-mastodon-v4-0-2","date":"2022-12-26 16:03 -05:00","tags":["mastodon","selfhost","fediverse","upgade","sysadmin"],"summary":"Upgrading my server to 4.0.2 wasn\u0027t bad, especially considering I was coming from a 3.x version. Also posting this to make sure that there have been no changes to the API and my website can still...","keywords":["upgrading","server","wasn","especially","considering","coming","from","version","also","posting"]},{"id":"notes-fitch-fsharp-cli-v0","title":"Fitch: CLI system information utility built with F#","content":"I wasn\u0027t really planning on this, but here we are. I\u0027ve been using Neofetch on my PC to display system information whenever the terminal opens. However, in some cases, that would hang and it would take a while before I was able to use my terminal. I then ran into Nitch which is simple and fast. However, it was missing the local and public IP information Neofetch provides out of the box. Nitch is written using the Nim programming language. Originally, I had planned on extending Nitch and I figured out the code I needed to write to support IP information. However, my program wouldn\u0027t build and I was unable to find any information online to unblock myself. That\u0027s when I looked at the existing components in Nitch and found that it was basically just reading files from the /proc and /etc system directories. At that point, I decided to see if I could build my own utility using F#. A couple of hours later and the result was Fitch . This works well enough for my needs and in the future, I might publish it as a dotnet global tool to make it easy for others to use.","contentType":"notes","url":"https://www.lqdev.me/notes/fitch-fsharp-cli-v0","date":"2022-12-25 20:10 -05:00","tags":["dotnet","fsharp","linux","system","utilities","cli","fitch"],"summary":"I wasn\u0027t really planning on this, but here we are. I\u0027ve been using Neofetch on my PC to display system information whenever the terminal opens. However, in some cases, that would hang and it would...","keywords":["nitch","information","using","however","build","neofetch","system","terminal","/etc","/proc"]},{"id":"reshare-personal-rss-aggregators-udell","title":"Jon Udell: Personal RSS aggregators","content":"Jon Udell: Personal RSS aggregators \u2192 https://jonudell.info/bytecols/2002-05-15.html","contentType":"reshare","url":"https://www.lqdev.me/responses/personal-rss-aggregators-udell","date":"2022-12-25 16:29 -05:00","tags":[],"summary":"Jon Udell: Personal RSS aggregators \u2192 https://jonudell.info/bytecols/2002-05-15.html","keywords":["udell:","personal","aggregators","https://jonudell","info/bytecols/2002","html"]},{"id":"bookmarks-comprehensive-rust-course","title":"Comprehensive Rust Course","content":"Comprehensive Rust Course \u2192 https://google.github.io/comprehensive-rust/ This is a four day Rust course developed by the Android team. The course covers the full spectrum of Rust, from basic syntax to advanced topics like generics and error handling. It also includes Android-specific content on the last day.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/comprehensive-rust-course","date":"2022-12-22 11:37 -05:00","tags":[],"summary":"Comprehensive Rust Course \u2192 https://google.github.io/comprehensive-rust/ This is a four day Rust course developed by the Android team. The course covers the full spectrum of Rust, from basic syntax...","keywords":["rust","course","android","comprehensive","specific","includes","also","handling","error","generics"]},{"id":"notes-azure-cdn-regex-url-redirect","title":"Use Azure CDN from Verizon Premium for Regex URL redirects","content":"Well, that\u0027s time that I\u0027m never going to get back. Imagine that you wanted to redirect a URL, From: https://mydomain/github/repo-name To: https://github.com/username/repo-name You could use regular expressions to parse out the repo-name part of the URL path from the source URL to the destination URL. This is what it might look like using something like NGINX. location ~ ^/github/(.*) { return 307 https://github.com/yourGHusername/$1; } Unless I wasn\u0027t looking at the right documentation, it\u0027s not obvious that the rules engine for most Azure CDN plans don\u0027t support this feature, except for the Verizon Premium . While the standard rules engine supports wildcards for source URLs, lack of regular expresssions support means there\u0027s no way to parameterize the values of the destination URL. If this is something you need, make sure to use the Verizon Premium plan.","contentType":"notes","url":"https://www.lqdev.me/notes/azure-cdn-regex-url-redirect","date":"2022-12-21 15:09 -05:00","tags":["azure","azurecdn","redirect","indieweb","selfhosting"],"summary":"Well, that\u0027s time that I\u0027m never going to get back. Imagine that you wanted to redirect a URL, From: https://mydomain/github/repo-name To: https://github.com/username/repo-name You could use regular...","keywords":["rules","engine","support","https://github","destination","regular","source","verizon","premium","never"]},{"id":"star-pine64-announces-pinetab-2-linux","title":"PINE64 Announces the PineTab2 Linux Tablet","content":"PINE64 Announces the PineTab2 Linux Tablet \u2192 https://9to5linux.com/pine64-announces-the-pinetab2-linux-tablet-with-up-to-8gb-ram-and-rk3566-soc","contentType":"star","url":"https://www.lqdev.me/responses/pine64-announces-pinetab-2-linux","date":"2022-12-21 01:20 -05:00","tags":[],"summary":"PINE64 Announces the PineTab2 Linux Tablet \u2192 https://9to5linux.com/pine64-announces-the-pinetab2-linux-tablet-with-up-to-8gb-ram-and-rk3566-soc","keywords":["announces","pinetab2","linux","tablet","pine64","https://9to5linux","com/pine64","rk3566"]},{"id":"posts-receive-webmentions-fsharp-az-functions-fsadvent","title":"Accept Webmentions using F#, Azure Functions, and RSS","content":"# Introduction \u003E This post is part of [F# Advent 2022](https://sergeytihon.com/2022/10/28/f-advent-calendar-in-english-2022/). Thanks to [Sergey Tihon](https://sergeytihon.com) for organizing it. While not planned this way, this post is timely considering recent developments in the social media space. While Twitter is in the headlines, it\u0027s not the only time people have questioned their engagement and relationship with these social media platforms. In the context of Twitter though, users have been looking for alternative platforms to best serve their communities. While Mastodon and the Fediverse appear to be one of those alternatives, they\u0027re not the only one. Blogosphere platforms like Tumblr and WordPress, both owned by Automattic, are also positioning themselves as viable alternatives to the point they\u0027ve [considered the idea of supporting ActivityPub](https://techcrunch.com/2022/11/21/tumblr-to-add-support-for-activitypub-the-social-protocol-powering-mastodon-and-other-apps/). ActivityPub is the protocol that powers many of the platforms on the Fediverse. However, it\u0027s not the only open protocol that enables communication and engagement on the open web. Webmentions is another protocol for enabling conversations on the web which I\u0027ll talk about more in this post. I\u0027ll also discuss how I used F#, Azure Functions, and RSS to implement a solution that helps me and my website engage in conversations on the open web thanks to Webmentions. ## What are Webmentions? First, let\u0027s start by talking a bit about Webmentions. According to the W3C specification, Webmentions are \u0022...a simple way to notify any URL when you mention it on your site. From the receiver\u0027s perspective, it\u0027s a way to request notifications when other sites mention it.\u0022. One of the goals according to the IndieWeb wiki is to enable cross-site conversations. Here\u0027s how a potential Webmention workflow might look like: \u0060\u0060\u0060mermaid sequenceDiagram autonumber actor A as Alice participant AS as Alice\u0027s \u0022Website\u0022 participant AW as Alice\u0027s Webmention Service participant BS as Bob\u0027s \u0022Website\u0022 actor B as Bob A -\u003E\u003E AS: Publishes post B -\u003E\u003E AS: Reads post B -\u003E\u003E BS: Publishes post with link to Alice\u0027s post rect rgb(230,230,230) alt Automated BS -\u003E\u003E AS: Fetch Webmention endpoint BS -\u003E\u003E AW: Sends Webmention else Manual B -\u003E\u003E AS: Fetch Webmention endpoint B -\u003E\u003E AW: Sends Webmention end end rect rgb(230,230,230) note over AW:Validates Webmention request note over AW: Validates Webmention note over AW: Accepts / processes Webmention alt Automated AW -\u003E\u003E BS: Sends back response else Manual AW -\u003E\u003E B: Sends back response end end opt Display Webmentions AS -\u003E\u003E AW: Fetch Webmentions note over AS: Display / publish Webmentions end \u0060\u0060\u0060 I use the term website here very loosely because it\u0027s not only limited to your own personal website. You can technically use Webmentions through any public website including social media platforms like Twitter, Mastodon, or even GitHub. This post will focus on the middle blocks consisting of sending and receiving Webmentions. For more information, check out the [specification](https://www.w3.org/TR/webmention/#abstract-p-1) and [Webmentions page](https://indieweb.org/Webmention) on the IndieWeb wiki. ## My webmention solution In September of this year, I [integrated a partial implementation of Webmentions](/notes/webmentions-partially-implemented/) into my website. This implementation only allowed me to send webmentions as part of my publishing process. If you\u0027re interested in the details, it\u0027s based on the blog post I wrote for last year\u0027s F# Advent, [Sending Webmentions with F#](/posts/sending-webmentions-fsharp-fsadvent/). A couple of days ago, I [successfully deployed and integrated](/notes/now-accepting-webmentions) the other part of the implementation which allowed me to receive webmentions. While there are many great [services](https://indieweb.org/Webmention#Publisher_Services) which will do this for you, I decided to roll my own for fun and to learn something new. To help with that, I released [WebmentionFs](https://www.nuget.org/packages/lqdev.WebmentionFs), an open-source .NET library written in F# with components for sending and receiving Webmentions. The backend service I use to receive webmentions is built on Azure Functions. If you\u0027re interested, here\u0027s the source code for [WebmentionFs](https://github.com/lqdev/WebmentionFs) and my [Webmention service](https://github.com/lqdev/WebmentionService). Before diving into the technical details of the solution, I\u0027ll first discuss some of the priorities driving my design decisions. ### Requirements While I don\u0027t want to trivialize it, at the end of the day, the Webmention service is just an endpoint that accepts HTTP POST requests. That means there\u0027s many ways to build it. However, here were the requirements important to me that guided my design choices: - **Cost:** The solution should be low cost - **Maintenance:** The solution should be easy to maintain - **Easy to consume:** I have multiple ways to view and access my webmentions. - **Notifications first:** Since I don\u0027t own or have responsibility over content that\u0027s not mine, prioritize Webmentions for notifications, not content. With that in mind, I ended up choosing a solution built on Azure Functions to receive Webmentions and publish them as an RSS feed. ### Why Azure Functions? I don\u0027t make money from my website. It\u0027s a labor of love in which I document my experiences. Selfishly, it\u0027s for me to remind myself how to solve a problem or share something that was important to me at that time. If my posts end up helping others solve their own problems, that\u0027s a side effect and a reward unto itself. To that effect, I want my website to be as low-cost to run as possible. One way I achieve that is by using static hosting. This dramatically reduces the cost associated with my site. If you\u0027re interested in how this site is built, you can check out the [colophon](/colophon). So where do Azure Functions come in? Azure Functions is a serverless solution which means there are no servers! (I kid :smirk:) #### Cost :heavy_check_mark: As a serverless solutions, it means web services are not constantly running. They only run whenever there is something to process so I only pay for what I need. That takes care of my low-cost requirement since processing Webmentions is not a long-running, resource-intensive operation that can easily run in a serverless environment. #### Maintenance :heavy_check_mark: Cost is not only monetary. There are also time costs associated with maintaining a solution. Because there is no infrastructure to maintain, that\u0027s not really a factor I need to consider. Instead I get to focus on writing the logic for accepting and processing Webmentions. Azure Functions provides a set of [prebuilt trigger and binding components](https://learn.microsoft.com/azure/azure-functions/functions-triggers-bindings) to reduce the amount of code I have to write to handle things like accepting HTTP requests or storing data in a table or blob storage. Also, in the rare instance that this blog post breaks the internet, Azure Functions enables my service to automatically handle that scale. Don\u0027t believe me? Check out this post frorm [Troy Hunt](https://www.troyhunt.com/) on [Have I Been Pwned usage of Azure Functions](https://www.troyhunt.com/serverless-to-the-max-doing-big-things-for-small-dollars-with-cloudflare-workers-and-azure-functions/). ### Why RSS? The choice of how to display and consume Webmentions is entirely up to you. At regular intervals, my Webmention service generates an RSS feed and publishes it to a private Azure Blob Storage. Here are some of the reasons why: #### Easy to consume :heavy_check_mark: The decision to use RSS comes from the fact that like Webmentions, it\u0027s an open protocol that gives choice to creators and consumers. Check out my post, [Rediscovering the RSS protocol](/posts/rediscovering-rss-user-freedom) for more of my thoughts on that. Despite the appearance of decline in use, it\u0027s a protocol that\u0027s been around for over 20 years and it\u0027s one of the many reasons why podcasts continue to be one of the more open platforms despite some attempts to lock down the ecosystem. With that it mind, by choosing RSS as the publishing format for my Webmentions feed, I can lean on the vast number of RSS readers ([NewsBlur](http://newsblur.com/) and [Elfeed](https://github.com/skeeto/elfeed) being my favorites) out there instead of having to write a custom UI. Not happy with my RSS reader? I can just add my feed to whichever other reader I move to and continue consuming my Webmentions without skipping a beat. ### Notifications first :heavy_check_mark: While not technical or specific to RSS, this is a topic I wrestled with as part of my implementation. However, choosing RSS made many parts of it simpler. Who owns your content? If you publish on your own website or a platform under your control, chances are you do. If I publish and redistribute your content on my website though, what parts do I own and which parts do you? I\u0027m not sure. What about spam, harrassment, and lewd content? I\u0027d have to put in place some sort of moderation instead of publishing directly on my site. I didn\u0027t want to have to deal with any of these questions so choosing to publish to a private RSS feed instead of directly on my site made it so Webmentions are for my eyes only. More importantly, I only capture the source and target links, not the content. Therefore, I don\u0027t store or redistribute your content. Now there\u0027s nothing stopping anyone from seeing your post since it\u0027s public but in cases where your site\u0027s content is not something I\u0027m comfortable with, I\u0027m not resharing or redistributing that content. If my RSS reader chooses to display the full contents of your post by using the \u0060link\u0060 element from the RSS feed, since your post is public, that\u0027s no different than me viewing the original post on your site. If at some point I feel differently about it, the changes to capture the content in the \u0060description\u0060 happen at the RSS file level and because the RSS is private, I\u0027m still not redistributing your content. In short, with my implementation I chose to prioritize Webmentions for notification purposes rather than building a commenting and interaction (like, reshare) system for my website. Now that I\u0027ve talked about the what and the why, in the next sections I\u0027ll get into the how. ## Sending Webmentions Once you\u0027ve created a post linking to someone else\u0027s website, sending webmentions is a two-step process: 1. URL discovery of webmention endpoint 2. Send webmention These steps can be automated or performed manually. ### URL discovery of Webmention endpoint The first thing you\u0027ll need is the endpoint where you\u0027ll be sending your Webmention to. According to the Webmention specification, a Webmention endpoint can be found in one of three places: - HTTP response headers - \u0060 \u0060 tag with \u0060rel=webmention\u0060 attribute - \u0060 \u0060 tag with \u0060rel=me\u0060 attribute When this process is automated, the discovery is performed in order and each subsequent option works as a fallback of the other. When done manually, either the website author will publish and advertise this on their website or you can inspect the website\u0027s source code to find it. ### Send webmention To send a Webmention, you need to send an HTTP POST request with an \u0060x-www-form-urlencoded\u0060 body containing two properties: \u0060source\u0060 and \u0060target\u0060. Source is the URL of your article and target is the URL of the original article. Again, this too can either be automated or done manually with tools like cURL, Insomnia, or any other HTTP client. ### Sending Webmentions in practice Let\u0027s use a redundant example to walk through the workflow. Pretend that you created or commented on a GitHub issue and added a link to one of my articles, [Sending Webmentions with F#](/posts/sending-webmentions-fsharp-fsadvent/) for example. You then wanted to notify me either to get my attention on that issue or just as an FYI. Now I say this is a redundant example because GitHub already has notifications. However, what this does illustrate are the cross-site capabilities of Webmentions. ![Image of GitHub Issue Post](http://cdn.lqdev.tech/files/images/send-gh-issue.png) At this point, if GitHub implemented Webmentions, it would automatically: 1. Fetch my webpage and look for my Webmention endpoint. 1. Compose and send the HTTP POST request. Sadly, today that\u0027s not the case so we\u0027ll walk through the workflow manually. On my website, I include a \u0060link\u0060 tag with the \u0060rel=webmention\u0060 attribute that looks like the following: \u0060\u0060\u0060html \u0060\u0060\u0060 My current Webmention endpoint is *https://lqdevwebmentions.azurewebsites.net/api/inbox*. Now that you have the Webmention endpoint, you could use any tool of your choice to create and send the HTTP POST request. I have made it simpler though and included a form on my website for each of my posts for this purpose. ![luisquintanilla.me send Webmention form](http://cdn.lqdev.tech/files/images/send-webmention-form.png) The source code looks like the following: \u0060\u0060\u0060html window.onload = function() { document.getElementById(\u0027webmention-target\u0027).value = window.location.href } Send me a webmention \u0060\u0060\u0060 Now to send the webmention, navigate to the post you want to send a Webmention to. In this case, it\u0027s [Sending Webmentions with F#](/posts/sending-webmentions-fsharp-fsadvent/). Then, paste the link to the GitHub issue that contains the link to my site into the post\u0027s text box and hit \u0022Send\u0022. ![luisquintanilla.me send Webmention form filled-in](http://cdn.lqdev.tech/files/images/send-webmention-form-populated.png) At that point, the request is sent to my Webmention service which receives and processes the request and responds with a message whether the request was successful or not. That\u0027s all there is to it! Before moving the the receiving process though, I\u0027ll briefly show how to send Webmentions using WebmentionFs. ### Sending Webmentions with WebmentionFs In WebmentionFs, you can use the [\u0060UrlDiscoveryService\u0060](https://github.com/lqdev/WebmentionFs/blob/main/UrlDiscoveryService.fs) and [\u0060WebmentionSenderService\u0060](https://github.com/lqdev/WebmentionFs/blob/main/WebmentionSenderService.fs) to send Webmentions. If you wanted to integrate sending Webmentions into your application or service, here\u0027s an example that shows how you\u0027d use those components. \u0060\u0060\u0060fsharp open System open WebmentionFs open WebmentionFs.Services let ds = new UrlDiscoveryService() let ws = new WebmentionSenderService(ds) let data = { Source = new Uri(\u0022https://twitter.com/ljquintanilla/status/1603602055435894784\u0022) Target = new Uri(\u0022/feed/mastodon-hashtag-rss-boffosocko\u0022) } ws.SendAsync(data) |\u003E Async.AwaitTask |\u003E Async.RunSynchronously \u0060\u0060\u0060 The main part to pay attention to is the \u0060SendAsync\u0060 function. This uses the \u0060UrlDiscoveryService\u0060 to perform the Webmention endpoint discovery and send the HTTP POST request with the provided data. ## Receiving webmentions Now that you know how to send webmentions, let\u0027s talk about what happens on the receiver end. At its core, it\u0027s about a three-step process: 1. Validate request 1. Validate Webmention 1. Process Webmention ### Validate request Before processing the Webmention, you want to make sure that the request is valid. To do so there are a few checks you perform: 1. Is the protocol valid? Check that the protocol for the source and target URLs in the request body are HTTP or HTTPS. 1. Are the URLs the same? Check that the source and target URLS in the request body are NOT the same. 1. Is the target URL a valid resource? This is more loosely defined and up to you. In my case I defined valid as: - I own the domain - The target URL doesn\u0027t return an HTTP 400 or 500 error. If the request is not valid, nothing is done with the Webmention and an error returns. Otherwise, it moves to the next step which is Webmention validation. ### Validate Webmention In the base case, Webmention validation just means you parse through the source document and make sure that the target URL is included in it. However, if you wanted to get more complex and take into account specific types of interactions such as likes, replies, reposts, bookmarks, etc. you can parse through the document looking for [microformat](http://microformats.org/wiki/h-entry#Core_Properties) annotations. For more information on these annotations, see the [Types of Posts](https://indieweb.org/posts#Types_of_Posts) or [reply](https://indieweb.org/reply#Post_a_reply) pages in the IndieWeb Wiki. If the target link is NOT in the source document, you return an error. Otherwise, you move to the next step of processing the Webmention ### Processing Webmentions This step is optional and loosely defined. Once you\u0027ve validated the request and Webmention, you can do with it as you please. One thing you can do is publish it (after moderation) on your website. In my case, I\u0027m storing it in an Azure Tables database. Then, every morning, I generate an RSS feed of all my webmentions and save it to a private container on Azure Blob Storage. ## Reciving Webmentions with WebmentionFs To receive Webmentions, I use the [\u0060RequestValidationService\u0060](https://github.com/lqdev/WebmentionFs/blob/main/RequestValidationService.fs), [\u0060WebmentionValidationService\u0060](https://github.com/lqdev/WebmentionFs/blob/main/WebmentionValidationService.fs) and [\u0060WebmentionReceiverService\u0060](https://github.com/lqdev/WebmentionFs/blob/main/WebmentionReceiverService.fs) inside my Azure Function. The source code for my Azure Function is available in the [WebmentionService GitHub repo](https://github.com/lqdev/WebmentionService). The main components are in the \u0060Startup.fs\u0060 and \u0060ReceiveWebmention.fs\u0060 files. The [\u0060Startup.fs\u0060](https://github.com/lqdev/WebmentionService/blob/main/Startup.fs) files registers each of the services I\u0027ll use to to receive Webmentions. By doing so, I define the initialization logic for use later in my application using dependency injection. The main thing to note here is that the \u0060WebmentionReceiverService\u0060 is initialized with instances of a \u0060RequestValidationService\u0060 and \u0060WebmentionValidationService\u0060. For the \u0060RequestValidationService\u0060, it\u0027s initialized with a list of host names. In this case, I\u0027m passing it in as a comma-delimited environment variable that looks something like \u0022lqdev.me,luisquintanilla.me\u0022. This is what the \u0060RequestValidationService\u0060 uses to check whether I own a domain. \u0060\u0060\u0060fsharp // Add request validation service builder.Services.AddScoped (fun _ -\u003E let hostNames = Environment.GetEnvironmentVariable(\u0022PERSONAL_WEBSITE_HOSTNAMES\u0022) let hostNameList = hostNames.Split(\u0027,\u0027) new RequestValidationService(hostNameList)) |\u003E ignore // Add webmention validation service builder.Services.AddScoped () |\u003E ignore // Add receiver service builder.Services.AddScoped ,WebmentionReceiverService\u003E(fun (s:IServiceProvider) -\u003E let requestValidationService = s.GetRequiredService () let webmentionValidationService = s.GetRequiredService () new WebmentionReceiverService(requestValidationService,webmentionValidationService)) |\u003E ignore \u0060\u0060\u0060 Then, in the [\u0060ReceiveWebmention.fs\u0060](https://github.com/lqdev/WebmentionService/blob/main/ReceiveWebmention.fs) file, I inject my \u0060WebmentionReceiverService\u0060 into the constructor and add all the logic for running my serverless function in the \u0060Run\u0060 function. In this snippet, some code is ommitted for brevity. \u0060\u0060\u0060fsharp type ReceiveWebmention (webmentionReceiver: IWebmentionReceiver ) = ///... [ ] member x.Run ([ ] req: HttpRequest) ([ ] t: TableClient) (log: ILogger) = task { log.LogInformation(\u0022Processing webmention request\u0022) let! validationResult = x.WebmentionReceiver.ReceiveAsync(req) let response = match validationResult with | ValidationSuccess m -\u003E let entity = mapMentionToTableEntity m try t.AddEntity(entity) |\u003E ignore OkObjectResult(\u0022Webmention processed successfully\u0022) :\u003E IActionResult with | ex -\u003E log.LogError($\u0022{ex}\u0022) BadRequestObjectResult($\u0022Error processing webmention. Webmention already exists\u0022) :\u003E IActionResult | ValidationError e -\u003E log.LogError(e) BadRequestObjectResult(e) :\u003E IActionResult return response } \u0060\u0060\u0060 The \u0060Run\u0060 function is triggered whenever an HTTP POST request is made to my Webmention endpoint. It then calls \u0060ReceiveAsync\u0060 on the \u0060WebmentionReceiverService\u0060 which under the hood uses the \u0060RequestValidationService\u0060 and \u0060WebmentionValidationService\u0060 for validation. For reference, here\u0027s the code that does that in WebmentionFS. \u0060\u0060\u0060fsharp member x.ReceiveAsync (data:UrlData) = task { let! requestValidationResult = x.RequestValidationService.ValidateAsync data match requestValidationResult with | RequestSuccess r -\u003E let! webmentionValidationResult = x.WembentionValidationService.ValidateAsync r.Source r.Target let (result:ValidationResult ) = match webmentionValidationResult with | AnnotatedMention m -\u003E ValidationSuccess {RequestBody = r; Mentions = m} | UnannotatedMention -\u003E ValidationSuccess { RequestBody = r Mentions = { IsBookmark = false IsLike = false IsReply = false IsRepost = false } } | MentionError e -\u003E ValidationError e return result | RequestError e -\u003E return ValidationError e } \u0060\u0060\u0060 \u003E A quick side-note. I love how elegant function composition in F# makes the request validation pipeline look. See for yourself :slightly_smiling_face: \u003E \u003E \u0060\u0060\u0060fsharp \u003Elet validateAsync = \u003E isProtocolValid \u003E\u003E isSameUrl \u003E\u003E isTargetUrlValidAsync \u003E\u0060\u0060\u0060 Inside my \u0060Run\u0060 function, if the request and Webmention validation are successful, I use the Azure Table Storage table client \u0060AddEntity\u0060 method to save the Webmention to a table. One thing to note is that \u0060AddEntity\u0060 ensures that an entry is only added once. If an entry already exists in the table, it returns an error. This is what the GitHub Webmention we sent looks like in the table. ![Webmentions table on Azure Table Storage](http://cdn.lqdev.tech/files/images/webmentions-azure-table.png) In this case, the \u0060PartitionKey\u0060 is the target URL and \u0060RowKey\u0060 is the source URL. The fact all other columns are set to false indicates this is an untagged mention with no special microformats annotations. Annotating your posts appropriately would set one or more of those columns to true. Now that we went over how I receive Webmentions, I\u0027ll show how I prepare my Webmentions for consumption. ## Viewing Webmentions I consume my Webmentions using RSS feeds and RSS readers. To do so, I have an Azure Function called [\u0060WebmentionToRss\u0060](https://github.com/lqdev/WebmentionService/blob/main/WebmentionToRss.fs) that triggers every day at 3 AM. This function queries the Azure Table Storage table containing my Webmentions and creates an RSS feed that\u0027s then stored to Azure Blob Storage. Like the other function to process my Webmentions, I: Register the \u0060RssService\u0060 responsible for generating the RSS feed in \u0060Startup.fs\u0060. \u0060\u0060\u0060fsharp // Add RSS service builder.Services.AddScoped () |\u003E ignore \u0060\u0060\u0060 Inject the service into my \u0060WebmentionToRss\u0060 constructor. Then in my \u0060Run\u0060 function, I call the \u0060getMentions\u0060 helper function to query the table, call \u0060BuildRssFeed\u0060 to generate the feed, and write it out to Azure Blob Storage using a \u0060Stream\u0060. \u0060\u0060\u0060fsharp type WebmentionToRss (rssService:RssService) = let getMentions (t:TableClient) = //... let webmentions = t.Query () webmentions //... [ ] member x.Run ([ ] info: TimerInfo) ([ ] t: TableClient) ([ ] rssBlob: Stream) (log: ILogger) = task { let mentions = getMentions t let rss = x.RssService.BuildRssFeed mentions \u0022lqdev\u0027s Webmentions\u0022 \u0022http://lqdev.me\u0022 \u0022lqdev\u0027s Webmentions\u0022 \u0022en\u0022 log.LogInformation(rss.ToString()) use xmlWriter = XmlWriter.Create(rssBlob) rss.WriteTo(xmlWriter) |\u003E ignore } \u0060\u0060\u0060 Now, to view my feed I add the URL to the RSS file in Azure Blob Storage to my RSS feed reader. The container and blob containing the file are private though, so how does my RSS reader get access to the file? I don\u0027t use the direct link. Instead, I use a link containing a [Shared Access Signatures (SAS) token](https://learn.microsoft.com/azure/storage/common/storage-sas-overview#sas-token). The RSS feed reader then fetches my Webmentions feed at regular intervals. This is what the GitHub post looks like in my RSS reader. ![Webmentions table on Azure Table Storage](http://cdn.lqdev.tech/files/images/newsblur-webmentions-rss-feed.png) That\u0027s all there is to it! ## What\u0027s next? Overall I\u0027m happy with my solution. I learned a few things along the way and got something working for myself. Publishing WebmentionFs as a library and making my Azure Function code open-source were attempts to help others looking to implement similar solutions get started. These solitions could be improved though. These are some updates I would like to make at some point: - Upgrade my website to use WebmentionFs for sending Webmentions. You can see the [current implementation](https://github.com/lqdev/luisquintanilla.me/blob/main/Services/Webmention.fs) on my website repo which is basically what\u0027s in \u0060UrlDiscoveryService\u0060 and \u0060WebmentionSenderService\u0060 today. - Add interfaces for all services in WebmentionFs. Currently most services have concrete implementations. By adding interfaces, I can make the library extensible and you can implement your own URL discovery and validation services. - Implement \u0060outbox\u0060 enpoint in WebmentionService to handle sending Webmentions. - Create Azure ARM template for deploying WebmentionService. Currently with some small edits, you could fork and get my WebmentionService project working for your website. However, that process isn\u0027t turnkey today. Maybe using something like [Farmer](https://compositionalit.github.io/farmer/) would make onboarding easier for folks. ## Conclusion Webmentions are a great way to enable cross-site conversations on the internet. In this post, I provided a brief introduction to the general workflow for sending and receiving Webmentions. I also detailed my own implementation which I use today on my website. That implementation relies on the WebmentionFs .NET library, Azure Functions, and RSS. By using the WebmentionFs .NET library, you can more quickly implement similar solutions into your own apps and services. If you need a reference, check out my WebmentionService repository on GitHub which contains the code I use for receiving Webmentions. I\u0027ll leave you with three things: 1. As you consider the next platform for your next online home, in addition to considering the health of the community, take some time to also think about the protocols it\u0027s built on top of and whether they give you more or less freedom. 1. Try out the WebmentionFs library on NuGet and add Webmentions to your site. 1. If you share this post anywhere publicly on the web, send me a Webmention. Catch you on the world wide web!","contentType":"posts","url":"https://www.lqdev.me/posts/receive-webmentions-fsharp-az-functions-fsadvent","date":"2022-12-21 00:33 -05:00","tags":["fsharp","indieweb","protocol","internet","web","dotnet"],"summary":"# Introduction \u003E This post is part of [F# Advent 2022](https://sergeytihon.com/2022/10/28/f-advent-calendar-in-english-2022/). Thanks to [Sergey Tihon](https://sergeytihon.com) for organizing it....","keywords":["webmentions","webmention","post","azure","website","request","send","webmentionfs","feed","source"]},{"id":"notes-now-accepting-webmentions","title":"Now accepting webmentions","content":"It took me a while to get all the pieces working but I\u0027m excited that my website now accepts webmentions ! A few months ago I implemented sending webmentions . Accepting them was a bit tricky because there were a few design decisions I was optimizing for. In the meantime, you can check out WebmentionFs , an F# library I built for validating and receiving webmentions along with the WebmentionService Azure Functions backend that I\u0027m using to process webmentions for my website. Also, feel free to send me a webmention. Here\u0027s my endpoint for now https://lqdevwebmentions.azurewebsites.net/api/inbox . Check out this blog post where I show how you can send webmentions using F#. You can use any HTTP Client of your choosing to send an HTTP POST request to that endpoint with the body containing a link to one of my articles (target) and the link to your post (source). Stay tuned for a more detailed post coming on December 21st!","contentType":"notes","url":"https://www.lqdev.me/notes/now-accepting-webmentions","date":"2022-12-18 22:41 -05:00","tags":["indieweb","webmentions","community","fsharp","azurefunctions","azure"],"summary":"It took me a while to get all the pieces working but I\u0027m excited that my website now accepts webmentions ! A few months ago I implemented sending webmentions . Accepting them was a bit tricky because...","keywords":["webmentions","post","send","check","using","http","endpoint","website","link","show"]},{"id":"bookmarks-norvig-pytudes-alphacode","title":"Norvig AlphaCode Review","content":"Norvig AlphaCode Review \u2192 https://github.com/norvig/pytudes/blob/main/ipynb/AlphaCode.ipynb Large language models have recently shown an ability to solve a variety of problems. In this notebook we consider programming problems (as solved by AlphaCode) and mathematics problems (as solved by Minerva). The questions we would like to get at are: In the future, what role will these generative models play in assisting a programmer or mathematician? What will be a workflow that incorporates these models? How will other existing tools (such as programming languages) change to accomodate this workflow?","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/norvig-pytudes-alphacode","date":"2022-12-16 22:03 -05:00","tags":[],"summary":"Norvig AlphaCode Review \u2192 https://github.com/norvig/pytudes/blob/main/ipynb/AlphaCode.ipynb Large language models have recently shown an ability to solve a variety of problems. In this notebook we...","keywords":["problems","models","solved","alphacode","programming","workflow","are:","future","role","generative"]},{"id":"reply-mastodon-hashtag-rss-boffosocko","title":"Mastodon Hashtag RSS feeds","content":"Mastodon Hashtag RSS feeds \u2192 https://boffosocko.com/2022/12/15/55812758/ Worked well for me. Thanks for sharing!","contentType":"reply","url":"https://www.lqdev.me/responses/mastodon-hashtag-rss-boffosocko","date":"2022-12-15 22:33 -05:00","tags":[],"summary":"Mastodon Hashtag RSS feeds \u2192 https://boffosocko.com/2022/12/15/55812758/ Worked well for me. Thanks for sharing!","keywords":["mastodon","hashtag","feeds","https://boffosocko","com/2022/12/15/55812758/","worked","thanks","sharing"]},{"id":"bookmarks-gpt-3-architecture-napkin","title":"The GPT-3 Architecture, on a Napkin","content":"The GPT-3 Architecture, on a Napkin \u2192 https://dugas.ch/artificial_curiosity/GPT_architecture.html","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/gpt-3-architecture-napkin","date":"2022-12-13 21:11 -05:00","tags":[],"summary":"The GPT-3 Architecture, on a Napkin \u2192 https://dugas.ch/artificial_curiosity/GPT_architecture.html","keywords":["architecture","napkin","https://dugas","ch/artificial","curiosity/gpt","html"]},{"id":"star-twiml-exploring-llm-chatgpt","title":"Exploring Large Language Models with ChatGPT","content":"Exploring Large Language Models with ChatGPT \u2192 https://podbay.fm/p/the-twiml-ai-podcast-formerly-this-week-in-machine-learning-and-artificial-intelligence/e/1670516880 Today we\u0027re joined by ChatGPT, the latest and coolest large language model developed by OpenAl. In our conversation with ChatGPT, we discuss the background and capabilities of large language models, the potential applications of these models, and some of the technical challenges and open questions in the field. We also explore the role of supervised learning in creating ChatGPT, and the use of PPO in training the model. Finally, we discuss the risks of misuse of large language models, and the best resources for learning more about these models and their applications.","contentType":"star","url":"https://www.lqdev.me/responses/twiml-exploring-llm-chatgpt","date":"2022-12-12 08:22 -05:00","tags":[],"summary":"Exploring Large Language Models with ChatGPT \u2192 https://podbay.fm/p/the-twiml-ai-podcast-formerly-this-week-in-machine-learning-and-artificial-intelligence/e/1670516880 Today we\u0027re joined by ChatGPT,...","keywords":["models","large","language","chatgpt","learning","applications","model","discuss","technical","challenges"]},{"id":"bookmarks-glow-markdown-cli","title":"Glow | GitHub","content":"Glow | GitHub \u2192 https://github.com/charmbracelet/glow Render markdown on the CLI","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/glow-markdown-cli","date":"2022-12-07 18:05 -05:00","tags":[],"summary":"Glow | GitHub \u2192 https://github.com/charmbracelet/glow Render markdown on the CLI","keywords":["glow","github","https://github","com/charmbracelet/glow","render","markdown"]},{"id":"reshare-gh-copilot-business-ga","title":"GitHub Copilot is generally available for businesses","content":"GitHub Copilot is generally available for businesses \u2192 https://github.blog/2022-12-07-github-copilot-is-generally-available-for-businesses/ GitHub Copilot for Business gives organizations: The power of AI . Millions of developers have already used GitHub Copilot to build software faster, stay in the flow longer, and solve problems in new ways\u2014all right from their editor of choice. Simple license management . Administrators can enable GitHub Copilot for their teams and select which organizations, teams, and developers receive licenses. Organization-wide policy management . You can easily set policy controls to enforce user settings for public code matching on behalf of your organization. Your code is safe with us . With Copilot for Business, we won\u2019t retain code snippets, store or share your code regardless if the data is from public repositories, private repositories, non-GitHub repositories, or local files.","contentType":"reshare","url":"https://www.lqdev.me/responses/gh-copilot-business-ga","date":"2022-12-07 17:59 -05:00","tags":[],"summary":"GitHub Copilot is generally available for businesses \u2192 https://github.blog/2022-12-07-github-copilot-is-generally-available-for-businesses/ GitHub Copilot for Business gives organizations: The power...","keywords":["github","copilot","code","repositories","teams","from","developers","policy","public","organization"]},{"id":"star-fsadvent-wrapinator-5000","title":"Simulating the Wrapinator 5000","content":"Simulating the Wrapinator 5000 \u2192 https://brandewinder.com/2022/12/04/simulating-wrapinator-5000/ Cool use of DiffSharp .","contentType":"star","url":"https://www.lqdev.me/responses/fsadvent-wrapinator-5000","date":"2022-12-07 16:19 -05:00","tags":[],"summary":"Simulating the Wrapinator 5000 \u2192 https://brandewinder.com/2022/12/04/simulating-wrapinator-5000/ Cool use of DiffSharp .","keywords":["wrapinator","simulating","5000","https://brandewinder","com/2022/12/04/simulating","5000/","cool","diffsharp"]},{"id":"reply-teams-communities-feature","title":"Microsoft Teams adds free communities feature","content":"Microsoft Teams adds free communities feature \u2192 https://www.theverge.com/2022/12/7/23497938/microsoft-teams-communities-feature This reminds me so much of the Rooms feature in Windows Phone. \uD83D\uDE22","contentType":"reply","url":"https://www.lqdev.me/responses/teams-communities-feature","date":"2022-12-07 15:50 -05:00","tags":[],"summary":"Microsoft Teams adds free communities feature \u2192 https://www.theverge.com/2022/12/7/23497938/microsoft-teams-communities-feature This reminds me so much of the Rooms feature in Windows Phone. \uD83D\uDE22","keywords":["feature","teams","communities","microsoft","adds","free","https://www","theverge","com/2022/12/7/23497938/microsoft","reminds"]},{"id":"bookmarks-waydroid-linux","title":"Waydroid","content":"Waydroid \u2192 https://waydro.id/ A container-based approach to boot a full Android system on a regular GNU/Linux system like Ubuntu.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/waydroid-linux","date":"2022-12-06 22:29 -05:00","tags":[],"summary":"Waydroid \u2192 https://waydro.id/ A container-based approach to boot a full Android system on a regular GNU/Linux system like Ubuntu.","keywords":["system","waydroid","https://waydro","container","based","approach","boot","full","android","regular"]},{"id":"star-florence-fsharp-geospatial","title":"Geospatial is a function of your life ","content":"Geospatial is a function of your life \u2192 https://dev.to/cognipla/geospatial-is-a-function-of-your-life-1924 Florence, a low-code geospatial library for everyone that ranks city places with (life-inspired) functions. Florence heavily laverages .NET Interactive/Polyglot Notebooks runtime to hide and execute code behind the scene (including breaking some language constraints).","contentType":"star","url":"https://www.lqdev.me/responses/florence-fsharp-geospatial","date":"2022-12-06 10:43 -05:00","tags":[],"summary":"Geospatial is a function of your life \u2192 https://dev.to/cognipla/geospatial-is-a-function-of-your-life-1924 Florence, a low-code geospatial library for everyone that ranks city places with...","keywords":["life","geospatial","function","florence","code","breaking","including","scene","behind","execute"]},{"id":"bookmarks-emacsconf-2022-videos","title":"EmacsConf 2022 Videos","content":"EmacsConf 2022 Videos \u2192 https://toobnix.org/c/emacsconf/videos","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/emacsconf-2022-videos","date":"2022-12-06 10:25 -05:00","tags":[],"summary":"EmacsConf 2022 Videos \u2192 https://toobnix.org/c/emacsconf/videos","keywords":["emacsconf","2022","videos","https://toobnix","org/c/emacsconf/videos"]},{"id":"reshare-msft-open-sources-farmvibes-ai","title":"Microsoft Open-Sources Agricultural AI Toolkit FarmVibes.AI ","content":"Microsoft Open-Sources Agricultural AI Toolkit FarmVibes.AI \u2192 https://www.infoq.com/news/2022/12/microsoft-farmvibes/ Microsoft Research recently open-sourced FarmVibes.AI , a suite of ML models and tools for sustainable agriculture. FarmVibes.AI includes data processing workflows for fusing multiple sets of spatiotemporal and geospatial data, such as weather data and satellite and drone imagery.","contentType":"reshare","url":"https://www.lqdev.me/responses/msft-open-sources-farmvibes-ai","date":"2022-12-06 09:55 -05:00","tags":[],"summary":"Microsoft Open-Sources Agricultural AI Toolkit FarmVibes.AI \u2192 https://www.infoq.com/news/2022/12/microsoft-farmvibes/ Microsoft Research recently open-sourced FarmVibes.AI , a suite of ML models and...","keywords":["farmvibes","data","microsoft","open","satellite","weather","geospatial","spatiotemporal","sets","multiple"]},{"id":"bookmarks-nlp-demystified-course","title":"Course: NLP Demystified","content":"Course: NLP Demystified \u2192 https://www.nlpdemystified.org/","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/nlp-demystified-course","date":"2022-12-03 18:53 -05:00","tags":[],"summary":"Course: NLP Demystified \u2192 https://www.nlpdemystified.org/","keywords":["course:","demystified","https://www","nlpdemystified","org/"]},{"id":"bookmarks-elfeed-view-videos","title":"Using Elfeed to view videos","content":"Using Elfeed to view videos \u2192 https://medium.com/emacs/using-elfeed-to-view-videos-6dfc798e51e6 Slightly modified the original script to use Streamlink and lower quality to 240p for bandwith and resource purposes. (require \u0027elfeed) (defun elfeed-v-mpv (url) \u0026quot;Watch a video from URL in MPV\u0026quot; (async-shell-command (format \u0026quot;streamlink -p mpv %s 240p\u0026quot; url))) (defun elfeed-view-mpv (\u0026amp;optional use-generic-p) \u0026quot;Youtube-feed link\u0026quot; (interactive \u0026quot;P\u0026quot;) (let ((entries (elfeed-search-selected))) (cl-loop for entry in entries do (elfeed-untag entry \u0027unread) when (elfeed-entry-link entry) do (elfeed-v-mpv it)) (mapc #\u0027elfeed-search-update-entry entries) (unless (use-region-p) (forward-line)))) (define-key elfeed-search-mode-map (kbd \u0026quot;v\u0026quot;) \u0027elfeed-view-mpv)","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/elfeed-view-videos","date":"2022-12-02 21:58 -05:00","tags":[],"summary":"Using Elfeed to view videos \u2192 https://medium.com/emacs/using-elfeed-to-view-videos-6dfc798e51e6 Slightly modified the original script to use Streamlink and lower quality to 240p for bandwith and...","keywords":["elfeed","\u0026quot","entry","view","search","entries","videos","defun","streamlink","using"]},{"id":"notes-til-firefox-task-manager","title":"TIL: Firefox Task Manager","content":"Today I learned about Firefox Task Manager . Typing about:performance into the address bar brings up the task manager which lets you inspect the tabs and extensions consuming the most energy and resources.","contentType":"notes","url":"https://www.lqdev.me/notes/til-firefox-task-manager","date":"2022-12-01 11:24 -05:00","tags":["firefox","browser","tools","web","internet","opensource"],"summary":"Today I learned about Firefox Task Manager . Typing about:performance into the address bar brings up the task manager which lets you inspect the tabs and extensions consuming the most energy and...","keywords":["task","manager","today","consuming","extensions","tabs","inspect","lets","which","brings"]},{"id":"bookmarks-openai-chatgpt","title":"ChatGPT: Optimizing Language Models for Dialogue","content":"ChatGPT: Optimizing Language Models for Dialogue \u2192 https://openai.com/blog/chatgpt/ We\u2019ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests. ChatGPT is a sibling model to InstructGPT , which is trained to follow an instruction in a prompt and provide a detailed response.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/openai-chatgpt","date":"2022-12-01 11:12 -05:00","tags":[],"summary":"ChatGPT: Optimizing Language Models for Dialogue \u2192 https://openai.com/blog/chatgpt/ We\u2019ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible...","keywords":["chatgpt","model","dialogue","which","trained","challenge","incorrect","premises","reject","inappropriate"]},{"id":"reshare-philtel-open-source-payphone","title":"How open source is revitalizing the payphone","content":"How open source is revitalizing the payphone \u2192 https://opensource.com/article/22/11/open-source-payphone-philtel PhilTel is a telephone collective based in Philadelphia, Pennsylvania, focusing on making communications accessible to everyone by installing free-to-use payphones. While you\u0027ll be able to make standard telephone calls through our phones, we\u0027re also focusing on offering interesting services or experiences. We don\u0027t want to only facilitate human-to-human interaction but also human-to-machine interaction and give people an environment where they can explore the telephone network and learn from it.","contentType":"reshare","url":"https://www.lqdev.me/responses/philtel-open-source-payphone","date":"2022-11-30 10:04 -05:00","tags":[],"summary":"How open source is revitalizing the payphone \u2192 https://opensource.com/article/22/11/open-source-payphone-philtel PhilTel is a telephone collective based in Philadelphia, Pennsylvania, focusing on...","keywords":["telephone","human","source","payphone","philtel","also","focusing","interaction","open","offering"]},{"id":"bookmarks-documentation-coping-serial-project-hoarder","title":"Coping strategies for the serial project hoarder","content":"Coping strategies for the serial project hoarder \u2192 https://simonwillison.net/2022/Nov/26/productivity/ Consider timezones: engineers in Madrid and engineers in San Francisco had almost no overlap in their working hours. Good asynchronous communication was essential. Over time, I noticed that the teams that were most effective at this scale were the teams that had a strong culture of documentation and automated testing. As I started to work on my own array of smaller personal projects, I found that the same discipline that worked for large teams somehow sped me up, when intuitively I would have expected it to slow me down.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/documentation-coping-serial-project-hoarder","date":"2022-11-29 10:00 -05:00","tags":[],"summary":"Coping strategies for the serial project hoarder \u2192 https://simonwillison.net/2022/Nov/26/productivity/ Consider timezones: engineers in Madrid and engineers in San Francisco had almost no overlap in...","keywords":["teams","engineers","coping","strong","culture","documentation","automated","testing","started","array"]},{"id":"bookmarks-art-of-command-line","title":"The Art of Command Line","content":"The Art of Command Line \u2192 https://github.com/jlevy/the-art-of-command-line","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/art-of-command-line","date":"2022-11-24 20:43 -05:00","tags":[],"summary":"The Art of Command Line \u2192 https://github.com/jlevy/the-art-of-command-line","keywords":["command","line","https://github","com/jlevy/the"]},{"id":"bookmarks-stable-diffusion-v2","title":"Stable Diffusion 2.0 Release","content":"Stable Diffusion 2.0 Release \u2192 https://stability.ai/blog/stable-diffusion-v2-release The Stable Diffusion 2.0 release includes robust text-to-image models trained using a brand new text encoder (OpenCLIP), developed by LAION with support from Stability AI, which greatly improves the quality of the generated images compared to earlier V1 releases. Stable Diffusion 2.0 also includes an Upscaler Diffusion model that enhances the resolution of images by a factor of 4.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/stable-diffusion-v2","date":"2022-11-24 20:39 -05:00","tags":[],"summary":"Stable Diffusion 2.0 Release \u2192 https://stability.ai/blog/stable-diffusion-v2-release The Stable Diffusion 2.0 release includes robust text-to-image models trained using a brand new text encoder...","keywords":["diffusion","stable","release","includes","text","images","greatly","improves","quality","generated"]},{"id":"star-perks-high-documentation-culture","title":"The Perks of a High-Documentation, Low-Meeting Work Culture","content":"The Perks of a High-Documentation, Low-Meeting Work Culture \u2192 https://www.tremendous.com/blog/the-perks-of-a-high-documentation-low-meeting-work-culture the intangible, overarching benefit of practicing meeting mindfulness is this: you spend less of your day sort-of-listening and more of your day really thinking.","contentType":"star","url":"https://www.lqdev.me/responses/perks-high-documentation-culture","date":"2022-11-23 22:12 -05:00","tags":[],"summary":"The Perks of a High-Documentation, Low-Meeting Work Culture \u2192 https://www.tremendous.com/blog/the-perks-of-a-high-documentation-low-meeting-work-culture the intangible, overarching benefit of...","keywords":["meeting","perks","high","documentation","culture","more","listening","sort","less","spend"]},{"id":"reshare-whatsapp-data-leak-alternatives","title":"Database of nearly 500 million WhatsApp users\u2019 mobile phones is up for sale","content":"Database of nearly 500 million WhatsApp users\u2019 mobile phones is up for sale \u2192 https://cybernews.com/news/whatsapp-data-leak/ Just a friendly reminder, there are alternatives .","contentType":"reshare","url":"https://www.lqdev.me/responses/whatsapp-data-leak-alternatives","date":"2022-11-23 15:00 -05:00","tags":[],"summary":"Database of nearly 500 million WhatsApp users\u2019 mobile phones is up for sale \u2192 https://cybernews.com/news/whatsapp-data-leak/ Just a friendly reminder, there are alternatives .","keywords":["database","nearly","million","whatsapp","users\u2019","mobile","phones","sale","https://cybernews","com/news/whatsapp"]},{"id":"reshare-tumblr-activitypub","title":"Tumblr to add support for ActivityPub","content":"Tumblr to add support for ActivityPub \u2192 https://techcrunch.com/2022/11/21/tumblr-to-add-support-for-activitypub-the-social-protocol-powering-mastodon-and-other-apps/ Automattic CEO Matt Mullenweg \u2014 whose company acquired Tumblr from Verizon in 2019 \u2014 suggested...Tumblr... would soon add ...activitypub. I\u0027m perfectly happy using my site as the main place to post content but considering it\u0027s under new management and adopting open standards, it might be time to checkout out Tumblr again.","contentType":"reshare","url":"https://www.lqdev.me/responses/tumblr-activitypub","date":"2022-11-23 14:49 -05:00","tags":[],"summary":"Tumblr to add support for ActivityPub \u2192 https://techcrunch.com/2022/11/21/tumblr-to-add-support-for-activitypub-the-social-protocol-powering-mastodon-and-other-apps/ Automattic CEO Matt Mullenweg \u2014...","keywords":["tumblr","activitypub","support","perfectly","happy","using","site","main","place","post"]},{"id":"notes-own-your-links-domain-verification","title":"Use your domain for verification everywhere","content":"I\u0027ve never had a blue checkmark on Twitter so I can\u0027t say what that process is like. Looking at it from the outside though it seems the process isn\u0027t simple or straightforward. My experience with verification has been with other applications such as Mastodon and IndieAuth . In both of these cases, the main verification techniques use your domain and/or the rel=me link type . Although with Mastodon you can add multiple verified links to your profile, verification on those external domains pose challenges which include but are not limited to: You can\u0027t edit the HTML directly so you can\u0027t add a rel=me link. You can only add one link, so you\u0027re forced to choose between your website and Mastodon. In scenarios where multiple links (i.e. e-mail, webpage, Twitter) are supported, Mastodon isn\u0027t one of the options So how do you overcome these challenges? By using your domain. A few months ago I wrote about owning your links . The general concept of owning your links is, using your domain to redirect to other properties and profiles you own. For example, instead of telling people to visit my Twitter profile at twitter.com/ljquintanilla , I can instead point them to lqdev.me/twitter . Not only does my domain become the source of truth, but in general it\u0027s easier to remember compared to usernames and proprietary domains. Since my site is statically generated, I own my links by using http-equiv meta tags . Although the only job for these pages is to redirect to the specified site, in the end they are still HTML pages. This means, I can embed any other HTML I want, including rel=me links. By including the rel=me link in the redirect page, you satisfy the requirements of applications like Mastodon because that\u0027s the page they\u0027ll land on before being redirected to the target site. As a result, I can verify my GitHub, Twitter, or any other online property I want with Mastodon using my own domain without having to make any changes or overcome any of the challenges imposed by those other platforms. What you end up with is something like this: Before signing off, I\u0027ll note that this isn\u0027t a perfect solution and technically anyone could do the same thing on their own domain in an attempt to impersonate you, so just be aware of that.","contentType":"notes","url":"https://www.lqdev.me/notes/own-your-links-domain-verification","date":"2022-11-22 10:19 -05:00","tags":["indieweb","ownyourlinks","internet","domain","identity","technology","ownership"],"summary":"I\u0027ve never had a blue checkmark on Twitter so I can\u0027t say what that process is like. Looking at it from the outside though it seems the process isn\u0027t simple or straightforward. My experience with...","keywords":["domain","mastodon","links","twitter","using","link","rel=me","challenges","verification","html"]},{"id":"star-digital-library-amateur-radio-25k","title":"Digital Library of Amateur Radio \u0026 Communications Surpasses 25,000 Items","content":"Digital Library of Amateur Radio \u0026amp; Communications Surpasses 25,000 Items \u2192 https://blog.archive.org/2022/11/16/digital-library-of-amateur-radio-communications-surpasses-25000-items/ In the six weeks since announcing that Internet Archive has begun gathering content for the Digital Library of Amateur Radio and Communications (DLARC), the project has quickly grown to more than 25,000 items, including ham radio newsletters, podcasts, videos, books, and catalogs.","contentType":"star","url":"https://www.lqdev.me/responses/digital-library-amateur-radio-25k","date":"2022-11-21 23:06 -05:00","tags":[],"summary":"Digital Library of Amateur Radio \u0026amp; Communications Surpasses 25,000 Items \u2192 https://blog.archive.org/2022/11/16/digital-library-of-amateur-radio-communications-surpasses-25000-items/ In the six...","keywords":["radio","library","amateur","communications","digital","surpasses","items","archive","project","quickly"]},{"id":"notes-net7-devcontainer-gh-codespaces","title":"Configured GitHub Codespaces Dev Container with .NET 7","content":"Soon after .NET 7 was released, I upgraded the static site generator I use as well as the GitHub Actions that build and publish my website. Having upgraded last year from .NET 6, the process was as smooth as I had expected with no code refactoring required. When it comes to development environments, for quick status updates like the ones on my feed or minor edits, I\u0027ve been using github.dev . However, there\u0027s been times where I\u0027ve needed to run and debug code to confirm that my changes work. This is where I hit some of the limitation of github.dev which means unless I set up a Codespace, I have to save my work and move offline to my PC. Codespaces are great, but given that Codespaces are nice to haves not a requirement for my workflow at this time, it didn\u0027t make sense for me to pay for them. That\u0027s why I was excited to learn that GitHub is providing up to 60 hours per month of free Codespace usage to all developers. That\u0027s more than enough for me. By default, Codespace images come preinstalled with the .NET 6 SDK which makes sense considering it\u0027s the latest LTS. However, since my static site generator targets .NET 7, I had to configure my Codespace to use .NET 7. This was just as easy as upgrading to .NET 7. All I had to do was provide the .NET 7 SDK Docker image as part of my devcontainer.json configuration file. From there, Codespaces takes care of the rest. As a result, I can now run and debug my code all in one place without interrupting my workflow. PS: This post was authored in GitHub Codespaces \uD83D\uDE42","contentType":"notes","url":"https://www.lqdev.me/notes/net7-devcontainer-gh-codespaces","date":"2022-11-21 11:28 -05:00","tags":["dotnet","github","codespaces","development","devcontainer","website","internet"],"summary":"Soon after .NET 7 was released, I upgraded the static site generator I use as well as the GitHub Actions that build and publish my website. Having upgraded last year from .NET 6, the process was as...","keywords":["github","codespaces","codespace","code","sense","which","debug","however","from","workflow"]},{"id":"star-telepathic-tech-writing","title":"Telepathic technical writing","content":"Telepathic technical writing \u2192 https://derw.substack.com/p/telepathic-technical-writing ...blog posts are always async, but they can lead to conversations and debates, once the reader is done reading. There\u0027s also the nature of blog posts being one-to-many, whereas chat is many-to-many, if done in a public channel. One-to-many forms of communication should generally be more formal, but can spread ideas and thoughts more coherently than many-to-many.","contentType":"star","url":"https://www.lqdev.me/responses/telepathic-tech-writing","date":"2022-11-21 10:29 -05:00","tags":[],"summary":"Telepathic technical writing \u2192 https://derw.substack.com/p/telepathic-technical-writing ...blog posts are always async, but they can lead to conversations and debates, once the reader is done...","keywords":["many","done","posts","blog","more","writing","technical","formal","ideas","generally"]},{"id":"reshare-digital-physical-books-wear","title":"Digital Books wear out faster than Physical Books","content":"Digital Books wear out faster than Physical Books \u2192 https://blog.archive.org/2022/11/15/digital-books-wear-out-faster-than-physical-books/ Our paper books have lasted hundreds of years on our shelves and are still readable. Without active maintenance, we will be lucky if our digital books last a decade.","contentType":"reshare","url":"https://www.lqdev.me/responses/digital-physical-books-wear","date":"2022-11-20 23:14 -05:00","tags":[],"summary":"Digital Books wear out faster than Physical Books \u2192 https://blog.archive.org/2022/11/15/digital-books-wear-out-faster-than-physical-books/ Our paper books have lasted hundreds of years on our shelves...","keywords":["books","digital","wear","faster","physical","lucky","maintenance","active","readable","shelves"]},{"id":"notes-fediverse-beyond-mastodon","title":"The Fediverse beyond Mastodon","content":"It\u0027s been great to see many old and new friends joining Mastodon and considering alternatives to online communities over the past few weeks. Selfishly my feed had been quiet since I started using it and self-hosting back in 2018 so it\u0027s nice to see more activity. As folks have started to settle in and get a better understanding of Mastodon, I\u0027d like to point out that Mastodon is only one of many apps and services on the Fediverse . Now I won\u0027t try to explain the Fediverse but in general, it\u0027s the collection of applications and services that interoperate in a federated manner, Mastodon being one of them. If in general you like Mastodon and are looking for alternatives to services like Instagram, YouTube, SoundCloud, the Fediverse has a few options: Instagram \u27A1\uFE0F Pixelfed YouTube \u27A1\uFE0F PeerTube SoundCloud \u27A1\uFE0F Funkwhale Twitch \u27A1\uFE0F Owncast You can create an account for each of these services using different instances, just like Mastodon. You also have the option of self-hosting which I find amazing. However, because for the most part these services implement the ActivityPub protocol, it means federation extends beyond Mastodon. You can follow people on PixelFed, PeerTube, and many of these other services using your Mastodon account which means you don\u0027t have to create accounts for those services just to follow people. While I don\u0027t have experience with all of these services, I recommend checking them out. I especially like what Pixelfed, PeerTube and Owncast are doing but those are the main ones I have experience with.","contentType":"notes","url":"https://www.lqdev.me/notes/fediverse-beyond-mastodon","date":"2022-11-20 19:28 -05:00","tags":["fediverse","mastodon","decentralization","owncast","peertube","pixelfed","funkwhale","alternatives"],"summary":"It\u0027s been great to see many old and new friends joining Mastodon and considering alternatives to online communities over the past few weeks. Selfishly my feed had been quiet since I started using it...","keywords":["services","mastodon","many","fediverse","pixelfed","using","peertube","experience","general","account"]},{"id":"star-feta-matrix-raspi","title":"Feta - Matrix Server Distribution","content":"Feta - Matrix Server Distribution \u2192 https://www.feta.bz/ Feta is a Matrix server distribution for the Raspberry Pi 3 and 4. This looks like a great project to get started with self-hosting and join the Matrix network. Having something similar for the fediverse would be great as well. I\u0027ve hosted Matrix and Mastodon servers on a Raspberry Pi before so I know it\u0027s up to the task.","contentType":"star","url":"https://www.lqdev.me/responses/feta-matrix-raspi","date":"2022-11-16 22:29 -05:00","tags":[],"summary":"Feta - Matrix Server Distribution \u2192 https://www.feta.bz/ Feta is a Matrix server distribution for the Raspberry Pi 3 and 4. This looks like a great project to get started with self-hosting and join...","keywords":["matrix","feta","server","distribution","raspberry","mastodon","hosted","fediverse","similar","having"]},{"id":"star-rss-privacy-efficiency","title":"Use RSS for privacy and efficiency","content":"Use RSS for privacy and efficiency \u2192 https://rsapkf.org/weblog/q2z/","contentType":"star","url":"https://www.lqdev.me/responses/rss-privacy-efficiency","date":"2022-11-16 00:08 -05:00","tags":[],"summary":"Use RSS for privacy and efficiency \u2192 https://rsapkf.org/weblog/q2z/","keywords":["privacy","efficiency","https://rsapkf","org/weblog/q2z/"]},{"id":"notes-net7-website-update","title":"Upgraded website to .NET 7","content":"Last year, I posted about upgrading this website to .NET 6 and how easy that process was. I just had a chance to upgrade to .NET 7 and the experience was just as smooth. All I had to do was change the version in two places: GitHub Actions *.yml file dotnet-version: \u00277.0.x\u0027 *.fsproj file \u0026lt;TargetFramework\u0026gt;net7.0\u0026lt;/TargetFramework\u0026gt; I\u0027m happy that with such a simple change, I can get all the benefits of .NET 7 without doing a lot of work.","contentType":"notes","url":"https://www.lqdev.me/notes/net7-website-update","date":"2022-11-11 09:59 -05:00","tags":["dotnet","website","net7","upgrade"],"summary":"Last year, I posted about upgrading this website to .NET 6 and how easy that process was. I just had a chance to upgrade to .NET 7 and the experience was just as smooth. All I had to do was change...","keywords":["file","change","last","simple","happy","/targetframework\u0026gt","0\u0026lt","net7","targetframework\u0026gt","fsproj"]},{"id":"notes-psu-future-allar","title":"Penn State future class looks promising","content":"Slow start but good bounce back by Penn State after two straight losses. This game also gave a good glimpse of the future class and it looks promising especially at the QB and RB spots. If they can stay healthy, they may be playoff contenders in a couple of years.","contentType":"notes","url":"https://www.lqdev.me/notes/psu-future-allar","date":"2022-11-05 19:08 -05:00","tags":["psu","football","college","ncaa","ncaafb","pennstate","collegefootball"],"summary":"Slow start but good bounce back by Penn State after two straight losses. This game also gave a good glimpse of the future class and it looks promising especially at the QB and RB spots. If they can...","keywords":["good","they","slow","contenders","playoff","healthy","stay","spots","especially","promising"]},{"id":"star-two-decades-blogging-free-bird-edition","title":"Two Decades of Blogging, The Free As A Bird Edition","content":"Two Decades of Blogging, The Free As A Bird Edition \u2192 https://www.zylstra.org/blog/2022/11/two-decades-of-blogging-the-free-as-a-bird-edition/","contentType":"star","url":"https://www.lqdev.me/responses/two-decades-blogging-free-bird-edition","date":"2022-11-05 16:52 -05:00","tags":[],"summary":"Two Decades of Blogging, The Free As A Bird Edition \u2192 https://www.zylstra.org/blog/2022/11/two-decades-of-blogging-the-free-as-a-bird-edition/","keywords":["decades","blogging","free","bird","edition","https://www","zylstra","org/blog/2022/11/two","edition/"]},{"id":"reshare-willison-mastodon","title":"It looks like I\u2019m moving to Mastodon","content":"It looks like I\u2019m moving to Mastodon \u2192 https://simonwillison.net/2022/Nov/5/mastodon/","contentType":"reshare","url":"https://www.lqdev.me/responses/willison-mastodon","date":"2022-11-05 15:42 -05:00","tags":[],"summary":"It looks like I\u2019m moving to Mastodon \u2192 https://simonwillison.net/2022/Nov/5/mastodon/","keywords":["looks","moving","mastodon","https://simonwillison","net/2022/nov/5/mastodon/"]},{"id":"reshare-how-to-leave-dying-social-platforms","title":"How to Leave Dying Social Media Platforms","content":"How to Leave Dying Social Media Platforms \u2192 https://doctorow.medium.com/how-to-leave-dying-social-media-platforms-9fc550fe5abf","contentType":"reshare","url":"https://www.lqdev.me/responses/how-to-leave-dying-social-platforms","date":"2022-10-30 12:18 -05:00","tags":[],"summary":"How to Leave Dying Social Media Platforms \u2192 https://doctorow.medium.com/how-to-leave-dying-social-media-platforms-9fc550fe5abf","keywords":["leave","dying","social","media","platforms","https://doctorow","medium","com/how","9fc550fe5abf"]},{"id":"bookmarks-alt-interoperability-adversarial","title":"alt.interoperability.adversarial","content":"alt.interoperability.adversarial \u2192 https://www.eff.org/deeplinks/2019/11/altinteroperabilityadversarial","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/alt-interoperability-adversarial","date":"2022-10-30 12:16 -05:00","tags":[],"summary":"alt.interoperability.adversarial \u2192 https://www.eff.org/deeplinks/2019/11/altinteroperabilityadversarial","keywords":["interoperability","adversarial","https://www","org/deeplinks/2019/11/altinteroperabilityadversarial"]},{"id":"notes-psu-osu-2022","title":"Penn State vs Ohio State","content":"11:58 Hoping for a good game. 12:22 Defense is clutch. As always in big games, they\u0027ll have to work overtime just to keep the offense in the game. 12:42 Progress...at least there was no turnover on the last series. 12:59 13:16 14:53 15:09 Going for it on 4th \u0026amp; 2 feels like that 4th \u0026amp; 5 from a few years ago. 15:30 Defense showed up today. Can\u0027t expect to win a game with 3 turnovers. 15:33 This is hard to watch.","contentType":"notes","url":"https://www.lqdev.me/notes/psu-osu-2022","date":"2022-10-29 11:58 -05:00","tags":["psu","osu","pennstate","ohiostate","ncaafb","football","ncaa","collegefootball"],"summary":"11:58 Hoping for a good game. 12:22 Defense is clutch. As always in big games, they\u0027ll have to work overtime just to keep the offense in the game. 12:42 Progress...at least there was no turnover on...","keywords":["game","defense","\u0026amp","11:58","14:53","15:09","going","feels","from","15:30"]},{"id":"star-proposal-news-mastodon-servers","title":"A Proposal for News Organization Mastodon Servers and More","content":"A Proposal for News Organization Mastodon Servers and More \u2192 https://newsletter.danhon.com/archive/4230/ Here\u2019s how it works in practice: A news organization (or any organization, let\u2019s just start with news) already asserts ownership of its domain e.g. via its certificate, so we piggyback trust off its domain. It stands up a Mastodon or other social server at a standard address. I\u2019d propose follow.washingtonpost.com but there\u2019s a bunch of reasons why you might do something else, see below, and uses the agreed well-known autodiscovery protocol to return the address for its Mastodon server (but I don\u2019t see an entry for activitypub or Mastodon yet). It creates accounts for its staff on its Mastodon server. Nobody else gets an account; the public can\u2019t sign up. What you get: Verified accounts. Instead of relying on a third party to \u201Cverify\u201D your account by sticking a blue check against your display name, account provenance and \u201Cverification\u201D is inherited as a property of the Mastodon server.... Ease of discovery...all a user would have to do, to find Washington Post accounts to follow, would be to know the washingtonpost.com domain. Autodiscovery would let your Mastodon client point itself to the appropriate server. Not just news organizations...anyone can set up a Mastodon server...the federation means that \u201Cofficial\u201D accounts become \u201Cmore official\u201D when their server home is hung off the domain of the actual organization. You wouldn\u2019t need Twitter (or anyone else, really) to verify that the UK Prime Minister\u2019s account is official, because you\u2019d have following.gov.uk as the Mastodon server, which means you can trust that server as much as you trust .gov.uk domains. Your university or college wants you to have a social media account? Sure, you can have it hosted at following.ucla.edu. And yes, brands can get in on it. Sure. That way there\u2019s a tiny chance you\u2019re following the Proper Brand Account rather than a Parody Brand Account, which\u2026 is probably for the best. Or it\u2019s easier to see that a Parody Account is a Parody Account because you can look at the parent server.","contentType":"star","url":"https://www.lqdev.me/responses/proposal-news-mastodon-servers","date":"2022-10-28 21:07 -05:00","tags":[],"summary":"A Proposal for News Organization Mastodon Servers and More \u2192 https://newsletter.danhon.com/archive/4230/ Here\u2019s how it works in practice: A news organization (or any organization, let\u2019s just start...","keywords":["server","mastodon","account","news","organization","accounts","domain","trust","parody","else"]},{"id":"bookmarks-communicating-effectively-developer","title":"How to communicate effectively as a developer","content":"How to communicate effectively as a developer \u2192 https://www.karlsutt.com/articles/communicating-effectively-as-a-developer/ Communicating effectively as an engineer means empathically increasing the resolution of your writing. ...\u201Clow-resolution writing\u201D...There is very little context, too much reliance on pronouns and unclear references. The writing is not emphatic \u2014the reader has to spend extra energy to work out what is being said Longer-form writing gives you an opportunity to dive deeper into why you are saying what you are saying. It is a chance to educate, to teach, to help understand and to level up. The quality of the API documentation will carry an astronomical amount of leverage. This leverage will work in both directions. Genuinely helpful documentation is the difference between being swamped by support requests from frustrated API users and significantly increasing the usage of your service. Happy users beget more happy users. Spoken words get forgotten. Written words are shared, preserved, and become the basis of a company\u0027s culture. source High resolution, empathic writing...You will have to spend more energy to make your writing easy to follow. You will have to grapple with your own confusion and holes in your understanding. You will have to figure out what the appropriate density for your writing is. It\u0027s not about you, though.. It\u0027s about them. Not only does a single recipient benefit from your extra effort, what if ten people read your good work? A hundred? A thousand? What if the company CEO reads it? Taking writing seriously at work or in your organisation and putting in the effort to delight the reader will, over time, compound into a massive body of quality writing that benefits everyone. It is a literal win-win-win Produce writing you would read with delight if you were on the other end.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/communicating-effectively-developer","date":"2022-10-28 20:57 -05:00","tags":[],"summary":"How to communicate effectively as a developer \u2192 https://www.karlsutt.com/articles/communicating-effectively-as-a-developer/ Communicating effectively as an engineer means empathically increasing the...","keywords":["writing","effectively","users","resolution","happy","energy","extra","spend","reader","quality"]},{"id":"reply-jellyfin-home-media-library","title":"Why I use Jellyfin for my home media library","content":"Why I use Jellyfin for my home media library \u2192 https://www.jeffgeerling.com/blog/2022/why-i-use-jellyfin-my-home-media-library It\u0027s also built with .NET \uD83D\uDE42","contentType":"reply","url":"https://www.lqdev.me/responses/jellyfin-home-media-library","date":"2022-10-27 20:13 -05:00","tags":[],"summary":"Why I use Jellyfin for my home media library \u2192 https://www.jeffgeerling.com/blog/2022/why-i-use-jellyfin-my-home-media-library It\u0027s also built with .NET \uD83D\uDE42","keywords":["jellyfin","home","media","library","https://www","jeffgeerling","com/blog/2022/why","also","built"]},{"id":"star-what-work-looks-like","title":"What work looks like","content":"What work looks like \u2192 https://blog.jim-nielsen.com/2022/what-work-looks-like/","contentType":"star","url":"https://www.lqdev.me/responses/what-work-looks-like","date":"2022-10-25 23:01 -05:00","tags":[],"summary":"What work looks like \u2192 https://blog.jim-nielsen.com/2022/what-work-looks-like/","keywords":["looks","https://blog","nielsen","com/2022/what","like/"]},{"id":"star-ott-suspension","title":"Suspension","content":"Suspension \u2192 https://matthiasott.com/notes/suspension write your most important thoughts on your own site. You can share the link on as many platforms as you like and have conversations with anyone who wants to connect with you and your work. But nobody can take it from you. You are in control. Forever.","contentType":"star","url":"https://www.lqdev.me/responses/ott-suspension","date":"2022-10-25 22:44 -05:00","tags":[],"summary":"Suspension \u2192 https://matthiasott.com/notes/suspension write your most important thoughts on your own site. You can share the link on as many platforms as you like and have conversations with anyone...","keywords":["suspension","from","nobody","connect","wants","anyone","conversations","platforms","control","many"]},{"id":"star-surface-duo-android-12L","title":"Android 12L for Surface Duo is now available","content":"Android 12L for Surface Duo is now available \u2192 https://www.windowscentral.com/hardware/laptops/android-12l-for-surface-duo-is-now-available-with-new-ui-and-more Downloading now.","contentType":"star","url":"https://www.lqdev.me/responses/surface-duo-android-12L","date":"2022-10-24 21:58 -05:00","tags":[],"summary":"Android 12L for Surface Duo is now available \u2192 https://www.windowscentral.com/hardware/laptops/android-12l-for-surface-duo-is-now-available-with-new-ui-and-more Downloading now.","keywords":["surface","available","android","https://www","windowscentral","com/hardware/laptops/android","more","downloading"]},{"id":"posts-tiddlywiki-vscode","title":"Get started with TiddlyWiki in VS Code","content":"## Introduction A while back, I set up a knowledge base (KB) section on my website. As part of that knowledge base, I have a [wiki](/resources/wiki) and [snippets](/resources/snippets) section. As opposed to the [blog posts](/posts) which tend to be more long-form and have structure or the microblog-like posts on my [main](/notes) and [response](/responses) feeds, the wiki and snippets section are intended to have self-contained blocks of information that are linked somehow but not a linear or chronological order. My current wiki and snippets solution was fairly basic. It was a series of documents authored in Markdown which then get converted to static HTML pages. The index pages for wiki articles and snippets contain links to each of those documents. While that\u0027s worked okay, I felt I was missing functionality when it came to linking between documents, tagging, and more importantly searching. While I had seen wiki solutions like [MediaWiki](https://www.mediawiki.org/wiki/MediaWiki), they felt too bloated for my use case. Since my site is statically hosted, I didn\u0027t want to have to create, maintain, and pay for a server only to run my wiki. That\u0027s where TiddlyWiki comes in. [TiddlyWiki](https://tiddlywiki.com/) describes itself as \u0022a unique non-linear notebook for capturing, organising and sharing complex information\u0022. In short, TiddlyWiki is a self-contained single HTML file with WYSIWYG-like functionality for creating a wiki. Because everything is contained in a single HTML file, it\u0027s static-site friendly. Now I had heard of TiddlyWiki before but dismissed it because I didn\u0027t get how it worked. For one, I was confused how the WYSIWYG GUI supported editing, viewing, and exporting of these notes since there was just a single HTML file. More importantly, when it comes to [saving changes](https://tiddlywiki.com/#Saving), the recommendations on the website point users to browser extensions and apps. If it\u0027s just an HTML page, why can\u0027t I use the tools I\u0027m already using to work with HTML? After giving it another look and tinkering a bit, I found I was right and could use tools like VS Code to work with TiddlyWiki. In this post, I\u0027ll show how you can get started with the empty edition of TiddlyWiki using VS Code and the Live Preview extension. ## Prerequisites - [Visual Studio Code](https://code.visualstudio.com/#alt-downloads) - [Live Server Extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode.live-server) This guide was written on a Linux (Manjaro) PC but should work the same on Windows and macOS. ## Download TiddlyWiki The quickest way to get started with TiddlyWiki is to: 1. Download the [empty edition](https://tiddlywiki.com/#Empty%20Edition). A file called *empty.html* should download to your computer. For simplicity, I would recommend creating a new directory on your computer that only contains that file. The empty edition contains the core functionality. For other editions, see [TiddlyWiki editions](https://tiddlywiki.com/#Editions). 1. Rename the file *index.html*. ## Edit and save your wiki ### Edit your wiki 1. Open the directory containing your *index.html* file in VS Code. 1. Open the command palette (View \u003E Command Palette). 1. Enter into the text box *Live Preview: Start Server* and press *Enter*. The server should start. ![lqdev TiddlyWiki Main Page](http://cdn.lqdev.tech/files/images/lqdev-tiddlywiki.png) At this point, you can use the TiddlyWiki interface to create and edit your wiki. ### Save your wiki Saving can be a bit tricky but it\u0027s not too bad. Use the checkmark in the side pane to save your changes. When there are unsaved changes, the checkmark turns red. ![TiddlyWiki Side Panel Red Checkmark Save Changes](http://cdn.lqdev.tech/files/images/tiddlywiki-save-changes.png) To save your changes, overwrite the existing file or if you want, you can also save it to a file with a different name. I would recommend saving it to the same file because it\u0027s easier to manage. You don\u0027t want to end up with *index-version12341.html*. That being said, I don\u0027t know if there may be risks in overwriting the file. In my limited testing I had no issues overwriting the file, but I\u0027d still tread lightly. ### Stop the server Once you\u0027re done viewing and editing your wiki, stop the Live Preview server: 1. In VS Code, open the command palette **(View \u003E Command Palette)**. 1. Enter into the text box *Live Preview: Stop Server* and press *Enter*. The server should stop. ## Tips ### Plugins I prefer working in Markdown and since some of my notes include code, I addded two plugins to my wiki: - [Markdown](https://tiddlywiki.com/#Markdown%20Plugin) - [Highlight](https://tiddlywiki.com/#Highlight%20Plugin) For more information, see [Installing a plugin from the plugin library](https://tiddlywiki.com/#Installing%20a%20plugin%20from%20the%20plugin%20library). ### Things to watch out for in VS Code When using the Live Preview extension, some things might not work such as keyboard shortcuts and navigating to external links. Since a local server is running you should be able to use the browser as well. However, I would recommend only using the browser to *view* your wiki. Trying to save changes made in the browser will download the HTML page containing your changes. ### Use Git Autosaving to Git is one of the ways you can save your TiddlyWiki. However, you have to enter and use your credentials. Since your wiki is a static HTML page, just save it like you would any other HTML file in a repo. Doing so also makes it easy to backup to a previous version in case data is corrupted or lost. ## Conclusion I this post, I shared how TiddlyWiki provides a relatively low overhead, static-website friendly way of creating wikis. If you use VS Code, you can use the Live Preview extension to view and edit your wiki. Although my intended use for TiddlyWiki is as a personal knowledge management solution, I can also think of ways you could use TiddlyWiki as a low-friction way of building your own website. The WYSIWYG functionality makes creating content extremely more accessible than static site generators and similar to WordPress. In contrast to WordPress though, since you only have a single HTML file, it\u0027s less overhead to manage and also low-cost since you can [host it on GitHub Pages for free](https://nesslabs.com/tiddlywiki-static-website-generator) or any other file hosting service. In the near future, I plan to migrate my wiki and snippets to use TiddlyWiki.","contentType":"posts","url":"https://www.lqdev.me/posts/tiddlywiki-vscode","date":"2022-10-21 08:32 -05:00","tags":["web","vscode","wiki","personalknowledgemanagement","git","website"],"summary":"## Introduction A while back, I set up a knowledge base (KB) section on my website. As part of that knowledge base, I have a [wiki](/resources/wiki) and [snippets](/resources/snippets) section. As...","keywords":["tiddlywiki","wiki","file","html","save","changes","code","server","https://tiddlywiki","since"]},{"id":"reshare-pytorch-lf-top-level-project","title":"PyTorch Joins Linux Foundation as Top Level Project","content":"PyTorch Joins Linux Foundation as Top Level Project \u2192 https://pytorch.org/blog/PyTorchfoundation/ PyTorch is moving to the Linux Foundation (LF) as a top-level project under the name PyTorch Foundation. The PyTorch Technical Governance now supports a hierarchical maintainer structure and clear outlining of processes around day to day work and escalations. This doesn\u2019t change how we run things, but it does add discipline and openness that at our scale feels essential and timely.","contentType":"reshare","url":"https://www.lqdev.me/responses/pytorch-lf-top-level-project","date":"2022-10-20 01:07 -05:00","tags":[],"summary":"PyTorch Joins Linux Foundation as Top Level Project \u2192 https://pytorch.org/blog/PyTorchfoundation/ PyTorch is moving to the Linux Foundation (LF) as a top-level project under the name PyTorch...","keywords":["pytorch","foundation","linux","level","project","around","escalations","doesn\u2019t","change","things"]},{"id":"star-pocket-casts-mobile-open-source","title":"Pocket Casts Mobile Apps Are Now Open Source","content":"Pocket Casts Mobile Apps Are Now Open Source \u2192 https://blog.pocketcasts.com/2022/10/19/pocket-casts-mobile-apps-are-now-open-source/ We\u2019ve been eager to take this step since we joined Automattic last year... Wow. Yet another company I didn\u0027t know was owned by Automaticc (parent company of WordPress). So not only do they have a stake in regular blogs and websites, but they also are in the microblogging space with Tumblr. With Pocket Casts they\u0027re now into podcasts too. Good for them. We believe that podcasting can not and should not be controlled by Apple and Spotify, and instead support a diverse ecosystem of third-party clients. I couldn\u0027t agree more. Though to be fair to Apple, in all the years since podcasts have been a thing, despite them being one of the main indices, they didn\u0027t make any overt attempts to lock down the ecosystem. It\u0027s not until companies like Amazon and Spotify tried to make certain content platform exclusives that the ecosystem has started to feel more closed.","contentType":"star","url":"https://www.lqdev.me/responses/pocket-casts-mobile-open-source","date":"2022-10-20 00:23 -05:00","tags":[],"summary":"Pocket Casts Mobile Apps Are Now Open Source \u2192 https://blog.pocketcasts.com/2022/10/19/pocket-casts-mobile-apps-are-now-open-source/ We\u2019ve been eager to take this step since we joined Automattic last...","keywords":["they","casts","ecosystem","pocket","podcasts","more","apple","company","since","didn"]},{"id":"bookmarks-fastai-from-dl-stable-diffusion","title":"From Deep Learning Foundations to Stable Diffusion","content":"From Deep Learning Foundations to Stable Diffusion \u2192 https://www.fast.ai/posts/part2-2022-preview.html In total, we\u2019re releasing four videos, with around 5.5 hours of content, covering the following topics (the lesson numbers start at \u201C9\u201D, since this is a continuation of Practical Deep Learning for Coders part 1, which had 8 lessons): Lesson 9 by Jeremy Howard: How to use Diffusers pipelines; What are the conceptual parts of Stable Diffusion Lesson 9A by Jonathan Whitaker: A deep dive into Stable Diffusion concepts and code Lesson 9B by Wasim Lorgat and Tanishq Abraham: The math of diffusion Lesson 10 by Jeremy Howard: Creating a custom diffusion pipeline; Starting \u201Cfrom the foundations\u201D","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/fastai-from-dl-stable-diffusion","date":"2022-10-19 23:52 -05:00","tags":[],"summary":"From Deep Learning Foundations to Stable Diffusion \u2192 https://www.fast.ai/posts/part2-2022-preview.html In total, we\u2019re releasing four videos, with around 5.5 hours of content, covering the following...","keywords":["diffusion","lesson","deep","stable","learning","jeremy","howard:","from","lessons","diffusers"]},{"id":"star-staltz-thoughts-bluesky-at-protocol","title":"Thoughts on Bluesky AT Protocol","content":"Thoughts on Bluesky AT Protocol \u2192 https://twitter.com/andrestaltz/status/1582448952057958401 My experience working on SSB (i.e. non-crypto fully decentralized protocol) is suitable for small world communication, and definitely not suitable for big world. Email proves that federation can do big world.","contentType":"star","url":"https://www.lqdev.me/responses/staltz-thoughts-bluesky-at-protocol","date":"2022-10-19 09:56 -05:00","tags":[],"summary":"Thoughts on Bluesky AT Protocol \u2192 https://twitter.com/andrestaltz/status/1582448952057958401 My experience working on SSB (i.e. non-crypto fully decentralized protocol) is suitable for small world...","keywords":["world","protocol","suitable","thoughts","email","definitely","communication","small","decentralized","fully"]},{"id":"bookmarks-appleton-garden-history","title":"A Brief History \u0026 Ethos of the Digital Garden","content":"A Brief History \u0026amp; Ethos of the Digital Garden \u2192 https://maggieappleton.com/garden-history The conversational feed design of email inboxes, group chats, and InstaTwitBook is fleeting \u2013 they\u0027re only concerned with self-assertive immediate thoughts that rush by us in a few moments...But streams only surface the Zeitgeisty ideas of the last 24 hours...Gardens present information in a richly linked landscape that grows slowly over time...The garden helps us move away from time-bound streams and into contextual knowledge spaces. The Six Patterns of Gardening: Topography over Timelines - Gardens are organised around contextual relationships and associative links; the concepts and themes within each note determine how it\u0027s connected to others. Continuous Growth - Gardens are never finished, they\u0027re constantly growing, evolving, and changing. Imperfection \u0026amp; Learning in Public - Gardens are imperfect by design. They don\u0027t hide their rough edges or claim to be a permanent source of truth. Playful, Personal, and Experimental - Gardens are non-homogenous by nature. You can plant the same seeds as your neighbour, but you\u0027ll always end up with a different arrangement of plants. Intercropping \u0026amp; Content Diversity - Gardens are not just a collection of interlinked words...Podcasts, videos, diagrams, illustrations, interactive web animations, academic papers, tweets, rough sketches, and code snippets should all live and grow in the garden. Independent Ownership - Gardening is about claiming a small patch of the web for yourself, one you fully own and control... If you give it a bit of forethought, you can build your garden in a way that makes it easy to transfer and adapt. Platforms and technologies will inevitably change. Using old-school, reliable, and widely used web native formats like HTML/CSS is a safe bet.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/appleton-garden-history","date":"2022-10-19 00:03 -05:00","tags":[],"summary":"A Brief History \u0026amp; Ethos of the Digital Garden \u2192 https://maggieappleton.com/garden-history The conversational feed design of email inboxes, group chats, and InstaTwitBook is fleeting \u2013 they\u0027re...","keywords":["gardens","garden","\u0026amp","they","rough","streams","design","contextual","history","illustrations"]},{"id":"reshare-bluesky-at-protocol","title":"Bluesky - The Authenticated Transport (AT) Protocol","content":"Bluesky - The Authenticated Transport (AT) Protocol \u2192 https://blueskyweb.org/blog/10-18-2022-the-at-protocol Bluesky was created to build a social protocol. In the spring, we released \u201CADX,\u201D the very first iteration of the protocol...ADX is now the \u201CAuthenticated Transport Protocol\u0026quot;...more simply, the \u201CAT Protocol.\u201D The \u201CAT Protocol\u201D is a new federated social network What makes AT Protocol unique: Account portability Algorithmic choice Interoperation Performance AT Procol website","contentType":"reshare","url":"https://www.lqdev.me/responses/bluesky-at-protocol","date":"2022-10-18 17:23 -05:00","tags":[],"summary":"Bluesky - The Authenticated Transport (AT) Protocol \u2192 https://blueskyweb.org/blog/10-18-2022-the-at-protocol Bluesky was created to build a social protocol. In the spring, we released \u201CADX,\u201D the very...","keywords":["protocol","bluesky","transport","social","performance","interoperation","choice","algorithmic","portability","account"]},{"id":"star-overdue-insights-daily-blogging","title":"Overdue insights on daily blogging","content":"Overdue insights on daily blogging \u2192 https://herbertlui.net/overdue-insights-on-daily-blogging/ Here are a bunch of not-so-obvious lessons I\u2019ve internalized through writing each day: Writing can be a starting point, not an ending one Write to think The power of DIFY: Do it for yourself, don\u2019t think too much about what you want other people to get out of it. Think small Gain energy A lot of books are collections of blog posts Small is an unblocker","contentType":"star","url":"https://www.lqdev.me/responses/overdue-insights-daily-blogging","date":"2022-10-18 17:20 -05:00","tags":[],"summary":"Overdue insights on daily blogging \u2192 https://herbertlui.net/overdue-insights-on-daily-blogging/ Here are a bunch of not-so-obvious lessons I\u2019ve internalized through writing each day: Writing can be a...","keywords":["small","insights","daily","writing","dify:","yourself","don\u2019t","much","want","overdue"]},{"id":"reshare-patterns-collaborative-blogging","title":"Patterns for collaborative blogging","content":"Patterns for collaborative blogging \u2192 https://visitmy.website/2022/10/04/patterns-for-collaborative-blogging/","contentType":"reshare","url":"https://www.lqdev.me/responses/patterns-collaborative-blogging","date":"2022-10-18 17:19 -05:00","tags":[],"summary":"Patterns for collaborative blogging \u2192 https://visitmy.website/2022/10/04/patterns-for-collaborative-blogging/","keywords":["collaborative","patterns","blogging","https://visitmy","website/2022/10/04/patterns","blogging/"]},{"id":"bookmarks-bidaf-mlnet-onnx","title":"Bidirectional Attention Flow (BiDAF) ML.NET ONNX Sample","content":"Bidirectional Attention Flow (BiDAF) ML.NET ONNX Sample \u2192 https://gist.github.com/luisquintanilla/164176ec414e465246d6323aa62b38df This sample shows how to use a pretrained Bidirectional Attention Flow (BiDAF) ONNX model in ML.NET for answering a query about a given context paragraph.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/bidaf-mlnet-onnx","date":"2022-10-18 17:16 -05:00","tags":[],"summary":"Bidirectional Attention Flow (BiDAF) ML.NET ONNX Sample \u2192 https://gist.github.com/luisquintanilla/164176ec414e465246d6323aa62b38df This sample shows how to use a pretrained Bidirectional Attention...","keywords":["bidirectional","attention","flow","bidaf","onnx","sample","given","query","answering","model"]},{"id":"bookmarks-illustrated-stable-diffusion","title":"The Illustrated Stable Diffusion","content":"The Illustrated Stable Diffusion \u2192 https://jalammar.github.io/illustrated-stable-diffusion/","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/illustrated-stable-diffusion","date":"2022-10-18 09:25 -05:00","tags":[],"summary":"The Illustrated Stable Diffusion \u2192 https://jalammar.github.io/illustrated-stable-diffusion/","keywords":["stable","illustrated","diffusion","https://jalammar","github","io/illustrated","diffusion/"]},{"id":"reshare-http-archive-state-of-web-2022-almanac","title":"HTTP Archive State of the Web 2022 Report","content":"HTTP Archive State of the Web 2022 Report \u2192 https://almanac.httparchive.org/en/2022/ The Web Almanac is a comprehensive report on the state of the web, backed by real data and trusted web experts. The 2022 edition is comprised of 23 chapters spanning aspects of page content, user experience, publishing, and distribution. Chapter 10: SEO Chapter 13: Privacy Chapter 17: PWA Chapter 19: Jamstack Chapter 21: Page Weight","contentType":"reshare","url":"https://www.lqdev.me/responses/http-archive-state-of-web-2022-almanac","date":"2022-10-18 09:20 -05:00","tags":[],"summary":"HTTP Archive State of the Web 2022 Report \u2192 https://almanac.httparchive.org/en/2022/ The Web Almanac is a comprehensive report on the state of the web, backed by real data and trusted web experts....","keywords":["chapter","state","2022","report","page","http","privacy","distribution","publishing","experience"]},{"id":"bookmarks-illustrated-gpt-2","title":"The Illustrated GPT-2","content":"The Illustrated GPT-2 \u2192 http://jalammar.github.io/illustrated-gpt2/","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/illustrated-gpt-2","date":"2022-10-17 19:03 -05:00","tags":[],"summary":"The Illustrated GPT-2 \u2192 http://jalammar.github.io/illustrated-gpt2/","keywords":["illustrated","http://jalammar","github","io/illustrated","gpt2/"]},{"id":"reshare-readium-lcp-drm-technology","title":"Readium LCP - Digital Rights Management Technology","content":"Readium LCP - Digital Rights Management Technology \u2192 https://blog.archive.org/2022/10/10/new-ebook-protection-software-gaining-popularity-among-publishers-and-libraries/ Readium LCP was developed five years ago to protect digital files from unauthorized distribution. Unlike proprietary platforms, the technology is open to anyone who wants to look inside the codebase and make improvements. It is a promising alternative for libraries and users wanting to avoid the limitations of traditional DRM.","contentType":"reshare","url":"https://www.lqdev.me/responses/readium-lcp-drm-technology","date":"2022-10-16 11:14 -05:00","tags":[],"summary":"Readium LCP - Digital Rights Management Technology \u2192 https://blog.archive.org/2022/10/10/new-ebook-protection-software-gaining-popularity-among-publishers-and-libraries/ Readium LCP was developed...","keywords":["readium","digital","technology","unlike","proprietary","platforms","open","anyone","wants","look"]},{"id":"media-fall-mountains","title":"Fall Mountains","content":":::media - url: \u0022http://cdn.lqdev.tech/files/images/sunrise.jpg\u0022 alt: \u0022Sunrise at mountain summit with mist and fall foliage\u0022 mediaType: \u0022image\u0022 aspectRatio: \u0022landscape\u0022 caption: \u0022Sunrise\u0022 - url: \u0022http://cdn.lqdev.tech/files/images/sunset.jpg\u0022 alt: \u0022Sunset at lake with mountains in the background\u0022 mediaType: \u0022image\u0022 aspectRatio: \u0022landscape\u0022 caption: \u0022Sunset\u0022 :::media","contentType":"media","url":"https://www.lqdev.me/media/fall-mountains","date":"2022-10-15 08:56 -05:00","tags":[],"summary":":::media - url: \u0022http://cdn.lqdev.tech/files/images/sunrise.jpg\u0022 alt: \u0022Sunrise at mountain summit with mist and fall foliage\u0022 mediaType: \u0022image\u0022 aspectRatio: \u0022landscape\u0022 caption: \u0022Sunrise\u0022 - url:...","keywords":[":::media","url:","http://cdn","lqdev","alt:","sunrise","sunset","caption:","landscape","aspectratio:"]},{"id":"star-leo-visualization","title":"Low Earth Orbit Visualization","content":"Low Earth Orbit Visualization \u2192 https://platform.leolabs.space/visualization","contentType":"star","url":"https://www.lqdev.me/responses/leo-visualization","date":"2022-10-14 21:57 -05:00","tags":[],"summary":"Low Earth Orbit Visualization \u2192 https://platform.leolabs.space/visualization","keywords":["earth","orbit","visualization","https://platform","leolabs","space/visualization"]},{"id":"reshare-discueu-comments-static-page","title":"Discu.eu - Blog comments on a static site via social networks","content":"Discu.eu - Blog comments on a static site via social networks \u2192 https://berk.es/2022/10/12/blog-comments-on-a-static-site-via-social-networks/ discu.eu, a fantastic service that you provide with an URL and then gives back results from various social networks. Places where that URL is discussed. Hackernews, Reddit, and/or Lobsters. It has an API, that I can call with some JavaScript and then insert in the page.","contentType":"reshare","url":"https://www.lqdev.me/responses/discueu-comments-static-page","date":"2022-10-14 15:41 -05:00","tags":[],"summary":"Discu.eu - Blog comments on a static site via social networks \u2192 https://berk.es/2022/10/12/blog-comments-on-a-static-site-via-social-networks/ discu.eu, a fantastic service that you provide with an...","keywords":["social","discu","comments","static","site","networks","then","places","discussed","hackernews"]},{"id":"reshare-ejabberd-matrix-protocol","title":"Matrix protocol added to ejabberd","content":"Matrix protocol added to ejabberd \u2192 https://www.process-one.net/blog/matrix-protocol-added-to-ejabberd/","contentType":"reshare","url":"https://www.lqdev.me/responses/ejabberd-matrix-protocol","date":"2022-10-14 15:39 -05:00","tags":[],"summary":"Matrix protocol added to ejabberd \u2192 https://www.process-one.net/blog/matrix-protocol-added-to-ejabberd/","keywords":["protocol","added","matrix","ejabberd","https://www","process","net/blog/matrix","ejabberd/"]},{"id":"reshare-hello-decentralized-identity-coop","title":"Identity - The Hell\u014D Cooperative","content":"Identity - The Hell\u014D Cooperative \u2192 https://www.hello.coop/pages/approach.html","contentType":"reshare","url":"https://www.lqdev.me/responses/hello-decentralized-identity-coop","date":"2022-10-14 15:36 -05:00","tags":[],"summary":"Identity - The Hell\u014D Cooperative \u2192 https://www.hello.coop/pages/approach.html","keywords":["identity","hell\u014D","cooperative","https://www","hello","coop/pages/approach","html"]},{"id":"reshare-swiss-dwelling-apartment-dataset","title":"Swiss Dwellings: A large dataset of apartment models","content":"Swiss Dwellings: A large dataset of apartment models \u2192 https://zenodo.org/record/7070952#.Y0m5eX7MK00 This dataset contains detailed data on 42,207 apartments (242,257 rooms) in 3,093 buildings including their geometries, room typology as well as their visual, acoustical, topological and daylight characteristics.","contentType":"reshare","url":"https://www.lqdev.me/responses/swiss-dwelling-apartment-dataset","date":"2022-10-14 15:33 -05:00","tags":[],"summary":"Swiss Dwellings: A large dataset of apartment models \u2192 https://zenodo.org/record/7070952#.Y0m5eX7MK00 This dataset contains detailed data on 42,207 apartments (242,257 rooms) in 3,093 buildings...","keywords":["dataset","swiss","topological","acoustical","visual","typology","room","geometries","including","buildings"]},{"id":"bookmarks-understanding-html-large-language-models","title":"Understanding HTML with Large Language Models","content":"Understanding HTML with Large Language Models \u2192 https://arxiv.org/abs/2210.03945 Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding...have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. Out of the LLMs we evaluate, we show evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/understanding-html-large-language-models","date":"2022-10-14 15:29 -05:00","tags":[],"summary":"Understanding HTML with Large Language Models \u2192 https://arxiv.org/abs/2210.03945 Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their...","keywords":["html","models","understanding","language","llms","large","capabilities","classification","elements","description"]},{"id":"bookmarks-kaggle-state-data-science-ml-2022","title":"Kaggle - State of Data Science and Machine Learning 2022","content":"Kaggle - State of Data Science and Machine Learning 2022 \u2192 https://www.kaggle.com/kaggle-survey-2022 Development Python and SQL remain the two most common programming skills for data scientists VSCode is now used by over 50% of working data scientists Notebooks are a popular environment as well. Colab notebooks are the most popular cloud-based Jupyter notebook environment Makes sense especially since Kaggle is owned by Google. Machine Learning Kaggle DS \u0026amp; ML Survey 2022 Scikit-learn is the most popular ML framework while PyTorch has been growing steadily year-over-year LightGBM, XGBoost are also among the top frameworks. Transformer architectures are becoming more popular for deep learning models (both image and text data) Cloud computing All major cloud computing providers saw strong year over year growth in 2022 Specialized hardware like Tensor Processing Units (TPUs) is gaining initial traction with Kaggle data scientists Resources PDF Dataset","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/kaggle-state-data-science-ml-2022","date":"2022-10-14 13:59 -05:00","tags":[],"summary":"Kaggle - State of Data Science and Machine Learning 2022 \u2192 https://www.kaggle.com/kaggle-survey-2022 Development Python and SQL remain the two most common programming skills for data scientists...","keywords":["kaggle","data","2022","popular","year","cloud","learning","scientists","computing","environment"]},{"id":"notes-impressed-az-cloud-shell","title":"Impressed by Azure Cloud Shell","content":"I can probably count the number of times I\u0027ve used the Azure Cloud Shell inside the Azure Portal with one hand. However, I recently had a need to use it and was impressed by the experience. I wanted to create an Azure Function app but didn\u0027t want to install all the tools required. Currently I\u0027m running Manjaro on an ASUS L210MA. That\u0027s my daily driver and although the specs are incredibly low I like the portability. The Azure set of tools run on Linux but are easier to install on Debian distributions. The low specs plus the Arch distro didn\u0027t make me want to go through the process of installing the Azure tools locally for a throwaway app. That\u0027s when I thought about using Azure Cloud Shell. Setup was relatively quick and easy. It was also great to see the latest version of .NET, Azure Functions Core Tools, and Azure CLI were already installed. Even Git was already there. After creating the function app, I was able to make some light edits using the built-in Monaco editor. When I was ready to test the app, I was able to open up a port, create a proxy, and test the function locally. It\u0027s not a high-end machine so I won\u0027t be training machine learning models on it anytime soon. However, for experimentation and working with my Azure account it\u0027s more than I need.","contentType":"notes","url":"https://www.lqdev.me/notes/impressed-az-cloud-shell","date":"2022-10-13 22:45 -05:00","tags":["azure","cloud","tools","development"],"summary":"I can probably count the number of times I\u0027ve used the Azure Cloud Shell inside the Azure Portal with one hand. However, I recently had a need to use it and was impressed by the experience. I wanted...","keywords":["azure","tools","function","create","already","install","able","need","machine","however"]},{"id":"reply-streaming-companies-lost-plot","title":"Streaming services lost the plot","content":"Streaming services lost the plot \u2192 https://www.jeffgeerling.com/blog/2022/streaming-services-lost-plot I got serious about consolidating the media my family consumes; I decided to buy blu-ray and DVD copies of all the movies and TV shows we actually cared about, and rip them onto a NAS. I\u0027m glad I\u0027ve gone down this path, and become more intentional about my media consumption, especially as companies are deleting already-purchased content from users\u0027 media libraries! It\u0027s sickening (and, IMHO, should be illegal) they can show a \u0027buy\u0027 button for a DRM\u0027ed digital downloads that the user never actually \u0027owns\u0027. Couldn\u0027t agree more. Although the focus of this post is on video, you can say the same for music and books.","contentType":"reply","url":"https://www.lqdev.me/responses/streaming-companies-lost-plot","date":"2022-10-13 15:06 -05:00","tags":[],"summary":"Streaming services lost the plot \u2192 https://www.jeffgeerling.com/blog/2022/streaming-services-lost-plot I got serious about consolidating the media my family consumes; I decided to buy blu-ray and DVD...","keywords":["media","actually","services","lost","plot","more","button","show","they","illegal"]},{"id":"reshare-matrix-element-android-redesign-2022","title":"Element - New app layout for Android","content":"Element - New app layout for Android \u2192 https://twitter.com/element_hq/status/1580555805803261952","contentType":"reshare","url":"https://www.lqdev.me/responses/matrix-element-android-redesign-2022","date":"2022-10-13 10:32 -05:00","tags":[],"summary":"Element - New app layout for Android \u2192 https://twitter.com/element_hq/status/1580555805803261952","keywords":["element","layout","android","https://twitter","com/element","hq/status/1580555805803261952"]},{"id":"star-doc-quixote","title":"Doc Quixote","content":"Doc Quixote \u2192 http://scripting.com/2022/10/01/133834.html?title=docQuixote I spent great time, energy and money, over many years to create the writing and programming environment I wanted to use and I wanted my peers to use, so we could work together to create species-saving communication tools, and just beauty... ...I read the story of David Bowie\u0027s last days, he did something amazing when he knew he had a short time to live. He stepped back and got out of the way. He understood this is no longer his world. When you\u0027re young, you think expansively, and as you get old reality sinks in and your imagination contracts. The horizon gets closer and closer. We don\u0027t get to mold the world, we are not gods, no matter how good or generous, smart of ruthless you may be, we all start out young and if we\u0027re lucky we get old and then we\u0027re gone.","contentType":"star","url":"https://www.lqdev.me/responses/doc-quixote","date":"2022-10-12 23:30 -05:00","tags":[],"summary":"Doc Quixote \u2192 http://scripting.com/2022/10/01/133834.html?title=docQuixote I spent great time, energy and money, over many years to create the writing and programming environment I wanted to use and...","keywords":["wanted","world","young","create","closer","live","stepped","back","understood","longer"]},{"id":"bookmarks-2022-state-of-ai-report","title":"State of AI Report 2022 ","content":"State of AI Report 2022 \u2192 https://www.stateof.ai/2022-report-launch.html Key takeaways: New independent research labs are rapidly open sourcing the closed source output of major labs. Safety is gaining awareness among major AI research entities The China-US AI research gap has continued to widen AI-driven scientific research continues to lead to breakthroughs","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/2022-state-of-ai-report","date":"2022-10-12 19:07 -05:00","tags":[],"summary":"State of AI Report 2022 \u2192 https://www.stateof.ai/2022-report-launch.html Key takeaways: New independent research labs are rapidly open sourcing the closed source output of major labs. Safety is...","keywords":["research","report","labs","major","state","safety","gaining","awareness","among","entities"]},{"id":"bookmarks-cs631-advanced-programming-unix-env","title":"Advanced Programming in the UNIX Environment","content":"Advanced Programming in the UNIX Environment \u2192 https://stevens.netmeister.org/631/ In this course, students will learn to develop complex system-level software in the C programming language while gaining an intimate understanding of the Unix operating system (and all OS that belong to this family, such as Linux, the BSDs, and even Mac OS X) and its programming environment. Topics covered will include the user/kernel interface, fundamental concepts of Unix, user authentication, basic and advanced I/O, fileystems, signals, process relationships, and interprocess communication. Fundamental concepts of software development and maintenance on Unix systems (development and debugging tools such as \u0026quot;make\u0026quot; and \u0026quot;gdb\u0026quot;) will also be covered.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/cs631-advanced-programming-unix-env","date":"2022-10-12 18:29 -05:00","tags":[],"summary":"Advanced Programming in the UNIX Environment \u2192 https://stevens.netmeister.org/631/ In this course, students will learn to develop complex system-level software in the C programming language while...","keywords":["unix","programming","advanced","fundamental","concepts","software","system","development","covered","environment"]},{"id":"star-how-wine-works","title":"How Wine works 101","content":"How Wine works 101 \u2192 https://werat.dev/blog/how-wine-works-101/","contentType":"star","url":"https://www.lqdev.me/responses/how-wine-works","date":"2022-10-12 18:25 -05:00","tags":[],"summary":"How Wine works 101 \u2192 https://werat.dev/blog/how-wine-works-101/","keywords":["wine","works","https://werat","dev/blog/how","101/"]},{"id":"star-sane-app","title":"Sane - Infopunk\u0027s Digital Garden","content":"Sane - Infopunk\u0026#39;s Digital Garden \u2192 https://www.sane.fyi/","contentType":"star","url":"https://www.lqdev.me/responses/sane-app","date":"2022-10-11 22:42 -05:00","tags":[],"summary":"Sane - Infopunk\u0026#39;s Digital Garden \u2192 https://www.sane.fyi/","keywords":["sane","infopunk\u0026#39","digital","garden","https://www","fyi/"]},{"id":"star-jamesg-fragmentation-links","title":"Adding fragmention links to my website","content":"Adding fragmention links to my website \u2192 https://jamesg.blog/2022/10/11/fragmention-links/","contentType":"star","url":"https://www.lqdev.me/responses/jamesg-fragmentation-links","date":"2022-10-11 21:40 -05:00","tags":[],"summary":"Adding fragmention links to my website \u2192 https://jamesg.blog/2022/10/11/fragmention-links/","keywords":["adding","fragmention","links","website","https://jamesg","blog/2022/10/11/fragmention","links/"]},{"id":"reshare-nlp-metasurvey-results","title":"Researchers Publish Results of NLP Community Metasurvey","content":"Researchers Publish Results of NLP Community Metasurvey \u2192 https://www.infoq.com/news/2022/10/nlp-community-metasurvey/ Relevant links: Results Report","contentType":"reshare","url":"https://www.lqdev.me/responses/nlp-metasurvey-results","date":"2022-10-11 20:38 -05:00","tags":[],"summary":"Researchers Publish Results of NLP Community Metasurvey \u2192 https://www.infoq.com/news/2022/10/nlp-community-metasurvey/ Relevant links: Results Report","keywords":["results","community","researchers","publish","metasurvey","https://www","infoq","com/news/2022/10/nlp","metasurvey/","relevant"]},{"id":"bookmarks-impnet-undetectable-backdoors-compiled-nn","title":"ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks","content":"ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks \u2192 https://arxiv.org/pdf/2210.00108.pdf ...backdoors can be added during compilation, circumventing any safeguards in the data preparation and model training stages. some backdoors, such as ImpNet, can only be reliably detected at the stage where they are inserted and removing them anywhere else presents a significant challenge. machine-learning model security requires assurance of provenance along the entire technical pipeline, including the data, model architecture, compiler, and hardware specification.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/impnet-undetectable-backdoors-compiled-nn","date":"2022-10-11 20:22 -05:00","tags":[],"summary":"ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks \u2192 https://arxiv.org/pdf/2210.00108.pdf ...backdoors can be added during compilation, circumventing any safeguards...","keywords":["backdoors","model","data","impnet:","removing","anywhere","else","presents","significant","challenge"]},{"id":"bookmarks-indieweb-bookmark-post","title":"Indieweb Bookmark Post","content":"Indieweb Bookmark Post \u2192 https://indieweb.org/bookmark A bookmark (or linkblog) is a post that is primarily comprised of a URL, often title text from that URL, sometimes optional text describing, tagging, or quoting from its contents. Bookmarks are useful for saving things to read later or build a recommended reading list.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/indieweb-bookmark-post","date":"2022-10-10 20:39 -05:00","tags":[],"summary":"Indieweb Bookmark Post \u2192 https://indieweb.org/bookmark A bookmark (or linkblog) is a post that is primarily comprised of a URL, often title text from that URL, sometimes optional text describing,...","keywords":["bookmark","post","text","from","indieweb","recommended","build","later","read","things"]},{"id":"star-manning-rediscover-joy-coding-fsharp","title":"Rediscover the Joy in Coding: learn F#!","content":"Rediscover the Joy in Coding: learn F#! \u2192 https://freecontent.manning.com/rediscover-the-joy-in-coding-learn-f","contentType":"star","url":"https://www.lqdev.me/responses/manning-rediscover-joy-coding-fsharp","date":"2022-10-10 18:28 -05:00","tags":[],"summary":"Rediscover the Joy in Coding: learn F#! \u2192 https://freecontent.manning.com/rediscover-the-joy-in-coding-learn-f","keywords":["learn","rediscover","coding:","https://freecontent","manning","com/rediscover","coding"]},{"id":"star-indie-social-media-save-us","title":"Can Indie Social Media Save Us?","content":"Can Indie Social Media Save Us? \u2192 https://www.newyorker.com/tech/annals-of-technology/can-indie-social-media-save-us","contentType":"star","url":"https://www.lqdev.me/responses/indie-social-media-save-us","date":"2022-10-09 22:09 -05:00","tags":[],"summary":"Can Indie Social Media Save Us? \u2192 https://www.newyorker.com/tech/annals-of-technology/can-indie-social-media-save-us","keywords":["indie","social","media","save","https://www","newyorker","com/tech/annals","technology/can"]},{"id":"notes-til-automaticc-tumblr","title":"TIL: Automaticc owns Tumblr","content":"Last I heard of Tumblr, they were owned by Verizon. While readying the book Indie Microblogging by Manton Reece, I learned (and later confirmed) they were owned by Automaticc, the company behind WordPress. Out of the the different homes it\u0027s had, I can\u0027t think of a better place for it.","contentType":"notes","url":"https://www.lqdev.me/notes/til-automaticc-tumblr","date":"2022-10-09 20:49 -05:00","tags":["tumblr","automattic","til","socialmedia","blogging"],"summary":"Last I heard of Tumblr, they were owned by Verizon. While readying the book Indie Microblogging by Manton Reece, I learned (and later confirmed) they were owned by Automaticc, the company behind...","keywords":["they","owned","last","homes","different","wordpress","behind","company","automaticc","confirmed"]},{"id":"star-handwrittern-blog","title":"Handwritten Blog","content":"Handwritten Blog \u2192 https://handwritten.blog/","contentType":"star","url":"https://www.lqdev.me/responses/handwrittern-blog","date":"2022-10-09 14:25 -05:00","tags":[],"summary":"Handwritten Blog \u2192 https://handwritten.blog/","keywords":["handwritten","blog","https://handwritten","blog/"]},{"id":"notes-almost-ready-to-receive-webmentions","title":"ALMOST ready to receive Webmentions","content":"Finally got a working prototype. A few days ago I added the request and webmention verification components. I just finished creating the web API and Azure Table Storage components. Now I just need to run some additional tests and deploy it \uD83D\uDE42","contentType":"notes","url":"https://www.lqdev.me/notes/almost-ready-to-receive-webmentions","date":"2022-10-06 22:29 -05:00","tags":["webmentions","indieweb","comments"],"summary":"Finally got a working prototype. A few days ago I added the request and webmention verification components. I just finished creating the web API and Azure Table Storage components. Now I just need to...","keywords":["components","additional","need","storage","table","azure","creating","finished","finally","verification"]},{"id":"reply-things-learned-blogging","title":"Things Learned Blogging","content":"Things Learned Blogging \u2192 https://blog.jim-nielsen.com/2021/things-learned-blogging/ Both this post as well as Tom MacWright\u0027s \u0026quot;How to Blog\u0026quot; resonate. I\u0027ve been posting more consistently to my microblog feeds. These posts are more informal, but just the aspect of producing something even if it\u0027s just a short snippet is gratifying. The informality of it makes the posts significantly shorter but the pace at which I publish content and share ideas is faster.","contentType":"reply","url":"https://www.lqdev.me/responses/things-learned-blogging","date":"2022-10-06 15:50 -05:00","tags":[],"summary":"Things Learned Blogging \u2192 https://blog.jim-nielsen.com/2021/things-learned-blogging/ Both this post as well as Tom MacWright\u0027s \u0026quot;How to Blog\u0026quot; resonate. I\u0027ve been posting more consistently to...","keywords":["posts","learned","more","producing","even","short","snippet","gratifying","informality","makes"]},{"id":"reshare-nielsen-favorite-rss-feeds","title":"Jim Nielsen: My Favorite RSS Feeds","content":"Jim Nielsen: My Favorite RSS Feeds \u2192 https://blog.jim-nielsen.com/2019/i-love-rss/ It\u0027s always fun to stumble upon these lists and finding more interesting people and websites to follow.","contentType":"reshare","url":"https://www.lqdev.me/responses/nielsen-favorite-rss-feeds","date":"2022-10-06 15:35 -05:00","tags":[],"summary":"Jim Nielsen: My Favorite RSS Feeds \u2192 https://blog.jim-nielsen.com/2019/i-love-rss/ It\u0027s always fun to stumble upon these lists and finding more interesting people and websites to follow.","keywords":["nielsen:","people","interesting","more","finding","lists","upon","stumble","always","rss/"]},{"id":"reshare-numerical-fnalysis-intro","title":"Numerical F#nalysis 01:Introduction","content":"Numerical F#nalysis 01:Introduction \u2192 https://www.youtube.com/watch?v=QMSmcpn8nLM\u0026amp;ab_channel=ResairchiaVideos","contentType":"reshare","url":"https://www.lqdev.me/responses/numerical-fnalysis-intro","date":"2022-10-06 09:24 -05:00","tags":[],"summary":"Numerical F#nalysis 01:Introduction \u2192 https://www.youtube.com/watch?v=QMSmcpn8nLM\u0026amp;ab_channel=ResairchiaVideos","keywords":["numerical","f#nalysis","01:introduction","https://www","youtube","com/watch","v=qmsmcpn8nlm\u0026amp","channel=resairchiavideos"]},{"id":"reshare-fsharp-python-tf-binding","title":"F# with Python - TensorFlow Binding","content":"F# with Python - TensorFlow Binding \u2192 https://www.compositional-it.com/news-blog/f-with-python-pt-2-tensorflow-binding/","contentType":"reshare","url":"https://www.lqdev.me/responses/fsharp-python-tf-binding","date":"2022-10-06 09:21 -05:00","tags":[],"summary":"F# with Python - TensorFlow Binding \u2192 https://www.compositional-it.com/news-blog/f-with-python-pt-2-tensorflow-binding/","keywords":["python","tensorflow","binding","https://www","compositional","com/news","blog/f","binding/"]},{"id":"reshare-unredacted-mag-issue-4","title":"Unredacted Magazine - Issue 004","content":"Unredacted Magazine - Issue 004 \u2192 https://unredactedmagazine.com/issues/004.pdf","contentType":"reshare","url":"https://www.lqdev.me/responses/unredacted-mag-issue-4","date":"2022-10-05 20:13 -05:00","tags":[],"summary":"Unredacted Magazine - Issue 004 \u2192 https://unredactedmagazine.com/issues/004.pdf","keywords":["unredacted","magazine","issue","https://unredactedmagazine","com/issues/004"]},{"id":"star-indiewebcamp-berlin-2022-location-venues","title":"Locations/Venues - IndieWebCamp Berlin 2022","content":"Locations/Venues - IndieWebCamp Berlin 2022 \u2192 https://archive.org/details/locations-venues-indie-web-camp-berlin-2022 Interesting discussion at about 21:30 on a federated wiki / review aggregator.","contentType":"star","url":"https://www.lqdev.me/responses/indiewebcamp-berlin-2022-location-venues","date":"2022-10-05 15:52 -05:00","tags":[],"summary":"Locations/Venues - IndieWebCamp Berlin 2022 \u2192 https://archive.org/details/locations-venues-indie-web-camp-berlin-2022 Interesting discussion at about 21:30 on a federated wiki / review aggregator.","keywords":["berlin","2022","locations/venues","indiewebcamp","https://archive","org/details/locations","venues","indie","camp","interesting"]},{"id":"reshare-og-social-network-other-peoples-websites","title":"The OG Social Network: Other People\u0027s Websites","content":"The OG Social Network: Other People\u0026#39;s Websites \u2192 https://blog.jim-nielsen.com/2022/other-peoples-websites/ Google\u002B was Google trying to mimic the walled garden of Facebook \u2014 their \u201Chow\u201D of extracting value from the people of the internet. But they already had an answer for Facebook\u2019s News Feed in front of them: Blogger / Google Reader, the read / write of the internet. They provide the tools \u2013 Reader, Blogger, Search \u2014 we provide the personal websites. The open, accessible, indexable web as The Next Great Social Network.","contentType":"reshare","url":"https://www.lqdev.me/responses/og-social-network-other-peoples-websites","date":"2022-10-05 15:25 -05:00","tags":[],"summary":"The OG Social Network: Other People\u0026#39;s Websites \u2192 https://blog.jim-nielsen.com/2022/other-peoples-websites/ Google\u002B was Google trying to mimic the walled garden of Facebook \u2014 their \u201Chow\u201D of...","keywords":["social","provide","reader","blogger","they","google","internet","websites","peoples","feed"]},{"id":"reply-patching-open-web","title":"Patching the open web","content":"Patching the open web \u2192 https://blog.jim-nielsen.com/2022/patching-open-web/#readlists I\u0027ve been doing something similar to readlists, except with RSS feeds. I create a custom recipe in Calibre which pulls and aggregates the 50 latest articles for each feed in my recipe. I limit it to only look at articles published in the past day since I do this every evening. Think of it like the evening paper. The result is an EPUB file which I then import into my e-book reader. A few advantages I\u0027ve found to doing this are: I take a break from the computer. Since the e-book reader is offline, I focus on reading the article and don\u0027t have the option to click on links and get distracted browsing the internet. Since I\u0027m already using the e-book reader, it\u0027s easy to transition to reading a book.","contentType":"reply","url":"https://www.lqdev.me/responses/patching-open-web","date":"2022-10-05 10:46 -05:00","tags":[],"summary":"Patching the open web \u2192 https://blog.jim-nielsen.com/2022/patching-open-web/#readlists I\u0027ve been doing something similar to readlists, except with RSS feeds. I create a custom recipe in Calibre which...","keywords":["book","since","reader","open","evening","doing","which","recipe","reading","articles"]},{"id":"star-minimalism-narcissism","title":"Minimalism as Narcissism","content":"Minimalism as Narcissism \u2192 https://www.thisdaysportion.com/posts/minimalism-as-narcissim/","contentType":"star","url":"https://www.lqdev.me/responses/minimalism-narcissism","date":"2022-10-03 20:56 -05:00","tags":[],"summary":"Minimalism as Narcissism \u2192 https://www.thisdaysportion.com/posts/minimalism-as-narcissim/","keywords":["minimalism","narcissism","https://www","thisdaysportion","com/posts/minimalism","narcissim/"]},{"id":"star-stellarium-1-0","title":"Stellarium 1.0 Released","content":"Stellarium 1.0 Released \u2192 https://stellarium.org/release/2022/10/01/stellarium-1.0.html Can\u0027t believe this is the first time I\u0027d heard of this desktop app. Usually using mobile apps like Sky Guide is convenient when on the go, but Stellarium not only seems to have lots of information, but also cross-platform and web-based.","contentType":"star","url":"https://www.lqdev.me/responses/stellarium-1-0","date":"2022-10-03 20:28 -05:00","tags":[],"summary":"Stellarium 1.0 Released \u2192 https://stellarium.org/release/2022/10/01/stellarium-1.0.html Can\u0027t believe this is the first time I\u0027d heard of this desktop app. Usually using mobile apps like Sky Guide is...","keywords":["stellarium","cross","also","information","lots","seems","convenient","guide","apps","platform"]},{"id":"star-bear-joplin","title":"Note-taking Apps: Bear and Joplin","content":"Note-taking Apps: Bear and Joplin \u2192 https://wilw.dev/blog/2022/10/02/bear-joplin/","contentType":"star","url":"https://www.lqdev.me/responses/bear-joplin","date":"2022-10-02 18:16 -05:00","tags":[],"summary":"Note-taking Apps: Bear and Joplin \u2192 https://wilw.dev/blog/2022/10/02/bear-joplin/","keywords":["note","taking","apps:","bear","joplin","https://wilw","dev/blog/2022/10/02/bear","joplin/"]},{"id":"reply-flylo-terrifier-2","title":"FlyLo - Terrifier 2","content":"FlyLo - Terrifier 2 \u2192 https://twitter.com/flyinglotus/status/1575517121122996225 This \u261D\uFE0F. If you know, you know. \uD83D\uDD25","contentType":"reply","url":"https://www.lqdev.me/responses/flylo-terrifier-2","date":"2022-09-29 12:32 -05:00","tags":[],"summary":"FlyLo - Terrifier 2 \u2192 https://twitter.com/flyinglotus/status/1575517121122996225 This \u261D\uFE0F. If you know, you know. \uD83D\uDD25","keywords":["flylo","terrifier","https://twitter","com/flyinglotus/status/1575517121122996225"]},{"id":"reshare-google-stadia-shutdown","title":"Stadia Shuts Down","content":"Stadia Shuts Down \u2192 https://www.theverge.com/2022/9/29/23378713/google-stadia-shutting-down-game-streaming-january-2023","contentType":"reshare","url":"https://www.lqdev.me/responses/google-stadia-shutdown","date":"2022-09-29 12:24 -05:00","tags":[],"summary":"Stadia Shuts Down \u2192 https://www.theverge.com/2022/9/29/23378713/google-stadia-shutting-down-game-streaming-january-2023","keywords":["stadia","down","shuts","https://www","theverge","com/2022/9/29/23378713/google","shutting","game","streaming","january"]},{"id":"star-how-to-build-personal-webpage-scratch","title":"How to - Build a personal webpage from scratch","content":"How to - Build a personal webpage from scratch \u2192 https://rutar.org/writing/how-to-build-a-personal-webpage-from-scratch/","contentType":"star","url":"https://www.lqdev.me/responses/how-to-build-personal-webpage-scratch","date":"2022-09-29 11:42 -05:00","tags":[],"summary":"How to - Build a personal webpage from scratch \u2192 https://rutar.org/writing/how-to-build-a-personal-webpage-from-scratch/","keywords":["build","personal","webpage","from","scratch","https://rutar","org/writing/how","scratch/"]},{"id":"star-appeal-small-pc","title":"Appeal of Small Computers","content":"Appeal of Small Computers \u2192 https://andregarzia.com/2022/09/The-appeal-of-small-computers.html","contentType":"star","url":"https://www.lqdev.me/responses/appeal-small-pc","date":"2022-09-28 21:42 -05:00","tags":[],"summary":"Appeal of Small Computers \u2192 https://andregarzia.com/2022/09/The-appeal-of-small-computers.html","keywords":["appeal","small","computers","https://andregarzia","com/2022/09/the","html"]},{"id":"reshare-mnt-pocket-reform","title":"MNT Pocket Reform - Open Source Pocket PC","content":"MNT Pocket Reform - Open Source Pocket PC \u2192 https://spectrum.ieee.org/meet-an-open-source-pc-that-can-fit-in-your-pocket","contentType":"reshare","url":"https://www.lqdev.me/responses/mnt-pocket-reform","date":"2022-09-28 21:38 -05:00","tags":[],"summary":"MNT Pocket Reform - Open Source Pocket PC \u2192 https://spectrum.ieee.org/meet-an-open-source-pc-that-can-fit-in-your-pocket","keywords":["pocket","open","source","reform","https://spectrum","ieee","org/meet"]},{"id":"reshare-dalle-no-waitlist","title":"DALL-E Available Without Waitlist","content":"DALL-E Available Without Waitlist \u2192 https://openai.com/blog/dall-e-now-available-without-waitlist/","contentType":"reshare","url":"https://www.lqdev.me/responses/dalle-no-waitlist","date":"2022-09-28 11:36 -05:00","tags":[],"summary":"DALL-E Available Without Waitlist \u2192 https://openai.com/blog/dall-e-now-available-without-waitlist/","keywords":["available","dall","waitlist","https://openai","com/blog/dall","waitlist/"]},{"id":"star-fed-wiki-udell","title":"Federated Wiki","content":"Federated Wiki \u2192 https://blog.jonudell.net/tag/fedwiki/","contentType":"star","url":"https://www.lqdev.me/responses/fed-wiki-udell","date":"2022-09-28 10:44 -05:00","tags":[],"summary":"Federated Wiki \u2192 https://blog.jonudell.net/tag/fedwiki/","keywords":["federated","wiki","https://blog","jonudell","net/tag/fedwiki/"]},{"id":"star-github-english-teachers","title":"GitHub for English Teachers","content":"GitHub for English Teachers \u2192 https://www.infoworld.com/article/3674309/github-for-english-teachers.html","contentType":"star","url":"https://www.lqdev.me/responses/github-english-teachers","date":"2022-09-28 10:42 -05:00","tags":[],"summary":"GitHub for English Teachers \u2192 https://www.infoworld.com/article/3674309/github-for-english-teachers.html","keywords":["english","teachers","github","https://www","infoworld","com/article/3674309/github","html"]},{"id":"reshare-fosdem-2023-dates","title":"FOSDEM 2023 Dates (Feb 4 \u0026 5 2023)","content":"FOSDEM 2023 Dates (Feb 4 \u0026amp; 5 2023) \u2192 https://fosdem.org/2023/news/2022-09-14-fosdem-2023-dates/ It feels like the 2022 conference was just yesterday. In any case, save the date February 4-5,2023.","contentType":"reshare","url":"https://www.lqdev.me/responses/fosdem-2023-dates","date":"2022-09-26 21:48 -05:00","tags":[],"summary":"FOSDEM 2023 Dates (Feb 4 \u0026amp; 5 2023) \u2192 https://fosdem.org/2023/news/2022-09-14-fosdem-2023-dates/ It feels like the 2022 conference was just yesterday. In any case, save the date February 4-5,2023.","keywords":["2023","fosdem","dates","\u0026amp","https://fosdem","org/2023/news/2022","dates/","feels","2022","conference"]},{"id":"reshare-joplin-interview","title":"The story behind Joplin","content":"The story behind Joplin \u2192 https://opensource.com/article/22/9/joplin-interview Good read. Other than Emacs, Joplin is my go-to notetaking application.","contentType":"reshare","url":"https://www.lqdev.me/responses/joplin-interview","date":"2022-09-26 20:44 -05:00","tags":[],"summary":"The story behind Joplin \u2192 https://opensource.com/article/22/9/joplin-interview Good read. Other than Emacs, Joplin is my go-to notetaking application.","keywords":["joplin","story","behind","https://opensource","com/article/22/9/joplin","interview","good","read","emacs","notetaking"]},{"id":"reshare-proton-drive-launch","title":"Launch of Proton Drive - Encrypted Cloud Storage","content":"Launch of Proton Drive - Encrypted Cloud Storage \u2192 https://proton.me/blog/proton-drive-launch","contentType":"reshare","url":"https://www.lqdev.me/responses/proton-drive-launch","date":"2022-09-26 20:15 -05:00","tags":[],"summary":"Launch of Proton Drive - Encrypted Cloud Storage \u2192 https://proton.me/blog/proton-drive-launch","keywords":["launch","drive","proton","encrypted","cloud","storage","https://proton","me/blog/proton"]},{"id":"star-web-matrix-rain","title":"Web-Based Matrix Rain","content":"Web-Based Matrix Rain \u2192 https://github.com/Rezmason/matrix Matrix Rain sample with different variations.","contentType":"star","url":"https://www.lqdev.me/responses/web-matrix-rain","date":"2022-09-25 22:36 -05:00","tags":[],"summary":"Web-Based Matrix Rain \u2192 https://github.com/Rezmason/matrix Matrix Rain sample with different variations.","keywords":["matrix","rain","based","https://github","com/rezmason/matrix","sample","different","variations"]},{"id":"star-mlperf-modular-ml-workgroup","title":"MLPerf - Modular ML for Benchmarks Working Group","content":"MLPerf - Modular ML for Benchmarks Working Group \u2192 https://github.com/mlcommons/ck/blob/master/docs/mlperf-education-workgroup.md","contentType":"star","url":"https://www.lqdev.me/responses/mlperf-modular-ml-workgroup","date":"2022-09-23 10:04 -05:00","tags":[],"summary":"MLPerf - Modular ML for Benchmarks Working Group \u2192 https://github.com/mlcommons/ck/blob/master/docs/mlperf-education-workgroup.md","keywords":["mlperf","modular","benchmarks","working","group","https://github","com/mlcommons/ck/blob/master/docs/mlperf","education","workgroup"]},{"id":"star-nct-goal-setting","title":"Narrative-Commitment-Task (NCT) - Goal Setting","content":"Narrative-Commitment-Task (NCT) - Goal Setting \u2192 https://mariachec.medium.com/on-objectives-okrs-ncts-lessons-from-reinventing-the-company-goal-setting-10eb969e4ca5 Alternative to OKRs","contentType":"star","url":"https://www.lqdev.me/responses/nct-goal-setting","date":"2022-09-23 09:57 -05:00","tags":[],"summary":"Narrative-Commitment-Task (NCT) - Goal Setting \u2192 https://mariachec.medium.com/on-objectives-okrs-ncts-lessons-from-reinventing-the-company-goal-setting-10eb969e4ca5 Alternative to OKRs","keywords":["goal","setting","okrs","narrative","company","reinventing","from","lessons","ncts","objectives"]},{"id":"star-moving-prototypes","title":"Moving with Prototypes","content":"Moving with Prototypes \u2192 https://blog.jim-nielsen.com/2022/moving-with-prototypes/","contentType":"star","url":"https://www.lqdev.me/responses/moving-prototypes","date":"2022-09-22 21:48 -05:00","tags":[],"summary":"Moving with Prototypes \u2192 https://blog.jim-nielsen.com/2022/moving-with-prototypes/","keywords":["moving","prototypes","https://blog","nielsen","com/2022/moving","prototypes/"]},{"id":"reshare-mutant-standard-emojis","title":"Mutant Standard Emojis","content":"Mutant Standard Emojis \u2192 https://mutant.tech/ an alternative...emoji set.","contentType":"reshare","url":"https://www.lqdev.me/responses/mutant-standard-emojis","date":"2022-09-22 20:33 -05:00","tags":[],"summary":"Mutant Standard Emojis \u2192 https://mutant.tech/ an alternative...emoji set.","keywords":["mutant","standard","emojis","https://mutant","tech/","alternative","emoji"]},{"id":"bookmarks-testing-web-design-color-contrast","title":"Testing Web Design Color Contrast","content":"Testing Web Design Color Contrast \u2192 https://web.dev/testing-web-design-color-contrast/ W3C\u2019s Web Accessibility Initiative provides strategies, standards, and resources to ensure that the internet is accessible for as many people as possible. The guidelines that underpin these standards are called the Web Content Accessibility Guidelines, or WCAG. Color contrast is an important piece of the puzzle for accessibility on the web, and adhering to it makes the web more usable for the greatest number of people in the most varied situations. Apps to test contrast: Pika VisBug Chrome Dev Tools","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/testing-web-design-color-contrast","date":"2022-09-22 20:06 -05:00","tags":[],"summary":"Testing Web Design Color Contrast \u2192 https://web.dev/testing-web-design-color-contrast/ W3C\u2019s Web Accessibility Initiative provides strategies, standards, and resources to ensure that the internet is...","keywords":["color","accessibility","design","contrast","guidelines","people","standards","testing","puzzle","adhering"]},{"id":"star-vs-extension-arm-support","title":"ARM Support for VS Extensions","content":"ARM Support for VS Extensions \u2192 https://devblogs.microsoft.com/visualstudio/now-introducing-arm64-support-for-vs-extensions/","contentType":"star","url":"https://www.lqdev.me/responses/vs-extension-arm-support","date":"2022-09-22 20:02 -05:00","tags":[],"summary":"ARM Support for VS Extensions \u2192 https://devblogs.microsoft.com/visualstudio/now-introducing-arm64-support-for-vs-extensions/","keywords":["support","extensions","https://devblogs","microsoft","com/visualstudio/now","introducing","arm64","extensions/"]},{"id":"reshare-tensorstore","title":"TensorStore","content":"TensorStore \u2192 https://ai.googleblog.com/2022/09/tensorstore-for-high-performance.html High-perf, scalable array storage that can be used for scenarios like language models.","contentType":"reshare","url":"https://www.lqdev.me/responses/tensorstore","date":"2022-09-22 19:56 -05:00","tags":[],"summary":"TensorStore \u2192 https://ai.googleblog.com/2022/09/tensorstore-for-high-performance.html High-perf, scalable array storage that can be used for scenarios like language models.","keywords":["high","tensorstore","https://ai","googleblog","com/2022/09/tensorstore","performance","html","perf","scalable","array"]},{"id":"star-open-ai-whisper","title":"Open AI Whisper","content":"Open AI Whisper \u2192 https://openai.com/blog/whisper/","contentType":"star","url":"https://www.lqdev.me/responses/open-ai-whisper","date":"2022-09-22 19:44 -05:00","tags":[],"summary":"Open AI Whisper \u2192 https://openai.com/blog/whisper/","keywords":["open","whisper","https://openai","com/blog/whisper/"]},{"id":"star-five-git-configurations-linux","title":"5 Git configurations I make on Linux","content":"5 Git configurations I make on Linux \u2192 https://opensource.com/article/22/9/git-configuration-linux Create global configuration Set default name Set default email address Set default branch name Set default editor","contentType":"star","url":"https://www.lqdev.me/responses/five-git-configurations-linux","date":"2022-09-22 19:44 -05:00","tags":[],"summary":"5 Git configurations I make on Linux \u2192 https://opensource.com/article/22/9/git-configuration-linux Create global configuration Set default name Set default email address Set default branch name Set...","keywords":["default","linux","configuration","configurations","https://opensource","com/article/22/9/git","create","global","email","address"]},{"id":"reshare-app-home-cooked","title":"An app can be a home-cooked meal","content":"An app can be a home-cooked meal \u2192 https://www.robinsloan.com/notes/home-cooked-app/ When you liberate programming from the requirement to be general and professional and scalable, it becomes a different activity altogether, just as cooking at home is really nothing like cooking in a commercial kitchen. Same goes for websites and self-hosting.","contentType":"reshare","url":"https://www.lqdev.me/responses/app-home-cooked","date":"2022-09-21 22:28 -05:00","tags":[],"summary":"An app can be a home-cooked meal \u2192 https://www.robinsloan.com/notes/home-cooked-app/ When you liberate programming from the requirement to be general and professional and scalable, it becomes a...","keywords":["home","cooked","cooking","websites","goes","same","kitchen","commercial","nothing","really"]},{"id":"reshare-welcome-to-la-terminal","title":"Welcome to La Terminal","content":"Welcome to La Terminal \u2192 https://xibbon.com/terminal/2022/09/21/welcome-to-la-terminal.html I\u0027m not an Apple user, but this is cool.","contentType":"reshare","url":"https://www.lqdev.me/responses/welcome-to-la-terminal","date":"2022-09-21 19:48 -05:00","tags":[],"summary":"Welcome to La Terminal \u2192 https://xibbon.com/terminal/2022/09/21/welcome-to-la-terminal.html I\u0027m not an Apple user, but this is cool.","keywords":["terminal","welcome","https://xibbon","com/terminal/2022/09/21/welcome","html","apple","user","cool"]},{"id":"bookmarks-resources-navigating-complex-leadership-work","title":"Resources for navigating complex leadership work","content":"Resources for navigating complex leadership work \u2192 https://larahogan.me/blog/management-resource-library/ Resource on: Influence \u0026amp; managing up: Enact positive change for yourself, your team, or the whole organization. Leading through crises: Strengthen your support network, meet your team where they\u2019re at, and weather the tough times. Cross-functional relationships: Strengthen relationships by creating role clarity and creatively supporting one another. One-on-ones: Set your teammates up for success during your one-on-one meetings! Hiring: Build consistent, repeatable, and equitable interviews and onboarding plans. Meetings: Support participants, hone the content, and nail the meeting goal. Feedback \u0026amp; performance reviews: Everyone deserves clear, actionable feedback! Communication \u0026amp; team dynamics: Plan ahead, facilitate well, and create clarity. Adapting your approach: As your work context and team evolves, your leadership approach will need to evolve, too.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/resources-navigating-complex-leadership-work","date":"2022-09-21 19:19 -05:00","tags":[],"summary":"Resources for navigating complex leadership work \u2192 https://larahogan.me/blog/management-resource-library/ Resource on: Influence \u0026amp; managing up: Enact positive change for yourself, your team, or...","keywords":["team","\u0026amp","strengthen","feedback","leadership","resource","clarity","support","goal","meeting"]},{"id":"bookmarks-distance-metrics-vector-search","title":"What are Distance Metrics in Vector Search?","content":"What are Distance Metrics in Vector Search? \u2192 https://weaviate.io/blog/2022/09/Distance-Metrics-in-Vector-Search.html Metrics covered: Cosine Dot Product L2-Squared Manhattan Hamming","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/distance-metrics-vector-search","date":"2022-09-21 12:44 -05:00","tags":[],"summary":"What are Distance Metrics in Vector Search? \u2192 https://weaviate.io/blog/2022/09/Distance-Metrics-in-Vector-Search.html Metrics covered: Cosine Dot Product L2-Squared Manhattan Hamming","keywords":["metrics","vector","search","distance","https://weaviate","io/blog/2022/09/distance","html","covered:","cosine","product"]},{"id":"bookmarks-1-percent-rule","title":"1% Rule","content":"1% Rule \u2192 https://en.wikipedia.org/wiki/1%25_rule ...about 1% of Internet users create content, while 99% are just consumers of that content","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/1-percent-rule","date":"2022-09-21 12:39 -05:00","tags":[],"summary":"1% Rule \u2192 https://en.wikipedia.org/wiki/1%25_rule ...about 1% of Internet users create content, while 99% are just consumers of that content","keywords":["rule","content","https://en","wikipedia","org/wiki/1%25","internet","users","create","while","consumers"]},{"id":"bookmarks-what-ive-learned-from-users","title":"What I\u0027ve Learned From Users","content":"What I\u0026#39;ve Learned From Users \u2192 http://paulgraham.com/users.html What have I learned from YC\u0027s users, the startups we\u0027ve funded? ...most startups have the same problems. ...the batch that broke YC was a powerful demonstration of how individualized the process of advising startups has to be. ...founders can be [bad] at realizing what their problems are. Founders will sometimes come in to talk about some problem, and we\u0027ll discover another much bigger one in the course of the conversation. Often founders know what their problems are, but not their relative importance. Focus is doubly important for early stage startups, because not only do they have a hundred different problems, they don\u0027t have anyone to work on them except the founders. If the founders focus on things that don\u0027t matter, there\u0027s no one focusing on the things that do. Speed defines startups. Focus enables speed. YC improves focus. Why are founders uncertain about what to do? Partly because startups almost by definition are doing something new, which means no one knows how to do it yet, or in most cases even what \u0026quot;it\u0026quot; is.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/what-ive-learned-from-users","date":"2022-09-20 21:21 -05:00","tags":[],"summary":"What I\u0026#39;ve Learned From Users \u2192 http://paulgraham.com/users.html What have I learned from YC\u0027s users, the startups we\u0027ve funded? ...most startups have the same problems. ...the batch that broke YC...","keywords":["founders","startups","problems","focus","from","they","speed","learned","things","users"]},{"id":"bookmarks-ditch-facebook-without-losing-friends","title":"How to Ditch Facebook Without Losing Your Friends","content":"How to Ditch Facebook Without Losing Your Friends \u2192 https://www.eff.org/deeplinks/2022/09/how-ditch-facebook-without-losing-your-friends-or-family-customers-or-communities disgruntled Facebook users keep using the service because they don\u2019t want to leave behind their friends, family, communities and customers. \u201CHow to Ditch Facebook Without Losing Your Friends\u201D explains the rationale behind these proposals - and offers a tour of what it would be like to use a federated, interoperable Facebook, from setting up your account to protecting your privacy and taking control of your own community\u2019s moderation policies, overriding the limits and permissions that Facebook has unilaterally imposed on its users. Video Highlight reel PDF","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/ditch-facebook-without-losing-friends","date":"2022-09-20 20:30 -05:00","tags":[],"summary":"How to Ditch Facebook Without Losing Your Friends \u2192 https://www.eff.org/deeplinks/2022/09/how-ditch-facebook-without-losing-your-friends-or-family-customers-or-communities disgruntled Facebook users...","keywords":["facebook","ditch","losing","friends","family","customers","communities","users","behind","setting"]},{"id":"bookmarks-firefox-profile-maker","title":"Firefox Profile Maker","content":"Firefox Profile Maker \u2192 https://ffprofile.com/ This tool will help you to create a Firefox profile with the defaults you like. You select which features you want to enable and disable and in the end you get a download link for a zip-file with your profile template.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/firefox-profile-maker","date":"2022-09-20 20:27 -05:00","tags":[],"summary":"Firefox Profile Maker \u2192 https://ffprofile.com/ This tool will help you to create a Firefox profile with the defaults you like. You select which features you want to enable and disable and in the end...","keywords":["profile","firefox","link","download","disable","enable","want","features","which","select"]},{"id":"reply-rss-20-at-20","title":"RSS at 20 still going string","content":"RSS at 20 still going string \u2192 https://www.zylstra.org/blog/2022/09/rss2-0-at-20/ 20 years and still going strong.","contentType":"reply","url":"https://www.lqdev.me/responses/rss-20-at-20","date":"2022-09-18 10:20 -05:00","tags":[],"summary":"RSS at 20 still going string \u2192 https://www.zylstra.org/blog/2022/09/rss2-0-at-20/ 20 years and still going strong.","keywords":["going","string","https://www","zylstra","org/blog/2022/09/rss2","years","strong"]},{"id":"reply-webmention-test-1","title":"Test Response","content":"Test Response \u2192 https://webmention.rocks/test/1 This is my comment 2","contentType":"reply","url":"https://www.lqdev.me/responses/webmention-test-1","date":"2022-09-16 14:37:00 -05:00","tags":[],"summary":"Test Response \u2192 https://webmention.rocks/test/1 This is my comment 2","keywords":["test","response","https://webmention","rocks/test/1","comment"]},{"id":"bookmarks-wordpress-indieweb-os-open-social-web","title":"WordPress\u002BIndieWeb as the OS of the Open Social Web","content":"WordPress\u002BIndieWeb as the OS of the Open Social Web \u2192 https://www.zylstra.org/blog/2022/09/wordpressindieweb-as-the-os-of-the-open-social-web/ ...2022 Netherlands WordCamp edition in Arnhem [presentation] on turning all WordPress sites into fully IndieWeb enabled sites. Meaning turning well over a third of the web into the open social web. Outside all the silos.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/wordpress-indieweb-os-open-social-web","date":"2022-09-16 11:44 -05:00","tags":[],"summary":"WordPress\u002BIndieWeb as the OS of the Open Social Web \u2192 https://www.zylstra.org/blog/2022/09/wordpressindieweb-as-the-os-of-the-open-social-web/ ...2022 Netherlands WordCamp edition in Arnhem...","keywords":["open","social","sites","turning","wordpress\u002Bindieweb","third","meaning","enabled","indieweb","fully"]},{"id":"bookmarks-rh-notecard-system","title":"The Notecard System","content":"The Notecard System \u2192 https://ryanholiday.net/the-notecard-system-the-key-for-remembering-organizing-and-using-everything-you-read/","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/rh-notecard-system","date":"2022-09-14 09:29 -05:00","tags":[],"summary":"The Notecard System \u2192 https://ryanholiday.net/the-notecard-system-the-key-for-remembering-organizing-and-using-everything-you-read/","keywords":["notecard","system","https://ryanholiday","net/the","remembering","organizing","using","everything","read/"]},{"id":"bookmarks-lisp-is-a-chameleon","title":"Lisp is a chameleon","content":"Lisp is a chameleon \u2192 http://www.paulgraham.com/chameleon.html When Lisp adopts a new paradigm it not only replicates existing practice, but goes beyond it to become a testbed for advancing the state of the art. Why has Lisp been able to adapt so easily when other languages have not? One reason is that Lisp is a programmable programming language.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/lisp-is-a-chameleon","date":"2022-09-14 09:17 -05:00","tags":[],"summary":"Lisp is a chameleon \u2192 http://www.paulgraham.com/chameleon.html When Lisp adopts a new paradigm it not only replicates existing practice, but goes beyond it to become a testbed for advancing the state...","keywords":["lisp","programmable","reason","languages","easily","adapt","able","state","advancing","testbed"]},{"id":"bookmarks-850-free-mooc-courses","title":"MOOC - 850\u002B Free Courses","content":"MOOC - 850\u002B Free Courses \u2192 https://www.freecodecamp.org/news/new-online-courses/ Subjects: Science Programming Education \u0026amp; Teaching Health \u0026amp; Medicine Social Sciences Business Information Security (InfoSec) Mathematics Computer Science Data Science Personal Development Humanities Engineering Art \u0026amp; Design","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/850-free-mooc-courses","date":"2022-09-14 09:14 -05:00","tags":[],"summary":"MOOC - 850\u002B Free Courses \u2192 https://www.freecodecamp.org/news/new-online-courses/ Subjects: Science Programming Education \u0026amp; Teaching Health \u0026amp; Medicine Social Sciences Business Information...","keywords":["science","\u0026amp","mooc","humanities","development","personal","data","computer","mathematics","infosec"]},{"id":"bookmarks-open-building-institute","title":"Open Building Institute","content":"Open Building Institute \u2192 https://www.openbuildinginstitute.org/ The OBI system is open source, collaborative and distributed. Our focus is on low cost and rapidly-built structures that are modular, ecological, and energy efficient.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/open-building-institute","date":"2022-09-11 11:58 -05:00","tags":[],"summary":"Open Building Institute \u2192 https://www.openbuildinginstitute.org/ The OBI system is open source, collaborative and distributed. Our focus is on low cost and rapidly-built structures that are modular,...","keywords":["open","ecological","modular","structures","built","rapidly","cost","focus","energy","distributed"]},{"id":"bookmarks-metablogging-ideaspace","title":"Metablogging","content":"Metablogging \u2192 https://ideaspace.substack.com/p/metablogging ...internally public blogs written by members of the...squad detailing what they\u2019re working on and thinking about.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/metablogging-ideaspace","date":"2022-09-09 09:49 -05:00","tags":[],"summary":"Metablogging \u2192 https://ideaspace.substack.com/p/metablogging ...internally public blogs written by members of the...squad detailing what they\u2019re working on and thinking about.","keywords":["metablogging","https://ideaspace","substack","com/p/metablogging","internally","public","blogs","written","members","squad"]},{"id":"bookmarks-coding-for-economists","title":"Coding for Economists","content":"Coding for Economists \u2192 https://aeturrell.github.io/coding-for-economists/intro.html ...a guide for economists on what programming is, why it\u2019s useful, and how to do it.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/coding-for-economists","date":"2022-09-08 19:22 -05:00","tags":[],"summary":"Coding for Economists \u2192 https://aeturrell.github.io/coding-for-economists/intro.html ...a guide for economists on what programming is, why it\u2019s useful, and how to do it.","keywords":["economists","coding","https://aeturrell","github","io/coding","economists/intro","html","guide","programming","it\u2019s"]},{"id":"bookmarks-google-haskell-trainings","title":" google/haskell-trainings","content":"google/haskell-trainings \u2192 https://github.com/google/haskell-trainings This repository contains the source for the slides and the exercises used in the Haskell trainings at Google.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/google-haskell-trainings","date":"2022-09-06 20:16 -05:00","tags":[],"summary":"google/haskell-trainings \u2192 https://github.com/google/haskell-trainings This repository contains the source for the slides and the exercises used in the Haskell trainings at Google.","keywords":["trainings","google/haskell","https://github","com/google/haskell","repository","contains","source","slides","exercises","used"]},{"id":"bookmarks-open-source-alternativeto","title":"Open Source Alternative To","content":"Open Source Alternative To \u2192 https://www.opensourcealternative.to/ Discover 350\u002B popular open source alternatives to your proprietary SaaS.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/open-source-alternativeto","date":"2022-09-06 18:51 -05:00","tags":[],"summary":"Open Source Alternative To \u2192 https://www.opensourcealternative.to/ Discover 350\u002B popular open source alternatives to your proprietary SaaS.","keywords":["open","source","alternative","https://www","opensourcealternative","discover","350\u002B","popular","alternatives","proprietary"]},{"id":"bookmarks-open-library","title":"Open Library","content":"Open Library \u2192 https://openlibrary.org/ Open Library is an initiative of the Internet Archive, a 501(c)(3) non-profit, building a digital library of Internet sites and other cultural artifacts in digital form.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/open-library","date":"2022-09-05 18:29 -05:00","tags":[],"summary":"Open Library \u2192 https://openlibrary.org/ Open Library is an initiative of the Internet Archive, a 501(c)(3) non-profit, building a digital library of Internet sites and other cultural artifacts in...","keywords":["library","open","internet","digital","https://openlibrary","org/","initiative","archive","profit","building"]},{"id":"bookmarks-deep-dive-transformers-tf-keras-pt1","title":"A Deep Dive into Transformers with TensorFlow and Keras: Part 1","content":"A Deep Dive into Transformers with TensorFlow and Keras: Part 1 \u2192 https://pyimagesearch.com/2022/09/05/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-1/","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/deep-dive-transformers-tf-keras-pt1","date":"2022-09-05 12:05 -05:00","tags":[],"summary":"A Deep Dive into Transformers with TensorFlow and Keras: Part 1 \u2192 https://pyimagesearch.com/2022/09/05/a-deep-dive-into-transformers-with-tensorflow-and-keras-part-1/","keywords":["deep","dive","transformers","tensorflow","part","keras:","https://pyimagesearch","com/2022/09/05/a","keras"]},{"id":"bookmarks-pine-blog-feed-directory","title":"The Feed Directory - Pine.blog","content":"The Feed Directory - Pine.blog \u2192 https://pine.blog/search Find feeds for all of your favorite sites and keep up with everything they post!","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/pine-blog-feed-directory","date":"2022-09-04 22:27 -05:00","tags":[],"summary":"The Feed Directory - Pine.blog \u2192 https://pine.blog/search Find feeds for all of your favorite sites and keep up with everything they post!","keywords":["feed","directory","pine","blog","https://pine","blog/search","find","feeds","favorite","sites"]},{"id":"bookmarks-how-to-create-commonplace-book-evernote","title":"Taking Note: How to Create Commomplace with Evernote","content":"Taking Note: How to Create Commomplace with Evernote \u2192 https://evernote.com/blog/how-to-create-commonplace-with-evernote/ Why commonplace? Commonplace is an ideal medium for the curation and cultivation of intellectual ideas, thoughts, and knowledge. It\u2019s also a proven and timeless system, tested by folks from a spectrum of backgrounds including authors, professors, and scientists. Remember things Write to recall Understand reading Personal reference system Filter ideas Unleash creativity Reflecting on your commonplace The most profound power of your commonplace is being able to thumb through reading and review material. Your commonplace book isn\u2019t just a filing cabinet\u2014it\u2019s an evolving record of your life and observations.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/how-to-create-commonplace-book-evernote","date":"2022-09-04 20:18 -05:00","tags":[],"summary":"Taking Note: How to Create Commomplace with Evernote \u2192 https://evernote.com/blog/how-to-create-commonplace-with-evernote/ Why commonplace? Commonplace is an ideal medium for the curation and...","keywords":["commonplace","ideas","create","reading","system","things","write","recall","understand","personal"]},{"id":"reshare-every-user-neo","title":"Every User a Neo","content":"Every User a Neo \u2192 https://www.linuxjournal.com/content/every-user-neo","contentType":"reshare","url":"https://www.lqdev.me/responses/every-user-neo","date":"2022-09-04 18:12 -05:00","tags":[],"summary":"Every User a Neo \u2192 https://www.linuxjournal.com/content/every-user-neo","keywords":["user","https://www","linuxjournal","com/content/every"]},{"id":"bookmarks-buck-rogers-25-century-ad","title":"Buck Rogers in the 25th Century A.D.","content":"Buck Rogers in the 25th Century A.D. \u2192 https://buckrogers.neocities.org/","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/buck-rogers-25-century-ad","date":"2022-09-04 18:01 -05:00","tags":[],"summary":"Buck Rogers in the 25th Century A.D. \u2192 https://buckrogers.neocities.org/","keywords":["buck","rogers","25th","century","https://buckrogers","neocities","org/"]},{"id":"bookmarks-small-web-jayless-wiki","title":"Small Web","content":"Small Web \u2192 https://www.jayeless.net/wiki/small-web.html ...the kind of web they define themselves against; that kind of bloated, corporate, algorithm-ruled and ad-ridden mess that constitutes the majority of highly-trafficked websites these day. ...for me, the term \u201CSmall Web\u201D refers to a couple of main things: independence from tech giants, and websites that are lightweight and high-performance. Related concepts: A late 90s-style, hand-crafted web Alternative protocols, like Gemini An independent web","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/small-web-jayless-wiki","date":"2022-09-02 00:03 -05:00","tags":[],"summary":"Small Web \u2192 https://www.jayeless.net/wiki/small-web.html ...the kind of web they define themselves against; that kind of bloated, corporate, algorithm-ruled and ad-ridden mess that constitutes the...","keywords":["websites","kind","small","couple","main","things:","independence","from","tech","giants"]},{"id":"bookmarks-blot-blogging-platform","title":"Blot - Blogging Platform","content":"Blot - Blogging Platform \u2192 https://blot.im/ Blot is a blogging platform with no interface. It turns a folder into a website.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/blot-blogging-platform","date":"2022-08-30 22:20 -05:00","tags":[],"summary":"Blot - Blogging Platform \u2192 https://blot.im/ Blot is a blogging platform with no interface. It turns a folder into a website.","keywords":["blot","blogging","platform","https://blot","interface","turns","folder","website"]},{"id":"bookmarks-osmlab-awesome-osm","title":"osmlab/awesome-openstreetmap","content":"osmlab/awesome-openstreetmap \u2192 https://github.com/osmlab/awesome-openstreetmap Curated list of awesome OpenSteetMap-projects","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/osmlab-awesome-osm","date":"2022-08-12 08:12 -05:00","tags":[],"summary":"osmlab/awesome-openstreetmap \u2192 https://github.com/osmlab/awesome-openstreetmap Curated list of awesome OpenSteetMap-projects","keywords":["openstreetmap","osmlab/awesome","https://github","com/osmlab/awesome","curated","list","awesome","opensteetmap","projects"]},{"id":"bookmarks-yesterweb","title":"The Yesterweb","content":"The Yesterweb \u2192 https://yesterweb.org/ This site is dedicated to a community (and a larger movement) about the internet how it\u0027s changed. We are creating, discovering and enjoying websites and digital spaces.","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/yesterweb","date":"2022-08-03 22:24 -05:00","tags":[],"summary":"The Yesterweb \u2192 https://yesterweb.org/ This site is dedicated to a community (and a larger movement) about the internet how it\u0027s changed. We are creating, discovering and enjoying websites and...","keywords":["yesterweb","https://yesterweb","org/","site","dedicated","community","larger","movement","internet","changed"]},{"id":"bookmarks-columbia-reimagine-internet","title":"Reimagine the internet","content":"Reimagine the internet \u2192 https://knightcolumbia.org/events/reimagine-the-internet A virtual conference exploring what the internet could become over the next decade Sessions: Pioneering Alternative Models for Community on the Internet Misinformation, Disinformation, and Media Literacy in a Less-Centralized Social Media Universe Interoperability and Alternative Social Media Lessons from Experiments in Local Community-Building Deplatforming and Innovation New Directions in Social Media Research","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/columbia-reimagine-internet","date":"2022-08-03 21:21 -05:00","tags":[],"summary":"Reimagine the internet \u2192 https://knightcolumbia.org/events/reimagine-the-internet A virtual conference exploring what the internet could become over the next decade Sessions: Pioneering Alternative...","keywords":["media","internet","social","community","alternative","less","centralized","universe","interoperability","lessons"]},{"id":"bookmarks-masterwiki","title":"MasterWiki","content":"MasterWiki \u2192 https://masterwiki.how/ ...masterWiki is the direct adaptation of MasterClass\u0027 video courses translated into wikiHow-style how-to guides...","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/masterwiki","date":"2022-08-03 18:48 -05:00","tags":[],"summary":"MasterWiki \u2192 https://masterwiki.how/ ...masterWiki is the direct adaptation of MasterClass\u0027 video courses translated into wikiHow-style how-to guides...","keywords":["masterwiki","https://masterwiki","how/","direct","adaptation","masterclass","video","courses","translated","wikihow"]},{"id":"bookmarks-iphone-settings-focus-productivity-health","title":"The Optimal iPhone Settings and Best Apps for Productivity, Focus, and Health","content":"The Optimal iPhone Settings and Best Apps for Productivity, Focus, and Health \u2192 https://betterhumans.pub/how-to-set-up-your-iphone-for-productivity-focus-and-your-own-longevity-bb27a68cc3d8 Here\u2019s how to read the post: Level 1 \u2014 Casual. Read the headlines \u2014 figure out the details yourself. Most of this isn\u2019t rocket science. Level 2 \u2014 Tutorial. Read the steps underneath the headline. I\u2019ve spelled out every step so that you can save your brain power for something else. Level 3 \u2014 Productivity Nerd. Below the tutorial steps, I\u2019ve included discussion of the behavior design implications. This is for true productivity nerds, i.e. the readers of Better Humans. Optimize First for Single Tasking #1. Turn OFF (almost) all notifications #2. Hide social media slot machines #3. Hide messaging slot machines #4. Disable app review requests #5. Turn on Do Not Disturb #6. Be strategic about your wallpaper #7. Turn off Raise to Wake #8. Add the Screen Time widget #9. Add Content Restrictions #10. (Optional) Use Restrictions to turn off Safari #11. Organize your Apps and Folders alphabetically Switch to Google Cloud to Work Faster #12. Choose GMail #13. Choose Google Calendar #14. Replace Apple Maps with Google Maps #15. Install the GBoard keyboard for faster typing #16. Switch to Google Photos Install These Apps for Productivity #17. Use Evernote for all note taking, to-do lists, everything #18. The Case for Calm as your go-to meditation app #19. Install the right goal tracker for you #20. Store all your passwords in a password manager, probably LastPass #21. Use Numerical as your default calculator #22. Put the Camera app in your toolbar #23. Use this Doppler Radar app #24. Use this Pomodoro app #25. Use Brain.fm for background noise Use These Apps and Configurations for Deep Learning #26. Subscribe to these podcasts #27. Install the Kindle app but never read it in bed #28. Use Safari this way #29. Organize your home screen for deep learning over shallow learning Use These Apps and Configurations for Longevity #30. Track steps this way #31. Prefer Time Restricted Eating Over Calorie Counting #32. Schedule Night Shift #33. Set up Medical ID Make The Finishing Touches with These Configurations #34. Change Siri to a man #35. Change your phone\u2019s name #36. Turn off advertising tracking #37. Set auto-lock to the maximum time #38. Set your personal hotspot password to a random three word phrase #39. Turn on control center everywhere #40. Turn on Background App Refresh #41. Delete Garage Band #42. Develop verbal memory for talking to Siri #43. Set up these text replacement shortcuts #44. Set your address #45. Backup this way","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/iphone-settings-focus-productivity-health","date":"2022-08-01 17:50 -05:00","tags":[],"summary":"The Optimal iPhone Settings and Best Apps for Productivity, Focus, and Health \u2192 https://betterhumans.pub/how-to-set-up-your-iphone-for-productivity-focus-and-your-own-longevity-bb27a68cc3d8 Here\u2019s...","keywords":["turn","apps","productivity","install","read","google","learning","steps","level","configurations"]},{"id":"bookmarks-open-source-organization-workshop","title":"Open Source and Organization Workshop","content":"Open Source and Organization Workshop \u2192 https://klemet.github.io/Workshop-Organization-EN/index.html Workshop objectives Discover and learn to use the basic functions of the following software: Joplin Zotero Nextcloud Learn how to use these different software programs together to manage information using the following methods: The Zettelkasten method The P.A.R.A method The inbox method","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/open-source-organization-workshop","date":"2022-07-27 22:01 -05:00","tags":[],"summary":"Open Source and Organization Workshop \u2192 https://klemet.github.io/Workshop-Organization-EN/index.html Workshop objectives Discover and learn to use the basic functions of the following software:...","keywords":["method","following","organization","workshop","learn","zettelkasten","methods:","using","information","manage"]},{"id":"bookmarks-practical-deep-learning-coders-2022","title":"Practical Deep Learning for Coders 2022","content":"Practical Deep Learning for Coders 2022 \u2192 https://www.fast.ai/2022/07/21/dl-coders-22/ A free course designed for people with some coding experience, who want to learn how to apply deep learning and machine learning to practical problems. Lessons Getting started Deployment Neural net foundations Natural Language (NLP) From-scratch Model Random forests Collaborative filtering and embeddings Convolutions (CNNs)","contentType":"bookmarks","url":"https://www.lqdev.me/bookmarks/practical-deep-learning-coders-2022","date":"2022-07-26 21:11 -05:00","tags":[],"summary":"Practical Deep Learning for Coders 2022 \u2192 https://www.fast.ai/2022/07/21/dl-coders-22/ A free course designed for people with some coding experience, who want to learn how to apply deep learning and...","keywords":["learning","practical","deep","coders","deployment","neural","foundations","natural","language","from"]},{"id":"notes-azure-costs-website-hosting-2021","title":"A year\u0027s worth of Azure hosting costs","content":"It\u0027s been a little over a year since I started hosting my website on Azure. The resources I use include: Azure Storage: Where all my site assets are stored. Azure CDN: Content delivery Azure Logic Apps: Event-based jobs to syndicate new posts on Twitter and Mastodon. I haven\u0027t tracked traffic for the entire year but since October of last year the site has gotten roughly 5,000 visitors. Taking all that into account, I\u0027ve only had to pay $2.06 most of it being the storage I use for the site. In comparison, I was previously hosting my website through my domain provider in a shared server where it cost about $60! If you\u0027re interested in how this site is built, check out the colophon .","contentType":"notes","url":"https://www.lqdev.me/notes/azure-costs-website-hosting-2021","date":"2022-05-04 20:36 -05:00","tags":["azure","website","cost","hosting","selfhosting","cloud","indieweb"],"summary":"It\u0027s been a little over a year since I started hosting my website on Azure. The resources I use include: Azure Storage: Where all my site assets are stored. Azure CDN: Content delivery Azure Logic...","keywords":["azure","site","year","since","hosting","website","little","october","last","gotten"]},{"id":"posts-proofread-read-aloud","title":"Proofread articles using Microsoft Edge\u0027s Immersive Reader and read aloud","content":"This will be a relatively short post but it\u0027s something I just discovered and thought I\u0027d share. Before publishing a post, I try to proofread and review the article to make sure spelling, grammar, and flow all make sense. Part of that process involves reading the article aloud. I\u0027ll miss things from time to time but generally it works well. I\u0027ve known about read aloud in general, but have often found voices to be robotic and sound unnatural when they encounter words outside their vocabulary. However, I recently tried out the read aloud feature in Microsoft Edge and was blown away at how natural the voice sounds and even how well it enunciates technical terms like product and programming API names. Here\u0027s what it looks like when reading one of the posts on my site. ![Microsoft Edge Immersive Reader read aloud reading blog post](https://user-images.githubusercontent.com/11130940/134442715-e0af0c4a-a522-49a9-a0b1-ac658b284c68.png) If you do any form of writing that can be displayed in Microsoft Edge, I highly encourage you to give read aloud a try. For more information, see [Use Immersive Reader in Microsoft Edge](https://support.microsoft.com/topic/use-immersive-reader-in-microsoft-edge-78a7a17d-52e1-47ee-b0ac-eff8539015e1). Happy writing!","contentType":"posts","url":"https://www.lqdev.me/posts/proofread-read-aloud","date":"2021-09-22 18:00:00 -05:00","tags":["writing","tools"],"summary":"This will be a relatively short post but it\u0027s something I just discovered and thought I\u0027d share. Before publishing a post, I try to proofread and review the article to make sure spelling, grammar,...","keywords":["microsoft","aloud","edge","read","reader","reading","immersive","post","article","writing"]},{"id":"posts-mlnet-predictions-no-schema","title":"Make predictions with ML.NET models without defining schema classes","content":"## Introduction To make predictions with ML.NET models you often have to define schema classes for your model inputs and outputs. In previous posts I wrote how you can [use Netron to inspect an ML.NET model](/posts/inspect-mlnet-models-netron/) to determine the name and types of your model inputs and outputs. If you have data samples of what your data input and output look like in JSON format, you can [automate the generation of model input and output classes by using Visual Studio\u0027s \u0022Paste JSON as Classes\u0022 feature](/posts/vs-automate-mlnet-schema-generation/). However, what if you want to make predictions without defining these classes? In this post I\u0027ll show how you can use the .NET DataFrame API to make predictions with ML.NET models without having to create model input and output classes. Code snippets are in F# but notebooks with complete [C#](https://github.com/lqdev/mlnet-noschema-predictions/blob/main/CSharp-NB.ipynb) \u0026 [F#](https://github.com/lqdev/mlnet-noschema-predictions/blob/main/FSharp-NB.ipynb) code can be found in the [mlnet-noschema-predictions repo](https://github.com/lqdev/mlnet-noschema-predictions) on GitHub. ## Install and reference packages In addition to the [Microsoft.ML](https://www.nuget.org/packages/Microsoft.ML/) ML.NET NuGet package, you\u0027ll also need the [Microsoft.Data.Analysis](https://www.nuget.org/packages/Microsoft.Data.Analysis/) NuGet package to use the .NET DataFrame API. For more information on the .NET DataFrame API, see [an introduction to DataFrame](https://devblogs.microsoft.com/dotnet/an-introduction-to-dataframe/). Once your packages are installed, reference them in your application. \u0060\u0060\u0060fsharp open Microsoft.ML open Microsoft.Data.Analysis \u0060\u0060\u0060 ## Initialize MLContext and load the model The \u0060MLContext\u0060 is the entrypoint of ML.NET applications. Use it to load your model. The model used in this case categorizes sentiment as positive or negative. See the [use Netron to inspect an ML.NET model](/posts/inspect-mlnet-models-netron/) blog post to learn more about the model. \u0060\u0060\u0060fsharp let ctx = MLContext() let model,schema = ctx.Model.Load(\u0022sentiment_model.zip\u0022) \u0060\u0060\u0060 Both the model and the input schema are returned when you load the model. The input schema is a [DataViewSchema](https://docs.microsoft.com/dotnet/api/microsoft.ml.dataviewschema?view=ml-dotnet) object containing a collection of [Columns](https://docs.microsoft.com/dotnet/api/microsoft.ml.dataviewschema.column?view=ml-dotnet). ## Define input and output column names The input and output column names are for the DataFrames containing your input data and predictions. They help the model map the input and output values. Use the \u0060schema\u0060 which was loaded with the model to get the name of your input columns. \u0060\u0060\u0060fsharp let inputColumnNames = schema |\u003E Seq.map(fun column -\u003E column.Name) |\u003E Array.ofSeq \u0060\u0060\u0060 Since this is a binary classification model by default only two columns are returned as part of the prediction: - Score - PredictedLabel You can create an array containing the names of these columns. For more information on default output columns, see the [ML.NET Tasks documentation](https://docs.microsoft.com/dotnet/machine-learning/resources/tasks#binary-classification-inputs-and-outputs). \u0060\u0060\u0060fsharp let outputColumnNames = [| \u0022PredictedLabel\u0022 ; \u0022Score\u0022 |] \u0060\u0060\u0060 ## Create input data for predictions Use the [\u0060LoadCsvFromString\u0060](https://docs.microsoft.com/dotnet/api/microsoft.data.analysis.dataframe.loadcsvfromstring?view=ml-dotnet-preview#Microsoft_Data_Analysis_DataFrame_LoadCsvFromString_System_String_System_Char_System_Boolean_System_String___System_Type___System_Int64_System_Int32_System_Boolean_) method to load your input data into a DataFrame. In this case, there\u0027s only one column and data instance so I represent it as a string literal. Additionally, I provide the name of the input columns. \u0060\u0060\u0060fsharp let sampleInput = \u0022This was a very bad steak\u0022 let inputDataFrame = DataFrame.LoadCsvFromString( sampleInput, header=false, columnNames=inputColumnNames) \u0060\u0060\u0060 ## Make predictions Now that you\u0027ve loaded your input data, it\u0027s time to use the model to make predictions. \u0060\u0060\u0060fsharp let predictionDV = inputDataFrame |\u003E model.Transform \u0060\u0060\u0060 Calling the [\u0060Transform\u0060](https://docs.microsoft.com/dotnet/api/microsoft.ml.itransformer.transform?view=ml-dotnet#Microsoft_ML_ITransformer_Transform_Microsoft_ML_IDataView_) method returns an [\u0060IDataView\u0060](https://docs.microsoft.com/dotnet/api/microsoft.ml.idataview?view=ml-dotnet) with your predictions. You can then convert the \u0060IDataView\u0060 into a DataFrame for further processing with the [\u0060ToDataFrame\u0060](https://docs.microsoft.com/dotnet/api/microsoft.ml.idataviewextensions.todataframe?view=ml-dotnet-preview) method. \u0060\u0060\u0060fsharp let prediction = predictionDV.ToDataFrame(1L, outputColumnNames) \u0060\u0060\u0060 The resulting DataFrame should look something like the following: | index | PredictedLabel | Score | |---|---|---| |0 |False | -2.1337974 | ## Conclusion If you want to load a model and make predictions without defining classes for your input and output schema\u0027s you can load your data into a DataFrame using the .NET DataFrame API. While this solution works, because DataFrames and IDataViews process data differently, I haven\u0027t tested whether this solution would scale for larger data sets.","contentType":"posts","url":"https://www.lqdev.me/posts/mlnet-predictions-no-schema","date":"2021-09-16 18:00:00 -05:00","tags":["dotnet","machine learning","mlnet","artificial intelligence"],"summary":"## Introduction To make predictions with ML.NET models you often have to define schema classes for your model inputs and outputs. In previous posts I wrote how you can [use Netron to inspect an...","keywords":["model","input","data","microsoft","dataframe","predictions","output","system","schema","\u0060\u0060\u0060fsharp"]},{"id":"posts-vs-automate-mlnet-schema-generation","title":"Automate ML.NET model schema generation from sample JSON data with Visual Studio","content":"## Introduction When using ML.NET models to make predictions, you often have to define classes for the model\u0027s input and output schema. In a previous post, I showed how you can [use Netron to inspect ML.NET models](/posts/inspect-mlnet-models-netron/) and manually define classes to represent the input and output schema for your model. That works for models that don\u0027t have a lot of features / columns. However, as the number of columns grows, it can become cumbersome to define these classes. Visual Studio has a feature that can help automate that process. Assuming you have a sample of your input and output data in JSON format, you can leverage a built-in feature \u0022Paste JSON As Classes\u0022 to take the sample and convert it to a class. In this post, I\u0027ll show how to do that. ## Prerequisites - Visual Studio 2019. Though I haven\u0027t tested with VS2022, I assume \u0022Paste JSON As Classes\u0022 is also available there. ## Convert sample JSON data to C# classes In this post, I\u0027ll work with the *sentiment_model.zip* model to classify sentiment, which you can find in the [dotnet/samples](https://github.com/dotnet/samples/blob/main/machine-learning/models/sentimentanalysis/sentiment_model.zip) repo. The model input has 3 columns: - SentimentText (string) - Label (boolean) - SamplingKeyColumn (float32) The model output has at least 2 columns since it uses a binary classification algorithm. For information on expected output columns based on the machine learning algorithm/task, see the [ML.NET tasks article](https://docs.microsoft.com/dotnet/machine-learning/resources/tasks#binary-classification-inputs-and-outputs). - Score (Single). Single is a single-precision floating-point number. - PredictedLabel (boolean) With that in mind, let\u0027s then assume that we have sample input data in JSON format that looks as follows. \u0060\u0060\u0060json { \u0022SentimentText\u0022: \u0022This was a very bad steak\u0022, \u0022Label\u0022: false, \u0022SamplingKeyColumn\u0022: 1.0 } \u0060\u0060\u0060 Inside a C# project in Visual Studio: 1. Create a C# class 1. Right-click your project. 2. Select **Add \u003E Class**. 3. Provide a name for your class in the New Item dialog. 2. Copy the input JSON data sample to your clipboard. 3. Place your cursor inside the namespace block of your newly created class. 4. In the Visual Studio toolbar, select **Edit \u003E Paste Special \u003E Paste JSON as Classes**. 5. The result should look similar to the following: \u0060\u0060\u0060csharp public class Rootobject { public string SentimentText { get; set; } public bool Label { get; set; } public float SamplingKeyColumn { get; set; } } \u0060\u0060\u0060 6. Rename the class to something more descriptive like \u0060ModelInput\u0060. The class should look similar to the following: \u0060\u0060\u0060csharp public class ModelInput { public string SentimentText { get; set; } public bool Label { get; set; } public float SamplingKeyColumn { get; set; } } \u0060\u0060\u0060 7. Create a JSON data sample for your output. In this case it\u0027d look something like: \u0060\u0060\u0060json { \u0022Score\u0022: 1.0, \u0022PredictedLabel\u0022: false } \u0060\u0060\u0060 8. Repeat steps 1-6 for your output JSON data sample. The resulting class should look similar to the following: \u0060\u0060\u0060csharp public class ModelOutput { public float Score { get; set; } public bool PredictedLabel { get; set; } } \u0060\u0060\u0060 It\u0027s important to note that the name of the class does not matter as long as the column names and types are the same the ones the model expects. At this point, you can go through the process of [using the model to make predictions on new data](https://docs.microsoft.com/dotnet/machine-learning/how-to-guides/machine-learning-model-predictions-ml-net). ## Conclusion Although the model used in this post does not have many columns, when you have many columns \u0022Paste JSON As Classes\u0022 can significantly simplify the process of creating your input and output schema classes for ML.NET models. Happy coding!","contentType":"posts","url":"https://www.lqdev.me/posts/vs-automate-mlnet-schema-generation","date":"2021-08-26 18:00:00 -05:00","tags":["dotnet","machine learning","mlnet","artificial intelligence","tooling","visual studio"],"summary":"## Introduction When using ML.NET models to make predictions, you often have to define classes for the model\u0027s input and output schema. In a previous post, I showed how you can [use Netron to inspect...","keywords":["model","class","public","json","classes","output","sample","data","input","columns"]},{"id":"posts-inspect-mlnet-models-netron","title":"Inspect ML.NET models with Netron","content":"## Introduction Once you\u0027ve trained a machine learning model, you typically serialize it and save it to a file. This serialized file contains information such as the model inputs and output schema (names, data types), the transformations and algorithms used by the model, weights / coefficients, hyperparameters, and all other sorts of information about the model. The model file is then embedded in an end-user application such as a web API which deserializes the contents of the file and uses the model to make predictions on new data. How the model is serialized depends on the framework you use to train your model. To standardize model serialization and interoperability, you can use something like Open Neural Network Exchange (ONNX) to represent your models if supported by the framework you\u0027re using. That\u0027s beyond the scope of this post though. Regardless of which framework or serialization format is used, if you were to open up the model file, the contents of the model file are often not human readable or difficult to interpret. When the person who trains the model is also putting it into production, they know the format input data needs to be in to make predictions. That\u0027s not often the case though. As a result, the people putting models into production need the ability to inspect a model to get a sense of how they need to collect and preprocess input data before making predictions. That\u0027s where Netron comes in. [Netron](https://github.com/lutzroeder/Netron) is a \u0022visualizer for neural network, deep learning, and machine learning models\u0022. In this post, I\u0027ll show how you can use Netron to inspect ML.NET models and use that information to define the model input and output schemas. ## Inspecting an ML.NET model ML.NET models are typically serialized and saved to files with the *.zip* file extension. Using the *.zip* file extension is standard convention. However, the extension can be whatever makes the most sense to you. A common question is, what\u0027s in the *zip* file? The easy but vague answer to that question is, a serialized version of the model. Using Netron, you can go deeper and see exactly what is inside the *zip* file. In this post, I\u0027m using a pretrained ML.NET model that classifies sentiment. The model can be found in the [dotnet/samples](https://github.com/dotnet/samples/blob/main/machine-learning/models/sentimentanalysis/sentiment_model.zip) repo. The same concept applies to any other ML.NET model. To inspect the ML.NET model using Netron: 1. [Download the model](https://github.com/dotnet/samples/raw/main/machine-learning/models/sentimentanalysis/sentiment_model.zip) 2. Navigate to [https://netron.app](https://netron.app/). Alternatively, if you\u0027d prefer to use Netron offline, you can also [download the latest version of Netron](https://github.com/lutzroeder/netron/releases) for your operating system (Windows, Mac, or Linux). In this post, I use the web app. 3. Select **Open Model...** and use the file browser to select your ML.NET model. In this case, our model is *sentiment_model.zip*. 4. After a few seconds, a graph describing you model appears. How long it takes for your model depends on its size. The larger your model, the longer it takes to load. The nodes in the graph represent the model inputs, transformations, algorithm, and outputs. 5. Usually the top nodes represent the model inputs and the last node represents the algorithm or trainer. Click on any of the top nodes to display more information about the inputs. ![Sentiment Classification ML.NET model in netron](https://user-images.githubusercontent.com/11130940/130704589-61ebb612-d65f-4364-b275-bd0d4991d3cf.png) For this model, we see that there are 3 input properties or columns: - SentimentText (string) - Label (boolean) - SamplingKeyColumn (float32) Using this information, we can represent the model inputs as a Plain-Old-CLR-Object (POCO) in our end-user application. \u0060\u0060\u0060csharp public class ModelInput { public string SentimentText {get;set;} public bool Label {get;set;} public float SamplingKeyColumn {get;set;} } \u0060\u0060\u0060 ![ML.NET Netron Binary Predictor](https://user-images.githubusercontent.com/11130940/130705880-0baea2f7-7b45-408a-b60c-16acceb54079.png) Looking at the last node \u0060BinaryPredXfer\u0060, we see that the algorithm used is for binary classification or predictions. Looking at the [ML.NET tasks documentation](https://docs.microsoft.com/dotnet/machine-learning/resources/tasks#binary-classification-inputs-and-outputs), we expect to get at least two columns in the prediction output: - Score (Single) - PredictedLabel (boolean) Like the input, we can also represent model outputs or predictions as follows: \u0060\u0060\u0060csharp public class ModelOutput { public float Score {get;set;} public bool PredictedLabel {get;set;} } \u0060\u0060\u0060 Keep in mind that the name of the class can be anything so long as the properties or column names and types match with those expected by the model. Once you have your model inputs and outputs defined in your end-user application, you can follow the standard process of [loading your model and using it to make predictions](https://docs.microsoft.com/dotnet/machine-learning/how-to-guides/machine-learning-model-predictions-ml-net). ## Conclusion Inspecting ML.NET models can be difficult since their serialized version is not human readable. When making predictions with ML.NET models but you\u0027re not familiar with what the input and output data should look like, use Netron to inspect the model. Then, use the information about the input data names and types, machine learning task, and algorithm to define model input and output schema classes in your end-user application. Once you\u0027ve defined your model input and output, you can use the model to make predictions on new data.","contentType":"posts","url":"https://www.lqdev.me/posts/inspect-mlnet-models-netron","date":"2021-08-25 18:00:00 -05:00","tags":["dotnet","machine learning","mlnet","netron","artificial intelligence","tooling"],"summary":"## Introduction Once you\u0027ve trained a machine learning model, you typically serialize it and save it to a file. This serialized file contains information such as the model inputs and output schema...","keywords":["model","file","predictions","input","netron","data","models","using","public","inputs"]},{"id":"posts-yunohost-self-hosting-simplified","title":"Self-hosting made easy with YunoHost","content":"## Introduction Over the past few months I\u0027ve started self-hosting a few of the services I use. Among those are a Matrix chat server and Mastodon instance. It may be of special interest to call out, many of these services are hosted on a Raspberry Pi 4, which not only speaks to the capabilities of the device but also shows opportunities for individuals to take control over the services they use and their data. Self-hosting however isn\u0027t the easiest thing to get started with. Mainly because each service has their own setup requirements. Technically you could use something like Docker to standardize the process, but that has its own complexities. In particular, there was a recent experience where I had trouble setting up a service which I got up and running in no time using YunoHost. YunoHost describes itself as \u0022...a libre operating system aiming to simplify server administration and democratize self-hosting.\u0022. In this post, I\u0027ll describe my experience with it and why you might consider it for your self-hosting project. ![Image of server racks by Taylor Vick (Unsplash @tvick)](https://user-images.githubusercontent.com/11130940/128647305-c30ef44e-901d-4bf1-8e9d-af2449d5fd1d.png) ## Why Self-Host? First off, why even go through the trouble of self-hosting? Service providers nowadays have made it so easy to use their services, self-hosting seems like a lot of work. However, there are several reasons why you might consider self-hosting the applications and services you use. A few that I find interesting include: 1. Explore \u0022new\u0022 technologies 2. Own your data 3. Learn something new ### Explore \u0022new\u0022 technologies The saying goes \u0022there\u0027s nothing new under the sun\u0022. However as with all things, people have found alternative uses and solutions to existing technologies. For example, in simple terms, the [Fediverse](https://en.wikipedia.org/wiki/Fediverse) can be summarized as E-mail \u002B RSS. It consists of a common decentralized protocol to handle communication between individuals who may not use the same server while providing \u0022follow\u0022 capabilities for services like micro-blogging, photos, and video sharing. Though the Fediverse has been around for some time, it\u0027s still relatively new. Whether it\u0027s a new technology or new to you, self-hosting can help you learn more about technologies like the Fediverse. ### Own your data Part of accepting terms of service agreements is sometimes giving up some ownership of your personal data. Although you hope the company and service provider is responsible with your data, you can\u0027t always be 100% sure. With self-hosting, you\u0027re fully in control of your data and in most cases don\u0027t rely on third parties. As a result, your data is as private and secure as you want it to. From a security standpoint though, I\u0027ll note that in many cases, service providers have dedicated security teams whose main job is to keep your data secure. Therefore chances are that your data is usually secure when hosted by service providers. In any case, if you want to have complete control and ownership of your data, self-hosting might be for you. ### Learn something new Over the past few months, I\u0027ve learned a few things that I don\u0027t usually deal with on my day-to-day like DNS records, Nginx, Systemd, certificates, and a few others. Self-hosting these services usually involves setting up several components some of which you may already be familiar with while others may be new to you. Self-hosting gives you the opportunity to learn new skills and as mentioned previously work with new technologies. ## Self-hosting with YunoHost Recently I was trying to set up an instance of [Pixelfed](https://pixelfed.org/), which describes itself as \u0022A free and ethical photo sharing platform\u0022. You can think of it as a self-hosted Instagram. The application is built with PHP and a few other components I\u0027m not very familiar with. I spent about a week trying to configure my Raspberry Pi to successfully run these services. Unfortunately, I had no luck. I had heard about YunoHost previously but had never tried it. Essentially they offer a way to simplify self-hosting. I noticed YunoHost provided a way to self-host Pixelfed. All I had to do was: 1. [Install YunoHost](https://yunohost.org/en/install). In my case, I installed it on a Linode VM (the smallest one) running Debian 10, but you have options including a Raspberry Pi. 2. [Install Pixelfed](https://github.com/YunoHost-Apps/pixelfed_ynh). YunoHost has a variety of [applications](https://yunohost.org/en/apps) with pre-configured scripts that make it easy to install applications like Pixelfed. The entire process took about one hour. Some parts I skipped mentioning were getting a domain name and configuring DNS records, but aside from that, getting started with YunoHost was incredibly simple. If you\u0027re looking to self-host but don\u0027t want to spend too much time configuring your server and each of the services you host on it, give YunoHost a try.","contentType":"posts","url":"https://www.lqdev.me/posts/yunohost-self-hosting-simplified","date":"2021-08-10 18:00:00 -05:00","tags":["fediverse","open-source","self-hosting","raspberry-pi","matrix","pixelfed","mastodon"],"summary":"## Introduction Over the past few months I\u0027ve started self-hosting a few of the services I use. Among those are a Matrix chat server and Mastodon instance. It may be of special interest to call out,...","keywords":["self","hosting","data","services","yunohost","service","technologies","server","learn","host"]},{"id":"posts-surface-duo-offline-navigation-hiking-setup","title":"Surface Duo offline navigation hiking setup","content":"## Introduction Over the past few months, I\u0027ve been going on a hike almost every weekend. Not only does hiking provide an opportunity to discover the natural wonders all around you, it\u0027s also good exercise and free (if you don\u0027t count entry fees for some state and national parks). Throughout these trips, the [Surface Duo](https://www.microsoft.com/surface/devices/surface-duo), Microsoft\u0027s dual-screen Android phone has been a reliable companion. Although it\u0027s not the smallest or most rugged device, its large screen(s) paired with OSMAnd~ and AllTrails make it easy to navigate while on the trail. In this post, I\u0027ll detail my navigation setup. ![Picture of half-dome taken from Four Mile Trail](https://user-images.githubusercontent.com/11130940/127246913-58472929-f067-41f0-9011-17193f863f71.JPG) ## Getting trail routes from AllTrails Although trails are often well marked, it\u0027s always a good idea to have a map of the trail and surrounding area. This is especially true if it\u0027s your first time there. I almost exclusively get my trail maps from [AllTrails](https://www.alltrails.com/). If you\u0027re not familiar with AllTrails, it\u0027s a platform and community where members share maps, reviews, and information about trails for all kinds of outdoor activities. The site is completely free and while you can pay for premium features, the free offering provides you with all the essentials you\u0027d need to plan your trip. Trail information, maps, and reviews are entirely user supported. One of the (free) features is the ability to download trails to a variety of formats. ![AllTrails Download Route](https://user-images.githubusercontent.com/11130940/127243740-5ef2e2e3-4c9c-4dbc-a8c5-be608fafeea5.png) Typically, the format I\u0027ll download routes to is GPX Route and GPX Track. [GPX](https://en.wikipedia.org/wiki/GPS_Exchange_Format) is an XML schema for representing GPS data in software applications. Because GPX is a standard format, you can open and view these files in any app that supports them. ## Offline navigation with OSMAnd~ AllTrails has a mobile app. However, for offline navigation, I use the [OSMAnd~](https://f-droid.org/en/packages/net.osmand.plus/) Android app which I downloaded from [F-Droid](https://f-droid.org/). It\u0027s also available in the Google Play Store and Apple App Store. OSMAnd~ is a free map and navigation app supported by [OpenStreetMaps](https://www.openstreetmap.org/). OpenStreetMaps, like AllTrails is free and entirely community driven. OSMAnd~ lets you download **detailed** maps for offline use. Detailed is the key here. Although you can save maps for offline use with applications like Here Maps and Google Maps, they often limit the size of the area you can save for offline use and don\u0027t provide much detail. With OSMAnd~ you can: - Download entire state and country maps for offline use. - View details like contour-lines, hill shading, and other trail details. - Turn-by-turn navigation. - View and edit GPX files Using the GPX files I get from AllTrails, I load them onto my Surface Duo and view them using OSMAnd~. Using offline maps and the Duo\u0027s GPS I\u0027m able to see my position on the trail and quickly find my way when I\u0027ve gone off it even if I have no reception. This combination makes for a navigation device and experience like no other because I have the full screen real estate that the Duo provides to view areas beyond the trail. ![OSMAnd Android App Displaying Track](https://user-images.githubusercontent.com/11130940/127246186-6b5f02ec-7fef-4f2e-909b-8a54615dc790.png) ## Conclusion By leveraging the power of crowd-sourcing and open-source, access to quality information has never been easier. When this information is displayed on a device like the Surface Duo it makes it easy to navigate when you\u0027re out on the trail especially in situations when you have no reception and you\u0027re not familiar with your surroundings. If you\u0027ve been looking for an offline navigation solutions, hopefully this post helps provide some guidance. If you do any outdoor activities especially in remote areas, even if you don\u0027t have a Duo, I\u0027d suggest giving AllTrails and OSMAnd~ a try. See you on the trail! P.S. Over the past month, I\u0027ve transitioned to the Garmin Fenix 6X Pro for navigation. It\u0027s a smaller device that supports loading GPX files and provides offline maps with turn-by-turn navigation. I\u0027m also not worried about dropping or scratching it because it\u0027s designed for the outdoors. This in no way takes away with how amazing the OSMAnd~ and Surface Duo pairing are for offline navigation. In fact, I still use the Duo for backup navigation when it\u0027s difficult to see the map on the Fenix.","contentType":"posts","url":"https://www.lqdev.me/posts/surface-duo-offline-navigation-hiking-setup","date":"2021-07-27 19:00:00 -05:00","tags":["hiking","openstreetmaps","open-source","surface","lifestyle","apps","fdroid"],"summary":"## Introduction Over the past few months, I\u0027ve been going on a hike almost every weekend. Not only does hiking provide an opportunity to discover the natural wonders all around you, it\u0027s also good...","keywords":["navigation","trail","alltrails","offline","maps","osmand~","free","download","view","from"]},{"id":"posts-fenix-social-media-targeted-ads","title":"Never change targeted ads","content":"The origin of this post comes from a random observation I made earlier today on how fascinating and disturbing targeted ads can be. But first, some context. Over the past year, I\u0027ve been doing a lot more hiking. Although most trails are well marked, it\u0027s always good to have some way to navigate in case you miss a turn. For remote locations with limited connectivity, one of the best ways to navigate, other than paper maps, is to download the trail GPX files and view them using a mapping application offline. This is just what I\u0027ve been doing on my Surface Duo with the help of [AllTrails](https://www.alltrails.com/) and [OSMAnd\u002B](http://osmand.net/). That setup and workflow are a post on its own, but I\u0027ll leave that for another time. While that\u0027s worked extremely well, there are some downsides to it. For starters, the Surface Duo is not exactly the smallest or most rugged device so it can be cumbersome to handle while on the trail. That means I have to be extra careful to keep it from falling on rocks or off a cliff. Having navigation turned on also drains the phone battery, which is less than ideal in the event I were to need the phone for an emergency. Therefore, I started looking for an alternative navigation solution. After significant research, I landed on a wearable device with offline mapping capabilities, specifically the [Garmin Fenix 6](https://discover.garmin.com/en-US/fenix-6/). Although I\u0027ve had my eye on this device for the past few months, I couldn\u0027t justify paying full price for it. Fast-forward to this past week. For work, I recently published some articles on Kubernetes. Around the same time, as part of Father\u0027s Day \u0026 Amazon Prime Day sales, the price of the device dropped significantly enough for me to buy it. This leads us to the main point of this post. While looking at my Twitter feed, I ran into this gem. ![Garmin Kubernetes Twitter targeted ad](https://user-images.githubusercontent.com/11130940/123006831-47e1f080-d386-11eb-98ac-af80f6f5f6f0.png) Coincidence? I think not! It\u0027s fascinating to see how hyper-specific ads are based on things I\u0027m interested in. In this case it\u0027s harmless. I\u0027m always up for learning a thing or two about Kubernetes and if I get the chance to win this watch, count me in! The disturbing part is, what else does it know about me and how is that information being used? Depending on where on the spectrum of fascinated / disturbed you land on, one way to control the amount of information shared is through the privacy settings. Unfortunately privacy settings only go so far and visibility into who has what data is limited. Maybe it\u0027s wishful thinking but it\u0027d be great to have a tool that provides more insights and control over what data is shared and with whom. Personally, I think whether targeted ads / recommendations are good or bad is not black or white. If it\u0027s something I\u0027m already doing or looking into, it\u0027d be nice to be presented with options to include as part of my research. On the other hand, if those recommendations / targeted ads are exploiting information from my interests to manipulate my behavior, that\u0027s not right. Where that line is drawn though is not always clear and often personal. In the meantime though, all I can say is, stay classy targeted ads!","contentType":"posts","url":"https://www.lqdev.me/posts/fenix-social-media-targeted-ads","date":"2021-06-22 17:40:00 -05:00","tags":["random","privacy","social media","ads"],"summary":"The origin of this post comes from a random observation I made earlier today on how fascinating and disturbing targeted ads can be. But first, some context. Over the past year, I\u0027ve been doing a lot...","keywords":["targeted","device","past","always","looking","doing","while","kubernetes","garmin","part"]},{"id":"posts-rediscovering-rss-user-freedom","title":"Rediscovering the RSS protocol","content":"## Introduction RSS provides a standardized way of aggregating content giving content creators the freedom to choose how they publish content while giving consumers the freedom to choose how they consume that content. I\u0027ve used RSS for a long time. One of my favorite RSS readers was Google Reader while it was still around and I believe [I\u0027m not the only fan](https://www.ripgooglereader.com/). Since then, I\u0027ve bounced around between different RSS feed readers not settling one until recently when I started using [NewsBlur](https://newsblur.com). As a result of finding an RSS feed reader I liked, I\u0027ve also started to expand the content I follow via RSS feeds to social media, videos, podcasts, and software projects. In part, this has gotten me thinking about some of the benefits of protocols. The essay [Protocols, Not Platforms: A Technological Approach To Free Speech](https://knightcolumbia.org/content/protocols-not-platforms-a-technological-approach-to-free-speech) makes good points on some of these benefits. Though the focus of that article is on free speech, I prefer to think about the benefits in terms of user freedom. ## Creator freedom using RSS RSS provides content creators with the freedom to choose how and where they publish content. As a content creator, depending on the type of content you produce you use different platforms to publish. For example, when I blog, I publish on this site, when I create videos, I publish to YouTube or livestream on Twitch and to further expand the reach of my content, I publish links on social media like Reddit or Twitter. In each case, I use different platforms to publish content. This might be okay because the audiences interested in the video content may not be interested in blog posts and vice versa. However, now the content is split across all these various platforms with their own accounts, interfaces, policies, and audiences. It also means, when I tell people where they can follow me, it usually involves an entire speech like: \u0022Follow me on GitHub at [lqdev](https://github.com/lqdev/), Twitter at [ljquintanilla](https://twitter.com/ljquintanilla), my blog at [luisquintanilla.me](/posts/1)...\u0022. Of course, I don\u0027t always list out all the platforms I\u0027m active on and it\u0027s highly dependent on the setting, but the more platforms I\u0027m active on the longer that speech gets. Fortunately, some of these platforms generate RSS feeds. On GitHub, RSS feeds are generated for releases, issues, and pull requests. For example, the ML.NET releases RSS feed can be accessed by appending *.atom* to the releases URL [https://github.com/dotnet/machinelearning/releases.atom](https://github.com/dotnet/machinelearning/releases.atom). Note that while Atom and RSS are not the same, they both offer a way to aggregate content. Similarly, YouTube provides the option to generate feeds for channels. For example, you can find the RSS feed to my YouTube channel at the following URL [https://www.youtube.com/feeds/videos.xml?channel_id=UCkA5fHdQ4cf3D1J19UNgV7A](https://www.youtube.com/feeds/videos.xml?channel_id=UCkA5fHdQ4cf3D1J19UNgV7A). On this blog, the RSS feed can be found at [/posts/feed.xml](/posts/feed.xml). For social media, the experience is a bit more limited since platforms like Twitter, Instagram, or Facebook don\u0027t generate RSS feeds which is unfortunate since they all have feeds of their own making them the perfect use case for RSS feeds. Platforms like Reddit provide RSS feeds to public subreddits and users by appending *.rss* at the end of the URL. For example, the RSS feed for the Zune subreddit can be found at [https://www.reddit.com/r/Zune.rss](https://www.reddit.com/r/Zune.rss). In each of these cases, though the content and platforms are different, the underlying protocol and format used is the same, RSS. Therefore, a strategy to improve how I publish my content using RSS is, instead of sending users to the various platforms, I can provide links to the different RSS feeds for each of the platforms I\u0027m active on in my personal site. Instead of having a long speech on the different platforms I\u0027m active on, I can just say, \u0022Go to my personal website at ...\u0022, and that\u0027s it! That becomes the gateway to the rest of my content. I can continue publishing on the different platforms because of the features they may provide me with for content creation and discovery, but only send users to a single place. For even more control over publishing, I can self-host videos or other content and create RSS feeds for it just like I do for my blog. In either case, users don\u0027t need to have an account in order to access and follow my content lowering the barrier to access content. This brings me to my next point of user freedom. ## User freedom using RSS RSS provides users with the freedom to choose how they consume content. One way it does so is by being a standard protocol for aggregating content. Because there\u0027s an agreed upon set of standards, clients or RSS feed readers that consume this protocol at minimum only need to be able to read an RSS feed. Once a client can read the RSS feed, it\u0027s free to build on top of that leading to diversity and competition among the various clients. Diversity and competition is good for users because it provides choice. If I want a minimal feed reader, I can subscribe to RSS feeds using an e-mail client like [Outlook](https://support.microsoft.com/en-us/office/subscribe-to-an-rss-feed-73c6e717-7815-4594-98e5-81fa369e951c) or [Thunderbird](https://support.mozilla.org/en-US/kb/how-subscribe-news-feeds-and-blogs). If I want more features, I can use something like NewsBlur. As an aside, one of the other things I like about NewsBlur is being provided with an e-mail address that I can use to subscribe to newsletters helping me both clear up my inbox and letting me access information in a single place. This shows how RSS readers don\u0027t have to stop at only reading RSS feeds and can build on top of the protocol while still performing their core functionality. Another reason to use RSS is having a single place to consume almost all your content. As mentioned before, an RSS feed can aggregate content from a YouTube channel, subreddit, blog, newspaper, podcast, etc. RSS is agnostic in the sense it doesn\u0027t care what\u0027s in the feed so long as you\u0027re able to link to the content. Now, because RSS readers can read any RSS feed, it means you can follow almost all of your content from a single place so long as you\u0027re able to subscribe to the RSS feed. In fact, aside from some social media, the only other content I don\u0027t consume via RSS is podcasts. Not because I\u0027m not able to since podcasts are usually aggregated using RSS feeds, but because I\u0027m not able to play and sync them with other devices. Being able to consume content in a single place provides users with an incredible amount of freedom by not requiring the use of multiple apps, websites, or accounts. One last way in which RSS gives user freedom to choose how they consume content is by giving them control of their feed. Various platforms like social media sites and YouTube generate feeds and recommend content using algorithms. These algorithms are sometimes useful by exposing you to new and diverse content. Other times, algorithms can create a bubble by reinforcing preferences and interests. In any case, there\u0027s little visibility or control into how the algorithms make choices and recommendations. RSS by default is ordered in chronological order. Therefore you know you\u0027re getting the newest content. Depending on the client, you might be able to sort content into folders in whichever way makes the most sense to you. You might also be able to choose how often the feeds you\u0027re subscribed to are checked for new content. If you don\u0027t want to get the latest updates every second, set it to update daily or weekly. If you want to get rid of the clutter and only see content from the previous day, you can automatically delete or mark older content as read. Note that all these capabilities are dependent on the client, but if this is important to you, you can choose a client that gives you these features making the possibilities endless. Finally, because you are actively choosing which feeds you subscribe to, you\u0027re not presented with content other people liked, retweeted, shared, or recommended. You only have access to the content you choose to subscribe to. If at any point you\u0027re no longer interested in the content, unsubscribe and you no longer see it on your feed. ## Conclusion As I continue my rediscovery of RSS, I want to find ways to subscribe to content on platforms I\u0027m already using via RSS. It\u0027s important to note that this is not an either or approach. You can subscribe to content from the various platforms using RSS to get updates in a single place and since RSS links to the original content, you can consume content on the platform itself helping content creators with more page views, likes, subscriptions, and other forms of engagement. Ultimately though, the choice is yours. A choice of how you publish and consume content.","contentType":"posts","url":"https://www.lqdev.me/posts/rediscovering-rss-user-freedom","date":"2021-04-05 19:30:00 -05:00","tags":["protocols","web","rss"],"summary":"## Introduction RSS provides a standardized way of aggregating content giving content creators the freedom to choose how they publish content while giving consumers the freedom to choose how they...","keywords":["content","platforms","feed","feeds","freedom","using","consume","publish","they","subscribe"]},{"id":"posts-alternatives-to-whatsapp","title":"Alternatives to WhatsApp","content":"## Introduction Recently I\u0027ve heard a lot of buzz around people moving away from the WhatsApp messaging app. I\u0027m not tuned into all the details as to why there\u0027s been a mass exodus, but from what I understand it has to do with its privacy policies. I\u0027ve seen many people including Elon Musk recommend Signal. Signal is a great alternative but there are a few others that are worth considering. This recommendation was originally planned as a tweet, but there\u0027s so many details that just can\u0027t be captured in a tweet or thread so instead I turned it into a blog post. If you haven\u0027t made the switch or if you\u0027re still not completely committed to the platform you\u0027ve switched to, this post is for you. ![Phone and notebook - Oscar Mucyo Unsplash](https://user-images.githubusercontent.com/11130940/104107289-08689180-5289-11eb-890d-54527902df97.png) ## Matrix ![Element web client screenshot](https://user-images.githubusercontent.com/11130940/104107320-3e0d7a80-5289-11eb-9449-71e34315b96e.png) [Matrix](https://matrix.org/) is my go-to messaging platform. Matrix is an open-source project that provides an open standard for real-time communication. Built on top of this standard are several implementations with [Synapse](https://github.com/matrix-org/synapse) being the most popular server and [Element](https://element.io/) being the most popular client. Matrix is relatively new. However, it has a lot of potential and there are a lot of exciting things happening around it. Some of the most notable include: - [Mozilla selected Matrix](https://matrix.org/blog/2019/12/19/welcoming-mozilla-to-matrix) to be the successor to its IRC communications. - Matrix recently [acquired Gitter](https://matrix.org/blog/2020/09/30/welcoming-gitter-to-matrix), a popular chat platform for software projects, and has quickly worked to [integrate Gitter with Matrix](https://matrix.org/blog/2020/12/07/gitter-now-speaks-matrix) allowing for it to take advantage of many of the features provided by the protocol. - [Matrix will power FOSDEM 2021](https://matrix.org/blog/2021/01/04/taking-fosdem-online-via-matrix) communications. - [Matrix is experimenting with P2P architectures](https://matrix.org/blog/2020/06/02/introducing-p-2-p-matrix) to help users have full control over their communications. Although Matrix provides several features commonly associated with messaging platforms such as 1:1 messaging, video/audio calls, and group chat, there are a few features worth highlighting: - End-to-end encryption - Interoperability with other communication platforms - Decentralization ### End-to-end encryption End-to-end encryption makes sure your communications stay private. Only you and the intended recipients of the message are able to see the messages. ### Interoperability One of the biggest challenges of moving over to a new platform is that your friends and family don\u0027t or won\u0027t join you, making it a lonely place. Matrix provides interoperability with other communication platforms and protocols through [bridges](https://matrix.org/bridges/). Bridges facilitate the exchange of messages with other platforms without requiring all participants to be on Matrix. The core team maintains bridges for Slack, IRC, XMPP, and Gitter. There are also community maintained bridges for WhatsApp, Discord, Facebook Messenger, Signal, and many others. ### Decentralization Decentralization means that there\u0027s no central authority that holds control over the platform or data. Matrix is decentralized through federation. One of the most common examples of federation is e-mail. I may have an AOL e-mail address and you may have a Hotmail e-mail address, yet we\u0027re still able to exchange messages. With Matrix, in a similar way, users and groups don\u0027t have to have their accounts on the same server in order to communicate. It also means that users have a choice over [which server they join](https://www.hello-matrix.net/public_servers.php). If I don\u0027t agree with the conversations or moderation of a particular server, I have the choice to open an account on another server without leaving Matrix altogether. Even better, because Matrix is built using open technologies, I have the option of hosting my own server. To get started with Matrix, visit the [Element](https://element.io/get-started) website and create an account. ## Jami ![Jami Screenshot](https://user-images.githubusercontent.com/11130940/104107348-64331a80-5289-11eb-8606-944269e17d10.png) Another project that I like but haven\u0027t used it to the same extent as Matrix is [Jami](https://jami.net/). Jami is an open-source project for real-time communication. Similar to Matrix, Jami is cross-platform, free (as in freedom and beer), end-to-end encrypted, and decentralized. Jami achieves decentralization by using P2P protocols. This means that the data is stored on your device so you\u0027re in full control of your data. Jami also doesn\u0027t require the internet to work. As long as you\u0027re in the same local network as your peers, you\u0027re able to communicate. To get started with Jami, [download the app](https://jami.net/download/) and create an account. ## Conclusion In this post, I\u0027ve mentioned some communication platforms that I consider great alternatives to WhatsApp. Regardless of the platform you choose, some things that I believe should be top of mind are security and privacy. It\u0027s important that you own your data and you have control over who has access to it and what they do with it. Feel free say hi on Matrix [@lqdev:lqdev.tech](https://matrix.to/#/@lqdev:lqdev.tech) or Jami where you can find me as lqdev1.","contentType":"posts","url":"https://www.lqdev.me/posts/alternatives-to-whatsapp","date":"2021-01-09 13:44:27 -05:00","tags":["privacy","self-hosted","security","open-source","apps"],"summary":"## Introduction Recently I\u0027ve heard a lot of buzz around people moving away from the WhatsApp messaging app. I\u0027m not tuned into all the details as to why there\u0027s been a mass exodus, but from what I...","keywords":["matrix","jami","https://matrix","platform","server","open","communication","platforms","decentralization","gitter"]},{"id":"posts-how-to-watch-twitch-using-vlc","title":"How to watch Twitch streams using VLC","content":"## Introduction Twitch has become a tool for various creators to connect with audiences. Originally, streams were centered around gaming but now there is a wide variety of creators that include musicians, software developers, and craftspeople. One of my favorites is [Brainfeeder](https://www.twitch.tv/brainfeeder). I myself try (and often fail) to build software projects on stream. If you\u0027re interested in hanging out and talking machine learning, .NET and all things tech, feel free to join me at [https://twitch.tv/lqdev1](https://twitch.tv/lqdev1). Watching Twitch streams on the desktop via the browser can take a toll on compute resources. A more efficient solution to this problem is to use VLC. VLC is a free, open-source, cross-platform media player that supports various protocols. While it was possible to watch Twitch streams using VLC before, it often required additional software such as [Streamlink](https://github.com/streamlink/streamlink). More recent versions make this requirement redundant. In this post, I\u0027ll show how to use VLC to watch Twitch streams. ![Twitch iOS App on iPhoneX - Caspar Camille Rubin](https://user-images.githubusercontent.com/11130940/103722126-4883f780-4f9d-11eb-8cc8-304d1a249ef9.png) ## Prerequisites - VLC (version **3.11.0 or higher**) ## Watch Twitch Stream \u003E **Update 11/12/2021** \u003E It\u0027s been a while since I\u0027ve gotten the original steps to work, so feel free to skip them. You can still watch Twitch streams on VLC using Streamlink. To use Streamlink: \u003E 1. Download and install [Streamlink](https://streamlink.github.io/install.html) \u003E 1. Open the terminal and run: \u003E \u003E \u0060\u0060\u0060 \u003E streamlink https://twitch.tv/ best \u003E \u0060\u0060\u0060\u0060 \u003E \u003E For more information, see the [Streamlink Command-Line Interface documentation](https://streamlink.github.io/cli.html) 1. Open VLC 2. From the menu, Select **Media \u003E Open Network Stream** 3. Paste the URL to the stream you want to watch in the text box (i.e. https://twitch.tv/lqdev1) At this point, the stream should start playing. ### CLI Alternatively on Unix (Mac / Linux) systems, you can use the CLI. To watch a stream, open a terminal and enter the following command: \u0060\u0060\u0060bash vlc https://twitch.tv/ \u0060\u0060\u0060 Make sure to replace the username with the streamer you want to watch. After a few seconds, VLC loads and starts playing the stream. ## Performance comparison Below is a comparison of the same stream playing on the browser vs VLC. These test were performed on a Surface Laptop 1 running Windows 10. By no means should you take this as a definitive performance comparison as there are many variables that may affect performance. However, it\u0027s a nice way to visualize each of the methods. ### Browser In the browser scenario, I only had a single tab playing the Twitch stream on Microsoft Edge (chromium). ![Browser task manager](https://user-images.githubusercontent.com/11130940/103723087-43c04300-4f9f-11eb-9969-df40541693bd.png) ### VLC ![VLC task manager](https://user-images.githubusercontent.com/11130940/103721933-e1fed980-4f9c-11eb-92f3-0147f2c09134.png) As it can be seen from this image, there\u0027s about a 4x decrease in CPU resource consumption when using VLC. ## Conclusion In this post I showed how you can use VLC to watch Twitch streams. One of the drawbacks is, you can\u0027t see the channels you follow and who\u0027s online. You also can\u0027t browse streams like you would on the Twitch web client. You also don\u0027t have access to chat, which is half the fun on many streams. However, if you\u0027re only interested in the content, VLC is an excellent and less resource-hungry way of watching Twitch streams. See you on Twitch!","contentType":"posts","url":"https://www.lqdev.me/posts/how-to-watch-twitch-using-vlc","date":"2021-01-05 21:14:39 -05:00","tags":["streaming","hacks","video","IT"],"summary":"## Introduction Twitch has become a tool for various creators to connect with audiences. Originally, streams were centered around gaming but now there is a wide variety of creators that include...","keywords":["twitch","streams","watch","stream","https://twitch","browser","open","streamlink","playing","performance"]},{"id":"posts-image-classification-mlnet-fsadvent2020","title":"Train an image classifier using F# and ML .NET","content":"# Introduction This post is part of F# Advent 2020. Thank you to [Sergey Tihon](https://twitter.com/sergey_tihon) for organizing this and the rest of the contributors for producing high-quality content. Make sure to check out the rest of the [F# Advent 2020 content](https://sergeytihon.com/2020/10/22/f-advent-calendar-in-english-2020/). When picking who\u0027s on the naughty or nice list, I often wonder how Santa decides. I took a shot at answering this question by training an image classifier using the ML.NET image classification API and images of Disney heroes and villains to tell whether they\u0027re naughty or nice. You shouldn\u0027t judge someone by the way they look (even if they are the Evil Queen), so it\u0027s safe to say, don\u0027t try this at home or with your neighbors \uD83D\uDE09. This sample is just for demo purposes. You can find the full code on [GitHub](https://gist.github.com/lqdev/0c4dc9eea93b7b8541f31ddd429afb53). ## Prerequisites This sample was built on a Windows 10 PC, but should work on Mac / Linux PCs - [.NET 5 SDK](https://dotnet.microsoft.com/download) ## The data The dataset contains images of Disney characters, both real and animated. They were obtained from the [Disney Fandom Wiki](https://disney.fandom.com/wiki/The_Disney_Wiki). The characters are split into two categories, [villains](https://disney.fandom.com/wiki/Category:Villains) and [heroes](https://disney.fandom.com/wiki/Category:Protagonists). For the purpose of this sample, we\u0027ll label heroes as nice and villains as naughty. The dataset used to train this model contains 2400 villain (naughty) and 675 hero (nice) images stored in top-level directories with the naughty/nice names. This means that the dataset is unbalanced and may skew predictions as it can be seen when making predictions. ## Install NuGet packages Use the \u0060#r\u0060 convention to install the necessary NuGet packages used in this sample. \u0060\u0060\u0060fsharp #r \u0022nuget:Microsoft.ML\u0022 #r \u0022nuget:Microsoft.ML.Vision\u0022 #r \u0022nuget:Microsoft.ML.ImageAnalytics\u0022 #r \u0022nuget:SciSharp.TensorFlow.Redist\u0022 \u0060\u0060\u0060 Then, import the packages. \u0060\u0060\u0060fsharp open System open System.IO open Microsoft.ML open Microsoft.ML.Data open Microsoft.ML.Vision \u0060\u0060\u0060 ## Define data types Start off by defining the data types containing your input and output schema. You can do this by creating two records, \u0060ImageData\u0060 and \u0060ImagePrediction\u0060. \u0060ImageData\u0060 is the input which contains the path to image file and the category it belongs to and the \u0060ImagePrediction\u0060 contains the prediction generated by the model. \u0060\u0060\u0060fsharp [ ] type ImageData = { ImagePath: string Label: string } [ ] type ImagePrediction = { PredictedLabel: string } \u0060\u0060\u0060 ## Training The training process loads a set of training images, preprocesses them, and uses the ML.NET image classification API to train an image classification model. ### Initialize MLContext Once you\u0027ve defined the data type, initialize the \u0060MLContext\u0060. \u0060MLContext\u0060 is the entrypoint for ML.NET applications. \u0060\u0060\u0060fsharp let ctx = new MLContext() \u0060\u0060\u0060 ### Load training data Then, load the data using the helper function \u0060loadImagesFromDirectory\u0060 and point it to the top-level directory containing the subdirectories of images in the nice and naughty categories. \u0060\u0060\u0060fsharp let imageData = loadImagesFromDirectory \u0022C:/Datasets/fsadvent2020/Train\u0022 true \u0060\u0060\u0060 The \u0060loadImagesFromDirectory\u0060 function looks like the following: \u0060\u0060\u0060fsharp let loadImagesFromDirectory (path:string) (useDirectoryAsLabel:bool) = let files = Directory.GetFiles(path, \u0022*\u0022,searchOption=SearchOption.AllDirectories) files |\u003E Array.filter(fun file -\u003E (Path.GetExtension(file) = \u0022.jpg\u0022) || (Path.GetExtension(file) = \u0022.png\u0022)) |\u003E Array.map(fun file -\u003E let mutable label = Path.GetFileName(file) if useDirectoryAsLabel then label Char.IsLetter) then label (imageData) \u0060\u0060\u0060 ### Define training pipeline Once your data is is loaded into an \u0060IDataView\u0060, set the classifier options by using \u0060ImageClassificationTrainer.Options\u0060. Use it to define the name of the network architecture, input and output columns, and some additional parameters. The network architecture used in this case is [\u0060ResNet V2\u0060](https://www.paperswithcode.com/method/inception-resnet-v2). \u0060\u0060\u0060fsharp let classifierOptions = ImageClassificationTrainer.Options() classifierOptions.FeatureColumnName (fun x -\u003E printfn \u0022%s\u0022 (x.ToString())) \u0060\u0060\u0060 Define the preprocessing steps, image classification trainer (along with the previously defined options) and postprocessing steps. \u0060\u0060\u0060fsharp let pipeline = EstimatorChain() .Append(ctx.Transforms.LoadRawImageBytes(\u0022Image\u0022,null,\u0022ImagePath\u0022)) .Append(ctx.Transforms.Conversion.MapValueToKey(\u0022LabelKey\u0022,\u0022Label\u0022)) .Append(ctx.MulticlassClassification.Trainers.ImageClassification(classifierOptions)) .Append(ctx.Transforms.Conversion.MapKeyToValue(\u0022PredictedLabel\u0022)) \u0060\u0060\u0060 The ML.NET image classification API leverages a technique known as transfer learning. Transfer learning uses pretrained models (usually neural networks) and retrains the last few layers using new data. This significantly cuts down the amount of time, resources, and data you need to train deep learning models. ML .NET is able to do this with the help of [TensorFlow .NET](https://github.com/SciSharp/TensorFlow.NET), a set of .NET bindings for the TensorFlow deep learning framework. Although transfer learning usually makes the process of training a deep learning models less resource intensive, the TensorFlow API is usually low level and still requires a significant amount of code. See this [transfer learning example](https://github.com/SciSharp/SciSharp-Stack-Examples/blob/master/src/TensorFlowNET.Examples/ImageProcessing/TransferLearningWithInceptionV3.cs) from TensorFlow.NET to see how you\u0027d do it in TensorFlow.NET. Although the low-level nature of the TensorFlow API gives you control over what you\u0027re building, many times you don\u0027t need that level of control. ML.NET through the image classification trainer greatly simplifies this process by providing a high-level API for achieving the same task. To train the model, use the \u0060Fit\u0060 method on the training image \u0060IDataView\u0060. \u0060\u0060\u0060fsharp let model = pipeline.Fit(imageIdv) \u0060\u0060\u0060 Throughout the training process, you should see output similar to the following: \u0060\u0060\u0060console Phase: Bottleneck Computation, Dataset used: Train, Image Index: 279 Phase: Bottleneck Computation, Dataset used: Train, Image Index: 280 Phase: Bottleneck Computation, Dataset used: Validation, Image Index: 1 \u0060\u0060\u0060 With the model trained, it\u0027s time to use it to make predictions. Optionally, you can save it for use in other applications. \u0060\u0060\u0060fsharp ctx.Model.Save(model,imageIdv.Schema,\u0022fsadvent2020-model.zip\u0022) \u0060\u0060\u0060 ## Make predictions Load the test images and create an \u0060IDataView\u0060 for them. The test images used are of Jack Skellington and The Grinch. \u0060\u0060\u0060fsharp let testImages = Directory.GetFiles(\u0022C:/Datasets/fsadvent2020/Test\u0022) |\u003E Array.map(fun file -\u003E {ImagePath=file; Label=\u0022\u0022}) let testImageIdv = ctx.Data.LoadFromEnumerable (testImages) \u0060\u0060\u0060 ![The grinch](https://www.khwiki.com/images/thumb/f/ff/Jack_Skellington_KHII.png/180px-Jack_Skellington_KHII.png) Then, use the model to make predictions. \u0060\u0060\u0060fsharp let predictionIdv = model.Transform(testImageIdv) \u0060\u0060\u0060 One of the easiest ways to access the predictions is to create an \u0060IEnumerable\u0060. To do so, use the \u0060CreateEnumerable\u0060 method. \u0060\u0060\u0060fsharp let predictions = ctx.Data.CreateEnumerable (predictionIdv,false) \u0060\u0060\u0060 Then, use the built-in F# sequence operations to display the predictions \u0060\u0060\u0060fsharp predictions |\u003E Seq.iter(fun pred -\u003E printfn \u0022%s is %s\u0022 (Path.GetFileNameWithoutExtension(pred.ImagePath)) pred.PredictedLabel) \u0060\u0060\u0060 The output should look like the following: \u0060\u0060\u0060console grinch is Naughty jack is Naughty \u0060\u0060\u0060 ## Conclusion In this post, I showed how you can use the ML.NET and TensorFlow.NET to train an image classification model to classify Disney characters as naughty or nice. Depending on the level of control you need, you might choose to use TensorFlow.NET or if you want a high-level API for training an image classifier, you can use the ML.NET. Most importantly, we figured out that Jack Skellington and The Grinch are naughty, so I guess no gifts for them this year? Happy coding! ### Call to Action Originally, I had planned on writing this article using [TensorFlow.Keras](https://github.com/SciSharp/TensorFlow.NET), which is part of the SciSharp TensorFlow.NET project. TensorFlow.Keras provides .NET bindings for Keras. Keras provides a high-level API for TensorFlow which makes the process of building custom neural networks much simpler than working with the TensorFlow API. Unfortunately, while trying to adapt my scenario to an [existing sample](https://github.com/SciSharp/SciSharp-Stack-Examples/blob/master/src/TensorFlowNET.Examples/ImageProcessing/ImageClassificationKeras.cs), I ran into an [issue](https://github.com/SciSharp/SciSharp-Stack-Examples/issues/23). This is not something I would have been able to resolve in time to publish this post, so I defaulted to using ML.NET. I\u0027m a big fan of the work being done by the SciSharp community and the machine learning and data science capabilities it brings to the .NET ecosystem. The work and efforts are all community driven, and as such, there\u0027s plenty of opportunities to contribute. Here are just some examples of ways to contribute, especially from an F# perspective. From my end, I plan on eventually converting this sample to use TensorFlow.Keras. See you in the SciSharp repos! \uD83D\uDE42 ![FsLab SciSharp contribute](https://user-images.githubusercontent.com/11130940/102030239-f4ac3700-3d7f-11eb-9898-f18990a56326.png)","contentType":"posts","url":"https://www.lqdev.me/posts/image-classification-mlnet-fsadvent2020","date":"2020-12-14 20:03:18 -05:00","tags":["image classification","mlnet","fsharp","dotnet","deep learning","fsadvent2020","tensorflow"],"summary":"# Introduction This post is part of F# Advent 2020. Thank you to [Sergey Tihon](https://twitter.com/sergey_tihon) for organizing this and the rest of the contributors for producing high-quality...","keywords":["tensorflow","image","\u0060\u0060\u0060fsharp","data","model","training","naughty","predictions","level","images"]},{"id":"posts-dotnet-jupyter-notebooks-azure-machine-learning-compute-instance","title":"Use .NET Interactive to run .NET code in Jupyter Notebooks on an Azure Machine Learning compute instance","content":"# Introduction In this post, I\u0027ll go over how you can run .NET code in Jupyter Notebooks using .NET Interactive and Azure Machine Learning compute instances. [Jupyter Notebooks](https://jupyter.org/) are an graphical interactive computing environment used in disciplines like data science and education. When it comes to prototyping or learning scenarios, they can help users see near real-time what their code is doing. The interactive computing protocol used by Jupyter Notebooks is extensible and as a result, you\u0027re able to run a variety of languages inside of this environment via kernels. Kernels are processes that take the input from the user, execute the code, and return the output for display and further processing. Using [.NET Interactive](https://github.com/dotnet/interactive), a .NET Core global tool, that among its many features provides a kernel for Jupyter Notebooks to run .NET (C#/F#) and PowerShell code. Although you can install and run Jupyter Notebooks on your local computer, services like [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/) provide single VMs known as compute instances which have Jupyter Notebooks as well as many popular data science libraries and development tools like Docker preinstalled. Therefore, installing .NET Interactive on one of these VMs is a seamless experience. Additionally, Azure Machine Learning gives you the ability to share computing resources with teams of various sizes. Let\u0027s get started! ## Prerequisites - [Azure account](https://azure.microsoft.com/free/). If you don\u0027t have one, create one. ## Create Azure Machine Learning workspace A workspace organizes all of you Azure Machine Learning resources and assets such as compute instances in a single place. In order to use Azure Machine Learning, you have to create a workspace. There\u0027s multiple ways to create a workspace. For this writeup, I use the Azure Portal. Navigate to [portal.azure.com](https://portal.azure.com) In the portal, select **Create a resource**. ![Create new resource Azure Portal](https://user-images.githubusercontent.com/11130940/91647989-d79d6380-ea2f-11ea-9d97-cc265c79801f.png) From the resource list, select **AI \u002B Machine Learning \u003E Machine Learning** ![Create Azure Machine Learning Resource](https://user-images.githubusercontent.com/11130940/91647999-01ef2100-ea30-11ea-954a-3c3280a9cbca.png) Fill in the form and select **Review \u002B create**. ![Create Azure Machine Learning workspace](https://user-images.githubusercontent.com/11130940/91648065-07993680-ea31-11ea-9273-c28b4ec4bf7c.png) Review your information before creating the workspace and select **Create**. Deployment takes a few minutes. As part of your workspace deployment, additional Azure resources are created such as Azure Container Registry, Azure Storage account, Azure Application Insights, and Azure KeyVault. For more information on workspaces, see the [Azure Machine Learning workspace documentation](https://docs.microsoft.com/azure/machine-learning/concept-workspace). ## Create an Azure Machine Learning compute instance As mentioned earlier, an Azure Machine Learning compute instance is a single VM that comes with a variety of development tools and libraries commonly used for data science and machine learning workflows preinstalled. Navigate to [Azure Machine Learning studio](https://ml.azure.com). The studio is a web interface for managing your Azure Machine Learning resources such as data, experiments, models, and compute. In studio, select **Create new \u003E Compute instance** to create a new compute instance. ![Create new Azure Machine Learning compute instance](https://user-images.githubusercontent.com/11130940/91648186-73c86a00-ea32-11ea-9986-205306fe76b0.png) Provide a name for your instance and select **Create**. You can also customize the size of your VM and whether you want it to be GPU enabled. In this case, I just selected the preselected option. ![Provision Azure Machine Learning compute instance](https://user-images.githubusercontent.com/11130940/91648251-1c76c980-ea33-11ea-91de-818e1f63767c.png) Once your compute instance is provisioned, select the **Jupyter** link to launch Jupyter Notebooks. ![Launch Jupyter Notebooks](https://user-images.githubusercontent.com/11130940/91648307-f271d700-ea33-11ea-9b80-57e44f0bcdf5.png) For more information on compute instances, see the [Azure Machine Learning compute instance documentation](https://docs.microsoft.com/azure/machine-learning/concept-compute-instance). ## Install the .NET Core 3.1 SDK At the time of this writing, the image used by compute instances is Ubuntu 16.04. Therefore, we\u0027ll be installing the Linux version of the .NET Core SDK. Inside the Jupyter Notebooks, select **New \u003E Terminal** to create a new terminal. ![image](https://user-images.githubusercontent.com/11130940/91648326-1c2afe00-ea34-11ea-8f84-48b98e1ec148.png) In the terminal, add the Microsoft package signing key to your list of trusted keys. \u0060\u0060\u0060bash wget https://packages.microsoft.com/config/ubuntu/16.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb \u0060\u0060\u0060 Install the .NET Core 3.1 SDK with the following command: \u0060\u0060\u0060bash sudo apt-get update; \\ sudo apt-get install -y apt-transport-https \u0026\u0026 \\ sudo apt-get update \u0026\u0026 \\ sudo apt-get install -y dotnet-sdk-3.1 \u0060\u0060\u0060 \u003E Note that as of the time of this writing, .NET Interactive requires .NET Core 3.1. You can also find these installation instructions in the [.NET Core installation documentation](https://docs.microsoft.com/dotnet/core/install/linux-ubuntu#1604-). ## Install .NET Interactive Start off by checking which kernels are installed. In the terminal, enter the following command. \u0060\u0060\u0060bash jupyter kernelspec list \u0060\u0060\u0060 The output should look something like the output below \u0060\u0060\u0060console Available kernels: python3 /anaconda/envs/azureml_py36/share/jupyter/kernels/python3 ir /usr/local/share/jupyter/kernels/ir python3-azureml /usr/local/share/jupyter/kernels/python3-azureml \u0060\u0060\u0060 Then, install the .NET Interactive global tool \u0060\u0060\u0060bash dotnet tool install -g --add-source \u0022https://dotnet.myget.org/F/dotnet-try/api/v3/index.json\u0022 Microsoft.dotnet-interactive \u0060\u0060\u0060 Use the .NET Interactive tool to install the .NET and powershell kernels. \u0060\u0060\u0060bash dotnet interactive jupyter install \u0060\u0060\u0060 Finally, run the following command to confirm that the kernels have been installed. \u0060\u0060\u0060bash jupyter kernelspec list \u0060\u0060\u0060 In the output you should see the C#, F#, and PowerShell kernels listed \u0060\u0060\u0060bash Available kernels: .net-csharp /home/azureuser/.local/share/jupyter/kernels/.net-csharp .net-fsharp /home/azureuser/.local/share/jupyter/kernels/.net-fsharp .net-powershell /home/azureuser/.local/share/jupyter/kernels/.net-powershell python3 /anaconda/envs/azureml_py36/share/jupyter/kernels/python3 ir /usr/local/share/jupyter/kernels/ir python3-azureml /usr/local/share/jupyter/kernels/python3-azureml \u0060\u0060\u0060 ## Add .NET global tools directory to PATH The Jupyter Notebook server is managed by a \u0060systemd\u0060 service. Although during installation .NET Interactive is added to your \u0060PATH\u0060 environment variable, it\u0027s not added to the environment file \u0060/etc/environment\u0060 which is used by \u0060systemd\u0060. Therefore, in order to run .NET Interactive, you have to add the .NET global tools directory to that file. The contents of the file should look similar to the one below \u0060\u0060\u0060text PATH=\u0022/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\u0022 AML_CloudName=AzureCloud \u0060\u0060\u0060 Open the \u0060/etc/environment\u0060 using your preferred text editor (vi/nano) and replace the \u0060PATH\u0060 with the following content: \u0060\u0060\u0060text PATH=\u0022/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/azureuser/.dotnet/tools\u0022 \u0060\u0060\u0060 Save your changes and restart the Jupyter Server. In the terminal, run the following command. \u0060\u0060\u0060bash sudo service jupyter restart \u0060\u0060\u0060 This will close your terminal session since the notebook server has been restarted. Close all Jupyter Notebook windows. ## Create new .NET Jupyter Notebook From Azure Machine Learning studio, launch Jupyter Notebooks again by selecting the **Jupyter** link. Then, in Jupyter Notebooks select **New** and create a new notebook (C#/F#/PowerShell). In this case, I created an F# notebook. Once the kernel is ready, enter code into the first cell and run it. ![FSharp Jupyter Notebook running in AML compute instance](https://user-images.githubusercontent.com/11130940/91648958-20f3b000-ea3c-11ea-95a1-3e4ba71c31d8.png) Congratulations! You should now be able to run .NET and PowerShell code inside of your Azure Machine Learning compute instance. ## Conclusion In this post, I showed how you can run .NET code in Jupyter Notebooks on an Azure Machine Learning compute instance with the help of .NET Interactive. This enables you to interactively prototype solutions remotely while still having control over your environment and dependencies. Now that you have .NET interactive setup, compute instances also give you the option of using JupyterLab. Feel free to tinker with the different environments and see which one works best for you. Happy coding! ## Sources - [Azure Machine Learning documentation](https://docs.microsoft.com/azure/machine-learning/) - [.NET Interactive GitHub repository](https://github.com/dotnet/interactive) - [Jupyter Notebooks](https://jupyter.org/)","contentType":"posts","url":"https://www.lqdev.me/posts/dotnet-jupyter-notebooks-azure-machine-learning-compute-instance","date":"2020-08-29 20:23:09 -05:00","tags":["dotnet","Jupyter Notebooks","dotnet Interactive","Azure Machine Learning","Azure","Programming","Development Tools"],"summary":"# Introduction In this post, I\u0027ll go over how you can run .NET code in Jupyter Notebooks using .NET Interactive and Azure Machine Learning compute instances. [Jupyter Notebooks](https://jupyter.org/)...","keywords":["azure","machine","learning","jupyter","compute","interactive","notebooks","create","microsoft","instance"]},{"id":"posts-serverless-racket-azure-functions-custom-handlers","title":"Serverless Racket Applications Using Azure Functions Custom Handlers","content":"## Introduction [Racket](https://racket-lang.org/) is a fun and powerful general purpose programming language based on the Scheme dialect of Lisp that provides tools and packages that allow individuals to quickly be productive. Although you can build traditional web applications with it, it would be nice to use it with cloud-native technologies like serverless. Azure Functions is designed for these types of event-driven workflows but unfortunately does not officially support Racket. Recently, a new feature called custom handlers was announced which allows individuals to run web applications written in any language that supports HTTP primitives as an Azure Function. When I learned of this feature, my immediate thought was, challenge accepted! Custom handlers require the following: 1. Write a web server to process requests 2. Define the bindings for the request and response function payloads 3. Configure the Azure Functions host to send request to the web server In this writeup, I\u0027ll show how to set up a Racket web server that processes \u0060GET\u0060 requests running as an Azure Function. The source code for this project can be found in the [RacketAzureFunctionsCustomHandlerSample GitHub repository](https://github.com/lqdev/RacketAzureFunctionsCustomHandlerSample) ## Prerequisites This project was built on a Windows 10 PC, but it should work cross-platform on Mac and Linux. - [Node.js](https://nodejs.org/en/) - [Racket](https://download.racket-lang.org/) - [Azure Functions Core Tools](https://docs.microsoft.com/azure/azure-functions/functions-run-local). This sample uses v2.x of the tool. ## Create Azure Functions project Create a new directory called \u0060RacketAzureFunctionsCustomHandlerSample\u0060 and navigate to it. ## Create Racket server The way Azure Functions custom handlers work is by having the Azure Functions host proxy requests to a web server written in the language of choice which processes the request and sends the response back to the Azure Functions host. Start by setting the environment variable where Azure Functions and the server listen on. Create and environment variable called \u0060FUNCTIONS_HTTPWORKER_PORT\u0060. In this example, I set the variable to \u00607071\u0060. Inside of your application directory, create a file called *server.rkt* which will contain the server logic. Open the *server.rkt* file. Define the language and import the required packages. \u0060\u0060\u0060racket #lang racket (require json) (require web-server/servlet) (require web-server/servlet-env) \u0060\u0060\u0060 Then, get the port where the server listens on from the \u0060FUNCTIONS_HTTPWORKER_PORT\u0060 environment variable. \u0060\u0060\u0060racket (define PORT (string-\u003Enumber (getenv \u0022FUNCTIONS_HTTPWORKER_PORT\u0022))) \u0060\u0060\u0060 Next, create a function called \u0060get-values\u0060 to process your request. In this case, the function receives a \u0060GET\u0060 request that returns a JSON object containing a list of integers. \u0060\u0060\u0060racket (define (get-values req) (response/full 200 #\u0022OK\u0022 (current-seconds) #\u0022application/json;charset=utf-8\u0022 empty (list (jsexpr-\u003Ebytes #hasheq((value . (1 2 3))))))) \u0060\u0060\u0060 After that, define the routes so your server dispatches requests to the appropriate endpoint. In this case, \u0060GET\u0060 requests to the \u0060/values\u0060 endpoint are sent to and processed by the \u0060get-values\u0060 function. \u0060\u0060\u0060racket (define-values (dispatch req) (dispatch-rules [(\u0022values\u0022) #:method \u0022get\u0022 get-values] [else (error \u0022Route does not exist\u0022)])) \u0060\u0060\u0060 The final *server.rkt* file should contains content similar to the one below: \u0060\u0060\u0060racket ;; Define language and import packages #lang racket (require json) (require web-server/servlet) (require web-server/servlet-env) ;; Get port where server listens on (define PORT (string-\u003Enumber (getenv \u0022FUNCTIONS_HTTPWORKER_PORT\u0022))) ;; Create function to handle GET /values request (define (get-values req) (response/full 200 #\u0022OK\u0022 (current-seconds) #\u0022application/json;charset=utf-8\u0022 empty (list (jsexpr-\u003Ebytes #hasheq((value . (1 2 3))))))) ;; Define routes (define-values (dispatch req) (dispatch-rules [(\u0022values\u0022) #:method \u0022get\u0022 get-values] [else (error \u0022Route does not exist\u0022)])) ;; Define and start server (serve/servlet (lambda (req) (dispatch req)) #:launch-browser? #f #:quit? #f #:port PORT #:servlet-path \u0022/\u0022 #:servlet-regexp #rx\u0022\u0022) \u0060\u0060\u0060 ## Test the Racket server Start the server by running the following command: \u0060\u0060\u0060bash racket --require server.rkt \u0060\u0060\u0060 Then, using an application like Postman or Insomina, make a \u0060GET\u0060 request to \u0060http://localhost:7071/values\u0060. The response should look like the following: \u0060\u0060\u0060json { \u0022value\u0022: [ 1, 2, 3 ] } \u0060\u0060\u0060 ## Define function bindings The way Azure Functions discovers functions is through subdirectories containing a binding definition called *function.json*. The name of the subdirectories must match the name of your function\u0027s route path. For example if the route path is \u0060/values\u0060, then the name of the subdirectory is \u0060values\u0060. Create a subdirectory inside the main application directory called *values* Inside the *values* subdirectory, create a file called *function.json* and add the following content to it. \u0060\u0060\u0060json { \u0022bindings\u0022: [ { \u0022type\u0022: \u0022httpTrigger\u0022, \u0022direction\u0022: \u0022in\u0022, \u0022name\u0022: \u0022req\u0022, \u0022methods\u0022: [\u0022get\u0022] }, { \u0022type\u0022: \u0022http\u0022, \u0022direction\u0022: \u0022out\u0022, \u0022name\u0022: \u0022res\u0022 } ] } \u0060\u0060\u0060 The *function.json* file defines the request and response payloads. In this case, the incoming request is an \u0060HttpTrigger\u0060 that only handles \u0060GET\u0060 requests and returns an HTTP response. ## Create server executable Package your server application into a single executable by entering the following command into the command prompt: \u0060\u0060\u0060bash raco exe server.rkt \u0060\u0060\u0060 Once your application is packaged, an executable with the name *server.exe* should be created in your application directory. ## Configure Azure Functions host In your application directory, create a file called *host.json* and add the following contents: \u0060\u0060\u0060json { \u0022version\u0022: \u00222.0\u0022, \u0022httpWorker\u0022: { \u0022description\u0022: { \u0022defaultExecutablePath\u0022: \u0022server.exe\u0022 } } } \u0060\u0060\u0060 This *host.json* configuration file tells the Azure Functions host where to find the web server executable. ## Run the Azure Functions application Inside the root application directory, enter the following command into the command prompt. \u0060\u0060\u0060bash func start \u0060\u0060\u0060 Using an application like Postman or Insomnia, make a \u0060GET\u0060 request to \u0060localhost:7071/api/values\u0060. The response should look like the following: \u0060\u0060\u0060json { \u0022value\u0022: [ 1, 2, 3 ] } \u0060\u0060\u0060 ## Conclusion In this writeup, I showed how to create a Racket serverless application that runs on Azure Functions by using custom handlers. Doing so requires you to: 1. Write a web server to process requests 2. Define the bindings for the request and response function payloads 3. Configure the Azure Functions host to send request to the web server Although in this example, the server was written in Racket, the same process is applicable to other languages. Keep in mind that at the time of this writing, custom handlers are preview and may change. Happy coding! ## Resources - [Azure Functions Custom Handlers](https://docs.microsoft.com/azure/azure-functions/functions-custom-handlers)","contentType":"posts","url":"https://www.lqdev.me/posts/serverless-racket-azure-functions-custom-handlers","date":"2020-03-21 13:45:43 -05:00","tags":["serverless","racket","lisp","azure-functions","azure","programming","web"],"summary":"## Introduction [Racket](https://racket-lang.org/) is a fun and powerful general purpose programming language based on the Scheme dialect of Lisp that provides tools and packages that allow...","keywords":["server","functions","azure","define","racket","create","request","application","function","called"]},{"id":"posts-deploy-machine-learning-mlnet-models-blazor-webassembly","title":"Deploy ML.NET Machine Learning Model in Blazor WebAssembly Static Website","content":"## Introduction There are many ways of deploying machine learning models. The most common way to do so is by exposing models as a web service through APIs or serverless functions. One of the considerations when deploying a model as a web service is latency and performance. The process of making predictions over HTTP using a model involves accepting the user input, loading the serialized version of the model from a file, using the model to make a prediction, and returning the prediction back to the user. Since models are typically just static files, another way of deploying a model is as a static asset over the web, just like any other HTML, CSS, or JavaScript file. This deployment method is similar to that of TensorFlow.js. Deploying in this way has several advantages. One advantage is that there is no longer a web service just to serve the model, making it more cost-efficient. Another advantage is that once the model has been downloaded onto the user\u0027s PC, the resources used at that point are those of the user\u0027s PC rather than the server the model would otherwise be hosted in. Finally, because the model is a static file, it can be distributed via CDNs. One of the challenges with this is that machine learning models are usually built using languages other than JavaScript. This makes using the same code / library the model was built difficult or nearly impossible. WebAssembly is changing that by allowing Rust, C\u002B\u002B, C# and other languages to run natively inside the browser. Having that ability, the code / logic to load the model and make predictions is much easier and almost comparable to that of a native platform. Blazor WebAssembly provides users the ability to create modern component-based web applications completely in C#. Additionally, Blazor WebAssembly allows users to publish and deploy their applications as static websites in an easy and cost-efficient way. ML.NET is an open-source, cross-platform framework that allows developers to create machine learning models using .NET. In this post, I\u0027ll show how to train a multiclass classification machine learning model that predicts iris flower species. Then, I\u0027ll take that model and deploy it alongside a Blazor WebAssembly static website to Azure Storage. The full code for this application may be found at the [MLNETBlazorWASMSample repository on GitHub](https://github.com/lqdev/MLNETBlazorWASMSample). ## Prerequisites This project was built on a Windows PC but should work cross platform on Mac and Linux. - [.NET Core SDK 3.1](https://dotnet.microsoft.com/download/dotnet-core/3.1) - [Blazor WebAssembly Template](https://www.nuget.org/packages/Microsoft.AspNetCore.Blazor.Templates/3.2.0-preview1.20073.1) - [Azure Subscription](http://aka.ms/amlFree) - [Azure Storage Explorer](https://azure.microsoft.com/en-us/features/storage-explorer/) ## Set up the solution The solution built in this post contains three projects: - SchemaLibrary: C# .NET Standard 2.0 class library that contains the schema definition classes of the data used to train the model as well as the prediction output generated by the model. - TrainingConsole: C# .NET Core 3.1 console application used to train the machine learning model. - BlazorWebApp: Blazor WebAssembly web application to make predictions using machine learning model trained by TrainingConsole application. ### Install Blazor WebAssembly Template Use the .NET CLI to run the following command in the command prompt: \u0060\u0060\u0060powershell dotnet new -i Microsoft.AspNetCore.Blazor.Templates::3.2.0-preview1.20073.1 \u0060\u0060\u0060 ### Create the solution Create a new directory for the solution called *MLNETBlazorWASMSample*. \u0060\u0060\u0060powershell mkdir MLNETBlazorWASMSample \u0060\u0060\u0060 Navigate to the newly created solution directory and create a solution: \u0060\u0060\u0060powershell cd MLNETBlazorWASMSample dotnet new sln \u0060\u0060\u0060 ### Create schema class library The data schema for the model input and output are shared during training as well as when making predictions. To share resources, create a class library that\u0027s shared by the \u0060ConsoleTraining\u0060 and \u0060BlazorWebApp\u0060 projects. In the solution directory, enter the following command: \u0060\u0060\u0060powershell dotnet new classlib -o SchemaLibrary \u0060\u0060\u0060 Install the *Microsoft.ML* NuGet package (This solution is built with version 1.4.0). The *Microsoft.ML* package is used throughout the entire solution. \u0060\u0060\u0060powershell dotnet add SchemaLibrary package Microsoft.ML \u0060\u0060\u0060 Add the library project to the solution. \u0060\u0060\u0060powershell dotnet sln add SchemaLibrary \u0060\u0060\u0060 ### Create the training console application The console application contains the series of data transformations and algorithms used to train the model. In the solution directory, create a new console application. \u0060\u0060\u0060powershell dotnet new console -o TrainingConsole \u0060\u0060\u0060 Add the console application to the solution. \u0060\u0060\u0060powershell dotnet sln add TrainingConsole \u0060\u0060\u0060 Reference the *SchemaLibrary* project. \u0060\u0060\u0060powershell dotnet add TrainingConsole reference SchemaLibrary \u0060\u0060\u0060 ### Create the Blazor WebAssembly web application The web application contains a few input elements so users can provide new data that the model then uses to make predictions. In the solution directory, create a new Blazor WebAssembly application. \u0060\u0060\u0060powershell dotnet new blazorwasm -o BlazorWebApp \u0060\u0060\u0060 Add the web application project to the solution. \u0060\u0060\u0060powershell dotnet sln add BlazorWebApp \u0060\u0060\u0060 Reference the *SchemaLibrary* project. \u0060\u0060\u0060powershell dotnet add BlazorWebApp reference SchemaLibrary \u0060\u0060\u0060 ## Define the schema ### Understand the data The data used to train the model comes from the [iris dataset](https://archive.ics.uci.edu/ml/datasets/Iris). It contains four numerical columns which are sepal and petal measurements and one numerical column for the species of iris flower. This is a sample of the data. |Sepal length (cm) |Sepal width (cm) |Petal length (cm) |Petal width (cm) |Class (iris species) | |---------|---------|---------|---------|---------| |5.1|3.5|1.4|0.2|Iris-setosa| |7.0|3.2|4.7|1.4|Iris-versicolor| |6.3|3.3|6.0|2.5|Iris-virginica| ### Define model input schema In the *SchemaLibrary* project, create a class called \u0060ModelInput\u0060 to model the data used for training and as model input. \u0060\u0060\u0060powershell ni ModelInput.cs \u0060\u0060\u0060 The \u0060ModelInput\u0060 class should look like the following: \u0060\u0060\u0060csharp using Microsoft.ML.Data; namespace SchemaLibrary { public class ModelInput { [LoadColumn(0)] public float SepalLength { get; set; } [LoadColumn(1)] public float SepalWidth { get; set; } [LoadColumn(2)] public float PetalLength { get; set; } [LoadColumn(3)] public float PetalWidth { get; set; } [LoadColumn(4)] public string Label { get; set; } } } \u0060\u0060\u0060 Notice that the \u0060Class\u0060 column is now a property called \u0060Label\u0060. This is for two reasons: 1. Avoid using the \u0060class\u0060 keyword. 2. In ML.NET, the default column name of the column to predict expected by algorithms is \u0060Label\u0060. Also notice the \u0060LoadColumn\u0060 attributes at the top of each property. This is used to tell the loader the index of the column where the data for the respective property is. ### Define model output schema Similar to the input schema, there\u0027s a schema for the output of the model. The type of model used in this solution is a multiclass classification model since there are more than two categories to choose from for iris flower species. Multiclass classification models output a column called \u0060PredictedLabel\u0060 which contains the name of the predicted category. In the *SchemaLibrary* project, create a class called \u0060ModelOutput\u0060 to model the predictions made by the model. \u0060\u0060\u0060powershell ni ModelOutput.cs \u0060\u0060\u0060 The \u0060ModelOutput\u0060 class should look like the following: \u0060\u0060\u0060csharp namespace SchemaLibrary { public class ModelOutput { public string PredictedLabel { get; set; } } } \u0060\u0060\u0060 ## Train the model Now it\u0027s time to create the application that trains the model. ### Get the data Download the data and save it inside the *TrainingConsole* project directory. \u0060\u0060\u0060powershell curl https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data -o iris.data \u0060\u0060\u0060 ### Define the data preparation and training steps In the *TrainingConsole* project, open the *Program.cs* file and add the following using statements at the top: \u0060\u0060\u0060csharp using System; using System.Linq; using Microsoft.ML; using SchemaLibrary; \u0060\u0060\u0060 Then, delete the contents inside the \u0060Main\u0060 method and replace it with the following. \u0060\u0060\u0060csharp // 1. Initialize MLContext MLContext mlContext = new MLContext(); // 2. Load the data IDataView data = mlContext.Data.LoadFromTextFile (\u0022iris.data\u0022, separatorChar:\u0027,\u0027); // 3. Shuffle the data IDataView shuffledData = mlContext.Data.ShuffleRows(data); // 3. Define the data preparation and training pipeline. IEstimator pipeline = mlContext.Transforms.Concatenate(\u0022Features\u0022,\u0022SepalLength\u0022,\u0022SepalWidth\u0022,\u0022PetalLength\u0022,\u0022PetalWidth\u0022) .Append(mlContext.Transforms.NormalizeMinMax(\u0022Features\u0022)) .Append(mlContext.Transforms.Conversion.MapValueToKey(\u0022Label\u0022)) .Append(mlContext.MulticlassClassification.Trainers.NaiveBayes()) .Append(mlContext.Transforms.Conversion.MapKeyToValue(\u0022PredictedLabel\u0022)); // 4. Train with cross-validation var cvResults = mlContext.MulticlassClassification.CrossValidate(shuffledData, pipeline); // 5. Get the highest performing model and its accuracy (ITransformer, double) model = cvResults .OrderByDescending(fold =\u003E fold.Metrics.MacroAccuracy) .Select(fold =\u003E (fold.Model, fold.Metrics.MacroAccuracy)) .First(); Console.WriteLine($\u0022Top performing model\u0027s macro-accuracy: {model.Item2}\u0022); // 6. Save the model mlContext.Model.Save(model.Item1, data.Schema, \u0022model.zip\u0022); Console.WriteLine(\u0022Model trained\u0022); \u0060\u0060\u0060 The training application loads the data from the \u0060iris.data\u0060 file and applies a series of transformations. First, all of the individual numerical columns are combined into a single vector and stored in a new column called \u0060Features\u0060. The \u0060Features\u0060 column is then normalized and the \u0060MapValueToKey\u0060 transform is used to convert the text in the \u0060Label\u0060 column to a number. Then, the transformed data is used to train a model using the \u0060NaiveBayes\u0060 algorithm. **Note that at the time of this writing, for multiclass classification problems, only Naive Bayes has been confirmed to work with Blazor WebAssembly**. Finally, the \u0060PredictedLabel\u0060 is stored as a number so it has to be converted back into text. Using the \u0060Fit\u0060 method, the data is applied to the pipeline. Because the dataset is small, a technique called cross-validation is used to build a more robust model. Once the model is trained, the model with the top performance is then serialized and saved to a file called *model.zip* for later use in the web application. The final *Program.cs* file should look like the content below: \u0060\u0060\u0060csharp using System; using System.Linq; using Microsoft.ML; using SchemaLibrary; namespace TrainingConsole { class Program { static void Main(string[] args) { // 1. Initialize MLContext MLContext mlContext = new MLContext(); // 2. Load the data IDataView data = mlContext.Data.LoadFromTextFile (\u0022iris.data\u0022, separatorChar:\u0027,\u0027); // 3. Shuffle the data IDataView shuffledData = mlContext.Data.ShuffleRows(data); // 3. Define the data preparation and training pipeline. IEstimator pipeline = mlContext.Transforms.Concatenate(\u0022Features\u0022,\u0022SepalLength\u0022,\u0022SepalWidth\u0022,\u0022PetalLength\u0022,\u0022PetalWidth\u0022) .Append(mlContext.Transforms.NormalizeMinMax(\u0022Features\u0022)) .Append(mlContext.Transforms.Conversion.MapValueToKey(\u0022Label\u0022)) .Append(mlContext.MulticlassClassification.Trainers.NaiveBayes()) .Append(mlContext.Transforms.Conversion.MapKeyToValue(\u0022PredictedLabel\u0022)); // 4. Train with cross-validation var cvResults = mlContext.MulticlassClassification.CrossValidate(shuffledData, pipeline); // 5. Get the highest performing model and its accuracy (ITransformer, double) model = cvResults .OrderByDescending(fold =\u003E fold.Metrics.MacroAccuracy) .Select(fold =\u003E (fold.Model, fold.Metrics.MacroAccuracy)) .First(); Console.WriteLine($\u0022Top performing model\u0027s macro-accuracy: {model.Item2}\u0022); // 6. Save the model mlContext.Model.Save(model.Item1, data.Schema, \u0022model.zip\u0022); Console.WriteLine(\u0022Model trained\u0022); } } } \u0060\u0060\u0060 ### Run the application In the *TrainConsole* project directory, use the following command to run the application and train the model: \u0060\u0060\u0060powershell dotnet run \u0060\u0060\u0060 ## Host the model Once you have the model saved, use the Azure Portal to create an Azure Storage account. ![Create Azure Storage Account](http://cdn.lqdev.tech/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly1.png) Then, navigate to your newly created storage account resource and create a blob container called \u0060models\u0060. ![Create Blob Container](http://cdn.lqdev.tech/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly2.png) Once your container is created, navigate to it and upload the *model.zip* file. ![Upload Model](http://cdn.lqdev.tech/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly3.png) ## Create prediction web page To make predictions, create a web page to take in user input. Then, provide the user input to the model and display the prediction to the user. ### Set up imports In the *BlazorWebApp* project directory, open the *_Imports.razor* file. This contains the using statements for the pages and components in your application. Add the following using statements: \u0060\u0060\u0060razor @using System.IO @using Microsoft.ML @using SchemaLibrary \u0060\u0060\u0060 ### Create user input page In the *BlazorWebApp* project, create a new razor page called *Prediction.razor* inside the *Pages* directory. \u0060\u0060\u0060powershell ni Prediction.razor \u0060\u0060\u0060 Add the following content to it: \u0060\u0060\u0060razor @page \u0022/prediction\u0022 @inject HttpClient _client Sepal Length: Sepal Width: Petal Length: Petal Width: Make prediction @if(@ModelPrediction == null) { Enter data to get a prediction } else { @ModelPrediction } @code { private PredictionEngine _predictionEngine; private string _sepalLength, _sepalWidth, _petalLength, _petalWidth, ModelPrediction; protected override async Task OnInitializedAsync() { Stream savedModel = await _client.GetStreamAsync(\u0022 \u0022); MLContext mlContext = new MLContext(); ITransformer _model = mlContext.Model.Load(savedModel,out DataViewSchema schema); _predictionEngine = mlContext.Model.CreatePredictionEngine (_model); } private void GetPrediction() { ModelInput input = new ModelInput { SepalLength=float.Parse(_sepalLength), SepalWidth=float.Parse(_sepalWidth), PetalLength=float.Parse(_petalLength), PetalWidth=float.Parse(_petalWidth) }; ModelOutput prediction = _predictionEngine.Predict(input); ModelPrediction = prediction.PredictedLabel; } } \u0060\u0060\u0060 The *Predict.razor* page contains a text input element for each of the columns the model was original trained on. When the page is initialized, the model is loaded from Azure Storage and a \u0060PredictionEngine\u0060 is created. **Make sure to replace \u0060 \u0060 with the URL of the blob that contains your \u0060model.zip\u0060**. A \u0060PredictionEngine\u0060 is a convenience API to make a single prediction. Traditionally when models are served as a web service, it is recommended to use the \u0060PredictionEnginePool\u0060 service because it\u0027s thread-safe and more performant in multi-threaded application. However, in this case since the model is downloaded onto the individual user\u0027s browser, it\u0027s okay to use \u0060PredictionEngine\u0060. After a user enters input values and clicks the \u0022Make Prediction\u0022 button, the \u0060GetPrediction\u0060 method executes by taking the user input and making a prediction using the \u0060PredictionEngine\u0060. The prediction is then displayed in the browser. ### Add to navigation menu In the *BlazorWebApp* project, open the *NavMenu.razor* file in the *Shared* directory. Add the following list item to the \u0060 \u0060 element. \u0060\u0060\u0060html Prediction \u0060\u0060\u0060 The final *NavMenu.razor* page should look like the following: \u0060\u0060\u0060razor BlazorWebApp Home Counter Fetch data Prediction @code { private bool collapseNavMenu = true; private string NavMenuCssClass =\u003E collapseNavMenu ? \u0022collapse\u0022 : null; private void ToggleNavMenu() { collapseNavMenu = !collapseNavMenu; } } \u0060\u0060\u0060 ## Configure the web application The web application will he hosted as a static site on Azure Storage. In the Azure Portal, navigate to the storage account resource where you are hosting your model. ### Enable static website Enable a static website for the storage account and set the index document name and error document path to *index.html*. ![Enable Static Website](http://cdn.lqdev.tech/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly4.png) At this point, a new container called *$web* is created in your storage account. This is where all your site\u0027s static files will reside. Additionally, a primary endpoint is created. This is the URL you will use to access your application ### Configure CORS The storage account has some default CORS settings. In order to download and use your model from your application, you\u0027ll have to configure them. ![Configure CORS](http://cdn.lqdev.tech/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly5.png) For \u0022Allowed origins\u0022, enter your primary endpoint. ## Publish and deploy the web application To publish your application, run the following command: \u0060\u0060\u0060powershell dotnet publish -c Release \u0060\u0060\u0060 This generates all the files you\u0027ll need to host your web application as a static site in the *bin/Release/netstandard2.1/publish/BlazorWebApp/dist* directory of your *BlazorWebApp* project. To deploy your application, use Azure Storage Explorer to copy all of the files in the *dist* directory into the *$web* container of your Azure Storage Account. ![Copy files into web container](http://cdn.lqdev.tech/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly6.png) ## Test the application In your browser, navigate to your static website\u0027s primary endpoint and select the Prediction page. Input data and click \u0022Make prediction\u0022. The page should look as follows. ![Test the application](http://cdn.lqdev.tech/files/images/deploy-machine-learning-mlnet-model-blazor-webassembly7.png) **You may note that performance of Naive Bayes on this dataset is not the best so some of the predictions may not be as accurate. I am okay with this at the moment because this is a proof-of-concept to show how these technologies might work together. Perhaps using a better dataset may yield better results.** ## Conclusion In this post, I went over how to deploy an ML.NET multiclass classification model alongside a Blazor WebAssembly static website to Azure Storage. While more limited than other methods of deployment due to the early stages of WebAssembly and Blazor WebAssembly, this shows the possibilities for the technologies. Deploying in this manner reduces the amount of resources required to deploy these models and shifts processing from a server or web service to the client\u0027s browser making deployment and distribution of machine learning models more efficient, scalable and cost-efficient.","contentType":"posts","url":"https://www.lqdev.me/posts/deploy-machine-learning-mlnet-models-blazor-webassembly","date":"2020-03-01 19:21:04 -05:00","tags":["azure","staticwebsites","machinelearning","ai","artifificalintelligence","ml","dotnet","dotnetcore","webassembly","webdevelopment","mldotnet","mlnet","blazor"],"summary":"## Introduction There are many ways of deploying machine learning models. The most common way to do so is by exposing models as a web service through APIs or serverless functions. One of the...","keywords":["model","data","mlcontext","application","using","create","blazor","\u0060\u0060\u0060powershell","prediction","learning"]},{"id":"posts-categorize-web-links-ml-net-fsharp-fsadvent2019","title":"Use machine learning to categorize web links with F# and ML.NET","content":"## Introduction This post is part of [F# Advent 2019](https://sergeytihon.com/2019/11/05/f-advent-calendar-in-english-2019/). Thank you [Sergey Tihon](https://twitter.com/sergey_tihon) for organizing this and the rest of the contributors for producing interesting, high-quality content. I scan and read articles on a constant basis, such as those published as part of F# Advent. Those that I find interesting, or I want to save for later, I bookmark using [Pocket](https://getpocket.com/). One of the neat features it provides is tagging. You can add as many tags as you want to organize the bookmarked links. When I first started using the service, I was fairly good at adding tags. However, I\u0027ve gotten lazy and don\u0027t do it as much. It would be nice if bookmarked links could automatically be categorized for me without having to provide the tags manually. Using machine learning, this task can be automated. In this writeup, I will show how to build a machine learning model using [ML.NET](https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet), a .NET, open-source, cross-platform machine learning framework to automatically categorize web links / articles. ## Prerequisites This application was built on a Windows 10 PC, but should work cross-platform. - [.NET Core SDK 2.1\u002B](https://dotnet.microsoft.com/download) - [Visual Studio Code](https://code.visualstudio.com/Download) - [Ionide](http://ionide.io/) - [Microsoft.ML NuGet package](https://www.nuget.org/packages/Microsoft.ML/) ## Create the solution Make a new directory and create a solution by using the .NET CLI. \u0060\u0060\u0060powershell mkdir FsAdvent2019 cd FsAdvent2019 dotnet new sln \u0060\u0060\u0060 Then, create an F# Console application. \u0060\u0060\u0060powershell dotnet new console -o FsAdvent2019 -lang f# \u0060\u0060\u0060 Navigate to the console application directory and install the Microsoft.ML NuGet package. \u0060\u0060\u0060powershell cd FsAdvent2019 dotnet add package Microsoft.ML -v 1.4.0 \u0060\u0060\u0060 ## Get the data [Click on this link to download and unzip the data anywhere on your PC](https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip). The data contains information about several articles that are separated into four categories: business (b), science and technology (t), entertainment (e) and health (h). Visit the [UCI Machine Learning repository website](https://archive.ics.uci.edu/ml/datasets/News\u002BAggregator) to learn more about the dataset. Below is a sample of the data. \u0060\u0060\u0060text ID Title Url Publisher Category Story Hostname Timestamp 2 Fed\u0027s Charles Plosser sees high bar for change in pace of tapering http://www.livemint.com/Politics/H2EvwJSK2VE6OF7iK1g3PP/Feds-Charles-Plosser-sees-high-bar-for-change-in-pace-of-ta.html Livemint b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.livemint.com 1394470371207 3 US open: Stocks fall after Fed official hints at accelerated tapering http://www.ifamagazine.com/news/us-open-stocks-fall-after-fed-official-hints-at-accelerated-tapering-294436 IFA Magazine b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.ifamagazine.com 1394470371550 4 Fed risks falling \u0027behind the curve\u0027, Charles Plosser says http://www.ifamagazine.com/news/fed-risks-falling-behind-the-curve-charles-plosser-says-294430 IFA Magazine b ddUyU0VZz0BRneMioxUPQVP6sIxvM www.ifamagazine.com 1394470371793 \u0060\u0060\u0060 Inside the console application directory, create a new directory called *data* and copy the *newsCorpora.csv* file to it. \u0060\u0060\u0060powershell mkdir data \u0060\u0060\u0060 ## Define the schema Open the *Program.fs* file and add the following \u0060open\u0060 statements at the top. \u0060\u0060\u0060fsharp open Microsoft.ML open Microsoft.ML.Data \u0060\u0060\u0060 Directly below the \u0060open\u0060 statements, define the data schema of the input and output of the machine learning model as records called \u0060ModelInput\u0060 and \u0060ModelOutput\u0060 respectively. \u0060\u0060\u0060fsharp [ ] type ModelInput = { [ ] Title:string [ ] Url:string [ ] Publisher:string [ ] Category:string [ ] Hostname:string } [ ] type ModelOutput = { PredictedLabel: string } \u0060\u0060\u0060 As input, only the **Title**, **Url**, **Publisher** and **Hostname** columns are used to train the machine learning model and make predictions. The label or value to predict in this case is the **Category**. When a prediction is output by the model, its value is stored in a column called **PredictedLabel**. ## Create the application entry point The \u0060MLContext\u0060 is the entry point of all ML.NET applications which binds all tasks like data loading, data transformations, model training, model evaluation, and model saving/loading. Inside of the \u0060main\u0060 function, create an instance of \u0060MLContext\u0060. \u0060\u0060\u0060fsharp let mlContext = MLContext() \u0060\u0060\u0060 ## Load the data Once the \u0060MLContext\u0060 is initialized, use the \u0060LoadFromTextFile\u0060 function and provide the path to the file containing the data. \u0060\u0060\u0060fsharp let data = mlContext.Data.LoadFromTextFile (\u0022data/newsCorpora.csv\u0022) \u0060\u0060\u0060 ## Create training and test datasets It\u0027s often good practice to split the data into train and test sets. The goal of a machine learning model is to accurately make predictions on data it has not seen before. Therefore, making predictions using inputs that are the same as those it was trained on may provide misleading accuracy metrics. Use the \u0060TrainTestSplit\u0060 to split the data into train / test sets with 90% of the data used for training and 10% used for testing. \u0060\u0060\u0060fsharp let datasets = mlContext.Data.TrainTestSplit(data,testFraction=0.1) \u0060\u0060\u0060 ## Define the transformation and algorithm pipelines Now that the data is split, define the set of transformations to be applied to the data. The purpose of transforming the data is to convert it into numbers which are more easily processed by machine learning algorithms. ### Preprocessing pipeline The preprocessing pipeline contains the series of transformations that take place before training the model. To create a pipeline, initialize an \u0060EstimatorChain\u0060 and append the desired transformations to it. \u0060\u0060\u0060fsharp let preProcessingPipeline = EstimatorChain() .Append(mlContext.Transforms.Text.FeaturizeText(\u0022FeaturizedTitle\u0022,\u0022Title\u0022)) .Append(mlContext.Transforms.Text.FeaturizeText(\u0022FeaturizedUrl\u0022,\u0022Url\u0022)) .Append(mlContext.Transforms.Text.FeaturizeText(\u0022FeaturizedPublisher\u0022,\u0022Publisher\u0022)) .Append(mlContext.Transforms.Text.FeaturizeText(\u0022FeaturizedHost\u0022,\u0022Hostname\u0022)) .Append(mlContext.Transforms.Concatenate(\u0022Features\u0022,[|\u0022FeaturizedTitle\u0022; \u0022FeaturizedUrl\u0022 ;\u0022FeaturizedPublisher\u0022; \u0022FeaturizedHost\u0022|])) .Append(mlContext.Transforms.Conversion.MapValueToKey(\u0022Label\u0022,\u0022Category\u0022)) \u0060\u0060\u0060 In this preprocessing pipeline, the following transformations are taking place: 1. Convert the *Title*, *Url*, *Publisher* and *Hostname* columns into numbers and store the transformed value into the *FeaturizedTitle*, *FeaturizedUrl*, *FeaturizedPublisher* and *FeaturizedHost* columns respectively. 2. Combine the *FeaturizedTitle*, *FeaturizedUrl*, *FeaturizedPublisher* and *FeaturizedHost* into one column called *Features*. 3. Create a mapping of the text value contained in the *Category* column to a numerical key and store the result into a new column called *Label*. ### Algorithm pipeline The algorithm pipeline contains the algorithm used to train the machine learning model. In this application, the multiclass classification algorithm used is \u0060LbfgsMaximumEntropy\u0060. To learn more about the algorithm, see the [ML.NET LbfgsMaximumEntropy multiclass trainer API documentation](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.trainers.lbfgsmaximumentropymulticlasstrainer?view=ml-dotnet#training-algorithm-details). \u0060\u0060\u0060fsharp let algorithm = mlContext.MulticlassClassification.Trainers.LbfgsMaximumEntropy() \u0060\u0060\u0060 ### Postprocessing pipeline The postprocessing pipeline contains the series of transformations to get the output of training into a more readable format. The only transformation performed in this pipeline is mapping back the numerical value mapping of the predicted value into text form. \u0060\u0060\u0060fsharp let postProcessingPipeline = mlContext.Transforms.Conversion.MapKeyToValue(\u0022PredictedLabel\u0022) \u0060\u0060\u0060 ### Create training pipeline Once the pipelines are defined, combine them into a single pipeline which applies all of the transformations to the data with a single function call. \u0060\u0060\u0060fsharp let trainingPipeline = preProcessingPipeline .Append(algorithm) .Append(postProcessingPipeline) \u0060\u0060\u0060 ## Train the model Use the \u0060Fit\u0060 function to train the model by applying the set of transformations defined by \u0060trainingPipeline\u0060 to the training dataset. \u0060\u0060\u0060fsharp let model = datasets.TrainSet |\u003E trainingPipeline.Fit \u0060\u0060\u0060 ## Evaluate the model Once the model is trained, evaluate how well it performs against the test dataset. First, use the trained model to get the predicted category by using the \u0060Transform\u0060 function. Then, provide the test dataset containing predictions to the \u0060Evaluate\u0060 function which calculates the model\u0027s performance metrics by comparing the predicted category to the actual category and print some of them out. \u0060\u0060\u0060fsharp let metrics = (datasets.TestSet |\u003E model.Transform) |\u003E mlContext.MulticlassClassification.Evaluate printfn \u0022Log Loss: %f | MacroAccuracy: %f\u0022 metrics.LogLoss metrics.MacroAccuracy \u0060\u0060\u0060 ## Using the model on real data Create a list of \u0060ModelInput\u0060 items and use the \u0060Transform\u0060 method to get the predicted category. \u0060\u0060\u0060fsharp let predictions = [ { Title=\u0022A FIRST LOOK AT SURFACE DUO, MICROSOFT\u2019S FOLDABLE ANDROID PHONE\u0022 Url=\u0022https://www.theverge.com/2019/10/3/20895268/microsoft-surface-duo-foldable-phone-dual-screen-android-hands-on-features-price-photos-video\u0022 Publisher=\u0022The Verge\u0022 Hostname=\u0022www.theverge.com\u0022 Category = \u0022\u0022 } { Title=\u0022This Shrinking Economy With Low Inflation Is Stuck on Rates\u0022 Url=\u0022https://www.bloomberg.com/news/articles/2019-12-12/when-a-shrinking-economy-and-low-inflation-don-t-mean-rate-cuts?srnd=economics-vp\u0022 Publisher=\u0022Bloomberg\u0022 Hostname=\u0022www.bloomberg.com\u0022 Category = \u0022\u0022 } ] |\u003E mlContext.Data.LoadFromEnumerable |\u003E model.Transform \u0060\u0060\u0060 Then, create a \u0060Sequence\u0060 of \u0060ModelOutput\u0060 values and print out the *PredictedLabel* values. \u0060\u0060\u0060fsharp mlContext.Data.CreateEnumerable (predictions,false) |\u003E Seq.iter(fun prediction -\u003E printfn \u0022Predicted Value: %s\u0022 prediction.PredictedLabel) \u0060\u0060\u0060 The final *Program.fs* file should look as follows: \u0060\u0060\u0060fsharp open System open Microsoft.ML open Microsoft.ML.Data [ ] type ModelInput = { [ ] Title:string [ ] Url:string [ ] Publisher:string [ ] Category:string [ ] Hostname:string } [ ] type ModelOutput = { PredictedLabel: string } [ ] let main argv = let mlContext = MLContext() let data = mlContext.Data.LoadFromTextFile (\u0022data/newsCorpora.csv\u0022) let datasets = mlContext.Data.TrainTestSplit(data,testFraction=0.1) let preProcessingPipeline = EstimatorChain() .Append(mlContext.Transforms.Text.FeaturizeText(\u0022FeaturizedTitle\u0022,\u0022Title\u0022)) .Append(mlContext.Transforms.Text.FeaturizeText(\u0022FeaturizedUrl\u0022,\u0022Url\u0022)) .Append(mlContext.Transforms.Text.FeaturizeText(\u0022FeaturizedPublisher\u0022,\u0022Publisher\u0022)) .Append(mlContext.Transforms.Text.FeaturizeText(\u0022FeaturizedHost\u0022,\u0022Hostname\u0022)) .Append(mlContext.Transforms.Concatenate(\u0022Features\u0022,[|\u0022FeaturizedTitle\u0022; \u0022FeaturizedUrl\u0022 ;\u0022FeaturizedPublisher\u0022; \u0022FeaturizedHost\u0022|])) .Append(mlContext.Transforms.Conversion.MapValueToKey(\u0022Label\u0022,\u0022Category\u0022)) let algorithm = mlContext.MulticlassClassification.Trainers.LbfgsMaximumEntropy() let postProcessingPipeline = mlContext.Transforms.Conversion.MapKeyToValue(\u0022PredictedLabel\u0022) let trainingPipeline = preProcessingPipeline .Append(algorithm) .Append(postProcessingPipeline) let model = datasets.TrainSet |\u003E trainingPipeline.Fit let metrics = (datasets.TestSet |\u003E model.Transform) |\u003E mlContext.MulticlassClassification.Evaluate printfn \u0022Log Loss: %f | MacroAccuracy: %f\u0022 metrics.LogLoss metrics.MacroAccuracy let predictions = [ { Title=\u0022A FIRST LOOK AT SURFACE DUO, MICROSOFT\u2019S FOLDABLE ANDROID PHONE\u0022 Url=\u0022https://www.theverge.com/2019/10/3/20895268/microsoft-surface-duo-foldable-phone-dual-screen-android-hands-on-features-price-photos-video\u0022 Publisher=\u0022The Verge\u0022 Hostname=\u0022www.theverge.com\u0022 Category = \u0022\u0022 } { Title=\u0022This Shrinking Economy With Low Inflation Is Stuck on Rates\u0022 Url=\u0022https://www.bloomberg.com/news/articles/2019-12-12/when-a-shrinking-economy-and-low-inflation-don-t-mean-rate-cuts?srnd=economics-vp\u0022 Publisher=\u0022Bloomberg\u0022 Hostname=\u0022www.bloomberg.com\u0022 Category = \u0022\u0022 } ] |\u003E mlContext.Data.LoadFromEnumerable |\u003E model.Transform mlContext.Data.CreateEnumerable (predictions,false) |\u003E Seq.iter(fun prediction -\u003E printfn \u0022Predicted Value: %s\u0022 prediction.PredictedLabel) 0 // return an integer exit code \u0060\u0060\u0060 ## Run the application This particular model achieved a macro-accuracy of 0.92, where closer to 1 is preferred and log loss of 0.20 where closer to 0 is preferred. \u0060\u0060\u0060bash Log Loss: 0.200502 | MacroAccuracy: 0.927742 \u0060\u0060\u0060 The predicted values are the following: \u0060\u0060\u0060bash Predicted Value: t Predicted Value: b \u0060\u0060\u0060 Upon inspection, they appear to be correct, science and technology for the first link and business for the second link. ## Conclusion In this writeup, I showed how to build a machine learning multiclass classification model that categorizes web links using ML.NET. Now that you have a model trained, you can save it and deploy it in another application (desktop, web) that bookmarks links. This model can be further improved and personalized by using data from Pocket which has already been tagged. Happy coding!","contentType":"posts","url":"https://www.lqdev.me/posts/categorize-web-links-ml-net-fsharp-fsadvent2019","date":"2019-12-17 19:59:06 -05:00","tags":["dotnet","dotnet-core","machine-learning","ai","artificial-intelligence","ml","fsharp","functional-programming"],"summary":"## Introduction This post is part of [F# Advent 2019](https://sergeytihon.com/2019/11/05/f-advent-calendar-in-english-2019/). Thank you [Sergey Tihon](https://twitter.com/sergey_tihon) for organizing...","keywords":["data","mlcontext","model","append","transforms","\u0060\u0060\u0060fsharp","create","pipeline","algorithm","learning"]},{"id":"posts-http-trigger-azure-functions-fsharp","title":"Create an HTTP Trigger Azure Function using FSharp","content":"## Introduction Since the upgrade to the 2.0 version of the Azure Functions runtime, .NET Core has been natively supported by the platform. As a result some changes took effect. Most notably, in version 1.0, a template for an F# HttpTrigger function was available. The template was removed in 2.0. However, that does not mean Azure Functions does not support F#. Azure Functions can be built in F# using a .NET Standard Class Library. This writeup provides a detailed walk-through of how to build an Azure Function that processes HTTP requests using F#. The complete code sample can be found on [GitHub](https://github.com/lqdev/FsHttpTriggerSample). ## Prerequisites This solution was built using a Windows PC but should work on Mac and Linux. - [.NET SDK (2.x or 3.x)](https://dotnet.microsoft.com/download/dotnet-core) - [Node.js](https://nodejs.org/en/download/) - [Azure Functions Core Tools](https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local#windows-npm) ## Create Solution Open the command prompt and create a new directory for your solution called \u0022FsHttpTriggerSample\u0022. \u0060\u0060\u0060bash mkdir FsHttpTriggerSample \u0060\u0060\u0060 Navigate into the new directory and create a solution using the .NET CLI. \u0060\u0060\u0060bash cd FsHttpTriggerSample dotnet new sln \u0060\u0060\u0060 ## Create Azure Functions Project Inside the *FsHttpTriggerSample* solution directory, use the .NET CLI to create a new F# .NET Standard Class Library project. \u0060\u0060\u0060bash dotnet new classlib -o FsHttpTriggerSample -lang f# \u0060\u0060\u0060 Add the project to the solution \u0060\u0060\u0060bash dotnet sln add FsHttpTriggerSample \u0060\u0060\u0060 ## Install NuGet Packages To use Azure Functions, install the [**Microsoft.Net.Sdk.Functions** NuGet package](https://www.nuget.org/packages/Microsoft.NET.Sdk.Functions). Inside the *FsHttpTriggerSample* project directory, enter the following command. \u0060\u0060\u0060bash dotnet add package Microsoft.Net.Sdk.Functions \u0060\u0060\u0060 ## Create the Azure Function ### Prepare Files Delete the default *Library.fs* file inside the *FsHttpTriggerSample* project directory. \u0060\u0060\u0060bash del Library.fs \u0060\u0060\u0060 Create a new file called *GreetFunction.fs*. \u0060\u0060\u0060bash type nul \u003E GreetFunction.fs \u0060\u0060\u0060 ### Configure Files Open the *FsHttpTriggerSample.fsproj* file and find the following snippet. \u0060\u0060\u0060xml \u0060\u0060\u0060 Replace the snippet with the content below. \u0060\u0060\u0060xml \u0060\u0060\u0060 ### Configure Host At a minimum, Azure Functions requires the runtime version to run. This information is provided by a file called *host.json*. Create a new file called *host.json* inside the *FsHttpTriggerSample* project directory. \u0060\u0060\u0060bash type nul \u003E host.json \u0060\u0060\u0060 Open the *host.json* file and add the following content \u0060\u0060\u0060json { \u0022version\u0022: \u00222.0\u0022 } \u0060\u0060\u0060 ### Implement Azure Function Open the *GreetFunction.fs* file and add the namespace and module for it. \u0060\u0060\u0060fsharp namespace FsHttpTriggerSample module GreetFunction = \u0060\u0060\u0060 Below the module definition, add the following \u0060open\u0060 statements: \u0060\u0060\u0060fsharp open Microsoft.AspNetCore.Mvc open Microsoft.Azure.WebJobs open Microsoft.AspNetCore.Http open Newtonsoft.Json open System.IO open Microsoft.Extensions.Logging \u0060\u0060\u0060 Define a \u0060User\u0060 type containing a single property called \u0060Name\u0060. \u0060\u0060\u0060fsharp type User = { Name: string } \u0060\u0060\u0060 The entrypoint of an Azure Function is the \u0060Run\u0060 function. Create a function called \u0060Run\u0060. \u0060\u0060\u0060fsharp [ ] let Run ([ ] req:HttpRequest) (log:ILogger) = \u0060\u0060\u0060 To register an Azure Function, use the \u0060FunctionName\u0060 attribute. In this case, the name of the function is \u0060Greet\u0060. The \u0060Run\u0060 function takes two parameters, an \u0060HttpRequest\u0060 and an \u0060ILogger\u0060. Since the binding used by HTTP Trigger functions is \u0060HttpTrigger\u0060, the request object is annotated with the \u0060HttpTrigger\u0060 attribute. Options such as the accepted methods can be provided through the \u0060HttpTrigger\u0060 attribute. In this case, only \u0060POST\u0060 requests are accepted. Create an \u0060async\u0060 computation expression inside the \u0060Run\u0060 function. \u0060\u0060\u0060fsharp async { } \u0060\u0060\u0060 Inside the \u0060async\u0060 expression, add logging to indicate that the function has initialized. \u0060\u0060\u0060fsharp \u0022Running function\u0022 |\u003E log.LogInformation \u0060\u0060\u0060 Below that, get the body of the request. \u0060\u0060\u0060fsharp let! body = new StreamReader(req.Body) |\u003E (fun stream -\u003E stream.ReadToEndAsync()) |\u003E Async.AwaitTask \u0060\u0060\u0060 Then, deserialized the body into an instance of \u0060User\u0060. \u0060\u0060\u0060fsharp let user = JsonConvert.DeserializeObject (body) \u0060\u0060\u0060 Return a personalized greeting with the user\u0027s name. \u0060\u0060\u0060fsharp return OkObjectResult(sprintf \u0022Hello %s\u0022 user.Name) \u0060\u0060\u0060 Finally, use the \u0060StartAsTask\u0060 function to start the \u0060async\u0060 expression as a \u0060Task\u0060. \u0060\u0060\u0060fsharp |\u003E Async.StartAsTask \u0060\u0060\u0060 Once finished, the contents of the *GreetFunction.fs* should look similar to the following. \u0060\u0060\u0060fsharp namespace FsHttpTriggerSample module GreetFunction = open Microsoft.AspNetCore.Mvc open Microsoft.Azure.WebJobs open Microsoft.AspNetCore.Http open Newtonsoft.Json open System.IO open Microsoft.Extensions.Logging type User = { Name: string } [ ] let Run ([ ] req:HttpRequest) (log:ILogger) = async { \u0022Runnning Function\u0022 |\u003E log.LogInformation let! body = new StreamReader(req.Body) |\u003E (fun stream -\u003E stream.ReadToEndAsync()) |\u003E Async.AwaitTask let user = JsonConvert.DeserializeObject (body) return OkObjectResult(sprintf \u0022Hello %s\u0022 user.Name) } |\u003E Async.StartAsTask \u0060\u0060\u0060 ## Run the Function Locally Build the project by using the \u0060build\u0060 command inside the *FsHttpTriggerSample* project directory. \u0060\u0060\u0060bash dotnet build \u0060\u0060\u0060 Then, navigate to the output directory \u0060\u0060\u0060bash cd bin\\Debug\\netstandard2.0 \u0060\u0060\u0060 Use the Azure Functions Core Tools to start the Azure Functions host locally. \u0060\u0060\u0060bash func host start \u0060\u0060\u0060 Once the host is initialized, the function is available at the following endpoint \u0060http://localhost:7071/api/Greet\u0060. ## Test the function Using a REST client like Postman or Insomnia, make a POST request to \u0060http://localhost:7071/api/Greet\u0060 with the following body. Feel free to replace the name with your own. \u0060\u0060\u0060json { \u0022Name\u0022: \u0022Luis\u0022 } \u0060\u0060\u0060 If successful, the response should look similar to the following output. \u0060\u0060\u0060text Hello Luis \u0060\u0060\u0060 ## Conclusion This writeup showed how to create an HTTP Trigger Azure Function using F#. Creating additional functions inside the same project is relatively trivial since the structure of the *GreetFunction.fs* file can be copied and the logic inside the \u0060Run\u0060 function can be adapted to meet your requirements. ## Resources - [Azure Functions F# Developer Reference](https://docs.microsoft.com/en-us/azure/azure-functions/functions-reference-fsharp) - [Azure Functions Host 2.x Reference](https://docs.microsoft.com/en-us/azure/azure-functions/functions-host-json) - [Azure Functions Zip Deployment](https://docs.microsoft.com/en-us/azure/azure-functions/deployment-zip-push)","contentType":"posts","url":"https://www.lqdev.me/posts/http-trigger-azure-functions-fsharp","date":"2019-11-16 17:33:59 -05:00","tags":["fsharp","serverless","azure-functions","azure","dotnet","dotnet-core","functional-programming"],"summary":"## Introduction Since the upgrade to the 2.0 version of the Azure Functions runtime, .NET Core has been natively supported by the platform. As a result some changes took effect. Most notably, in...","keywords":["azure","function","open","functions","microsoft","\u0060\u0060\u0060fsharp","create","\u0060\u0060\u0060bash","body","inside"]},{"id":"posts-restaurant-inspections-etl-data-enrichment-spark-auto-ml-net","title":"Restaurant Inspections ETL \u0026 Data Enrichment with Spark.NET and ML.NET Automated (Auto) ML","content":"## Introduction Apache Spark is an open-source, distributed, general-purpose analytics engine. For years, it has been a staple in the Big Data ecosystem for batch and real-time processing on large datasets. Although native support for the platform is limited to the JVM set of languages, other languages typically used for data processing and analytics like Python and R have plugged into Spark\u0027s Interop layer to make use of its functionality. Around the Build 2019 conference, Microsoft announced Spark.NET. Spark.NET provides bindings written for the Spark Interop layer that allow you to work with components like Spark SQL and Spark Streaming inside your .NET applications. Because Spark.NET is .NET Standard 2.0 compliant, it can run operating systems like Windows, Mac and Linux. Spark.NET is an evolution of the Mobius project which provided .NET bindings for Spark. This sample takes a restaurant violation dataset from the NYC Open Data portal and processes it using Spark.NET. Then, the processed data is used to train a machine learning model that attempts to predict the grade an establishment will receive after an inspection. The model will be trained using ML.NET, an open-source, cross-platform machine learning framework. Finally, data for which no grade currently exists will be enriched using the trained model to assign an expected grade. The source code for this sample can be found in the [lqdev/RestaurantInspectionsSparkMLNET GitHub repo](https://github.com/lqdev/RestaurantInspectionsSparkMLNET). ## Pre-requisites This project was built using Ubuntu 18.04 but should work on Windows and Mac devices. - [.NET Core 2.1 SDK](https://dotnet.microsoft.com/download/dotnet-core/2.1) - [Java 8](https://www.java.com/en/download/) - [Apache Spark 2.4.1 with Hadoop 2.7](https://archive.apache.org/dist/spark/spark-2.4.1/) - [.NET Spark Worker 0.4.0](https://github.com/dotnet/spark/releases) ### Install Java Since Spark runs on the JVM, you\u0027ll need Java on your PC. The minimum version required is version 8. To install and Java, enter the following command into the terminal: \u0060\u0060\u0060bash sudo apt install openjdk-8-jdk openjdk-8-jre \u0060\u0060\u0060 Then, make sure that the recently installed version is the default \u0060\u0060\u0060bash sudo update-alternatives --config java \u0060\u0060\u0060 ### Download and configure Spark Download Spark 2.4.1 with Hadoop 2.7 onto your computer. In this case, I\u0027m placing it into my *Downloads* folder. \u0060\u0060\u0060bash wget https://archive.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz -O ~/Downloads/spark-2.4.1-bin-hadoop2.7.tgz \u0060\u0060\u0060 Extract the contents of the recently downloaded file into the */usr/bin/local* directory. \u0060\u0060\u0060bash sudo tar -xvf ~/Downloads/spark-2.4.1-bin-hadoop2.7.tgz --directory /usr/local/bin \u0060\u0060\u0060 ### Download and configure .NET Spark Worker Download the .NET Spark worker onto your computer. In this case, I\u0027m placing it into the *Downloads* folder. \u0060\u0060\u0060bash wget https://github.com/dotnet/spark/releases/download/v0.4.0/Microsoft.Spark.Worker.netcoreapp2.1.linux-x64-0.4.0.tar.gz -O ~/Downloads/Microsoft.Spark.Worker.netcoreapp2.1.linux-x64-0.4.0.tar.gz \u0060\u0060\u0060 Extract the contents of the recently downloaded file into the */usr/bin/local* directory. \u0060\u0060\u0060bash sudo tar -xvf ~/Downloads/Microsoft.Spark.Worker.netcoreapp2.1.linux-x64-0.4.0.tar.gz --directory /usr/local/bin \u0060\u0060\u0060 Finally, raise the permissions on the \u0060Microsoft.Spark.Worker\u0060 program. This is required to execute User-Defined Functions (UDF). \u0060\u0060\u0060bash sudo chmod \u002Bx /usr/local/bin/Microsoft.Spark.Worker-0.4.0/Microsoft.Spark.Worker \u0060\u0060\u0060 ### Configure environment variables Once you download and configure the pre-requisites, configure their locations in the system as environment variables. Open the *~/.bashrc* file and add the following content at the end of the file. \u0060\u0060\u0060bash export SPARK_PATH=/usr/local/bin/spark-2.4.1-bin-hadoop2.7 export PATH=$SPARK_PATH/bin:$PATH export HADOOP_HOME=$SPARK_PATH export SPARK_HOME=$SPARK_PATH export DOTNET_WORKER_DIR=/usr/local/bin/Microsoft.Spark.Worker-0.4.0 \u0060\u0060\u0060 ## Solution description ### Understand the data The dataset used in this solution is the [*DOHMH New York City Restaurant Inspection Results*](https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j) and comes from the NYC Open Data portal. It is updated daily and contains assigned and pending inspection results and violation citations for restaurants and college cafeterias. The dataset excludes establishments that have gone out of business. Although the dataset contains several columns, only a subset of them are used in this solution. For a detailed description of the dataset, visit the dataset website. ### Understand the solution This solution is made up of four different .NET Core applications: - *RestaurantInspectionsETL*: .NET Core Console application that takes raw data and uses Spark.NET to clean and transform the data into a format that is easier to use as input for training and making predictions with a machine learning model built with ML.NET. - *RestaurantInspectionsML*: .NET Core Class Library that defines the input and output schema of the ML.NET machine learning model. Additionally, this is where the trained model is saved to. - *RestaurantInspectionsTraining*: .NET Core Console application that uses the graded data generated by the *RestaurantInspectionsETL* application to train a multiclass classification machine learning model using ML.NET\u0027s Auto ML. - *RestaurantInspectionsEnrichment*: .NET Core Console application that uses the ungraded data generated by the *RestaurantInspectionsETL* application as input for the trained ML.NET machine learning model which predicts what grade an establishment is most likely to receive based on the violations found during inspection. ## Set up the solution ### Create solution directory Create a new directory for your projects called *RestaurantInspectionsSparkMLNET* and navigate to it with the following command. \u0060\u0060\u0060bash mkdir RestaurantInspectionsSparkMLNET \u0026\u0026 cd RestaurantInspectionsSparkMLNET \u0060\u0060\u0060 Then, create a solution using the \u0060dotnet cli\u0060. \u0060\u0060\u0060bash dotnet new sln \u0060\u0060\u0060 To ensure that the 2.1 version of the .NET Core SDK is used as the target framework, especially if you have multiple versions of the .NET SDK installed, create a file called *globals.json* in the *RestaurantInspectionsSparkMLNET* solution directory. \u0060\u0060\u0060bash touch global.json \u0060\u0060\u0060 In the *global.json* file, add the following content. Make sure to use the specific version of the SDK installed on your computer. In this case, I have version \u00602.1.801\u0060 installed on my computer. You can use the \u0060dotnet --list-sdks\u0060 command to list the installed SDK versions. \u0060\u0060\u0060json { \u0022sdk\u0022: { \u0022version\u0022: \u00222.1.801\u0022 } } \u0060\u0060\u0060 ### Create and configure the ETL project The ETL project is responsible for taking the raw source data and using Spark to apply a series of transformations to prepare the data to train the machine learning model as well as to enrich data with missing grades. Inside the *RestaurantInspectionsSparkMLNET* solution directory, create a new console application called *RestaurantInspectionsETL* using the \u0060dotnet cli\u0060. \u0060\u0060\u0060bash dotnet new console -o RestaurantInspectionsETL \u0060\u0060\u0060 Add the newly created project to the solution with the \u0060dotnet cli\u0060. \u0060\u0060\u0060bash dotnet sln add ./RestaurantInspectionsETL/ \u0060\u0060\u0060 Since this project uses the \u0060Microsoft.Spark\u0060 NuGet package, use the \u0060dotnet cli\u0060 to install it. \u0060\u0060\u0060bash dotnet add ./RestaurantInspectionsETL/ package Microsoft.Spark --version 0.4.0 \u0060\u0060\u0060 ### Create and configure the ML model project The ML model class library will contain the domain model that defines the schema of model inputs and outputs as well as the trained model itself. Inside the *RestaurantInspectionsSparkMLNET* solution directory, create a new class library called *RestaurantInspectionsML* using the \u0060dotnet cli\u0060. \u0060\u0060\u0060bash dotnet new classlib -o RestaurantInspectionsML \u0060\u0060\u0060 Add the newly created project to the solution with the \u0060dotnet cli\u0060. \u0060\u0060\u0060bash dotnet sln add ./RestaurantInspectionsML/ \u0060\u0060\u0060 Since this project uses the \u0060Microsoft.ML\u0060 NuGet package, use the \u0060dotnet cli\u0060 to install it. \u0060\u0060\u0060bash dotnet add ./RestaurantInspectionsML/ package Microsoft.ML --version 1.3.1 \u0060\u0060\u0060 ### Create and configure the ML training project The purpose of the training project is to use the pre-processed graded data output by the *RestaurantInspectionsETL* project as input to train a multiclass classification model with ML.NET\u0027s Auto ML API. The trained model will then be saved in the *RestaurantInspectionsML* directory. Inside the *RestaurantInspectionsSparkMLNET* solution directory, create a new console application called *RestaurantInspectionsTraining* using the \u0060dotnet cli\u0060. \u0060\u0060\u0060bash dotnet new console -o RestaurantInspectionsTraining \u0060\u0060\u0060 Add the newly created project to the solution with the \u0060dotnet cli\u0060. \u0060\u0060\u0060bash dotnet sln add ./RestaurantInspectionsTraining/ \u0060\u0060\u0060 This project depends on the domain model created in the *RestaurantInspectionsML* project, so you need to add a reference to it. \u0060\u0060\u0060bash dotnet add ./RestaurantInspectionsTraining/ reference ./RestaurantInspectionsML/ \u0060\u0060\u0060 Since this project uses the \u0060Microsoft.Auto.ML\u0060 NuGet package, use the \u0060dotnet cli\u0060 to install it. \u0060\u0060\u0060bash dotnet add ./RestaurantInspectionsTraining/ package Microsoft.ML.AutoML --version 0.15.1 \u0060\u0060\u0060 ### Create and configure the data enrichment project The data enrichment application uses the trained machine learning model created by the *RestaurantInspectionsTraining* application and use it on the pre-processed ungraded data created by the *RestaurantInspectionsETL* application to predict what grade that establishment is most likely to receive based on the violations found during inspection. Inside the *RestaurantInspectionsSparkMLNET* solution directory, create a new console application called *RestaurantInspectionsEnrichment* using the \u0060dotnet cli\u0060. \u0060\u0060\u0060bash dotnet new console -o RestaurantInspectionsEnrichment \u0060\u0060\u0060 Add the newly created project to the solution with the \u0060dotnet cli\u0060. \u0060\u0060\u0060bash dotnet sln add ./RestaurantInspectionsEnrichment/ \u0060\u0060\u0060 This project depends on the domain model created in the *RestaurantInspectionsML* project, so you need to add a reference to it. \u0060\u0060\u0060bash dotnet add ./RestaurantInspectionsEnrichment/ reference ./RestaurantInspectionsML/ \u0060\u0060\u0060 This uses the following NuGet packages: - Microsoft.Spark - Microsoft.ML.LightGBM (This is not required but predictions may fail if the final model is a LightGBM model). Install the packages with the following commands: \u0060\u0060\u0060bash dotnet add ./RestaurantInspectionsEnrichment/ package Microsoft.Spark --version 0.4.0 dotnet add ./RestaurantInspectionsEnrichment/ package Microsoft.ML.LightGBM --version 1.3.1 \u0060\u0060\u0060 ## Build ETL application The first step is to prepare the data. To do so, apply a set of transformations using Spark.NET. ### Download the data Navigate to the *RestaurantInspectionsETL* project and create a *Data* directory. \u0060\u0060\u0060bash mkdir Data \u0060\u0060\u0060 Then, download the data into the newly created *Data* directory. \u0060\u0060\u0060bash wget https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv?accessType=DOWNLOAD -O Data/NYC-Restaurant-Inspections.csv \u0060\u0060\u0060 ### Build the ETL pipeline Add the following usings to the *Program.cs* file. \u0060\u0060\u0060csharp using System; using System.IO; using Microsoft.Spark.Sql; using static Microsoft.Spark.Sql.Functions; \u0060\u0060\u0060 Not all of the columns are relevant. Inside the \u0060Main\u0060 method of the *Program.cs* file, define the columns to be removed. \u0060\u0060\u0060csharp string[] dropCols = new string[] { \u0022CAMIS\u0022, \u0022CUISINE DESCRIPTION\u0022, \u0022VIOLATION DESCRIPTION\u0022, \u0022BORO\u0022, \u0022BUILDING\u0022, \u0022STREET\u0022, \u0022ZIPCODE\u0022, \u0022PHONE\u0022, \u0022ACTION\u0022, \u0022GRADE DATE\u0022, \u0022RECORD DATE\u0022, \u0022Latitude\u0022, \u0022Longitude\u0022, \u0022Community Board\u0022, \u0022Council District\u0022, \u0022Census Tract\u0022, \u0022BIN\u0022, \u0022BBL\u0022, \u0022NTA\u0022 }; \u0060\u0060\u0060 The entrypoint of Spark applications is the \u0060SparkSession\u0060. Create \u0060SparkSession\u0060 inside the \u0060Main\u0060 method of the *Program.cs* file. \u0060\u0060\u0060csharp var sc = SparkSession .Builder() .AppName(\u0022Restaurant_Inspections_ETL\u0022) .GetOrCreate(); \u0060\u0060\u0060 Then, load the data stored in the *NYC-Restaurant-Inspections.csv* file into a \u0060DataFrame\u0060. \u0060\u0060\u0060csharp DataFrame df = sc .Read() .Option(\u0022header\u0022, \u0022true\u0022) .Option(\u0022inferSchema\u0022, \u0022true\u0022) .Csv(\u0022Data/NYC-Restaurant-Inspections.csv\u0022); \u0060\u0060\u0060 \u0060DataFrames\u0060 can be thought of as tables in a database or sheets in Excel. Spark has various ways of representing data but \u0060DataFrames\u0060 are the format supported by Spark.NET. Additionally, the \u0060DataFrame\u0060 API is higher-level and easier to work with. Once the data is loaded, get rid of the data that are not needed by creating a new \u0060DataFrame\u0060 that excludes the \u0060dropCols\u0060 as well as missing values. \u0060\u0060\u0060csharp DataFrame cleanDf = df .Drop(dropCols) .WithColumnRenamed(\u0022INSPECTION DATE\u0022,\u0022INSPECTIONDATE\u0022) .WithColumnRenamed(\u0022INSPECTION TYPE\u0022,\u0022INSPECTIONTYPE\u0022) .WithColumnRenamed(\u0022CRITICAL FLAG\u0022,\u0022CRITICALFLAG\u0022) .WithColumnRenamed(\u0022VIOLATION CODE\u0022,\u0022VIOLATIONCODE\u0022) .Na() .Drop(); \u0060\u0060\u0060 Typically, machine learning models expect values to be numerical, so in the ETL step try to convert as many values as possible into numerical values. The \u0060CRITICALFLAG\u0060 column contains \u0022Y\u0022/\u0022N\u0022 values that can be encoded as 0 and 1. \u0060\u0060\u0060csharp DataFrame labeledFlagDf = cleanDf .WithColumn(\u0022CRITICALFLAG\u0022, When(Functions.Col(\u0022CRITICALFLAG\u0022) == \u0022Y\u0022,1) .Otherwise(0)); \u0060\u0060\u0060 This dataset contains one violation per row which correspond to different inspections. Therefore, all of the violations need to be aggregated by business and inspection. \u0060\u0060\u0060csharp DataFrame groupedDf = labeledFlagDf .GroupBy(\u0022DBA\u0022, \u0022INSPECTIONDATE\u0022, \u0022INSPECTIONTYPE\u0022, \u0022CRITICALFLAG\u0022, \u0022SCORE\u0022, \u0022GRADE\u0022) .Agg(Functions.CollectSet(Functions.Col(\u0022VIOLATIONCODE\u0022)).Alias(\u0022CODES\u0022)) .Drop(\u0022DBA\u0022, \u0022INSPECTIONDATE\u0022) .WithColumn(\u0022CODES\u0022, Functions.ArrayJoin(Functions.Col(\u0022CODES\u0022), \u0022,\u0022)) .Select(\u0022INSPECTIONTYPE\u0022, \u0022CODES\u0022, \u0022CRITICALFLAG\u0022, \u0022SCORE\u0022, \u0022GRADE\u0022); \u0060\u0060\u0060 Now that the data is in the format used to train and make predictions, split the cleaned \u0060DataFrame\u0060 into two new \u0060DataFrames\u0060, graded and ungraded. The graded dataset is the data used for training the machine learning model. The ungraded data will be used for enrichment. \u0060\u0060\u0060csharp DataFrame gradedDf = groupedDf .Filter( Col(\u0022GRADE\u0022) == \u0022A\u0022 | Col(\u0022GRADE\u0022) == \u0022B\u0022 | Col(\u0022GRADE\u0022) == \u0022C\u0022 ); DataFrame ungradedDf = groupedDf .Filter( Col(\u0022GRADE\u0022) != \u0022A\u0022 \u0026 Col(\u0022GRADE\u0022) != \u0022B\u0022 \u0026 Col(\u0022GRADE\u0022) != \u0022C\u0022 ); \u0060\u0060\u0060 Take the \u0060DataFrames\u0060 and save them as csv files for later use. \u0060\u0060\u0060csharp var timestamp = ((DateTimeOffset) DateTime.UtcNow).ToUnixTimeSeconds().ToString(); var saveDirectory = Path.Join(\u0022Output\u0022,timestamp); if(!Directory.Exists(saveDirectory)) { Directory.CreateDirectory(saveDirectory); } gradedDf.Write().Csv(Path.Join(saveDirectory,\u0022Graded\u0022)); ungradedDf.Write().Csv(Path.Join(saveDirectory,\u0022Ungraded\u0022)); \u0060\u0060\u0060 ### Publish and run the ETL application The final *Program.cs* file should look as follows: \u0060\u0060\u0060csharp using System; using System.IO; using Microsoft.Spark.Sql; using static Microsoft.Spark.Sql.Functions; namespace RestaurantInspectionsETL { class Program { static void Main(string[] args) { // Define columns to remove string[] dropCols = new string[] { \u0022CAMIS\u0022, \u0022CUISINE DESCRIPTION\u0022, \u0022VIOLATION DESCRIPTION\u0022, \u0022BORO\u0022, \u0022BUILDING\u0022, \u0022STREET\u0022, \u0022ZIPCODE\u0022, \u0022PHONE\u0022, \u0022ACTION\u0022, \u0022GRADE DATE\u0022, \u0022RECORD DATE\u0022, \u0022Latitude\u0022, \u0022Longitude\u0022, \u0022Community Board\u0022, \u0022Council District\u0022, \u0022Census Tract\u0022, \u0022BIN\u0022, \u0022BBL\u0022, \u0022NTA\u0022 }; // Create SparkSession var sc = SparkSession .Builder() .AppName(\u0022Restaurant_Inspections_ETL\u0022) .GetOrCreate(); // Load data DataFrame df = sc .Read() .Option(\u0022header\u0022, \u0022true\u0022) .Option(\u0022inferSchema\u0022, \u0022true\u0022) .Csv(\u0022Data/NYC-Restaurant-Inspections.csv\u0022); //Remove columns and missing values DataFrame cleanDf = df .Drop(dropCols) .WithColumnRenamed(\u0022INSPECTION DATE\u0022,\u0022INSPECTIONDATE\u0022) .WithColumnRenamed(\u0022INSPECTION TYPE\u0022,\u0022INSPECTIONTYPE\u0022) .WithColumnRenamed(\u0022CRITICAL FLAG\u0022,\u0022CRITICALFLAG\u0022) .WithColumnRenamed(\u0022VIOLATION CODE\u0022,\u0022VIOLATIONCODE\u0022) .Na() .Drop(); // Encode CRITICAL FLAG column DataFrame labeledFlagDf = cleanDf .WithColumn(\u0022CRITICALFLAG\u0022, When(Functions.Col(\u0022CRITICALFLAG\u0022) == \u0022Y\u0022,1) .Otherwise(0)); // Aggregate violations by business and inspection DataFrame groupedDf = labeledFlagDf .GroupBy(\u0022DBA\u0022, \u0022INSPECTIONDATE\u0022, \u0022INSPECTIONTYPE\u0022, \u0022CRITICALFLAG\u0022, \u0022SCORE\u0022, \u0022GRADE\u0022) .Agg(Functions.CollectSet(Functions.Col(\u0022VIOLATIONCODE\u0022)).Alias(\u0022CODES\u0022)) .Drop(\u0022DBA\u0022, \u0022INSPECTIONDATE\u0022) .WithColumn(\u0022CODES\u0022, Functions.ArrayJoin(Functions.Col(\u0022CODES\u0022), \u0022,\u0022)) .Select(\u0022INSPECTIONTYPE\u0022, \u0022CODES\u0022, \u0022CRITICALFLAG\u0022, \u0022SCORE\u0022, \u0022GRADE\u0022); // Split into graded and ungraded DataFrames DataFrame gradedDf = groupedDf .Filter( Col(\u0022GRADE\u0022) == \u0022A\u0022 | Col(\u0022GRADE\u0022) == \u0022B\u0022 | Col(\u0022GRADE\u0022) == \u0022C\u0022 ); DataFrame ungradedDf = groupedDf .Filter( Col(\u0022GRADE\u0022) != \u0022A\u0022 \u0026 Col(\u0022GRADE\u0022) != \u0022B\u0022 \u0026 Col(\u0022GRADE\u0022) != \u0022C\u0022 ); // Save DataFrames var timestamp = ((DateTimeOffset) DateTime.UtcNow).ToUnixTimeSeconds().ToString(); var saveDirectory = Path.Join(\u0022Output\u0022,timestamp); if(!Directory.Exists(saveDirectory)) { Directory.CreateDirectory(saveDirectory); } gradedDf.Write().Csv(Path.Join(saveDirectory,\u0022Graded\u0022)); ungradedDf.Write().Csv(Path.Join(saveDirectory,\u0022Ungraded\u0022)); } } } \u0060\u0060\u0060 Publish the application with the following command. \u0060\u0060\u0060bash dotnet publish -f netcoreapp2.1 -r ubuntu.18.04-x64 \u0060\u0060\u0060 Run the application with \u0060spark-submit\u0060. \u0060\u0060\u0060bash spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master local bin/Debug/netcoreapp2.1/ubuntu.18.04-x64/publish/microsoft-spark-2.4.x-0.4.0.jar dotnet bin/Debug/netcoreapp2.1/ubuntu.18.04-x64/publish/RestaurantInspectionsETL.dll \u0060\u0060\u0060 ## Build ML Domain ### Define the model input schema Navigate to the *RestaurantInspectionsTraining* project directory and create a new file called *ModelInput.cs*. \u0060\u0060\u0060bash touch ModelInput.cs \u0060\u0060\u0060 Open the *ModelInput.cs* file and add the following code. \u0060\u0060\u0060csharp using Microsoft.ML.Data; namespace RestaurantInspectionsML { public class ModelInput { [LoadColumn(0)] public string InspectionType { get; set; } [LoadColumn(1)] public string Codes { get; set; } [LoadColumn(2)] public float CriticalFlag { get; set; } [LoadColumn(3)] public float InspectionScore { get; set; } [LoadColumn(4)] [ColumnName(\u0022Label\u0022)] public string Grade { get; set; } } } \u0060\u0060\u0060 Using attributes in the schema, five properties are defined: - InspectionType: The type of inspection performed. - Codes: Violation codes found during inspection. - CriticalFlag: Indicates if any of the violations during the inspection were critical (contribute to food-borne illness). - InspectionScore: Score assigned after inspection. - Grade: Letter grade assigned after inspection The \u0060LoadColumn\u0060 attribute defines the position of the column in the file. Data in the last column is assigned to the \u0060Grade\u0060 property but is then referenced as \u0060Label\u0060 in the \u0060IDataView\u0060. The reason for using the \u0060ColumnName\u0060 attribute is ML.NET algorithms have default column names and renaming properties at the schema class level removes the need to define the feature and label columns as parameters in the training pipeline. ### Define the model output schema In the *RestaurantInspectionsTraining* project directory and create a new file called *ModelOutput.cs*. \u0060\u0060\u0060bash touch ModelOutput.cs \u0060\u0060\u0060 Open the *ModelOutput.cs* file and add the following code. \u0060\u0060\u0060csharp namespace RestaurantInspectionsML { public class ModelOutput { public float[] Scores { get; set; } public string PredictedLabel { get; set; } } } \u0060\u0060\u0060 For the output schema, the \u0060ModelOutput\u0060 class uses properties with the default column names of the outputs generated by the model training process: - Scores: A float vector containing the probabilties for all the predicted classes. - PredictedLabel: The value of the prediction. In this case, the \u0060PredictedLabel\u0060 is the predicted grade expected to be assigned after inspection given the set of features for that inspection. ## Build the model training application The application trains a multiclass classification algorithm. Finding the \u0022best\u0022 algorithm with the right parameters requires experimentation. Fortunately, ML.NET\u0027s Auto ML does this for you given you provide it with the type of algorithm you want to train. ### Load the graded data Navigate to the *RestaurantInspectionsTraining* project directory and add the following using statements to the *Program.cs* class. \u0060\u0060\u0060csharp using System; using System.IO; using System.Linq; using Microsoft.ML; using static Microsoft.ML.DataOperationsCatalog; using Microsoft.ML.AutoML; using RestaurantInspectionsML; \u0060\u0060\u0060 Inside the \u0060Main\u0060 method of the *Program.cs* file, define the path where the data files are stored. \u0060\u0060\u0060csharp string solutionDirectory = \u0022/home/lqdev/Development/RestaurantInspectionsSparkMLNET\u0022; string dataLocation = Path.Combine(solutionDirectory,\u0022RestaurantInspectionsETL\u0022,\u0022Output\u0022); \u0060\u0060\u0060 The entrypoint of an ML.NET application is the \u0060MLContext\u0060. Initialize an \u0060MLContext\u0060 instance. \u0060\u0060\u0060csharp MLContext mlContext = new MLContext(); \u0060\u0060\u0060 Next, get the paths of the data files. The output generated by the *RestaurantInspectionsETL* application contains both the csv files as well as files containing information about the partitions that created them. For training, only the csv files are needed. \u0060\u0060\u0060csharp var latestOutput = Directory .GetDirectories(dataLocation) .Select(directory =\u003E new DirectoryInfo(directory)) .OrderBy(directoryInfo =\u003E directoryInfo.Name) .Select(directory =\u003E Path.Join(directory.FullName,\u0022Graded\u0022)) .First(); var dataFilePaths = Directory .GetFiles(latestOutput) .Where(file =\u003E file.EndsWith(\u0022csv\u0022)) .ToArray(); \u0060\u0060\u0060 Then, load the data into an \u0060IDataView\u0060. An \u0060IDataView\u0060 is similar to a \u0060DataFrame\u0060 in that it is a way to represent data as rows, columns and their schema. \u0060\u0060\u0060csharp var dataLoader = mlContext.Data.CreateTextLoader (separatorChar:\u0027,\u0027, hasHeader:false, allowQuoting:true, trimWhitespace:true); IDataView data = dataLoader.Load(dataFilePaths); \u0060\u0060\u0060 It\u0027s good practice to split the data into training and test sets for evaluation. Split the data into 80% training and 20% test sets. \u0060\u0060\u0060csharp TrainTestData dataSplit = mlContext.Data.TrainTestSplit(data,testFraction:0.2); IDataView trainData = dataSplit.TrainSet; IDataView testData = dataSplit.TestSet; \u0060\u0060\u0060 ### Create the experiment Auto ML takes the data and runs experiments using different models and hyper-parameters in search of the \u0022best\u0022 model. Define the settings for your experiment. In this case, the model will run for 600 seconds or 10 minutes and will try to find the model with the lowest log loss metric. \u0060\u0060\u0060csharp var experimentSettings = new MulticlassExperimentSettings(); experimentSettings.MaxExperimentTimeInSeconds = 600; experimentSettings.OptimizingMetric = MulticlassClassificationMetric.LogLoss; \u0060\u0060\u0060 Then, create the experiment. \u0060\u0060\u0060csharp var experiment = mlContext.Auto().CreateMulticlassClassificationExperiment(experimentSettings); \u0060\u0060\u0060 After creating the experiment, run it. \u0060\u0060\u0060csharp var experimentResults = experiment.Execute(data, progressHandler: new ProgressHandler()); \u0060\u0060\u0060 By default, running the application won\u2019t display progress information. However, a \u0060ProgressHandler\u0060 object can be passed into the \u0060Execute\u0060 method of an experiment which calls the implemented \u0060Report\u0060 method. Inside the *RestaurantInspectionsTraining* project directory, create a new file called *ProgressHandler.cs*. \u0060\u0060\u0060bash touch ProgressHandler.cs \u0060\u0060\u0060 Then, add the following code: \u0060\u0060\u0060csharp using System; using Microsoft.ML.Data; using Microsoft.ML.AutoML; namespace RestaurantInspectionsTraining { public class ProgressHandler : IProgress \u003E { public void Report(RunDetail run) { Console.WriteLine($\u0022Trained {run.TrainerName} with Log Loss {run.ValidationMetrics.LogLoss:0.####} in {run.RuntimeInSeconds:0.##} seconds\u0022); } } } \u0060\u0060\u0060 The ProgressHandler class derives from the \u0060IProgress \u0060 interface which requires the implementation of the \u0060Report\u0060 method. The object being passed into the Report method after each run is an \u0060RunDetail \u0060 object. Each time a run is complete, the \u0060Report\u0060 method is called and the code inside it executes. ### Evaluate the results Once the experiment has finished running, get the model from the best run. Add the following code to the \u0060Main\u0060 method of the *Program.cs*. \u0060\u0060\u0060csharp var bestModel = experimentResults.BestRun.Model; \u0060\u0060\u0060 Evaluate the performance of the model using the test dataset and measure it\u0027s Micro-Accuracy metric. \u0060\u0060\u0060csharp IDataView scoredTestData = bestModel.Transform(testData); var metrics = mlContext.MulticlassClassification.Evaluate(scoredTestData); Console.WriteLine($\u0022MicroAccuracy: {metrics.MicroAccuracy}\u0022); \u0060\u0060\u0060 ### Save the trained model Finally, save the trained model to the *RestaurantInspectionsML* project. \u0060\u0060\u0060csharp string modelSavePath = Path.Join(solutionDirectory,\u0022RestaurantInspectionsML\u0022,\u0022model.zip\u0022); mlContext.Model.Save(bestModel, data.Schema, modelSavePath); \u0060\u0060\u0060 A file called *model.zip* should be created inside the *RestaurantInspectionsML* project. Make sure that the trained model file is copied to the output directory by adding the following contents to the *RestaurantInspectionsML.csproj* file in the *RestaurantInspectionsML* directory. \u0060\u0060\u0060xml PreserveNewest \u0060\u0060\u0060 Copying it to the output directory of the *RestaurantInspectionsML* makes it easier to reference from the *RestaurantInspectionsEnrichment* project since that project already contains a reference to the *RestaurantInspectionsML* class library. ## Train the model The final *Program.cs* file should look as follows: \u0060\u0060\u0060csharp using System; using System.IO; using System.Linq; using Microsoft.ML; using static Microsoft.ML.DataOperationsCatalog; using Microsoft.ML.AutoML; using RestaurantInspectionsML; namespace RestaurantInspectionsTraining { class Program { static void Main(string[] args) { // Define source data directory paths string solutionDirectory = \u0022/home/lqdev/Development/RestaurantInspectionsSparkMLNET\u0022; string dataLocation = Path.Combine(solutionDirectory,\u0022RestaurantInspectionsETL\u0022,\u0022Output\u0022); // Initialize MLContext MLContext mlContext = new MLContext(); // Get directory name of most recent ETL output var latestOutput = Directory .GetDirectories(dataLocation) .Select(directory =\u003E new DirectoryInfo(directory)) .OrderBy(directoryInfo =\u003E directoryInfo.Name) .Select(directory =\u003E Path.Join(directory.FullName,\u0022Graded\u0022)) .First(); var dataFilePaths = Directory .GetFiles(latestOutput) .Where(file =\u003E file.EndsWith(\u0022csv\u0022)) .ToArray(); // Load the data var dataLoader = mlContext.Data.CreateTextLoader (separatorChar:\u0027,\u0027, hasHeader:false, allowQuoting:true, trimWhitespace:true); IDataView data = dataLoader.Load(dataFilePaths); // Split the data TrainTestData dataSplit = mlContext.Data.TrainTestSplit(data,testFraction:0.2); IDataView trainData = dataSplit.TrainSet; IDataView testData = dataSplit.TestSet; // Define experiment settings var experimentSettings = new MulticlassExperimentSettings(); experimentSettings.MaxExperimentTimeInSeconds = 600; experimentSettings.OptimizingMetric = MulticlassClassificationMetric.LogLoss; // Create experiment var experiment = mlContext.Auto().CreateMulticlassClassificationExperiment(experimentSettings); // Run experiment var experimentResults = experiment.Execute(data, progressHandler: new ProgressHandler()); // Best Run Results var bestModel = experimentResults.BestRun.Model; // Evaluate Model IDataView scoredTestData = bestModel.Transform(testData); var metrics = mlContext.MulticlassClassification.Evaluate(scoredTestData); Console.WriteLine($\u0022MicroAccuracy: {metrics.MicroAccuracy}\u0022); // Save Model string modelSavePath = Path.Join(solutionDirectory,\u0022RestaurantInspectionsML\u0022,\u0022model.zip\u0022); mlContext.Model.Save(bestModel, data.Schema, modelSavePath); } } } \u0060\u0060\u0060 Once all the code and configurations are complete, from the *RestaurantInspectionsTraining* directory, run the application using the \u0060dotnet cli\u0060. Remember this will take 10 minutes to run. \u0060\u0060\u0060bash dotnet run \u0060\u0060\u0060 The console output should look similar to the output below: \u0060\u0060\u0060text Trained LightGbmMulti with Log Loss 0.1547 in 1.55 seconds Trained FastTreeOva with Log Loss 0.0405 in 65.58 seconds Trained FastForestOva with Log Loss 0.0012 in 53.37 seconds Trained LightGbmMulti with Log Loss 0.0021 in 4.55 seconds Trained FastTreeOva with Log Loss 0.8315 in 5.22 seconds MicroAccuracy: 0.999389615839469 \u0060\u0060\u0060 ## Build the data enrichment application Now that the model is trained, it can be used to enrich the ungraded data. ### Initialize the PredictionEngine Navigate to the *RestaurantInspectionsEnrichment* project directory and add the following using statements to the *Program.cs* class. \u0060\u0060\u0060csharp using System.IO; using System.Linq; using Microsoft.ML; using Microsoft.Spark.Sql; using static Microsoft.Spark.Sql.Functions; using RestaurantInspectionsML; \u0060\u0060\u0060 To make predictions, the model has to be loaded into the applicaton and because predictions are made one row at a time, a \u0060PredictionEngine\u0060 has be created as well. Inside the \u0060Program\u0060 class, define the \u0060PredictionEngine\u0060. \u0060\u0060\u0060csharp private static readonly PredictionEngine _predictionEngine; \u0060\u0060\u0060 Then, create a constructor to load the model and initialize it. \u0060\u0060\u0060csharp static Program() { MLContext mlContext = new MLContext(); ITransformer model = mlContext.Model.Load(\u0022model.zip\u0022,out DataViewSchema schema); _predictionEngine = mlContext.Model.CreatePredictionEngine (model); } \u0060\u0060\u0060 ### Load the ungraded data In the \u0060Main\u0060 method of the \u0060Program\u0060 class, define the location of the data files. \u0060\u0060\u0060csharp string solutionDirectory = \u0022/home/lqdev/Development/RestaurantInspectionsSparkMLNET\u0022; string dataLocation = Path.Combine(solutionDirectory,\u0022RestaurantInspectionsETL\u0022,\u0022Output\u0022); \u0060\u0060\u0060 Then, get the path of the most recent ungraded data generated by the *RestaurantInspectionsETL* application. \u0060\u0060\u0060csharp var latestOutput = Directory .GetDirectories(dataLocation) .Select(directory =\u003E new DirectoryInfo(directory)) .OrderBy(directoryInfo =\u003E directoryInfo.Name) .Select(directory =\u003E directory.FullName) .First(); \u0060\u0060\u0060 Initialize a \u0060SparkSession\u0060 for your enrichment application. \u0060\u0060\u0060csharp var sc = SparkSession .Builder() .AppName(\u0022Restaurant_Inspections_Enrichment\u0022) .GetOrCreate(); \u0060\u0060\u0060 The data generated by the *RestaurantInspectionsETL* does not have headers. However, the schema can be defined and set when the data is loaded. \u0060\u0060\u0060csharp var schema = @\u0022 INSPECTIONTYPE string, CODES string, CRITICALFLAG int, INSPECTIONSCORE int, GRADE string\u0022; DataFrame df = sc .Read() .Schema(schema) .Csv(Path.Join(latestOutput,\u0022Ungraded\u0022)); \u0060\u0060\u0060 ### Define the UDF There is no built-in function in Spark that allows you to use a \u0060PredictionEngine\u0060. However, Spark can be extended through UDFs. Keep in mind that UDFs are not optimized like the built-in functions. Therefore, whenever possible, try to use the built-in functions as much as possible. In the \u0060Program\u0060 class, create a new method called \u0060PredictGrade\u0060 which takes in the set of features that make up the \u0060ModelInput\u0060 expected by the trained model. \u0060\u0060\u0060csharp public static string PredictGrade( string inspectionType, string violationCodes, int criticalFlag, int inspectionScore) { ModelInput input = new ModelInput { InspectionType=inspectionType, Codes=violationCodes, CriticalFlag=(float)criticalFlag, InspectionScore=(float)inspectionScore }; ModelOutput prediction = _predictionEngine.Predict(input); return prediction.PredictedLabel; } \u0060\u0060\u0060 Then, inside the \u0060Main\u0060 method, register the \u0060PredictGrade\u0060 method as a UDF in your \u0060SparkSession\u0060. \u0060\u0060\u0060csharp sc.Udf().Register (\u0022PredictGrade\u0022,PredictGrade); \u0060\u0060\u0060 ### Enrich the data Once the UDF is registered, use it inside of a \u0060Select\u0060 statement which creates a new \u0060DataFrame\u0060 that includes the input features as well as the predicted grade output by the trained mdoel. \u0060\u0060\u0060csharp var enrichedDf = df .Select( Col(\u0022INSPECTIONTYPE\u0022), Col(\u0022CODES\u0022), Col(\u0022CRITICALFLAG\u0022), Col(\u0022INSPECTIONSCORE\u0022), CallUDF(\u0022PredictGrade\u0022, Col(\u0022INSPECTIONTYPE\u0022), Col(\u0022CODES\u0022), Col(\u0022CRITICALFLAG\u0022), Col(\u0022INSPECTIONSCORE\u0022) ).Alias(\u0022PREDICTEDGRADE\u0022) ); \u0060\u0060\u0060 Finally, save the enriched \u0060DataFrame\u0060 \u0060\u0060\u0060csharp string outputId = new DirectoryInfo(latestOutput).Name; string enrichedOutputPath = Path.Join(solutionDirectory,\u0022RestaurantInspectionsEnrichment\u0022,\u0022Output\u0022); string savePath = Path.Join(enrichedOutputPath,outputId); if(!Directory.Exists(savePath)) { Directory.CreateDirectory(enrichedOutputPath); } enrichedDf.Write().Csv(savePath); \u0060\u0060\u0060 ### Publish and run the enrichment application The final *Program.cs* file should look as follows. \u0060\u0060\u0060csharp using System.IO; using System.Linq; using Microsoft.ML; using Microsoft.Spark.Sql; using static Microsoft.Spark.Sql.Functions; using RestaurantInspectionsML; namespace RestaurantInspectionsEnrichment { class Program { private static readonly PredictionEngine _predictionEngine; static Program() { MLContext mlContext = new MLContext(); ITransformer model = mlContext.Model.Load(\u0022model.zip\u0022,out DataViewSchema schema); _predictionEngine = mlContext.Model.CreatePredictionEngine (model); } static void Main(string[] args) { // Define source data directory paths string solutionDirectory = \u0022/home/lqdev/Development/RestaurantInspectionsSparkMLNET\u0022; string dataLocation = Path.Combine(solutionDirectory,\u0022RestaurantInspectionsETL\u0022,\u0022Output\u0022); var latestOutput = Directory .GetDirectories(dataLocation) .Select(directory =\u003E new DirectoryInfo(directory)) .OrderBy(directoryInfo =\u003E directoryInfo.Name) .Select(directory =\u003E directory.FullName) .First(); var sc = SparkSession .Builder() .AppName(\u0022Restaurant_Inspections_Enrichment\u0022) .GetOrCreate(); var schema = @\u0022 INSPECTIONTYPE string, CODES string, CRITICALFLAG int, INSPECTIONSCORE int, GRADE string\u0022; DataFrame df = sc .Read() .Schema(schema) .Csv(Path.Join(latestOutput,\u0022Ungraded\u0022)); sc.Udf().Register (\u0022PredictGrade\u0022,PredictGrade); var enrichedDf = df .Select( Col(\u0022INSPECTIONTYPE\u0022), Col(\u0022CODES\u0022), Col(\u0022CRITICALFLAG\u0022), Col(\u0022INSPECTIONSCORE\u0022), CallUDF(\u0022PredictGrade\u0022, Col(\u0022INSPECTIONTYPE\u0022), Col(\u0022CODES\u0022), Col(\u0022CRITICALFLAG\u0022), Col(\u0022INSPECTIONSCORE\u0022) ).Alias(\u0022PREDICTEDGRADE\u0022) ); string outputId = new DirectoryInfo(latestOutput).Name; string enrichedOutputPath = Path.Join(solutionDirectory,\u0022RestaurantInspectionsEnrichment\u0022,\u0022Output\u0022); string savePath = Path.Join(enrichedOutputPath,outputId); if(!Directory.Exists(savePath)) { Directory.CreateDirectory(enrichedOutputPath); } enrichedDf.Write().Csv(savePath); } public static string PredictGrade( string inspectionType, string violationCodes, int criticalFlag, int inspectionScore) { ModelInput input = new ModelInput { InspectionType=inspectionType, Codes=violationCodes, CriticalFlag=(float)criticalFlag, InspectionScore=(float)inspectionScore }; ModelOutput prediction = _predictionEngine.Predict(input); return prediction.PredictedLabel; } } } \u0060\u0060\u0060 From the *RestaurantInspectionsEnrichment* project publish the application with the following command. \u0060\u0060\u0060bash dotnet publish -f netcoreapp2.1 -r ubuntu.18.04-x64 \u0060\u0060\u0060 Navigate to the *publish* directory. In this case, it\u0027s *bin/Debug/netcoreapp2.1/ubuntu.18.04-x64/publish*. From the *publish* directory, run the application with \u0060spark-submit\u0060. \u0060\u0060\u0060bash spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master local microsoft-spark-2.4.x-0.4.0.jar dotnet RestaurantInspectionsEnrichment.dll \u0060\u0060\u0060 The file output should look similar to the contents below: \u0060\u0060\u0060text Cycle Inspection / Initial Inspection,04N,1,13,A Cycle Inspection / Re-inspection,08A,0,9,A Cycle Inspection / Initial Inspection,\u002210B,10H\u0022,0,10,A Cycle Inspection / Initial Inspection,10F,0,10,A Cycle Inspection / Reopening Inspection,10F,0,3,C \u0060\u0060\u0060 ## Conclusion This solution showcased how Spark can be used within .NET applications with Spark.NET. Because it\u0027s part of the .NET ecosystem, other components and frameworks such as ML.NET can be leveraged to extend the system\u0027s capabilities. Although this sample was developed and run on a local, single-node cluster, Spark was made to run at scale. As such, this application can be further improved by setting up a cluster and running the ETL and enrichment workloads on there. ###### Resources [Apache Spark](https://spark.apache.org/) [Spark.NET](https://dotnet.microsoft.com/apps/data/spark) [Spark.NET GitHub](https://github.com/dotnet/spark) [Mobius](https://github.com/Microsoft/Mobius) [NYC OpenData](https://opendata.cityofnewyork.us/)","contentType":"posts","url":"https://www.lqdev.me/posts/restaurant-inspections-etl-data-enrichment-spark-auto-ml-net","date":"2019-09-15 15:12:01 -05:00","tags":["mlnet","ml","machine-learning","ai","artificial-intelligence","big-data","spark","dotnet","dotnetcore"],"summary":"## Introduction Apache Spark is an open-source, distributed, general-purpose analytics engine. For years, it has been a staple in the Big Data ecosystem for batch and real-time processing on large...","keywords":["data","directory","spark","using","model","string","\u0060\u0060\u0060csharp","\u0060\u0060\u0060bash","project","microsoft"]},{"id":"posts-ml-lifecycle-management-mlflow-automated-ml-net","title":"Machine Learning Lifecycle Management with ML.NET, Automated ML and MLFlow","content":"## Introduction As machine learning matures, best practices are starting to be adopted. Application lifecycle management has been a common practice within software development for some time. Now, some of those practices are starting to become adopted in the machine learning space. One of the challenges application lifecycle management addresses in machine learning is reproducibility. Machine learning is extremely experimental in nature. Therefore, in order to find the \u0022best\u0022 model, various algorithms and hyper-parameters need to be tested. This can sometimes be a manual process. At Build 2019, [Automated ML was announced](https://devblogs.microsoft.com/dotnet/announcing-ml-net-1-0/) for ML.NET. In addition to automating the training process, this framework will try to find the best model by iterating over various algorithms and hyper-parameters until it finds the \u0022best\u0022 model based on the selected optimization metric. The output will consist of results for the best run along with results for all other runs. These runs contain performance metrics, learned model parameters, the training pipeline used and the trained model for the respective run. This information can then be used for auditing purposes as well as to reproduce results. The results from running Automated ML can be persisted locally or in a database. However in 2018 a product called MLFlow was launched. [MLFlow](https://mlflow.org/) is an open source machine learning lifecycle management platform. Since its announcement, MLFlow has seen adoption throughout the industry and most recently [Microsoft announced native support for it inside of Azure ML](https://databricks.com/company/newsroom/press-releases/databricks-collaborates-with-microsoft-on-mlflow-open-source-project). Although MLFlow does not natively support .NET, it has a REST API that allows extensibility to non-natively supported languages. This means that if throughout your enterprise or projects, MLFlow has been adopted in Python or R applications, using the REST API you can integrate MLFlow into your ML.NET applications. In this writeup, I will go over how to automatically build an ML.NET classification model that predicts iris flower types using Automated ML and then integrate MLFlow to log the results generated by the best run. The code for this sample can be found on [GitHub](https://github.com/lqdev/MLNETMLFlowSample). ## Prerequisites This project was built on an Ubuntu 18.04 PC but should work on Windows and Mac. Note that MLFlow does not natively run on Windows at the time of this writing. To run it on Windows use [Windows Subsystem for Linux (WSL)](https://docs.microsoft.com/en-us/windows/wsl/install-win10). - [Python 3.x](https://www.python.org/downloads/) - [MLFlow](https://www.mlflow.org/docs/latest/quickstart.html) - [.NET SDK 2.x](https://dotnet.microsoft.com/download) ## Setup ### Create Solution First we\u0027ll start off by creating a solution for our project. In the terminal enter the following commands: \u0060\u0060\u0060bash mkdir MLNETMLFlowSample \u0026\u0026 cd MLNETMLFlowSample dotnet new sln \u0060\u0060\u0060 ### Create Console Application Once the solution is created, from the root solution directory, enter the following commands into the terminal to create a console application. \u0060\u0060\u0060bash dotnet new console -o MLNETMLFlowSampleConsole \u0060\u0060\u0060 Then, navigate into the newly created console application folder. \u0060\u0060\u0060bash cd MLNETMLFlowSampleConsole \u0060\u0060\u0060 ### Install NuGet Packages For this solution, you\u0027ll need the following NuGet packages: - [Microsoft.ML](https://www.nuget.org/packages/Microsoft.ML/) - [Microsoft.ML.Auto](https://www.nuget.org/packages/Microsoft.ML.AutoML/) - [MLFlow.NET](https://www.nuget.org/packages/MLFlow.NET/1.0.0-CI-20181206-054144) From the console application directory, enter the following commands into the terminal to install the packages. \u0060\u0060\u0060bash dotnet add package Microsof.ML dotnet add package Microsoft.ML.AutoML dotnet add package MLFlow.NET --version 1.0.0-CI-20181206-054144 \u0060\u0060\u0060 ### Get The Data The data used in this dataset comes from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php) and looks like the data below: \u0060\u0060\u0060text 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa \u0060\u0060\u0060 First, create a directory for the data inside the console application directory: \u0060\u0060\u0060bash mkdir Data \u0060\u0060\u0060 Then, download and save the file into the \u0060Data\u0060 directory. \u0060\u0060\u0060bash curl -o Data/iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data \u0060\u0060\u0060 Finally, make sure that the data file is copied into your output directory by adding the following section to your console application \u0060csproj\u0060 file inside the \u0060Project\u0060 tags: \u0060\u0060\u0060xml PreserveNewest \u0060\u0060\u0060 ## Configure MLFlow Service ### MLFlow.NET Settings MLFlow.NET requires two settings, the base URL where the MLFlow server listens on and the API endpoint. In this case since it will be running locally, the base URL is \u0060http://localhost:5000\u0060. In the console application directory, create a file called \u0060appsettings.json\u0060 and add the following content: \u0060\u0060\u0060json { \u0022MLFlowConfiguration\u0022: { \u0022MLFlowServerBaseUrl\u0022: \u0022http://localhost:5000\u0022, \u0022APIBase\u0022: \u0022api/2.0/preview/mlflow/\u0022 } } \u0060\u0060\u0060 To make sure that your \u0060appsettings.json\u0060 is copied into your output directory, add the following content to your \u0060csproj\u0060 file under the content tags that include the \u0060iris.data\u0060 file. \u0060\u0060\u0060xml PreserveNewest \u0060\u0060\u0060 ### Register MLFLow.NET Service In this sample, the MLFlow.NET service is registered and used via dependency injection. However, in order to use dependency injection in our console application it first needs to be configured. In the console application directory, create a new file called \u0060Startup.cs\u0060 and add the following code to it: \u0060\u0060\u0060csharp using System; using Microsoft.Extensions.Configuration; using Microsoft.Extensions.DependencyInjection; using MLFlow.NET.Lib; using MLFlow.NET.Lib.Contract; using MLFlow.NET.Lib.Model; namespace MLNETMLFlowSampleConsole { public class Startup { private readonly IConfiguration Configuration; public IServiceProvider Services; public Startup() { Configuration = new ConfigurationBuilder() .AddJsonFile(\u0022appsettings.json\u0022, false) .Build(); } public void ConfigureServices() { var services = new ServiceCollection(); // Add and configure MLFlow Service services.AddMFlowNet(); services.Configure ( Configuration.GetSection(nameof(MLFlowConfiguration)) ); Services = services.BuildServiceProvider(); } } } \u0060\u0060\u0060 The \u0060Startup\u0060 class loads configuration settings from the \u0060appsettings.json\u0060 file in the constructor. Then, the \u0060ConfigureServices\u0060 method registers the MLFlow.NET service and configures it using the settings defined in the \u0060appsettings.json\u0060 file. Once that this is set up, the service can be used throughout the application. ## Create Data Schema When working with ML.NET, it often helps to create data models or classes that define the data\u0027s schema. For this sample, there are four columns with float values which will be the input data or features. The last column is the type of iris flower which will be used as the label or the value to predict. First, create a new directory called \u0060Domain\u0060 inside the console application directory to store the data model classes. \u0060\u0060\u0060bash mkdir Domain \u0060\u0060\u0060 Inside the \u0060Domain\u0060 directory, create a new file called \u0060IrisData.cs\u0060 and add the following contents to the file: \u0060\u0060\u0060csharp using Microsoft.ML.Data; namespace MLNETMLFlowSample.Domain { public class IrisData { [LoadColumn(0, 3), VectorType(4), ColumnName(\u0022Features\u0022)] public float Features { get; set; } [LoadColumn(4), ColumnName(\u0022Label\u0022)] public string IrisType { get; set; } } } \u0060\u0060\u0060 Using attributes in the schema, we define two properties: \u0060Features\u0060 and \u0060IrisType\u0060. Data from columns in positions 0-3 will be loaded as a float vector of size four into the \u0060Features\u0060 property. ML.NET will then reference that column by the name \u0060Features\u0060. Data in the last column will be loaded into the \u0060IrisType\u0060 property and be referenced by the name \u0060Label\u0060. The reason for setting column names is ML.NET algorithms have a default column names and renaming properties at the schema level removes the need to define the feature and label columns in the pipeline. ## Create Progress Handler By default, running the application won\u0027t display progress information. However, a \u0060ProgressHandler\u0060 object can be passed into the \u0060Execute\u0060 method of an experiment which calls the implemented \u0060Report\u0060 method. Inside the console application directory, create a new file called \u0060ProgressHandler.cs\u0060 and add the following code: \u0060\u0060\u0060csharp using System; using Microsoft.ML.AutoML; using Microsoft.ML.Data; using Microsoft.ML.Trainers; namespace MLNETMLFlowSampleConsole { public class ProgressHandler : IProgress \u003E { public void Report(RunDetail runDetails) { Console.WriteLine($\u0022Ran {runDetails.TrainerName} in {runDetails.RuntimeInSeconds:0.##} with Log Loss {runDetails.ValidationMetrics.LogLoss:0.####}\u0022); } } } \u0060\u0060\u0060 The \u0060ProgressHandler\u0060 class derives from the \u0060IProgress \u0060 interface which requires the implementation of the \u0060Report\u0060 method. The object being passed into the \u0060Report\u0060 method after each run is an \u0060RunDetail \u0060 object. Each time a run is complete, the \u0060Report\u0060 method is called and the code inside it executes. ## Create Experiment Open the \u0060Program.cs\u0060 file and add the following \u0060using\u0060 statements. \u0060\u0060\u0060csharp using System; using System.IO; using System.Threading.Tasks; using Microsoft.ML; using Microsoft.ML.Data; using Microsoft.ML.AutoML; using Microsoft.Extensions.DependencyInjection; using MLNETMLFlowSampleConsole.Domain; using MLFlow.NET.Lib; using MLFlow.NET.Lib.Contract; using MLFlow.NET.Lib.Model; using MLFlow.NET.Lib.Model.Responses.Experiment; using MLFlow.NET.Lib.Model.Responses.Run; \u0060\u0060\u0060 ### Initialize Services Then, inside of the \u0060Program\u0060 class, define the \u0060IMLFlowService\u0060: \u0060\u0060\u0060csharp private readonly static IMLFlowService _mlFlowService; \u0060\u0060\u0060 Directly after that, create a constructor which is where \u0060_mlFLowService\u0060 will be instantiated. \u0060\u0060\u0060csharp static Program() { // Initialize app configuration var appConfig = new Startup(); appConfig.ConfigureServices(); // Initialize MLFlow service _mlFlowService = appConfig.Services.GetService (); } \u0060\u0060\u0060 ### Define Experiment Steps Then, add a method called \u0060RunExperiment\u0060 inside the \u0060Program\u0060 class that contains the following code: \u0060\u0060\u0060csharp public static async Task RunExperiment() { // 1. Create MLContext MLContext ctx = new MLContext(); // 2. Load data IDataView data = ctx.Data.LoadFromTextFile (\u0022Data/iris.data\u0022, separatorChar: \u0027,\u0027); // 3. Define Automated ML.NET experiment settings var experimentSettings = new MulticlassExperimentSettings(); experimentSettings.MaxExperimentTimeInSeconds = 30; experimentSettings.OptimizingMetric = MulticlassClassificationMetric.LogLoss; // 4. Create Automated ML.NET var experiment = ctx.Auto().CreateMulticlassClassificationExperiment(experimentSettings); // 5. Create experiment in MLFlow var experimentName = Guid.NewGuid().ToString(); var experimentRequest = await _mlFlowService.GetOrCreateExperiment(experimentName); // 6. Run Automated ML.NET experiment var experimentResults = experiment.Execute(data, progressHandler: new ProgressHandler()); // 7. Log Best Run LogRun(experimentRequest.ExperimentId,experimentResults); string savePath = Path.Join(\u0022MLModels\u0022, $\u0022{experimentName}\u0022); string modelPath = Path.Join(savePath, \u0022model.zip\u0022); if (!Directory.Exists(savePath)) { Directory.CreateDirectory(savePath); } // 8. Save Best Trained Model ctx.Model.Save(experimentResults.BestRun.Model, data.Schema, modelPath); } \u0060\u0060\u0060 The \u0060RunExperiment\u0060 method does the following: 1. Creates an \u0060MLContext\u0060 object. 2. Loads data from the \u0060iris.data\u0060 file into an \u0060IDataView\u0060. 3. Configures experiment to run for 30 seconds and optimize the Log Loss metric. 4. Creates a new Automated ML.NET experiment. 5. Creates a new experiment in MLFlow to log information to. 6. Runs the Automated ML.NET experiment and provide an instance of \u0060ProgressHandler\u0060 to output progress to the console. 7. Uses the \u0060LogRun\u0060 method to log the results of the best run to MLFlow. 8. Creates a directory inside the \u0060MLModels\u0060 directory using the name of the experiment and saves the trained model inside it under the \u0060model.zip\u0060 file name. ### Log Progress After the \u0060RunExperiment\u0060 method, create the \u0060LogRun\u0060 method and add the following code to it: \u0060\u0060\u0060csharp static async void LogRun(int experimentId, ExperimentResult experimentResults) { // Define run var runObject = new CreateRunRequest(); runObject.ExperimentId = experimentId; runObject.StartTime = ((DateTimeOffset)DateTime.UtcNow).ToUnixTimeMilliseconds(); runObject.UserId = Environment.UserName; runObject.SourceType = SourceType.LOCAL; // Create new run in MLFlow var runRequest = await _mlFlowService.CreateRun(runObject); // Get information for best run var runDetails = experimentResults.BestRun; // Log trainer name await _mlFlowService.LogParameter(runRequest.Run.Info.RunUuid, nameof(runDetails.TrainerName), runDetails.TrainerName); // Log metrics await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.RuntimeInSeconds), (float)runDetails.RuntimeInSeconds); await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.ValidationMetrics.LogLoss), (float)runDetails.ValidationMetrics.LogLoss); await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.ValidationMetrics.MacroAccuracy), (float)runDetails.ValidationMetrics.MacroAccuracy); await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.ValidationMetrics.MicroAccuracy), (float)runDetails.ValidationMetrics.MicroAccuracy); } \u0060\u0060\u0060 The \u0060LogRun\u0060 method takes in the experiment ID and results. Then, it does the following: 1. Configures local \u0060RunRequest\u0060 object to log in MLFlow. 2. Creates run in MLFLow using predefined configuration. 3. Logs the name of the machine learning algorithm used to train the best model in MLFlow. 4. Logs the amount of seconds it took to train the model in MLFLow. 5. Logs various performance metrics in MLFlow ### Implement Main Method Finally, in the \u0060Main\u0060 method of the \u0060Program\u0060 class, call the \u0060RunExperiment\u0060 method. \u0060\u0060\u0060csharp static async Task Main(string[] args) { // Run experiment await RunExperiment(); } \u0060\u0060\u0060 Since the \u0060Main\u0060 method in the \u0060Program\u0060 class will be async, you need to use the latest version of C#. To do so, add the following configuration inside the \u0060PropertyGroup\u0060 section of your \u0060csproj\u0060 file. \u0060\u0060\u0060xml latest \u0060\u0060\u0060 ## Run The Application ### Start MLFlow Server In the terminal, from the console application directory, enter the following command to start the MLFlow Server: \u0060\u0060\u0060bash mlflow server \u0060\u0060\u0060 Navigate to \u0060http://localhost:5000\u0060 in your browser. This will load the MLFLow UI. ![](http://cdn.lqdev.tech/files/images/ml-lifecycle-management-mlflow-automated-ml-net-1.png) ### Train Model Then, in another terminal, from the console application directory, enter the following command to run the experiment: \u0060\u0060\u0060bash dotnet build dotnet run \u0060\u0060\u0060 As the experiment is running, the progress handler should be posting output to the console after each run. \u0060\u0060\u0060text Ran AveragedPerceptronOva in 1.24 with Log Loss 0.253 Ran SdcaMaximumEntropyMulti in 3.21 with Log Loss 0.0608 Ran LightGbmMulti in 0.64 with Log Loss 0.2224 Ran FastTreeOva in 1.54 with Log Loss 0.3978 Ran LinearSvmOva in 0.25 with Log Loss 0.2874 Ran LbfgsLogisticRegressionOva in 0.36 with Log Loss 0.566 Ran SgdCalibratedOva in 0.78 with Log Loss 0.7224 Ran FastForestOva in 1.28 with Log Loss 0.125 Ran LbfgsMaximumEntropyMulti in 0.25 with Log Loss 0.4537 Ran SdcaMaximumEntropyMulti in 0.19 with Log Loss 0.4576 Ran LightGbmMulti in 0.25 with Log Loss 0.2592 Ran FastForestOva in 1.98 with Log Loss 0.0961 Ran SdcaMaximumEntropyMulti in 0.39 with Log Loss 0.108 \u0060\u0060\u0060 Navigate to \u0060http://localhost:5000\u0060 in your browser. You should then see the results of your experiment and runs there. ![](http://cdn.lqdev.tech/files/images/ml-lifecycle-management-mlflow-automated-ml-net-2.png) Inspecting the detailed view of the best run for that experiment would look like the image below: ![](http://cdn.lqdev.tech/files/images/ml-lifecycle-management-mlflow-automated-ml-net-3.png) You\u0027ll also notice that two directories were created inside the console application directory. On is an \u0060MLModels\u0060 directory, inside of which a nested directory with the name of the experiment contains the trained model. Another called \u0060mlruns\u0060. In the \u0060mlruns\u0060 directory are the results of the experiments logged in MLFlow. ## Conclusion In this writeup, we built an ML.NET classification model using Automated ML. Then, the results of the best run generated by Automated ML were logged to an MLFlow server for later analysis. Note that although some of these tools are still in their nascent stage, as open source software, the opportunities for extensibility and best practice implementations are there. Happy coding!","contentType":"posts","url":"https://www.lqdev.me/posts/ml-lifecycle-management-mlflow-automated-ml-net","date":"2019-05-09 19:19:20 -05:00","tags":["machine-learning","artificial-intelligence","mlnet","dotnet","dotnetcore","automatedml"],"summary":"## Introduction As machine learning matures, best practices are starting to be adopted. Application lifecycle management has been a common practice within software development for some time. Now,...","keywords":["mlflow","using","data","directory","model","experiment","create","application","microsoft","console"]},{"id":"posts-automate-machine-learning-service-provisioning-azure-terraform","title":"Automating Resource Provisioning for Machine Learning in Azure with Cognitive Services and Terraform","content":"## Introduction Resources in the cloud, particularly Azure can be created in several ways some of which include Powershell, Azure CLI, Azure Portal and programatically through code. Another tool that can be leveraged to create resources in Azure is [Terraform](https://www.terraform.io/). Terraform via configuration files can build and manage infrastructure across various providers such as AWS, Azure and Google. One of the benefits of Terraform is that it allows infastructure and resources to be defined as code. The configuration files serve as a blueprint as well as an execution plan of what needs to be provisioned in the cloud. Not only does this allow for automation and removal of human error in configurations but it also serves as a way to document the different changes infrastructure has gone through thoroughout the application lifecycle. The purpose of this writeup is to show how to create an environment using Terraform for an application that utilizes Azure Cognitive Services. Our application will create a Computer Vision service that we can then use via HTTP requests. Source code for this writeup can be found at the following [link](https://github.com/lqdev/azcognitiveserviceterraformsample). # Prerequisites This writeup was built and tested using a PC running Ubuntu 18.04 but should work on both Windows and Mac. It also assumes that you have an Azure account as well as Azure CLI and Terraform CLI installed. Below are links to get all of the resources needed: - [Azure Account](https://azure.microsoft.com/en-us/free/) - [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest) - [Terraform CLI](https://www.terraform.io/downloads.html) ## Log into Azure CLI The first thing we need to do is authenticate with Azure CLI so that Terraform can use your account information to create services. In the terminal, enter the following command: \u0060\u0060\u0060bash az login \u0060\u0060\u0060 ## Create Terraform Resource Configuration Script Once we\u0027re logged in, it\u0027s time to create the configuration file that Terraform will use to provision our services. First we\u0027ll need to create a directory for our application: \u0060\u0060\u0060bash mkdir azcognitiveserviceterraformsample \u0060\u0060\u0060 Then, we\u0027ll enter that directory and create our configuration file: \u0060\u0060\u0060bash cd azcognitiveserviceterraformsample touch azcomputervision.tf \u0060\u0060\u0060 Using your preferred text editor, open the newly created \u0060azcomputervision.tf\u0060 file and begin editing. ### Defining the Provider At the top of our file we\u0027ll want to configure our provider. This tells Terraform where our resources will be deployed to. In the \u0060azcomputervision.tf\u0060 file enter the following: \u0060\u0060\u0060bash # Configure Azure Provider provider \u0022azurerm\u0022 { version=\u0022=1.20.0\u0022 } \u0060\u0060\u0060 Because we\u0027ve already authenticated with the Azure CLI, there\u0027s no need to provide credentials. The only thing we need to do is specify the provider name and the version of the provider that will be used by the configuration file. For more details on how to configure the Azure provider, visit this [link](https://www.terraform.io/docs/providers/azurerm/index.html). ### Create Resource Group After defining the provider, it\u0027s time to create a Resource Group that will contain the resources we create. In the configuration file, under the provider definition, enter the following: \u0060\u0060\u0060bash # Create Resource Group resource \u0022azurerm_resource_group\u0022 \u0022terraform_cognitive_sample\u0022 { name=\u0022terraform_cognitive_sample\u0022 location=\u0022East US\u0022 } \u0060\u0060\u0060 This defines the name of our resource groups as well as where it is hosted. The syntax for resources looks like the snippet below where the \u0060type\u0060 is the type of resources as required by Terraform and the \u0060name\u0060 is any value of your choosing. \u0060\u0060\u0060bash resource \u0022type\u0022 \u0022name\u0022 { # Properties } \u0060\u0060\u0060 For more details on Resource Group configuration visit this [link](https://www.terraform.io/docs/providers/azurerm/d/resource_group.html). ### Create Azure Cognitive Service With our resource group defined, it\u0027s time to define our Cognitive Service resource. Below our Resource Group definition, enter the following: \u0060\u0060\u0060bash resource \u0022azurerm_cognitive_account\u0022 \u0022computer_vision_service\u0022 { name=\u0022computer_vision_service\u0022 resource_group_name=\u0022${azurerm_resource_group.terraform_cognitive_sample.name}\u0022 location=\u0022${azurerm_resource_group.terraform_cognitive_sample.location}\u0022 kind=\u0022ComputerVision\u0022 sku { name=\u0022F0\u0022 tier=\u0022Free\u0022 } } \u0060\u0060\u0060 Like our Resource Group, we provide a name for our service. Additional properties we need to provide are the name of the Resource Group and the location of where to deploy our service to. Because this has been previously defined in our Resource Group, Terraform allows us to access the configuration values as variables. Additionally, we need to specify the \u0060kind\u0060 of cognitive service to deploy. In our case it will be the Computer Vision service so we use \u0060ComputerVision\u0060. The last thing we need to do is specify the pricing tier to use. We\u0027ll be using the free tier for this project. There are several services and tiers to choose from. To get more details on acceptable values for these properties visit the following [link](https://www.terraform.io/docs/providers/azurerm/r/cognitive_account.html). ### Store Output Variables When our application is deployed, in order to use it we\u0027ll need the endpoint to where we will be making HTTP requests to. This can be persisted by Terraform by defining an output variable inside of the configuration file. To get the enpoint of our cognitive service, enter the following in the \u0060azcomputervision.tf\u0060 file: \u0060\u0060\u0060bash output \u0022computer_vision_endpoint\u0022 { value=\u0022${azurerm_cognitive_account.computer_vision_service.endpoint}\u0022 } \u0060\u0060\u0060 Once finished, the \u0060azcomputervision.tf\u0060 file should look like this: \u0060\u0060\u0060bash # Configure Azure Provider provider \u0022azurerm\u0022 { version=\u0022=1.20.0\u0022 } # Create Resource Group resource \u0022azurerm_resource_group\u0022 \u0022terraform_cognitive_sample\u0022 { name=\u0022terraform_cognitive_sample\u0022 location=\u0022East US\u0022 } # Create Cognitive Service resource \u0022azurerm_cognitive_account\u0022 \u0022computer_vision_service\u0022 { name=\u0022computer_vision_service\u0022 resource_group_name=\u0022${azurerm_resource_group.terraform_cognitive_sample.name}\u0022 location=\u0022${azurerm_resource_group.terraform_cognitive_sample.location}\u0022 kind=\u0022ComputerVision\u0022 sku { name=\u0022F0\u0022 tier=\u0022Free\u0022 } } # Output Cognitive Services Endpoint output \u0022computer_vision_endpoint\u0022 { value=\u0022${azurerm_cognitive_account.computer_vision_service.endpoint}\u0022 } \u0060\u0060\u0060 ## Provision Resources ### Initialize Terraform Resources When provisioning resources we first want to get the necessary plugins needed by Terraform to create such resources. Using Terraform CLI, enter the following in the terminal: \u0060\u0060\u0060bash terraform init \u0060\u0060\u0060 ### Check Execution Plan Although this is a single resource provision, it\u0027s always a good idea to see the execution plan of the resources that will be created. To do so, enter the following command in the terminal: \u0060\u0060\u0060bash terraform plan \u0060\u0060\u0060 The output should look similar to the content below: \u0060\u0060\u0060text Refreshing Terraform state in-memory prior to plan... The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage. ------------------------------------------------------------------------ An execution plan has been generated and is shown below. Resource actions are indicated with the following symbols: \u002B create Terraform will perform the following actions: \u002B azurerm_cognitive_account.computer_vision_service id: endpoint: kind: \u0022ComputerVision\u0022 location: \u0022eastus\u0022 name: \u0022computer_vision_service\u0022 resource_group_name: \u0022terraform_cognitive_sample\u0022 sku.#: \u00221\u0022 sku.0.name: \u0022F0\u0022 sku.0.tier: \u0022Free\u0022 tags.%: \u002B azurerm_resource_group.terraform_cognitive_sample id: location: \u0022eastus\u0022 name: \u0022terraform_cognitive_sample\u0022 tags.%: Plan: 2 to add, 0 to change, 0 to destroy. \u0060\u0060\u0060 ## Create Resources It\u0027s now time to create our Computer Vision resource. Enter the following command in the terminal: \u0060\u0060\u0060bash terraform apply \u0060\u0060\u0060 You will be asked to review the execution plan once again. If everything looks good, type \u0060yes\u0060 in the terminal to continue with the creation of resources. If everything deployed successfully, you should be able to see it in Azure. ## Test Provisioned Resources Using Azure CLI, enter the following command in the terminal to see whether your resource was deployed: \u0060\u0060\u0060bash az resource list --resource-group terraform_cognitive_sample --output table \u0060\u0060\u0060 That command will output something similar to the content below: \u0060\u0060\u0060bash Name ResourceGroup Location Type Status ----------------------- -------------------------- ---------- ------------------------------------ -------- computer_vision_service terraform_cognitive_sample eastus Microsoft.CognitiveServices/accounts \u0060\u0060\u0060 ### Get Resource Endpoint and Keys In order to make a request to our service, we need both the endpoint and a key to authenticate our request. To get the endpoint, we can use Terraform CLI to extract the output variable defined in our configuration file. We can do that by using the following command in the terminal: \u0060\u0060\u0060bash terraform output computer_vision_endpoint \u0060\u0060\u0060 Save the output value somewhere because that\u0027s what will be used to make a request to the Computer Vision Cognitive Service. To get the keys, we\u0027ll use Azure CLI. In the terminal, enter: \u0060\u0060\u0060bash az cognitiveservices account keys list --resource-group terraform_cognitive_sample --name computer_vision_service \u0060\u0060\u0060 This will output the keys of your deployed service. \u0060\u0060\u0060json { \u0022key1\u0022: \u0022 \u0022, \u0022key2\u0022: \u0022 \u0022 } \u0060\u0060\u0060 ### Make HTTP Request Either key can be used to make requests. To test the service, I\u0027ll be using an image from the web. All you need is to get the URL for that image. The image I will be using can be found at this URL \u0060https://upload.wikimedia.org/wikipedia/commons/1/10/Empire_State_Building_%28aerial_view%29.jpg\u0060. Using cURL, I will make a POST request to the endpoint with the key provided. One thing to note is that the endpoint is generic to many Cognitive Services. Therefore, we need to append to the path which service we will be using and the action we want to perform. In our case, we\u0027ll append the following path the the endpoint \u0060/vision/v1.0/describe\u0060. For more details, visit the Computer Vision API documentation at this [link](https://eastus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fe). To make the request, enter the following into the terminal where \u0060 \u0060 is one of the keys of your Computer Vision service. \u0060\u0060\u0060bash curl -H \u0027Ocp-Apim-Subscription-Key: \u0027 -H \u0022Content-type: application/json\u0022 -d \u0027{\u0022url\u0022:\u0022https://upload.wikimedia.org/wikipedia/commons/1/10/Empire_State_Building_%28aerial_view%29.jpg\u0022}\u0027 \u0027https://eastus.api.cognitive.microsoft.com/vision/v1.0/describe\u0027 \u0060\u0060\u0060 If on Windows, you might want to try using a REST client such as POSTMAN or Insomnia. If the request is successful, the output should be similar to the following: \u0060\u0060\u0060json { \u0022description\u0022:{ \u0022tags\u0022:[ \u0022mountain\u0022,\u0022building\u0022,\u0022city\u0022,\u0022sitting\u0022,\u0022table\u0022,\u0022view\u0022,\u0022full\u0022,\u0022filled\u0022,\u0022large\u0022,\u0022old\u0022,\u0022skyscraper\u0022,\u0022many\u0022,\u0022stacked\u0022,\u0022water\u0022,\u0022room\u0022,\u0022white\u0022 ], \u0022captions\u0022:[ { \u0022text\u0022:\u0022a view of a city\u0022,\u0022confidence\u0022:0.927147938733121 } ] }, \u0022requestId\u0022:\u0022a9aaf779-2aa1-4012-b1e0-2d2d9c20c6a5\u0022, \u0022metadata\u0022:{ \u0022width\u0022:846, \u0022height\u0022:1270, \u0022format\u0022:\u0022Jpeg\u0022 } } \u0060\u0060\u0060 ## Conclusion In this writeup I went over how to create a Terraform configuration file for automating the creation of a Computer Vision Cognitive Service resource in Azure. Although this is a simple example, using the same concepts, more complex environments and infrastructure can be provisioned just as seamlessly in a safe and efficient manner. An example of how to extend this application would be to provision an Azure Storage Account as well as an Azure Function application that utilizes the Computer Vision Cognitive Service to automatically classify photos when they are uploaded to an Azure Storage container. In such scenario, the benefits of automation and documentation that Terraform provides can leveraged to a greater extent.","contentType":"posts","url":"https://www.lqdev.me/posts/automate-machine-learning-service-provisioning-azure-terraform","date":"2019-01-05 22:58:39 -05:00","tags":["azure","cognitive services","terraform","machine learning","cloud"],"summary":"## Introduction Resources in the cloud, particularly Azure can be created in several ways some of which include Powershell, Azure CLI, Azure Portal and programatically through code. Another tool that...","keywords":["terraform","resource","cognitive","service","azure","group","computer","vision","create","\u0060\u0060\u0060bash"]},{"id":"posts-case-fsharp-machine-learning","title":"The Case for Doing Machine Learning with F#","content":"## Introduction This post is part of the 2018 [FsAdvent](https://sergeytihon.com/tag/fsadvent/) series organized by [Sergey Tihon](https://twitter.com/sergey_tihon). When searching for tools and languages to implement machine learning applications, there are numerous options to choose from each with their own set of advantages and disadvantages. Out of all, however, Python seems to be the most salient. Not only is it a popular language but also many of the tools available for machine learning are either implemented in Python or support it in some capacity whether it\u0027s native or through community libraries. Very rarely though is F# mentioned in these discussions despite having many of the features that make languages like Python so loved and extending them to empower users. In this writeup, I will do a short review of many of the advantages of Python such as succinctness, platform support, library availability as well as many others and compare it to F#\u0027s capabilities. ![2017 Top Languages](http://cdn.lqdev.tech/files/images/case-fsharp-ml-0.PNG) ## Python\u0027s Advantages ### Learning Curve One of the reasons why Python is so widely adopted is its learning curve. Whether an individual knows how to program or not, at times, Python can look like pseudocode making it accessible to not only readers but also writers. As with anything the more complex the task, the steeper the learning curve. However, at a simpler level, Python makes it as easy as possible to get started. Although at first it may not appear to be the case with F#, the learning curve is not much steeper than that of Python. The syntax can sometimes look intimidating to individuals, but the steepest part of the learning curve doesn\u0027t necessarily come from the language itself but rather from the way of reasoning about the logic of the programs. As a functional language, there is somewhat of a paradigm shift from that of a procedural execution model. Below is an example that defines a function that doubles an integer in both languages. ##### Python \u0060\u0060\u0060python def double(x): return x * 2 \u0060\u0060\u0060 ##### FSharp \u0060\u0060\u0060fsharp let double x = x * 2 \u0060\u0060\u0060 As it can be seen, despite some minor syntax and character differences the functions are essentially the same. ### Intended Purpose Depending on the task at hand, some languages are more adept for handling respective tasks. For example, R and Julia are excellent languages when performing statistical tasks. However, outside of those types of tasks their abilities are more limited. Python, being a general-purpose language means that not only can you use it for machine learning tasks but also to build n-tier applications entirely in Python without having to worry about integrations, plugins or having to learn an entirely different language to perform such actions. Similarly, F# is a general-purpose language which allows you to build web and console applications as well as machine learning applications all from the comfort of the same ecosystem. ### Strong Library Support When performing data analysis and machine learning, practicioners use a variety of libraries for their development such as [NumPy](http://www.numpy.org/) and [Pandas](https://pandas.pydata.org/) for data wrangling, [scikit-learn](https://scikit-learn.org/stable/index.html) for machine learning algorithms and [matplotlib](https://matplotlib.org/) for creating visualizations. Although all of these tasks could most certainly be implemented from scratch, libraries speed up the development process allowing practitioners to focus more on the domain and experiment with the models that best solve the respective problem they are facing. F#, like Python has exceptional library support, specifically as it regards data science and machine learning. [FsLab](https://fslab.org/) is a collection of open source F# packages for machine learning that contain libraries such as [FSharp.Data](https://fsharp.github.io/FSharp.Data/) and [Deedle](https://fslab.org/Deedle/) for data wrangling, [Math.NET Numerics](https://numerics.mathdotnet.com/) for machine learning algorithms and [XPlot](https://fslab.org/XPlot/) to help with data visualization. Furthermore, at Build 2018, Microsoft released [ML.NET](https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet), an open-source, cross-platform machine learning framework for .NET which allows .NET users to not only perform machine learning tasks within .NET but also allows extensibility via Tensorflow, ONNX and Python extensions. For a more detailed writup on using ML.NET with F#, check out the post [A Christmas Classifier](https://towardsdatascience.com/f-advent-calendar-a-christmas-classifier-72fb9e9d8f23) by Willie Tetlow or my post [Classification with F# ML.NET Models](http://luisquintanilla.me/2018/06/13/mlnet-classification-fsharp/). Libraries for F# are also not confined only to those written in F#. In many instances, because F# is a .NET language, there is interoperability with C# libraries further extending the capabilities of F#. ### Platform Support One of the things that makes Python so attractive is that it runs cross-platform. It does not matter whether you\u0027re on Windows, Mac or Linux; Python code runs the same. That being said though, not all platforms are created equal and although it is possible to run Python code on all platforms, essential libraries such as NumPy, Pandas and scikit-learn run best on Unix environments. It is possible to run them on Windows but the set up is not as straightforward. As a .NET Language, F# runs on Windows. With the help of the Mono runtime and most recently .NET Core, it also runs on Mac and Linux. Like Python, depending on the runtime and dependencies used by the respective software packages there may be limitations as to which platform code can be run on. However, from my experience most of the FsLab set of packages work cross-platform. ### Succintness Productivity is an important measure of a language. One way to achieve it is to write logic using the least number of characters. This is where Python shines. As a dynamically typed, whitespace ruled language, Python does not require developers to declare types associated with the objects and variables defined nor does it require the use of brackets and any other special characters. As expected, this allows for developers to write the same logic with less characters much faster. Unlike Python, F# is a statically typed language. However, thanks to type inference and the help of the compiler, writing programs often does not require developers to explicitly define what types objects and variables are. Additionally, it is a whitespace ruled language therefore removing the need for brackets and additional characters further speeding up the development process in a safe manner. ### Immediate Feedback Writing safe and effective code takes time and experience. However, even the most experienced developers often makes mistakes. Therefore, getting immediate feedback before adding certain logic to programs goes a long way to making code that is safe, efficient and tested. #### Read-Evaluate-Print Loop (REPL) One way in which both Python and F# provide immediate feedback is via the command line using the Read-Evaluate-Print Loop (REPL). The REPL is a programming environment that reads the user input, evaluates it and prints out the results, hence the name. Below are screenshots of what that environment looks like using both Python and F#. ##### Python ![](http://cdn.lqdev.tech/files/images/case-fsharp-ml-1.png) ##### FSharp ![](http://cdn.lqdev.tech/files/images/case-fsharp-ml-2.png) As it can be seen, getting that immediate feedback makes it easier to see whether the code is behaving the way it should. Although useful, this environment is not ideal when experimenting and making tweaks to the code. Additionally, because of the lack of a graphical user interface, the navigation can be less than ideal. Fortunately there is a solution out there that provides the same level of interactivity along with a graphical user interface that allows for ad-hoc experimentation and re-running of code at different points in time which is essential when developing machine learning applications. #### Jupyter Notebooks Much of machine learning deals with experimentation. Experimentation involves having a way to tweak parameters and evaluate the results. In addition, experiments should have a way of being documented and reproduced. As alluded to previously, such development environment and capabilities can be found in [Jupyter Notebook](https://jupyter.org/). As mentioned on the project\u0027s website: \u003E The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Jupyter Notebooks work with various languages which include but are not limited to Python and F# and can be run both on local machines as well as via a hosted service. One such service is Microsoft\u0027s [Azure Notebooks](https://notebooks.azure.com/) which allows you to use Jupyter Notebooks in the cloud for free. All you need is to have a Microsoft account. Below are screenshots of what that environment looks like in both Python and F#. ##### Python ![](http://cdn.lqdev.tech/files/images/case-fsharp-ml-3.png) ##### FSharp ![](http://cdn.lqdev.tech/files/images/case-fsharp-ml-4.png) ## Beyond Python F#, is comparable to Python on many of the features that make Python a great language for both development and machine learning. However, there are some areas where F# provides additional safety and functionality that greatly improve the way in which machine learning applications are built. ### Typing As previously mentioned, F# is a statically typed language. However, it also makes use of type inference which means that declaring types is not always required. With the help of the compiler, based on the structure of functions and variables it is possible to determine which type an object or variable is. This has several advantages. One advantage of being strongly typed is that when the types are declared the code becomes self-documented because it is easier to deduce what functions are doing based on the types being passed in and returned. Another advantage is that it makes it harder to write bad code. Having the compiler help you when you write your code allows you to find errors prior to compilation or running the code. This along with the REPL gives you additional reassurance that your code is executing the intended logic. Python is making strides in acquiring some of that functionality with the introduction of type hints in version 3.6. However, this has not always been a core feature of the language and is only in its nascent stages. ### Immutability As a functional language, immutability is something that is a native part of the language. While in some cases it can change the way in which code is written, immutability has one advantage, especially when it comes to machine learning. With immutability, parallelization can be fully exploited for those algorithms which take advantage of it. ## Conclusion In this writeup, I went over how F# is comparable to Python on many of the features that make it such a popular language such as library support, succinctness, general purpose and interactivity. However, F# has additional capabilities such as static typing and immutability that further enhance its capabilities as a language for machine learning. That is not to say that one is better than the other as they both are more adept for performing certain tasks. When it comes to machine learning it does become a matter of choice as they are both robust, powerful and strongly supported languages. Therefore next time you\u0027re looking to build a machine learning application, hopefully you give F# a try. Happy coding! ###### Resources [2018 Top Programming Languages](https://spectrum.ieee.org/at-work/innovation/the-2018-top-programming-languages) [Get Programming with F#](https://www.manning.com/books/get-programming-with-f-sharp) [ML.NET Machine Learning Samples](https://github.com/dotnet/machinelearning-samples)","contentType":"posts","url":"https://www.lqdev.me/posts/case-fsharp-machine-learning","date":"2018-12-14 23:34:50 -05:00","tags":["mlnet","machine learning","fsharp","dotnet","programming","python","ai","dotnetcore","artificial intelligence","fsadvent","artificialintelligence","machinelearning","data science","functional programming"],"summary":"## Introduction This post is part of the 2018 [FsAdvent](https://sergeytihon.com/tag/fsadvent/) series organized by [Sergey Tihon](https://twitter.com/sergey_tihon). When searching for tools and...","keywords":["python","learning","machine","language","code","which","however","languages","fsharp","allows"]},{"id":"posts-azdevops-mlnet-aci","title":"Operationalizing Machine Learning with ML.NET, Azure DevOps and Azure Container Instances","content":"## Introduction Azure DevOps, formerly known as Visual Studio Team Services (VSTS), helps individuals and organizations plan, collaborate and ship products faster. One if its noteworthy services, Azure Pipelines, helps developers build Continuous Integration (CI) and Continuous Delivery (CD) pipelines that automate and standardize the build, test and deploy phases of the software development process. In addition, Azure Pipelines provides native container support and works with any language, platform and cloud. Machine learning like software development is also a process that includes a build, test and deploy phase which makes it a good candidate for automation and standardization. At Build 2018, Microsoft announced [ML.NET](https://github.com/dotnet/machinelearning), an open-source, cross-plaform machine learning framework for .NET. If we were to put all of these tools and services together, it means that we can automate and standardize the training of a machine learning model built with ML.NET, package it into a Docker container and deploy it to Azure Container Instances (ACI). In this writeup, I will go through the process of building a CI/CD pipeline in Azure Devops that trains, packages and deploys an ML.NET machine learning model to predict which class an Iris flower belongs to using a variety of measurements. Source code for this project can be found at this [link](https://github.com/lqdev/mlnetazdevopssample). ## Prerequisites - [Git](https://git-scm.com/) - [GitHub Account](https://github.com/) - [.NET Core SDK](https://www.microsoft.com/net/download) - [Azure Account](https://azure.microsoft.com/en-us/free/) ## The Application Because the purpose of this post is to demonstrate the functionality of Azure Devops and not that of ML.NET, I\u0027ll start with a pre-built application. For some more information and detail into the functionality of ML.NET, check out the official documentation [page](https://docs.microsoft.com/en-us/dotnet/machine-learning/) as well as some of my previous posts: - [Serverless Machine Learning with ML.NET and Azure Functions](http://luisquintanilla.me/2018/08/21/serverless-machine-learning-mlnet-azure-functions/) - [Deploy .NET Machine Learning Models with ML.NET, ASP.NET Core, Docker and Azure Container Instances](http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/). The application used in this writeup contains three .NET Core projects within it. One is a class library which is what we\u0027ll use to wrap ML.NET functionality for training models as well as loading pre-trained models that will then be used to make predictions. Another is a .NET Core console application which references the class library to train and persist an ML.NET model. Finally, there\u0027s the ASP.NET Core Web API which also references the class library application to load the pre-trained model created by the console application and then makes predictions via HTTP. This application can be utilized and deployed standalone but in this writeup it will be packaged into a Docker image that will then be deployed to Azure Container Instances. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-1.png) ### Class Library The class library can be found in the \u0060MLModel\u0060 directory. The class library defines the observation and prediction data classes which can be found in the \u0060IrisData.cs\u0060 and \u0060IrisPrediction.cs\u0060 files respectively. Additionally, the \u0060Model\u0060 class contains helper methods that train and save machine learning models, load pre-trained models and use these models to make predictions. ### Console Application In the solution directory we also have a console application in the \u0060ModelTrainer\u0060 directory. This application references the class library in the \u0060MLModel\u0060 directory to train and persist the machine learning model. ### API The \u0060ModelApi\u0060 directory contains an ASP.NET Core Web API application that references the \u0060MLModel\u0060 class library project to load the pre-trained model that is trained by the \u0060ModelTrainer\u0060 console application and makes predictions via HTTP. The logic for making predictions can be found in the \u0060PredictController.cs\u0060 class in the \u0060Controllers\u0060 directory of the \u0060ModelApi\u0060 application. ## CI/CD Pipeline Flow Conceptually, when the application is built and deployed manually, the machine learning model is defined and developed inside the \u0060MLModel\u0060 class library. Once satisfied with the model, the class library is built. The console application which references the \u0060MLModel\u0060 class library is built as well as run to train and persist a classification model in a file called \u0060model.zip\u0060. The \u0060MLModel\u0060 class library is also referenced in the \u0060ModelApi\u0060 ASP.NET Core project. Because \u0060ModelApi\u0060 is the application we\u0027re looking to deploy in order to expose our pre-trained machine learning model, we need to find a way to package it for deployment. We\u0027ll be deploying \u0060ModelApi\u0060 using Azure Container Instances which means we need to create a Docker image of the project that will then be pushed to a Docker registry where it will be made available for public consumption. The building of multiple projects as well as the building, publishing and deployment of the Docker image to Azure Container Instances can be standardized and automated using Azure DevOps. The rest of this write-up will focus on demonstrating step-by-step how to operationalize this machine learning application via CI/CD pipelines in Azure DevOps using Azure Pipelines. ### Getting The Code Before getting started, the first thing you\u0027ll want to do is fork the [mlnetazdevopssample](https://github.com/lqdev/mlnetazdevopssample) GitHub repository into your own GitHub account. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-2.png) ### Creating the Project Navigate to [https://devops.azure.com](https://devops.azure.com), click \u0060Start Free\u0060 and follow the prompts to either create a new account or sign into your existing account. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-3.png) Once logged in, click \u0060Create Project\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-4.png) Enter the name of your project as well as a short description. Then, click \u0060Create\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-5.png) ## The Continuous Integration (CI) Pipeline Using Azure Pipelines, we\u0027ll configure a CI pipeline for the build and packaging steps of our application. Below is an illustration of all the steps involved in our CI pipeline: ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-6.png) 1. Build the class library application 2. Build the console application 3. Train and persist the ML.NET Model by running the console application. 4. Copy ML.NET model file created by console application into ASP.NET Core Web API application directory 5. Build ASP.NET Core Web API application 6. Build Docker image 7. Push Docker image to Docker Hub ### CI Pipeline Setup Once the project is created, in the main project page, hover over \u0060Pipelines\u0060 and click on \u0060Builds\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-7.png) In the \u0060Builds\u0060 pipeline page, click \u0060New pipeline\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-8.png) Select GitHub as the source and connect your GitHub account with Azure DevOps. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-9.png) Once you have authorized Azure DevOps to use your GitHub account, select the repository and branch that will be used for this build pipeline. In our case, we\u0027ll be using the master branch of the \u0060mlnetazdevopssample\u0060 repository. When finished configuring, click \u0060Continue\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-10.png) The next step will be to select the jobs to execute in our pipeline. Because there are multiple steps in this build pipeline, let\u0027s start with an \u0060Empty Job\u0060 and customize it to our needs. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-11.png) From inside the build pipeline page, before we start adding jobs, lets select the agent that will execute the jobs. For this pipeline, select the \u0060Hosted Ubuntu 1604\u0060 option from the dropdown. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-12.png) ### 1. Build the Class Library Application The first step in our CI Pipeline will be to build our class library which contains methods that wrap the training, loading and prediction functionality of the ML.NET framework and persisted models. To achieve that, we\u0027ll add a .NET Core task to our \u0060Agent Job 1\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-13.png) Once added to the pipeline, let\u0027s configure this task. To make it more descriptive, we can give it a name such as \u0060Build Class Library\u0060. Because this task will be responsible for building the .NET Core class library, we\u0027ll leave the default \u0060build\u0060 Command setting as is. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-14.png) The other setting we\u0027ll want to configure is the \u0060Working Directory\u0060. We can do so by clicking the \u0060Advanced\u0060 tab. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-15.png) For this task we\u0027ll use the \u0060MLModel\u0060 directory. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-16.png) When finished with the configuration, click \u0060Save \u0026 Queue\u0060 -\u003E \u0060Save\u0060 on the top toolbar. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-17.png) Enter a detailed comment describing the change to the pipeline and click \u0060Save\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-18.png) ### 2. Building The Console Application Once we\u0027ve built the class library application which we\u0027ll reference from the .NET Core console and ASP.NET Core Web API applications, it\u0027s time to build the console application which will serve the purpose of training and persisting the ML.NET model. Similar to the previous step, add a new .NET Core *build* task to the pipeline. The only setting that will change for this task is the \u0060Working Directory\u0060 which will have the value of \u0060ModelTrainer\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-19.png) Although not required, when finished configuring the task, click \u0060Save \u0026 Queue\u0060 -\u003E \u0060Save\u0060 to save and comment the changes to the pipeline. ### 3. Train and persist the ML.NET Model Now that our console application is built, it\u0027s time to run it in order to train and persist the ML.NET model. To do so, we\u0027ll add another .NET Core task. The difference is that the \u0060Command\u0060 setting will now be configured with the \u0060run\u0060 value. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-20.png) The \u0060Working Directory\u0060 will be set to \u0060ModelTrainer\u0060 like in the previous task. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-21.png) Remember to save and comment the new changes to the pipeline. ### 4. Copy ML.NET Model to Web API Directory After the console application is run and the ML.NET model is trained, it is persisted in a file called \u0060model.zip\u0060 inside the \u0060ModelTrainer\u0060 directory. We can use this persisted version of the model to make predictions from both the console application or any other application of our choice. In this case, we\u0027ll be making predictions via an ASP.NET Core Web API. In order for our API to reference this file, we need to copy it into the root directory of our \u0060ModelApi\u0060 directory. A way to perform that task is via bash script. To add a bash script to our pipeline, all we need to do is add a Bash task to it. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-22.png) Once added to our pipeline, it\u0027s time to configure the task. We\u0027ll set the \u0060Type\u0060 setting to \u0060Inline\u0060 which will bring up a text box for us to type in the script. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-23.png) Inside of the text box, enter the following content: \u0060\u0060\u0060bash # Write your commands here cp ../ModelTrainer/model.zip . # Use the environment variables input below to pass secret variables to this script \u0060\u0060\u0060 This command will copy the \u0060model.zip\u0060 file from the \u0060ModelTrainer\u0060 directory to the \u0060ModelApi\u0060 directory. We can set the \u0060Working Directory\u0060 of this step to \u0060ModelApi\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-24.png) Once finished, save and comment the new changes to the pipeline. ### 5. Build ASP.NET Core Web API application Now that we have the necessary files inside our \u0060ModelApi\u0060 application, it\u0027s time to build it. We\u0027ll add a .NET Core task to our pipeline and set the \u0060Command\u0060 to \u0060build\u0060. The \u0060Working Directory\u0060 will be \u0060ModelApi\u0060 like the previous task. Save and comment the new changes to the pipeline when finished. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-25.png) ### 6. Build ASP.NET Core Web API Docker Image The method of deployment for the ASP.NET Core Web API application is via containers. Therefore, after building the application, we have to build a Docker image for it that can then be pushed to a Docker registry of your choice. To build a Docker image, we\u0027ll add a Docker task to our pipeline. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-26.png) When we configure the task, we\u0027ll start off by setting the \u0060Container Registry Type\u0060 to \u0060Container Registry\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-27.png) This will prompt the setup of a service connection to a Docker registry if one does not already exist. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-28.png) The Docker registry type we\u0027ll be using is Docker Hub. Give the connection a name, enter the credentials to your Docker Hub account and click \u0060Verify this connection\u0060 to make sure that your credentials are valid and a connection can be established with Docker Hub. When finished click \u0060OK\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-29.png) The \u0060Command\u0060 setting will be \u0060build\u0060 so we can leave the default as is as well as the \u0060Dockerfile\u0060 setting which will use the Dockerfile in the root \u0060mlnetazdevopssample\u0060 directory. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-30.png) Finally, we\u0027ll configure the \u0060Image name\u0060 setting. The convention we\u0027ll use is \u0060 / \u0060. In my case, \u0060lqdev\u0060 is my Docker Hub username and I\u0027ll name my image \u0060mlnetazdevopssample\u0060 resulting in \u0060lqdev/mlnetazdevopssample\u0060. Additionally, check the \u0060Include latest tag\u0060 checkbox to have every build be the latest as opposed to tagging it with versions numbers. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-31.png) Remember to save and comment the recent changes to the pipeline. ### 7. Push Docker Image to Docker Hub The last step in our CI pipeline is to push our newly built image to Docker Hub. To do so we\u0027ll use anoter Docker task. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-32.png) Like in the previous task, we\u0027ll set the \u0060Container registry type\u0060 to \u0060Container Registry\u0060. Set the \u0060Docker registry service connection\u0060 to the most recently created connection by selecting it from the dropdown. We\u0027ll be changing our \u0060Command\u0060 to \u0060push\u0060 and set the \u0060Image name\u0060 to the name of the image built in the previous step. The naming convention is \u0060 / :latest\u0060. The latest tag was added by our previous Docker build task so make sure that you include it in this task. Once finished, click \u0060Save \u0026 Queue\u0060 -\u003E \u0060Save \u0026 Queue\u0060. As opposed to only clicking \u0060Save\u0060, this action will manually trigger the CI pipeline. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-33.png) Don\u0027t forget to comment your changes and click \u0060Save \u0026 queue\u0060 to kick off the CI pipeline. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-34.png) ### Monitoring the Build When the build starts, you can click on \u0060Builds\u0060 under the \u0060Pipelines\u0060 section on the left pane. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-35.png) Select the first build from the list to get more details on the build. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-36.png) This will take you to the logs which show the status of the pipeline near real-time. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-37.png) ### Confirming CI Pipeline Success If the build is successful, navigate to [https://hub.docker.com/](https://hub.docker.com/) to check whether the Docker image was pushed to the registry. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-38.png) ## The Continuous Delivery (CD) Pipeline Now that we have our CI pipeline set up which will build and package our application, it\u0027s time to deploy it. We could do this ourselves or automate it using a CD pipeline. Our application wil be deployed to Azure Container Instances which is an Azure service that offers a quick way to run containers without having to worry about the management of virtual machines or orchestration services. The steps involved in our CD pipeline are the following: 1. Create Azure Resource Group for deployment 2. Deploy application to Azure Container Instances. ### CD Pipeline Setup To get started setting up a CD pipeline, from the Azure DevOps project main page, hover over \u0060Pipelines\u0060 and click on \u0060Releases\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-39.png) Once in that page, click on \u0060New pipeline\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-40.png) As with our CI pipeline, we\u0027ll start off with an \u0060Empty Job\u0060 which we\u0027ll configure at a later time. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-41.png) ### Triggering Deployments Once our pipeline is created, it\u0027s time to configure it. The first thing we\u0027ll want to do is add an artifact. An artifact can be a variety of things including the output of our build pipeline. In our case, the end our CI pipeline will be the trigger for our CD pipeline. To add an artifact, click \u0060Add an artifact\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-42.png) In the configuration form, set the \u0060Source type\u0060 to \u0060Build\u0060 and the \u0060Source\u0060 to the name of the CI pipeline created in the previous steps. When finished, click \u0060Add\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-43.png) After configuring our artifact, it\u0027s time to configure the steps in the CD pipeline. To do so, click on the \u0060Stage 1\u0060 option in the \u0060Stages\u0060 section of the release pipeline page and change the name to something more descriptive. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-44.png) When finished, close out the form and click on the hyperlink below the stage title. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-45.png) You should now be on a page similar to the CI pipeline job configuration page. On this page, we\u0027ll want to click on the \u0060Agent Job\u0060 panel to set the \u0060Agent pool\u0060 setting to \u0060Hosted Ubuntu 1604\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-46.png) Once that is complete, it\u0027s time to configure the tasks in the CD pipeline. ### 1. Create Azure Resource Group Start off adding an \u0060Azure CLI\u0060 task to the pipeline. In this task we\u0027ll create a resource group in Azure to which we\u0027ll deploy our application to. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-47.png) Before doing anything else, link DevOps to an Azure Subscription by selecting one from the dropdown and clicking \u0060Authorize\u0060 which will prompt you to authenticate your subscription. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-48.png) Once an Azure subscription has been linked, let\u0027s change the \u0060Script Location\u0060 setting to \u0060Inline Script\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-49.png) In the \u0060Inline Script\u0060 text box enter the following: \u0060\u0060\u0060bash az group create --name mlnetazdevopssampleresourcegroup --location eastus \u0060\u0060\u0060 This script will create a resource group in Azure called \u0060mlnetazdevopssampleresourcegroup\u0060 that is located in \u0060eastus\u0060. Both of these are configurable to your preference. ### 2. Deploy Docker Image to Azure Container Instances The next and final step in our CD pipeline is the deployment to Azure Container Instances. To deploy our application, we\u0027ll add another \u0060Azure CLI\u0060 task. This time, since we already configured our \u0060Azure subscription\u0060 in the previous task, we can select the service connection as opposed to a subscription from the dropdown. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-50.png) Like in the previous task, our script will be inline. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-51.png) In the \u0060Inline Script\u0060 text box enter the following: \u0060\u0060\u0060bash az container create --resource-group mlnetazdevopssampleresourcegroup --name mlnetcontainer --image lqdev/mlnetazdevopssample:latest --ports 80 --ip-address public \u0060\u0060\u0060 This script creates a container in the resource group created by the previous task of the pipeline with the name \u0060mlnetcontainer\u0060 from the Docker image that was pushed to Docker Hub by the CI pipeline. Additionally, it opens up port 80 and assigns a publicly accessible IP address for the container to be accessed externally. Once this step has been configured, make sure to save and comment all your changes by clicking \u0060Save\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-52.png) ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-53.png) Then, to make it easily recognizable, edit the name of the pipeline by hovering near \u0060New release pipeline\u0060 and clicking on the pencil icon. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-54.png) Make sure to save and comment your changes. ## Automating CI/CD Pipelines In the previous steps, we configured CI and CD pipelines. However, we have still not fully automated the triggers that kick off both of these. ### CI Pipeline Trigger First, lets start off by automating the CI pipeline. To do so, go the project\u0027s main page, hover over \u0060Pipelines\u0060 and click on \u0060Builds\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-55.png) This will take you to the CI pipeline page. While on this page, click \u0060Edit\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-56.png) Then, click on \u0060Triggers\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-57.png) Once on this page, check the \u0060Enable continous integration\u0060 checkbox and save and comment your changes by clicking \u0060Save \u0026 Queue\u0060 -\u003E \u0060Save\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-58.png) ### CD Pipeline Trigger To automate the CD pipeline trigger, click on \u0060Releases\u0060 under the \u0060Pipelines\u0060 page to automate the CD pipeline. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-59.png) Once on the CD pipeline\u0027s page, click \u0060Edit\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-60.png) Then, click on the lightning icon in the Artifacts section which will show a configuration form. In this form, toggle the \u0060Continuous deployment trigger\u0060 setting to \u0060Enabled\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-61.png) When finished, save and comment your changes. ## Running CI/CD Pipelines Although going forward builds and deployments will be started when new changes are checked into the master branch of the \u0060mlnetazdevopssample\u0060 repository, for demonstration purposes we will manually kick off the CI/CD pipelines we have just configured. To do so, click on \u0060Builds\u0060 under the \u0060Pipelines\u0060 section on the left pane. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-62.png) From the CI pipeline page click \u0060Queue\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-63.png) This will prompt a modal to show up in which you can just click \u0060Queue\u0060 to start the build. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-64.png) This will kick off a new CI build which subsequently will also kick off the CD pipeline of your application. ## Testing The Deployment If all is successful, a Docker image of an ASP.NET Core Web API application will be deployed to Azure Container Instances which can be accessed via a public IP address. To see whether the deployment worked, navigate to [https://portal.azure.com/](https://portal.azure.com/) and click on \u0060Resource groups\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-65.png) At this point, you should see the resource group that was created by the CD pipeline. If that\u0027s the case, click on it. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-66.png) This will then show a page that displays the container that was deployed to this resource group. Click on that. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-67.png) The container page will display diagnostic and configuration information about the container. The information we\u0027re interested in is the \u0060IP address\u0060. Hover to the right of it and click on the icon that says \u0060Click to copy\u0060. This will copy the address to the clipboard. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-68.png) In an application like Postman or Insomnia, make an HTTP POST request to \u0060http:// /api/predict\u0060 where \u0060ip-address\u0060 is the public IP address of the container in Azure with the following body. \u0060\u0060\u0060json { \u0022SepalLength\u0022:3.3, \u0022SepalWidth\u0022:1.6, \u0022PetalLength\u0022:0.2, \u0022PetalWidth\u0022:5.1 } \u0060\u0060\u0060 If successful, the response will be \u0060Iris-virginica\u0060. ![](http://cdn.lqdev.tech/files/images/azdevops-mlnet-69.png) ## Conclusion In this writeup, we operationalized the building, packaging and deployment of an ML.NET application that predicts the class of an Iris flower using a variety of mesurements with Azure DevOps. We created both a Continous Integration as well as a Continous Delivery pipeline which deploys the Docker image of an ASP.NET Core Web API to Azure Container Instances. Keep in mind this is just one way of doing it and Azure DevOps is flexible in how all of these tasks and workflows are configured to meet your requirements. Happy coding! ###### Resources [ML.NET Samples](https://github.com/dotnet/machinelearning-samples) [DevOps for Data Science](https://www.youtube.com/watch?v=bUTBBS1TECc)","contentType":"posts","url":"https://www.lqdev.me/posts/azdevops-mlnet-aci","date":"2018-11-26 23:50:23 -05:00","tags":["mlnet","machinelearning","ai","dotnet","dotnetcore","devops","azure","docker","containers","microsoft","artificialintelligence","programming","webapi","aci","development"],"summary":"## Introduction Azure DevOps, formerly known as Visual Studio Team Services (VSTS), helps individuals and organizations plan, collaborate and ship products faster. One if its noteworthy services,...","keywords":["mlnet","lqdev","tech/files/images/azdevops","http://cdn","pipeline","application","azure","docker","click","build"]},{"id":"posts-blog-tools","title":"Blogging Tools","content":"## TLDR These are the tools I use to write this blog: - **Editor**: [Visual Studio Code](https://code.visualstudio.com/) - **Markup**: [Markdown](https://daringfireball.net/projects/markdown/) - **Static Site Generator**: [Hexo](https://hexo.io/) - **Blog Theme**: [Cactus](https://github.com/probberechts/hexo-theme-cactus) - **Hosting**: [Namecheap](https://www.namecheap.com/) ## Introduction I have gotten a few questions regarding what I use to write this blog so I thought it would be a good idea to write a post about it. In this writeup I\u0027ll discuss what I use for writing posts as well as building and hosting the site. ## Writing ### Editor For writing, I use Visual Studio Code. Visual Studio Code is an open source, cross-platform text editor. One of the nice features it includes is Markdown Preview. Markdown Preview allows you to see how the content would look when rendered as a web page. Additionally, Visual Studio Code allows you to split panes. What that provides is the ability to write and preview the content side-by-side. ![](http://cdn.lqdev.tech/files/images/blog-tools-1.PNG) ### Markup I write my content in Markdown. Markdown is a plain text formatting syntax that allows authors to write easy-to-read and easy-to-write plain text formats that are then converted to HTML. It can take a while to get used to the syntax especially when including images, links and code snippets but once you get comfortable with it, it becomes natural and no different than writing sentences in plain text. While this is not a requirement for everyone, since a fair amount of my content includes code snippets, Markdown makes it easy to include such content in the language of choice. ## Website ### Static Site Generation While Markdown helps me format my plain text, at some point it needs to be converted into HTML or some format that can be rendered on the web. To help with that, I use Hexo. Hexo is an open source blog framework that allows you to write content using Markdown or other languages and generates static files which have a theme applied to them. Hexo is built on JavaScript, therefore you\u0027ll need NodeJS to configure it. ### Theme Hexo has numerous themes that can be used to style your site. The one I use is the \u0060Cactus\u0060 theme. ## Hosting For hosting, I use Namecheap. There is no special reason for this other than I had a slot available in an existing hosting plan. My method of deployment is via FTP. Since my site is all static assets, I upload them into a folder that is publicly accessible. With that in mind, by having my site made up of static assets I have the flexibility of hosting it on an S3 bucket or Azure Storage. This is the method I\u0027m looking to use for deployments and hosting in the near future. Here is a [link](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-static-website) that details how to host a static site on Azure Storage.","contentType":"posts","url":"https://www.lqdev.me/posts/blog-tools","date":"2018-11-18 00:07:36 -05:00","tags":["markdown","vscode","tools","development","blogging"],"summary":"## TLDR These are the tools I use to write this blog: - **Editor**: [Visual Studio Code](https://code.visualstudio.com/) - **Markup**: [Markdown](https://daringfireball.net/projects/markdown/) -...","keywords":["write","markdown","site","content","static","hosting","code","text","hexo","theme"]},{"id":"posts-serverless-machine-learning-mlnet-azure-functions","title":"Serverless Machine Learning with ML.NET and Azure Functions","content":"## Introduction In a previous blog [post](http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/), I explored how to build and deploy machine learning models built with the \u0060ML.NET\u0060 framework using an ASP.NET Core Web API, Docker and Azure Container Instances. While this is certainly a good way to deploy such models especially those that are critical and require high availability and/or consist of long-running processes, it\u0027s not the case when those requirements are not needed. In such cases serverless computing makes more sense from a cost and resource utilization standpoint. Therefore, in this blog post I will go over how to train a classification model with \u0060ML.NET\u0060 and deploy it using Azure Functions. Source code for this post can be found at the following [link](https://github.com/lqdev/azfnmlnetdemo). ## Prerequisites Prior to starting, make sure you have all of the necessary software to build this project. Although this project was built on a system running Ubuntu 16.04 it should work cross-platform. - [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest) - [Azure Functions Core Tools Version 2.x](https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local) - [.NET Core SDK 2.0\u002B](https://www.microsoft.com/net/download) ## Set Up Azure Environment Before writing any code we want to configure our Azure environment. To do so we\u0027ll be using the Azure CLI. Although in these examples I am providing the resource group name, storage account name and function application name feel free to use one of your choosing. Naming is not as important for resource group or storage account but definitely is the case for the application. Fist we want to log into our account using the following command \u0060\u0060\u0060bash az login \u0060\u0060\u0060 This will guide you through a series of prompts that will eventually result in you being logged in. To make sure you are logged in you can use the \u0060account\u0060 command. \u0060\u0060\u0060bash az account list \u0060\u0060\u0060 The following output should appear if successfull. \u0060\u0060\u0060bash [ { \u0022cloudName\u0022: \u0022AzureCloud\u0022, \u0022id\u0022: \u0022 \u0022, \u0022isDefault\u0022: true, \u0022name\u0022: \u0022Pay-As-You-Go\u0022, \u0022state\u0022: \u0022Enabled\u0022, \u0022tenantId\u0022: \u0022 \u0022, \u0022user\u0022: { \u0022name\u0022: \u0022 \u0022, \u0022type\u0022: \u0022user\u0022 } } ] \u0060\u0060\u0060 Next, we want to create a resource group to contain all of our Azure resources for this application. \u0060\u0060\u0060bash az group create --name azfnmlnetdemo --location eastus \u0060\u0060\u0060 Once our resource group is created, it\u0027s time to start adding resources for it. First we\u0027ll add a storage account which will contain our trained model. \u0060\u0060\u0060bash az storage account create --name azfnmlnetdemostorage --location eastus --resource-group azfnmlnetdemo --sku Standard_LRS \u0060\u0060\u0060 Then we\u0027ll create an Serverless Function Application and link it to our storage account. We\u0027ll want to create a unique name for it. An easy way to do so is to add the date to the end of the name of our application (i.e. myappname20180816). \u0060\u0060\u0060bash az functionapp create --name azfnmlnetdemo20180821 --storage-account azfnmlnetdemostorage --consumption-plan-location eastus --resource-group azfnmlnetdemo \u0060\u0060\u0060 The final step in the environment setup is to set the runtime of our Serverless Function Application in the Application Settings to \u0060beta\u0060 which supports \u0060.NET Core\u0060. \u0060\u0060\u0060bash az functionapp config appsettings set --name azfnmlnetdemo20180821 --resource-group azfnmlnetdemo --settings FUNCTIONS_EXTENSION_VERSION=beta \u0060\u0060\u0060 Now we\u0027re ready to build our machine learning model and upload it to our storage account ## Building The Model Once our environment is set up we can start building our solution. The first step is to create a directory and initialize our solution inside of it. ### Set Up The Solution \u0060\u0060\u0060bash mkdir azfnmlnetdemo cd azfnmlnetdemo dotnet new sln \u0060\u0060\u0060 ### Create The Model Project Then we want to create a console project for our model and add it to our solution. \u0060\u0060\u0060bash dotnet new console -o model dotnet sln add model/model.csproj \u0060\u0060\u0060 ### Add Dependencies Since we\u2019ll be using the \u0060ML.NET\u0060 framework, we need to add it to our model project. \u0060\u0060\u0060 cd model dotnet add package Microsoft.ML dotnet restore \u0060\u0060\u0060 ### Download The Data Before we start training the model, we need to download the data we\u2019ll be using to train. We do so by creating a directory called \u0060data\u0060 and downloading the data file onto there. \u0060\u0060\u0060bash mkdir data curl -o data/iris.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data \u0060\u0060\u0060 If we take a look at the data file, it should look something like this: \u0060\u0060\u0060bash 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 5.4,3.9,1.7,0.4,Iris-setosa 4.6,3.4,1.4,0.3,Iris-setosa 5.0,3.4,1.5,0.2,Iris-setosa 4.4,2.9,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa \u0060\u0060\u0060 ### Train The Model Now that we have all our dependencies set up, it\u2019s time to train our model. I leveraged the demo that is used on the [ML.NET Getting-Started website](https://www.microsoft.com/net/learn/machine-learning-and-ai/get-started-with-ml-dotnet-tutorial). #### Define Data Structures In the root directory of our \u0060model\u0060 project, let\u2019s create two classes called \u0060IrisData\u0060 and \u0060IrisPrediction\u0060 which will define our features and predicted attribute respectively. Both of them will use \u0060Microsoft.ML.Runtime.Api\u0060 to add the property attributes. Here is what our \u0060IrisData\u0060 class looks like: \u0060\u0060\u0060csharp using Microsoft.ML.Runtime.Api; namespace model { public class IrisData { [Column(\u00220\u0022)] public float SepalLength; [Column(\u00221\u0022)] public float SepalWidth; [Column(\u00222\u0022)] public float PetalLength; [Column(\u00223\u0022)] public float PetalWidth; [Column(\u00224\u0022)] [ColumnName(\u0022Label\u0022)] public string Label; } } \u0060\u0060\u0060 Similarly, here is the \u0060IrisPrediction\u0060 class: \u0060\u0060\u0060csharp using Microsoft.ML.Runtime.Api; namespace model { public class IrisPrediction { [ColumnName(\u0022PredictedLabel\u0022)] public string PredictedLabels; } } \u0060\u0060\u0060 #### Build the Training Pipeline The way the \u0060ML.NET\u0060 computations process data is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. Therefore, we can create a class called \u0060Model\u0060 to perform all of these tasks for us. \u0060\u0060\u0060csharp using Microsoft.ML.Data; using Microsoft.ML; using Microsoft.ML.Runtime.Api; using Microsoft.ML.Trainers; using Microsoft.ML.Transforms; using Microsoft.ML.Models; using System; using System.Threading.Tasks; namespace model { class Model { public static async Task \u003E Train(LearningPipeline pipeline, string dataPath, string modelPath) { // Load Data pipeline.Add(new TextLoader(dataPath).CreateFrom (separator:\u0027,\u0027)); // Transform Data // Assign numeric values to text in the \u0022Label\u0022 column, because // only numbers can be processed during model training pipeline.Add(new Dictionarizer(\u0022Label\u0022)); // Vectorize Features pipeline.Add(new ColumnConcatenator(\u0022Features\u0022, \u0022SepalLength\u0022, \u0022SepalWidth\u0022, \u0022PetalLength\u0022, \u0022PetalWidth\u0022)); // Add Learner pipeline.Add(new StochasticDualCoordinateAscentClassifier()); // Convert Label back to text pipeline.Add(new PredictedLabelColumnOriginalValueConverter() {PredictedLabelColumn = \u0022PredictedLabel\u0022}); // Train Model var model = pipeline.Train (); // Persist Model await model.WriteAsync(modelPath); return model; } } } \u0060\u0060\u0060 In addition to building our pipeline and training our machine learning model, the \u0060Model\u0060 class also serialized and persisted the model for future use in a file called \u0060model.zip\u0060. ### Test The Model Now that we have our data structures and model training pipeline set up, it\u2019s time to test everything to make sure it\u2019s working. We\u2019ll put our logic inside of our \u0060Program.cs\u0060 file. \u0060\u0060\u0060csharp using System; using Microsoft.ML; namespace model { class Program { static void Main(string[] args) { string dataPath = \u0022model/data/iris.txt\u0022; string modelPath = \u0022model/model.zip\u0022; var model = Model.Train(new LearningPipeline(),dataPath,modelPath).Result; // Test data for prediction var prediction = model.Predict(new IrisData() { SepalLength = 3.3f, SepalWidth = 1.6f, PetalLength = 0.2f, PetalWidth = 5.1f }); Console.WriteLine($\u0022Predicted flower type is: {prediction.PredictedLabels}\u0022); } } } \u0060\u0060\u0060 All set to run. We can do so by entering the following command from our solution directory: \u0060\u0060\u0060bash dotnet run -p model/model.csproj \u0060\u0060\u0060 Once the application has been run, the following output should display on the console. \u0060\u0060\u0060bash Automatically adding a MinMax normalization transform, use \u0027norm=Warn\u0027 or \u0027norm=No\u0027 to turn this behavior off.Using 2 threads to train. Automatically choosing a check frequency of 2.Auto-tuning parameters: maxIterations = 9998. Auto-tuning parameters: L2 = 2.667734E-05. Auto-tuning parameters: L1Threshold (L1/L2) = 0.Using best model from iteration 882. Not training a calibrator because it is not needed. Predicted flower type is: Iris-virginica \u0060\u0060\u0060 Additionally, you\u2019ll notice that a file called \u0060model.zip\u0060 was created in the root directory of our model project. This persisted model can now be used outside of our application to make predictions, but first we need to upload it to our Azure Storage account. ### Upload The Model Now that we have a trained model and it has been persisted to the \u0060model.zip\u0060 file, it\u0027s time to upload it to Azure Storage so that it is available to our Azure Functions application. To get started with that, first we need the access keys for our storage account. You can get those by using the following command. \u0060\u0060\u0060bash az storage account keys list --account-name azfnmlnetdemostorage --resource-group azfnmlnetdemo \u0060\u0060\u0060 The result of that command should return your primary and secondary keys. You can use either one for the following steps. Although we can upload directly to the account, it\u0027s best to create a container to upload our model to. To keep it simple, I\u0027ll call the container \u0060models\u0060. \u0060\u0060\u0060bash az storage container create --name models --account-key --account-name azfnmlnetdemostorage --fail-on-exist \u0060\u0060\u0060 Once our container\u0027s created, we can upload our \u0060model.zip\u0060 file to it. \u0060\u0060\u0060bash az storage blob upload --container-name models --account-name azfnmlnetdemostorage --file model/model.zip --name model.zip \u0060\u0060\u0060 To verify that the file has been uploaded, you can list the files inside the \u0060models\u0060 storage container. \u0060\u0060\u0060bash az storage blob list --container-name models --account-name azfnmlnetdemostorage --output table \u0060\u0060\u0060 That command should produce output similar to that below: \u0060\u0060\u0060bash Name Blob Type Blob Tier Length Content Type Last Modified Snapshot --------- ----------- ----------- -------- --------------- ------------------------- ---------- model.zip BlockBlob 4373 application/zip 2018-08-21T19:26:09\u002B00:00 \u0060\u0060\u0060 That\u0027s all there is to the upload process. It\u0027s now time to build our Azure Functions Application ## Build The Azure Functions Application ### Initialize Azure Function Project In our solution directory, we want to create a new directory for our Azure Function project \u0060\u0060\u0060bash mkdir serverlessfunctionapp dotnet sln add serverlessfunctionapp/serverlessfunctionapp.csproj \u0060\u0060\u0060 Then, we can scaffold an Azure Functions project inside our newly created \u0060serverlessfunctionapp\u0060 project directory using Azure Functions Core Tools \u0060\u0060\u0060bash cd serverlessfunctionapp func init \u0060\u0060\u0060 At this point you will be prompted to select the runtime for your application. For this application select \u0060dotnet\u0060. This will generate a few files in the \u0060serverlessfunctionapp\u0060 directory. Keep in mind though that this does not create the function. ### Add Dependencies Before we create any functions, we need to add the dependencies for our Azure Functions application. Since we\u0027ll be using \u0060Microsoft.ML\u0060 in our Azure Function application, we\u0027ll need to add it as a dependency. From the \u0060serverlessfunctionapp\u0060 enter the following command: \u0060\u0060\u0060bash dotnet add package Microsoft.ML dotnet restore \u0060\u0060\u0060 ### Create Serverless Function Once we\u0027ve added the dependencies it\u0027s time to create a new function. To do so we\u0027ll use the Azure Functions Core Tools \u0060new\u0060 command. Although not required, it\u0027s good practice to separate functions and related files into their own directory. \u0060\u0060\u0060bash mkdir Predict cd Predict func new \u0060\u0060\u0060 At this time you will be prompted to select a template. For our classification model, we\u0027ll be using an HttpTrigger which is exactly what it sounds like. An HTTP request is what calls or invokes our function. With that being said, select the \u0060HttpTrigger\u0060 option. You will then be prompted to enter a name for your function. You can use any name but to make things easy, name it the same as the directory the function is in. Once that process is complete, there should be a file called \u0060Predict.cs\u0060 inside our \u0060serverlessfunctionapp/Predict\u0060 directory. This is where we\u0027ll write the logic for our application. ### Define Data Structures We\u0027ll also be making use of the IrisData and IrisPrediction classes inside our \u0060Predict\u0060 function. Therefore, we need to create classes for them inside our \u0060Predict\u0060 directory. The content will be the same as when we trained our model with the exception of the namespace which will now be \u0060serverlessfunctionapp.Predict\u0060. The content of those files should look like the code below: \u0060\u0060\u0060csharp //IrisData.cs using Microsoft.ML.Runtime.Api; namespace serverlessfunctionapp.Predict { public class IrisData { [Column(\u00220\u0022)] public float SepalLength; [Column(\u00221\u0022)] public float SepalWidth; [Column(\u00222\u0022)] public float PetalLength; [Column(\u00223\u0022)] public float PetalWidth; [Column(\u00224\u0022)] [ColumnName(\u0022Label\u0022)] public string Label; } } //IrisPrediction.cs using Microsoft.ML.Runtime.Api; namespace serverlessfunctionapp.Predict { public class IrisPrediction { [ColumnName(\u0022PredictedLabel\u0022)] public string PredictedLabels; } } \u0060\u0060\u0060 ### Write Function Logic With our dependencies and data structures set up, it\u0027s time to write our function logic to make predictions. The first thing we want to do is replace the \u0060Run\u0060 method inside the \u0060Predict\u0060 class with the following code. \u0060\u0060\u0060csharp public static IActionResult Run( [HttpTrigger(AuthorizationLevel.Function, \u0022get\u0022, \u0022post\u0022, Route = null)]HttpRequest req, [Blob(\u0022models/model.zip\u0022, FileAccess.Read, Connection = \u0022AzureWebJobsStorage\u0022)] Stream serializedModel, TraceWriter log) { // Workaround for Azure Functions Host if (typeof(Microsoft.ML.Runtime.Data.LoadTransform) == null || typeof(Microsoft.ML.Runtime.Learners.LinearClassificationTrainer) == null || typeof(Microsoft.ML.Runtime.Internal.CpuMath.SseUtils) == null || typeof(Microsoft.ML.Runtime.FastTree.FastTree) == null) { log.Error(\u0022Error loading ML.NET\u0022); return new StatusCodeResult(500); } //Read incoming request body string requestBody = new StreamReader(req.Body).ReadToEnd(); log.Info(requestBody); //Bind request body to IrisData object IrisData data = JsonConvert.DeserializeObject (requestBody); //Load prediction model var model = PredictionModel.ReadAsync (serializedModel).Result; //Make prediction IrisPrediction prediction = model.Predict(data); //Return prediction return (IActionResult)new OkObjectResult(prediction.PredictedLabels); } \u0060\u0060\u0060 There are a few notable change worth looking at. One of them is the workaround at the beginning of the function. \u0060\u0060\u0060csharp if (typeof(Microsoft.ML.Runtime.Data.LoadTransform) == null || typeof(Microsoft.ML.Runtime.Learners.LinearClassificationTrainer) == null || typeof(Microsoft.ML.Runtime.Internal.CpuMath.SseUtils) == null || typeof(Microsoft.ML.Runtime.FastTree.FastTree) == null) { log.Error(\u0022Error loading ML.NET\u0022); return new StatusCodeResult(500); } \u0060\u0060\u0060 There are some issues with Azure Functions and ML.NET Assemblies which are being worked on by both teams at Microsoft (see [Github Issue](https://github.com/Azure/azure-functions-host/issues/3190)). In the meantime, it\u0027s safe to just include that code in there. The other addition to note is the method signature. As you can see, I have added an additional parameter called \u0060serializedModel\u0060 which is decorated by the \u0060Blob\u0060 attribute. \u0060\u0060\u0060csharp [Blob(\u0022models/model.zip\u0022, FileAccess.Read, Connection = \u0022AzureWebJobsStorage\u0022)] Stream serializedModel \u0060\u0060\u0060 What this code is doing is telling the function to import the blob \u0060model.zip\u0060 as a \u0060Stream\u0060 and bind it to \u0060serializedModel\u0060. Using additional arguments, I tell my function to only have \u0060Read\u0060 access to the \u0060model.zip\u0060 blob inside the \u0060models\u0060 container which can be accessed with the \u0060AzureWebJobsStorage\u0060 connection string. Right now that last part might seem confusing, but this is something we configured when we set up the Azure environment and linked \u0060azfnmlnetdemostorage\u0060 account with our \u0060azfnmlnetdemo20180821\u0060 serverless function app using the \u0060--storage-account\u0060 option. Although the production environment is configured, if we try to test our application locally we won\u0027t be able to access our storage account because we have not configured the connection string locally. We can do so by looking in the \u0060local.settings.json\u0060 file inside our \u0060serverlessfunctionapp\u0060 directory. The contents should look like the following. \u0060\u0060\u0060json { \u0022IsEncrypted\u0022: false, \u0022Values\u0022: { \u0022AzureWebJobsStorage\u0022: \u0022\u0022, \u0022AzureWebJobsDashboard\u0022: \u0022\u0022, \u0022FUNCTIONS_WORKER_RUNTIME\u0022: \u0022dotnet\u0022 } } \u0060\u0060\u0060 Our function running locally will look in this file, try to find \u0060AzureWebJobsStorage\u0060 and use the connection string value in the \u0060Predict\u0060 function. To get the connection string for our \u0060azfnmlnetdemostorage\u0060 account, enter the following command. \u0060\u0060\u0060bash az storage account show-connection-string --name azfnmlnetdemostorage \u0060\u0060\u0060 The output of that command should look like the following: \u0060\u0060\u0060json { \u0022connectionString\u0022: \u0022 \u0022 } \u0060\u0060\u0060 At this point, you just need to copy the value of \u0060connectionString\u0060 to your \u0060local.settings.json\u0060 file and replace the current empty string for \u0060AzureWebJobsStorage\u0060. It\u0027s important to note that it\u0027s okay to paste the connection string in here since the \u0060local.settings.json\u0060 file is not committed to version control. (See \u0060.gitignore\u0060 inside \u0060serverlessfunctionapp\u0060 directory). Now the application is ready to be tested locally. ### Testing The Function Locally To test the application, first build your project by entering the following command from the \u0060serverlessfunctionapp\u0060 directory. \u0060\u0060\u0060bash dotnet build \u0060\u0060\u0060 Then, navigate to the build directory \u0060./bin/Debug/netstandard2.0\u0060 and enter the following command: \u0060\u0060\u0060bash func host start \u0060\u0060\u0060 Finally, using a tool like Postman or Insomnia make an HTTP POST request to the \u0060http://localhost:7071/api/Predict\u0060 endpoint with the following body: \u0060\u0060\u0060json { \u0022SepalLength\u0022: 3.3, \u0022SepalWidth\u0022: 1.6, \u0022PetalLength\u0022: 0.2, \u0022PetalWidth\u0022: 5.1 } \u0060\u0060\u0060 If everything is set up correctly, you should receive the following output \u0060\u0060\u0060bash Iris-virginica \u0060\u0060\u0060 Once satisfied with testing, press \u0060Ctrl \u002B C\u0060 to stop the application. ## Deploy To Azure ### Push Build Great! Now on to the final step, deploying our application to production. Since we already configured everything it should only require a few commands to do so. First, make sure you are logged in. Using Azure Functions Core Tools log in with the following command: \u0060\u0060\u0060bash func azure login \u0060\u0060\u0060 Like with the Azure CLI, you will follow a series of prompts to log into your account. Once you have successfully logged in, it\u0027s time to publish our application to Azure. From the \u0060serverlessfunctionapp\u0060 directory enter the following command. \u0060\u0060\u0060bash func azure functionapp publish azfnmlnetdemo20180821 \u0060\u0060\u0060 When our deployment is complete, we can check whether our function was published successfully by using the following command. \u0060\u0060\u0060bash func azure functionapp list-functions azfnmlnetdemo20180821 \u0060\u0060\u0060 The output should look similar to that below. \u0060\u0060\u0060bash Functions in azfnmlnetdemo20180821: Predict - [httpTrigger] \u0060\u0060\u0060 ### Configure Platform For the last part of the deployment step, we\u0027ll need to head over to the Azure Portal. To do so, visit [https://portal.azure.com](https://portal.azure.com) and log in. Once logged in, type the name of your application into the search bar at the top of the page and select your Azure Function application of type \u0060App Service\u0060 ![](http://cdn.lqdev.tech/files/images/azfnmlnetdemo1.png) Then, from the accordion element on the left, select the top-most item with your appplication name on it. Then, select the \u0060Platform features\u0060 tab and open the \u0060Application settings\u0060 option. ![](http://cdn.lqdev.tech/files/images/azfnmlnetdemo2.png) When the \u0060Application settings\u0060 page loads, change the \u0060Platform\u0060 setting to \u006064-bit\u0060. The reason for this is \u0060ML.NET\u0060 has to be built and run on a 64-bit environment due to some of its native dependencies. ![](http://cdn.lqdev.tech/files/images/azfnmlnetdemo3.png) That\u0027s all there is to it. ### Test The Deployed Function Now it\u0027s time to test our deployed function. We can do so from the portal by going back to the accordion and selecting the function name below the \u0060Functions\u0060 parent element and clicking on the \u0060Test\u0060 button on the far right. Doing so will show a form that will allow us to test our application. Make sure the \u0060HTTP method\u0060 option is set to POST. In the text area for the \u0060Request body\u0060 paste the following content: \u0060\u0060\u0060json { \u0022SepalLength\u0022: 3.3, \u0022SepalWidth\u0022: 1.6, \u0022PetalLength\u0022: 0.2, \u0022PetalWidth\u0022: 5.1 } \u0060\u0060\u0060 Once the form is filled in, click \u0060Run\u0060 at the top of the page and if successful \u0060Iris-virginica\u0060 should show up in the \u0060Output\u0060 area. ![](http://cdn.lqdev.tech/files/images/azfnmlnetdemo4.png) To test the function outside the portal, you can click on the \u0060Get function URL\u0060 link next to the \u0060Run\u0060 button and make an HTTP POST request using that link. ## Conclusion In this writeup, we trained a classification model that predicts a class of flower using Microsoft\u0027s \u0060ML.NET\u0060 framework. Then, we exposed this model for inference via an Azure Functions serverless application. In doing so, we can more efficiently manage our cost as well as our resource utilization. Happy coding! ###### Resources [Create a function app for serverless code execution](https://docs.microsoft.com/en-us/azure/azure-functions/scripts/functions-cli-create-serverless) [Using the Azure CLI 2.0 with Azure Storage](https://docs.microsoft.com/en-us/azure/storage/common/storage-azure-cli) [Work with Azure Functions Core Tools](https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local)","contentType":"posts","url":"https://www.lqdev.me/posts/serverless-machine-learning-mlnet-azure-functions","date":"2018-08-21 19:13:47 -05:00","tags":["serverless","azurefunctions","mlnet","machinelearning","dotnet","dotnetcore","csharp","microsoft","devops","artificialintelligence","ai"],"summary":"## Introduction In a previous blog [post](http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/), I explored how to build and deploy machine learning models built with the \u0060ML.NET\u0060 framework...","keywords":["model","azure","using","function","microsoft","\u0060\u0060\u0060bash","application","account","create","functions"]},{"id":"posts-hn-vue-topstories-client","title":"Hacker News Vue Top Stories Client","content":"# Introduction Over the past few years, I\u0027ve used AngularJS as my front-end framework for most web projects. With the release of Angular and React, I tried both of them and while both are great tools the learning took significantly longer because I also had to learn the bundler systems and in the case of Angular, TypeScript. Although the CLI tools go a long way towards automating a lot of the setup process, building an application from scratch in these frameworks is not intuitive especially for a newcomer. In the early days of Vue, I was able to attend a Meetup presentation by [Evan You](https://twitter.com/youyuxi) which went over the philosophy of Vue as well as its capabilities. I was instantly interested because of how easy it was to get started. Since that presentation, I have not had time to try it but with AngularJS being phased out, I felt it was about time to start exploring another framework. To get a better understanding of the framework\u0027s features prior to using CLI tools, I created a standalone application that relies only on NPM packages. In this writeup, I will create a Hacker News client with Vue that calls the Hacker News API and displays the top 50 stories. Source code for this project can be found in the following [repository](https://github.com/lqdev/hnvuedemo). ## Prerequisites - [Node](https://nodejs.org/en/) ## Initialize Project The first thing we want to do is create a directory for our project and initialize it with NPM \u0060\u0060\u0060bash mkdir hnvuedemo cd hnvuedemo npm init -y \u0060\u0060\u0060 ## Install Dependencies Next, we want to install our dependencies. (Express is optional) \u0060\u0060\u0060bash npm install --save vue axios express \u0060\u0060\u0060 ## Create Server (Optional) We can set up a server to host and serve our content. This is optional though because the application should still work when the \u0060index.html\u0060 file is opened in the browser. In our root project directory, we can create the \u0060server.js\u0060 file and add the following content: \u0060\u0060\u0060javascript var express = require(\u0022express\u0022); const PORT = 3000 || process.env.PORT; var app = express(); app.use(express.static(\u0022.\u0022)); app.get(\u0022/\u0022, (req, res) =\u003E { res.sendFile(\u0022index.html\u0022); }); app.listen(PORT, () =\u003E { console.log(\u0060Listening on port ${PORT}\u0060); }); \u0060\u0060\u0060 ## Create Application ### Create The View Now it\u0027s time to create our application. Let\u0027s start by scaffolding the \u0060index.html\u0060 which is where our application will be displayed. \u0060\u0060\u0060html \u0060\u0060\u0060 We want to add the scripts to the \u0060Vue\u0060 and \u0060axios\u0060 packages in our \u0060head\u0060 element. We\u0027ll hold off on the content for now. ### Get Data Our data source for this project will be the Hacker News [API](https://github.com/HackerNews/API). To interact with it, we\u0027ll be using the \u0060axios\u0060 NPM package. The data collection happens over several HTTP requests, therefore to help with it we\u0027ll be creating a file called \u0060apihelpers.js\u0060 in our root project directory that will contain functions to get and manipulate the data. #### Get Top Stories The first thing we want to do is get a list of top stories. We can do so via the \u0060/topstories\u0060 endpoint. The response of this request returns a list of ids which can then be used in conjunction with the \u0060/item/{id}\u0060 endpoint to get the individual story data. We can then validate the story object to make sure it is a story as opposed to a user or job posting and return a list of all the story objects. The functions that will help us with that are \u0060getIds\u0060, \u0060isStory\u0060 and \u0060extractStories\u0060. \u0060\u0060\u0060javascript /** * Checks whether the item is a story * @param {Object} story - Story object */ function isStory(story) { return story.type == \u0022story\u0022; } /** * Gets ids of stories * @param {string} url - Url to fetch story ids */ function getIds(url) { return axios.get(url); } /** * * @param {Array } - List of resolved promises */ function extractStories(...responses) { var stories = responses.map(story =\u003E story = story.data); return stories.filter(isStory); } \u0060\u0060\u0060 ### Create Vue App Now that we have our helper methods to make calls to the API, we can create our application. We can start by creating our Vue instance which is where our application will live. We can create a file called \u0060app.js\u0060 in our root project directory and add the following content: \u0060\u0060\u0060javascript //Components Go Here new Vue({ el: \u0022#app\u0022, data: { title: \u0022Hacker News Vue Simple Reader\u0022, loading: true, topStories: [] }, methods: { getTopStories: function() { getIds( \u0022https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty\u0022 ) .then(ids =\u003E buildRequest(ids.data.splice(0, 50))) //Only get first 50 .then(getStories) .then(axios.spread(extractStories)) .then(stories =\u003E { this.topStories = stories; this.loading = false; }) .catch(e =\u003E console.log(e)); } }, created: function() { this.getTopStories(); } }); \u0060\u0060\u0060 There\u0027s a lot happening here, so let\u0027s break it down based on the properties of the object passed to the Vue constructor. The \u0060el\u0060 property is like a selector which tells the application which element it should operate on. The value \u0060#app\u0060 tells it to look for an element where the \u0060id\u0060 attribute is \u0060app\u0060. This of course can be anything of your choosing. The \u0060data\u0060 property specifies the properties that contain the data of our application. These properties are reactive which means that whenever they change, that change is reflected in our view. As such, even if they have no value when the application starts, in order for them to automatically change when data is passed to them, they need to be declared in the \u0060data\u0060 property. In our application, we have a \u0060title\u0060 property which will be the title of our web page, \u0060topStories\u0060 which is where we\u0027ll store the list of Hacker News stories and \u0060loading\u0060 which we\u0027ll use to let us know when data is being loaded into our application. In the \u0060methods\u0060 property, we define functions that we want our application to use. In this case, I created the \u0060getTopStories\u0060 method which chains together all the functions defined in our \u0060apihelpers.js\u0060 file to return the top 50 stories on Hacker News. Finally, Vue has instance lifecycle hooks. The \u0060created\u0060 property defines what should happen when our instance is created. In our case, we want to call the \u0060getTopProperties\u0060 method which is defined in our \u0060methods\u0060 property to load the data, update our \u0060topStories\u0060 data property and set \u0060loading\u0060 to \u0060false\u0060 because our data has loaded successfully. ### Add App to View Now that we have created the logic of our application, it\u0027s time to add it to our view. We can do so by adding the \u0060apihelpers.js\u0060 and \u0060app.js\u0060 files to our \u0060index.html\u0060 file via \u0060script\u0060 elements. \u0060\u0060\u0060html \u0060\u0060\u0060 ### Display Data Although we can now use our application logic inside of our view, we still can\u0027t see any of it because we have not added elements to display it. To view our application data, we can add the following code to our \u0060index.html\u0060 file below the \u0060 \u0060 section. \u0060\u0060\u0060html {{title}} Loading... \u0060\u0060\u0060 The final \u0060index.html\u0060 contents should look like the content below: \u0060\u0060\u0060html {{title}} Loading... \u0060\u0060\u0060 As mentioned earlier, we created an element, in this case a \u0060div\u0060 that has the \u0060id\u0060 attribute with a value of \u0060app\u0060. This is how our application knows where to display our content. Inside of an \u0060h2\u0060 element, we display our \u0060title\u0060 data property. Below it, we have an \u0060h5\u0060 element that displays the text \u0022Loading...\u0022. However, this is to only be displayed when our \u0060loading\u0060 data property is \u0060true\u0060. If not, it should not be visible. We can achieve this conditional rendering via the \u0060v-if\u0060 directive. This directive evaluates the expression inside of it and renders content based on its truthiness. Finally, there\u0027s one last piece that looks like an element, but not one of the built-in HTML elements. So then, what is it? It\u0027s a component. The Vue website defines a component as \u0022...a reusable Vue instance with a name: in this case, \u0060 \u0060. We can use this component as a custom element inside a root Vue instance...\u0022. In our component, the \u0060v-for\u0060 directive is what it sounds like. It creates a sequence of \u0060story\u0060 components based on a list of objects defined in our \u0060Vue\u0060 instance\u0027s \u0060data\u0060 property. Our \u0060story\u0060 component iterates over the \u0060topStories\u0060 data property and assigns the value of the individual object in the list to the variable \u0060story\u0060. We bind the \u0060id\u0060 property of the \u0060story\u0060 object to the \u0060key\u0060 attribute of the component and pass in the entire object to the component via the \u0060story\u0060 prop. A prop is a custom attribute that you can register for the component. We can use props to pass data into the component. In all cases, the \u0060:\u0060 prefix on the attributes and props of the component are shorthand for the \u0060v-bind\u0060 directive which dynamically binds an expression to an attribute or component prop. With all that being said your next question might be, how does the view know about this component? The answer is it doesn\u0027t at least not until you define it which is what we\u0027ll do next in our \u0060app.js\u0060 file. In order for our \u0060story\u0060 component to be usable, we need to define it above the instantiation of our \u0060Vue\u0060 instance. The definition looks like the following: \u0060\u0060\u0060javascript Vue.component(\u0027story\u0027,{ props: [\u0027story\u0027], template: \u0060 {{story.title}} \u0060 }); \u0060\u0060\u0060 Like our \u0060Vue\u0060 instance, let\u0027s unpack what\u0027s happening here. The first parameter is a string with the name of our component. The \u0060props\u0060 property is a list of the props or custom attributes that are accepted by our component. The \u0060template\u0060 property is where we set up the template of what will be rendered in place of our component in the view. In our case, we\u0027ll have an \u0060h3\u0060 element with a nested \u0060a\u0060 element whose \u0060href\u0060 attribute is the \u0060url\u0060 property of our \u0060story\u0060 object and the display text is the \u0060title\u0060 property of our \u0060story\u0060 object. ## Run Application At this point, our application should be ready to run. You can either start the server with the following command \u0060npm start\u0060 and navigate to \u0060http://localhost:3000\u0060 or open the \u0060index.html\u0060 page in the browser of your choice. The result should look like the screenshot below: ![](http://cdn.lqdev.tech/files/images/hnvueclient.png) ## Conclusion In this writeup, I built a standalone Hacker News client that displays the top 50 stories using Vue while also highlighting some of its main features. Overall, I really enjoyed building this application. The setup process was extremely simple and after a few hours looking through the excellent documentation and working through some bugs, it took less than two hours to get this application up and running from start to finish. For prototyping and learning purposes, Vue is great because you\u0027re able to take advantage of the core features of the framework without having too much overhead. Although this may not be the most appropriate way to build production-ready applications, it\u0027s nice to know you can have a modern web application with minimal setup required. My next steps will be to continue learning some of the other features the framework provides and eventually build up to learning how to use the CLI tools.","contentType":"posts","url":"https://www.lqdev.me/posts/hn-vue-topstories-client","date":"2018-06-21 18:30:38 -05:00","tags":["vue","web development","programming","api","development","frontend","js"],"summary":"# Introduction Over the past few years, I\u0027ve used AngularJS as my front-end framework for most web projects. With the release of Angular and React, I tried both of them and while both are great tools...","keywords":["application","data","property","story","component","which","stories","element","create","object"]},{"id":"posts-mlnet-classification-fsharp","title":"Classification with F# ML.NET Models","content":"# Introduction In a previous [post](http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/), I detailed how to build and deploy C# \u0060ML.NET\u0060 models with \u0060Docker\u0060 and \u0060ASP.NET Core\u0060. With inspiration from [Jeff Fritz](https://twitter.com/csharpfritz), I have been learning F# for the past week and a half or so. When trying to think of projects to start practicing my F#, porting over the code I had built in C# naturally came to mind. After overcoming many obstacles and with much guidance from [Alan Ball](https://github.com/voronoipotato) and [Isaac Abraham](https://twitter.com/isaac_abraham) whose F# [book](https://www.amazon.com/Get-Programming-guide-NET-developers/dp/1617293997/ref=sr_1_1?ie=UTF8\u0026qid=1528929802\u0026sr=8-1\u0026keywords=get\u002Bprogramming\u002Bwith\u002BF%23) I highly recommend, I was able to successfully port over the main parts of my code which highlight \u0060ML.NET\u0060 functionality. In this writeup, I will port a C# \u0060ML.NET\u0060 classification model to F# which predicts the type of flower based on four numerical measurement inputs. I tried to keep the organization of this post nearly identical to that of the C# article where possible. Sample code for this project can be found at the following [link](https://github.com/lqdev/fsmlnetdemo). ## Prerequisites This project was built on a Linux PC but should work cross-platform on Mac and Windows. - [.NET Core SDK 2.0\u002B](https://www.microsoft.com/net/download/linux) - [Ionide Extension - Option 1](https://fsharp.org/use/linux/) - [ML.NET v 0.2.0](https://www.nuget.org/packages/Microsoft.ML/) ## Setting Up The Project The first thing we want to do is create a folder for our solution. \u0060\u0060\u0060bash mkdir fsharpmlnetdemo \u0060\u0060\u0060 Then, we want to create a solution inside our newly created folder. \u0060\u0060\u0060bash cd fsharpmlnetdemo dotnet new sln \u0060\u0060\u0060 ## Building The Model ### Setting Up The Model Project First, we want to create the project. From the solution folder enter: \u0060\u0060\u0060bash dotnet new console -o model -lang f# \u0060\u0060\u0060 Now we want to add this new project to our solution. \u0060\u0060\u0060bash dotnet sln add model/model.fsproj \u0060\u0060\u0060 ### Adding Dependencies Since we\u2019ll be using the \u0060ML.NET\u0060 framework, we need to add it to our \u0060model\u0060 project. \u0060\u0060\u0060bash dotnet add model/model.fsproj package Microsoft.ML \u0060\u0060\u0060 ### Download The Data Before we start training the model, we need to download the data we\u2019ll be using to train. We do so by downloading the data file into our root solution directory. \u0060\u0060\u0060bash curl -o iris-data.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data \u0060\u0060\u0060 If we take a look at the data file, it should look something like this: \u0060\u0060\u0060bash 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 5.4,3.9,1.7,0.4,Iris-setosa 4.6,3.4,1.4,0.3,Iris-setosa 5.0,3.4,1.5,0.2,Iris-setosa 4.4,2.9,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa \u0060\u0060\u0060 ### Train Model Now that we have all our dependencies set up, it\u2019s time to build our model. I leveraged the demo that is used on the \u0060ML.NET\u0060 Getting-Started [website](https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet/get-started/linux/ubuntu16-04). #### Defining Data Structures At the time of this writing, \u0060ML.NET\u0060 version \u00600.2.0\u0060 does not fully support F# Records. A workaround for this are mutable classes. Not really inline with F# paradigms, but it should be good enough. In the \u0060Program.fs\u0060 file of our \u0060model\u0060 project directory, let\u2019s create two mutable classes called \u0060IrisData\u0060 and \u0060IrisPrediction\u0060 which will define our features and predicted attribute respectively. Both of them will use \u0060Microsoft.ML.Runtime.Api\u0060 to add the property attributes. Here is what our \u0060IrisData\u0060 class looks like: \u0060\u0060\u0060fsharp type IrisData() = [ ] val mutable public SepalLength: float32 [ ] val mutable public SepalWidth: float32 [ ] val mutable public PetalLength:float32 [ ] val mutable public PetalWidth:float32 [ ] val mutable public Label: string \u0060\u0060\u0060 Similarly, here is the \u0060IrisPrediction\u0060 class: \u0060\u0060\u0060fsharp type IrisPrediction() = [ ] val mutable public PredictedLabel : string \u0060\u0060\u0060 #### Building Training Pipeline The way the \u0060ML.NET\u0060 computations process is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. We can add that logic inside the \u0060main\u0060 function of our \u0060Program.fs\u0060 file. \u0060\u0060\u0060fsharp let dataPath = \u0022./iris-data.txt\u0022 // Initialize Compute Graph let pipeline = new LearningPipeline() // Load Data pipeline.Add((new TextLoader(dataPath).CreateFrom separator=\u0027,\u0027)) // Transform Data // Assign numeric values to text in the \u0022Label\u0022 column, because // only numbers can be processed during model training pipeline.Add(new Transforms.Dictionarizer(\u0022Label\u0022)) // Vectorize Features pipeline.Add(new ColumnConcatenator(\u0022Features\u0022,\u0022SepalLength\u0022, \u0022SepalWidth\u0022, \u0022PetalLength\u0022, \u0022PetalWidth\u0022)) // Add Learner pipeline.Add(new StochasticDualCoordinateAscentClassifier()) // Convert Label back to text pipeline.Add(new ransforms.PredictedLabelColumnOriginalValueConverterPredictedLabelColumn = \u0022PredictedLabel\u0022)) //Train the model let model = pipeline.Train () \u0060\u0060\u0060 #### Testing Our Model Now that we have our data structures and model trained, it\u2019s time to test it to make sure it\u0027s working. Following our training operation, we can add the following code. \u0060\u0060\u0060fsharp // Test data for prediction let testInstance = IrisData() testInstance.SepalLength ] val mutable public SepalLength: float32 [ ] val mutable public SepalWidth: float32 [ ] val mutable public PetalLength:float32 [ ] val mutable public PetalWidth:float32 [ ] val mutable public Label: string type IrisPrediction() = [ ] val mutable public PredictedLabel : string [ ] let main argv = let dataPath = \u0022./iris-data.txt\u0022 // Initialize Compute Graph let pipeline = new LearningPipeline() // Load Data pipeline.Add((new TextLoader(dataPath)).CreateFrom (separator=\u0027,\u0027)) // Transform Data // Assign numeric values to text in the \u0022Label\u0022 column, because // only numbers can be processed during model training pipeline.Add(new Transforms.Dictionarizer(\u0022Label\u0022)) // Vectorize Features pipeline.Add(new ColumnConcatenator(\u0022Features\u0022,\u0022SepalLength\u0022, \u0022SepalWidth\u0022, \u0022PetalLength\u0022, \u0022PetalWidth\u0022)) // Add Learner pipeline.Add(new StochasticDualCoordinateAscentClassifier()) // Convert Label back to text pipeline.Add(new Transforms.PredictedLabelColumnOriginalValueConverter(PredictedLabelColumn = \u0022PredictedLabel\u0022)) //Train the model let model = pipeline.Train () // Test data for prediction let testInstance = IrisData() testInstance.SepalLength \u003C- 3.3f testInstance.SepalWidth \u003C- 1.6f testInstance.PetalLength \u003C- 0.2f testInstance.PetalWidth \u003C- 5.1f //Get Prediction let prediction = model.Predict(testInstance) //Output Prediction printfn \u0022Predicted flower type is: %s\u0022 prediction.PredictedLabel 0 // return an integer exit code \u0060\u0060\u0060 All set to run. We can do so by entering the following command from our solution directory: \u0060\u0060\u0060fsharp dotnet run -p model/model.fsproj \u0060\u0060\u0060 Once the application has been run, the following output should display on the console. \u0060\u0060\u0060bash Automatically adding a MinMax normalization transform, use \u0027norm=Warn\u0027 or \u0027norm=No\u0027 to turn this behavior off. Using 2 threads to train. Automatically choosing a check frequency of 2. Auto-tuning parameters: maxIterations = 9998. Auto-tuning parameters: L2 = 2.667734E-05.Auto-tuning parameters: L1Threshold (L1/L2) = 0. Using best model from iteration 1066.Not training a calibrator because it is not needed. Predicted flower type is: Iris-virginica \u0060\u0060\u0060 ## Conclusion In this post, we ported over a C# \u0060ML.NET\u0060 classification model to F# which predicts the class of flower based on numerical measurement inputs. While several workarounds needed to be made, \u0060ML.NET\u0060 is still in its infancy. As more people become involved and provide feedback hopefully in the near future, F# support and functionality will become more stable. Happy coding!","contentType":"posts","url":"https://www.lqdev.me/posts/mlnet-classification-fsharp","date":"2018-06-13 18:19:05 -05:00","tags":["fsharp","dotnet","dotnetcore","programming","development","mlnet","machinelearning","artificialintelligence","functionalprogramming"],"summary":"# Introduction In a previous [post](http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/), I detailed how to build and deploy C# \u0060ML.NET\u0060 models with \u0060Docker\u0060 and \u0060ASP.NET Core\u0060. With...","keywords":["model","data","pipeline","mutable","public","iris","setosa","net\u0060","\u0060\u0060\u0060bash","testinstance"]},{"id":"posts-fsharpmodulesample","title":"Organizing F# Modules Into Separate Files","content":"# Introduction After watching [Jeff Fritz\u0027s](https://twitter.com/csharpfritz) F# Friday [Stream](https://www.twitch.tv/videos/268107540), I was inspired to do some tinkering of my own. I have looked at F# in the past, but often times have not done much with it. After the stream, I decided to keep plugging away at it and one of the first things I thought about doing was organizing my code into separate files. As simple as it may seem, it was no easy task and while there are many different ways of doing it, the method described in this writeup is one of the easiest I found. Keep in mind though that this may not be the appropriate way of doing it although it seems to work nicely for our purposes. In this solution I will create a set of mathematical operations that will be organized into a single module that will then be imported and used inside of my console application. Source code for this sample solution can be found at this [link](https://github.com/lqdev/fsharpmoduledemo). ## Prerequisites This solution was built on a Mac, but should work on both Windows and Linux environments. - [.NET Core SDK](https://www.microsoft.com/net/download/macos) - [Ionide Extension - Option 1](https://fsharp.org/use/mac/) ## Create Project Once you have everything installed, you can create a new F# project by entering the following command into the console. \u0060\u0060\u0060bash dotnet new console -lang f# \u0060\u0060\u0060 ## Create Module The goal of this solution will be to create functions that add, subtract, multiply and divide two numbers. Since all of these functions are mathematical operations, we can organize them inside a single module. Therefore, we can create a new file inside of our project called \u0060Math.fs\u0060 which will contain all of our functions. \u0060\u0060\u0060fsharp module Math let add x y = x \u002B y let subtract x y = x - y let multiply x y = x * y let divide x y = x / y \u0060\u0060\u0060 ## Compile File Now that our file has been created, we need to add it to our list of files to be compiled by our project. To do so, we can add an entry for our \u0060Math.fs\u0060 file to the \u0060ItemGroup\u0060 property inside our \u0060fsproj\u0060 file. The contents of the file should look as follows after the addition: \u0060\u0060\u0060xml Exe netcoreapp2.0 \u0060\u0060\u0060 ## Import Module Into Console Application Once we have our module created and our \u0060fsproj\u0060 file configured, we can call the module from our console application. To do so, we need to first import it into our \u0060Program.fs\u0060 file. \u0060\u0060\u0060fsharp open Math \u0060\u0060\u0060 Once imported, we can call the \u0060Math\u0060 module functions from our \u0060main\u0060 function in the \u0060Program.fs\u0060 file. \u0060\u0060\u0060fsharp printfn \u0022Add: %i\u0022 (add 1 2) printfn \u0022Subtract: %i\u0022 (subtract 1 2) printfn \u0022Multiply: %i\u0022 (multiply 1 2) printfn \u0022Divide: %i\u0022 (divide 4 2) \u0060\u0060\u0060 ## Run Application Now that everything is set up, we can build and run our console application with the following commands: \u0060\u0060\u0060bash dotnet build dotnet run \u0060\u0060\u0060 The output should be the following: \u0060\u0060\u0060bash Add: 3 Subtract: -1 Multiply: 2 Divide: 2 \u0060\u0060\u0060 ## Conclusion In this writeup, I went over how to create an F# console application that organizes a set of mathematical functions into a module whose contents are stored in a separate file. This is good for organization and to reduce clutter. Happy coding!","contentType":"posts","url":"https://www.lqdev.me/posts/fsharpmodulesample","date":"2018-06-05 20:56:52 -05:00","tags":["dotnet","fsharp","programming","development","functionalprogramming","dotnetcore"],"summary":"# Introduction After watching [Jeff Fritz\u0027s](https://twitter.com/csharpfritz) F# Friday [Stream](https://www.twitch.tv/videos/268107540), I was inspired to do some tinkering of my own. I have looked...","keywords":["file","module","create","console","application","functions","printfn","project","solution","inside"]},{"id":"posts-deploy-netml-docker-aci","title":"Deploy .NET Machine Learning Models with ML.NET, ASP.NET Core, Docker and Azure Container Instances","content":"# Introduction Leading up to and during MS Build 2018 Microsoft has released a wide range of products that reduce the complexity that comes with building and deploying software. The focus this year was on Machine Learning and Artificial Intelligence. Some of the products I found particularly interesting are [Azure Container Instances](https://azure.microsoft.com/en-us/services/container-instances/) which makes it easier to run containerized applications without provisioning or managing servers and [ML.NET](https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet) which is a .NET cross-platform machine learning framework. In this writeup, I will make use of both these products by creating a machine learning classification model with \u0060ML.NET\u0060, exposing it via an ASP.NET Core Web API, packaging it into a Docker container and deploying it to the cloud via Azure Container Instances. Source code for this project can be found [here](https://github.com/lqdev/mlnetacidemo). ## Prerequisites This writeup assumes that you have some familiarity with Docker. The following software/dependencies are also required to build and deploy the sample application. It\u0027s important to note the application was built on a Ubuntu 16.04 PC, but all the software is cross-platform and should work on any environment. - [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/) - [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest) - [.NET Core 2.0](https://www.microsoft.com/net/download/linux) - [Docker Hub Account](https://hub.docker.com/) ## Setting Up The Project The first thing we want to do is create a folder for our solution. \u0060\u0060\u0060bash mkdir mlnetacidemo \u0060\u0060\u0060 Then, we want to create a solution inside our newly created folder. \u0060\u0060\u0060bash cd mlnetacidemo dotnet new sln \u0060\u0060\u0060 ## Building The Model Inside our solution folder, we want to create a new console application which is where we\u0027ll build and test our machine learning model. ### Setting Up the Model Project First, we want to create the project. From the solution folder enter: \u0060\u0060\u0060bash dotnet new console -o model \u0060\u0060\u0060 Now we want to add this new project to our solution. \u0060\u0060\u0060bash dotnet sln mlnetacidemo.sln add model/model.csproj \u0060\u0060\u0060 ### Adding Dependencies Since we\u0027ll be using the \u0060ML.NET\u0060 framework, we need to add it to our \u0060model\u0060 project. \u0060\u0060\u0060bash cd model dotnet add package Microsoft.ML dotnet restore \u0060\u0060\u0060 ### Download The Data Before we start training the model, we need to download the data we\u0027ll be using to train. We do so by creating a directory called \u0060data\u0060 and downloading the data file onto there. \u0060\u0060\u0060bash mkdir data curl -o data/iris.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data \u0060\u0060\u0060 If we take a look at the data file, it should look something like this: \u0060\u0060\u0060text 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 5.4,3.9,1.7,0.4,Iris-setosa 4.6,3.4,1.4,0.3,Iris-setosa 5.0,3.4,1.5,0.2,Iris-setosa 4.4,2.9,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa \u0060\u0060\u0060 ### Train Model Now that we have all our dependencies set up, it\u0027s time to build our model. I leveraged the demo that is used on the [ML.NET Getting-Started website](https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet/get-started/linux/ubuntu16-04). #### Defining Data Structures In the root directory of our \u0060model\u0060 project, let\u0027s create two classes called \u0060IrisData\u0060 and \u0060IrisPrediction\u0060 which will define our features and predicted attribute respectively. Both of them will use \u0060Microsoft.ML.Runtime.Api\u0060 to add the property attributes. Here is what our \u0060IrisData\u0060 class looks like: \u0060\u0060\u0060csharp using Microsoft.ML.Runtime.Api; namespace model { public class IrisData { [Column(\u00220\u0022)] public float SepalLength; [Column(\u00221\u0022)] public float SepalWidth; [Column(\u00222\u0022)] public float PetalLength; [Column(\u00223\u0022)] public float PetalWidth; [Column(\u00224\u0022)] [ColumnName(\u0022Label\u0022)] public string Label; } } \u0060\u0060\u0060 Similarly, here is the \u0060IrisPrediction\u0060 class: \u0060\u0060\u0060csharp using Microsoft.ML.Runtime.Api; namespace model { public class IrisPrediction { [ColumnName(\u0022PredictedLabel\u0022)] public string PredictedLabels; } } \u0060\u0060\u0060 #### Building Training Pipeline The way the \u0060ML.NET\u0060 computations process is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. Therefore, we can create a class called \u0060Model\u0060 to perform all of these tasks for us. \u0060\u0060\u0060csharp using Microsoft.ML.Data; using Microsoft.ML; using Microsoft.ML.Runtime.Api; using Microsoft.ML.Trainers; using Microsoft.ML.Transforms; using Microsoft.ML.Models; using System; using System.Threading.Tasks; namespace model { class Model { public static async Task \u003E Train(LearningPipeline pipeline, string dataPath, string modelPath) { // Load Data pipeline.Add(new TextLoader(dataPath).CreateFrom (separator:\u0027,\u0027)); // Transform Data // Assign numeric values to text in the \u0022Label\u0022 column, because // only numbers can be processed during model training pipeline.Add(new Dictionarizer(\u0022Label\u0022)); // Vectorize Features pipeline.Add(new ColumnConcatenator(\u0022Features\u0022, \u0022SepalLength\u0022, \u0022SepalWidth\u0022, \u0022PetalLength\u0022, \u0022PetalWidth\u0022)); // Add Learner pipeline.Add(new StochasticDualCoordinateAscentClassifier()); // Convert Label back to text pipeline.Add(new PredictedLabelColumnOriginalValueConverter() {PredictedLabelColumn = \u0022PredictedLabel\u0022}); // Train Model var model = pipeline.Train (); // Persist Model await model.WriteAsync(modelPath); return model; } } } \u0060\u0060\u0060 In addition to building our pipeline and training our machine learning model, the \u0060Model\u0060 class also serialized and persisted the model for future use in a file called \u0060model.zip\u0060. #### Testing Our Model Now that we have our data structures and model training pipeline set up, it\u0027s time to test everything to make sure it\u0027s working. We\u0027ll put our logic inside of our \u0060Program.cs\u0060 file. \u0060\u0060\u0060csharp using System; using Microsoft.ML; namespace model { class Program { static void Main(string[] args) { string dataPath = \u0022model/data/iris.txt\u0022; string modelPath = \u0022model/model.zip\u0022; var model = Model.Train(new LearningPipeline(),dataPath,modelPath).Result; // Test data for prediction var prediction = model.Predict(new IrisData() { SepalLength = 3.3f, SepalWidth = 1.6f, PetalLength = 0.2f, PetalWidth = 5.1f }); Console.WriteLine($\u0022Predicted flower type is: {prediction.PredictedLabels}\u0022); } } } \u0060\u0060\u0060 All set to run. We can do so by entering the following command from our solution directory: \u0060\u0060\u0060bash dotnet run -p model/model.csproj \u0060\u0060\u0060 Once the application has been run, the following output should display on the console. \u0060\u0060\u0060text Automatically adding a MinMax normalization transform, use \u0027norm=Warn\u0027 or \u0027norm=No\u0027 to turn this behavior off.Using 2 threads to train. Automatically choosing a check frequency of 2.Auto-tuning parameters: maxIterations = 9998. Auto-tuning parameters: L2 = 2.667734E-05. Auto-tuning parameters: L1Threshold (L1/L2) = 0.Using best model from iteration 882. Not training a calibrator because it is not needed. Predicted flower type is: Iris-virginica \u0060\u0060\u0060 Additionally, you\u0027ll notice that a file called \u0060model.zip\u0060 was created in the root directory of our \u0060model\u0060 project. This persisted model can now be used outside of our application to make predictions, which is what we\u0027ll do next via an API. ## Exposing The Model Once a machine learning model is built, you want to deploy it so it can start making predictions. One way to do that is via a REST API. At it\u0027s core, all our API needs to do is accept data input from the client and respond back with a prediction. To help us do that, we\u0027ll be using an ASP.NET Core API. ### Setting Up The API Project The first thing we want to do is create the project. \u0060\u0060\u0060bash dotnet new webapi -o api \u0060\u0060\u0060 Then we want to add this new project to our solution \u0060\u0060\u0060bash dotnet sln mlnetacidemo.sln add api/api.csproj \u0060\u0060\u0060 ### Adding Dependencies Because we\u0027ll be loading our model and making predictions via our API, we need to add the \u0060ML.NET\u0060 package to our \u0060api\u0060 project. \u0060\u0060\u0060bash cd api dotnet add package Microsoft.ML dotnet restore \u0060\u0060\u0060 ### Referencing Our Model In the previous step when we built our machine learning model, it was saved to a file called \u0060model.zip\u0060. This is the file we\u0027ll be referencing in our API to help us make predictions. To reference it in our API, simply copy it from the model project directory into our \u0060api\u0060 project directory. ### Creating Data Models Our model was built using data structures \u0060IrisData\u0060 and \u0060IrisPrediction\u0060 to define the features as well as the predicted attribute. Therefore, when our model makes predictions via our API, it needs to reference these data types as well. As a result, we need to define \u0060IrisData\u0060 and \u0060IrisPrediction\u0060 classes inside of our \u0060api\u0060 project. The contents of the classes will be nearly identical to those in the \u0060model\u0060 project with the only exception of our namespace changing from \u0060model\u0060 to \u0060api\u0060. \u0060\u0060\u0060csharp using Microsoft.ML.Runtime.Api; namespace api { public class IrisData { [Column(\u00220\u0022)] public float SepalLength; [Column(\u00221\u0022)] public float SepalWidth; [Column(\u00222\u0022)] public float PetalLength; [Column(\u00223\u0022)] public float PetalWidth; [Column(\u00224\u0022)] [ColumnName(\u0022Label\u0022)] public string Label; } } \u0060\u0060\u0060 \u0060\u0060\u0060csharp using Microsoft.ML.Runtime.Api; namespace api { public class IrisPrediction { [ColumnName(\u0022PredictedLabel\u0022)] public string PredictedLabels; } } \u0060\u0060\u0060 ### Building Endpoints Now that our project is set up, it\u0027s time to add a controller that will handle prediction requests from the client. In the \u0060Controllers\u0060 directory of our \u0060api\u0060 project we can create a new class called \u0060PredictController\u0060 with a single \u0060POST\u0060 endpoint. The contents of the file should look like the code below: \u0060\u0060\u0060csharp using System; using System.Collections.Generic; using System.Linq; using System.Threading.Tasks; using Microsoft.AspNetCore.Mvc; using Microsoft.ML; namespace api.Controllers { [Route(\u0022api/[controller]\u0022)] public class PredictController : Controller { // POST api/predict [HttpPost] public string Post([FromBody] IrisData instance) { var model = PredictionModel.ReadAsync (\u0022model.zip\u0022).Result; var prediction = model.Predict(instance); return prediction.PredictedLabels; } } } \u0060\u0060\u0060 ### Testing The API Once our \u0060predict\u0060 endpoint is set up, it\u0027s time to test it. From the root directory of our \u0060mlnetacidemo\u0060 solution, enter the following command. \u0060\u0060\u0060bash dotnet run -p api/api.csproj \u0060\u0060\u0060 In a client like POSTMAN or Insomnia, send an HHTP POST request to the endpoint \u0060http://localhost:5000/api/predict\u0060. The body our request should look similar to the snippet below: \u0060\u0060\u0060json { \u0022SepalLength\u0022: 3.3, \u0022SepalWidth\u0022: 1.6, \u0022PetalLength\u0022: 0.2, \u0022PetalWidth\u0022: 5.1, } \u0060\u0060\u0060 If successful, the output returned should equal \u0060Iris-virginica\u0060 just like our console application. ## Packaging The Application Great! Now that our application is successfully running locally, it\u0027s time to package it up into a Docker container and push it to Docker Hub. ### Creating The Dockerfile In our \u0060mlnetacidemo\u0060 solution directory, create a \u0060Dockerfile\u0060 with the following content: \u0060\u0060\u0060Dockerfile FROM microsoft/dotnet:2.0-sdk AS build WORKDIR /app # copy csproj and restore as distinct layers COPY *.sln . COPY api/*.csproj ./api/ RUN dotnet restore # copy everything else and build app COPY api/. ./api/ WORKDIR /app/api RUN dotnet publish -c release -o out FROM microsoft/aspnetcore:2.0 AS runtime WORKDIR /app COPY api/model.zip . COPY --from=build /app/api/out ./ ENTRYPOINT [\u0022dotnet\u0022, \u0022api.dll\u0022] \u0060\u0060\u0060 ### Building Our Image To build the image, we need to enter the following command into the command prompt. This make take a while because it needs to download the .NET Core SDK and ASP.NET Core runtime Docker images. \u0060\u0060\u0060bash docker build -t / :latest . \u0060\u0060\u0060 ### Test Image Locally We need to test our image locally to make sure it can run on the cloud. To do so, we can use the \u0060docker run\u0060 command. \u0060\u0060\u0060bash docker run -d -p 5000:80 / :latest \u0060\u0060\u0060 Although the API is exposing port 80, we bind it to the local port 5000 just to keep our prior API request intact. When sending a POST request to \u0060http://localhost:5000/api/predict\u0060 with the appropriate body, the response should again equal \u0060Iris-virginica\u0060. To stop the container, use \u0060Ctrl \u002B C\u0060. ### Push to Docker Hub Now that the Docker image is successfully running locally, it\u0027s time to push to Docker Hub. Again, we use the Docker CLI to do this. \u0060\u0060\u0060bash docker login docker push / :latest \u0060\u0060\u0060 ## Deploying To The Cloud Now comes the final step which is to deploy and expose our machine learning model and API to the world. Our deployment will occur via Azure Container Instances because it requires almost no provisioning or management of servers. ### Prepare Deployment Manifest Although deployments can be performed inline in the command line, it\u0027s usually best to place all the configurations in a file for documentation and to save time not having to type in the parameters every time. With Azure, we can do that via a JSON file. \u0060\u0060\u0060json { \u0022$schema\u0022: \u0022https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0022, \u0022contentVersion\u0022: \u00221.0.0.0\u0022, \u0022parameters\u0022: { \u0022containerGroupName\u0022: { \u0022type\u0022: \u0022string\u0022, \u0022defaultValue\u0022: \u0022mlnetacicontainergroup\u0022, \u0022metadata\u0022: { \u0022description\u0022: \u0022Container Group name.\u0022 } } }, \u0022variables\u0022: { \u0022containername\u0022: \u0022mlnetacidemo\u0022, \u0022containerimage\u0022: \u0022 / :latest\u0022 }, \u0022resources\u0022: [ { \u0022name\u0022: \u0022[parameters(\u0027containerGroupName\u0027)]\u0022, \u0022type\u0022: \u0022Microsoft.ContainerInstance/containerGroups\u0022, \u0022apiVersion\u0022: \u00222018-04-01\u0022, \u0022location\u0022: \u0022[resourceGroup().location]\u0022, \u0022properties\u0022: { \u0022containers\u0022: [ { \u0022name\u0022: \u0022[variables(\u0027containername\u0027)]\u0022, \u0022properties\u0022: { \u0022image\u0022: \u0022[variables(\u0027containerimage\u0027)]\u0022, \u0022resources\u0022: { \u0022requests\u0022: { \u0022cpu\u0022: 1, \u0022memoryInGb\u0022: 1.5 } }, \u0022ports\u0022: [ { \u0022port\u0022: 80 } ] } } ], \u0022osType\u0022: \u0022Linux\u0022, \u0022ipAddress\u0022: { \u0022type\u0022: \u0022Public\u0022, \u0022ports\u0022: [ { \u0022protocol\u0022: \u0022tcp\u0022, \u0022port\u0022: \u002280\u0022 } ] } } } ], \u0022outputs\u0022: { \u0022containerIPv4Address\u0022: { \u0022type\u0022: \u0022string\u0022, \u0022value\u0022: \u0022[reference(resourceId(\u0027Microsoft.ContainerInstance/containerGroups/\u0027, parameters(\u0027containerGroupName\u0027))).ipAddress.ip]\u0022 } } } \u0060\u0060\u0060 It\u0027s a lot to look at but for now we can use this template and save it to the file \u0060azuredeploy.json\u0060 in the root directory of our \u0060mlnetacidemo\u0060 solution. The only thing that needs to be changed is the value of the \u0060containerimage\u0060 property. Replace it with your Docker Hub username and the name of the image you just pushed to Docker Hub. ### Deploy In order to deploy our application we need to make sure to log into our Azure account. To do so via the Azure CLI, type into the command prompt: \u0060\u0060\u0060bash az login \u0060\u0060\u0060 Follow the prompts to log in. Once logged in, it\u0027s time to create a resource group for our container. \u0060\u0060\u0060bash az group create --name mlnetacidemogroup --location eastus \u0060\u0060\u0060 After the group has been successfully created it\u0027s time to deploy our application. \u0060\u0060\u0060bash az group deployment create --resource-group mlnetacidemogroup --template-file azuredeploy.json \u0060\u0060\u0060 Give it a few minutes for your deployment to initialize. If the deployment was successful, you should see some output on the command line. Look for the \u0060ContainerIPv4Address\u0060 property. This is the IP Address where your container is accessible. In POSTMAN or Insomnia, replace the URL to which you previously made a POST request to with \u0060http:// /api/predict\u0060 where \u0060ContainerIPv4Address\u0060 is the value that was returned to the command line after the deployment. If successful, the response should be just like previous requests \u0060Iris-virginica\u0060. Once you\u0027re finished, you can clean up resources with the following command: \u0060\u0060\u0060bash az group delete --name mlnetacidemogroup \u0060\u0060\u0060 ## Conclusion In this writeup, we built a classification machine learning model using \u0060ML.NET\u0060 that predicts the class of an iris plant given four measurement features, exposed it via an ASP.NET Core REST API, packaged it into a container and deployed it to the cloud using Azure Container Instances. As the model changes and becomes more complex, the process is standardized enough that extending this example would require minimal changes to our existing application. Happy Coding!","contentType":"posts","url":"https://www.lqdev.me/posts/deploy-netml-docker-aci","date":"2018-05-11 21:17:00 -05:00","tags":["azure","devops","dotnet","ml","ai","microsoft","programming","development","csharp","aci","docker","mlnet","webapi","aspnetcore","aspnet","machinelearning","artificialintelligence"],"summary":"# Introduction Leading up to and during MS Build 2018 Microsoft has released a wide range of products that reduce the complexity that comes with building and deploying software. The focus this year...","keywords":["model","using","microsoft","public","docker","project","\u0060\u0060\u0060bash","data","dotnet","learning"]},{"id":"posts-e2e-mean-docker-build","title":"E2E Dockerizing a MEAN Stack Application","content":"## Introduction Lately I\u0027ve been getting familiar with [Docker](https://docker.com). I have built single container applications using a \u0060Dockerfile\u0060 and run them locally. This works fine, especially for deployment purposes to a production VM but it\u0027s no different than setting up the VM with all the required dependencies and pushing updates via FTP or source control. However, one of the features that I have found extremely useful is multi-container building and deployment via \u0060docker-compose\u0060. With \u0060docker-compose\u0060, not only can I build and run my application, but also dependent services like databases, caches, proxies, etc. Best of all, the builds are standardized and initialized at once without having to individually install the dependencies and components. This writeup explores how to containerize a MEAN stack application and set up a \u0060docker-compose.yml\u0060 file for it to build and start the server and database services defined within it. ## Requirements This writeup assumes that \u0060Docker\u0060, \u0060Docker-Compose\u0060 and \u0060Node\u0060 are installed on your PC. ## The Application The application is a CRUD todo MEAN stack application. The repo for this application can be found [here](https://github.com/lqdev/todomeandockerdemo). ### Project Structure \u0060\u0060\u0060text |_models (Mongoose models) | |_todo.model.js |_public | |_scripts | | |_controllers | | | |_main.controller.js | | |_services | | |_todo.service.js | |_views | | |_main.html | |_app.js (front-end application) | |_index.html |_Dockerfile (server service) |_docker-compose.yml |_api.js (todo api routes) |_config.js |_server.js (back-end application) \u0060\u0060\u0060 The front-end is built with \u0060AngularJS\u0060 and the back-end is built with \u0060NodeJS\u0060 using the \u0060Express\u0060 web framework and \u0060MongoDB\u0060 database. \u0060MongoDB\u0060 models are defined with the \u0060Mongoose\u0060 package. In the application, users can create, view, update and delete todo tasks. The \u0060Dockerfile\u0060 is used to define the container for the web application and the \u0060docker-compose.yml\u0060 defines both the \u0060MongoDB\u0060 database container as well as the web application container defined in the \u0060Dockerfile\u0060. ## The Docker File \u0060\u0060\u0060docker #Define base image FROM node:8 #Set Working Directory WORKDIR /app #Copy pakage.json file from current directory to working directory ADD package.json /app #Install npm packages RUN npm install #Copy all application files from local directory to working directory ADD . /app #Open port where app will be listening EXPOSE 3000 #Start application CMD [\u0027npm\u0027,\u0027start\u0027] \u0060\u0060\u0060 ### Define Docker Image and Application Directory The \u0060Dockerfile\u0060 has no extension and the syntax is like a standard text file. \u0060#\u0060 characters denote comments. \u0060Docker\u0060 works based off images which are basically pre-built packages that are stored in one of many registries such as \u0060DockerHub\u0060. \u0060DockerHub\u0060 can be thought of as a package repository/manager like \u0060npm\u0060 or \u0060dpkg\u0060. In the first two lines of the file we define which base image we want to create our container with. Since this is a MEAN stack application built entirely in \u0060JavaScript\u0060, we\u0027ll be using the \u0060node\u0060 version 8 image. Then we want set the directory in which our application will reside. We do this by using the \u0060WORKDIR\u0060 command and setting \u0060/app\u0060 as our application directory, but any directory of your choosing is valid. ### Install Dependecies All of our dependencies should be defined in our \u0060package.json\u0060 file. In order to install these dependencies in our container, we need to copy our local \u0060package.json\u0060 file into our container application directory. This can be done with the \u0060ADD\u0060 command by passing the \u0060package.json\u0060 and application directory \u0060/app\u0060 as arguments. Once that file has been copied, it\u0027s time to install the dependencies. To run commands through the build process of the application we use the \u0060RUN\u0060 command. The command is no different than the one you\u0027d use on your local machine. Therefore, to install the dependencies defined in the \u0060package.json\u0060 file we use the command \u0060RUN npm install\u0060. ### Copy Application Files Once our dependencies are installed, we need to copy the rest of the files in our local project directory to our container application directory. Like with the \u0060package.json\u0060 file we use the \u0060ADD\u0060 command and pass \u0060.\u0060 and \u0060/app\u0060 as our source and destination arguments respectively. #### .dockerignore Something to keep in mind is that locally we have a \u0060node_modules\u0060 directory containing our installed dependencies. In the previous step, we ran the \u0060npm install\u0060 command which will create the \u0060node_modules\u0060 directory inside our container. Therefore, there is no need to copy all these files over. Like \u0060git\u0060, we can set up a \u0060.dockerignore\u0060 file which will contain the files and directory to be ignored by \u0060Docker\u0060 when packaging and building the container. The \u0060.dockerignore\u0060 file looks like the following. \u0060\u0060\u0060text node_modules/* \u0060\u0060\u0060 ### Opening Ports Our application will be listening for connections on a port. This particular application will use port 3000. We need to define the port to listen on in the \u0060Dockerfile\u0060 as well. To do so, we\u0027ll use the \u0060EXPOSE\u0060 command and pass the port(s) that the application will listen on. (MongoDB listens on 27017, but since the \u0060Dockerfile\u0060 only deals with the web application and not the database we only need to specify the web application\u0027s port). ### Starting the Application After our container is set up, dependencies are installed and port is defined, it\u0027s time to start our application. Unlike the process of running commands while building the container using the \u0060RUN\u0060 command, we\u0027ll use the \u0060CMD\u0060 command to start our application. The arguments accepted by this are an array of strings. In this case, we start our application like we would locally by typing in \u0060npm start\u0060. The \u0060Dockerfile\u0060 command to start our application is the following \u0060CMD [\u0027npm\u0027,\u0027start\u0027]\u0060. ## The docker-compose.yml File \u0060\u0060\u0060yaml version: \u00272\u0027 services: db: image: mongo ports: - 27017:27017 web: build: . ports: - 3000:3000 links: - \u0022db\u0022 \u0060\u0060\u0060 The \u0060docker-compose.yml\u0060 file is a way of defining, building and starting multi-container applications using \u0060docker-compose\u0060. In our case we have a two container application, one of the containers is the web application we defined and built in the \u0060Dockerfile\u0060 and the other is a \u0060MongoDB\u0060 database. The \u0060docker-compose.yml\u0060 file can take many options, but the only ones we\u0027ll be using are the \u0060version\u0060 and \u0060services\u0060 option. The \u0060version\u0060 option defines which syntax version of the \u0060docker-compose.yml\u0060 file we\u0027ll be using. In our case we\u0027ll be using version 2. The \u0060services\u0060 option defines the individual containers to be packaged and initialized. ### Services As mentioned, we have two containers. The names of our containers are \u0060web\u0060 and \u0060db\u0060. These names can be anything you want, as long as they\u0027re descriptive and make sense to you. The \u0060web\u0060 container will be our web application and the \u0060db\u0060 container will be our \u0060MongoDB\u0060 database. Notice that we have listed our \u0060db\u0060 service first and then our \u0060web\u0060 service. The reason for this is we want to build and initialize our database prior to our application so that by the time that the web application is initialized, it\u0027s able to successfully connect to the database. If done the other way around, an error will be thrown because the database will not be listening for connections and the web application won\u0027t be able to connect. Another way to ensure that our database is initialized prior to our web application is to use the \u0060links\u0060 option in our \u0060web\u0060 service and add the name of the database service \u0060db\u0060 to the list of dependent services. The \u0060ports\u0060 option like in our \u0060Dockerfile\u0060 defines which ports that container will need to operate. In this case, our \u0060web\u0060 app listens on port 3000 and the \u0060db\u0060 service will listen on port 27017. #### Container Images The \u0060docker-compose.yml\u0060 can build containers based on images hosted in a registry as well as those defined by a \u0060Dockerfile\u0060. To use images from a registry, we use the \u0060image\u0060 option inside of our service. Take the \u0060db\u0060 service for example. \u0060MongoDB\u0060 already has an image in the \u0060DockerHub\u0060 registry which we will use to build the container. Our \u0060web\u0060 container does not have an image that is listed in a registry. However, we can still build an image based off a \u0060Dockerfile\u0060. To do this, we use the \u0060build\u0060 option inside our service and pass the directory of the respective \u0060Dockerfile\u0060 containing the build instructions for the container. ## Building and Running Containers Now that our container definitions and files are set up, we\u0027re ready to build and run our application. This can be done by typing \u0060docker-compose up -d\u0060 in the terminal from inside our local project directory. The \u0060-d\u0060 option runs the command detached allowing us to continue using the terminal. This command will both build and start our containers simultaneously. Once the \u0060web\u0060 and \u0060db\u0060 containers are up and running, we can visit \u0060http://localhost:3000\u0060 from our browser to view and test our application. To stop the application, inside the local project directory, we can type \u0060docker-compose stop\u0060 in our terminal to stop both containers. ## Conclusion This writeup uses a pre-configured MEAN stack CRUD todo application and explores how to define a single container \u0060Dockerfile\u0060 as well as a multi-container application using \u0060docker-compose\u0060. Docker streamlines how applications are built and deployed while \u0060docker-compose\u0060 allows more complex multi-container applications to be orchestrated, linked, deployed and managed simultaneously allowing developers to spend more time developing solutions and less time managing infrastructure and dependencies. ###### Links/Resources [Docker Community Edition](https://www.docker.com/community-edition#/download) [Docker: Getting Started](https://docs.docker.com/get-started/) [NodeJS](https://nodejs.org/en/)","contentType":"posts","url":"https://www.lqdev.me/posts/e2e-mean-docker-build","date":"2018-04-29 20:10:09 -05:00","tags":["devops","MEAN","mongodb","docker","angular","angularjs","nodejs","javascript","programming","development","docker-compose"],"summary":"## Introduction Lately I\u0027ve been getting familiar with [Docker](https://docker.com). I have built single container applications using a \u0060Dockerfile\u0060 and run them locally. This works fine, especially...","keywords":["application","container","directory","file","\u0060docker","command","\u0060dockerfile\u0060","database","using","compose"]},{"id":"posts-type-driven-development-scaling-safely-with-python","title":"Type Driven Development - Scaling Safely with Python","content":"# Introduction Python is a great language. The syntax is readable and allows pseudocode to be converted into code nearly verbatim. While this is great for prototyping and moving fast, scaling can become an issue. One of the issues is with regards to documentation. In statically typed languages, even if there\u0027s no documentation, types help provide some sort of documentation that allow new contributors as well current developers to remember where they left off and what their code does. There are ways around this using docstrings as well as unit tests. However, this often involves performing tasks outside of writing code which can be time consuming. In Python 3.5, type hints or optional static typing are allowed and tools like \u0060mypy\u0060 help write safer, more scalable code. The best part is, if the code already has docstrings and unit tests, optional static typing adds an additional layer of safety and documentation to existing projects. This writeup explores practices for documenting and developing scalable Python code as well as illustrating how to use optional static types and type checkers. ## Prerequisites This writeup assumes that Python 3.5 or greater is being used and both \u0060mypy\u0060 and \u0060pytest\u0060 packages are installed. To install them using \u0060pip\u0060 we can type the following command in the shell: \u0060\u0060\u0060bash pip install pytest mypy \u0060\u0060\u0060 ## Docstrings [PEP 257](https://www.python.org/dev/peps/pep-0257/) provides a great overview of what docstrings are and how to use them. The summarized version of it is a string literal in classes and functions that allows developers to document logic, inputs and outputs of those particular sections of code. Below are examples of code with and without docstrings: ### No Docstring \u0060\u0060\u0060python def combine(a,b): return a \u002B b \u0060\u0060\u0060 ### With Docstring \u0060\u0060\u0060python def combine(a,b): \u0022\u0022\u0022 Returns the sum of two numbers Keyword arguments: a -- the first number b -- the second number Returns: Sum of a and b \u0022\u0022\u0022 return a \u002B b \u0060\u0060\u0060 As we can see, the string literal or docstring allows individuals who are looking at the code for the first time as well as someone who worked on it and has forgotten the logic of a program to easily decipher what the code does. Something else to notice is the function name. In this case, the logic is relatively simple and the name may make some sense at the time of writing the code. However, this simple logic is dangerous. Without knowing what is expected as input and output, there\u0027s not a clear way of knowing what this code should do. For example, someone might try to run the undocumented version of this function with parameter \u0060a\u0060 having the value \u0027Hello\u0027 and \u0060b\u0060 with the value \u0027World\u0027. The function would not throw any errors and return \u0027HelloWorld\u0027 as a result. However, if we look at the intented logic as well as expected input and output provided by the docstring, we\u0027d know that \u0060a\u0060 and \u0060b\u0060 are both supposed to be numerical values, not strings or any other type. It\u0027s clear that writing a docstring can become tedious and take up a substantial amount of time as a project grows. However, the benefits are reaped when extending the code and using it in production because more time is spent being productive rather than figuring out what the code should do and whether it\u0027s being done correctly. Docstrings however are not a panacea since there is no way to enforce what is documented in the code and serves as more of an FYI for developers using and extending the code. ## Unit Testing One way to prevent code from being misused is by writing tests. By writing unit tests and making sure that they pass, developers can test edge cases such as passing a string and immediately getting feedback through failing tests. Here\u0027s an example of what a unit test would look like for the \u0060combine\u0060 function written above. In the file \u0060main.py\u0060, we can write the logic for our \u0060combine\u0060 function. However, keeping in mind the docstring, we might want to add some exception handling. \u0060\u0060\u0060python \u0022\u0022\u0022 Module Containing Logic \u0022\u0022\u0022 def combine(a,b): \u0022\u0022\u0022 Returns the sum of two numbers Keyword arguments: a -- the first number b -- the second number Returns: Sum of a and b \u0022\u0022\u0022 if(type(a) == str or type(b) == str) return a \u002B b else: raise TypeError \u0060\u0060\u0060 In another file called \u0060test_main.py\u0060, we can write our tests. Our test file will look like the code below: \u0060\u0060\u0060python import pytest from main import combine testparams = [(1,2,3),(2,4,6)] @pytest.mark.parametrize(\u0022a,b,expected\u0022,testparams) def test_combine(a,b,expected): assert combine(a,b) == expected testparams = [(\u0027a\u0027,\u0027b\u0027),(\u0027a\u0027,\u0027b\u0027)] @pytest.mark.parametrize(\u0022a,b\u0022,testparams) def test_combine_exception(a,b): with pytest.raises(TypeError): combine(a,b) \u0060\u0060\u0060 In our shell we can enter the \u0060pytest\u0060 command inside of our project directory and get the following output. \u0060\u0060\u0060bash pytest \u0060\u0060\u0060 ![](http://cdn.lqdev.tech/files/images/typedrivendevelopment1.png) The results from \u0060pytest\u0060 ensure that passing the expected parameters returns the expected output which is the sum of two numbers and passing in the wrong parameters such as those of type string return a \u0060TypeError\u0060. This gets us closer to where we want to be where we\u0027re able to test whether the functionality of our application does what it\u0027s supposed to. Like docstrings, there is additional work and time that needs to be accounted for when writing tests. However, this is a practice that should be taking place already and in the case of Python which does not provide the type checking or compilation is a way to if not ensure that our logic is sound, at least it provides us with an additional form of documentation and peace of mind that the code is being used accurately. ## Type Hints (Optional Static Types) Good practice would have us write docstrings to document our code and unit tests to ensure the soundness of our logic and code. However, what if that seems like too much work or there\u0027s not much time to perform those tasks. Is there a shorthand way that we can both document our code for posterity as well as ensure that we can only use the code as intended. That\u0027s where type hints comes in and starting with Python 3.5 have been accepted by the Python community per [PEP 484](https://www.python.org/dev/peps/pep-0484/). With type hints our code would not change much and with a few extra characters, we can write safer code. Our \u0060combine\u0060 function from previous examples would look as follows with type hints: \u0060\u0060\u0060python def combine(a:float,b:float) -\u003E float: return a \u002B b \u0060\u0060\u0060 If we run this, it should run as expected given the appropriate parameters. That being said, as with the undocumented example, if we pass in parameters \u0027Hello\u0027 and \u0027World\u0027, it should work as well and we get the result \u0027HelloWorld\u0027. If we still don\u0027t get the result we want and our code is still unsafe, then what\u0027s the point? One of the benefits is the documentation piece. In the event that we had no docstring, we could still tell that \u0060a\u0060 and \u0060b\u0060 are both of type \u0060float\u0060 and return a \u0060float\u0060. The second benefit comes from the use of \u0060mypy\u0060, a type checker for Python. To see it in action, we can create a script called \u0060mainmypy.py\u0060 and add the following code: \u0060\u0060\u0060python def combine(a:float,b:float) -\u003E float: return a \u002B b combine(1,2) combine(\u0027Hello\u0027,\u0027World\u0027) \u0060\u0060\u0060 In the shell, we can then use the \u0060mypy\u0060 command on our script to check types. \u0060\u0060\u0060bash mypy mainmypy.py \u0060\u0060\u0060 The output is the following: ![](http://cdn.lqdev.tech/files/images/typedrivendevelopment2.png) As we can see, without having to run our code, \u0060mypy\u0060 checks the types and throws errors that we would not have found unless we ran our code. Therefore, we get both documentation by defining the types of parameters and outputs we expect which make it easier for individuals using or writing code to safely do so without having to write long, descriptive docstrings. With \u0060mypy\u0060, we enforce the good use of code by checking that the correct parameters are being passed in and the correct results are being returned prior to runtime making it safe to scale and write correct code most of the time. ## Conclusion Python is a very expressive language that allows applications to be prototyped in no time. However, the tradeoff is that writing new code or returning to it at a later time without documenting it, particularly the types needed by functions or classes to produce an accurate result can be unsafe. Some existing practices such as docstrings and unit tests can help with documenting and writing safe code. However, tools like \u0060mypy\u0060 and the recently introduced type hints achieve what both docstrings and unit tests do in less time and code. This is not to say that these tools are perfect and ideally, unit tests, docstrings and type hints are all integrated to make developers more productive and create safe, scalable code. Happy coding!","contentType":"posts","url":"https://www.lqdev.me/posts/type-driven-development-scaling-safely-with-python","date":"2018-03-24 18:16:31 -05:00","tags":["python","static typing","productivity","programming","development"],"summary":"# Introduction Python is a great language. The syntax is readable and allows pseudocode to be converted into code nearly verbatim. While this is great for prototyping and moving fast, scaling can...","keywords":["code","type","combine","python","tests","docstrings","however","logic","unit","writing"]},{"id":"posts-scaffold-a-webpage-in-one-line","title":"Scaffolding A Web Page In One Line","content":"## Introduction A few months ago, I saw VS Code released an update that had [Emmet](https://www.emmet.io/) abbreviations built in. At the time, I saw a demo and it looked like an interesting tool but I knew very little about it. This past week, while practicing \u0060css\u0060 and scaffolding a blog-like web page for practice I decided to give Emmet a try. After reading some of the documentation and trying it out, within five minutes I was able to replicate what I had done manually with one line of \u0022code\u0022. The entire process is described below. ## What We\u0027ll Build Displayed below is the scaffold of the webpage I built. As it can be seen, in the \u0060body\u0060 element there is a \u0060nav\u0060 element and four \u0060div\u0060 elements that will contain our articles and their metadata. \u0060\u0060\u0060html \u0060\u0060\u0060 ## How We\u0027ll Build It The \u0060hmtl\u0060 above can be programatically built using the abbreviation below. \u0060\u0060\u0060text html\u003E(head\u003Etitle\u002Blink)\u002B(body\u003E(nav\u003Eul\u003Eli.nav-item*3)\u002B(div\u003Ediv.article\u003Eh2.article-title\u002Bh6.article-date\u002Bp.article-description)*4 \u0060\u0060\u0060 To break down some of the things that are going on here, we can start with the syntax. The \u0060\u003E\u0060 character denotes that the item on the left is the parent of the item(s) on the right. Similarly, the \u0060\u002B\u0060 character is used to describe siblings. Similar to math operations, parentheses evaluate what\u0027s inside them. Elements themselves can contain additional data such as class names and ids. This data can be appended to the element name just like \u0060css\u0060 with \u0060.\u0060 for classes and \u0060#\u0060 for ids. With that knowledge, we can see that we have an \u0060html\u0060 root element with \u0060head\u0060 and \u0060body\u0060 children. The \u0060head\u0060 element has a \u0060title\u0060 and \u0060link\u0060 child elements. The \u0060body\u0060 element has a \u0060nav\u0060 element with a \u0060ul\u0060 child which contains three \u0060li\u0060 elements. The \u0060nav\u0060 element has a \u0060div\u0060 sibling which contains four \u0060div\u0060 children that serve as the containers for our articles and their metadata. ## Adding some style To help visualize what the page looks like we can add some \u0060css\u0060. \u0060\u0060\u0060css * { margin: 0; } nav { padding: 0; background-color: black; color: white; } .nav-item { margin: 20px; display: inline-block; font-weight: bold; font-size: 20px; } .nav-item:hover { opacity: 0.7; } .article { display: block; margin: 20px; padding-left: 5px; background-color: lightgray; border: black solid 2px; } .article-title { display: block; font-family: serif; font-size: 30px; text-decoration: underline; display: inline; } .article-date { font-size: 20px; font-family: fantasy; float: right; margin-right: 5px; } .article-description { display: block; font-size: 18px; font-family: sans-serif; margin-top: 5px; } \u0060\u0060\u0060 ## Result After linking our \u0060css\u0060 with the webpage as well as adding some content to the page, the result will look as follows. ![](http://cdn.lqdev.tech/files/images/scaffoldawebpageoneline1.png) \u0060\u0060\u0060html HTML CSS Practice Home About Contact Hello World January 8, 2017 This is a short description of my program. There are more things than this. Second Post February 8, 2017 I think that I got my feet wet a little bit. Maybe there are other things that I can write about Third Post March 8, 2017 This is the third go around. I think I got pretty good at this design stuff. Fourth Post June 8, 2017 I really like my description of the things that are happening here and there is another post on the way very soon. \u0060\u0060\u0060 ###### Sources [Emmet](https://www.emmet.io/)","contentType":"posts","url":"https://www.lqdev.me/posts/scaffold-a-webpage-in-one-line","date":"2018-03-10 16:07:00 -05:00","tags":["html","web development","development","tools","emmet"],"summary":"## Introduction A few months ago, I saw VS Code released an update that had [Emmet](https://www.emmet.io/) abbreviations built in. At the time, I saw a demo and it looked like an interesting tool but...","keywords":["font","element","article","emmet","display:","block","2017","elements","post","description"]},{"id":"posts-testing-deploying-python-projects-travisci","title":"Testing and Deploying Python Projects with Travis CI","content":"# Introduction When working on projects, especially those that others or yourself may depend upon, it is important to test to make sure that everything is working as expected. Furthermore, being able to deploy your code/packages to a central repository with a package manager makes distribution easier. Although this can all be done manually, it can also be automated. Both testing and deployment can be automated using Travis CI which makes the entire process as easy as pushing your most recent changes to GitHub. In this writeup, I will go over how to create a Python project with unit tests and deploy to PyPI. A sample project can be found at this [link](https://github.com/lqdev/TravisTest) ## Prerequisites - [GitHub Login](https://github.com/) - [PyPI Login](https://pypi.python.org/pypi) - [virtualenv](https://virtualenv.pypa.io/en/stable/) ### Install virtualenv \u0060\u0060\u0060bash sudo pip install virtualenv \u0060\u0060\u0060 # Create The Project For the test project, I will create a module that performs adding, subtracting, increment and decrement operations. ## Define Folder Structure We start out by creating a directory for our project. \u0060\u0060\u0060bash mkdir travistest \u0060\u0060\u0060 Inside that directory, we want to have have a directory for our module as well as for our tests. Therefore, we need to create a directory for both of them. \u0060\u0060\u0060bash mkdir travistest mkdir test \u0060\u0060\u0060 Finally, we want to initialize the virtual environment for our project. To do so, we can use the \u0060virtualenv\u0060 package. For the project \u0060python 3.5\u0060 is the version that will be used. \u0060\u0060\u0060bash virtualenv -p python3.5 ENV \u0060\u0060\u0060 After installation, a folder with the name \u0060ENV\u0060 should appear in the root directory of the project. The final directory structure should look like so: \u0060\u0060\u0060text travistest |_ENV |_travistest |_test \u0060\u0060\u0060 ## Install Modules For this project, I\u0027ll be using \u0060pytest\u0060 for unit testing. Before installing anything however, I\u0027ll need to activate the virtual environment. \u0060\u0060\u0060bash source ENV/bin/activate \u0060\u0060\u0060 Once our virtual environment is activated, we can install \u0060pytest\u0060. \u0060\u0060\u0060bash pip install pytest \u0060\u0060\u0060 After installation, we can persist installed packages inside a \u0060requirements.txt\u0060 file with the \u0060freeze\u0060 command. \u0060\u0060\u0060bash pip freeze \u003E requirements.txt \u0060\u0060\u0060 ## Create The Module Inside the \u0060travistest\u0060 module directory, the easiest way to create a module is to include an \u0060__init__.py\u0060 file inside the directory. It\u0027s okay if it\u0027s empty. Therefore, we can start by creating the \u0060__init__\u0060 file in that directory. \u0060\u0060\u0060bash touch __init__.py \u0060\u0060\u0060 Once that\u0027s created, we can begin writing the main functionality of our module. Inside a file called \u0060Operations.py\u0060, we can put the following code in. \u0060\u0060\u0060python class Operations: def __init__(self): pass def add(self,x,y): return x \u002B y def subtract(self,x,y): return x - y def increment(self,x): return x \u002B 1 def decrement(self,x): return x - 1 \u0060\u0060\u0060 ## Unit Test Once we have our code, we need to write tests for it. Navigating to the \u0060test\u0060 directory, we can add the following code to the \u0060test_operations.py\u0060 file. \u0060\u0060\u0060python from pytest import fixture @fixture def op(): from travistest.Operations import Operations return Operations() def test_add(op): assert op.add(1,2) == 3 def test_subtract(op): assert op.subtract(2,1) == 1 def test_increment(op): assert op.increment(1) == 2 def test_decrement(op): assert op.decrement(2) == 1 \u0060\u0060\u0060 To make sure everything is working correctly, from the project\u0027s root directory, we can run the \u0060pytest\u0060 command. If all goes well, an output similar to the one below should appear. \u0060\u0060\u0060bash ============================= test session starts ============================== platform linux -- Python 3.5.2, pytest-3.4.0, py-1.5.2, pluggy-0.6.0 collected 4 items test/test_operations.py .... [100%] =========================== 4 passed in 0.04 seconds =========================== \u0060\u0060\u0060 # Prepare For Deployment To prepare for deployment and uploading to PyPI, we need to add a \u0060setup.py\u0060 file to the root directory of our project. The contents of this file for our purposes are mostly metadata that will populate information in PyPI. \u0060\u0060\u0060python from distutils.core import setup setup( name=\u0027travistest\u0027, packages=[\u0027travistest\u0027], version=\u00270.0.7\u0027, description=\u0027Test project to get acquainted with TravisCI\u0027, url=\u0027https://github.com/lqdev/TravisTest\u0027, ) \u0060\u0060\u0060 ## Setup Travis ### Enable Repository Assuming that you have a \u0060GitHub\u0060 login and a repository has been created for your project: 1. Log into [travis-ci.org](https://travis-ci.org/) with your GitHub credentials. 2. Once all of your repositores are synced, toggle the switch next to the repository containing your project. ### Configure .travis.yml Once the project has been enabled, we need fo configure Travis. This is all done using the \u0060.travis.yml\u0060 file. In this file we\u0027ll tell Travis that the language of our project is Python version 3.5 and that we\u0027ll be using a virtual environment. Additionally we\u0027ll require sudo priviliges and target the Ubuntu Xenial 16.04 distribution. All of these configurations can be done as follows. \u0060\u0060\u0060yaml sudo: required dist: xenial language: python virtualenv: system_site_packages: true python: - \u00273.5\u0027 \u0060\u0060\u0060 Once that is set up, we can tell it to install all of the dependencies stored in our \u0060requirements.txt\u0060 file. \u0060\u0060\u0060yaml install: - pip install -r requirements.txt \u0060\u0060\u0060 After this, we need to tell Travis to run our tests just like we would on our local machine. \u0060\u0060\u0060yaml script: pytest \u0060\u0060\u0060 Once our tests have run, we need to make sure we are back in the root directory of our project for deployment. \u0060\u0060\u0060yaml after_script: cd ~ \u0060\u0060\u0060 We\u0027re done with the automated testing script portion of our project. Now we need to setup deployment options. This section will mainly contain the credentials of your PyPI account. \u0060\u0060\u0060yaml deploy: provider: pypi user: \u0022YOURUSERNAME\u0022 \u0060\u0060\u0060 After setting the provider and user, we need to set the password. Because this will be a public repository DO NOT enter your password on this file. Instead, we can set an encrypted version that only Travis can decrypt. To do so, while in the root directory of our project, we can enter the following command into the terminal. \u0060\u0060\u0060bash travis encrypt --add deployment.password \u0060\u0060\u0060 Type your password into the terminal and press \u0060Ctrl \u002B D\u0060. If you take a look at your \u0060.travis.yml\u0060 file you should see something like the following in your \u0060deploy\u0060 options \u0060\u0060\u0060bash deploy: provider: pypi user: \u0022YOURUSERNAME\u0022 password: secure: \u0022YOURPASSWORD\u0022 \u0060\u0060\u0060 The final \u0060.travis.yml\u0060 file should like like so \u0060\u0060\u0060yaml sudo: required dist: xenial language: python virtualenv: system_site_packages: true python: - \u00273.5\u0027 install: - pip install -r requirements.txt script: pytest after_script: cd ~ deploy: provider: pypi user: \u0022YOURUSERNAME\u0022 password: secure: \u0022YOURPASSWORD\u0022 \u0060\u0060\u0060 # Deploy Now that we have everything set up, deployment should be relatively easy. A build is triggered when changes are pushed to the repository on GitHub. Therefore, pushing your local changes to the remote GitHub repository should initialize the build. To track progress, visit the project\u0027s page on Travis CI. **NOTE: WHEN PUSHING NEW CHANGES, INCREMENT VERSION NUMBER IN THE \u0060SETUP.PY\u0060 FILE SO THAT EXISTING FILE ERRORS DO NOT CRASH BUILDS** # Conclusion In this writeup, we created a Python package that performs basic operations and set up automated testing and deployment with Travis CI. Configurations can be further customized and refined to adapt more complex build processes.","contentType":"posts","url":"https://www.lqdev.me/posts/testing-deploying-python-projects-travisci","date":"2018-02-18 20:51:32 -05:00","tags":["devops","travisci","python","ci/cd","programming","development"],"summary":"# Introduction When working on projects, especially those that others or yourself may depend upon, it is important to test to make sure that everything is working as expected. Furthermore, being able...","keywords":["project","file","directory","travis","test","\u0060\u0060\u0060bash","need","deployment","pypi","install"]},{"id":"posts-transcribing-podcasts-microsoft-speech-api","title":"Transcribing Podcasts with Microsoft Speech API","content":"# Introduction I enjoy listening to podcasts in a wide range of topics that include but are not limited to politics, software development, history, comedy and true crime. While some of the more entertainment related podcasts are best enjoyed through audio, those that are related to software development or other type of hands-on topics would benefit greatly from having a transcript. Having a transcript allows me to go back after having listened to an interesting discussion and look directly at the content I am interested in without having to listen to the podcast again. This however is not always feasible given that it costs both time and money to produce a transcript. Fortunately, there are tools out there such as Google Cloud Speech and Microsoft Speech API which allow users to convert speech to text. For this writeup, I will be focusing on the Microsoft Speech API. Because podcasts tend to be long-form, I will be using the C# client library because it allows for long audio (greater than 15 seconds) to be transcribed. The purpose of this exercise is to create a console application that takes audio segments, converts them to text and stores the results in a text file with the goal of evaluating how well the Microsoft Speech API works. The source code for the console application can be found on [GitHub](https://github.com/lqdev/PodcastsBingSpeechAPIDemo) ## Prerequisites - [Visual Studio Community](https://www.visualstudio.com/vs/community/) - [Windows Subsystem for Linux (Ubuntu)](https://www.microsoft.com/store/productId/9NBLGGH4MSV6) - [ffmpeg](https://ffmpeg.org/) ## Install/Enable Windows Subsystem for Linux 1. Open Powershell as Administrator and input \u0060\u0060\u0060powershell Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux \u0060\u0060\u0060 2. Restart your computer 3. Open the Microsoft Store and install [Ubuntu](https://www.microsoft.com/store/productId/9NBLGGH4MSV6) Linux distribution 4. Once installed, click \u0060Launch\u0060 5. Create your LINUX user account (Keep in mind that this is not the same as your Windows account therefore it can be different). ## Install ffmpeg Once Ubuntu is installed on your computer and you have created your LINUX user account, it\u0027s time to install \u0060ffmpeg\u0060. This will allow us to convert our file from \u0060mp3\u0060 which is usually the format podcasts are in to \u0060wav\u0060 which is the format accepted by the Microsoft Speech API \u0060\u0060\u0060bash sudo apt install -y ffmpeg \u0060\u0060\u0060 ## Get Bing Speech API Key In order to use the Microsoft Speech API, an API key is required. This can be obtained using the following steps. 1. Navigate to the [Azure Cognitive Services](https://azure.microsoft.com/en-us/try/cognitive-services/) page. 2. Select the \u0060Speech\u0060 tab 3. Click \u0060Get API Key\u0060 and follow the instructions 4. Once you have an API key, make sure to store it somewhere like a text file in your computer for future use. # File Prep Before transcribing the files, they need to be converted to \u0060wav\u0060 format if they are not already in it. In order to ease processing, they should be split into segments as opposed to having the API process an entire multi megabyte file. To do all this, inside the Ubuntu shell, first navigate to your \u0060Documents\u0060 folder and create a new directory for the project. \u0060\u0060\u0060bash cd /mnt/c/Users/ /Documents mkdir testspeechapi \u0060\u0060\u0060 \u0060USERNAME\u0060 is your Windows user name. This can be found by typing the following command into the Windows \u0060CMD\u0060 prompt \u0060\u0060\u0060bash echo %USERNAME% \u0060\u0060\u0060 ## Download Audio Files One of the podcasts I listen to is [Talk Python To Me](https://talkpython.fm/) by [Michael Kennedy](https://twitter.com/mkennedy). For the purpose of this exercise, aside from having great content, all of the episodes are transcribed and a link is provided to the \u0060mp3\u0060 file. The file I will be using is from episode [#149](https://talkpython.fm/episodes/show/149/4-python-web-frameworks-compared), but it can easily be any of the episodes. In the Ubuntu shell, download the file into the \u0060testspeechapi\u0060 directory that was recently created using \u0060wget\u0060. \u0060\u0060\u0060bash wget https://talkpython.fm/episodes/download/149/4-python-web-frameworks-compared.mp3 -O originalinput.mp3 \u0060\u0060\u0060 ## Download Transcripts The transcripts can be found in GitHub at this [link](https://github.com/mikeckennedy/talk-python-transcripts/tree/master/transcripts). The original transcript will allow me to compare it to the output of the Microsoft Speech API and evaluate the accuracy. We can download this file into the \u0060testspeechapi\u0060 directory just like the audio file \u0060\u0060\u0060bash wget https://raw.githubusercontent.com/mikeckennedy/talk-python-transcripts/master/transcripts/149.txt -O originaltranscript.txt \u0060\u0060\u0060 ## Convert MP3 to WAV Now that we have both the original audio file and transcript, it\u0027s time to convert the format from \u0060mp3\u0060 to \u0060wav\u0060. To do this, we can use \u0060ffmpeg\u0060. In the Ubuntu shell, enter the following command. \u0060\u0060\u0060bash ffmpeg -i originalinput.mp3 -f wav originalinput.wav \u0060\u0060\u0060 ## Create Audio Segments Once we have the proper format, it\u0027s time to make the files more manageable for processing. This can be done by splitting them up into equal segments using \u0060ffmpeg\u0060. In this case I\u0027ll be splitting it up into sixty second segments and storing them into a directory called \u0060input\u0060 with the name \u0060filexxxx.wav\u0060. The \u0060%04d\u0060 indicates that there will be 4 digits in the file name. Inside the Ubuntu shell and \u0060testspeechapi\u0060 directory enter the following commands. \u0060\u0060\u0060bash mkdir input ffmpeg -i originalinput.wav -f segment -segment_time 60 -c copy input/file%04d.wav \u0060\u0060\u0060 # Building The Console Application To get started building the console application, we can leverage the [Cognitive-Speech-STT-ServiceLibrary](https://github.com/Azure-Samples/Cognitive-Speech-STT-ServiceLibrary) sample program. ## Download The Sample Program The first step will be to clone the sample program from GitHub onto your computer. In the Windows \u0060CMD\u0060 prompt navigate to the \u0060testspeechapi\u0060 folder and enter the following command. \u0060\u0060\u0060bash git clone https://github.com/Azure-Samples/Cognitive-Speech-STT-ServiceLibrary \u0060\u0060\u0060 Once the project is on your computer, open the directory and launch the \u0060SpeechClient.sln\u0060 solution in the \u0060sample\u0060 directory When the solution launches, open the \u0060Program.cs\u0060 file and begin making modifications. ## Reading Files To read files, we can create a function that will return the list of files in the specified directory. \u0060\u0060\u0060csharp private static string[] GetFiles(string directory) { string[] files = Directory.GetFiles(directory); return files; } \u0060\u0060\u0060 ## Process Files Once we have the list of files, we can then process each file individually using the \u0060Run\u0060 method. To do so, we need to make a slight modification to our \u0060Main\u0060 method so that it iterates over each file and calls the \u0060Run\u0060 method on it. To store the responses from the API, we\u0027ll also need a \u0060StringBuilder\u0060 object which is declared at the top of our \u0060Program.cs\u0060 file. \u0060\u0060\u0060csharp finalResponse = new StringBuilder(); string[] files = GetFiles(args[0]); foreach(var file in files) { p.Run(file,\u0022en-us\u0022,LongDictationUrl,args[1]).Wait(); Console.WriteLine(\u0022File {0} processed\u0022,file); } \u0060\u0060\u0060 ## Transcribe Audio The \u0060Run\u0060 method can be left intact. However, the \u0060Run\u0060 method uses the \u0060OnRecognitionResult\u0060 method to handle the result of API responses. In the \u0060OnRecognitionResult\u0060 method, we can remove almost everything that is originally there and replace it. The response from the API returns various results of potential phrases as well as a confidence value. Generally, most of the phrases are alike and the first value is good enough for our purposes. The code for this part will take the response from the API, append it to a \u0060StringBuilder\u0060 object and return when completed. \u0060\u0060\u0060csharp public Task OnRecognitionResult(RecognitionResult args) { var response = args; if(response.Phrases != null) { finalResponse.Append(response.Phrases[0].DisplayText); finalResponse.Append(\u0022\\n\u0022); } return CompletedTask; } \u0060\u0060\u0060 ## Output Transcribed Speech When all the audio files have been processed, we can save the final output to a text file in the \u0060testspeechapi\u0060 directory. This can be done with the \u0060SaveOutput\u0060 function to which we pass in a file name and the \u0060StringBuilder\u0060 object that captured responses from the API. \u0060\u0060\u0060csharp private static void SaveOutput(string filename,StringBuilder content) { StreamWriter writer = new StreamWriter(filename); writer.Write(content.ToString()); writer.Close(); } \u0060\u0060\u0060 \u0060SaveOutput\u0060 can then be called from our \u0060Main\u0060 method like so. \u0060\u0060\u0060csharp string username = Environment.GetEnvironmentVariable(\u0022USERNAME\u0022, EnvironmentVariableTarget.Process); SaveOutput(String.Format(@\u0022C:\\\\Users\\\\{0}\\\\Documents\\\\testspeechapi\\\\apitranscript.txt\u0022,username), finalResponse); \u0060\u0060\u0060 The final \u0060Main\u0060 method should look similar to the code below \u0060\u0060\u0060csharp public static void Main(string[] args) { // Send a speech recognition request for the audio. finalResponse = new StringBuilder(); string[] files = GetFiles(args[0]); var p = new Program(); foreach (var file in files) { p.Run(file, \u0022en-us\u0022, LongDictationUrl, args[1]).Wait(); Console.WriteLine(\u0022File {0} processed\u0022, file); } string username = Environment.GetEnvironmentVariable(\u0022USERNAME\u0022, EnvironmentVariableTarget.Process); SaveOutput(String.Format(@\u0022C:\\\\Users\\\\{0}\\\\Documents\\\\testspeechapi\\\\apitranscript.txt\u0022,username), finalResponse); } \u0060\u0060\u0060 # Output The program will save the output of the \u0060StringBuilder\u0060 object \u0060finalResponse\u0060 to the file \u0060apitranscript.txt\u0060. Prior to running the program it needs to be built. In Visual Studio change the Solutions Configurations option from \u0060Debug\u0060 to \u0060Release\u0060 and build the application. To run the program, navigate to the \u0060C:\\Users\\%USERNAME%\\Documents\\testspeechapi\\Cognitive-Speech-STT-ServiceLibrary\\sample\\SpeechClientSample\\bin\\Release\u0060 directory in the Windows \u0060CMD\u0060 prompt and enter the following command and pass in the \u0060input\u0060 directory where all the audio segments are stored and your API Key. \u0060\u0060\u0060bash SpeechClientSample.exe C:\\\\Users\\\\%USERNAME%\\\\Documents\\\\testspeechapi\\\\input \u0060\u0060\u0060 This process may take a while due to the number of files being processed, so be patient. ## Sample Output Original \u003E Are you considering getting into web programming? Choosing a web framework like Pyramid, Flask, or Django can be daunting. It would be great to see them all build out the same application and compare the results side-by-side. That\u0027s why when I heard what Nicholas Hunt-Walker was up to, I had to have him on the podcast. He and I chat about four web frameworks compared. He built a data-driven web app with Flask, Tornado, Pyramid, and Django and then put it all together in a presentation. We\u0027re going to dive into that right now. This is Talk Python To Me, Episode 149, recorded January 30th, 2018. Welcome to Talk Python to Me, a weekly podcast on Python, the language, the libraries, the ecosystem, and the personalities. This is your host, Michael Kennedy, follow me on Twitter where I\u0027m @mkennedy. Keep up with the show and listen to past episodes at talkpython.fm, and follow the show on Twitter via @talkpython. Nick, welcome to Talk Python. API Response \u003E Are you considering getting into web programming cheese in the web framework like plaster. Django can be daunting. It would be great to see them. Although that the same application and compare the results side by side. That\u0027s Why? When I heard. But Nicholas Hunt. Walker was up to, I had him on the podcast. He night chat about 4 web frameworks, compared he built a data driven web app with last tornado. Peermade Angangueo and then put it all together in a presentation. We have dive into that right now. This is talk by the to me, I was 149 recorded January 30th 2018. Welcome to talk python to me a weekly podcast on Python Language the library ecosystem in the personalities. Did you host Michael Kennedy? Follow me on Twitter where I\u0027m at in Kennedy keep up with the show in the past episodes at talk python. Damn info, the show on Twitter via at Popeyes on. Nick Welcome to talk by phone. # Conclusion In this writeup, I converted an \u0060mp3\u0060 podcast file to \u0060wav\u0060 format and split it into sixty second segments using \u0060ffmpeg\u0060. The segments were then processed by a console application which uses the Microsoft Speech API to convert speech to text and the results were saved to a text file. When compared to the original transcript, the result produced by the API is inconsistent and at times incomprehensible. This does not mean that overall the API is unusable or inaccurate as many things could have contributed to the inaccuracy. However, the results were not as good as I would have hoped for. ###### Sources - [Windows Subsystem for Linux Install Instructions](https://docs.microsoft.com/en-us/windows/wsl/install-win10) - [Microsoft Speech API](https://docs.microsoft.com/en-us/azure/cognitive-services/speech/home)","contentType":"posts","url":"https://www.lqdev.me/posts/transcribing-podcasts-microsoft-speech-api","date":"2018-02-11 20:34:37 -05:00","tags":["azure","csharp","apis","nlp","machine-learning","artificial-intelligence","microsoft"],"summary":"# Introduction I enjoy listening to podcasts in a wide range of topics that include but are not limited to politics, software development, history, comedy and true crime. While some of the more...","keywords":["file","speech","files","microsoft","directory","from","python","audio","string","windows"]},{"id":"posts-real-time-sentiment-analysis-csharp","title":"Real-Time Sentiment Analysis with C#","content":"###### This is strictly for use with the .NET Framework. With Mono it might be able to work on other platforms. \u0060SimpleNetNlp\u0060 does not currently work with .NET Core/Standard In this project, I will demonstate how to perform sentiment analysis on tweets using various C# libraries. ## Dependencies - TweetInviAPI - SimpleNetNlp - SimpleNetNlp.Models.LexParser - SimpleNetNlP.Models.Sentiment ## Create Console Application In Visual Studio, Click File \u003E New \u003E New Project \u003E Console Application All of the code below will be placed in the \u0060Program\u0060 class. ## Creating A Stream ### Authenticate Thanks to the \u0060Tweetinvi\u0060 library, the authentication with the Twitter API is a breeze. Assuming that an application has been registered at [http://apps.twitter.com](http://apps.twitter.com), the \u0060SetUserCredentials\u0060 method can be used and the \u0060Consumer Key\u0060, \u0060Consumer Secret\u0060,\u0060Access Token\u0060 and \u0060Access Token Secret\u0060 can be passed into it. This type of global authentication makes it easy to perform authenticated calls throughout the entire application. \u0060\u0060\u0060csharp Auth.SetUserCredentials(\u0022consumer-key\u0022,\u0022consumer-secret\u0022,\u0022access-token\u0022,\u0022access-token-secret\u0022); \u0060\u0060\u0060 ### Build Stream Like the authentication, creating a stream is seamless. We can create a stream by calling the \u0060CreateFilteredStream\u0060 method. \u0060\u0060\u0060csharp var stream = Stream.CreateFilteredStream(); \u0060\u0060\u0060 We can then add conditions to filter on using the \u0060AddTrack\u0060 method. In this case, I will be filtering for cryptocurrencies, ether, bitcoin, and litecoin. \u0060\u0060\u0060csharp stream.AddTrack(\u0022cryptocurrencies\u0022); stream.AddTrack(\u0022bitcoin\u0022); stream.AddTrack(\u0022ether\u0022); stream.AddTrack(\u0022Litecoin\u0022); \u0060\u0060\u0060 Additionally, we can filter by language. In my case, I will only be filtering on English. This can be done by using the \u0060AddTweetLanguageFilter\u0060 method. \u0060\u0060\u0060csharp stream.AddTweetLanguageFilter(\u0022en\u0022); \u0060\u0060\u0060 Once we have all the filters set up, we need to handle what will happen when a matching tweet is detected. This will be handled by an \u0060EventHandler\u0060 called \u0060MatchingTweetReceived\u0060. \u0060\u0060\u0060csharp stream.MatchingTweetReceived \u002B= OnMatchedTweet; \u0060\u0060\u0060 \u0060MatchingTweetReceived\u0060 will be bound to the \u0060OnMatchedTweet\u0060 method which I created. \u0060\u0060\u0060csharp private static void OnMatchedTweet(object sender, MatchedTweetReceivedEventArgs args) { //Do Stuff } \u0060\u0060\u0060 The logic inside of this method will perform sentiment analysis and output the sentiment as well as the full text of the tweet. ## Data Cleaning Tweets can contain many non-ascii characters. Therefore, we need to sanitize it as best as possible so that it can be processed by the sentiment analyzer. To help with that, I used regular expresions to replace non-ascii characters inside of the \u0060sanitize\u0060 method. \u0060\u0060\u0060csharp private static string sanitize(string raw) { return Regex.Replace(raw, @\u0022(@[A-Za-z0-9]\u002B)|([^0-9A-Za-z \\t])|(\\w\u002B:\\/\\/\\S\u002B)\u0022, \u0022 \u0022).ToString(); } \u0060\u0060\u0060 ## Sentiment Analysis In order to perform sentiment analysis, we will be using the \u0060SimpleNetNlp\u0060 library. This library is built on top of the Stanford CoreNLP library. In order to get the sentiment of a piece of text, we need to create a \u0060Sentence\u0060 object which takes a string as a parameter and then get the \u0060Sentiment\u0060 property. In our case, the parameter that will be used to instantiate a new \u0060Sentence\u0060 object will be the sanitized text of a tweet. \u0060\u0060\u0060csharp var sanitized = sanitize(args.Tweet.FullText); string sentence = new Sentence(sanitized); \u0060\u0060\u0060 The code above will be placed inside of the \u0060OnMatchedTweet\u0060 method. ## Produce Output Now that we have everything set up, we can just output to the console the sentiment and raw text of the tweet. To do that, we can place the code below inside the \u0060OnMatchedTweet\u0060 method. \u0060\u0060\u0060csharp Console.WriteLine(sentence.Sentiment \u002B \u0022|\u0022 \u002B args.Tweet); \u0060\u0060\u0060 The final \u0060OnMatchedTweet\u0060 method looks as follows: \u0060\u0060\u0060csharp private static void OnMatchedTweet(object sender, MatchedTweetReceivedEventArgs args) { var sanitized = sanitize(args.Tweet.FullText); //Sanitize Tweet var sentence = new Sentence(sanitized); //Get Sentiment //Output Tweet and Sentiment Console.WriteLine(sentence.Sentiment \u002B \u0022|\u0022 \u002B args.Tweet); } \u0060\u0060\u0060 ## Run Once we run the application, our console application should look something like this: ![](http://cdn.lqdev.tech/files/images/sentiment-analysis-1.png) ## Conclusion C# is not always the first language that comes to mind when doing analytics and machine learning. However, tasks such as sentiment analysis can be trivially performed thanks to libraries such as \u0060Tweetinvi\u0060 and \u0060SimpleNetNlp\u0060. In its current state, this application is not very useful because it just outputs to the console sentiments and the respective tweets. In order to make it more useful, we can collect and aggregate the data someplace for more robust analysis.","contentType":"posts","url":"https://www.lqdev.me/posts/real-time-sentiment-analysis-csharp","date":"2018-01-18 03:41:47 -05:00","tags":["data analysis","sentiment analysis","nlp","machine learning","csharp","c#","twitter","api",".net","dotnet"],"summary":"###### This is strictly for use with the .NET Framework. With Mono it might be able to work on other platforms. \u0060SimpleNetNlp\u0060 does not currently work with .NET Core/Standard In this project, I will...","keywords":["sentiment","stream","method","tweet","\u0060\u0060\u0060csharp","application","console","analysis","sentence","args"]},{"id":"posts-authorization-code-flow-python","title":"Authorization Code Authentication Flow in Python","content":"For the longest time, authentication had been my worst enemy. I took small steps towards becoming comfortable with it. At first I learned Implicit Authorization with AngularJS, then I learned Client Credentials Authorization with Python and C# and finally, I learned Authorization Code authentication with C#. The one that always gave me trouble was Authorization Code authentication because it requires user credentials. To help with that, I started with the \u0060WebAuthenticationBroker\u0060 in C#. Although, most of it was done for me in terms of creating a \u0060WebView\u0060 for users to authenticate with their credentials, I wanted to know whether I could do it myself without having to rely on a service to do it for me. First, I had to find an API to test this out with and in an attempt to reduce complexity, I decided to prototype it in Python. As an avid user of [Pocket](http://getpocket.com), I chose its API to learn Authorization Flow authentication. # Getting Started ## Register An Application In order to get started, we need to register an application with Pocket. This can be done by visiting the following [website](https://getpocket.com/developer/apps/new). ## Install Requests In order to create HTTP requests, we need to install the \u0060requests\u0060 module \u0060\u0060\u0060 pip install requests \u0060\u0060\u0060 # Get Request Token ## Import Modules \u0060\u0060\u0060python import requests import json \u0060\u0060\u0060 ## Prepare Request \u0060\u0060\u0060python consumer_key = \u0022YOUR_CONSUMER_KEY\u0022 request_url = \u0022https://getpocket.com/v3/oauth/request\u0022 headers = {\u0022Content-Type\u0022:\u0022application/json\u0022,\u0022X-Accept\u0022:\u0022application/json\u0022} redirect_uri = \u0022http://www.google.com\u0022 payload = {\u0022consumer_key\u0022:consumer_key,\u0022redirect_uri\u0022:redirect_uri} \u0060\u0060\u0060 Notice how the \u0060redirect_uri\u0060 is set to \u0060http://www.google.com\u0060. This, for the most part is irrelevant, especially for console/desktop applications. Since this is a console application and we are not hosting a server for it, the value assigned to this field is arbitrary. ## Make Request \u0060\u0060\u0060python request = requests.post(request_url,headers=headers,data=json.dumps(payload)) code = request.json()[\u0027code\u0027] code \u0060\u0060\u0060 # Redirect User to Pocket to Continue Authorization ## Import Modules \u0060\u0060\u0060python import webbrowser \u0060\u0060\u0060 \u0060\u0060\u0060python webbrowser.open(\u0027https://getpocket.com/auth/authorize?request_token=\u0027\u002B code \u002B \u0027\u0026redirect_uri=\u0027 \u002B redirect_uri) \u0060\u0060\u0060 This will open your default browser and redirect you to an authorization page. After logging in, should you accept giving the application access to your account, it should redirect you to \u0060http://www.google.com\u0060. Once on the Google page, you can close out of it. # Convert A Request Token Into A Pocket Access Token After authorizing the application to access your account, you need to exchange the code you received for an access token. ## Prepare the Request \u0060\u0060\u0060python access_token_url = \u0022https://getpocket.com/v3/oauth/authorize\u0022 payload = {\u0022consumer_key\u0022:consumer_key,\u0022code\u0022:code} \u0060\u0060\u0060 ## Get Access Token \u0060\u0060\u0060python access_token_request = requests.post(access_token_url,headers=headers,data=json.dumps(payload)) access_token = access_token_request.json()[\u0027access_token\u0027] access_token \u0060\u0060\u0060 # Make Authenticated Request In order to test whether the authentication was successful, try making an authenticated request. ## Prepare Request \u0060\u0060\u0060python get_url = \u0022https://getpocket.com/v3/get\u0022 get_payload = {\u0022consumer_key\u0022:consumer_key,\u0022access_token\u0022:access_token,\u0022contentType\u0022:\u0022article\u0022,\u0022count\u0022:10} get_request = requests.get(get_url,data=json.dumps(get_payload),headers=headers) \u0060\u0060\u0060 ## Get Response \u0060\u0060\u0060python response = get_request.json() \u0060\u0060\u0060 ## Sample Response \u0060\u0060\u0060json {\u0027complete\u0027: 1, \u0027error\u0027: None, \u0027list\u0027: {\u00271519049132\u0027: {\u0027excerpt\u0027: \u0027Heads up! As part of our efforts to improve security and standards-based interoperability, we have implemented several new features in our authentication flows and made changes to existing ones.\u0027, \u0027favorite\u0027: \u00270\u0027, \u0027given_title\u0027: \u0027\u0027, \u0027given_url\u0027: \u0027https://auth0.com/docs/api-auth/grant/authorization-code-pkce\u0027, \u0027has_image\u0027: \u00270\u0027, \u0027has_video\u0027: \u00270\u0027, \u0027is_article\u0027: \u00271\u0027, \u0027is_index\u0027: \u00270\u0027, \u0027item_id\u0027: \u00271519049132\u0027, \u0027resolved_id\u0027: \u00271519049132\u0027, \u0027resolved_title\u0027: \u0027Calling APIs from Mobile Apps\u0027, \u0027resolved_url\u0027: \u0027https://auth0.com/docs/api-auth/grant/authorization-code-pkce\u0027, \u0027sort_id\u0027: 5, \u0027status\u0027: \u00270\u0027, \u0027time_added\u0027: \u00271514493110\u0027, \u0027time_favorited\u0027: \u00270\u0027, \u0027time_read\u0027: \u00270\u0027, \u0027time_updated\u0027: \u00271514493110\u0027, \u0027word_count\u0027: \u0027445\u0027}, \u00271607678551\u0027: {\u0027excerpt\u0027: \u0027What is the smallest number of Democrats that could have changed the outcome of the 2016 United States presidential election by relocating to another state? And where should they have moved? It turns out this question is a variant of the knapsack problem, an NP-hard computer science classic.\u0027, \u0027favorite\u0027: \u00270\u0027, \u0027given_title\u0027: \u0027\u0027, \u0027given_url\u0027: \u0027https://mike.place/2017/ecknapsack/\u0027, \u0027has_image\u0027: \u00270\u0027, \u0027has_video\u0027: \u00270\u0027, \u0027is_article\u0027: \u00271\u0027, \u0027is_index\u0027: \u00270\u0027, \u0027item_id\u0027: \u00271607678551\u0027, \u0027resolved_id\u0027: \u00271607678551\u0027, \u0027resolved_title\u0027: \u0027The Electoral College and the knapsack problem\u0027, \u0027resolved_url\u0027: \u0027http://mike.place/2017/ecknapsack/\u0027, \u0027sort_id\u0027: 7, \u0027status\u0027: \u00270\u0027, \u0027time_added\u0027: \u00271514478701\u0027, \u0027time_favorited\u0027: \u00270\u0027, \u0027time_read\u0027: \u00270\u0027, \u0027time_updated\u0027: \u00271514478701\u0027, \u0027word_count\u0027: \u00271564\u0027}, \u00271616406703\u0027: {\u0027excerpt\u0027: \u0027OAuth 2.0 is a protocol that allows a user to grant limited access to their resources on one site, to another site. This is done without the users having to expose their credentials. According to OAuth\u2018s website the protocol is not unlike a valet key. Many luxury cars today come with a valet key.\u0027, \u0027favorite\u0027: \u00270\u0027, \u0027given_title\u0027: \u0027\u0027, \u0027given_url\u0027: \u0027https://auth0.com/docs/protocols/oauth2\u0027, \u0027has_image\u0027: \u00271\u0027, \u0027has_video\u0027: \u00270\u0027, \u0027is_article\u0027: \u00271\u0027, \u0027is_index\u0027: \u00270\u0027, \u0027item_id\u0027: \u00271616406703\u0027, \u0027resolved_id\u0027: \u00271616406703\u0027, \u0027resolved_title\u0027: \u0027OAuth 2.0\u0027, \u0027resolved_url\u0027: \u0027https://auth0.com/docs/protocols/oauth2\u0027, \u0027sort_id\u0027: 4, \u0027status\u0027: \u00270\u0027, \u0027time_added\u0027: \u00271514493120\u0027, \u0027time_favorited\u0027: \u00270\u0027, \u0027time_read\u0027: \u00270\u0027, \u0027time_updated\u0027: \u00271514493120\u0027, \u0027word_count\u0027: \u0027823\u0027}, \u00271859670338\u0027: {\u0027excerpt\u0027: \u0027Last Tuesday saw the official launch of Will Robots Take My Job? and 5 days later we have passed 500K visitors and 4M page views. To say this surpassed any expectations we had would be a major understatement. This is a recap of how the project got started and the first 5 days after launch.\u0027, \u0027favorite\u0027: \u00270\u0027, \u0027given_title\u0027: \u0027\u0027, \u0027given_url\u0027: \u0027https://hackernoon.com/from-idea-to-4m-page-views-in-4-weeks-622aa194787d?ref=quuu\u0027, \u0027has_image\u0027: \u00271\u0027, \u0027has_video\u0027: \u00271\u0027, \u0027is_article\u0027: \u00271\u0027, \u0027is_index\u0027: \u00270\u0027, \u0027item_id\u0027: \u00271859670338\u0027, \u0027resolved_id\u0027: \u00271773505341\u0027, \u0027resolved_title\u0027: \u0027From Idea to 4M Page Views in 4\\xa0Weeks\u0027, \u0027resolved_url\u0027: \u0027https://hackernoon.com/from-idea-to-4m-page-views-in-4-weeks-622aa194787d\u0027, \u0027sort_id\u0027: 0, \u0027status\u0027: \u00270\u0027, \u0027time_added\u0027: \u00271514494606\u0027, \u0027time_favorited\u0027: \u00270\u0027, \u0027time_read\u0027: \u00270\u0027, \u0027time_updated\u0027: \u00271514494606\u0027, \u0027word_count\u0027: \u00271837\u0027}, \u00272001727306\u0027: {\u0027amp_url\u0027: \u0027https://venturebeat.com/2017/12/22/how-ibm-builds-an-effective-data-science-team/amp/\u0027, \u0027excerpt\u0027: \u0027Data science is a team sport. This sentiment rings true not only with our experiences within IBM, but with our enterprise customers, who often ask us for advice on how to structure data science teams within their own organizations.\u0027, \u0027favorite\u0027: \u00270\u0027, \u0027given_title\u0027: \u0027\u0027, \u0027given_url\u0027: \u0027https://venturebeat.com/2017/12/22/how-ibm-builds-an-effective-data-science-team/\u0027, \u0027has_image\u0027: \u00271\u0027, \u0027has_video\u0027: \u00270\u0027, \u0027is_article\u0027: \u00271\u0027, \u0027is_index\u0027: \u00270\u0027, \u0027item_id\u0027: \u00272001727306\u0027, \u0027resolved_id\u0027: \u00272001727306\u0027, \u0027resolved_title\u0027: \u0027How IBM builds an effective data science team\u0027, \u0027resolved_url\u0027: \u0027https://venturebeat.com/2017/12/22/how-ibm-builds-an-effective-data-science-team/\u0027, \u0027sort_id\u0027: 1, \u0027status\u0027: \u00270\u0027, \u0027time_added\u0027: \u00271514493979\u0027, \u0027time_favorited\u0027: \u00270\u0027, \u0027time_read\u0027: \u00270\u0027, \u0027time_updated\u0027: \u00271514493979\u0027, \u0027word_count\u0027: \u0027773\u0027}, \u00272006596547\u0027: {\u0027excerpt\u0027: \u0027I ran, a lot, this year\u2014 2017 miles over 280 hours, or just short of 12 full days. For many of you, that seems like a ridiculous amount of distance and time, but to put it in perspective, elite marathoners and ultrarunners easily run two or three times that distance in a year.\u0027, \u0027favorite\u0027: \u00270\u0027, \u0027given_title\u0027: \u0027\u0027, \u0027given_url\u0027: \u0027https://medium.com/run-ruminate/17-thoughts-on-running-life-and-startups-5d305223669b\u0027, \u0027has_image\u0027: \u00271\u0027, \u0027has_video\u0027: \u00270\u0027, \u0027is_article\u0027: \u00271\u0027, \u0027is_index\u0027: \u00270\u0027, \u0027item_id\u0027: \u00272006596547\u0027, \u0027resolved_id\u0027: \u00272006596547\u0027, \u0027resolved_title\u0027: \u002717 thoughts on running, startups, and life while running 2017 miles in\\xa02017\u0027, \u0027resolved_url\u0027: \u0027https://medium.com/run-ruminate/17-thoughts-on-running-life-and-startups-5d305223669b\u0027, \u0027sort_id\u0027: 6, \u0027status\u0027: \u00270\u0027, \u0027time_added\u0027: \u00271514479812\u0027, \u0027time_favorited\u0027: \u00270\u0027, \u0027time_read\u0027: \u00270\u0027, \u0027time_updated\u0027: \u00271514479812\u0027, \u0027word_count\u0027: \u00271561\u0027}, \u00272006920392\u0027: {\u0027excerpt\u0027: \u0027What the heck is \u201Cwork\u201D anyway? In the Information Age, the dictionary definition of the word just doesn\u2019t cut it anymore. Skill sets, jobs, and entire companies are forming daily based on new technology, market demands, and trends that didn\u2019t exist even just a few years ago.\u0027, \u0027favorite\u0027: \u00270\u0027, \u0027given_title\u0027: \u0027\u0027, \u0027given_url\u0027: \u0027https://www.hellosign.com/blog/the-future-of-work\u0027, \u0027has_image\u0027: \u00271\u0027, \u0027has_video\u0027: \u00270\u0027, \u0027is_article\u0027: \u00271\u0027, \u0027is_index\u0027: \u00270\u0027, \u0027item_id\u0027: \u00272006920392\u0027, \u0027resolved_id\u0027: \u00272006920392\u0027, \u0027resolved_title\u0027: \u00274 Pillars Of The Future of Work\u0027, \u0027resolved_url\u0027: \u0027https://www.hellosign.com/blog/the-future-of-work\u0027, \u0027sort_id\u0027: 2, \u0027status\u0027: \u00270\u0027, \u0027time_added\u0027: \u00271514493449\u0027, \u0027time_favorited\u0027: \u00270\u0027, \u0027time_read\u0027: \u00270\u0027, \u0027time_updated\u0027: \u00271514493449\u0027, \u0027word_count\u0027: \u00271805\u0027}, \u00272007117419\u0027: {\u0027excerpt\u0027: \u0027Finding a job is not an easy task. There are multiple ways to look for jobs. Multiple steps has to be completed before you are called for an interview. After completing all these the most annoying part is when your interview doesn\u2019t even last for 10 minutes.\u0027, \u0027favorite\u0027: \u00270\u0027, \u0027given_title\u0027: \u0027\u0027, \u0027given_url\u0027: \u0027https://www.jobsstudios.com/blog/interview-tips\u0027, \u0027has_image\u0027: \u00270\u0027, \u0027has_video\u0027: \u00270\u0027, \u0027is_article\u0027: \u00271\u0027, \u0027is_index\u0027: \u00270\u0027, \u0027item_id\u0027: \u00272007117419\u0027, \u0027resolved_id\u0027: \u00272007117419\u0027, \u0027resolved_title\u0027: \u00277 Tips to prepare for an Interview\u0027, \u0027resolved_url\u0027: \u0027https://www.jobsstudios.com/blog/interview-tips\u0027, \u0027sort_id\u0027: 8, \u0027status\u0027: \u00270\u0027, \u0027time_added\u0027: \u00271514477292\u0027, \u0027time_favorited\u0027: \u00270\u0027, \u0027time_read\u0027: \u00270\u0027, \u0027time_updated\u0027: \u00271514477292\u0027, \u0027word_count\u0027: \u0027468\u0027}, \u00272007245672\u0027: {\u0027excerpt\u0027: \u0027RESTful API Versioning, though a simple and elegant concept, is a LOT harder to enforce than it sounds. It\u2019s hard to not break backward compatibility on a continually evolving API, and though API versioning is a great concept, it\u2019s rarely followed without flaw.\u0027, \u0027favorite\u0027: \u00270\u0027, \u0027given_title\u0027: \u0027\u0027, \u0027given_url\u0027: \u0027https://shift.infinite.red/snapshot-testing-api-calls-the-right-way-58ef59b7f71b\u0027, \u0027has_image\u0027: \u00271\u0027, \u0027has_video\u0027: \u00271\u0027, \u0027is_article\u0027: \u00271\u0027, \u0027is_index\u0027: \u00270\u0027, \u0027item_id\u0027: \u00272007245672\u0027, \u0027resolved_id\u0027: \u00272007245672\u0027, \u0027resolved_title\u0027: \u0027Snapshot Testing API Calls The Right\\xa0Way\u0027, \u0027resolved_url\u0027: \u0027https://shift.infinite.red/snapshot-testing-api-calls-the-right-way-58ef59b7f71b\u0027, \u0027sort_id\u0027: 3, \u0027status\u0027: \u00270\u0027, \u0027time_added\u0027: \u00271514493189\u0027, \u0027time_favorited\u0027: \u00270\u0027, \u0027time_read\u0027: \u00270\u0027, \u0027time_updated\u0027: \u00271514493189\u0027, \u0027word_count\u0027: \u00271230\u0027}, \u0027234097661\u0027: {\u0027excerpt\u0027: \u0027Sanchez examines the question, concluding that intellectuals support government intervention because it makes their work have greater importance.\u0027, \u0027favorite\u0027: \u00270\u0027, \u0027given_title\u0027: \u0027\u0027, \u0027given_url\u0027: \u0027https://www.libertarianism.org/blog/why-do-intellecuals-support-government-solutions\u0027, \u0027has_image\u0027: \u00270\u0027, \u0027has_video\u0027: \u00270\u0027, \u0027is_article\u0027: \u00271\u0027, \u0027is_index\u0027: \u00270\u0027, \u0027item_id\u0027: \u0027234097661\u0027, \u0027resolved_id\u0027: \u0027234097661\u0027, \u0027resolved_title\u0027: \u0027Why Do Intellectuals Support Government Solutions?\u0027, \u0027resolved_url\u0027: \u0027https://www.libertarianism.org/blog/why-do-intellecuals-support-government-solutions\u0027, \u0027sort_id\u0027: 9, \u0027status\u0027: \u00270\u0027, \u0027time_added\u0027: \u00271514477287\u0027, \u0027time_favorited\u0027: \u00270\u0027, \u0027time_read\u0027: \u00270\u0027, \u0027time_updated\u0027: \u00271514477287\u0027, \u0027word_count\u0027: \u00271423\u0027}}, \u0027search_meta\u0027: {\u0027search_type\u0027: \u0027normal\u0027}, \u0027since\u0027: 1514514495, \u0027status\u0027: 1} \u0060\u0060\u0060 Hope this helps! ###### Sources [Pocket API](https://getpocket.com/developer/docs/authentication) [Requests](http://docs.python-requests.org/en/master/)","contentType":"posts","url":"https://www.lqdev.me/posts/authorization-code-flow-python","date":"2017-12-29 18:45:19 -05:00","tags":["python","api","programming","security","authentication"],"summary":"For the longest time, authentication had been my worst enemy. I took small steps towards becoming comfortable with it. At first I learned Implicit Authorization with AngularJS, then I learned Client...","keywords":["resolved","title","given","request","token","access","word","count","status","article"]},{"id":"posts-client-credentials-authentication-csharp","title":"Client Credentials Authorization in C#","content":"# Getting Started For this writeup, I\u0027m going to use the Spotify API. Spotify API supports different authorization flows. In this writeup, I will be using the client credentials authorization flow. Generally this works for server-to-server authentication. Because this does not allow users the ability to provide their own credentials, there is no access to endpoints that contain user data. ![](http://cdn.lqdev.tech/files/images/clientcredentialsflowdiagram.png) # Create a new .NET Core Console Application \u0060\u0060\u0060bash dotnet new console -o authtest \u0060\u0060\u0060 # Add Dependencies \u0060\u0060\u0060bash dotnet add package Newtonsoft.Json --version 10.0.3 \u0060\u0060\u0060 # Access Token Model When a request is made, it needs to be parsed. To better capture the data into a Plain Old CLR Object (POCO), a model can be created. \u0060\u0060\u0060csharp using System; namespace authtest { class AccessToken { public string access_token { get; set; } public string token_type { get; set; } public long expires_in { get; set; } } } \u0060\u0060\u0060 # Request Token In order to extract our token, an HTTP Request needs to be made to the Spotify API in order to get an access token. To do so, we can leverage the \u0060HTTPClient\u0060 functionalities. \u0060\u0060\u0060csharp private static async Task GetToken() { string clientId = \u0022YOUR CLIENT ID\u0022; string clientSecret = \u0022YOUR CLIENT SECRET\u0022; string credentials = String.Format(\u0022{0}:{1}\u0022,clientId,clientSecret); using(var client = new HttpClient()) { //Define Headers client.DefaultRequestHeaders.Accept.Clear(); client.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\u0022application/json\u0022)); client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\u0022Basic\u0022,Convert.ToBase64String(Encoding.UTF8.GetBytes(credentials))); //Prepare Request Body List \u003E requestData = new List \u003E(); requestData.Add(new KeyValuePair (\u0022grant_type\u0022,\u0022client_credentials\u0022)); FormUrlEncodedContent requestBody = new FormUrlEncodedContent(requestData); //Request Token var request = await client.PostAsync(\u0022https://accounts.spotify.com/api/token\u0022,requestBody); var response = await request.Content.ReadAsStringAsync(); return JsonConvert.DeserializeObject (response); } } \u0060\u0060\u0060 We can then use this function in our main method to request the token. \u0060\u0060\u0060csharp static void Main(string[] args) { Console.WriteLine(\u0022Spotify API\u0022); AccessToken token = GetToken().Result; Console.WriteLine(String.Format(\u0022Access Token: {0}\u0022,token.access_token)); } \u0060\u0060\u0060 This should produce the following output \u0060\u0060\u0060bash Spotify API Getting Token Access Token: \u0022YOUR ACCESS TOKEN\u0022 \u0060\u0060\u0060 The token can now be used to make requests to the Spotify API. Source Code can be found here [Source Code](https://gist.github.com/lqdev/5e82a5c856fcf0818e0b5e002deb0c28) ###### Sources [Spotify Authorization Guide](https://developer.spotify.com/web-api/authorization-guide/#client_credentials_flow)","contentType":"posts","url":"https://www.lqdev.me/posts/client-credentials-authentication-csharp","date":"2017-12-25 19:28:48 -05:00","tags":["csharp","authentication","web applications",".net","api",".net core"],"summary":"# Getting Started For this writeup, I\u0027m going to use the Spotify API. Spotify API supports different authorization flows. In this writeup, I will be using the client credentials authorization flow....","keywords":["token","spotify","client","access","string","request","credentials","console","authorization","\u0060\u0060\u0060csharp"]},{"id":"posts-read-file-from-url-dotnet-core","title":"Read A Text File From a URL in .NET Core","content":"# Create New .NET Core Project \u0060\u0060\u0060bash dotnet new console -o urlreader \u0060\u0060\u0060 # Navigate to Project Folder \u0060\u0060\u0060bash cd urlreader \u0060\u0060\u0060 # Import Dependencies \u0060\u0060\u0060csharp using System.IO; using System.Net.Http; using System.Threading.Tasks; \u0060\u0060\u0060 # Create Reader Create a function in the \u0060Program.cs\u0060 file that will return a \u0060StreamReader\u0060 from the specified URL. Because in this case it will be used inside the \u0060Main\u0060 method which is \u0060static\u0060, the new function will also have to be \u0060static\u0060. \u0060\u0060\u0060csharp static StreamReader URLStream(String fileurl){ return new StreamReader(new HttpClient().GetStreamAsync(fileurl).Result); } \u0060\u0060\u0060 Inside the function, we return a \u0060StreamReader\u0060 which is instantiated by creating an \u0060HTTPClient\u0060 and asynchronously executing a \u0060GET\u0060 request which returns a \u0060Task \u0060. In order to resolve the object, the \u0060Result\u0060 property needs to be requested. # Reading File Contents Replace the contents of the \u0060Main\u0060 method with the following \u0060\u0060\u0060csharp static void Main(string[] args) { string line; StreamReader s = URLStream(@\u0022https://algs4.cs.princeton.edu/15uf/tinyUF.txt\u0022); String myline = s.ReadLine(); //First Line while((line = s.ReadLine()) != null) //Subsequent Lines { Console.WriteLine(line); } } \u0060\u0060\u0060 In this case, the format of the first line is different from the others. However, reading the contents of that line is no different than the other lines. # Run The Program \u0060\u0060\u0060bash dotnet run #Expected Output #4 3 #3 8 #6 5 #9 4 #2 1 #8 9 #5 0 #7 2 #6 1 #1 0 #6 7 \u0060\u0060\u0060","contentType":"posts","url":"https://www.lqdev.me/posts/read-file-from-url-dotnet-core","date":"2017-12-18 21:09:15 -05:00","tags":[".net",".net core","microsoft","programming"],"summary":"# Create New .NET Core Project \u0060\u0060\u0060bash dotnet new console -o urlreader \u0060\u0060\u0060 # Navigate to Project Folder \u0060\u0060\u0060bash cd urlreader \u0060\u0060\u0060 # Import Dependencies \u0060\u0060\u0060csharp using System.IO; using...","keywords":["line","string","create","function","which","system","using","\u0060\u0060\u0060csharp","return","\u0060\u0060\u0060bash"]},{"id":"posts-build-your-house-minecraft","title":"Design and Build Your Next Home in Minecraft","content":"#### TLDR Using Minecraft and Mods, virtual representations of the real world can be built to train autonomous agents to navigate and perform tasks in this virtual environment. These virtual models can then be used by physical agents such as Boston Dynamics bots to implement the tasks and instructions learned using the virtual models. Ultimately this makes it reasonable to think of a time where we can design and have our house built in a short time span by autonomous artificial agents. # Minecraft Minecraft is so hard to describe. Is it a game or is it an educational tool? At first I didn\u2019t get it. I watched younger siblings play this game endlessly always finding ways to improve their worlds. Eventually it was bought by Microsoft and since then it has left the core experience relatively untouched. Instead of shelving it away, Microsoft has expanded across more platforms and loaded it with tons of new features such as an education edition and cross-platform play. In Minecraft I have seen individuals build houses, cities, and even rudimentary models of circuits using Redstone. Furthermore, because the project is programmable, individuals can build modifications also known as Mods to extend the default capabilities. ![](http://cdn.lqdev.tech/files/images/buildyourhouseminecraft1.png) # Dynamic Mapping A virtual world is great, but what about the real world? Can we model the real world? A few months ago I came across a project by the New York Public Library which generated topographically accurate Minecraft maps using historical map scans and Python scripts. Although the accuracy of historical maps may be questionable, we can only hope that mapping practices have advanced significantly since then and can provide more details and data. Regardless, this project demonstrates that it is possible to model the real world in a virtual environment given the appropriate data. ![](http://cdn.lqdev.tech/files/images/buildyourhouseminecraft2.png) # Project Malmo Aside from having the obvious benefit of having a relatively accurate virtual model of the real world, there is the additional benefit of having a starting point for training autonomous agents to navigate this environment. To aid in this endeavor, Project Malmo is an adequate tool. Released to the public in 2015 by Microsoft Research, Project Malmo is a platform for artificial intelligence experimentation and research built on top of Minecraft. The project is cross-platform and agents can be built in Python, Lua, C#, C\u002B\u002B or Java. While the benefits of Project Malmo aside from the pedagogical exercise may not be clearly evident, this project is extremely valuable. The cost not only in terms of price but also time to build physical autonomous bots can be high. If for some reason they were to break or malfunction during field tests it can be costly. Therefore a more effective solution might be to simulate the navigation and mobility of these physical autonomous agents in a real world scenario using virtual environments. [![Project Malmo](http://cdn.lqdev.tech/files/images/buildyourhouseminecraft3.png)](http://www.youtube.com/watch?v=KkVj_ddseO8) # Manual Labor Once trained, we just need to find or build the physical agents that will carry out the work and instructions based on training performed in the virtual environments. There have been recent demonstrations of agents that may be able of handling such tasks. One of those is Boston Dynamics. For some time, Boston Dynamics has been creating anthropomorphic and zoomorphic robots that perform amazing feats. It can only be assumed that given a model that has been trained in a real-world environment, these physical agents can perform a task like building a home. The thought of a bot building a house is not too abstract. On March of 2017 several reports suggested that a home in Russia was built in under 24 hours using 3-D printing technologies. While the house had a predefined layout and the robotic arm that built it was limited in its mobility, such capabilities could be expanded with the aid of a trained model being executed by sophisticated physical autonomous agents such as those built by Boston Dynamics. ![](http://cdn.lqdev.tech/files/images/buildyourhouseminecraft4.png) ###### Sources [4K Minecraft](https://www.engadget.com/2017/06/14/minecraft-4k-slider-xbox-one-x-e3-microsoft-mojang/) [Historical Minecraft](https://github.com/NYPL/historical-minecraft/) [Project Malmo](https://www.microsoft.com/en-us/research/project/project-malmo/) [Boston Dynamics](https://www.bostondynamics.com) [House Built in 24 Hours](http://mentalfloss.com/article/92757/3d-printed-house-built-24-hours-russia)","contentType":"posts","url":"https://www.lqdev.me/posts/build-your-house-minecraft","date":"2017-12-15 22:34:12 -05:00","tags":["microsoft","artificial intelligence","thoughts","robots"],"summary":"#### TLDR Using Minecraft and Mods, virtual representations of the real world can be built to train autonomous agents to navigate and perform tasks in this virtual environment. These virtual models...","keywords":["project","agents","virtual","built","world","minecraft","real","using","autonomous","malmo"]},{"id":"posts-dotnet-in-the-classroom","title":"The Case for .NET in the Classroom","content":"# Introduction I recently finished by M.S. Computer Science degree and throughout the course, I used a variety of programming languages to complete assignments such as Matlab (Octave), Python, R, Java, Scala, Scheme (Lisp), and C. While all powerful in their own way, in my personal time (if and when I had any), I tried to learn about .NET tools and the ecosystem as a whole mainly because I have been a long time user of Microsoft products that include but are not limited to Zune, Kin, Surface and Windows Phone. Coincidentally, around the time I started my program, Microsoft was in the process of making an aggressive push to be more open and return to its roots of making tools and services that developers love. That being said, this is not exactly something that many people are aware of and the company\u0027s image in the eyes of consumers and developers using open source technologies has only recently started to improve. With recent acquisitions of companies like Skype and Xamarin as well as products like HoloLens, the opportunities for developers to build transformative solutions for consumers and businesses across a wide range of devices is limitless. Equally exciting and advantageous is the fact that the underlying ecosystem many of these technologies rely on is .NET. Keeping all that in mind, I strongly believe that there is no better time to introduce .NET into the classroom, especially at the university level in order to equip the next generation of developers with the necessary tools to achieve more in a wide range of environments. ## Microsoft Past ### Enterprise and Licensing For the longest time, Microsoft has been synonymous with enterprise and rightfully so as most businesses run Windows and Office. What this also meant was licensing. Many of the products and services that Microsoft provided for businesses and developers were licensed products. Even though there were in some instances free versions of the products, they had their limitations. As a result, this naturally created a barrier to entry to try and experiment with their products. ### Overhead Even when trying to do the simplest thing with .NET Framework (particularly C#, ASP.NET or F#), you needed to run Visual Studio. Like the enterprise and licensing, this provided a barrier to entry for someone trying to learn the language. Conversely, in the most closely related language Java, you could\u0027ve gotten started with a text editor and command line (among other things, this might be one reason Java has been widely used in the classroom). ### Platform Specific Although most of the world ran Windows, developer tools and products like the .NET Framework targeted Windows only. While this was acceptable a few years ago, the world is much different today than it was then. The current state of computing, while not entirely a statement on Microsoft and their products is one that Microft has astutely recognized. People want their services and experiences to travel with them regardless of the platform or products they use. Along these lines is the type of devices individuals performed their computing on. A few years ago, the Windows PC was the main device for productivity. Therefore, there was little to no need to focus on mobile, let alone cross-platform capabilities. Now, individuals can perform elaborate and deeply complex workflows from the palm of their hand. Therefore, the focus has shifted from a single device category to multi-device and multi-platform one. ## Microsoft Today Microsoft has significantly transformed their business practices and refocused on what at one time made them one of the top companies in the world by making experiences and products consumers and more importantly developers love. ### Mobile-First, Cloud-First This was the strategy Satya Nadella tried to get the entire team to rally around when he became CEO of the company. At the time, Microsoft had recently purchased Nokia, yet its mobile phone efforts were non-existent aside from the three percent. Therefore, this strategy seemed ambiguous and unattainable at least from a consumer perspective. Upon further inspection, this strategy is not limited to mobile phones. Instead it refers to the ability of individuals\u0027 computing experiences, with the aid of the cloud, to go wherever they can be most productive. The hardware and to some extent, the software in terms of the platform no longer bind individuals as the services are available nearly everywhere. A shining example of this is Xamarin. Xamarin is a mobile app development platform built on the .NET ecosystem. With this purchase, Microsoft allowed developers who were native .NET developers to work in the same environment they were used to, while targeting a new family of devices running Android or iOS. Aside from learning the some of the inner workings the Xamarin framework, at the core, individuals can leverage the .NET tools they\u0027re used to using. ### Emerging Technologies #### Artificial Intelligence Artificial intelligence, data science and machine learning have been at the forefront of discussions involving technology both in the enterprise as well as academia. Microsoft has made strides on many fronts to position themselves and provide the means for individuals to use the Microsoft set of tools to create solutions. One of the ways Microsoft has worked on providing developers and individuals with tools for deep learning is the Microsoft Cognitive Toolkit (CNTK). This framework is very similar to Tensorflow. Although models can be built using BrainScript or Python, the models themselves can be easily plugged into .NET solutions seamlessly. Another way in which Microsoft is providing tools for artificial intelligence is through its Bot Framework. The Bot Framework has different parts. One of them is the creation of chat bots using either NodeJS or C#. These bots can target different chat applications that include but are not limited to Skype, Facebook Messenger, Slack and many others. These bots can be made even more intelligent with Microsoft\u0027s Language Understanding Service (LUIS) which takes care of the natural language processing the bot might need to process requests adequately. The second part of the Bot Framework is Cortana Skills. Using LUIS and the Bot Framework, individuals can create speech and text-enabled solutions for individuals that interact with Cortana enabled devices which include but are not limited to smart speakers, PCs and mobile devices. This new kind of experiences again is entirely backed by the .NET ecosystem many developers already use today. #### Virtual and Mixed Reality While still in its infancy, virtual and mixed reality have received tons of support from technology companies like Facebook with Oculus, Google with Google VR, Apple with AR Kit. Likewise, Microsoft\u0027s solution is HoloLens. Applications for HoloLens can be developed with .NET tools and Unity. Yet again, developers can leverage their .NET skills to build applications for yet another family of devices to build new immersive experiences that not too long ago were closer to science-fiction than reality. ### .NET Core and .NET Standard A lot of computing functions overall are fairly standard. For example, reading from the console is a relatively universal procedure regardless of the platform. Microsoft has found a way to take advantage of the power of .NET for these scenarios and released .NET Standard and .NET Core which are cross-platform environments to develop C#, ASP.NET and F# solutions. These applications can run on the web as well as any OS platform further expanding the capabilities of the ecosystem as well as extending the number of solutions developers can create. Furthermore, Visual Studio, while helpful is not a requirement to start creating applications. Most things if not all can be done via the command line and a text editor. Therefore, the barrier to entry both in terms of cost and start time has been greatly reduced and individuals get started building applications in little to no time for free. ## Conclusion While this is not the entire gamut of tools and services Microsoft provides, it is a good demonstration of how learning a single set of skills like C# and F# in conjunction with the standard curriculum can greatly empower the next generation of computer scientists and developers by expanding the number of devices and platforms they can target across a wide variety of industries and disciplines making the number of applications nearly limitless.","contentType":"posts","url":"https://www.lqdev.me/posts/dotnet-in-the-classroom","date":"2017-12-14 16:57:39 -05:00","tags":[".net","microsoft","thoughts","programming","development"],"summary":"# Introduction I recently finished by M.S. Computer Science degree and throughout the course, I used a variety of programming languages to complete assignments such as Matlab (Octave), Python, R,...","keywords":["microsoft","developers","products","individuals","tools","platform","framework","while","from","applications"]},{"id":"posts-working-with-urls-racket","title":"Working with URLs in Racket","content":"Often times when working with web applications, parameters and values are passed in the path as well as the query string of a URL. In order to do something useful, parsing of the url is required. Noted below are examples of how this can be done in an elegant manner using Racket. The first step is to import the appropriate packages \u0060\u0060\u0060lisp (require net/url) (require net/url-structs) \u0060\u0060\u0060 Assuming we are working with the url string \u0022https://hacker-news.firebaseio.com/v0/item/192327.json?print=pretty\u0022, we can convert the string to a URL struct using the \u0060string-\u003Eurl\u0060 procedure. \u0060\u0060\u0060lisp (define myurl (string-\u003Eurl \u0022https://hacker-news.firebaseio.com/v0/item/192327.json?print=pretty\u0022)) \u0060\u0060\u0060 The result is the following output \u0060\u0060\u0060lisp myurl ;;(url ;; \u0022https\u0022 ;; #f ;; \u0022hacker-news.firebaseio.com\u0022 ;; #f ;; #t ;; (list ;; (path/param \u0022v0\u0022 \u0027()) ;; (path/param \u0022item\u0022 \u0027()) ;; (path/param \u0022192327.json\u0022 \u0027())) ;; \u0027((print . \u0022pretty\u0022)) ;; #f) \u0060\u0060\u0060 The structure of the URL struct is as follows and directly maps to the values displayed above. \u0060\u0060\u0060lisp (struct url ( scheme user host port path-absolute? path query fragment) #:extra-constructor-name make-url) scheme : (or/c false/c string?) user : (or/c false/c string?) host : (or/c false/c string?) port : (or/c false/c exact-nonnegative-integer?) path-absolute? : boolean? path : (listof path/param?) query : (listof (cons/c symbol? (or/c false/c string?))) fragment : (or/c false/c string?) \u0060\u0060\u0060 To access the individual data members of the struct, the syntax \u0060 - \u0060. What is returned can be further manipulated based on its type (string, list, boolean) \u0060\u0060\u0060lisp (url-host myurl) ;;\u0022hacker-news.firebaseio.com\u0022 \u0060\u0060\u0060 ###### Source [URLS and HTTP](https://docs.racket-lang.org/net/url.html)","contentType":"posts","url":"https://www.lqdev.me/posts/working-with-urls-racket","date":"2017-12-13 16:01:57 -05:00","tags":["lisp","racket"],"summary":"Often times when working with web applications, parameters and values are passed in the path as well as the query string of a URL. In order to do something useful, parsing of the url is required....","keywords":["string","false/c","or/c","\u0060\u0060\u0060lisp","path","news","struct","firebaseio","path/param","myurl"]},{"id":"posts-reading-json-in-r","title":"Reading Local JSON Files in R","content":"The following example is a way of reading local JSON files into R scripts. This can be useful for config files, although it\u0027s still also acceptable for data files as well. # Import package to work with JSON and lists \u0060\u0060\u0060r require(rlist) require(rjson) \u0060\u0060\u0060 # Load file into script \u0060\u0060\u0060r configPath \u003C- \u0027.\u0027 fileName \u003C- \u0027config.json\u0027 jsonFile \u003C- fromJSON(file=paste(configPath,fileName,sep=\u0022/\u0022),method=\u0022C\u0022) \u0060\u0060\u0060 The data gets loaded in as a list. It may be tedious to access nested contents via indices. To aid with this, a helper function can be created to extract the individual nested data elements of the file. \u0060\u0060\u0060r #Extract values form nested list #top_level = list.select(obj$topkey,obj$subkey) extractData \u003C- function(top_level) { output \u003C- unlist(lapply(top_level,\u0027[[\u0027,1)); return(output); } \u0060\u0060\u0060 # Extract data Below is a default sample json file that includes both nested and non-nested data. \u0060\u0060\u0060json { \u0022port\u0022: 2100, \u0022connectionStrings\u0022: { \u0022default\u0022: \u0022123.567.890\u0022 } } \u0060\u0060\u0060 The data in that file can be accessed as shown below. \u0060\u0060\u0060r #Not Nested port \u003C- config$port # Should store 2100 #Nested connectionString \u003C- extractData(list.select(config$connectionStrings,default)) #Should store 123.567.890 \u0060\u0060\u0060","contentType":"posts","url":"https://www.lqdev.me/posts/reading-json-in-r","date":"2017-12-11 14:21:21 -05:00","tags":["R","programming"],"summary":"The following example is a way of reading local JSON files into R scripts. This can be useful for config files, although it\u0027s still also acceptable for data files as well. # Import package to work...","keywords":["nested","data","file","json","list","\u0060\u0060\u0060r","files","default","level","below"]},{"id":"posts-windows-10-intel-display-driver-blank-screen","title":"Windows 10 Intel Display Driver Blank Screen Fix","content":"Recently my Dell Inspiron P57G was experiencing issues when entering sleep mode. When trying to wake the PC, a blank screen was coming on. After some research, I concluded it was due to bugs with the most recent version of the display driver. These were the steps I took to remediate the issue. 1. Go to [Dell Support](http://www.dell.com/support/home/us/en/04?app=drivers\u0026c=us\u0026l=en\u0026~ck=mn) page and find the Intel HD Graphics 510 515 520 530 540 Driver. 2. Click \u0027Other Versions\u0027 3. Download the most recent driver prior to 08/13/2017. 4. Follow the installation instructions and restart the PC. - If prompted to overwrite newer drivers, select \u0027Yes\u0027 After restart, either force the PC to sleep or let it sleep on its own. The problem should\u0027ve been fixed.","contentType":"posts","url":"https://www.lqdev.me/posts/windows-10-intel-display-driver-blank-screen","date":"2017-12-09 14:27:31 -05:00","tags":["sysadmin","drivers","pc issues"],"summary":"Recently my Dell Inspiron P57G was experiencing issues when entering sleep mode. When trying to wake the PC, a blank screen was coming on. After some research, I concluded it was due to bugs with the...","keywords":["dell","driver","sleep","restart","recent","recently","page","find","intel","graphics"]},{"id":"posts-create-login-screen-react-native","title":"Creating A Login Screen With React Native","content":"This assumes that you have Node and NPM installed on your PC. If not, you can go to the official website and install them. Then, come back to this page. # Part I \u2013 Install React-Native \u0060\u0060\u0060bash npm install -g create-react-native-app \u0060\u0060\u0060 # Part II \u2013 Create and Run Project \u0060\u0060\u0060bash create-react-native-app myfirstreactnativeapp cd myfirstreactnativeapp npm start \u0060\u0060\u0060 # Part III \u2013 Create Login Screen In the \u0060App.js\u0060 file that is created, we\u2019re going to want to edit a few details. At the top of the file, add the following line of code to import the visual components we will utilize to create the page. \u0060\u0060\u0060javascript Import { TextInput, StyleSheet, Button } from \u0027react-native\u0027; \u0060\u0060\u0060 Inside of your \u0060App\u0060 class, you want to add a few things. The first would be to add a constructor for the component. This will allow us to define and keep track of the text input being provided by the user. \u0060\u0060\u0060javascript constructor(props) { super(props); this.state = { name: \u0022\u0022, email: \u0022\u0022, password: \u0022\u0022 } } \u0060\u0060\u0060 Then, we want to update our render method to show the text inputs and submit button. \u0060\u0060\u0060javascript render(){ return ( this.setState({name: text})} placeholder=\u0022Name\u0022 /\u003E this.setState({email: text})} placeholder=\u0022E-Mail\u0022 /\u003E this.setState({password:text})} secureTextEntry={true} placeholder=\u0022Password\u0022 /\u003E ) } \u0060\u0060\u0060 Using the \u0060StyleSheet\u0060 component, we\u2019ll also want to add some styling to our inputs. We can do so by adding the following lines of code outside of our App class definition. These styles are applied as properties of the \u0060TextInput\u0060 components inside the App class. \u0060\u0060\u0060javascript const styles = StyleSheet.create({ input: { width: 250, margin: 5 } }); \u0060\u0060\u0060 Finally, we need to create a submit method to handle the press event on the submit button. I have left it blank, since how you pass the e-mail and password is entirely up to you (although hopefully it is in the body of an HTTP Request as opposed to url parameters). \u0060\u0060\u0060javascript submit() { //Do Something } \u0060\u0060\u0060","contentType":"posts","url":"https://www.lqdev.me/posts/create-login-screen-react-native","date":"2017-12-09 13:15:57 -05:00","tags":["react","react native","javascript"],"summary":"This assumes that you have Node and NPM installed on your PC. If not, you can go to the official website and install them. Then, come back to this page. # Part I \u2013 Install React-Native \u0060\u0060\u0060bash npm...","keywords":["create","\u0060\u0060\u0060javascript","text","submit","want","react","native","setstate","install","placeholder="]},{"id":"posts-back-to-school-emacs-edition","title":"Back To School: Emacs Edition","content":"As the summer comes to a close and I prepare to enter the fall semester, I am getting back into school mode. When programming in the terminal, I usually used vim. When compared to Emacs, the learning curve was relatively flat and it handled file editing just fine. I\u2019ve read many articles that rave about Emacs and how powerful it can be. Therefore, a couple of months ago I took the plunge and decided to give it a try. Using dired mode to browse through the various directories in my system or project was a neat feature but for my purposes of basic file editing it did not offer anything that would warrant a switch. Furthermore, the default key bindings seemed a bit too much for me which caused me to revert back to vim. As part of the articles that I have read in praise of Emacs, one particular feature seemed to be a recurring theme \u2013 Org-Mode. Although I had heard about it, I never really understood what it was. Out of curiosity, I decided to try it out for myself and see what it had to offer. While taking notes in it seemed normal enough, it was the export functionality that truly rocked my world and made me realize how amazing this could be for note-taking. Traditionally a pen-and-paper note taker, one of the main reasons I dislike taking notes on my PC is the lack of continuity and organization. Although tools like OneNote of Evernote are extremely powerful, they were not exactly the best when it came to a semester\u2019s worth of notes and although they could be made to have all of the notes for a semester on one page, this defeated the purpose of having different pages and notebooks. Enter Org-Mode. Again, while the process of taking notes is nothing special, the export capabilities are amazing. Furthermore, by default a table of contents is created with links so you can always find what you want when you want. Here are a few examples of what I mean. ![](http://cdn.lqdev.tech/files/images/backtoschoolemacsedition1.png) As it can be seen, other than * or \u002B characters in front of titles and lists, there is nothing special in the terminal. This is how it looks like in HTML format ![](http://cdn.lqdev.tech/files/images/backtoschoolemacsedition2.png) Amazing! Not only do you get an organized set of notes, but you can see how this can easily contain a semester\u2019s worth of notes in a single HTML file with main ideas being easily accessible via links. Now, maybe you don\u2019t have an internet connection or prefer to put these notes on your tablet or phone. No problem. Just export it to a PDF. ![](http://cdn.lqdev.tech/files/images/backtoschoolemacsedition3.png) Magic! This is definitely going to take my note-taking to the next level by allowing me to have these documents synced across all my devices. Furthermore, it will keep notes organized and in one place. If you\u2019re interested in getting started there may be just a few things that you need to do. # Install Org-Mode Org Mode is built in to the latest version of Emacs. However, in the event that it is not installed you can simply try the following commands in the Emacs mini buffer. \u0060M-x package-install RET org RET\u0060 # Export to HTML To access the export menu, simply type the following in the mini buffer: \u0060C-c C-e\u0060 HTML export is included by default. To do that, we can add on to the command used to access the export menu as follows: \u0060C-c C-e h h\u0060 PDF export is a bit mode complicated. The file is first exported to LaTeX format and then converted to PDF. However, in order to do this you need to install a package on your system. Assuming you are on a Unix machine, the following command should download everything necessary to convert LaTeX files to PDF. Keep in mind, this is relatively large and should take up roughly 1-2 GB in your system. In return, it limits the number of headaches due to installing necessary packages piecemeal that have multiple dependencies. To download the package, type in the following into your terminal: # Install texlive-full In the terminal, enter: \u0060\u0060\u0060bash sudo apt-get install texlive-full \u0060\u0060\u0060 Once installed, you can use a similar command to that of HTML export. # Export to PDF \u0060C-c C-e l p\u0060 This should be all you need to get started. For more information on how to use Org Mode, here\u2019s a guide with live examples. Happy note-taking! [Emacs Examples and Cookbook](http://home.fnal.gov/~neilsen/notebook/orgExamples/org-examples.html)","contentType":"posts","url":"https://www.lqdev.me/posts/back-to-school-emacs-edition","date":"2017-12-09 13:01:19 -05:00","tags":["emacs","org-mode"],"summary":"As the summer comes to a close and I prepare to enter the fall semester, I am getting back into school mode. When programming in the terminal, I usually used vim. When compared to Emacs, the learning...","keywords":["export","notes","mode","html","taking","emacs","install","file","note","following"]},{"id":"posts-hp-probook-430g1-ubuntu-bios-upgrade","title":"HP ProBook 430 G1 Ubuntu BIOS Upgrade","content":"I recently got an HP ProBook 430 G1 which came preinstalled with Windows 10. I have several Windows 10 devices, but needed a Linux system while on the go. I had given my HP Chromebook a try using Crouton. This worked okay. However, I was limited by the specs of the Chromebook and wanted something with more power for development purposes. Therefore, I erased Windows 10 on the ProBook and installed Ubuntu. The PC worked well. My only problem with it was that I found my battery meter was inaccurate and would reach a percentage in the 60\u2019s before suddenly dropping to zero and shutting down. This was not something I could have, especially during the middle of a work session where I might lose some/all of my work. Immediately, I assumed it had something to do with drivers/BIOS being out of date. To my surprise, BIOS updates were only available for Windows. Fortunately, through some research I was able to find a solution that allowed me to update my BIOS and fix my battery problems. ## Prerequisites: 1. Make sure PC is connected to the charger 2. 4GB USB Drive ## Procedure: 1. Format USB drive. (FAT32 required) 2. Create folder inside USB drive called Hewlett-Packard\\BIOS\\New 3. Download latest BIOS .exe file from HP Support website 4. Unzip .exe file. While 7Zip is not required, I found this to work well. 5. Copy .bin file to Hewlett-Packard\\BIOS\\New folder 6. Copy entire BIOSUpdate folder to Hewlett-Packard folder. 7. Restart PC 8. Continuously press F10 to enter BIOS Setup 9. Select Update System BIOS menu option 10. Select Update BIOS Using Local Media option 11. Select Accept At this point, the installation process will take effect and the PC will restart itself at which point the your BIOS should be up to date.","contentType":"posts","url":"https://www.lqdev.me/posts/hp-probook-430g1-ubuntu-bios-upgrade","date":"2017-12-09 12:57:12 -05:00","tags":["sysadmin","ubuntu","bios"],"summary":"I recently got an HP ProBook 430 G1 which came preinstalled with Windows 10. I have several Windows 10 devices, but needed a Linux system while on the go. I had given my HP Chromebook a try using...","keywords":["bios","folder","windows","file","update","drive","hewlett","select","packard\\bios\\new","restart"]},{"id":"posts-setting-up-new-ubuntu-pc","title":"Setting Up A New Ubuntu PC","content":"# Users This assumes that you are currently the root user. Enter the adduser to command to create a new user. \u0060\u0060\u0060bash adduser \u0060\u0060\u0060 Immediately, you will be prompted for a password. Enter the password and continue the process. ## Adding Root Privileges This is only applicable when granting root priviliges to the newly created user. Enter the usermod command to add the new user to the sudo (root) group \u0060\u0060\u0060bash usermod -aG sudo \u0060\u0060\u0060 The user should now have root priviliges. To test that it worked log in as that user and try to run a command with sudo \u0060\u0060\u0060bash su - sudo ls -la /root \u0060\u0060\u0060 # Emacs By default, Ubuntu usually comes preloaded or has the default 24.x version of Emacs. I often like to use Emacs along with org-mode and to take full advantage, especially as it pertains to working with source code I have found version 25.x is much better suited for the task. While there are many ways to go about it, installing it via package manager appears to be the easiest. ## Add repository to PPA \u0060\u0060\u0060bash sudo add-apt-repository ppa:kelleyk/emacs \u0060\u0060\u0060 ## Update the packages \u0060\u0060\u0060bash sudo apt update \u0060\u0060\u0060 ## Install Emacs. ### Text-Only Interface \u0060\u0060\u0060bash sudo apt-get install emacs25-nox \u0060\u0060\u0060 ### GUI Interface \u0060\u0060\u0060bash sudo apt-get install emacs25 \u0060\u0060\u0060 ## Backup files Sometimes Emacs keeps a backup file by default. This can cause a lot of clutter in the current directory where work is being performed. There is a way to disable this. Navigate to \u0060~/.emacs.d/\u0060 and edit the \u0060init.el\u0060 file and add the following line. \u0060\u0060\u0060lisp (setq make-backup-files nil) \u0060\u0060\u0060 # Java Make sure that your packages and repositories are up to date \u0060\u0060\u0060bash sudo apt-get update \u0060\u0060\u0060 ## OpenJDK \u0060\u0060\u0060bash sudo apt-get install default-jdk \u0060\u0060\u0060 ## Oracle JDK ### Add PPA Repository \u0060\u0060\u0060bash sudo add-apt-repository ppa:webupd8team/java sudo apt-get update \u0060\u0060\u0060 ### Install the installer \u0060\u0060\u0060bash sudo apt-get install oracle-java8-installer \u0060\u0060\u0060 #### Sources [Users](https://www.digitalocean.com/community/tutorials/how-to-create-a-sudo-user-on-ubuntu-quickstart) [Emacs](http://ubuntuhandbook.org/index.php/2017/04/install-emacs-25-ppa-ubuntu-16-04-14-04/) [Java](https://www.digitalocean.com/community/tutorials/how-to-install-java-with-apt-get-on-ubuntu-16-04)","contentType":"posts","url":"https://www.lqdev.me/posts/setting-up-new-ubuntu-pc","date":"2017-12-09 12:49:19 -05:00","tags":["linux","ubuntu","java","sysadmin","emacs"],"summary":"# Users This assumes that you are currently the root user. Enter the adduser to command to create a new user. \u0060\u0060\u0060bash adduser \u0060\u0060\u0060 Immediately, you will be prompted for a password. Enter the password...","keywords":["sudo","\u0060\u0060\u0060bash","emacs","install","user","root","update","ubuntu","default","repository"]},{"id":"posts-octave-development-in-the-terminal","title":"Octave Development in the Terminal","content":"MatLab is a powerful tool that is used througout various disciplines for data processing and analysis. However, in order to get access to it and all its features, there is a fee associated with it. Depending on the needs of the user, this price can at times be somewhat large. Luckily, an open source alternative known as Octave exists which works just as good in many cases at zero monetary cost. Octave\u0027s syntax is for the most part identical to MatLab making the transition a relatively simple one. # Tools - Octave - Tmux - Text Editor (Emacs) # Installation ## Tmux \u0060\u0060\u0060bash sudo apt-get install tmux \u0060\u0060\u0060 ## Octave \u0060\u0060\u0060 sudo apt-add-repository ppa:octave/stable sudo apt-get update sudo apt-get install octave \u0060\u0060\u0060 # Setup ## Create a new tmux session \u0060\u0060\u0060bash tmux new -s octave-ide \u0060\u0060\u0060 ## Configure Layout \u0060\u0060\u0060ascii ========================= | | | | Console | | | | | |-----------| Script | | | | | Terminal | | | | | ========================= \u0060\u0060\u0060 In order to get the layout shown above, the following steps need to be taken ### Split panes horizontally \u0060Ctrl\u002Bb %\u0060 ### Split panes vertically #### Switch to pane 0 \u0060Ctrl\u002Bb q 0\u0060 #### Do vertical split \u0060Ctrl\u002Bb \u0022\u0060 # Workflow Now that the layout is setup, work can finally be done. To start off, initialize Octave in pane 0. ## Test code snippets in the console Enter the following snippet in the console window \u0060\u0060\u0060octave function res = square(x) res = x * x endfunction \u0060\u0060\u0060 Then call it to make sure it does what is expected \u0060\u0060\u0060octave square(2) #Should equal 4 \u0060\u0060\u0060 ## Write code on script file Once satisfied that the code works: 1. Switch to pane 2 using \u0060Ctrl\u002Bb q 2\u0060 2. Create a script file \u0060myscript.m\u0060 3. Edit \u0060myscript.m\u0060 with desired code \u0060\u0060\u0060octave 1; # Prevents octave from processing file as function file. function res = square(x) res = x * x endfunction square(2); \u0060\u0060\u0060 4. Save the script ## Run script file Switch to pane 1 \u0060Ctrl\u002Bb q 1\u0060 and try to run the newly created script \u0060\u0060\u0060bash octave -W myscript.m \u0060\u0060\u0060 The output should look the same as that in the console window. ###### Sources [Octave](http://wiki.octave.org/Octave_for_Debian_systems)","contentType":"posts","url":"https://www.lqdev.me/posts/octave-development-in-the-terminal","date":"2017-12-09 12:46:54 -05:00","tags":["octave","tmux","developer tools"],"summary":"MatLab is a powerful tool that is used througout various disciplines for data processing and analysis. However, in order to get access to it and all its features, there is a fee associated with it....","keywords":["octave","script","tmux","\u0060ctrl\u002Bb","file","square","sudo","pane","console","code"]},{"id":"posts-hello-world","title":"Hello World","content":"Welcome to my personal blog. This will be a blog post for documenting my learning process as well as sharing projects and ideas I come across. I often find myself doing things, researching how to do them and completing a task. A few weeks/months later I might have to do something similar and have to go through the process again. Using this blog will help me keep me reference that knowledge.","contentType":"posts","url":"https://www.lqdev.me/posts/hello-world","date":"2017-12-09 12:00:00 -05:00","tags":[],"summary":"Welcome to my personal blog. This will be a blog post for documenting my learning process as well as sharing projects and ideas I come across. I often find myself doing things, researching how to do...","keywords":["blog","process","welcome","keep","help","using","again","similar","later","weeks/months"]},{"id":"posts-how-to-listen-internet-radio-using-vlc","title":"How to listen to internet radio using VLC","content":"## Introduction Music streaming services like Spotify generally do a good job of making recommendations based on your music tastes. By applying algorithms to your listening history, they\u0027re able to provide you with a listening experience tailored to you. When it comes to music discovery though, it can be difficult to find new music so you often end up listening to music that sounds very similar. Internet radio can help expose you to music you otherwise wouldn\u0027t normally listen to. Radio stations like those on [SomaFM](https://somafm.com/) provide a wide variety of high quality music channels. There\u0027s other online radio stations out there, many which provide a built-in audio player. As I\u0027ve written in previous posts though ([How to watch Twitch streams using VLC](/posts/how-to-watch-twitch-using-vlc/) \u0026 [Podcast management with RSS \u0026 VLC](/notes/rss-vlc-podcast-management)), VLC is a versatile tool. As a result, it\u0027s no surprise you can also use it to listen to online radio streams. This post is mostly a note to myself, but I encourage you to try it as well. ## Prerequisites - [VLC](https://www.videolan.org/index.html) The commands below were run on Linux. Since VLC is cross-platform, the same commands should work on those platforms as well. ## Listen to internet radio The first thing you\u0027ll need is the URL of the stream you want to listen to. Usually it\u0027s provided on the station\u0027s website. You can also browse through [RadioBrowser](https://www.radio-browser.info/) to find a station and its stream URL. Once you have the URL, in the terminal, enter the following command: \u0060\u0060\u0060bash vlc \u0060\u0060\u0060 Replace \u0060 \u0060 with the address of your stream. After a few seconds, VLC launches and your stream should start playing. If you prefer to stream audio without VLC\u0027s visual interface, use the following command. \u0060\u0060\u0060bash cvlc \u0060\u0060\u0060 Note that without the UI you won\u0027t have access to any playback controls. Happy listening!","contentType":"posts","url":"https://www.lqdev.me/posts/how-to-listen-internet-radio-using-vlc","date":"12/28/2021 16:24 -05:00","tags":["vlc","open source","radio"],"summary":"## Introduction Music streaming services like Spotify generally do a good job of making recommendations based on your music tastes. By applying algorithms to your listening history, they\u0027re able to...","keywords":["music","radio","stream","listen","listening","provide","online","station","find","audio"]},{"id":"reviews-thrill-switch","title":"Thrill Switch","content":"Thrill Switch by Tim Hawking Read Rating: 4.2/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/thrill-switch","date":"12/27/2023 14:08 -05:00","tags":[],"summary":"Thrill Switch by Tim Hawking Read Rating: 4.2/5","keywords":["thrill","switch","hawking","read","rating:"]},{"id":"snippets-lqdev-me-website-post-metrics","title":"lqdev.me Post Metrics","content":"Description Generates an aggregate analysis of posts on lqdev.me / luisquintanilla.me . Usage dotnet fsi stats.fsx Snippet stats.fsx // Reference DLL #r \u0026quot;../bin/Debug/net8.0/PersonalSite.dll\u0026quot; // Add modules open Domain open Builder open System // Load posts let posts = loadPosts() let notes = loadFeed () let responses = loadReponses () // Organize posts by year let postCountsByYear = posts |\u0026gt; Array.countBy (fun (x:Post) -\u0026gt; DateTime.Parse(x.Metadata.Date) |\u0026gt; _.Year) |\u0026gt; Array.sortByDescending fst let noteCountsByYear = notes |\u0026gt; Array.countBy (fun (x:Post) -\u0026gt; DateTime.Parse(x.Metadata.Date) |\u0026gt; _.Year) |\u0026gt; Array.sortByDescending fst let responseCountsByYear = responses |\u0026gt; Array.countBy (fun (x:Response) -\u0026gt; DateTime.Parse(x.Metadata.DatePublished) |\u0026gt; _.Year) |\u0026gt; Array.sortByDescending fst // Organize responses by type let responsesByType = responses |\u0026gt; Array.filter(fun x -\u0026gt; (DateTime.Parse(x.Metadata.DatePublished) |\u0026gt; _.Year) = DateTime.UtcNow.Year) |\u0026gt; Array.countBy(fun x -\u0026gt; x.Metadata.ResponseType) |\u0026gt; Array.sortByDescending(snd) // Organize responses by tag let responsesByTag = responses |\u0026gt; Array.filter(fun x -\u0026gt; (DateTime.Parse(x.Metadata.DatePublished) |\u0026gt; _.Year) = DateTime.UtcNow.Year) |\u0026gt; Array.collect(fun x -\u0026gt; match x.Metadata.Tags with | null -\u0026gt; [|\u0026quot;untagged\u0026quot;|] | [||] -\u0026gt; [|\u0026quot;untagged\u0026quot;|] | _ -\u0026gt; x.Metadata.Tags ) |\u0026gt; Array.countBy(fun x -\u0026gt; x) |\u0026gt; Array.sortByDescending(snd) // Organize responses by host name (domain) let responsesByDomain = responses |\u0026gt; Array.filter(fun x -\u0026gt; (DateTime.Parse(x.Metadata.DatePublished) |\u0026gt; _.Year) = DateTime.UtcNow.Year) |\u0026gt; Array.countBy(fun x -\u0026gt; Uri(x.Metadata.TargetUrl).Host) |\u0026gt; Array.sortByDescending(snd) // Utility function to display counts let printEntryCounts\u0026lt;\u0027a\u0026gt; (title:string) (entryCounts:(\u0027a * int) array) (n:int) = printfn $\u0026quot;{title}\u0026quot; match entryCounts.Length with | 0 -\u0026gt; printfn $\u0026quot;No entries\u0026quot; printfn $\u0026quot;\u0026quot; | a when a \u0026gt; 0 -\u0026gt; match n with | n when n = -1 || n \u0026gt; entryCounts.Length -\u0026gt; entryCounts |\u0026gt; Array.iter(fun x -\u0026gt; printfn $\u0026quot;{fst x} {snd x}\u0026quot;) |\u0026gt; fun _ -\u0026gt; printfn $\u0026quot;\u0026quot; | n when n \u0026gt; 0 -\u0026gt; entryCounts |\u0026gt; Array.take n |\u0026gt; Array.iter(fun x -\u0026gt; printfn $\u0026quot;{fst x} {snd x}\u0026quot;) |\u0026gt; fun _ -\u0026gt; printfn $\u0026quot;\u0026quot; // Print yearly counts printEntryCounts \u0026quot;Blogs\u0026quot; postCountsByYear 2 printEntryCounts \u0026quot;Notes\u0026quot; noteCountsByYear 2 printEntryCounts \u0026quot;Responses\u0026quot; responseCountsByYear 2 // Print response types printEntryCounts \u0026quot;Response Types\u0026quot; responsesByType -1 // Print response tag counts printEntryCounts \u0026quot;Response Tags\u0026quot; responsesByTag 5 // Print response by host name printEntryCounts \u0026quot;Domains\u0026quot; responsesByDomain 5 Expected Output Blogs 2023 5 2022 7 Notes 2023 34 2022 36 Responses 2023 216 2022 146 Response Types bookmark 151 reshare 48 reply 10 star 7 Response Tags ai 104 llm 42 untagged 41 opensource 31 internet 17 Domains github.com 15 huggingface.co 11 arxiv.org 10 openai.com 6 www.theverge.com 4","contentType":"snippets","url":"https://www.lqdev.me/resources/snippets/lqdev-me-website-post-metrics","date":"12/23/2023 13:13 -05:00","tags":["analytics","website","blog","blogging","pkm","indieweb","dotnet","windows","linux","mac"],"summary":"Description Generates an aggregate analysis of posts on lqdev.me / luisquintanilla.me . Usage dotnet fsi stats.fsx Snippet stats.fsx // Reference DLL #r...","keywords":["|\u0026gt","array","\u0026quot","year","metadata","responses","datetime","$\u0026quot","response","printfn"]},{"id":"notes-net6-website-update","title":"Site updated to .NET 6","content":"That was easy. Site\u0027s been updated to .NET 6 and all it took was a one-line change.","contentType":"notes","url":"https://www.lqdev.me/notes/net6-website-update","date":"12/22/2021 20:27 -05:00","tags":["dotnet","website","net6","upgrade","selfhost"],"summary":"That was easy. Site\u0027s been updated to .NET 6 and all it took was a one-line change.","keywords":["easy","site","updated","took","line","change"]},{"id":"wiki-nixos-dotnet-packages-source","title":"Install .NET packages on NixOS from GitHub","content":"Overview This guide shows how to build a .NET package from GitHub source. The project used to illustrate the process can be found on GitHub . Create derivation The first thing you\u0027ll want to do is create a derivation for your package. Create a file for your package. In this case, I called mine fitch.nix . Since I\u0027m building a .NET package from the GitHub source, I use buildDotnetModule , dotnetCorePackages , and fetchFromGithub . To get the rev, use the git log command. It\u0027s the hash of the latest commit. Fake SHA { fetchFromGitHub, buildDotnetModule, dotnetCorePackages }: buildDotnetModule { name = \u0026quot;fitch\u0026quot;; src = fetchFromGitHub { owner = \u0026quot;lqdev\u0026quot;; repo = \u0026quot;fitch\u0026quot;; rev = \u0026quot;e5fb91ddf57eb5611e0e313af29126e590cd149f\u0026quot;; sha256 = \u0026quot;\u0026quot;; }; projectFile = \u0026quot;src/fitch.fsproj\u0026quot;; executables = \u0026quot;fitch\u0026quot;; dotnet-sdk = dotnetCorePackages.sdk_8_0; dotnet-runtime = dotnetCorePackages.runtime_8_0; nugetDeps = ./deps.nix; } At this point the sha256 will be empty because you\u0027ll get it later in the process. Create default.nix let pkgs = import \u0026lt;nixpkgs\u0026gt; { }; in { fitch = pkgs.callPackage ./fitch.nix { }; } Fetch dependencies Create a file called deps.nix . This will contain the dependencies you need for your application. touch deps.nix Then, run nix-build -A fitch.fetch-deps . This should fail because of the empty SHA. Update SHA The empty SHA will fail. However, the error message will contain the actual SHA value. Copy that and paste it into the sha256 property. { lib fetchFromGitHub, buildDotnetModule, dotnetCorePackages }: buildDotnetModule { name = \u0026quot;fitch\u0026quot;; src = fetchFromGitHub { owner = \u0026quot;lqdev\u0026quot;; repo = \u0026quot;fitch\u0026quot;; rev = \u0026quot;e5fb91ddf57eb5611e0e313af29126e590cd149f\u0026quot;; sha256 = \u0026quot;74xddAUGQNVliVs5o3zQQEAUAoF9r8iBmiiBr4qrUgw=\u0026quot;; }; projectFile = \u0026quot;src/fitch.fsproj\u0026quot;; executables = \u0026quot;fitch\u0026quot;; dotnet-sdk = dotnetCorePackages.sdk_7_0; dotnet-runtime = dotnetCorePackages.runtime_7_0; nugetDeps = ./deps.nix; } Get dependencies Now that you have the correct SHA, try getting the dependencies again. These dependencies are created in a series of steps: Run the following command sudo nix-build -A fitch.fetch-deps . The result of this command is an executable script called result . Run the result script. sudo ./result deps.nix . This will generate the lockfile called deps.nix and is referenced by the nugetDeps property. Build package Once everything is set up, run nix-build -A fitch . This will package and install the package onto your system. Add to configuration To add the newly built package to your configuration.nix , add it to your packages: environment.systemPackages = with pkgs; [ #... (callPackage ./fitch.nix {}) #... ]; Then, run nixos-rebuild switch . This should now install the package onto your PC. (Optional) Initialize when shell starts In my case, since I want fitch to run when the terminal starts, I edit my bash configuration in the configuration.nix file. programs.bash = { interactiveShellInit = \u0026quot;fitch\u0026quot;; } Sources Dotnet Nixpkgs Joining the NixOS Pyramid Scheme Packaging existing software with Nix","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/nixos-dotnet-packages-source","date":"12/17/2023 17:56 -05:00","tags":["dotnet","nixos","linux","sysadmin",""],"summary":"Overview This guide shows how to build a .NET package from GitHub source. The project used to illustrate the process can be found on GitHub . Create derivation The first thing you\u0027ll want to do is...","keywords":["\u0026quot","package","fitch\u0026quot","dotnetcorepackages","fitch","deps","build","dotnet","fetchfromgithub","builddotnetmodule"]},{"id":"posts-sending-webmentions-fsharp-fsadvent","title":"Sending Webmentions with F#","content":"## Introduction \u003E This post is part of the [F# Advent 2021](https://sergeytihon.com/2021/10/18/f-advent-calendar-2021). Webmentions is a W3C specification that defines a standard way of implementing notifications between websites. It enables rich interactions between websites using standard protocols. According to the spec, \u0022Webmention is a simple way to notify any URL when you mention it on your site. From the receiver\u0027s perspective, it\u0027s a way to request notifications when other sites mention it\u0022. If you\u0027re interested in learning more about the spec and overall use cases, see the following links: - [Webmentions: Enabling better communication on the internet](https://boffosocko.com/2018/07/19/webmentions-enabling-better-communication-on-the-internet-2/) - [Sending your first Webmention from scratch](https://aaronparecki.com/2018/06/30/11/your-first-webmention) - [Webmentions W3C specification](https://www.w3.org/TR/webmention/) - [Webmentions implementation tests](https://webmention.rocks/) The Webmentions protocol has send and receive components. In this post, I\u0027ll go over a lightweight implementation for sending Webmentions using F# and test it using the [webmention.rocks](https://webmention.rocks/) website. Source code for this post can be found at the [fsadvent-2021-webmentions GitHub repository](https://github.com/lqdev/fsadvent-2021-webmentions). ## Sending Webmention workflow Let\u0027s say you want to use webmentions to reply to or mention content from another site. The general workflow works as follows: 1. You create a document (source) that mentions content from another website (target). 1. You perform discovery on the target website for the webmention endpoint URL. The URL might be in one of three places: 1. HTTP response headers. 2. \u0060 \u0060 tag with \u0060rel=webmention\u0060 attribute. 3. \u0060 \u0060 tag with \u0060rel=webmention\u0060 attribute. The discovery is performed in order and each subsequent option works as a fallback of the other. 1. You make an HTTP POST request notifying the source that you\u0027ve mentioned their article on your site. The request body is in \u0060x-www-form-urlencoded\u0060 form containing the \u0060source\u0060 and \u0060target\u0060 parameters where source is the URL of your article and the target is the URL of article being mentioned. For more details on this workflow, see the [Sending Webmentions section](https://www.w3.org/TR/webmention/#sending-webmentions) of the specification. ## Create a document The document can be any publicly hosted HTML document as long as you have a source URL to provide during your notification request. For example, you can have something like the following: \u0060\u0060\u0060html F# Advent 2021 - Webmentions F# Advent 2021 - This is a great post \u0060\u0060\u0060 Using [microformats](http://microformats.org/wiki/h-entry) though, you can annotate your HTML to provide more context to the receiver. The same document with microformats providing the author, content, and source document being replied to might look like the following with microformats: \u0060\u0060\u0060html F# Advent 2021 - Webmentions lqdev In reply to: @webmention.rocks F# Advent 2021 - This is a great post \u0060\u0060\u0060 Receiving Webmentions is beyond the scope of this post, so I\u0027ll skip over them. Once you have a document and make it publicly accessible to the web, it\u0027s time to find where to send notifications to. ## Discover Webmention endpoint URL To discover where to send the Webmention to, send an HTTP GET or HTTP HEAD request to the target URL. Once you get the response, the Webmention endpoint URL should be in one of three places: - HTTP headers \u0060\u0060\u0060text GET /test/1 HTTP/1.1 Host: webmention.rocks HTTP/1.1 200 OK Link: ; rel=\u0022webmention\u0022 \u0060\u0060\u0060 - \u0060 \u0060 tag \u0060\u0060\u0060html ... ... \u0060\u0060\u0060 - \u0060 \u0060 tag \u0060\u0060\u0060html .... webmention ... \u0060\u0060\u0060 The following examples show how you\u0027d implement the discovery process for each of these scenarios in F#. ### Discover Webmention URL in header The \u0060discoverUrlInHeaderAsync\u0060 function uses the .NET HttpClient to make an HTTP HEAD request to get the headers from the target URL provided. Then, a search is performed for a \u0022link\u0022 header containing the text \u0022webmention\u0022. Once the header is found, it\u0027s sanitized to extract the Webmention endpoint URL of the target site. \u0060\u0060\u0060fsharp let discoverUrlInHeaderAsync (url:string) = async { // Initialize HttpClient use client = new HttpClient() // Prepare request message let reqMessage = new HttpRequestMessage(new HttpMethod(\u0022HEAD\u0022), url) // Send request let! response = client.SendAsync(reqMessage) |\u003E Async.AwaitTask // Get request headers let responseHeaders = [ for header in response.Headers do header.Key.ToLower(), header.Value ] // Look for webmention header try // Find \u0022link\u0022 header that contains \u0022webmention\u0022 let webmentionHeader = responseHeaders |\u003E Seq.filter(fun (k,_) -\u003E k = \u0022link\u0022) |\u003E Seq.map(fun (_,v) -\u003E v |\u003E Seq.filter(fun header -\u003E header.Contains(\u0022webmention\u0022))) |\u003E Seq.head |\u003E List.ofSeq |\u003E List.head // Get first part of \u0022link\u0022 header let webmentionUrl = webmentionHeader.Split(\u0027;\u0027) |\u003E Array.head // Remove angle brackets from URL let sanitizedWebmentionUrl = webmentionUrl .Replace(\u0022 \u0022,\u0022\u0022) .Trim() return Some sanitizedWebmentionUrl with | _ -\u003E return None } \u0060\u0060\u0060 ### Discover Webmention URL endpoint in link tag The next place to check for a Webmention endpoint URL is a \u0060 \u0060 tag in the document. In this example, the \u0060discoverUrlInLinkTagAsync\u0060 function uses \u0060FSharp.Data\u0060 library to get the contents of the target URL and parse the HTML contents to find a link tag with a \u0060rel=webmention\u0060 attribute value. \u0060\u0060\u0060fsharp let discoverUrlInLinkTagAsync (url:string) = async { // Load HTML document let! htmlDoc = HtmlDocument.AsyncLoad(url) // Get webmention URL from \u0022 \u0022 tag try let webmentionUrl = htmlDoc.CssSelect(\u0022link[rel=\u0027webmention\u0027]\u0022) |\u003E List.map(fun link -\u003E link.AttributeValue(\u0022href\u0022)) |\u003E List.head return Some webmentionUrl with | _ -\u003E return None } \u0060\u0060\u0060 ### Discover Webmention endpoint URL in anchor tag The last place to check for the Webmention endpoint URL is in an anchor tag in the contents of the target URL. In this example, the \u0060discoverUrlInAnchorTagAsync\u0060 uses the \u0060FSharp.Data\u0060 library to get the contents of the target URL and parse the HTML contents to find an anchor tag with a \u0060rel=webmention\u0060 attribute value. \u0060\u0060\u0060fsharp let discoverUrlInAnchorTagAsync (url:string) = async { // Load HTML document let! htmlDoc = HtmlDocument.AsyncLoad(url) // Get webmention URL from \u0022 \u0022 tag try let webmentionUrl = htmlDoc.CssSelect(\u0022a[rel=\u0027webmention\u0027\u0022) |\u003E List.map(fun a -\u003E a.AttributeValue(\u0022href\u0022)) |\u003E List.head return Some webmentionUrl with | _ -\u003E return None } \u0060\u0060\u0060 Once the utility functions are in place to handle the different scenarios, create a new function to perform the discovery workflow. \u0060\u0060\u0060fsharp // Apply webmention URL discovery workflow // 1. Check header // 2. Check link tag // 3. Check anchor tag let discoverWebmentionUrlAsync (url:string) = async { let! headerUrl = discoverUrlInHeaderAsync url let! linkUrl = discoverUrlInLinkTagAsync url let! anchorUrl = discoverUrlInAnchorTagAsync url // Aggregate results let discoveryResults = [headerUrl; linkUrl; anchorUrl] // Unwrap and take the first entry containing a value let webmentionUrl = discoveryResults |\u003E List.choose(fun url -\u003E url) |\u003E List.head return webmentionUrl } \u0060\u0060\u0060 The \u0060discoverWebmentionUrlAsync\u0060 calls all of the discovery utility methods and chooses only the results that successfully extracted a Webmention endpoint URL. In the event of multiple endpoints, the first one is takes precedence. ## Send Webmention Now that you have a publicly accessible document and the Webmention URL endpoint to send your Webmention to, it\u0027s time to send it. \u0060\u0060\u0060fsharp let sendWebMentionAsync (url:string) (req:IDictionary ) = async { use client = new HttpClient() let content = new FormUrlEncodedContent(req) let! response = client.PostAsync(url, content) |\u003E Async.AwaitTask return response.IsSuccessStatusCode } \u0060\u0060\u0060 The \u0060sendWebMentionAsync\u0060 function uses the .NET HttpClient to send an HTTP POST request to the Webmention endpoint URL you just extracted. Define your source and target URLs \u0060\u0060\u0060fsharp let sourceUrl = new Uri(\u0022https://raw.githubusercontent.com/lqdev/fsadvent-2021-webmentions/main/reply.html\u0022) let targetUrl = new Uri(\u0022https://webmention.rocks/test/1\u0022) \u0060\u0060\u0060 The source URL I used in this example is hosted on [GitHub](https://raw.githubusercontent.com/lqdev/fsadvent-2021-webmentions/main/reply.html) and the target URL is the first implementation test in the [webmention.rocks](https://webmention.rocks/test/1) website. Then, create a function to run the entire workflow end-to-end. \u0060\u0060\u0060fsharp let runWebmentionWorkflow () = async { // Discover webmention endpoint URL of target URL let! discoveredUrl = targetUrl.OriginalString |\u003E discoverWebmentionUrlAsync // Construct URL depending on whether it\u0027s absolute or relative let authority = targetUrl.GetLeftPart(UriPartial.Authority) let constructedUrl = match (discoveredUrl.Contains(\u0022http\u0022)) with | true -\u003E discoveredUrl | false -\u003E let noQueryUrl = discoveredUrl.Split(\u0027?\u0027) |\u003E Array.head $\u0022{authority}{noQueryUrl}\u0022 // Prepare webmention request data let reqData = dict [ (\u0022source\u0022, sourceUrl.OriginalString) (\u0022target\u0022, targetUrl.OriginalString) ] // Send web mentions return! sendWebMentionAsync constructedUrl reqData } \u0060\u0060\u0060 Now that you have everything set up, you\u0027re ready to send your webmention! \u0060\u0060\u0060fsharp runWebmentionWorkflow () |\u003E Async.RunSynchronously \u0060\u0060\u0060 If your post is successful, you should see it on the target URL. ![Display of Webmention post on webmention.rocks](https://user-images.githubusercontent.com/11130940/145919398-2576bfef-3a5d-44f4-9813-9cf933539ede.png) ## Conclusion In this post, I showed how you can use F# to implement sending webmentions using F#. This is not a full implementation as there are still scenarios I need to account for like receiving webmentions. There are community maintained [libraries](https://indieweb.org/Webmention-developer#Libraries), [plugins](https://indieweb.org/Webmention#Publishing_Software), and [services](https://indieweb.org/Webmention#Services) to simplify the process, but because the specification is built on open protocols and standards, it\u0027s possible to build your own implementation in F# and integrate it into your website. Happy coding!","contentType":"posts","url":"https://www.lqdev.me/posts/sending-webmentions-fsharp-fsadvent","date":"12/13/2021 20:53 -05:00","tags":[],"summary":"## Introduction \u003E This post is part of the [F# Advent 2021](https://sergeytihon.com/2021/10/18/f-advent-calendar-2021). Webmentions is a W3C specification that defines a standard way of implementing...","keywords":["webmention","webmentions","target","header","endpoint","request","document","send","post","2021"]},{"id":"wiki-configure-opacity-manjaro","title":"Configure opacity - Manjaro","content":"Overview This article shows how to configure background opacity for the kitty terminal emulator using picom and i3. By defaut, i3 doesn\u0027t support composting. Programs like picom add support for it. In addition, terminal emulators like kitty are hardware accelerated which means there\u0027s opportunities to offload from the CPU and save power. Install \u0026amp; configure picom Install picom sudo pacman -S picom Move default configuration file to ~/.config/picom/picom.conf Define opacity rule: opacity-rule = [\u0026quot;90:class_g = \u0027kitty\u0027\u0026quot;]; Save changes Install \u0026amp; configure kitty Install picom sudo pacman -S kitty Move default configuration file to ~/.config/kitty/kitty.conf Set background opacity: background_opacity 0.90 Save changes Configure i3 Open your i3 configuration and add the following line to it: exec_always picom Restart i3. References Picom Kitty Kitty emulator not working - Reddit","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/configure-opacity-manjaro","date":"12/11/2022 18:56 -05:00","tags":["i3","wm","linux","manjaro","kitty","terminal","wiki","configuration","picom"],"summary":"Overview This article shows how to configure background opacity for the kitty terminal emulator using picom and i3. By defaut, i3 doesn\u0027t support composting. Programs like picom add support for it....","keywords":["picom","kitty","configure","opacity","install","background","configuration","save","changes","conf"]},{"id":"wiki-intel-gpu-tools","title":"Intel GPU Tools - Linux","content":"Overview This article talks about Intel GPU tools for Linux to monitor usage of integrated Intel GPUs When to use The scenarios in which I\u0027ve used this tool is to confirm that my browser is leveraging hardware acceleration. By default, Firefox doesn\u0027t enable hardware acceleration on Linux. As a result, everything runs on the CPU causing unnecessary load and faster battery drain. To alleviate that, you can enable hardware acceleration and make sure that the GPU is being used by running tools like intel_gpu_top . Install To install the tools, run the following command sudo pacman -S intel-gpu-tools Run intel_gpu_top To view GPU activity, run the following command: sudo intel_gpu_top This will display process graphs of GPU usage. References Intel Grapics - Arch Wiki Intel GPU Tools Firefox hardware acceleration Firefox Performance Settings","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/intel-gpu-tools","date":"12/11/2022 18:05 -05:00","tags":["linux","intel","gpu","manjaro","performances"],"summary":"Overview This article talks about Intel GPU tools for Linux to monitor usage of integrated Intel GPUs When to use The scenarios in which I\u0027ve used this tool is to confirm that my browser is...","keywords":["intel","tools","hardware","acceleration","firefox","sudo","used","enable","following","install"]},{"id":"wiki-video-hardware-acceleration-linux","title":"Hardware Video Acceleration - Linux","content":"Overview This article talks about using hardware video acceleration so the video card encodes/decodes video offloading the CPU and saving power. Install intel-media-driver To install the Intel media driver sudo pacman -S intel-media-driver Enable acceleration MPV For mpv, make sure that decoding and video acceleration is enabled. To do so: Create or edit the user config file in ~/.config/mpv/mpv.conf with the following settings: vo=gpu hwdec=auto Reboot your PC. Verification Use intel_gpu_top and make sure that when you play video using tools like VLC or MPV, the video bar is active. References Hardware Video Acceleration - Intel GPU Intel GPU Tools Sample MPV Conf MPV Configuration Files","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/video-hardware-acceleration-linux","date":"12/11/2022 17:36 -05:00","tags":["performance","wiki","hardware","video","hardwareacceleration","linux","arch","manjaro"],"summary":"Overview This article talks about using hardware video acceleration so the video card encodes/decodes video offloading the CPU and saving power. Install intel-media-driver To install the Intel media...","keywords":["video","intel","acceleration","media","driver","install","conf","using","hardware","tools"]},{"id":"wiki-power-management-manjaro","title":"Power Management Manjaro","content":"Overview The following shows how you can optimize your computer using Powertop to generate runing recommendations and apply those optimizations at boot using an rc-local service. Powertop Install powertop sudo pacman -S powertop Run powertop report (on battery power) sudo powertop -r After a few seconds, a file called powertop.html is created. Open the powertop.html file in your browser and inspect the Tuning tab for tuning recommendations. Configure rc-local service Create rc.local file in /etc directory with the following content #!/bin/sh -e echo \u0026quot;Hello World\u0026quot; # Insert powertop optimization scripts here. exit 0 Create rc-local.service file in /etc/systemd/system directory with the following content # This file is part of systemd. # # systemd is free software; you can redistribute it and/or modify it # under the terms of the GNU General Public License as published by # the Free Software Foundation; either version 2 of the License, or # (at your option) any later version. [Unit] Description=/etc/rc.local Compatibility ConditionPathExists=/etc/rc.local [Service] Type=oneshot ExecStart=/etc/rc.local TimeoutSec=0 StandardOutput=tty RemainAfterExit=yes SysVStartPriority=99 [Install] WantedBy=multi-user.target Enable and start rc-local service sudo systemctl enable rc-local sudo systemctl start rc-local Confirm that the service has successfully started sudo systemctl status rc-local References Arch Wiki - Powertop","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/power-management-manjaro","date":"12/11/2022 16:15 -05:00","tags":["manjaro","bash","powermanagement","tech","wiki","linux"],"summary":"Overview The following shows how you can optimize your computer using Powertop to generate runing recommendations and apply those optimizations at boot using an rc-local service. Powertop Install...","keywords":["local","powertop","service","file","sudo","following","systemctl","html","tuning","directory"]},{"id":"notes-new-jami-username","title":"New Jami username","content":"I got a reminder on the importance of backups last night. New Jami username is lqdev1. Fortunately I wasn\u0027t as active on there so not a big deal to lose my original username.","contentType":"notes","url":"https://www.lqdev.me/notes/new-jami-username","date":"11/19/2021 10:37 -05:00","tags":["jami","messaging","opensource","gnu"],"summary":"I got a reminder on the importance of backups last night. New Jami username is lqdev1. Fortunately I wasn\u0027t as active on there so not a big deal to lose my original username.","keywords":["username","reminder","importance","backups","last","night","jami","lqdev1","fortunately","wasn"]},{"id":"reviews-concentration-inner-awakening-tibetan-buddhism","title":"Concentration \u0026 Inner Awakening: According to Tibetan Buddhism","content":"Concentration \u0026amp; Inner Awakening: According to Tibetan Buddhism by Fernand Schwarz Read Rating: 2.7/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/concentration-inner-awakening-tibetan-buddhism","date":"11/18/2024 17:54 -05:00","tags":[],"summary":"Concentration \u0026amp; Inner Awakening: According to Tibetan Buddhism by Fernand Schwarz Read Rating: 2.7/5","keywords":["concentration","\u0026amp","inner","awakening:","according","tibetan","buddhism","fernand","schwarz","read"]},{"id":"reviews-soul-boom","title":"Soul Boom: Why We Need a Spiritual Revolution","content":"Soul Boom: Why We Need a Spiritual Revolution by Rainn Wilson Read Rating: 4.6/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/soul-boom","date":"11/18/2024 16:59 -05:00","tags":[],"summary":"Soul Boom: Why We Need a Spiritual Revolution by Rainn Wilson Read Rating: 4.6/5","keywords":["soul","boom:","need","spiritual","revolution","rainn","wilson","read","rating:"]},{"id":"notes-terms-of-service-cliffnotes-tosdr","title":"TOSDR: Terms of Service CliffNotes","content":"I recently came accross the Terms of Service, Didn\u0027t Read (TOSDR) website. Like most people, I\u0027m guilty of clicking \u0026quot;I have read and agree to the terms of service\u0026quot; without actually doing so. TOSDR simplifies terms of service documents by highlighting the main points for many popular services. I\u0027m not sure how maintenance and the frequency of that maintenance work. There\u0027s a rating systems which I also am not sure how it works. However, you are provided with the documents that were used to come up with the ratings and summaries, so you can look into it yourself. Overall, I think it\u0027s an incredibly useful tool to make sense of how your data is managed across these platforms.","contentType":"notes","url":"https://www.lqdev.me/notes/terms-of-service-cliffnotes-tosdr","date":"11/16/2021 13:10 -05:00","tags":["tos","tools","tosdr"],"summary":"I recently came accross the Terms of Service, Didn\u0027t Read (TOSDR) website. Like most people, I\u0027m guilty of clicking \u0026quot;I have read and agree to the terms of service\u0026quot; without actually doing...","keywords":["terms","sure","service","read","tosdr","documents","maintenance","recently","systems","which"]},{"id":"notes-rss-vlc-podcast-management","title":"Podcast management with RSS and VLC","content":"I\u0027ve written before about how versatile the RSS protocol is for publishing and consuming content . Podcasts are one of the many media types that largely rely on RSS, making it easy to subscribe and manage using an RSS client of your choice. Like RSS, VLC is versatile in the type of media it can play. When you combine the two, you\u0027re able to stay updated on the latest episodes from your favorite podcasts using the RSS reader of your choice and use the episode\u0027s URL to stream the audio in VLC.","contentType":"notes","url":"https://www.lqdev.me/notes/rss-vlc-podcast-management","date":"11/05/2021 10:27 -05:00","tags":["rss","vlc","podcast","media","audio"],"summary":"I\u0027ve written before about how versatile the RSS protocol is for publishing and consuming content . Podcasts are one of the many media types that largely rely on RSS, making it easy to subscribe and...","keywords":["using","podcasts","media","versatile","choice","reader","favorite","from","episodes","type"]},{"id":"notes-surface-duo-blogging-github-dev","title":"Blogging on the Surface Duo with github.dev","content":"Lately I\u0027ve been using github.dev to author posts and make light edits on this site. One use case where it\u0027s come extremely handy has been authoring content, specifically these microblog-like posts from my Surface Duo. Instead of opening the laptop, I can open up my Surface Duo and visit my site\u0027s repo through github.dev. When I orient the device in landscape mode I have one screen display the VS Code-like web environment while the other screen displays the keyboard. Snippets, version control, extensions, and themes all carry over making the experience consistent. Committing changes through the version control utilities immediately kicks off the GitHub Action to build and publish the site with the latest changes. Pairing the GitHub online editor environment with the Duo\u0027s dual-screen form factor showcases the productivity and convenience both of these products provide.","contentType":"notes","url":"https://www.lqdev.me/notes/surface-duo-blogging-github-dev","date":"10/31/2021 13:51 -05:00","tags":["surface","duo","blogging","github","surfaceduo","microsoft"],"summary":"Lately I\u0027ve been using github.dev to author posts and make light edits on this site. One use case where it\u0027s come extremely handy has been authoring content, specifically these microblog-like posts...","keywords":["github","site","screen","surface","control","environment","posts","version","changes","immediately"]},{"id":"notes-personal-site-stats-goatcounter","title":"Personal Site Stats - GoatCounter","content":"I\u0027ve been looking at different alternatives to Google Analytics for website stats. I don\u0027t need anything fancy, just basic page counts. Eventually I settled on GoatCounter because it\u0027s lightweight, open-source, and privacy-friendly. You can either use their managed service or self-host. Deployment with Docker felt a bit much so I did a standalone deployment to a VM. The Linode StackScript is probably the easiest way to do it.","contentType":"notes","url":"https://www.lqdev.me/notes/personal-site-stats-goatcounter","date":"10/31/2021 11:57 -05:00","tags":["analytics","web","goatcounter","indieweb","internet"],"summary":"I\u0027ve been looking at different alternatives to Google Analytics for website stats. I don\u0027t need anything fancy, just basic page counts. Eventually I settled on GoatCounter because it\u0027s lightweight,...","keywords":["deployment","looking","privacy","friendly","either","managed","service","self","host","docker"]},{"id":"notes-early-radio-history-us","title":"Early Radio History (U.S)","content":"While browsing the interwebs, I ran into the site United States Early Radio History which contains events spanning different time periods. As an operator it\u0027s fascinating to learn about the history and see how radio continues to play a large role in our day-to-day.","contentType":"notes","url":"https://www.lqdev.me/notes/early-radio-history-us","date":"10/29/2021 02:05 -05:00","tags":["radio","communication","hamradion","history","unitedstates"],"summary":"While browsing the interwebs, I ran into the site United States Early Radio History which contains events spanning different time periods. As an operator it\u0027s fascinating to learn about the history...","keywords":["radio","history","while","play","continues","learn","fascinating","operator","periods","different"]},{"id":"notes-web-neural-network-api-working-draft","title":"Web Neural Network API - Working Draft","content":"While browsing the interwebs I came across the Web Neural Network API spec from the W3C Web Machine Learning Working Group . The abstract defines it as \u0026quot;a dedicated low-level API for neural network inference hardware acceleration.\u0026quot;. Although there are already a few frameworks like ONNX \u0026amp; TensorFlow.js that allow you to inference in the browser, this spec looks interesting because it provides an abstraction that allows you to take advantage of hardware acceleration using the framework of your choice. As the explainer document mentions, \u0026quot;this architecture allows JavaScript frameworks to tap into cutting-edge machine learning innovations in the operating system and the hardware platform underneath it without being tied to platform-specific capabilities, bridging the gap between software and hardware through a hardware-agnostic abstraction layer.\u0026quot;. It\u0027s just a working draft for now but I\u0027m looking forward to how this develops.","contentType":"notes","url":"https://www.lqdev.me/notes/web-neural-network-api-working-draft","date":"10/24/2021 20:36 -05:00","tags":["webnn","ai","neuralnetwork","w3c","standards","api","protocol"],"summary":"While browsing the interwebs I came across the Web Neural Network API spec from the W3C Web Machine Learning Working Group . The abstract defines it as \u0026quot;a dedicated low-level API for neural...","keywords":["hardware","\u0026quot","working","acceleration","inference","platform","learning","machine","frameworks","spec"]},{"id":"notes-top-10-listicles","title":"Top 10 listicles","content":"The other day I was going down a Wikipedia rabbit hole and learned those click-baity \u0026quot;Top 10 ...\u0026quot; or \u0026quot;Worst 10 ...\u0026quot; articles apparently have a name, listicles . The more you know...","contentType":"notes","url":"https://www.lqdev.me/notes/top-10-listicles","date":"10/24/2021 19:33 -05:00","tags":["til","internet","listicles","wikipedia"],"summary":"The other day I was going down a Wikipedia rabbit hole and learned those click-baity \u0026quot;Top 10 ...\u0026quot; or \u0026quot;Worst 10 ...\u0026quot; articles apparently have a name, listicles . The more you...","keywords":["\u0026quot","going","down","wikipedia","rabbit","hole","learned","those","click","baity"]},{"id":"posts-super-simple-captcha","title":"Super simple captcha","content":"## Introduction I\u0027ve been thinking about implementing webmentions on this site. According to the spec, \u0022Webmention is a simple way to notify any URL when you mention it on your site. From the receiver\u0027s perspective, it\u0027s a way to request notifications when other sites mention it\u0022. I won\u0027t get into the technical details of webmentions in this post, but the simplest way to implement webmentions is to have a text input box on your site for each of your articles, kind of like a comment box. When someone wants to mention your post on their site, they can use the text input box on your site to add the URL from their site where they mention your post. When the user submits the webmention, there\u0027s some endpoint that listens for webmention submissions and processes them accordingly. If you\u0027re interested in learning more about webmentions, you can check out the following resources: - [Webmentions spec](https://www.w3.org/TR/webmention/) - [Webmention Rocks](https://webmention.rocks/) - [Sending Your First Webmention - IndieWebCamp D\u00FCsseldorf 2019](https://yewtu.be/watch?v=ZOlkS6xP2Zk) While it\u0027s great that you can mention and comment on other people\u0027s content from your site, like comments, writing a script to submit spam is relatively simple. Therefore, I want to create some sort of [CAPTCHA](https://en.wikipedia.org/wiki/CAPTCHA) challenge as an initial form of validation to prevent spam. At the same time, I don\u0027t want to have any external dependencies. In this post, I\u0027ll show how I went about implementing a captcha-like solution with zero dependencies to prevent spam submissions. ## Simple captcha The solution I came up with asks the users to add the day of the month (1-31) to a random number between 1-100. While not entirely foolproof, it\u0027s \u0022complex\u0022 enough that it\u0027s not the same thing every time. Below is what the implementation looks like. ![Captcha implementation asking for date](https://user-images.githubusercontent.com/11130940/138604591-dfe4c301-78fe-4338-a751-799b420a1791.png) The HTML markup for the webpage looks like the following: \u0060\u0060\u0060html Super Simple Captcha Submit \u0060\u0060\u0060 When the \u0060body\u0060 element of the page loads, it invokes the \u0060setQuery\u0060 function, which displays the challenge the user is supposed to solve in the \u0060query\u0060 span element. The user then submits their answer via the \u0060answerInput\u0060 text input box. The answer is then checked by invoking the \u0060displayResult\u0060 function. The \u0060displayResult\u0060 function checks the user\u0027s answer against the expected answer. If the answer is correct, the text \u0022OK\u0022 is displayed on the page\u0027s \u0060result\u0060 H3 element. Otherwise, the text \u0022Try again\u0022 displays on the webpage. All of the code that handles this logic is in the \u0060main.js\u0060 file. \u0060\u0060\u0060javascript let date = new Date() let day = date.getDate() // Day of the month let randomNumber = Math.floor(Math.random() * 100) \u002B 1 let answer = day \u002B randomNumber // Expected answer let setQuery = () =\u003E { let element = document.getElementById(\u0027query\u0027); element.innerText = \u0060Enter the sum of ${day} \u002B ${randomNumber}\u0060; } let checkAnswer = () =\u003E { let userAnswer = parseInt(document.getElementById(\u0022answerInput\u0022).value); return answer === userAnswer } let displayResult = () =\u003E { let result = checkAnswer() ? \u0022OK\u0022 : \u0022Try again\u0022 // Ternary function to check if answer is correct document.getElementById(\u0022result\u0022).innerText = result; } \u0060\u0060\u0060 That\u0027s all there is to it! ## Conclusion In this post, I showed how to implement a dependency-free solution to present a challenge to users submitting comments / webmentions to your site. Although the solution isn\u0027t foolproof, it\u0027s just complex enough any spammers would have to work a little harder. Happy coding!","contentType":"posts","url":"https://www.lqdev.me/posts/super-simple-captcha","date":"10/24/2021 13:00 -05:00","tags":["web development","html","javascript","webmentions"],"summary":"## Introduction I\u0027ve been thinking about implementing webmentions on this site. According to the spec, \u0022Webmention is a simple way to notify any URL when you mention it on your site. From the...","keywords":["answer","site","webmentions","post","captcha","element","webmention","text","mention","date"]},{"id":"notes-lqdevme-redirect","title":"lqdev.me redirect","content":"That was easier than I thought. Visiting lqdev.me now redirects to this site. Redirect isn\u0027t limited to the main page though. You can also access any other resources as well. For example, visiting lqdev.me/feed would redirect you to the main feed on this site.","contentType":"notes","url":"https://www.lqdev.me/notes/lqdevme-redirect","date":"10/22/2021 13:57 -05:00","tags":["website","domain","redirect","ownyourlinks"],"summary":"That was easier than I thought. Visiting lqdev.me now redirects to this site. Redirect isn\u0027t limited to the main page though. You can also access any other resources as well. For example, visiting...","keywords":["main","visiting","lqdev","site","redirect","example","resources","access","also","though"]},{"id":"reviews-unfuck-your-writing","title":"Unfuck Your Writing: Write Better, Reach Readers, \u0026 Share Your Inner World","content":"Unfuck Your Writing: Write Better, Reach Readers, \u0026amp; Share Your Inner World by Joe Biel, Faith Harper Read Rating: 3.0/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/unfuck-your-writing","date":"10/21/2022 20:57 -05:00","tags":[],"summary":"Unfuck Your Writing: Write Better, Reach Readers, \u0026amp; Share Your Inner World by Joe Biel, Faith Harper Read Rating: 3.0/5","keywords":["unfuck","writing:","write","better","reach","readers","\u0026amp","share","inner","world"]},{"id":"notes-rel-canonical-tags","title":"TIL: Canonical HTML tags","content":"Today I learned about canonical URLs. While looking into how I can syndicate content from this site, specifically longer form blog posts on other sites like dev.to, I came across canonical tags. Basically they\u0027re a way of telling the internet, specifically search engines, what version of your content is the main copy or single-source of truth. Currently I\u0027ve configured my site so it\u0027s not indexed or crawled. However, for sites I don\u0027t own I don\u0027t have the same level of control. Check out the canonicalization and SEO best practices for canonical URLs articles from Moz if you\u0027re interested in learning more.","contentType":"notes","url":"https://www.lqdev.me/notes/rel-canonical-tags","date":"10/21/2021 21:31 -05:00","tags":["html","til","canonical","web","w3c","standards"],"summary":"Today I learned about canonical URLs. While looking into how I can syndicate content from this site, specifically longer form blog posts on other sites like dev.to, I came across canonical tags....","keywords":["canonical","urls","content","from","site","specifically","sites","today","source","truth"]},{"id":"notes-nvidia-canvas-next-gen-stick-figures-ai","title":"Next gen stick figures using AI and NVIDIA Canvas","content":"I have almost zero artistic skills, so when I came across NVIDIA Canvas I was extremely excited! Essentially it\u0027s an application that takes simple brushstrokes as input and uses a Generative Adversarial Network (GAN) to turn them into realistic landscape photos. I had known about GANs being used to generate faces (see This Person Does Not Exist ) but it\u0027s one of the first times I\u0027ve seen them used to enhance drawings. Time to take my stick figures to the next level!","contentType":"notes","url":"https://www.lqdev.me/notes/nvidia-canvas-next-gen-stick-figures-ai","date":"10/19/2021 10:51 PM -05:00","tags":["nvidia","ai","genai","art"],"summary":"I have almost zero artistic skills, so when I came across NVIDIA Canvas I was extremely excited! Essentially it\u0027s an application that takes simple brushstrokes as input and uses a Generative...","keywords":["used","almost","landscape","photos","known","gans","generate","faces","realistic","person"]},{"id":"notes-coffee-with-codex","title":"Coffee with a Codex","content":"Came accross this event series via Chris Aldrich\u0027s microblog . Here\u0027s Chris\u0027 original post Every Monday, I sit down with a cup of coffee and learn something interesting about history through old manuscripts. Join our friends Dot Porter ( @leoba ) and the Schoenberg Institute for Manuscript Studies ( @sims_mss ) at Coffee with a Codex . Basically it\u0027s a virtual meeting where experts go over manuscripts on a variety of topics. I\u0027ll probably check out next week\u0027s event.","contentType":"notes","url":"https://www.lqdev.me/notes/coffee-with-codex","date":"10/19/2021 10:21 AM -05:00","tags":["event","learning","history"],"summary":"Came accross this event series via Chris Aldrich\u0027s microblog . Here\u0027s Chris\u0027 original post Every Monday, I sit down with a cup of coffee and learn something interesting about history through old...","keywords":["event","manuscripts","chris","coffee","came","institute","manuscript","studies","@sims","codex"]},{"id":"wiki-plaintext","title":"Plain text","content":"Overview This guide provides an overview of plain text and how it can be used for a variety of tasks and advantages over rich text and custom text formats. What is plain text? In general, plain text is text that isn\u0027t formatted and generally uses common characters. Benefits of plain text Simplicity: Focus on the content rather than the formatting or medium Portability: Any text editor can create and display plain text. That means you\u0027re not locked in to any particular tool or service. Local-first: Because you can create plain text files using any phone or computer, it means your files belong to you and you have access to them any time regardless of network connectivity. While you can upload and sync your files with your preferred cloud storage provider for redundancy and backup, it\u0027s not a requirement. Longevity: Computers have been able to create and display plain text for decades. Since you don\u0027t need proprietary file formats or tools to create and consume plain text, it\u0027s likely many decades from now you\u0027ll be able to read plain text. Additional plain markup and text formats You can write plain text on its own. However, you can format your plain text to provide additional visual cues, information, and context. Some popular plain formats and markup languages include: Markdown Org LaTeX Editors Below are a few popular plain text editors you can use to read and create plain text files: Emacs vi/vim VS Code Notepad Use cases Note taking The easiest way to start taking notes using plain text is to open any text editor of choice and start capturing your thoughts. Productivity / Personal Knowledge Management An extension to note taking is personal knowlege management and productivity. Whether it\u0027s scheduling appointments, journaling, tracking habits (exercise, reading, meditation), or managing to-do lists, you can use plain text for these tasks. Some tools and formats that can help with that include: Emacs Org Mode todo.txt Email While e-mail clients support HTML, in general e-mail communication is predomenantly text-based. Therefore, plain text is all you need. If you wanted to only use plain text, e-mail clients enable you to configure them to only support plain text. While your HTML e-mails may look weird, there are some advantages. The use plaintext email guide can provide guidance as to how to get started using plaintext only for e-mails Personal finance There are many apps for managing your personal finance. However, if you want to keep it simple, you can track your personal finances using plain text as well. Some tools that can help you do that include: Ledger Databases When you think of a database, you might think of a complicated system hosted in a server. While database systems are optimized for enterprise production workloads, at the end of the day, the purpose of a database is to store data. That means plain text can work for that too. Some tools that can help you here include: GNU Recutils Blogging and building websites The more capable our computing devices have become, our software has suffered from inflation by becoming larger. However, larger doesn\u0027t mean the software is richer. If you\u0027re just trying to transmit information, you can upload your raw text files containing your blog posts or website to a server and they can be displayed in the browser. Examples of these sites include: minim.blog 250kb club 512kb club Often you can build these websites with the help of static site generators such as. The Static Site Generators catalog provides more details on popular ones. For hosting, you can do so for free using offerings like GitHub Pages. The Setting up a GitHub Pages site with Jekyll can help you get started there. Additional Resources Plaintext Productivity The Plain Text Project Org Mode - Organize Your Life In Plain Text! The Plain Text Life: Note Taking, Writing and Life Organization Using Plain Text Files Write plain text files","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/plaintext","date":"10/14/2023 12:56 -05:00","tags":["plaintext","productivity","email","notes","personalknowledgemanagement","pkm","wiki","calendar","finance","tools","databases","writing"],"summary":"Overview This guide provides an overview of plain text and how it can be used for a variety of tasks and advantages over rich text and custom text formats. What is plain text? In general, plain text...","keywords":["text","plain","using","files","create","include:","help","personal","formats","tools"]},{"id":"reviews-on-writing","title":"On Writing","content":"On Writing by Stephen King InProgress","contentType":"reviews","url":"https://www.lqdev.me/reviews/on-writing","date":"10/09/2022 19:10 -05:00","tags":[],"summary":"On Writing by Stephen King InProgress","keywords":["writing","stephen","king","inprogress"]},{"id":"notes-terrifier-2-100-10","title":"Terrifier 2 - 100/10","content":"Terrifier 2 did not disappoint. 100/10. Can\u0027t wait for Terrifier 3.","contentType":"notes","url":"https://www.lqdev.me/notes/terrifier-2-100-10","date":"10/09/2022 14:21 -05:00","tags":["terrifier","horror","movie","art","terrifier2"],"summary":"Terrifier 2 did not disappoint. 100/10. Can\u0027t wait for Terrifier 3.","keywords":["terrifier","disappoint","100/10","wait"]},{"id":"notes-matrix-migration-complete","title":"Success! Matrix migration complete","content":"It only took 6 hours but I was able to get it done with a little help.","contentType":"notes","url":"https://www.lqdev.me/notes/matrix-migration-complete","date":"10/09/2021 12:23 AM -05:00","tags":["selfhost","matrix","messaging","opensource","element","synapse"],"summary":"It only took 6 hours but I was able to get it done with a little help.","keywords":["took","hours","able","done","little","help"]},{"id":"wiki-waydroid-nixos-x11","title":"Running Waydroid on NixOS using X11","content":"Overview Packages weston - Wayland compositor NixOS Configuration virtualisation = { waydroid = { enable = true; }; }; environment.systemPackages = with pkgs; [weston]; Initialize Waydroid This downloads the LineageOS image and installs it # Fetch WayDroid images. # You can add the parameters \u0026quot;-s GAPPS -f\u0026quot; to have GApps support. sudo waydroid init Start Waydroid Open the terminal and start the waydroid container sudo systemctl start waydroid-container Check status of container sudo journalctl -u waydroid-container Open terminal and start weston weston In the weston window, open a terminal and start Waydroid sesson in the background waydroid session start \u0026amp; Start the UI waydroid show-full-ui Stop Waydroid Close weston window Stop waydroid container sudo systemctl stop waydroid-container Remove user data Only do this if you don\u0027t want to persist anything from that container # Removing images and user data sudo rm -r /var/lib/waydroid/* ~/.local/share/waydroid Update Android sudo waydroid upgrade General Usage # Start Android UI waydroid show-full-ui # List Android apps waydroid app list # Start an Android app waydroid app launch \u0026lt;application name\u0026gt; # Install an Android app waydroid app install \u0026lt;/path/to/app.apk\u0026gt; # Enter the LXC shell sudo waydroid shell # Overrides the full-ui width waydroid prop set persist.waydroid.width 608 Resources NixOS Wiki - WayDroid Script to start LineageOS on X11 Any plan to support X11?","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/waydroid-nixos-x11","date":"10/05/2023 22:25 -05:00","tags":["android","linux","nixos","x11","waydroid"],"summary":"Overview Packages weston - Wayland compositor NixOS Configuration virtualisation = { waydroid = { enable = true; }; }; environment.systemPackages = with pkgs; [weston]; Initialize Waydroid This...","keywords":["waydroid","start","container","sudo","weston","android","stop","full","terminal","open"]},{"id":"notes-windows-11-upgrade","title":"Windows 11 upgrade","content":"No Windows 11 upgrade for me. \uD83D\uDE25 Just because I can\u0027t doesn\u0027t mean you can\u0027t though. Go and check if you\u0027re eligible at Windows Update Start \u0026gt; Settings \u0026gt; Update \u0026amp; Security \u0026gt; Windows Update . See Manage updates in Windows for more details. For alternative install options, check out the Download Windows 11 page.","contentType":"notes","url":"https://www.lqdev.me/notes/windows-11-upgrade","date":"10/05/2021 08:55 AM -05:00","tags":["windows10","windows","windows11","microsoft"],"summary":"No Windows 11 upgrade for me. \uD83D\uDE25 Just because I can\u0027t doesn\u0027t mean you can\u0027t though. Go and check if you\u0027re eligible at Windows Update Start \u0026gt; Settings \u0026gt; Update \u0026amp; Security \u0026gt; Windows...","keywords":["windows","update","check","options","install","alternative","details","more","updates","manage"]},{"id":"posts-automate-yaml-front-matter-vs-code-snippets","title":"Automate YAML front-matter generation with custom Visual Studio Code snippets","content":"## Introduction When creating content with static site generators such as Jekyll, Hexo, and many others, page metadata such as publish date, title, tags, and other details is created using YAML front-matter. Typically these pages are authored in markdown. At the top of each page, there\u0027s a YAML formatted section containing all of these details. Sometimes these static site generators include tooling to make it easier to scaffold new pages. In the case of my website, I created my own static site generator. Therefore, scaffolding a new page, especially writing the boilerplate YAML front-matter is cumbersome and time consuming. Since I use Visual Studio Code for authoring content and developing the static site generator, I\u0027ve decided to use [snippets](https://code.visualstudio.com/docs/editor/userdefinedsnippets) to help me automate YAML front-matter generation. In this post, I\u0027ll show the general process. ## Create snippets file The first thing you\u0027ll want to do is create a snippets file. You can have one or as many as you want. For this website, I have a single snippets file called *metadata.code-snippets* for all my YAML front-matter. To create a snippets file: 1. Open the Visual Studio Code command palette. 1. Enter the command \u0060\u003EPreferences: Configure User Snippets\u0060 into the text box. 1. Select the **New Snippets file for \\ ** from the list of options. ![Create snippet file from Visual Studio Code command palette](https://user-images.githubusercontent.com/11130940/135934846-a76dfa16-caed-4489-9e50-95183add673d.png) 1. Provide a name for your snippets file and press **Enter**. 1. A file with the *.code-snippets* extension is created in the *.vscode* directory inside your project. One benefit of creating a snippets file and saving it in the *.vscode* directory is you can check it into source control and use it anywhere, even github.dev. ## Create snippets Now that you\u0027ve created the file, it\u0027s time to define your snippets. 1. Open your snippets file. ![Snippet file for luisquintanilla.me](https://user-images.githubusercontent.com/11130940/135935306-1cbd48e1-6e48-423c-b87f-cb6f7a8eb085.png) 1. Create a new JSON object and define your snippet. For example, the snippet for blog posts on this site looks like the following. \u0060\u0060\u0060json \u0022Article Post metadata\u0022: { \u0022scope\u0022: \u0022markdown\u0022, \u0022prefix\u0022: \u0022article\u0022, \u0022body\u0022: [ \u0022---\u0022, \u0022post_type: \\\u0022article\\\u0022 \u0022, \u0022title: \\\u0022\\\u0022\u0022, \u0022published_date: \\\u0022$CURRENT_MONTH/$CURRENT_DATE/$CURRENT_YEAR $CURRENT_HOUR:$CURRENT_MINUTE\\\u0022\u0022, \u0022tags: \u0022, \u0022---\u0022 ], \u0022description\u0022: \u0022Blog post article metadata\u0022 } \u0060\u0060\u0060 Let\u0027s break down each of the properties: - \u0060scope\u0060: By default, snippets apply to all languages and projects. In my case, since I intend only to use these snippets in markdown files, I set the scope to \u0060markdown\u0060. - \u0060prefix\u0060: The prefix is the word in the page that is associated with. When I type \u0060article\u0060 in a markdown page, a recommendation to use the snippet appears. - \u0060body\u0060: The code or content of my snippet. In this case, the expected output is similar to the following. \u0060\u0060\u0060yaml --- post_type: \u0022article\u0022 title: \u0022\u0022 published_date: \u002210/04/2021 18:58 -05:00\u0022 tags: --- \u0060\u0060\u0060 VS Code also has built-in variables that you can use to automatically set date, time, and other values. - \u0060description\u0060: A text description of your snippet. For more information on creating snippets, see [Create your own snippets](https://code.visualstudio.com/docs/editor/userdefinedsnippets#_create-your-own-snippets). ## Use your snippets 1. Create a new markdown file. 1. Type your prefix into the file. In my case, for blog posts my prefix is \u0060article\u0060. 1. Press **Ctrl \u002B Space**. For Macs you might have to use Cmd instead of Ctrl. ![Populate article front-matter using snippet](https://user-images.githubusercontent.com/11130940/135935808-46ca8314-c2be-47f1-9c9b-9b722e37d908.png) 1. A tooltip with your snippet\u0027s description appears. Press **Enter**. At this point, your snippet\u0027s prefix is replaced with the content defined in the snippet\u0027s body. ## Conclusion In this post I\u0027ve shown how you can use Visual Studio Code snippets to automate YAML front-matter generation when working with common static site generators that don\u0027t provide tooling to scaffold new pages. Snippets can be used for a variety of scenarios and if there\u0027s any boilerplate code or content you constantly have to write, they can save you a lot of time. Happy writing!","contentType":"posts","url":"https://www.lqdev.me/posts/automate-yaml-front-matter-vs-code-snippets","date":"10/04/2021 19:58 -05:00","tags":["blogging","tooling","visual-studio","visual-studio-code","markdown","yaml"],"summary":"## Introduction When creating content with static site generators such as Jekyll, Hexo, and many others, page metadata such as publish date, title, tags, and other details is created using YAML...","keywords":["snippets","file","snippet","create","code","site","post","matter","front","yaml"]},{"id":"notes-fb-ig-personal-feed-up","title":"PSA: Personal site not down","content":"Just jumping on the bandwagon. A few online services are down at the moment \uD83D\uDE2C Not to worry...the personal site is still chugging along \uD83D\uDE0C","contentType":"notes","url":"https://www.lqdev.me/notes/fb-ig-personal-feed-up","date":"10/04/2021 12:55 PM -05:00","tags":["indieweb","website","socialmedia","feed","personalwebsite"],"summary":"Just jumping on the bandwagon. A few online services are down at the moment \uD83D\uDE2C Not to worry...the personal site is still chugging along \uD83D\uDE0C","keywords":["jumping","bandwagon","online","services","down","moment","worry","personal","site","chugging"]},{"id":"notes-digital-gardens-technology-review-article","title":"Cultivating digital gardens","content":"Just read an interesting article publised in Technology Review called Digital gardens let you cultivate your own little bit of the internet . In a sense, some of the updates to this site like the addition of the different feeds can be thought of as \u0026quot;cultivating\u0026quot; my own digital garden.","contentType":"notes","url":"https://www.lqdev.me/notes/digital-gardens-technology-review-article","date":"10/03/2021 09:57 PM -05:00","tags":["indieweb","digitalgarden","website","internet"],"summary":"Just read an interesting article publised in Technology Review called Digital gardens let you cultivate your own little bit of the internet . In a sense, some of the updates to this site like the...","keywords":["digital","read","\u0026quot","thought","feeds","different","addition","site","updates","sense"]},{"id":"notes-many-saints-of-newark","title":"The Many Saints of Newark","content":"I would\u0027ve been okay with The Many Saints of Newark being a 4 hour movie. I didn\u0027t want it to end. Now I have to rewatch The Sopranos.","contentType":"notes","url":"https://www.lqdev.me/notes/many-saints-of-newark","date":"10/02/2021 08:19 PM -05:00","tags":["sopranos","movie","newjersey","newark","nj"],"summary":"I would\u0027ve been okay with The Many Saints of Newark being a 4 hour movie. I didn\u0027t want it to end. Now I have to rewatch The Sopranos.","keywords":["okay","many","saints","newark","hour","movie","didn","want","rewatch","sopranos"]},{"id":"notes-skype-future-annoucement-09-2021","title":"The future of Skype","content":"For personal use, I tend to default to Skype for my video / audio call needs. Great to see there are some long-term plans for the app and it\u0027ll be around for a while more. The future of Skype: Fast, playful, delightful and buttery smooth\u200B","contentType":"notes","url":"https://www.lqdev.me/notes/skype-future-annoucement-09-2021","date":"10/01/2021 4:20 PM -05:00","tags":["skype","messaging","microsoft","roadmap"],"summary":"For personal use, I tend to default to Skype for my video / audio call needs. Great to see there are some long-term plans for the app and it\u0027ll be around for a while more. The future of Skype: Fast,...","keywords":["personal","delightful","playful","fast","skype:","future","more","while","around","buttery"]},{"id":"notes-feed-one-year","title":"Personal Feed - One Year In","content":"It\u0027s been a year since I created this feed and started posting mainly on my website. Since then, I\u0027ve: Used a shorter domain to redirect to my website domain. Created VS Code snippets to simplify metadata tagging. Used github.dev as the main interface for authoring and editing posts. For longer posts, I use VS Code locally but for these smaller microblog-style posts, github.dev makes it really easy. Syndicated posts to Twitter and Mastodon . Created a response feed for interactions like replies, reshares (repost), and likes (favorites) with support for sending Webmentions . Created RSS feeds for each of my feeds. It hasn\u0027t happened overnight. Instead, it\u0027s been the result of small incremental efforts over time. Overall, I don\u0027t think I\u0027ve posted any more or less than I do on other platforms. What I have enjoyed the most though has been: Learning. Building the tools and processes to author and share content. Owning my content. My website acts as the single source of truth. Not requiring others to have or create accounts on individual platforms to access content. Choosing how I author content. I\u0027m sure this post is way over the 280 character limit and if there\u0027s a typo, I can just edit the post and republish the website \uD83D\uDE42 Going forward, I plan to: Update RSS feeds to include post content, not just link to the post. Accept Webmentions. I\u0027m about halfway done with the main parts of my implementation . Implement tags for easier discoverability / search. Consolidate metadata for posts (articles, microblogs, responses).","contentType":"notes","url":"https://www.lqdev.me/notes/feed-one-year","date":"09/29/2022 10:55 -05:00","tags":["microblog","blog","feed","rss","indieweb","openweb","internet"],"summary":"It\u0027s been a year since I created this feed and started posting mainly on my website. Since then, I\u0027ve: Used a shorter domain to redirect to my website domain. Created VS Code snippets to simplify...","keywords":["posts","content","website","post","created","feeds","code","webmentions","main","github"]},{"id":"notes-hello-world","title":"Hello World","content":"Hello World! First post in the personal feed.","contentType":"notes","url":"https://www.lqdev.me/notes/hello-world","date":"09/29/2021 07:16 PM -05:00","tags":["blog","microblog","indieweb","internet","feed","rss"],"summary":"Hello World! First post in the personal feed.","keywords":["hello","world","post","personal","feed"]},{"id":"reviews-the-internet-con-doctorow","title":"The Internet Con: How to Seize the Means of Computation ","content":"The Internet Con: How to Seize the Means of Computation by Cory Doctorow Read Rating: 4.0/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/the-internet-con-doctorow","date":"09/28/2023 20:27 -05:00","tags":[],"summary":"The Internet Con: How to Seize the Means of Computation by Cory Doctorow Read Rating: 4.0/5","keywords":["internet","con:","seize","means","computation","cory","doctorow","read","rating:"]},{"id":"reviews-jung-very-short-introduction-stevens","title":"Jung: A Very Short Introduction","content":"Jung: A Very Short Introduction by Anthony Stevens Read Rating: 4.8/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/jung-very-short-introduction-stevens","date":"09/27/2024 07:47 -05:00","tags":[],"summary":"Jung: A Very Short Introduction by Anthony Stevens Read Rating: 4.8/5","keywords":["jung:","short","introduction","anthony","stevens","read","rating:"]},{"id":"reviews-creative-act-way-of-being-rubin","title":"The Creative Act: A Way of Being","content":"The Creative Act: A Way of Being by Rick Rubin Read Rating: 4.8/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/creative-act-way-of-being-rubin","date":"09/27/2024 07:40 -05:00","tags":[],"summary":"The Creative Act: A Way of Being by Rick Rubin Read Rating: 4.8/5","keywords":["creative","act:","rick","rubin","read","rating:"]},{"id":"snippets-webmentions-verification","title":"Webmentions Verification","content":"Description Sample script that shows how to perform Webmention verification per Webmentions specification . Usage dotnet fsi webmention-verification.fsx Snippet webmention-verification.fsx // https://www.w3.org/TR/webmention/#webmention-verification #r \u0026quot;nuget:FSharp.Data\u0026quot; #r \u0026quot;nuget: Microsoft.AspNetCore.WebUtilities, 2.2.0\u0026quot; open System open System.Net open System.Net.Http open System.Net.Http.Headers open System.Collections.Generic open Microsoft.AspNetCore.WebUtilities open FSharp.Data type WebmentionVerificationResult = | TaggedMention of {| Replies: bool; Likes: bool; Reposts: bool|} | UntaggedMention | Error of string let getFormContent (request:HttpRequestMessage) = async { let! content = request.Content.ReadAsStringAsync() |\u0026gt; Async.AwaitTask let query = QueryHelpers.ParseQuery(content) let source = query[\u0026quot;source\u0026quot;] |\u0026gt; Seq.head let target = query[\u0026quot;target\u0026quot;] |\u0026gt; Seq.head return source,target } let cont = dict [ (\u0026quot;source\u0026quot;,\u0026quot;https://raw.githubusercontent.com/lqdev/fsadvent-2021-webmentions/main/reply.html\u0026quot;) (\u0026quot;target\u0026quot;,\u0026quot;https://webmention.rocks/test/1\u0026quot;) ] let buildSampleRequestMessage (content:IDictionary\u0026lt;string,string\u0026gt;) = let reqMessage = new HttpRequestMessage() reqMessage.Content \u0026lt;- new FormUrlEncodedContent(content) reqMessage let req = buildSampleRequestMessage cont // verification let source,target = req |\u0026gt; getFormContent |\u0026gt; Async.RunSynchronously let getMentionUsingCssSelector (doc:HtmlDocument) (selector:string) (target:string) = doc.CssSelect(selector) |\u0026gt; List.map(fun x -\u0026gt; x.AttributeValue(\u0026quot;href\u0026quot;)) |\u0026gt; List.filter(fun x -\u0026gt; x = target) let hasMention (mentions:string list) = mentions |\u0026gt; List.isEmpty |\u0026gt; not let verifyWebmentions (source:string) (target:string)= async { use client = new HttpClient() let reqMessage = new HttpRequestMessage(new HttpMethod(\u0026quot;Get\u0026quot;), source) reqMessage.Headers.Accept.Clear() // Only accept text/html content reqMessage.Headers.Accept.Add(new MediaTypeWithQualityHeaderValue(\u0026quot;text/html\u0026quot;)) // Get document let! res = client.SendAsync(reqMessage) |\u0026gt; Async.AwaitTask // Verify webmention let webmentions = match res.IsSuccessStatusCode with | true -\u0026gt; // Get document contents let body = async { return! res.Content.ReadAsStringAsync() |\u0026gt; Async.AwaitTask } |\u0026gt; Async.RunSynchronously // Parse document let doc = HtmlDocument.Parse(body) // Get links tagged as replies using microformats let replies = getMentionUsingCssSelector doc \u0026quot;.u-in-reply-to\u0026quot; target // Get links tagged as likes using microformats let likes = getMentionUsingCssSelector doc \u0026quot;.u-in-like-of\u0026quot; target // Get links tagged as repost using microformats let shares = getMentionUsingCssSelector doc \u0026quot;.u-in-repost-of\u0026quot; target // Get untagged mentions let mentions = getMentionUsingCssSelector doc \u0026quot;a\u0026quot; target // Collect all tagged mentions let knownInteractions = [replies;likes;shares] |\u0026gt; List.collect(id) // Choose tagged mentions before untagged mentions match knownInteractions.IsEmpty,mentions.IsEmpty with | true,true -\u0026gt; Error \u0026quot;Target not mentioned\u0026quot; | true,false | false,false -\u0026gt; TaggedMention {| Replies = hasMention replies Likes = hasMention likes Reposts = hasMention shares |} | false,true -\u0026gt; UntaggedMention | false -\u0026gt; Error \u0026quot;Unable to get source\u0026quot; return webmentions } verifyWebmentions source target |\u0026gt; Async.RunSynchronously Sample Output Interactions { Likes = false Replies = true Shares = false }","contentType":"snippets","url":"https://www.lqdev.me/resources/snippets/webmentions-verification","date":"09/25/2022 15:13 -05:00","tags":["f#","indieweb","webmentions","internet","web","social","interactive","script"],"summary":"Description Sample script that shows how to perform Webmention verification per Webmentions specification . Usage dotnet fsi webmention-verification.fsx Snippet webmention-verification.fsx //...","keywords":["\u0026quot","|\u0026gt","target","async","open","reqmessage","content","mentions","false","true"]},{"id":"snippets-webmentions-request-verification","title":"Webmentions Request Verification","content":"Description Sample script that shows how to perform Webmention request verification per Webmentions specification . Usage dotnet fsi request-verification.fsx Snippet request-verification.fsx // https://www.w3.org/TR/webmention/#request-verification // 1. Send response with 202 Accepted to acknowledge successful request // 2. DONE: Check that the protocol is http or https // 3. DONE: Source URL is different than Target URL // 4. DONE Check that Target URL is a valid resource #r \u0026quot;nuget: Microsoft.AspNetCore.WebUtilities, 2.2.0\u0026quot; open System open System.Net open System.Net.Http open System.Collections.Generic open Microsoft.AspNetCore.WebUtilities type RequestVerificationResult = | Ok of HttpRequestMessage | Error of string // Parse Form URL Encoded string let getFormContent (request:HttpRequestMessage) = async { let! content = request.Content.ReadAsStringAsync() |\u0026gt; Async.AwaitTask let query = QueryHelpers.ParseQuery(content) let source = query[\u0026quot;source\u0026quot;] |\u0026gt; Seq.head let target = query[\u0026quot;target\u0026quot;] |\u0026gt; Seq.head return source,target } // Check protocol is HTTP or HTTPS let checkProtocol (request: RequestVerificationResult) = match request with | Ok m -\u0026gt; let source,target = async { return! getFormContent(m) } |\u0026gt; Async.RunSynchronously let isProtocolValid = match source.StartsWith(\u0026quot;http\u0026quot;),target.StartsWith(\u0026quot;http\u0026quot;) with | true,true -\u0026gt; Ok m | true,false -\u0026gt; Error \u0026quot;Target invalid protocol\u0026quot; | false,true -\u0026gt; Error \u0026quot;Source invalid protocol\u0026quot; | false,false -\u0026gt; Error \u0026quot;Source and Target invalid protocol\u0026quot; isProtocolValid | Error s -\u0026gt; Error $\u0026quot;{s}\u0026quot; // Check the URLs are not the same let checkUrlsSame (request:RequestVerificationResult) = match request with | Ok m -\u0026gt; let source,target = async { return! getFormContent(m) } |\u0026gt; Async.RunSynchronously let check = match source.Equals(target) with | true -\u0026gt; Error \u0026quot;Urls are the same\u0026quot; | false -\u0026gt; Ok m check | Error s -\u0026gt; Error s // Helper functions let uriIsMine (url:string) = let uri = new Uri(url) uri.Host.Equals(\u0026quot;lqdev.me\u0026quot;) || uri.Host.Equals(\u0026quot;www.luisquintanilla.me\u0026quot;) || uri.Host.Equals(\u0026quot;luisquintanilla.me\u0026quot;) let isValid (url:string) (msg:HttpResponseMessage) = let isMine = uriIsMine url isMine \u0026amp; msg.IsSuccessStatusCode // Check URL is a valid resource // Valid means, the URL is one of my domains and returns a non-400 or 500 HTML status code let checkUrlValidResource (request:RequestVerificationResult) = match request with | Ok m -\u0026gt; let res = async { let! source,target = getFormContent(m) use client = new HttpClient() let reqMessage = new HttpRequestMessage(HttpMethod.Head, target) let! resp = client.SendAsync(reqMessage) |\u0026gt; Async.AwaitTask return isValid target resp } |\u0026gt; Async.RunSynchronously match res with | true -\u0026gt; Ok m | false -\u0026gt; Error \u0026quot;Target is not a valid resource\u0026quot; | Error s -\u0026gt; Error s // Combine validation steps into single function let validate = checkProtocol \u0026gt;\u0026gt; checkUrlsSame \u0026gt;\u0026gt; checkUrlValidResource // Test application let buildSampleRequestMessages (content:IDictionary\u0026lt;string,string\u0026gt;) = let reqMessage = new HttpRequestMessage() reqMessage.Content \u0026lt;- new FormUrlEncodedContent(content) let liftedReqMessage = Ok reqMessage liftedReqMessage let sampleContent = [ dict [ (\u0026quot;source\u0026quot;,\u0026quot;http://lqdev.me\u0026quot;) (\u0026quot;target\u0026quot;,\u0026quot;http://lqdev.me\u0026quot;) ] dict [ (\u0026quot;source\u0026quot;,\u0026quot;http://://lqdev.me\u0026quot;) (\u0026quot;target\u0026quot;,\u0026quot;protocol://lqdev.me\u0026quot;) ] dict [ (\u0026quot;source\u0026quot;,\u0026quot;http://lqdev.me\u0026quot;) (\u0026quot;target\u0026quot;,\u0026quot;http://github.com/lqdev\u0026quot;) ] dict [ (\u0026quot;source\u0026quot;,\u0026quot;http://github.com/lqdev\u0026quot;) (\u0026quot;target\u0026quot;,\u0026quot;http://lqdev.me\u0026quot;) ] ] sampleContent |\u0026gt; List.map(buildSampleRequestMessages) |\u0026gt; List.map(validate) Sample Output [ Error \u0026quot;Urls are the same\u0026quot;; Error \u0026quot;Target invalid protocol\u0026quot;; Error \u0026quot;Target is not a valid resource\u0026quot;; Ok Method: GET, RequestUri: \u0027\u0026lt;null\u0026gt;\u0027, Version: 1.1, Content: System.Net.Http.FormUrlEncodedContent, Headers: { Content-Type: application/x-www-form-urlencoded Content-Length: 67 } { Content = System.Net.Http.FormUrlEncodedContent; Headers = seq []; Method = GET; Options = seq []; Properties = seq []; RequestUri = null; Version = 1.1; VersionPolicy = RequestVersionOrLower; } ]","contentType":"snippets","url":"https://www.lqdev.me/resources/snippets/webmentions-request-verification","date":"09/23/2022 15:41 -05:00","tags":["f#","indieweb","webmentions","internet","web","social","interactive","script"],"summary":"Description Sample script that shows how to perform Webmention request verification per Webmentions specification . Usage dotnet fsi request-verification.fsx Snippet request-verification.fsx //...","keywords":["\u0026quot","target","error","source","|\u0026gt","async","me\u0026quot","request","content","check"]},{"id":"notes-webmentions-partially-implemented","title":"Webmentions (partially) implemented","content":"Success! I just partially implemented Webmentions for my website. Although I haven\u0027t figured out a good way to receive Webmentions yet, I\u0027m able to send them. Fortunately most of the work was done, as detailed in the post Sending Webmentions with F# . The rest was mainly a matter of adapting it to my static site generator. Below is an example of a post on my website being displayed in the Webmention test suite website . Source Target What does this mean? It means I can comment on any website I want, regardless of whether they allow comments or not. As if that weren\u0027t enough, I have full ownership of my content as my website is the single source of truth. As a bonus, if the website I comment on supports receiving Webmentions, my post will be displayed on their website / articles as a comment. The next step is to handle deleted comments, but so far I\u0027m happy with the progress.","contentType":"notes","url":"https://www.lqdev.me/notes/webmentions-partially-implemented","date":"09/17/2022 19:59 -05:00","tags":["indieweb","webmentions","fsharp","website"],"summary":"Success! I just partially implemented Webmentions for my website. Although I haven\u0027t figured out a good way to receive Webmentions yet, I\u0027m able to send them. Fortunately most of the work was done,...","keywords":["website","webmentions","post","comment","displayed","comments","source","articles","mean","means"]},{"id":"reviews-serviceberry-robin-wall-kimerrer","title":"The Serviceberry","content":"The Serviceberry by Robin Wall Kimerrer Read Rating: 4.8/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/serviceberry-robin-wall-kimerrer","date":"09/13/2025 14:03 -05:00","tags":[],"summary":"The Serviceberry by Robin Wall Kimerrer Read Rating: 4.8/5","keywords":["serviceberry","robin","wall","kimerrer","read","rating:"]},{"id":"reviews-hackers-heroes-computer-revolution","title":"Hackers: Heroes of the Computer Revolution","content":"Hackers: Heroes of the Computer Revolution by Steven Levy InProgress","contentType":"reviews","url":"https://www.lqdev.me/reviews/hackers-heroes-computer-revolution","date":"09/05/2022 18:19 -05:00","tags":[],"summary":"Hackers: Heroes of the Computer Revolution by Steven Levy InProgress","keywords":["hackers:","heroes","computer","revolution","steven","levy","inprogress"]},{"id":"reviews-building-a-second-brain","title":"Building a Second Brain:A Proven Method to Organize Your Digital Life and Unlock Your Creative Potential","content":"Building a Second Brain:A Proven Method to Organize Your Digital Life and Unlock Your Creative Potential by Tiago Forte Read Rating: 4.0/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/building-a-second-brain","date":"09/05/2022 18:19 -05:00","tags":[],"summary":"Building a Second Brain:A Proven Method to Organize Your Digital Life and Unlock Your Creative Potential by Tiago Forte Read Rating: 4.0/5","keywords":["building","second","brain:a","proven","method","organize","digital","life","unlock","creative"]},{"id":"reviews-ten-arguments-delete-social-media","title":"Ten arguments for deleting your social media accounts right now","content":"Ten arguments for deleting your social media accounts right now by Jaron Lanier Read Rating: 3.2/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/ten-arguments-delete-social-media","date":"09/05/2022 16:52 -05:00","tags":[],"summary":"Ten arguments for deleting your social media accounts right now by Jaron Lanier Read Rating: 3.2/5","keywords":["arguments","deleting","social","media","accounts","jaron","lanier","read","rating:"]},{"id":"reviews-stand-by-for-mars","title":"Stand by for Mars!","content":"Stand by for Mars! by Carey Rockwell Read Rating: 3.8/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/stand-by-for-mars","date":"09/05/2022 16:52 -05:00","tags":[],"summary":"Stand by for Mars! by Carey Rockwell Read Rating: 3.8/5","keywords":["stand","mars","carey","rockwell","read","rating:"]},{"id":"wiki-commonplace-book","title":"Commonplace Book","content":"Overview A way to store notes and knowledge discovered and learned. This can include quotes from books, articles, courses, and any other sources of information. References Commonplace Book Taking Note: How To Create a Commonplace with Evernote What is a commonplace book? The Glass Box and The Commonplace Book","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/commonplace-book","date":"09/04/2022 20:25 -05:00","tags":["knowledgemanagement","commonplace","notetaking","information","technology","internet","books"],"summary":"Overview A way to store notes and knowledge discovered and learned. This can include quotes from books, articles, courses, and any other sources of information. References Commonplace Book Taking...","keywords":["commonplace","book","overview","create","note:","taking","references","information","sources","courses"]},{"id":"notes-pray-ohio-state-football-downfall-nd-tweet","title":"Praying for OSU downfall","content":"Same... \uD83D\uDE4F Praying on Ohio State\u0027s downfall \u2014 Barstool Penn State ( @PSUBarstool ) September 3, 2022 Source: https://twitter.com/PSUBarstool/status/1566230451378950144","contentType":"notes","url":"https://www.lqdev.me/notes/pray-ohio-state-football-downfall-nd-tweet","date":"09/03/2022 20:09 -05:00","tags":["psu","osu","pennstate","ohiostate","ncaafb","football","ncaa","collegefootball"],"summary":"Same... \uD83D\uDE4F Praying on Ohio State\u0027s downfall \u2014 Barstool Penn State ( @PSUBarstool ) September 3, 2022 Source: https://twitter.com/PSUBarstool/status/1566230451378950144","keywords":["state","same","praying","ohio","downfall","barstool","penn","@psubarstool","september","2022"]},{"id":"wiki-markdig-advanced-extensions","title":"Markdig Advanced Extensions","content":"Overview Markdig advanced extensions are a series of CommonMark compliant syntax extensions for the markdown language. Task lists Tasks lists allow you to create unorder list items with checkboxes. Syntax - [ ] Item1 - [x] Item2 - [ ] Item3 Output Item1 Item2 Item3 Tables Pipe tables Syntax | a | b | c | - | - | - | 1 | 2 | 3 Output a b c 1 2 3 Diagrams Support for diagram syntax such as Mermaid . Syntax \u0060\u0060\u0060mermaid flowchart LR Start --\u0026gt; Stop \u0060\u0060\u0060 Output flowchart LR; Start --\u0026gt; Stop; Emojis Support for emojis. Syntax We all need :), it makes us :muscle:. (and :ok_hand:). Output We all need \uD83D\uDE03, it makes us \uD83D\uDCAA. (and \uD83D\uDC4C). Resources Markdig Specs","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/markdig-advanced-extensions","date":"09/03/2022 19:26 -05:00","tags":["markdown","markdig","dotnet","fsharp","csharp","website"],"summary":"Overview Markdig advanced extensions are a series of CommonMark compliant syntax extensions for the markdown language. Task lists Tasks lists allow you to create unorder list items with checkboxes....","keywords":["syntax","output","item2","tables","support","flowchart","item3","start","stop","lists"]},{"id":"wiki-unicode-emojis","title":"Unicode Emojis","content":"Overview Unicode emoji characters and sequences. Usage Unicode \u0026lt;span\u0026gt;\u0026amp;#x1F642;\u0026lt;/span\u0026gt; \uD83D\uDE42 CLDR Name I am :slightly_smiling_face: I am \uD83D\uDE42 Emoji List Emoji Unicode CLDR Name \uD83D\uDE42 U\u002B1F642 :slightly_smiling_face: \uD83D\uDE02 U\u002B1F602 :joy: \uD83D\uDE43 U\u002B1F643 :upside_down_face: \uD83D\uDE11 U\u002B1F611 :expressionless: \uD83D\uDE44 U\u002B1F644 :roll_eyes: \uD83E\uDD2E U\u002B1F92E :vomiting_face: \uD83E\uDD2F U\u002B1F92F :exploding_head: \uD83D\uDE15 U\u002B1F615 :confused: \uD83D\uDE41 U\u002B1F641 :slightly_frowning_face: \uD83D\uDE2E U\u002B1F62E :open_mouth: \uD83D\uDE21 U\u002B1F621 :rage: \uD83D\uDC7B U\u002B1F47B :ghost: \uD83D\uDC7D U\u002B1F47D :alien: \uD83E\uDD16 U\u002B1F916 :robot: \u2764\uFE0F U\u002B2764 :heart: :bison: U\u002B1F9AC :bison: :bagel: U\u002B1F96F :bagel: \uD83C\uDF7F U\u002B1F37F :popcorn: \uD83C\uDF7A U\u002B1F37A :beer: \uD83C\uDFDC\uFE0F U\u002B1F3DC :desert: \uD83D\uDE80 U\u002B1F680 :rocket: \uD83D\uDEF8 U\u002B1F6F8 :flying_saucer: \u23F0 U\u002B23F0 :alarm_clock: \uD83C\uDF0C U\u002B1F30C :milky_way: \uD83D\uDD25 U\u002B1F525 :fire: \uD83C\uDF83 U\u002B1F383 :jack_o_lantern: \uD83C\uDFC8 U\u002B1F3C8 :football: \uD83C\uDFA7 U\u002B1F3A7 :headphones: \uD83D\uDCFB U\u002B1F4FB :radio: \uD83D\uDCBB U\u002B1F4BB :computer: \uD83D\uDCD6 U\u002B1F4D6 :book: \uD83D\uDCD3 U\u002B1F4D3 :notebook: \uD83D\uDD16 U\u002B1F516 :bookmark: \uD83D\uDCF0 U\u002B1F4F0 :newspaper: \uD83C\uDFF7\uFE0F U\u002B1F3F7 :label: \u2709\uFE0F U\u002B2709 :envelope: \u270F\uFE0F U\u002B270F :pencil2: \uD83D\uDCDD U\u002B1F4DD :memo: \uD83D\uDCCD U\u002B1F4CD :round_pushpin: \uD83D\uDD12 U\u002B1F512 :lock: \uD83D\uDD10 U\u002B1F510 :closed_lock_with_key: \uD83D\uDD17 U\u002B1F517 :link: \uD83D\uDE4B\u200D\u2642\uFE0F U\u002B1F64B :raising_hand_man: References Full Emojis List, v14.0","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/unicode-emojis","date":"09/03/2022 19:26 -05:00","tags":["technology","web","unicode","internet","opensource","emojis"],"summary":"Overview Unicode emoji characters and sequences. Usage Unicode \u0026lt;span\u0026gt;\u0026amp;#x1F642;\u0026lt;/span\u0026gt; \uD83D\uDE42 CLDR Name I am :slightly_smiling_face: I am \uD83D\uDE42 Emoji List Emoji Unicode CLDR Name \uD83D\uDE42 U\u002B1F642...","keywords":["face:","unicode","emoji",":slightly","cldr",":bagel:","list","smiling",":bison:",":headphones:"]},{"id":"notes-til-vs-code-live-preview-extension","title":"TIL: VS Code Live Preview Extension","content":"Today I Learned about the VS Code Live Preview extension . While reading the release notes for the latest version of VS Code, I noticed a note on the Live Preview extension. Usually to preview static content, I use Python\u0027s HTTP server with the python -m http.server command. This extension simplifies the process because it\u0027s built into Visual Studio Code. If you do any kind of web development, it\u0027s definitely worth a look.","contentType":"notes","url":"https://www.lqdev.me/notes/til-vs-code-live-preview-extension","date":"09/03/2022 18:47 -05:00","tags":["til","vscode","vs","extension","development","web"],"summary":"Today I Learned about the VS Code Live Preview extension . While reading the release notes for the latest version of VS Code, I noticed a note on the Live Preview extension. Usually to preview static...","keywords":["code","preview","extension","python","server","live","http","command","simplifies","process"]},{"id":"notes-psu-survives-purdue","title":"1-0 Penn State survives Purdue","content":"College football is definitely back. I don\u0027t miss those 4th quarter heart attacks, but a win is a win.","contentType":"notes","url":"https://www.lqdev.me/notes/psu-survives-purdue","date":"09/01/2022 23:18 -05:00","tags":["pennstate","purdue","football","college","ncaafb","ncaa","collegefootball"],"summary":"College football is definitely back. I don\u0027t miss those 4th quarter heart attacks, but a win is a win.","keywords":["college","football","definitely","back","miss","those","quarter","heart","attacks"]},{"id":"notes-qrz-new-ham-jumstart-program","title":"QRZ New Ham Jumpstart Program","content":"Very cool! QRZ just announced the New Ham Jumpstart Program , a new program to make it easier for new ham radio operators to quickly get on the air.","contentType":"notes","url":"https://www.lqdev.me/notes/qrz-new-ham-jumstart-program","date":"08/31/2022 21:20 -05:00","tags":["radio","qrz","hamradio","fcc"],"summary":"Very cool! QRZ just announced the New Ham Jumpstart Program , a new program to make it easier for new ham radio operators to quickly get on the air.","keywords":["program","cool","announced","jumpstart","easier","radio","operators","quickly"]},{"id":"snippets-update-matrix-synapse-homeserver","title":"Upgrade Matrix Synapse homeserver","content":"Description Upgrade a Matrix Synapse homeserver using pip. For more information, see the official article on upgrading between Synapse versions . Usage ./update-matrix-homeserver.sh Snippet update-matrix-homeserver.sh # Initialize Python virtual environment source ./env/bin/activate # Upgrade using pip # For PostgreSQL packages, use matrix-synapse[postgres] pip install --upgrade matrix-synapse # Restart server synctl restart # Check version curl http://localhost:8008/_synapse/admin/v1/server_version # Deactivate Python virtual environment deactivate","contentType":"snippets","url":"https://www.lqdev.me/resources/snippets/update-matrix-synapse-homeserver","date":"08/31/2022 19:40 -05:00","tags":["matrix","synapse","homeserver","selfhost","python","internet","network"],"summary":"Description Upgrade a Matrix Synapse homeserver using pip. For more information, see the official article on upgrading between Synapse versions . Usage ./update-matrix-homeserver.sh Snippet...","keywords":["matrix","synapse","upgrade","homeserver","python","version","restart","environment","virtual","deactivate"]},{"id":"snippets-create-matrix-user-cli","title":"Create a new Matrix user using the CLI","content":"Description Create a new user in your Matrix Synapse homeserver using register_new_matrix_user CLI utility. This is helpful when creating accounts on a homeserver where registrations are closed. Usage N/A Snippet register_new_matrix_user --user user1 --password p@ssword --config homeserver-config.yaml --user : Local part of the new user. Will prompt if omitted. --password : New password for user. Will prompt if omitted. Supplying the password on the command line is not recommended. Use the STDIN instead. --config : Path to server config file containing the shared secret.","contentType":"snippets","url":"https://www.lqdev.me/resources/snippets/create-matrix-user-cli","date":"08/31/2022 19:40 -05:00","tags":["bash","linux","matrix","synapse","communication"],"summary":"Description Create a new user in your Matrix Synapse homeserver using register_new_matrix_user CLI utility. This is helpful when creating accounts on a homeserver where registrations are closed....","keywords":["user","password","config","matrix","homeserver","prompt","register","omitted","description","part"]},{"id":"notes-open-ai-dall-e-outpainting-generative-models","title":"Dall-E Outpainting and generative models","content":"Things are heating up in the AI generative model space. OpenAI just announced Outpainting . A few days ago, Stability.ai released Stable Diffusion and recently a painting generated using Midjourney won first place in the digital category of the Colorado State Fair fine art competition.","contentType":"notes","url":"https://www.lqdev.me/notes/open-ai-dall-e-outpainting-generative-models","date":"08/31/2022 17:49 -05:00","tags":["dalle","openai","genai","ai"],"summary":"Things are heating up in the AI generative model space. OpenAI just announced Outpainting . A few days ago, Stability.ai released Stable Diffusion and recently a painting generated using Midjourney...","keywords":["things","fair","state","colorado","category","digital","place","midjourney","using","generated"]},{"id":"reviews-four-agreements-ruiz","title":"The Four Agreements","content":"The Four Agreements by Don Miguel Ruiz Read Rating: 4.4/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/four-agreements-ruiz","date":"08/30/2025 19:08 -05:00","tags":[],"summary":"The Four Agreements by Don Miguel Ruiz Read Rating: 4.4/5","keywords":["four","agreements","miguel","ruiz","read","rating:"]},{"id":"snippets-fsharp-data-rss-parser","title":"Analyze RSS feeds with FSharp.Data XML Type Provider","content":"Description Use the FSharp.Data XML Type Provider to load and analyze RSS feeds. Usage dotnet fsi rss-parser.fsx Snippet rss-parser.fsx // Install NuGet packages #r \u0026quot;nuget:FSharp.Data\u0026quot; // Import NuGet packages open System.Xml.Linq open FSharp.Data // Define Rss type using XML Type Provider type Rss = XmlProvider\u0026lt;\u0026quot;http://luisquintanilla.me/posts/index.xml\u0026quot;\u0026gt; // Load RSS feed using Rss type let blogFeed = Rss.Load(\u0026quot;http://luisquintanilla.me/posts/index.xml\u0026quot;) // Get Feed Title blogFeed.Channel.Title // Get the 5 latest posts blogFeed.Channel.Items |\u0026gt; Array.sortByDescending(fun item -\u0026gt; item.PubDate) |\u0026gt; Array.take 5 // Get the title and URL of 5 latest posts blogFeed.Channel.Items |\u0026gt; Array.sortByDescending(fun item -\u0026gt; item.PubDate) |\u0026gt; Array.take 5 |\u0026gt; Array.map(fun item -\u0026gt; {|Title=item.Title;Url=item.Link|})","contentType":"snippets","url":"https://www.lqdev.me/resources/snippets/fsharp-data-rss-parser","date":"08/30/2022 12:50 -05:00","tags":["xml","dotnet","fsharp","typeprovider","rss"],"summary":"Description Use the FSharp.Data XML Type Provider to load and analyze RSS feeds. Usage dotnet fsi rss-parser.fsx Snippet rss-parser.fsx // Install NuGet packages #r \u0026quot;nuget:FSharp.Data\u0026quot; //...","keywords":["type","item","array","|\u0026gt","blogfeed","title","load","channel","\u0026quot","nuget"]},{"id":"wiki-machine-learning","title":"Machine Learning","content":"Overview Machine learning is a subset of artificial intelligence. Resources Frameworks References LF AI \u0026amp; Data Foundation Interactive Landscape","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/machine-learning","date":"08/26/2022 08:15 -05:00","tags":["technology","machinelearning","artificialintelligence","index"],"summary":"Overview Machine learning is a subset of artificial intelligence. Resources Frameworks References LF AI \u0026amp; Data Foundation Interactive Landscape","keywords":["overview","machine","learning","subset","artificial","intelligence","resources","frameworks","references","\u0026amp"]},{"id":"wiki-ml-frameworks","title":"Machine Learning Frameworks","content":"Overview Machine learning frameworks provide APIs for training and consuming machine learning models. Frameworks ML.NET Scikit-Learn TensorFlow Keras PyTorch Fast.AI ONNX HuggingFace","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/ml-frameworks","date":"08/26/2022 08:10 -05:00","tags":["technology","artificialintelligence","machinelearning","software"],"summary":"Overview Machine learning frameworks provide APIs for training and consuming machine learning models. Frameworks ML.NET Scikit-Learn TensorFlow Keras PyTorch Fast.AI ONNX HuggingFace","keywords":["machine","learning","frameworks","overview","fast","pytorch","keras","tensorflow","learn","models"]},{"id":"wiki-privacy-guides","title":"Privacy Guides","content":"Overview Privacy guides contain tips, information, and recommendations on physical and digital privacy. Resources Techlore Privacy Guides Go Incognito Privacy.do PRISM Break Decentralize Today - Privacy Cookbook","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/privacy-guides","date":"08/25/2022 21:30 -05:00","tags":["technology","privacy"],"summary":"Overview Privacy guides contain tips, information, and recommendations on physical and digital privacy. Resources Techlore Privacy Guides Go Incognito Privacy.do PRISM Break Decentralize Today -...","keywords":["privacy","guides","overview","decentralize","break","prism","incognito","techlore","resources","digital"]},{"id":"wiki-neofetch","title":"Neofetch","content":"Overview Neofetch is a command-line system information tool. Neofetch displays information about your operating system, software and hardware. Install using Pacman Open the terminal Run the following command pacman -S neofetch Configure The configuration file for neofetch is located at $HOME/.config/neofetch/config.conf For more information on configuration options, see the Customizing info article . Run Open the terminal and enter the following command: neofetch Run when terminal launches Open your ~/.bashrc file in your preferred text editor. Add the following line to your file and save it. neofetch Refresh your terminal with the following command. source ~/.bashrc References Neofetch GitHub Repository","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/neofetch","date":"08/25/2022 15:19 -05:00","tags":["technology","linux","tools","terminal"],"summary":"Overview Neofetch is a command-line system information tool. Neofetch displays information about your operating system, software and hardware. Install using Pacman Open the terminal Run the following...","keywords":["neofetch","following","terminal","command","information","open","file","line","system","pacman"]},{"id":"wiki-privacy","title":"Privacy","content":"Overview Privacy is the right of individuals to selectively disclose and share information about themselves. Resources Privacy Guides Alternative Frontends Podcasts Privacy, Security \u0026amp; OSINT Show Surveillance Report YouTube Channels Techlore","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/privacy","date":"08/25/2022 15:18 -05:00","tags":["technology","privacy","index"],"summary":"Overview Privacy is the right of individuals to selectively disclose and share information about themselves. Resources Privacy Guides Alternative Frontends Podcasts Privacy, Security \u0026amp; OSINT Show...","keywords":["privacy","overview","youtube","report","surveillance","show","osint","\u0026amp","security","podcasts"]},{"id":"wiki-wiki","title":"Wiki","content":"Overview A wiki is a publication on subjects related to a particular topic. It often serves as a knowledge base. Examples Wikipedia IndieWeb Wiki Arch Wiki References Massive Wiki IndieWeb Wikipage The Garden and the Stream: A Technopastoral List of Wikis","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/wiki","date":"08/25/2022 15:13 -05:00","tags":["education","information","wiki","index"],"summary":"Overview A wiki is a publication on subjects related to a particular topic. It often serves as a knowledge base. Examples Wikipedia IndieWeb Wiki Arch Wiki References Massive Wiki IndieWeb Wikipage...","keywords":["wiki","indieweb","overview","technopastoral","stream:","garden","wikipage","massive","references","arch"]},{"id":"wiki-alternative-frontends","title":"Alternative Frontends","content":"Overview Alternative frontends are third-party frontends that display information from web services like social media. These frontends are generally open-source and privacy-respecting. Resources Nitter (Twitter) Invidious (YouTube) Teddit (Reddit)","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/alternative-frontends","date":"08/25/2022 15:05 -05:00","tags":["technology","internet","socialmedia","opensource","privacy"],"summary":"Overview Alternative frontends are third-party frontends that display information from web services like social media. These frontends are generally open-source and privacy-respecting. Resources...","keywords":["frontends","overview","youtube","invidious","twitter","nitter","resources","respecting","privacy","source"]},{"id":"snippets-nixos-configuration","title":"NixOS Configuration","content":"Description This is my NixOS Configuration file Usage Update configuration file Run the following command to apply changes sudo nixos-rebuild switch Snippet # Edit this configuration file to define what should be installed on # your system. Help is available in the configuration.nix(5) man page # and in the NixOS manual (accessible by running \u2018nixos-help\u2019). { config, pkgs, ... }: { imports = [ # Include the results of the hardware scan. ./hardware-configuration.nix ]; # Bootloader. boot.loader.systemd-boot.enable = true; boot.loader.efi.canTouchEfiVariables = true; networking.hostName = \u0026quot;nixos\u0026quot;; # Define your hostname. # networking.wireless.enable = true; # Enables wireless support via wpa_supplicant. # Configure network proxy if necessary # networking.proxy.default = \u0026quot;http://user:password@proxy:port/\u0026quot;; # networking.proxy.noProxy = \u0026quot;127.0.0.1,localhost,internal.domain\u0026quot;; # Enable networking networking.networkmanager.enable = true; # Set your time zone. time.timeZone = \u0026quot;America/New_York\u0026quot;; # Select internationalisation properties. i18n.defaultLocale = \u0026quot;en_US.UTF-8\u0026quot;; i18n.extraLocaleSettings = { LC_ADDRESS = \u0026quot;en_US.UTF-8\u0026quot;; LC_IDENTIFICATION = \u0026quot;en_US.UTF-8\u0026quot;; LC_MEASUREMENT = \u0026quot;en_US.UTF-8\u0026quot;; LC_MONETARY = \u0026quot;en_US.UTF-8\u0026quot;; LC_NAME = \u0026quot;en_US.UTF-8\u0026quot;; LC_NUMERIC = \u0026quot;en_US.UTF-8\u0026quot;; LC_PAPER = \u0026quot;en_US.UTF-8\u0026quot;; LC_TELEPHONE = \u0026quot;en_US.UTF-8\u0026quot;; LC_TIME = \u0026quot;en_US.UTF-8\u0026quot;; }; # Enable the X11 windowing system. # services.xserver.enable = true; services.xserver = { enable = true; desktopManager = { xterm.enable = false; }; displayManager = { defaultSession = \u0026quot;none\u002Bi3\u0026quot;; }; windowManager.i3 = { enable = true; extraPackages = with pkgs; [ dmenu i3status j4-dmenu-desktop i3lock ]; }; }; # Enable the XFCE Desktop Environment. # services.xserver.displayManager.lightdm.enable = true; # services.xserver.desktopManager.xfce.enable = true; # Configure keymap in X11 services.xserver = { layout = \u0026quot;us\u0026quot;; xkbVariant = \u0026quot;\u0026quot;; }; # Enable CUPS to print documents. services.printing.enable = true; # Enable sound with pipewire. sound.enable = true; hardware.pulseaudio.enable = false; security.rtkit.enable = true; services.pipewire = { enable = true; alsa.enable = true; alsa.support32Bit = true; pulse.enable = true; # If you want to use JACK applications, uncomment this #jack.enable = true; # use the example session manager (no others are packaged yet so this is enabled by default, # no need to redefine it in your config for now) #media-session.enable = true; }; # Enable touchpad support (enabled default in most desktopManager). # services.xserver.libinput.enable = true; # Enable gvfs services.gvfs.enable = true; # Define a user account. Don\u0027t forget to set a password with \u2018passwd\u2019. users.users.lqdev = { isNormalUser = true; description = \u0026quot;lqdev\u0026quot;; extraGroups = [ \u0026quot;networkmanager\u0026quot; \u0026quot;wheel\u0026quot; \u0026quot;docker\u0026quot;]; packages = with pkgs; [ firefox thunderbird vscode element-desktop ]; }; #Enable docker virtualisation.docker.enable = true; # Allow unfree packages nixpkgs.config.allowUnfree = true; # List packages installed in system profile. To search, run: # $ nix search wget environment.systemPackages = with pkgs; [ # vim # Do not forget to add an editor to edit configuration.nix! The Nano editor is also installed by default. # Utilities wget emacs alacritty mc du-dust htop feh duf shutter gparted keepassxc bitwarden git yt-dlp streamlink ffmpeg gnome.seahorse xfce.thunar-volman xfce.xfconf # Media mpv vlc # Dotnet (with dotnetCorePackages; combinePackages [ dotnet-sdk dotnet-sdk_7 ]) # Development docker cargo rustc # Office libreoffice ]; # Some programs need SUID wrappers, can be configured further or are # started in user sessions. # programs.mtr.enable = true; # programs.gnupg.agent = { # enable = true; # enableSSHSupport = true; # }; # Bash Configuration programs.bash = { shellAliases = { emacs=\u0026quot;emacs -nw\u0026quot;; }; }; # Thunar programs.thunar.enable = true; # List services that you want to enable: # Enable the OpenSSH daemon. # services.openssh.enable = true; # Open ports in the firewall. # networking.firewall.allowedTCPPorts = [ ... ]; # networking.firewall.allowedUDPPorts = [ ... ]; # Or disable the firewall altogether. # networking.firewall.enable = false; # This value determines the NixOS release from which the default # settings for stateful data, like file locations and database versions # on your system were taken. It\u2018s perfectly fine and recommended to leave # this value at the release version of the first install of this system. # Before changing this value read the documentation for this option # (e.g. man configuration.nix or on https://nixos.org/nixos/options.html). system.stateVersion = \u0026quot;23.05\u0026quot;; # Did you read the comment? }","contentType":"snippets","url":"https://www.lqdev.me/resources/snippets/nixos-configuration","date":"08/22/2023 18:45 -05:00","tags":["nix","linux","configuration","nixos","os"],"summary":"Description This is my NixOS Configuration file Usage Update configuration file Run the following command to apply changes sudo nixos-rebuild switch Snippet # Edit this configuration file to define...","keywords":["enable","true","\u0026quot","services","8\u0026quot","networking","configuration","system","xserver","default"]},{"id":"snippets-opml-file-generator","title":"OPML File Generator","content":"Description Script to take information stored in a JSON file and converts it into OPML format. This works for RSS readers as well as podcast clients that support OPML import. Usage dotnet fsi opml-generator.fsx \u0026quot;\u0026lt;feed-title\u0026gt;\u0026quot; \u0026quot;\u0026lt;author-url\u0026gt;\u0026quot; \u0026quot;\u0026lt;data-file-path\u0026gt;\u0026quot; dotnet fsi opml-generator.fsx \u0026quot;My Blogroll\u0026quot; \u0026quot;http://lqdev.me\u0026quot; \u0026quot;blogroll.json\u0026quot; Snippet opml-generator.fsx open System.IO open System.Linq open System.Text.Json open System.Xml.Linq type OpmlMetadata = { Title: string OwnerId: string } type Outline = { Title: string Type: string HtmlUrl: string XmlUrl: string } let opmlFeed (head:XElement) = XElement(XName.Get \u0026quot;opml\u0026quot;, XAttribute(XName.Get \u0026quot;version\u0026quot;, \u0026quot;2.0\u0026quot;), head, XElement(XName.Get \u0026quot;body\u0026quot;)) let headElement (metadata:OpmlMetadata) = XElement(XName.Get \u0026quot;head\u0026quot;, XElement(XName.Get \u0026quot;title\u0026quot;, metadata.Title), XElement(XName.Get \u0026quot;ownerId\u0026quot;, metadata.OwnerId)) let outlineElement (data:Outline) = XElement(XName.Get \u0026quot;outline\u0026quot;, XAttribute(XName.Get \u0026quot;title\u0026quot;, data.Title), XAttribute(XName.Get \u0026quot;text\u0026quot;, data.Title), XAttribute(XName.Get \u0026quot;type\u0026quot;, data.Type), XAttribute(XName.Get \u0026quot;htmlUrl\u0026quot;, data.HtmlUrl), XAttribute(XName.Get \u0026quot;xmlUrl\u0026quot;, data.XmlUrl)) let loadLinks (filePath:string) = File.ReadAllText(filePath) |\u0026gt; fun x -\u0026gt; x |\u0026gt; JsonSerializer.Deserialize\u0026lt;Outline array\u0026gt; let buildOpmlFeed (title:string) (ownerId:string) (filePath:string) = let fileName = Path.GetFileNameWithoutExtension(filePath) let head = { Title=title OwnerId=ownerId } |\u0026gt; headElement let links = filePath |\u0026gt; loadLinks |\u0026gt; Array.map(outlineElement) let feed = opmlFeed head feed.Descendants(XName.Get \u0026quot;body\u0026quot;).First().Add(links) File.WriteAllText($\u0026quot;{fileName}.opml\u0026quot;, feed.ToString()) let args = fsi.CommandLineArgs let title = args[1] let ownerId = args.[2] let dataPath = args.[3] buildOpmlFeed title ownerId dataPath blogroll.json [ { \u0026quot;Title\u0026quot;: \u0026quot;Blogroll.org\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;rss\u0026quot;, \u0026quot;HtmlUrl\u0026quot;: \u0026quot;https://blogroll.org/\u0026quot;, \u0026quot;XmlUrl\u0026quot;: \u0026quot;https://blogroll.org/feed/\u0026quot; }, { \u0026quot;Title\u0026quot;: \u0026quot;Cheapskate\u0027s Guide\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;rss\u0026quot;, \u0026quot;HtmlUrl\u0026quot;: \u0026quot;https://cheapskatesguide.org/\u0026quot;, \u0026quot;XmlUrl\u0026quot;: \u0026quot;https://cheapskatesguide.org/cheapskates-guide-rss-feed.xml\u0026quot; }, { \u0026quot;Title\u0026quot;: \u0026quot;JWZ\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;rss\u0026quot;, \u0026quot;HtmlUrl\u0026quot;: \u0026quot;https://www.jwz.org/blog/\u0026quot;, \u0026quot;XmlUrl\u0026quot;: \u0026quot;https://cdn.jwz.org/blog/feed/\u0026quot; } ]","contentType":"snippets","url":"https://www.lqdev.me/resources/snippets/opml-file-generator","date":"08/14/2022 13:45 -05:00","tags":["dotnet","fsharp","script"],"summary":"Description Script to take information stored in a JSON file and converts it into OPML format. This works for RSS readers as well as podcast clients that support OPML import. Usage dotnet fsi...","keywords":["\u0026quot","xname","xelement","data","xattribute","string","opml","feed","title","title\u0026quot"]},{"id":"snippets-curl-download-wikipedia-mainpage","title":"Download Main Wikipedia Page as PDF","content":"Description Download the main page of Wikipedia locally as a PDF Usage curl -o Main_Page.pdf https://en.wikipedia.org/api/rest_v1/page/pdf/Main_Page Snippet script.sh curl -o Main_Page.pdf https://en.wikipedia.org/api/rest_v1/page/pdf/Main_Page","contentType":"snippets","url":"https://www.lqdev.me/resources/snippets/curl-download-wikipedia-mainpage","date":"08/11/2022 19:19 -05:00","tags":["bash","linux"],"summary":"Description Download the main page of Wikipedia locally as a PDF Usage curl -o Main_Page.pdf https://en.wikipedia.org/api/rest_v1/page/pdf/Main_Page Snippet script.sh curl -o Main_Page.pdf...","keywords":["page","main","wikipedia","curl","https://en","org/api/rest","v1/page/pdf/main","description","download","locally"]},{"id":"snippets-remove-installed-pip-packages","title":"Remove all installed Python packages with pip","content":"Description I recently had the need to get rid of all the packages I\u0027d installed due to conflicting dependencies. Usage To uninstall packages you need to: Get a list of the packages Uninstall them This works both for virtual environments as well as system-wide installations. Get all packages pip freeze \u0026gt; requirements.txt Uninstall packages pip uninstall -r requirements.txt -y Snippet N/A","contentType":"snippets","url":"https://www.lqdev.me/resources/snippets/remove-installed-pip-packages","date":"08/08/2024 10:26 -05:00","tags":["python","pip","packages"],"summary":"Description I recently had the need to get rid of all the packages I\u0027d installed due to conflicting dependencies. Usage To uninstall packages you need to: Get a list of the packages Uninstall them...","keywords":["packages","uninstall","need","requirements","description","freeze","installations","wide","system","environments"]},{"id":"snippets-qr-code-generator","title":"QR Code Generator","content":"Description Script to generate a QR Code and save as PNG image from a URL Usage dotnet fsi qr-code-generator.fsx \u0026quot;my-qr-code.svg\u0026quot; \u0026quot;https://twitter.com/user-profile\u0026quot; Snippet qr-code-generator.fsx open System.Threading printfn \u0026quot;Loading packages\u0026quot; #r \u0026quot;nuget:Net.Codecrete.QrCodeGenerator\u0026quot; Thread.Sleep(5000) printfn \u0026quot;Loaded QrCodeGenerator\u0026quot; open Net.Codecrete.QrCodeGenerator open System.IO open System.Text let createQrCode (savePath:string) (target:string) = let qr = QrCode.EncodeText(target,QrCode.Ecc.High) let svgString = qr.ToSvgString(4) File.WriteAllText(savePath,svgString, Encoding.UTF8) let args = fsi.CommandLineArgs let savePath = args.[1] let target = args.[2] createQrCode savePath target","contentType":"snippets","url":"https://www.lqdev.me/resources/snippets/qr-code-generator","date":"08/03/2022 20:07 -05:00","tags":["dotnet","f#","script"],"summary":"Description Script to generate a QR Code and save as PNG image from a URL Usage dotnet fsi qr-code-generator.fsx \u0026quot;my-qr-code.svg\u0026quot; \u0026quot;https://twitter.com/user-profile\u0026quot; Snippet...","keywords":["\u0026quot","open","code","args","savepath","target","system","qrcode","printfn","svgstring"]},{"id":"reviews-project-hail-mary","title":"Project Hail Mary","content":"Project Hail Mary by Andy Weir Read Rating: 5.0/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/project-hail-mary","date":"08/01/2024 20:12 -05:00","tags":[],"summary":"Project Hail Mary by Andy Weir Read Rating: 5.0/5","keywords":["project","hail","mary","andy","weir","read","rating:"]},{"id":"posts-static-website-redirects","title":"Site redirects in static websites","content":"## Introduction I recently came across the concept of [owning your links](https://indieweb.org/own_your_links) on the IndieWeb wiki and right away it made lots of sense to me. The main idea behind owning your links is using your domain as the source of truth and using it to redirect to your content on various other platforms you own. For example, when sharing my Twitter profile, instead of saying, \u0022Go to https://twitter.com/ljquintanilla\u0022, I can instead point people to [http://lqdev.me/twitter](http://lqdev.me/twitter)\u0022. The link on my domain ultimately redirects to Twitter, but the main entrypoint is my website. There\u0027s various ways to handle redirects, but in most cases it\u0027s done on the server side. For static websites, if you\u0027re hosting them in GitHub Pages, Netlify, or Azure Blob Storage, you don\u0027t have a server. Therefore configuring redirects can be a challenge. In this post, I\u0027ll go through how you can configure redirects for your static website without a server or JavaScript. ## Create redirect page Depending on which generator you use to create your static website, how you create your redirect pages will differ. However, the end result should look like the the following: \u0060\u0060\u0060html \u0060\u0060\u0060 The only thing you need to perform the redirect is add a \u0060meta\u0060 tag to the endpoint you want to handle the redirect. If I wanted to redirect to Twitter, I might create an HTML page at \u0060http://lqdev.me/twitter\u0060 on my website. Setting the \u0060http-equiv\u0060 attribute to \u0060refresh\u0060 instructs the browser to refresh the page. Then, you use the \u0060content\u0060 attribute to define your redirect target and how long you want to wait before redirecting the user. In the snippet above, the number \u00600\u0060 indicates you want to wait 0 seconds before redirecting your users and the \u0060REDIRECT-TARGET-URL\u0060 is where you want to redirect to. This could be your Twitter profile, YouTube channel, or anywhere else. Wherever you want to redirect to, you place it in the \u0060content\u0060 attribute. It\u0027s important to note that the redirect happens automatically which takes control away from the user causing potential accesibility issues. Until now, I haven\u0027t found a better way of doing this that doesn\u0027t require JavaScript. For more information, see the [web content accessibility guidelines](https://www.w3.org/WAI/standards-guidelines/wcag/). For more information on creating redirects in HTML, see the article [how to redirect a web page in HTML](https://www.w3docs.com/snippets/html/how-to-redirect-a-web-page-in-html.html). ## Conclusion So far I\u0027ve created redirect pages for most of my social media profiles. You can see samples in my [contact page](/contact/). It\u0027s important to note that while this works for social profiles and any other properties you own on the internet, it could also be used in cases where you\u0027ve changed the structure of your website and want to gracefully handle redirects for broken / updated links. Happy coding!","contentType":"posts","url":"https://www.lqdev.me/posts/static-website-redirects","date":"07/31/2022 14:52 -05:00","tags":["http","web","staticweb","indieweb","html"],"summary":"## Introduction I recently came across the concept of [owning your links](https://indieweb.org/own_your_links) on the IndieWeb wiki and right away it made lots of sense to me. The main idea behind...","keywords":["redirect","want","page","redirects","html","website","create","links","twitter","attribute"]},{"id":"posts-manage-reading-list-org-mode-emacs","title":"Manage your reading list in Emacs using Org mode","content":"## Introduction Emacs is a rich and customizable text editor. [Org mode](https://orgmode.org/) for Emacs makes it easy to manage to-do lists, projects, and author technical documents. Many of my workflows like writing, planning, and organization make heavy use of plain text. One reason I enjoy working in plain text is, it\u0027s easy for my thoughts and ideas to naturally flow without having tools get in the way. Another reason I like plain text is, I\u0027m not locked-in to any tool. Because plain text can be authored and rendered almost anywhere, it\u0027s easy for me to use whichever tool I find most productive. Some that I\u0027ve found great success with recently have been [Visual Studio Code](https://code.visualstudio.com/), [Joplin](https://joplinapp.org/), and Emacs with Org mode. I\u0027ve been trying to manage my reading list in plain text and while it\u0027s worked well, I\u0027d like a way to track the state for each book by tagging them as read or in various stages of unread. One of the features of Org mode is the ability to track workflows. In this guide, I\u0027ll show how you can extend the TODO Org mode functionality to keep track of your reading list. ## Prerequisites - [Emacs](https://www.gnu.org/software/emacs/) Since Emacs is cross-platform, the contents of this guide should work the same across Linux, Mac, and Windows. ## Create org file Org mode in Emacs provides to-do list management features. By default, the built-in states are TODO and DONE. However, you can define your own states to best match your workflow. For more information, see the [TODO item section in the Org mode guide](https://orgmode.org/manual/TODO-Items.html#TODO-Items). Start by creating a new file in Emacs with the *.org* file extension (i.e. reading-list.org). Then, add the following headers at the top of the file. \u0060\u0060\u0060text #\u002BTODO: WISHLIST(w) QUEUED(q) INPROGRESS(p) | READ(r!) \u0060\u0060\u0060 What this heading does is, it defines the different states of completion of a TODO item. The scope of this definition is the file. Although you can define global states, I want these states to only exist in the context of this file. The complete and incomplete states are separated by the pipe character (\u0060|\u0060). In this case, WISHLIST, QUEUED, and INPROGRESS are incomplete states and READ is the complete state. The letters in the parentheses are ways to quickly assign each of the states. One last thing you\u0027ll notice is the READ state in addition to having a letter also has an exclamation point. The exclamation point tells Emacs to add a timestamp whenever this book transitions to the READ state. ## Add your book list Once you\u0027ve define your reading states, it\u0027s time to add your books. TODO items in Emacs Org-mode only work in the context of headings. Therefore you have to prefix your books with asterisks. For more information on headlines, see the [Headlines section in the Org mode guide](https://orgmode.org/manual/Headlines.html). Here are examples of some books in my reading list. \u0060\u0060\u0060text * Hackers (Steven Levy) * Ghost In The Wires (Kevin Mitnick) \u0060\u0060\u0060 Since I don\u0027t intend to have sub headlines, I just used a top-level headline for each of the books. ## Assign state Now that you\u0027ve populated your list of books, it\u0027s time to assign states to them. To assign a state: 1. Place your pointer / cursor on the same line as the book you want to assign the state to. 1. Press \u0060Ctrl-c\u0060 \u002B \u0060Ctrl-t\u0060 or in Emacs notation \u0060C-c C-t\u0060. 1. A buffer opens prompting you to choose a state. Enter one of the letters to choose your state. For example, if you want to apply the QUEUED state to your book, press the letter \u0060q\u0060. The state you selected is prepended to your book. For example: \u0060\u0060\u0060text * QUEUED Hackers (Steven Levy) \u0060\u0060\u0060 ## Update state To update the state, you can follow the steps in the assign state section or you can cycle through the various states by pressing \u0060Alt-Left\u0060 or \u0060Alt-Right\u0060 or in Emacs notation \u0060M-LEFT\u0060 or \u0060M-RIGHT\u0060. ## Marking a book as READ To assign the READ state to a book, you need to update its state. The only difference between READ and the other states is, a timestamp is applied when the book transitions to the state. Here\u0027s an example of what a book in the READ state looks like: \u0060\u0060\u0060text * READ In The Beginning...Was The Command Line (Neal Stephenson) - State \u0022READ\u0022 from \u0022INPROGRESS\u0022 [2022-07-26 Tue 00:18] \u0060\u0060\u0060 ## Querying your reading list Org mode has agenda views that allow you to view TODO, timestamped, and tagged items in a single place. One of the built-in agendas that is useful for querying TODO items is the match view. The match view shows headlines based on the tags, properties, and TODO state associated with them. For more information, see the [Agenda views section in the Org-mode guide](https://orgmode.org/manual/Agenda-Views.html). Before viewing your books in an agenda view, you need to add the file containing your reading list to the list of files tracked by Org. To do so, open the file in Emacs and press \u0060Ctrl-c \u002B [\u0060 or in Emacs notation \u0060C-c [\u0060. To launch the match agenda view in org-mode: 1. Press \u0060Alt-x\u0060 or in Emacs notation \u0060M-x\u0060 1. In the minibuffer, type \u0060org-agenda\u0060 and press Enter. 1. A buffer appears prompting you to select a view. Press the letter \u0060m\u0060 to choose the match view. ### Unread items In this case, I want to see all the unfinished books which in this case are under the WISHLIST, QUEUED, and INPROGRESS states. In the match view, enter the following query into the minibuffer: \u0060\u0060\u0060text TODO=\u0022WISHLIST\u0022|TODO=\u0022QUEUED\u0022|TODO=\u0022INPROGRESS\u0022 \u0060\u0060\u0060 Press \u0060Enter\u0060. The result of the query should look similar to the following: \u0060\u0060\u0060text Headlines with TAGS match: TODO=\u0022WISHLIST\u0022|TODO=\u0022QUEUED\u0022|TODO=\u0022INPROGRESS\u0022 Press \u2018C-u r\u2019 to search again reading-list:QUEUED Hackers (Steven Levy) reading-list:WISHLIST Ghost In The Wires (Kevin Mitnick) \u0060\u0060\u0060 ### Read items To query for read items, you go through the same process, except the query is different. In the match view, enter the following query into the minibuffer: \u0060\u0060\u0060text TODO=\u0022READ\u0022 \u0060\u0060\u0060 Press \u0060Enter\u0060. The result of the query should look similar to the following: \u0060\u0060\u0060text Headlines with TAGS match: TODO=\u0022READ\u0022 Press \u2018C-u r\u2019 to search again reading-list:READ In The Beginning...Was The Command Line (Neal Stephenson) \u0060\u0060\u0060 ## Conclusion Reading lists are just one of the many things you can track using Emacs and Org mode. You can use this general process to track movies, TV shows, podcasts, etc. Thanks to the extensibility of Emacs, the options are endless!","contentType":"posts","url":"https://www.lqdev.me/posts/manage-reading-list-org-mode-emacs","date":"07/26/2022 19:15 -05:00","tags":["linux","emacs","organization"],"summary":"## Introduction Emacs is a rich and customizable text editor. [Org mode](https://orgmode.org/) for Emacs makes it easy to manage to-do lists, projects, and author technical documents. Many of my...","keywords":["state","emacs","read","states","reading","mode","list","book","press","view"]},{"id":"reviews-simple-siegel-etzkorn","title":"Simple: Conquering the Crisis of Complexity","content":"Simple: Conquering the Crisis of Complexity by Alan Siegel and Irene Etzkorn Read Rating: 3.3/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/simple-siegel-etzkorn","date":"07/14/2024 13:19 -05:00","tags":[],"summary":"Simple: Conquering the Crisis of Complexity by Alan Siegel and Irene Etzkorn Read Rating: 3.3/5","keywords":["simple:","conquering","crisis","complexity","alan","siegel","irene","etzkorn","read","rating:"]},{"id":"wiki-mastodon-server-cleanup","title":"Mastodon Server Cleanup","content":"Overview General commands for cleaning up resources on Mastodon servers Setup Stop services sudo systemctl stop mastodon-sidekiq mastodon-streaming mastodon-web Restart postgresql sudo systemctl restart postgresql Log into mastodon user sudo su - mastodon Go to live directory cd /home/mastodon/live Check media usage RAILS_ENV=production ./bin/tootctl media usage Remove media RAILS_ENV=production ./bin/tootctl media remove Remove media headers RAILS_ENV=production ./bin/tootctl media remove --prune-profiles Remove preview cards RAILS_ENV=production ./bin/tootctl preview_cards remove Restart services sudo systemctl restart mastodon-sidekiq mastodon-streaming mastodon-web Resources Using the Admin CLI","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/mastodon-server-cleanup","date":"07/14/2024 03:24 -05:00","tags":["mastodon","linux","socialmedia","sysadmin"],"summary":"Overview General commands for cleaning up resources on Mastodon servers Setup Stop services sudo systemctl stop mastodon-sidekiq mastodon-streaming mastodon-web Restart postgresql sudo systemctl...","keywords":["mastodon","media","remove","rails","env=production","/bin/tootctl","restart","sudo","systemctl","stop"]},{"id":"wiki-mastodon-server-upgrades","title":"Mastodon Server Upgrades","content":"Overview This provides a guide for upgrading specific versions. Backup Instructions These instructions backup the database and environment variables file. It does not back-up media files. Create new directory called backups/\u0026lt;DATE\u0026gt; . Copy live/.env.production to backups/\u0026lt;DATE\u0026gt; directory. Dump database pg_dump -Fc mastodon_production -f /home/mastodon/backups/\u0026lt;DATE\u0026gt;/backup.dump Versions 3.4.1 Stop services - systemctl stop mastodon-*.service Backup database and env file. Fetch tags - git fetch --tags Checkout 3.4.1 tag - git checkout v3.4.1 Install Ruby dependencies - bundle install Install JS dependencies - yarn install Migrate DB - RAILS_ENV=production bundle exec rails db:migrate Precompile Assets - RAILS_ENV=production bundle exec rails assets:precompile Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 3.4.2 Stop services - systemctl stop mastodon-*.service Backup database and env file. Fetch tags - git fetch --tags Checkout 3.4.2 tag - git checkout v3.4.2 Install Ruby dependencies - bundle install Install JS dependencies - yarn install Migrate DB - RAILS_ENV=production bundle exec rails db:migrate Precompile Assets - RAILS_ENV=production bundle exec rails assets:precompile Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 3.4.3 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch --tags Checkout 3.4.3 tag - git checkout v3.4.3 Install Ruby dependencies - bundle install Install JS dependencies - yarn install Migrate DB - RAILS_ENV=production bundle exec rails db:migrate Precompile Assets - RAILS_ENV=production bundle exec rails assets:precompile Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 3.4.4 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch --tags Checkout 3.4.4 tag - git checkout v3.4.4 Precompile Assets - RAILS_ENV=production bundle exec rails assets:precompile Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 3.4.5 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch --tags Checkout 3.4.5 tag - git checkout v3.4.5 Install Ruby dependencies - bundle install Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 3.4.6 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch --tags Checkout 3.4.6 tag - git checkout v3.4.6 Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 3.4.7 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch --tags Checkout 3.4.7 tag - git checkout v3.4.7 Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 3.5.0 Stop services - systemctl stop mastodon-*.service Backup database and env file Checkout 3.5.0 tag - git checkout v3.5.0 Update available rbenv version - git -C /home/mastodon/.rbenv/plugins/ruby-build pull Install Ruby 3.0.3 - RUBY_CONFIGURE_OPTS=--with-jemalloc rbenv install 3.0.3 Install Ruby dependencies - bundle install Install JS dependencies - yarn install Run predeployment DB migration - SKIP_POST_DEPLOYMENT_MIGRATIONS=true RAILS_ENV=production bundle exec rails db:migrate Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile Update service files - cp /home/mastodon/live/dist/mastodon-*.service /etc/systemd/system/ Reload systemd daemon - systemctl daemon-reload Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming Clear cache - RAILS_ENV=production bin/tootctl cache clear Run postdeployment DB migration - RAILS_ENV=production bundle exec rails db:migrate Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 3.5.1 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch --tags Checkout 3.5.1 tag - git checkout v3.5.1 Install Ruby dependencies - bundle install Install JS dependencies - yarn install Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 3.5.2 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch --tags Checkout 3.5.2 tag - git checkout v3.5.2 Install Ruby dependencies - bundle install Install JS dependencies - yarn install Run predeployment DB migration - SKIP_POST_DEPLOYMENT_MIGRATIONS=true RAILS_ENV=production bundle exec rails db:migrate Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming Run postdeployment DB migration - RAILS_ENV=production bundle exec rails db:migrate Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 3.5.3 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch --tags Checkout 3.5.3 tag - git checkout v3.5.3 Install Ruby dependencies - bundle install Install JS dependencies - yarn install Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 4.0.0 Stop services - systemctl stop mastodon-*.service Backup database and env file Upgade NodeJS - curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash - \u0026amp;\u0026amp; sudo apt-get install -y nodejs Install Ruby 3.0.4 - RUBY_CONFIGURE_OPTS=--with-jemalloc rbenv install 3.0.4 Fetch tags - git fetch --tags Checkout 4.0.0 tag - git checkout v4.0.0 Install Ruby dependencies - bundle install Install JS dependencies - yarn install Run predeployment DB migration - SKIP_POST_DEPLOYMENT_MIGRATIONS=true RAILS_ENV=production bundle exec rails db:migrate Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile Update service files - cp /home/mastodon/live/dist/mastodon-*.service /etc/systemd/system/ Reload systemd daemon - systemctl daemon-reload Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming Run postdeployment DB migration - RAILS_ENV=production bundle exec rails db:migrate Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 4.0.2 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch --tags Checkout 4.0.2 tag - git checkout v4.0.2 Install Ruby dependencies - bundle install Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming NOTE: Node 18 not supported yet. If you run into issues upgrading directly from 3.5.3, checkout v4.0.0 tag and the upgrade to v4.0.2 4.1.0 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch --tags Checkout 4.1.0 tag - git checkout v4.1.0 Install Ruby dependencies - bundle install Install JS dependencies - yarn install Run DB migration - RAILS_ENV=production bundle exec rails db:migrate Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 4.1.1 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.1.1 tag - git checkout v4.1.1 Install Ruby dependencies - bundle install Install JS dependencies - yarn install Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 4.1.2 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.1.3 tag - git checkout v4.1.2 Update rbenv version - git -C /home/mastodon/.rbenv/plugins/ruby-build pull Install Ruby 3.0.6 - RUBY_CONFIGURE_OPTS=--with-jemalloc rbenv install 3.0.6 Install Ruby dependencies - bundle install Install JS dependencies - yarn install Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 4.1.3 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.1.3 tag - git checkout v4.1.3 Install Ruby dependencies - bundle install Install JS dependencies - yarn install (Optional) Upgrade reverse proxy Content-Security-Policy: default-src \u0027none\u0027; form-action \u0027none\u0027 \u0026amp; X-Content-Type-Options: nosniff . More info can be found in dist/nginx.conf . Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 4.1.4 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.1.4 tag - git checkout v4.1.4 Install Ruby dependencies - bundle install Install JS dependencies - yarn install (Optional) Upgrade reverse proxy Content-Security-Policy: default-src \u0027none\u0027; form-action \u0027none\u0027 \u0026amp; X-Content-Type-Options: nosniff . More info can be found in dist/nginx.conf . Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 4.1.5 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.1.5 tag - git checkout v4.1.5 Install Ruby dependencies - bundle install Install JS dependencies - yarn install Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 4.1.6 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.1.6 tag - git checkout v4.1.6 Install Ruby dependencies - bundle install Install JS dependencies - yarn install Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 4.1.7 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.1.7 tag - git checkout v4.1.7 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 4.1.8 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.1.8 tag - git checkout v4.1.8 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 4.1.9 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.1.9 tag - git checkout v4.1.9 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming 4.2.0 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.1.9 tag - git checkout v4.2.0 Update Streaming Server Service sudo cp ~mastodon/live/dist/mastodon-streaming*.service /etc/systemd/system/ sudo systemctl daemon-reload Update rbenv version - git -C /home/mastodon/.rbenv/plugins/ruby-build pull Install Ruby 3.2.2 - RUBY_CONFIGURE_OPTS=--with-jemalloc rbenv install 3.2.2 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile Run predeployment DB migration - SKIP_POST_DEPLOYMENT_MIGRATIONS=true RAILS_ENV=production bundle exec rails db:migrate Restart services - systemctl start mastodon-sidekiq mastodon-web mastodon-streaming Run postdeployment DB migration - RAILS_ENV=production bundle exec rails db:migrate 4.2.1 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.2.1 tag - git checkout v4.2.1 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile 4.2.2 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.2.2 tag - git checkout v4.2.2 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile 4.2.3 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.2.3 tag - git checkout v4.2.3 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile 4.2.4 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.2.4 tag - git checkout v4.2.4 Update rbenv version - git -C /home/mastodon/.rbenv/plugins/ruby-build pull Install Ruby 3.2.3 - RUBY_CONFIGURE_OPTS=--with-jemalloc rbenv install 3.2.3 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile 4.2.5 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.2.5 tag - git checkout v4.2.5 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile 4.2.6 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.2.6 tag - git checkout v4.2.6 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile 4.2.7 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.2.7 tag - git checkout v4.2.7 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile 4.2.8 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.2.8 tag - git checkout v4.2.8 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile 4.2.9 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.2.9 tag - git checkout v4.2.9 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile 4.2.10 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.2.10 tag - git checkout v4.2.10 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile 4.2.11 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.2.11 tag - git checkout v4.2.11 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile 4.2.12 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.2.12 tag - git checkout v4.2.12 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile 4.2.13 Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.2.13 tag - git checkout v4.2.13 Install Ruby dependencies - bundle install Install JS dependencies - yarn install --frozen-lockfile 4.3.0 NOT WORKING Stop services - systemctl stop mastodon-*.service Backup database and env file Fetch tags - git fetch \u0026amp;\u0026amp; git fetch --tags Checkout 4.3.0 tag - git checkout v4.3.0 Install yarn4 - corepack enable then, corepack prepare Install Ruby dependencies - bundle install Install JS dependencies - yarn install --immutable Generate secrets - RAILS_ENV=production bin/rails db:encryption:init Copy secrets to .env.production file Precompile assets - RAILS_ENV=production bundle exec rails assets:precompile Run predeployment database migrations - SKIP_POST_DEPLOYMENT_MIGRATIONS=true RAILS_ENV=production bundle exec rails db:migrate Restart services - sudo systemctl restart mastodon-sidekiq mastodon-streaming mastodon-web Run postdeployment database migrations - RAILS_ENV=production bundle exec rails db:migrate Restart Services sudo systemctl restart mastodon-sidekiq mastodon-streaming mastodon-web Documentation https://docs.joinmastodon.org/admin/troubleshooting/index-corruption https://docs.joinmastodon.org/admin/install/ https://docs.joinmastodon.org/admin/upgrading/ https://docs.joinmastodon.org/admin/backups/ https://docs.joinmastodon.org/admin/migrating/ https://docs.joinmastodon.org/admin/tootctl/","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/mastodon-server-upgrades","date":"07/14/2024 03:16 -05:00","tags":["selfhost","socialmedia","mastodon","fediverse"],"summary":"Overview This provides a guide for upgrading specific versions. Backup Instructions These instructions backup the database and environment variables file. It does not back-up media files. Create new...","keywords":["install","mastodon","fetch","checkout","stop","tags","systemctl","rails","bundle","dependencies"]},{"id":"reviews-eat-that-frog-tracy","title":"Eat That Frog!","content":"Eat That Frog! by Brian Tracy Read Rating: 3.2/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/eat-that-frog-tracy","date":"07/09/2024 21:05 -05:00","tags":[],"summary":"Eat That Frog! by Brian Tracy Read Rating: 3.2/5","keywords":["frog","brian","tracy","read","rating:"]},{"id":"posts-lqvlc-network-protocol-firefox","title":"The lqvlc network protocol","content":"## Introduction Whether it\u0027s videos, online radio, podcasts, or my personal music collection, VLC is my default player. I especially enjoy the ability of playing an online audio or video stream using the URL. What I don\u0027t like about that process is that I usually have to manually copy the link and use either the terminal or graphical user interface to play the stream. While not a large number of additional steps, it\u0027s still not ideal. The experience I would like is to automatically launch VLC whenever I click a link to an audio or video stream. That\u0027s possible using custom network protocols in Firefox. In this guide, I\u0027ll show how I defined and configured my own protocol (lqvlc) in Firefox to automatically launch VLC. ## Prerequisites - VLC - Firefox This guide was built to work on unix systems. However, setting up something similar with a PowerShell or CMD script on Windows should work similarly. ## Network protocols A network protocol defines how data is exchanged and transferred over a network. Some common protocols are HTTP, FTP, and SSH. In the context of browsers, the most common protocol is HTTP and the way the browser identifies the protocol is based on the scheme portion of the URL. So for the website [http://lqdev.me](http://lqdev.me), http is the scheme that tells the browser which protocol to use. For more information on network protocols and URLs, see the article [What is a URL?](https://developer.mozilla.org/docs/Learn/Common_questions/What_is_a_URL). Firefox has a set of standard protocols like HTTP that are configured out of the box. However, you have the option of configuring a custom protocol. ## What is the lqvlc protocol Now that you have a general idea of what network protocols are and how browsers handle them, it\u0027s time to introduce the lqvlc protocol. The intent of the lqvlc protocol is to automatically take a URL from an online audio or video stream and launch it in VLC. lqvlc protocol URLs use lqvlc as the scheme instead of http. For example: Original Audio Stream Link lqvlc Link https://somafm.com/u80s64.pls lqvlc://somafm.com/u80s64.pls You\u0027ll notice the only difference is the scheme. That\u0027s enough information for the browser to know, lqvlc is a protocol I need to handle in some way. In the next sections, I\u0027ll describe what you need to do to get Firefox to recognize and handle the lqvlc protocol. ## Configure network protocols in Firefox To configure a custom network protocol 1. Launch Firefox 1. In the address bar, navigate to \u0060about:config\u0060 and select **Accept the Risk and continue**. As the warning mentions, these are advanced configurations so proceed with caution. 1. Add the following configuration preferences: Configuration Preference Type Value network.protocol-handler.expose.lqvlc Boolean false network.protocol-handler.external.lqvlc Boolean true network.protocol-handler.warn-external.lqvlc Boolean true ![Firefox lqvlc network protocol handler configuration preferences](https://user-images.githubusercontent.com/11130940/177898847-79cce773-9275-4e4d-8e7c-a6b5fbfe8f48.png) The last two tell Firefox to let an external application handle files with the protocol you\u0027ve defined. In this case, that\u0027s \u0060lqvlc\u0060. If you\u0027re configuring your own protocol, you\u0027d replace the lqvlc in the configuration preference with your own. ## Create a handler Now that Firefox knows that it must use an external application to handle lqvlc URLs, it\u0027s time to configure the handler. The easiest way to create a handler is to use a bash script. 1. Create a new file called *lqvlc-handler.sh* with the following content: \u0060\u0060\u0060bash #!/bin/bash link=$1 vlc \u0022${link/lqvlc/http}\u0022 \u0060\u0060\u0060 The script takes the first argument it\u0027s passed, an lqvlc URL, replaces the scheme with \u0060http\u0060, and launches it in VLC. 1. Make the file executable. In the terminal, run the following command: \u0060\u0060\u0060bash chmod \u002Bx lqvlc-handler.sh \u0060\u0060\u0060 That\u0027s it! Now you just need to test it out. ## Test it out To use it for your own purposes, you\u0027d have to create or update an HTML page and set the \u0060href\u0060 in your anchor tags to use \u0060lqvlc\u0060 instead of \u0060http\u0060. However, I\u0027ve already configured lqvlc links in my own website. If you go to the [radio page](/radio) on my website, you can click on any of the lqvlc links. Alternatively, You can also click on the link to the [SOMA FM Underground 80\u0027s station lqvlc link](lqvlc://somafm.com/u80s64.pls). A prompt will pop up asking you to choose which app you want to use to handle the link. Select the lqvlc-handler.sh script you created. If everything is configured correctly, VLC should launch and the radio stations should start playing. ## Next Steps Although I\u0027ve gotten this working for my personal site, I\u0027d like to be able to use the lqvlc protocol in any website I visit that has links to audio and video streams. Arguably, I can also launch lqvlc links from other websites. However, that would require website owners to configure them. The next step I think to make that happen would be to create an Add-On that automatically rewrites stream links to use the lqvlc protocol.","contentType":"posts","url":"https://www.lqdev.me/posts/lqvlc-network-protocol-firefox","date":"07/07/2022 18:47 -05:00","tags":["firefox","vlc","web","open source","linux"],"summary":"## Introduction Whether it\u0027s videos, online radio, podcasts, or my personal music collection, VLC is my default player. I especially enjoy the ability of playing an online audio or video stream using...","keywords":["lqvlc","protocol","network","handler","firefox","protocols","link","stream","handle","launch"]},{"id":"notes-terry-carnation","title":"Terry Carnation","content":"Can\u0027t believe I hadn\u0027t heard about these podcasts before: Radio Rental Dark Air Love the horror anthology vibes like Tales From The Crypt and Freakshow for Radio Rental. For Dark Air, I really like the Space Ghost Coast to Coast and actual Coast to Coast feel. Best of all, the host Terry Carnation is Rainn Wilson. Even Terry\u0027s website has a great 90\u0027s web feel to it. Looks like Dark Air has been off the air for some time, but hopefully Radio Rental keeps publishing content.","contentType":"notes","url":"https://www.lqdev.me/notes/terry-carnation","date":"07/03/2022 22:04 -05:00","tags":["podcast","terrycarnation","radiorental","darkair"],"summary":"Can\u0027t believe I hadn\u0027t heard about these podcasts before: Radio Rental Dark Air Love the horror anthology vibes like Tales From The Crypt and Freakshow for Radio Rental. For Dark Air, I really like...","keywords":["coast","radio","rental","dark","feel","terry","best","host","carnation","rainn"]},{"id":"wiki-devcontainers-configurations","title":"DevContainer configurations","content":"Overview A collection of DevContainer configurations Base Debian Image { \u0026quot;name\u0026quot;: \u0026quot;lqdev.me Base Debian DevContainer\u0026quot;, \u0026quot;image\u0026quot;: \u0026quot;mcr.microsoft.com/devcontainers/base:debian\u0026quot;, \u0026quot;features\u0026quot;: { \u0026quot;ghcr.io/devcontainers/features/git:1\u0026quot;: {}, \u0026quot;ghcr.io/devcontainers/features/docker-in-docker:2\u0026quot;: {} }, \u0026quot;customizations\u0026quot;: { \u0026quot;vscode\u0026quot;: { \u0026quot;extensions\u0026quot;: [ \u0026quot;ms-vscode-remote.vscode-remote-extensionpack\u0026quot;, \u0026quot;ms-azuretools.vscode-docker\u0026quot;, \u0026quot;GitHub.copilot\u0026quot;, \u0026quot;GitHub.copilot-chat\u0026quot;, \u0026quot;saoudrizwan.claude-dev\u0026quot; ] } } } Python { \u0026quot;name\u0026quot;: \u0026quot;lqdev.me Python DevContainer\u0026quot;, \u0026quot;image\u0026quot;: \u0026quot;mcr.microsoft.com/devcontainers/base:debian\u0026quot;, \u0026quot;features\u0026quot;: { \u0026quot;ghcr.io/devcontainers/features/git:1\u0026quot;: {}, \u0026quot;ghcr.io/devcontainers/features/docker-in-docker:2\u0026quot;: {}, \u0026quot;ghcr.io/devcontainers/features/python:1\u0026quot;: { \u0026quot;version\u0026quot;: \u0026quot;3.11\u0026quot; }, \u0026quot;ghcr.io/va-h/devcontainers-features/uv:1\u0026quot;: {} }, \u0026quot;customizations\u0026quot;: { \u0026quot;vscode\u0026quot;: { \u0026quot;extensions\u0026quot;: [ \u0026quot;ms-vscode-remote.vscode-remote-extensionpack\u0026quot;, \u0026quot;ms-azuretools.vscode-docker\u0026quot;, \u0026quot;ms-python.python\u0026quot;, \u0026quot;GitHub.copilot\u0026quot;, \u0026quot;GitHub.copilot-chat\u0026quot;, \u0026quot;saoudrizwan.claude-dev\u0026quot; ] } } } Python (GPU) { \u0026quot;name\u0026quot;: \u0026quot;lqdev.me Python (GPU) DevContainer\u0026quot;, \u0026quot;image\u0026quot;: \u0026quot;mcr.microsoft.com/devcontainers/base:debian\u0026quot;, \u0026quot;features\u0026quot;: { \u0026quot;ghcr.io/devcontainers/features/git:1\u0026quot;: {}, \u0026quot;ghcr.io/devcontainers/features/docker-in-docker:2\u0026quot;: {}, \u0026quot;ghcr.io/devcontainers/features/python:1\u0026quot;: { \u0026quot;version\u0026quot;: \u0026quot;3.11\u0026quot; }, \u0026quot;ghcr.io/devcontainers/features/nvidia-cuda:1\u0026quot;: {}, \u0026quot;ghcr.io/va-h/devcontainers-features/uv:1\u0026quot;: {} }, \u0026quot;customizations\u0026quot;: { \u0026quot;vscode\u0026quot;: { \u0026quot;extensions\u0026quot;: [ \u0026quot;ms-vscode-remote.vscode-remote-extensionpack\u0026quot;, \u0026quot;ms-azuretools.vscode-docker\u0026quot;, \u0026quot;ms-python.python\u0026quot;, \u0026quot;GitHub.copilot\u0026quot;, \u0026quot;GitHub.copilot-chat\u0026quot;, \u0026quot;saoudrizwan.claude-dev\u0026quot; ] } }, \u0026quot;runArgs\u0026quot;: [ \u0026quot;--gpus\u0026quot;, \u0026quot;all\u0026quot; ] } .NET { \u0026quot;name\u0026quot;: \u0026quot;lqdev.me .NET DevContainer\u0026quot;, \u0026quot;image\u0026quot;: \u0026quot;mcr.microsoft.com/devcontainers/base:debian\u0026quot;, \u0026quot;features\u0026quot;: { \u0026quot;ghcr.io/devcontainers/features/git:1\u0026quot;: {}, \u0026quot;ghcr.io/devcontainers/features/docker-in-docker:2\u0026quot;: {}, \u0026quot;ghcr.io/devcontainers/features/dotnet:2\u0026quot;: { \u0026quot;version\u0026quot;: \u0026quot;9.0\u0026quot; } }, \u0026quot;customizations\u0026quot;: { \u0026quot;vscode\u0026quot;: { \u0026quot;extensions\u0026quot;: [ \u0026quot;ms-vscode-remote.vscode-remote-extensionpack\u0026quot;, \u0026quot;ms-azuretools.vscode-docker\u0026quot;, \u0026quot;ms-dotnettools.csharp\u0026quot;, \u0026quot;Ionide.Ionide-fsharp\u0026quot;, \u0026quot;GitHub.copilot\u0026quot;, \u0026quot;GitHub.copilot-chat\u0026quot;, \u0026quot;saoudrizwan.claude-dev\u0026quot;, \u0026quot;ms-dotnettools.csdevkit\u0026quot; ] } } } .NET (GPU) { \u0026quot;name\u0026quot;: \u0026quot;lqdev.me .NET (GPU) DevContainer\u0026quot;, \u0026quot;image\u0026quot;: \u0026quot;mcr.microsoft.com/devcontainers/base:debian\u0026quot;, \u0026quot;features\u0026quot;: { \u0026quot;ghcr.io/devcontainers/features/git:1\u0026quot;: {}, \u0026quot;ghcr.io/devcontainers/features/docker-in-docker:2\u0026quot;: {}, \u0026quot;ghcr.io/devcontainers/features/dotnet:2\u0026quot;: { \u0026quot;version\u0026quot;: \u0026quot;9.0\u0026quot; }, \u0026quot;ghcr.io/devcontainers/features/nvidia-cuda:1\u0026quot;: {} }, \u0026quot;customizations\u0026quot;: { \u0026quot;vscode\u0026quot;: { \u0026quot;extensions\u0026quot;: [ \u0026quot;ms-vscode-remote.vscode-remote-extensionpack\u0026quot;, \u0026quot;ms-azuretools.vscode-docker\u0026quot;, \u0026quot;ms-dotnettools.csharp\u0026quot;, \u0026quot;Ionide.Ionide-fsharp\u0026quot;, \u0026quot;GitHub.copilot\u0026quot;, \u0026quot;GitHub.copilot-chat\u0026quot;, \u0026quot;saoudrizwan.claude-dev\u0026quot;, \u0026quot;ms-dotnettools.csdevkit\u0026quot; ] } }, \u0026quot;runArgs\u0026quot;: [ \u0026quot;--gpus\u0026quot;, \u0026quot;all\u0026quot; ] } Python and .NET { \u0026quot;name\u0026quot;: \u0026quot;lqdev.me Python and .NET DevContainer\u0026quot;, \u0026quot;image\u0026quot;: \u0026quot;mcr.microsoft.com/devcontainers/base:debian\u0026quot;, \u0026quot;features\u0026quot;: { \u0026quot;ghcr.io/devcontainers/features/git:1\u0026quot;: {}, \u0026quot;ghcr.io/devcontainers/features/docker-in-docker:2\u0026quot;: {}, \u0026quot;ghcr.io/devcontainers/features/python:1\u0026quot;: { \u0026quot;version\u0026quot;: \u0026quot;3.11\u0026quot; }, \u0026quot;ghcr.io/devcontainers/features/dotnet:2\u0026quot;: { \u0026quot;version\u0026quot;: \u0026quot;9.0\u0026quot; }, \u0026quot;ghcr.io/va-h/devcontainers-features/uv:1\u0026quot;: {} }, \u0026quot;customizations\u0026quot;: { \u0026quot;vscode\u0026quot;: { \u0026quot;extensions\u0026quot;: [ \u0026quot;ms-vscode-remote.vscode-remote-extensionpack\u0026quot;, \u0026quot;ms-azuretools.vscode-docker\u0026quot;, \u0026quot;ms-python.python\u0026quot;, \u0026quot;ms-dotnettools.csharp\u0026quot;, \u0026quot;Ionide.Ionide-fsharp\u0026quot;, \u0026quot;GitHub.copilot\u0026quot;, \u0026quot;GitHub.copilot-chat\u0026quot;, \u0026quot;saoudrizwan.claude-dev\u0026quot;, \u0026quot;ms-dotnettools.csdevkit\u0026quot; ] } } } Python and .NET (GPU) { \u0026quot;name\u0026quot;: \u0026quot;lqdev.me Python and .NET (GPU) DevContainer\u0026quot;, \u0026quot;image\u0026quot;: \u0026quot;mcr.microsoft.com/devcontainers/base:debian\u0026quot;, \u0026quot;features\u0026quot;: { \u0026quot;ghcr.io/devcontainers/features/git:1\u0026quot;: {}, \u0026quot;ghcr.io/devcontainers/features/docker-in-docker:2\u0026quot;: {}, \u0026quot;ghcr.io/devcontainers/features/python:1\u0026quot;: { \u0026quot;version\u0026quot;: \u0026quot;3.11\u0026quot; }, \u0026quot;ghcr.io/devcontainers/features/dotnet:2\u0026quot;: { \u0026quot;version\u0026quot;: \u0026quot;9.0\u0026quot; }, \u0026quot;ghcr.io/devcontainers/features/nvidia-cuda:1\u0026quot;: {}, \u0026quot;ghcr.io/va-h/devcontainers-features/uv:1\u0026quot;: {} }, \u0026quot;customizations\u0026quot;: { \u0026quot;vscode\u0026quot;: { \u0026quot;extensions\u0026quot;: [ \u0026quot;ms-vscode-remote.vscode-remote-extensionpack\u0026quot;, \u0026quot;ms-azuretools.vscode-docker\u0026quot;, \u0026quot;ms-python.python\u0026quot;, \u0026quot;ms-dotnettools.csharp\u0026quot;, \u0026quot;Ionide.Ionide-fsharp\u0026quot;, \u0026quot;GitHub.copilot\u0026quot;, \u0026quot;GitHub.copilot-chat\u0026quot;, \u0026quot;saoudrizwan.claude-dev\u0026quot;, \u0026quot;ms-dotnettools.csdevkit\u0026quot; ] } }, \u0026quot;runArgs\u0026quot;: [ \u0026quot;--gpus\u0026quot;, \u0026quot;all\u0026quot; ] } Additional Resources Developing inside a DevContainer Pre-built DevContainer images Pre-built DevContainer features GitHub Codespaces overview VS Code Extensions Marketplace","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/devcontainers-configurations","date":"06/29/2024 14:48 -05:00","tags":["devcontainer","vscode","codespaces","development","software","tech","programming","python","dotnet","csharp","fsharp","docker","git","debian"],"summary":"Overview A collection of DevContainer configurations Base Debian Image { \u0026quot;name\u0026quot;: \u0026quot;lqdev.me Base Debian DevContainer\u0026quot;, \u0026quot;image\u0026quot;:...","keywords":["\u0026quot","ghcr","vscode","github","remote","python","version\u0026quot","dotnettools","ionide","claude"]},{"id":"reviews-ahh-thats-what-i-call-horror","title":"AHH! That\u0027s What I Call Horror: An Anthology of \u002790s Horror","content":"AHH! That\u0026#39;s What I Call Horror: An Anthology of \u0026#39;90s Horror by Chelsea Pumpkins (Editor), S.E. Denton, Bridget D. Brave, Christopher O\u0026#39;Halloran, Caleb Stephens, Edith Lockwood, C.B. Jones, Mathew Wend, P.L. McMillan, Carson Winter, Patrick Barb, J.W. Donley, J.V. Gachs, Damien B.Raphael Read Rating: 4.1/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/ahh-thats-what-i-call-horror","date":"06/18/2023 19:04 -05:00","tags":[],"summary":"AHH! That\u0026#39;s What I Call Horror: An Anthology of \u0026#39;90s Horror by Chelsea Pumpkins (Editor), S.E. Denton, Bridget D. Brave, Christopher O\u0026#39;Halloran, Caleb Stephens, Edith Lockwood, C.B....","keywords":["that\u0026#39","raphael","damien","gachs","donley","barb","patrick","winter","carson","mcmillan"]},{"id":"wiki-matrix-synapse-admin","title":"Matrix Synapse Server Administration","content":"Overview The following is a guide for managing Matrix Synapse servers. Upgrade Initialize Python virtual environment source env/bin/activate Stop server synctl stop Upgrade Synapse package pip install --upgrade matrix-synapse[postgres] Restart server synctl restart Deactivate Python virtual environment deactivate Check version curl http://localhost:8008/_synapse/admin/v1/server_version Clean up storage Delete old media Run the following command to clean local meadia older than 30 days. curl -X POST -H \u0026quot;Authorization: Bearer access_token\u0026quot; http://localhost:8008/_synapse/admin/v1/media/delete?before_ts=$(date \u002B%s000 --date \u0026quot;30 days ago\u0026quot;) Start PSQL psql Connect to database \\c synapse Clean up unreferenced itemes VACUUM FULL; Delete items in database Start PSQL psql Connect to database \\c synapse Run the following SQL script DROP FUNCTION IF EXISTS synapse_clean_redacted_messages(); CREATE FUNCTION synapse_clean_redacted_messages() RETURNS void AS $$ DECLARE BEGIN UPDATE events SET content = \u0027{}\u0027 FROM redactions AS rdc WHERE events.event_id = rdc.redacts AND (events.type = \u0027m.room.encrypted\u0027 OR events.type = \u0027m.room.message\u0027); END; $$ LANGUAGE \u0027plpgsql\u0027; DROP FUNCTION IF EXISTS synapse_get_server_name(); CREATE FUNCTION synapse_get_server_name() RETURNS text AS $$ DECLARE _someUser TEXT; _serverName TEXT; BEGIN select user_id from account_data limit 1 INTO _someUser; select regexp_replace(_someUser, \u0027^.*:\u0027, \u0027:\u0027) INTO _serverName; RETURN _serverName; END; $$ LANGUAGE \u0027plpgsql\u0027; DROP FUNCTION IF EXISTS synapse_get_unused_rooms(); CREATE FUNCTION synapse_get_unused_rooms() RETURNS TABLE(room_id TEXT) AS $$ DECLARE BEGIN RETURN QUERY SELECT r.room_id FROM rooms AS r WHERE r.room_id NOT IN ( SELECT DISTINCT(m.room_id) FROM room_memberships as m INNER JOIN current_state_events as c ON m.event_id = c.event_id AND m.room_id = c.room_id AND m.user_id = c.state_key WHERE c.type = \u0027m.room.member\u0027 AND m.membership = \u0027join\u0027 AND m.user_id LIKE concat(\u0027%\u0027, synapse_get_server_name()) ); END; $$ LANGUAGE \u0027plpgsql\u0027; DROP FUNCTION IF EXISTS synapse_clean_unused_rooms(); CREATE FUNCTION synapse_clean_unused_rooms() RETURNS void AS $$ DECLARE _count INT; BEGIN CREATE TEMP TABLE synapse_clean_unused_rooms__tmp ON COMMIT DROP AS SELECT room_id FROM synapse_get_unused_rooms(); SELECT COUNT(*) FROM synapse_clean_unused_rooms__tmp INTO _count; RAISE NOTICE \u0027synapse_clean_unused_rooms() Cleaning up % unused rooms\u0027, _count; DELETE FROM event_forward_extremities AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: event_forward_extremities\u0027; DELETE FROM event_backward_extremities AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: event_backward_extremities\u0027; DELETE FROM event_edges AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: event_edges\u0027; DELETE FROM room_depth AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: room_depth\u0027; DELETE FROM events AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: events\u0027; DELETE FROM event_json AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: event_json\u0027; DELETE FROM state_events AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: state_events\u0027; DELETE FROM current_state_events AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: current_state_events\u0027; DELETE FROM room_memberships AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: room_memberships\u0027; DELETE FROM destination_rooms AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: destination_rooms\u0027; DELETE FROM event_failed_pull_attempts AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: event_failed_pull_attempts\u0027; DELETE FROM rooms AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: rooms\u0027; DELETE FROM room_aliases AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: room_aliases\u0027; DELETE FROM state_groups AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: state_groups\u0027; DELETE FROM state_groups_state AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: state_groups_state\u0027; DELETE FROM receipts_graph AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: receipts_graph\u0027; DELETE FROM receipts_linearized AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: receipts_linearized\u0027; DELETE FROM room_tags AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: room_tags\u0027; DELETE FROM room_tags_revisions AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: room_tags_revisions\u0027; DELETE FROM room_account_data AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: room_account_data\u0027; DELETE FROM event_push_actions AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: event_push_actions\u0027; DELETE FROM pusher_throttle AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: pusher_throttle\u0027; DELETE FROM event_reports AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: event_reports\u0027; DELETE FROM stream_ordering_to_exterm AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: stream_ordering_to_exterm\u0027; DELETE FROM event_auth AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: event_auth\u0027; DELETE FROM appservice_room_list AS x WHERE x.room_id IN (SELECT y.room_id FROM synapse_clean_unused_rooms__tmp AS y); RAISE NOTICE \u0027DONE: appservice_room_list\u0027; END; $$ LANGUAGE \u0027plpgsql\u0027; SELECT synapse_clean_redacted_messages(); SELECT synapse_clean_unused_rooms(); Clean up unreferenced tuples VACUUM FULL; Resources Matrix Synapse Docs Upgrading Synapse Admin Media API Synapse Scripts","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/matrix-synapse-admin","date":"06/11/2023 21:13 -05:00","tags":["messaging","how-to","matrix","self-host","synapse","sql","python"],"summary":"Overview The following is a guide for managing Matrix Synapse servers. Upgrade Initialize Python virtual environment source env/bin/activate Stop server synctl stop Upgrade Synapse package pip...","keywords":["room","from","synapse","rooms","clean","unused","select","delete","raise","notice"]},{"id":"posts-install-rtl8821ce-manjaro","title":"Install Manjaro RTL8821CE WiFi drivers","content":"Until recently, I was usign an HP ProBook 430 G1 with Manjaro and a Surface Go (1st Gen) for my computing needs. To cut down on the devices I used, I purchased an [ASUS L210MA](https://www.asus.com/us/laptops/for-home/everyday-use/asus-l210/) which is about the size of the Surface Go but I can run Manjaro on it. Although it doesn\u0027t have the same storage or compute power as the ProBook, I\u0027ve set up a VM in Azure I can use for those instances where I need more compute power than the ASUS laptop can provide. As is sometimes the case, when you install Linux on a new PC, you have to install the right drivers to use your WiFi and other peripheral devices. In this guide I\u0027ll show how to install the RTL-8821-CE WiFi drivers. ## Prerequisites The instructions on this guide were run on a Manjaro XCFE 21.2.6. ## Enable AUR and Install yay The WiFi drivers you\u0027ll need are hosted in the [Arch User Repository (AUR)](https://aur.archlinux.org/). Before installing any packages, you\u0027ll have to enable it on your PC. To do so, install the \u0060yay\u0060 AUR helper package by running the following command in the terminal: \u0060\u0060\u0060bash sudo pacman -S git git clone https://aur.archlinux.org/yay.git cd yay makepkg -si \u0060\u0060\u0060 ## Install Kernel Headers The first thing yuo\u0027ll need to install are your kernel headers. To check which version of the kernel you\u0027re using, run the following command in the terminal: \u0060\u0060\u0060bash uname -r \u0060\u0060\u0060 The output should look similar to the following: \u0060\u0060\u0060text 5.15.41-1-MANJARO \u0060\u0060\u0060 The first two values are your major kernel version. For example, in the output above the kernel version is 5.15. To install the kernel headers, run the following command in the terminal: \u0060\u0060\u0060bash sudo pacman -S linux-headers \u0060\u0060\u0060 Make sure that the kernel version that\u0027s installed is the same as the one output after running the \u0060uname -r\u0060 command. ## Install RTL8821CE drivers Once you\u0027ve installed the headers, it\u0027s time to install the WiFi drivers. Use \u0060yay\u0060 to run the following command in the terminal: \u0060\u0060\u0060bash yay -S rtl8821ce-dkms-git \u0060\u0060\u0060 After some time, the packages should be installed on your device. Reboot your PC and you should now be able to connect to the internet wirelessly","contentType":"posts","url":"https://www.lqdev.me/posts/install-rtl8821ce-manjaro","date":"06/11/2022 15:48 -05:00","tags":["networking","manjaro","linux"],"summary":"Until recently, I was usign an HP ProBook 430 G1 with Manjaro and a Surface Go (1st Gen) for my computing needs. To cut down on the devices I used, I purchased an [ASUS...","keywords":["install","kernel","headers","command","drivers","version","manjaro","following","terminal:","\u0060\u0060\u0060bash"]},{"id":"reviews-mycelium-running","title":"Mycelium Running: How Mushrooms Can Help Save the World","content":"Mycelium Running: How Mushrooms Can Help Save the World by Paul Stamets Read Rating: 4.8/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/mycelium-running","date":"05/20/2023 22:10 -05:00","tags":[],"summary":"Mycelium Running: How Mushrooms Can Help Save the World by Paul Stamets Read Rating: 4.8/5","keywords":["mycelium","running:","mushrooms","help","save","world","paul","stamets","read","rating:"]},{"id":"reviews-user-not-found","title":"User Not Found","content":"User Not Found by Felicity Fenton Read Rating: 4.0/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/user-not-found","date":"05/12/2023 21:17 -05:00","tags":[],"summary":"User Not Found by Felicity Fenton Read Rating: 4.0/5","keywords":["user","found","felicity","fenton","read","rating:"]},{"id":"posts-overdrive-wine-manjaro","title":"Install OverDrive Media Console on Manjaro","content":"## Introduction For the last few months, Manjaro has been the operating system I use as my daily driver for personal tasks. I still have my Surface Go tablet which runs Windows 10 that I use for tasks line syncing my Garmin watch and GPS. On one of the [posts in my feed](/notes/ipod-touch-discontinued), I briefly mentioned my audio solution for podcasts and audiobooks which mainly consists of a standalone MP3 player ([FiiO M6](https://fiio.com/m6)). For podcasts, I manage them through my [NewsBlur RSS feed reader](https://newsblur.com/). When I want to download them for offline listening, I\u0027ve written a .NET console app ([podnet](https://github.com/lqdev/podnet)) to manage that process for me. Audiobooks are a little trickier. Public domain works I get through [Librivox](https://librivox.org/). The limitation there though is that I\u0027m only public domain works are available. Therefore if I want other audiobooks, I either have to purchase them (which has its own issues because of DRM) or borrow them from my local library. I prefer to borrow my books and audiobooks from the library. Not only am I supporting my local library but I also save money in the process. Many libraries including my own manage digital assets through [OverDrive](https://www.overdrive.com/). OverDrive offers a variety of ways for transferring books and audiobooks to your devices. Linux however is not one of those solutions. Until February 2022, OverDrive provided a Windows desktop application (OverDrive Media Console) for managing books and audiobooks. Although there is no Linux desktop application, Linux has Wine. [Wine](https://www.winehq.org/) according to their website is \u0022...a compatibility layer capable of running Windows applications on several POSIX-compliant operating systems, such as Linux, macOS, \u0026 BSD.\u0022. In this post, I\u0027ll show how you can use Wine to install OverDrive Media Console on a PC running the Manjaro Linux distribution. ## Install Wine In order to install OverDrive Media Console, you\u0027ll need to install Wine. To do so: 1. Open the terminal. 1. Install Wine and all its dependencies with the following command: \u0060\u0060\u0060bash sudo pacman -S wine winetricks wine-mono wine_gecko \u0060\u0060\u0060 ## Install OverDrive Media Console The next step is to install the OverDrive Media Console application for Windows desktop. 1. Download the latest version of [OverDrive Media Console for Windows Desktop application](https://www.overdrive.com/apps/overdrive./). This should be a Microsoft Installer file with the \u0060.msi\u0060 extension. Note that as of February 2022, the desktop application is [no longer available for install](https://help.overdrive.com/en-us/0733.html?tocpath=Home%7CGet%20help%20with%20the%20OverDrive%20app%7CWindows%20(desktop)%7C_____7). 1. Open the terminal and use Wine to install OverDrive Media Console with the following command: \u0060\u0060\u0060bash wine msiexec /i ODMediaConsoleSetup.msi \u0060\u0060\u0060 Update the name of the \u0060.msi\u0060 file as needed. ## Run OverDrive Media Console Once the installation is complete, OverDrive Media Console should be in your list of applications. Run it as you would any other app. ![OverDrive Media Console running on Manjaro Linux](https://user-images.githubusercontent.com/11130940/167964715-bb8bfb21-5f72-4a86-9fb0-33c9d1fe9eae.png) Here\u0027s a guide for information on getting started with [OverDrive](https://help.overdrive.com/en-us/categories/getting-started.htm).","contentType":"posts","url":"https://www.lqdev.me/posts/overdrive-wine-manjaro","date":"05/11/2022 19:22 -05:00","tags":["linux","books","audiobooks","manjaro"],"summary":"## Introduction For the last few months, Manjaro has been the operating system I use as my daily driver for personal tasks. I still have my Surface Go tablet which runs Windows 10 that I use for...","keywords":["overdrive","wine","console","media","install","desktop","audiobooks","linux","application","windows"]},{"id":"notes-ipod-touch-discontinued","title":"iPod Touch discontinued","content":"Apple just announced they\u0027re discontinuing the iPod Touch . I\u0027m surprised it hadn\u0027t happened earlier! Still, it\u0027s sad to see the end of an era for these devices which are effectively smartphones without the phone / cellular data connectivity. I still have my iPod Touch from 2014 that I use occasionally for FaceTime which aside from poor battery life and being stuck on an older version of iOS, it still works smoothly. For the past two years, my audio solution has been a dedicated MP3 device. Since most of the content I consume I download and listen to offline, a dedicated MP3 device doesn\u0027t drain my phone\u0027s battery or take up storage space. I started with the FiiO M5 , which is like an iPod Shuffle. The device was great, but all it took was one drop for the screen to crack at which point I upgraded to the FiiO M6 . The nice thing about the M6 is that the OS is a stripped down version of Android meaning you can side-load applications such as Spotify. The bad thing, the hardware can\u0027t handle (even heavily optimized versions) of these applications making the user experience frustrating. The touchscreen interface is great for navigation especially when you have large collections. However, since I only use the device for podcasts and audiobooks and don\u0027t get the benefits of installing streaming music apps, I\u0027d be better off with a more affordable device like those from SanDisk .","contentType":"notes","url":"https://www.lqdev.me/notes/ipod-touch-discontinued","date":"05/10/2022 12:27 -05:00","tags":["apple","ipod","music","ipodtouch"],"summary":"Apple just announced they\u0027re discontinuing the iPod Touch . I\u0027m surprised it hadn\u0027t happened earlier! Still, it\u0027s sad to see the end of an era for these devices which are effectively smartphones...","keywords":["device","which","ipod","from","phone","battery","dedicated","since","applications","version"]},{"id":"notes-chipmaking-chip-shortage","title":"Chip-making Chip Shortage","content":"Chips for chipmaking machines are affected by chip shortage, says TSMC and Intel - via 9to5Mac","contentType":"notes","url":"https://www.lqdev.me/notes/chipmaking-chip-shortage","date":"05/03/2022 18:50 -05:00","tags":["chip","pc","hardware","news"],"summary":"Chips for chipmaking machines are affected by chip shortage, says TSMC and Intel - via 9to5Mac","keywords":["chips","chipmaking","machines","affected","chip","shortage","says","tsmc","intel","9to5mac"]},{"id":"notes-twitter-circles-myspace-top-friends","title":"Twitter Circles: Top 150 friends","content":"Twitter is testing a new feature called Twitter Circles where you can restrict the reach of your tweets to up to 150 people. I remember the days of MySpace where many relationships were made / broken as a result of being in someone\u0027s Top 8 Friends list \uD83D\uDE06","contentType":"notes","url":"https://www.lqdev.me/notes/twitter-circles-myspace-top-friends","date":"05/03/2022 14:31 -05:00","tags":["twitter","myspace","socialmedia"],"summary":"Twitter is testing a new feature called Twitter Circles where you can restrict the reach of your tweets to up to 150 people. I remember the days of MySpace where many relationships were made / broken...","keywords":["twitter","someone","result","broken","made","relationships","many","myspace","days","remember"]},{"id":"notes-mastodon-posse-enabled","title":"Mastodon POSSE enabled","content":"Hello Mastodon from lqdev.me. Got POSSE (Publish (on your) Own Site, Syndicate Elsewhere) working for Mastodon! I already had an Azure Logic App which took the latest post from my RSS feed and published to Twitter. Now it\u0027s also enabled for Mastodon. Might do a writeup later in the blog on how to do it.","contentType":"notes","url":"https://www.lqdev.me/notes/mastodon-posse-enabled","date":"04/27/2022 21:29 -05:00","tags":["mastodon","fediverse","posse","indieweb"],"summary":"Hello Mastodon from lqdev.me. Got POSSE (Publish (on your) Own Site, Syndicate Elsewhere) working for Mastodon! I already had an Azure Logic App which took the latest post from my RSS feed and...","keywords":["mastodon","from","hello","writeup","enabled","also","twitter","published","feed","post"]},{"id":"notes-atom-rss-nws-weather-alerts","title":"ATOM Weather Alerts","content":"Yesterday I ran into more feed goodness. When going to a website to get the weather forecast, weather alerts are displayed (if there are any). I thought, wouldn\u0027t it be great to be able to subscribe to those alerts without having to go to the website? A quick search revealed the National Weather Service provides these alerts in ATOM format for each of the states in the U.S. You have the option of tracking updates at the national, state, zone, and county level. Subscribing is as easy as adding the link for the respective feed to your favorite feed reader. If you\u0027re interested, you can find the list of feeds at https://alerts.weather.gov/","contentType":"notes","url":"https://www.lqdev.me/notes/atom-rss-nws-weather-alerts","date":"04/27/2022 20:19 -05:00","tags":["rss","atom","weather","alerts"],"summary":"Yesterday I ran into more feed goodness. When going to a website to get the weather forecast, weather alerts are displayed (if there are any). I thought, wouldn\u0027t it be great to be able to subscribe...","keywords":["weather","feed","alerts","national","website","yesterday","updates","state","zone","county"]},{"id":"notes-welcome-to-fediverse-tips","title":"Welcome to the Fediverse","content":"In light of recent developments on Twitter, I see more people looking for alternative platforms. Personally, I don\u0027t think anything beats the level of customization and control you get from using your own website and RSS for sharing ideas and publishing content. However, it\u0027s great to see so many new friends in the Fediverse on platforms like Mastodon . I won\u0027t go into what the Fediverse is but you can check out the Fediverse wiki for more info. In many ways the Fediverse will feel familiar, yet foreign. Many people have shared tips on getting started, but I\u0027ll share some of the ones I\u0027ve personally used to organically curate my feed. Use hashtags. Similar to Twitter, hashtags are a good way of finding people and content related to the topic of interest. Follow feditips and FediFollows . As the names suggest, they provide good tips and suggest interesting accounts to follow in the Fediverse. As an alternative if you\u0027re not ready to create an account, you can use RSS. I\u0027m a big fan of RSS . Appending .rss to any account on platforms like Mastodon (for example https://mastodon.online/@FediFollows.rss ) provides you with the RSS feed of an account\u0027s posts letting you subscribe to their content through your favorite RSS feed reader. Feel free to say hi at toot.lqdev.tech/@lqdev . See you in the Fediverse!","contentType":"notes","url":"https://www.lqdev.me/notes/welcome-to-fediverse-tips","date":"04/27/2022 13:04 -05:00","tags":["fediverse","mastodon","twitter","socialmedia","decentralization"],"summary":"In light of recent developments on Twitter, I see more people looking for alternative platforms. Personally, I don\u0027t think anything beats the level of customization and control you get from using...","keywords":["fediverse","many","feed","content","people","account","platforms","hashtags","suggest","feel"]},{"id":"wiki-pandoc","title":"Pandoc","content":"Overview If you need to convert files from one markup format into another, pandoc is your swiss-army knife. Website Recipes List of recipes Emacs org-mode to Microsoft Word Personally, I like drafting and structuring documents in org-mode. However, for sharing and collaboration, that usually takes place in Microsoft Word. This is a command I use on a regular basis. pandoc -s -o \u0026lt;document-name\u0026gt;.docx \u0026lt;document-name\u0026gt;.org","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/pandoc","date":"04/25/2024 22:00 -05:00","tags":["pandoc","plaintext","docx","documents","word","org","emacs","utility","linux"],"summary":"Overview If you need to convert files from one markup format into another, pandoc is your swiss-army knife. Website Recipes List of recipes Emacs org-mode to Microsoft Word Personally, I like...","keywords":["microsoft","mode","recipes","name\u0026gt","pandoc","word","document","takes","usually","command"]},{"id":"wiki-github-actions-azure","title":"GitHub Actions for Azure","content":"Overview How to use GitHub Actions with Azure Documentation Authenticate to Azure from GitHub Documentation This workflow is useful when using the az login action in GitHub Actions .","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/github-actions-azure","date":"04/23/2024 23:04 -05:00","tags":["github","cicd","githubactions","azure"],"summary":"Overview How to use GitHub Actions with Azure Documentation Authenticate to Azure from GitHub Documentation This workflow is useful when using the az login action in GitHub Actions .","keywords":["github","actions","azure","documentation","overview","authenticate","from","workflow","useful","using"]},{"id":"reviews-lurking-how-person-becomes-user","title":"Lurking: How a Person Became a User","content":"Lurking: How a Person Became a User by Joanne McNeil Read Rating: 3.9/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/lurking-how-person-becomes-user","date":"04/23/2024 22:03 -05:00","tags":[],"summary":"Lurking: How a Person Became a User by Joanne McNeil Read Rating: 3.9/5","keywords":["lurking:","person","became","user","joanne","mcneil","read","rating:"]},{"id":"reviews-desert-oracle-vol-1","title":"Desert Oracle: Volume 1","content":"Desert Oracle: Volume 1 by Ken Layne Read Rating: 4.5/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/desert-oracle-vol-1","date":"04/23/2024 22:03 -05:00","tags":[],"summary":"Desert Oracle: Volume 1 by Ken Layne Read Rating: 4.5/5","keywords":["desert","oracle:","volume","layne","read","rating:"]},{"id":"notes-2022-04-21-assorted-links","title":"Assorted links - Ubuntu 22.04 LTS, Bitwarden, DALLE-2, Emacs","content":"Here\u0027s a few links I found in the interwebs today that I thought were worth sharing. Ubuntu 22.04 LTS Released For my personal computing needs, Manjaro has been my default Linux distro. However, for hosting and cloud VMs I go back and forth between Debian and Ubuntu depending on which better supports the software I\u0027m running. Canonical announced the release of the latest Ubuntu LTS version 22.04. Check out their blog post for more details on what\u0027s new. Self-hosting Bitwarden on DigitalOcean When it comes to password managers, other than KeePass, my favorite it Bitwarden . It\u0027s open-source, built on .NET, has excellent features that get even better with the low-cost paid version, but most importantly provides you the option of self-hosting. If you\u0027re looking to self-host Bitwarden on DigitalOcean, make sure to check out the guide they just published. How DALLE-2 Actually Works A few weeks ago, OpenAI announced the release of DALLE-2 . Open AI describes DALLE-2 as, \u0026quot;...a new AI system that can create realistic images and art from a description in natural language.\u0026quot; If you want to understand the details of how it works, you could read the 27 page research paper Hierarchical Text-Conditional Image Generation with CLIP Latents . AssemblyAI just published an approachable guide into the inner workings of DALLE-2. If you\u0027re interested in that, check out their blog post on the topic. Emacs Configuration Generator In my early days of using Emacs, I often found it difficult to customize it to my needs. Not only did I not know what was possible, but it also caused some anxiety because I didn\u0027t feel confident enough making major tweaks. If you\u0027re getting started with Emacs and aren\u0027t sure how to configure it, this neat utility helps you generate a config file for Emacs. PostgresML If you\u0027re building machine learning applications, one of the main things you\u0027ll need is data. This data can reside in databases such as MySQL, SQL Server, and Postgres. Algorithms are applied to this data downstream using libraries written in languages like Python, R, and .NET. This not only means potentially moving the data from the database to another process but also using another library to train models. According to the project description on GitHub, \u0026quot;PostgresML is an end-to-end machine learning system. It enables you to train models and make online predictions using only SQL, without your data ever leaving your favorite database.\u0026quot; If that\u0027s something you\u0027re interested in, check out the project on GitHub .","contentType":"notes","url":"https://www.lqdev.me/notes/2022-04-21-assorted-links","date":"04/21/2022 19:44 -05:00","tags":["links","assortedlinks","ubuntu","bitwarden","dalle","openai","emacs","linkblog"],"summary":"Here\u0027s a few links I found in the interwebs today that I thought were worth sharing. Ubuntu 22.04 LTS Released For my personal computing needs, Manjaro has been my default Linux distro. However, for...","keywords":["data","emacs","using","check","\u0026quot","dalle","ubuntu","bitwarden","hosting","self"]},{"id":"reviews-notes-from-the-underground-dostoyevsky","title":"Notes From The Undergrond","content":"Notes From The Undergrond by Fyodor Dostoyevsky InProgress","contentType":"reviews","url":"https://www.lqdev.me/reviews/notes-from-the-underground-dostoyevsky","date":"03/31/2024 19:31 -05:00","tags":[],"summary":"Notes From The Undergrond by Fyodor Dostoyevsky InProgress","keywords":["notes","from","undergrond","fyodor","dostoyevsky","inprogress"]},{"id":"reviews-slow-productivity-newport","title":"Slow Productivity: The Lost Art of Accomplishment Without Burnout","content":"Slow Productivity: The Lost Art of Accomplishment Without Burnout by Cal Newport Read Rating: 4.2/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/slow-productivity-newport","date":"03/23/2024 12:42 -05:00","tags":[],"summary":"Slow Productivity: The Lost Art of Accomplishment Without Burnout by Cal Newport Read Rating: 4.2/5","keywords":["slow","productivity:","lost","accomplishment","burnout","newport","read","rating:"]},{"id":"wiki-nixos-garbage-collection","title":"Garbage Collections NixOS","content":"Overview One of the nice things about NixOS is, whenever you update the system, old configurations remain in place. That way if something were to go wrong, you can always revert back. However, there are tradeoffs. Each old configuration takes up space on your hard drive. If you\u0027re updating your system every week or every few weeks, this adds up. The following is a general guide on how to clean these up. Delete old generations This command deletes every generation, except the most current one. nix-env --delete-generations old If you\u0027d like to keep a few of the most recent ones. Say, from the last 14 days, you can pass an argument. nix-env --delete-generations 14d Run the garbage collector nix-store --gc Use the nix-collect-garbage utility The following utility will delete older generations across all profiles on your system. nix-collect-garbage -d Resources NixOS Garbage Collection","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/nixos-garbage-collection","date":"03/10/2024 14:01 -05:00","tags":["nixos","system","os","linux"],"summary":"Overview One of the nice things about NixOS is, whenever you update the system, old configurations remain in place. That way if something were to go wrong, you can always revert back. However, there...","keywords":["generations","garbage","delete","system","following","nixos","utility","collect","overview","command"]},{"id":"reviews-agency-gibson","title":"Agency","content":"Agency by William Gibson Read Rating: 2.5/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/agency-gibson","date":"02/19/2024 15:19 -05:00","tags":[],"summary":"Agency by William Gibson Read Rating: 2.5/5","keywords":["agency","william","gibson","read","rating:"]},{"id":"presentations-hello-world","title":"Hello World!","content":"## Slide 1 A paragraph with some text and a [link](https://www.luisquintanilla.me). --- ## Slide 2 --- ## Slide 3","contentType":"presentations","url":"https://www.lqdev.me/resources/presentations/hello-world","date":"02/08/2021 21:45 -05:00","tags":[],"summary":"## Slide 1 A paragraph with some text and a [link](https://www.luisquintanilla.me). --- ## Slide 2 --- ## Slide 3","keywords":["slide","paragraph","text","link","https://www","luisquintanilla"]},{"id":"reviews-the-declassification-engine","title":"The Declassification Engine: What History Reveals About America\u0027s Top Secrets","content":"The Declassification Engine: What History Reveals About America\u0026#39;s Top Secrets by Matthew Connelly InProgress","contentType":"reviews","url":"https://www.lqdev.me/reviews/the-declassification-engine","date":"02/06/2024 18:52 -05:00","tags":[],"summary":"The Declassification Engine: What History Reveals About America\u0026#39;s Top Secrets by Matthew Connelly InProgress","keywords":["declassification","engine:","history","reveals","america\u0026#39","secrets","matthew","connelly","inprogress"]},{"id":"wiki-owncast","title":"Owncast","content":"Overview Owncast is a free and open source live video and web chat server for use with existing popular broadcasting software. Simulcast By default, when you set up streaming software, it will only stream to your Owncast instance. If you want to simultaneously broadcast to various services, you\u0027ll have to either use something like Restream or you can also use FFMPEG. YouTube With your brodcast stream started, use the following FFMPEG command to simulcast to YouTube. ffmpeg -v verbose -re -i https://YOUR-OWNCAST-SERVER/hls/stream.m3u8 -c:v libx264 -c:a aac -f flv rtmp://a.rtmp.youtube.com/live2/YOUR-STREAM-KEY This command will copy the video and audio feeds from your HLS Owncast live stream and forward them to YouTube. Ressources Owncast OBS Studio: Stream to multiple platforms or channels at once","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/owncast","date":"01/30/2025 20:19 -05:00","tags":["owncast","fediverse","livestream"],"summary":"Overview Owncast is a free and open source live video and web chat server for use with existing popular broadcasting software. Simulcast By default, when you set up streaming software, it will only...","keywords":["stream","owncast","youtube","ffmpeg","command","live","video","simulcast","software","libx264"]},{"id":"presentations-reactor-mlnet-container-apps","title":"Deploy ML.NET Models to Azure Container Apps","content":"","contentType":"presentations","url":"https://www.lqdev.me/resources/presentations/reactor-mlnet-container-apps","date":"01/26/2022 10:19 -05:00","tags":[],"summary":"","keywords":[]},{"id":"reviews-armageddon-2419-ad","title":"Armageddon 2419 A.D.","content":"Armageddon 2419 A.D. by Philip Francis Nowlan Read Rating: 3.2/5","contentType":"reviews","url":"https://www.lqdev.me/reviews/armageddon-2419-ad","date":"01/22/2023 13:45 -05:00","tags":[],"summary":"Armageddon 2419 A.D. by Philip Francis Nowlan Read Rating: 3.2/5","keywords":["armageddon","2419","philip","francis","nowlan","read","rating:"]},{"id":"reviews-how-to-sell-a-haunted-house","title":"How to Sell a Haunted House","content":"How to Sell a Haunted House by Grady Hendrix InProgress","contentType":"reviews","url":"https://www.lqdev.me/reviews/how-to-sell-a-haunted-house","date":"01/16/2024 19:52 -05:00","tags":[],"summary":"How to Sell a Haunted House by Grady Hendrix InProgress","keywords":["sell","haunted","house","grady","hendrix","inprogress"]},{"id":"reviews-a-city-on-mars","title":"A City on Mars - Can we settle space, should we settle space, and have we really thought this through?","content":"A City on Mars - Can we settle space, should we settle space, and have we really thought this through? by Kelly Weinersmith and Zach Weinersmith Wishlist","contentType":"reviews","url":"https://www.lqdev.me/reviews/a-city-on-mars","date":"01/16/2024 19:52 -05:00","tags":[],"summary":"A City on Mars - Can we settle space, should we settle space, and have we really thought this through? by Kelly Weinersmith and Zach Weinersmith Wishlist","keywords":["settle","space","weinersmith","city","mars","really","thought","kelly","zach","wishlist"]},{"id":"reviews-palo-alto-history-california","title":"Palo Alto - A History of California, Capitalism, and the World","content":"Palo Alto - A History of California, Capitalism, and the World by Malcom Harris Wishlist","contentType":"reviews","url":"https://www.lqdev.me/reviews/palo-alto-history-california","date":"01/15/2024 15:42 -05:00","tags":[],"summary":"Palo Alto - A History of California, Capitalism, and the World by Malcom Harris Wishlist","keywords":["palo","alto","history","california","capitalism","world","malcom","harris","wishlist"]},{"id":"reviews-making-it-in-america","title":"Making It in America - The Almost Impossible Quest to Manufacture in the U.S.A. (And How It Got That Way)","content":"Making It in America - The Almost Impossible Quest to Manufacture in the U.S.A. (And How It Got That Way) by Rachel Slade InProgress","contentType":"reviews","url":"https://www.lqdev.me/reviews/making-it-in-america","date":"01/15/2024 15:23 -05:00","tags":[],"summary":"Making It in America - The Almost Impossible Quest to Manufacture in the U.S.A. (And How It Got That Way) by Rachel Slade InProgress","keywords":["making","america","almost","impossible","quest","manufacture","rachel","slade","inprogress"]},{"id":"notes-archive-81-black-tapes","title":"Archive 81 - so far so good!","content":"About 2 episodes into the show Archive 81 on Netflix and so far so good. I listened to a few episodes of the podcast (by the same name) it\u0027s based on a few years ago. Now it makes me want to go back and listen to it again. It also makes me think about what the Black Tapes could\u0027ve been had it gotten a TV or movie adaptation. If you\u0027re into the found footage, cult horror type of stories (think Blair Witch \u002B Midsommar \u002B Rosemary\u0027s Baby), definitely check it out.","contentType":"notes","url":"https://www.lqdev.me/notes/archive-81-black-tapes","date":"01/15/2022 21:01 -05:00","tags":["podcast","tv","archive81","netflix","horror","suspense"],"summary":"About 2 episodes into the show Archive 81 on Netflix and so far so good. I listened to a few episodes of the podcast (by the same name) it\u0027s based on a few years ago. Now it makes me want to go back...","keywords":["episodes","makes","movie","adaptation","found","footage","cult","gotten","horror","stories"]},{"id":"snippets-winget-config","title":"Winget Configuration","content":"Description My Winget Configuration file Usage winget configure -f \u0026lt;FILENAME\u0026gt;.dsc Snippet # yaml-language-server: $schema=https://aka.ms/configuration-dsc-schema/0.2 # Reference: https://github.com/microsoft/winget-create#building-the-client # WinGet Configure file Generated By Dev Home. properties: resources: - resource: Microsoft.Windows.Developer/DeveloperMode directives: description: Enable Developer Mode allowPrerelease: true settings: Ensure: Present - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Microsoft.VisualStudio.2022.Community allowPrerelease: true securityContext: current settings: id: \u0026quot;Microsoft.VisualStudio.2022.Community\u0026quot; source: winget id: Microsoft.VisualStudio.2022.Community - resource: Microsoft.VisualStudio.DSC/VSComponents dependsOn: - Microsoft.VisualStudio.2022.Community directives: description: Install required VS workloads allowPrerelease: true settings: productId: Microsoft.VisualStudio.Product.Community channelId: VisualStudio.17.Release components: - Microsoft.VisualStudio.Workload.Azure - Microsoft.VisualStudio.Workload.NetWeb - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Microsoft.VisualStudioCode allowPrerelease: true securityContext: current settings: id: \u0026quot;Microsoft.VisualStudioCode\u0026quot; source: winget id: Microsoft.VisualStudioCode # https://github.com/microsoft/winget-cli/discussions/3958 # - resource: vscode/VSCodeExtension # directives: # description: Install Remote Development Extension # allowPrerelease: true # settings: # Name: ms-vscode-remote.vscode-remote-extensionpack # Ensure: Present # - resource: vscode/VSCodeExtension # directives: # description: Install YAML Extension # allowPrerelease: true # settings: # Name: redhat.vscode-yaml # Ensure: Present # - resource: vscode/VSCodeExtension # directives: # description: Install Ionide Extension # allowPrerelease: true # settings: # Name: Ionide.Ionide-fsharp # Ensure: Present # - resource: vscode/VSCodeExtension # directives: # description: Install C# Extension # allowPrerelease: true # settings: # Name: ms-dotnettools.csdevkit # Ensure: Present - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Git.Git allowPrerelease: true securityContext: current settings: id: \u0026quot;Git.Git\u0026quot; source: winget id: Git.Git - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Microsoft.PowerShell allowPrerelease: true securityContext: current settings: id: \u0026quot;Microsoft.PowerShell\u0026quot; source: winget id: Microsoft.PowerShell - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Docker.DockerDesktop allowPrerelease: true securityContext: current settings: id: \u0026quot;Docker.DockerDesktop\u0026quot; source: winget id: Docker.DockerDesktop - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Debian.Debian allowPrerelease: true securityContext: current settings: id: \u0026quot;Debian.Debian\u0026quot; source: winget id: Debian.Debian - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Microsoft.DotNet.SDK.8 allowPrerelease: true securityContext: current settings: id: \u0026quot;Microsoft.DotNet.SDK.8\u0026quot; source: winget id: Microsoft.DotNet.SDK.8 - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Microsoft.DotNet.SDK.9 allowPrerelease: true securityContext: current settings: id: \u0026quot;Microsoft.DotNet.SDK.9\u0026quot; source: winget id: Microsoft.DotNet.SDK.9 - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing OBSProject.OBSStudio allowPrerelease: true securityContext: current settings: id: \u0026quot;OBSProject.OBSStudio\u0026quot; source: winget id: OBSProject.OBSStudio - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Microsoft.WSL allowPrerelease: true securityContext: current settings: id: \u0026quot;Microsoft.WSL\u0026quot; source: winget id: Microsoft.WSL - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Ollama.Ollama allowPrerelease: true securityContext: current settings: id: \u0026quot;Ollama.Ollama\u0026quot; source: winget id: Ollama.Ollama - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Microsoft.WindowsTerminal allowPrerelease: false securityContext: current settings: id: \u0026quot;Microsoft.WindowsTerminal\u0026quot; source: winget id: Microsoft.WindowsTerminal - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Brave Browser allowPrerelease: true securityContext: current settings: id: \u0026quot;Brave.Brave\u0026quot; source: winget id: Brave.Brave # https://github.com/microsoft/winget-pkgs/issues/155070 # - resource: Microsoft.WinGet.DSC/WinGetPackage # directives: # description: Installing NordVPN # allowPrerelease: true # securityContext: current # settings: # id: \u0026quot;NordSecurity.NordVPN\u0026quot; # source: winget # id: NordSecurity.NordVPN - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Thunderbird allowPrerelease: true securityContext: current settings: id: \u0026quot;Mozilla.Thunderbird\u0026quot; source: winget id: Mozilla.Thunderbird - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing ProtonMail allowPrerelease: true securityContext: current settings: id: \u0026quot;Proton.ProtonMail\u0026quot; source: winget id: Proton.ProtonMail - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing Bitwarden allowPrerelease: true securityContext: current settings: id: \u0026quot;Bitwarden.Bitwarden\u0026quot; source: winget id: Bitwarden.Bitwarden - resource: Microsoft.WinGet.DSC/WinGetPackage directives: description: Installing VLC allowPrerelease: true securityContext: current settings: id: \u0026quot;VideoLAN.VLC\u0026quot; source: winget id: VideoLAN.VLC configurationVersion: 0.2.0","contentType":"snippets","url":"https://www.lqdev.me/resources/snippets/winget-config","date":"01/14/2025 20:51 -05:00","tags":["windows","powershell","yaml"],"summary":"Description My Winget Configuration file Usage winget configure -f \u0026lt;FILENAME\u0026gt;.dsc Snippet # yaml-language-server: $schema=https://aka.ms/configuration-dsc-schema/0.2 # Reference:...","keywords":["microsoft","winget","resource:","description:","directives:","settings:","allowprerelease:","true","\u0026quot","source:"]},{"id":"presentations-mlnet-globalai-2022","title":"Implementacion de modelos ML.NET con Azure Container Apps","content":"","contentType":"presentations","url":"https://www.lqdev.me/resources/presentations/mlnet-globalai-2022","date":"01/14/2022 10:10 -05:00","tags":[],"summary":"","keywords":[]},{"id":"wiki-linux-ls-commands","title":"Linux ls Commands","content":"Overview An overview of ls Linux commands ls Display information about a directory and its contents lscpu Display information about the CPU architecture lsgpu Display information about GPU architecture lsblk Display information about block devices lsmem List ranges of available memory with their online status. lspci Display information about PCI devices lsusb Display information about USB devices","contentType":"wiki","url":"https://www.lqdev.me/resources/wiki/linux-ls-commands","date":"01/02/2023 15:22 -05:00","tags":["linux","commandline","cli","hardware","software"],"summary":"Overview An overview of ls Linux commands ls Display information about a directory and its contents lscpu Display information about the CPU architecture lsgpu Display information about GPU...","keywords":["display","information","devices","overview","architecture","status","online","memory","available","ranges"]}]