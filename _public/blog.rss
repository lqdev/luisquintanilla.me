<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - Posts</title>
    <link>https://www.luisquintanilla.me/posts</link>
    <description>Blog posts by Luis Quintanilla</description>
    <lastBuildDate>2025-08-14 01:45 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>Mobile-First Static Site Publishing: Discord Bot Pipeline via Azure and GitHub</title>
      <description>&lt;![CDATA[
## The problem

Ever since I published my [first note](/notes/hello-world)([microblog post](https://indieweb.org/note)) on my website, I've always wanted a way to quickly publish while on the go. Unfortunately, I never found a good solution. 

Because my website is statically generated and the source is hosted on GitHub(check out the [colophon](/colophon) for more details), there is no backend for me to talk to. At the same time, I didn't want to build an entire backend to support my website because I want to keep things as lean and cost-efficient as possible. 

Since my posts are just frontmatter and Markdown, I [use VS Code as my editor](/uses). For some time, back when I used to have a Surface Duo, I [authored posts from mobile using the github.dev experience](https://www.lqdev.me/notes/surface-duo-blogging-github-dev). On two screens, while not ideal, it was manageable. After switching devices (because sadly there were no more security updates on the Surface Duo) and upgrading to a dumbphone and later a single screen smartphone, that workflow wasn't feasible.

At that point, what I resorted to was sending messages to myself via [Element](https://element.io/). The message would contain a link I wanted to check out later. Once I was on my laptop, I would check out the link and if I wanted to post about it on my website, I'd do so then. 

That process, while it worked, wasn't necessarily scalable. In part that's a feature because I could spend more time digesting the content and writing a thoughtful article. However, it stopped me from sharing more in the moment and there were posts that were never authored or bookmarks that weren't captured because eventually that link got lost in the river of other links. 

Basically what I wanted to replicate was the more instant posting that social media gives you, but do so on my own site. 

That led me to doing some thinking and requirement gathering around the type of experience I wanted to have.

## Requirements

When it came to requirements for my solution, I was focused more on the workflow and experience rather on technical details. 

Here is a list of those solution requirements:

- Mobile is the primary publishing interface. Desktop publishing is a nice to have.
- Be as low-friction as sharing a link via Element or posting on social media
- Doesn't require implementing my own client or frontend
- Doesn't require me to use existing micropub clients
- Handles short-form and media posts supported by my website
    - Notes
    - Responses
        - Repost
        - Reply
        - Like
    - Bookmark
    - Media
        - Image
        - Audio (not used as often but technically supported)
        - Video (not used as often but technically supported)
- Low-cost

## The solution

For years, I had been struggling with actually implementing this system. The main part that gave me pause was not implementing my own client or relying on existing micropub clients. 

Eventually, I just accepted that it might never happen. 

One day, it eventually hit me. If the notes to self Element workflow worked so well, why not use a chat client as the frontend for publishing. At least have it serve as the capture system that would then format the content into a post that gets queued for publishing on GitHub. I'd seen [Benji do something similar with his micropub endpoint](https://github.com/benjifs/serverless-micropub). 

While I could've used Element since that's my preferred platform, I've been contemplating no longer hosting my own Matrix server. So if I went through with this, I'd want something that I didn't feel bad about investing the time on this solution if that chat client went away. 

That then left me with Discord as the next best option. Primarily because of its support for bots as well as its cross-platform support across mobile and desktop. 

In the end, the solution then ended up being fairly straightforward. 

More importantly, with the help of AI, I wrote none of the code. 

Using Copilot and Claude Sonnet 4, I was able to go from idea to deployment in 1-2 days. At that time the solution supported all of the posts except for media which I hadn't figured out what the best way of uploading media through Discord was. Figuring that out, implementing it, and deploying it took another day or two. 

Since I wanted for my solution to be as low-cost as possible, serverless seemed like a good option. I only pay for compute when it's actually being used which can be infrequent in my case. I don't need the server running 24/7 or even to be powerful. However, I didn't want to write my system as an Azure Function. I wanted the flexibility of deploying on a shared VM or container. A VM though wasn't an option since it's running 24/7. Keeping all of that in mind, my choice was narrowed down to Azure Container Apps which gave me the characteristics I was looking for. Serverless containers. 

Once that decision was made, I used Copilot again to figure out how to optimize my container image so that it's space and resource efficient. And while at it, I used Copilot again to figure out the right incantations to get the container deployed to Azure Container Apps. 

All-in-all, the solution had been staring at me in the face since I already had a workflow that for the most part worked for me, it just needed some optimizations and with the help of AI, I was able to quickly build and deploy something I'd been ruminating over for years.  

## Workflow

The workflow for publishing is as follows:

1. Invoke the bot in Discord to capture my input using slash command `/post` and the respective post type.

    ![Using slash commands to invoke discord publishing bot](/files/images/invoke-discord-publishing-bot.png)

1. Provide post details. For media posts, I can provide an attachment which gets uploaded to Azure Blob Storage.

    ![A modal in discord with note post fields filled in](/files/images/discord-publishing-client-modal.png)

1. [Bot creates a branch and PR in my repo](https://github.com/lqdev/luisquintanilla.me/pull/152) with the post content
1. While logged into GitHub from my phone, if everything looks good, I merge the PR which kicks off my GitHub Actions workflow to build and publish the site including the new post. 
1. Post [displays on my website](/media/test-media-post-from-mobile/).

## Challenges

The solution is not perfect. 

One of the problems I've run into is cold-start. Since I scale my solution down to zero when it's not being used to save on costs, I suffer from the cold start problem. Therefore, when I first invoke the bot, it fails. I have to give it a few seconds and retry the invocation. It's usually about 5 seconds so it's not a huge issue but it does add some friction. 

## Next steps

Overall I'm happy with my solution but there are a few improvements I'd like to make.

- **Open-source the repo** - Currently I've kept the repo private since it was all AI generated. Since my system is already in production and processes were documented, I need to do a more thorough pass to make sure that no secrets or credentials are checked in or documented anywhere.
- **Improve UX** - Discord limits modal fields to 5. Therefore, I'm playing around with the right balance between how much of the input should come from slash commands and how much should come from the modal.
- **Expand supported post types** - I'd like to expand the number of posts supported by my publishing client. Reviews are a good example of the type of post I'd like to support as well as RSVPs. [Reviews I already support](https://www.lqdev.me/reviews) on my website but RSVPs I don't yet. Also, I'd have to fix my Webmentions which are currently broken after [upgrading my website](/notes/hello-world-new-site-2025-08/). 
- **Make it generator agnostic** - Currently this only works for my website. With a few tweaks and refactoring, I think I can get the project to a place where it should work with other popular static site generators. 
- **One-click deployment** - Currently the solution is packaged up as a container so it can be deployed from anywhere. I want to make it even simpler to deploy. One click if possible. 

]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/website-mobile-publishing-discord-client</link>
      <guid>https://www.luisquintanilla.me/posts/website-mobile-publishing-discord-client</guid>
      <pubDate>2025-08-14 01:45 -05:00</pubDate>
      <category>discord</category>
      <category>azure</category>
      <category>indieweb</category>
      <category>mobile</category>
      <category>microblogging</category>
      <category>web</category>
      <category>bot</category>
      <category>aca</category>
      <category>azurecontainerapps</category>
    </item>
    <item>
      <title>IndieWeb Create Day - July 2025</title>
      <description>&lt;![CDATA[
Since it was a holiday weekend in the U.S. that kind of snuck up on me, I found myself with nothing planned on a Saturday. So I chose to spend it creating stuff for my website with the IndieWeb community during [IndieWeb Create Day](https://events.indieweb.org/2025/07/indieweb-create-day-3q2PTCbGioi9).

Over the last few months I've been overthinking my website redesign and while I've made several attempts at it, I've never been satisfied the outcome. I end up throwing away all the progress I've made and go back to the drawing board. 

Yesterday, I decided to not let perfect be the enemy of good and the approach I took was creating just a simpler piece of functionality outside of my website. How I integrate it into my website is a future me problem. But I want to work from a place of creativity and complete freedom to think of what could be rather than what is. 

With that in mind, I set out to sketch out how I want to create and render media (image, audio, video) posts. The approach I took used a combination of front-matter YAML and custom markdown media extensions. The front-matter YAML is something that I already use for my website and it's something that I want to continue using. However, in contrast to my current website, I like that the front-matter was kept simple and only includes a basic amount of information. The actual post content was handled by my custom markdown extension which leveraged YAML-like syntax to define media content. What's great about this is that it is composable so once I got one type of media working, the rest for the most part "just worked". I could even mix different media types within the same post with no additional work or code changes required. Once I had the skeleton, it was all about refactoring, documentation, adding finishing touches, and vibe-coding some CSS which Claude did a relatively good job with given the aesthetic I was going for. 

Overall, I'm happy with the end result. 

![A screenshot of a website post containing an image and audio player](/files/images/indieweb-create-day-2025-07.png)

For more details, you can check out the [repo](https://github.com/lqdev/indieweb-create-day-2025-07).

At some point, I want to be able to integrate these media posts into my static site generator but for the time being, there are other kinds of posts such as reviews, RSVPs, and other [post types](https://indieweb.org/posts#Types_of_Posts) that I want to design and eventually also support on my website. I liked the approach I took this time around because it gave me the freedom to explore posibilities rather than constrain my creativity to what I've already built. So I think I'll keep doing the same for subsequent post types. 

At the end of the day, it was nice seeing everyone else's projects. My favorite one was Cy's recipe website. I want to be like them when I grow up 🙂.]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/indieweb-create-day-2025-07</link>
      <guid>https://www.luisquintanilla.me/posts/indieweb-create-day-2025-07</guid>
      <pubDate>2025-07-06 20:09</pubDate>
      <category>indieweb</category>
      <category>iwc</category>
      <category>community</category>
      <category>website</category>
    </item>
    <item>
      <title>FediForum Day One Recap</title>
      <description>&lt;![CDATA[
Just wrapped up a successful first day of FediForum.

The vibes and energy were high. Tons of great conversations and projects around the social web.

A few emerging themes I noticed:

- Identity
- Portability / Interoperability
- Feeds
- Commerce

Ian Forrester kicked us off with his Public Service &amp; The Fediverse keynote [(Slides)](https://docs.google.com/presentation/d/1HuQF_sI19zwByM793uvdl7BECneRmni_H9uOG-oFkdM/edit?slide=id.g420c517061_0_0#slide=id.g420c517061_0_0). 

One of the ideas that struck a chord of public service integrated into the fediverse. More specifically the interest that sparked in me was that publishing and social shouldn't be two separate things. Following the POSSE principle from the IndieWeb. You publish on your own site and then it's syndicated elsewhere. 

This was interesting enough for me I even hosted a session on the topic, I think it was called Tightening the Loop between CMS and the Fediverse. It was my first unconference, so I appreciated the way the agenda was built. Announce your topic, see whether there's interest, put it on the agenda, chat with fellow participants. Super easy.

## Identity &amp; Portability

These are such a huge topic but for the purpose of this post, I'm lumping them together.

[https://bounce-migrate.appspot.com/](https://bounce.so) is one of the projects aiming to make portability easy. What's so interesting is they're making it easy to migrate across protocols. So if you're in one network like ATProto (Bluesky), migrating to the Fediverse should be relatively seamless with https://bounce.so. 

Some great discussions that emerged on the topic as well include:

- **Reputation** - How do you build a web of trust?
- **Compartmentalization and Deduplication** - A single identity or multiple identities? When "following" someone, which of their feeds takes priority?

## Feeds

Talk of feeds was everywhere. I made a note to myself throughout the conference.

&gt; It's amazing how big the feeds theme is. Feed ownership, customization, and sharing. All powered by open protocols.

- [Bonfire releases 1.0](https://bonfirenetworks.org/posts/bonfire_social_rc/) - Congrats to the Bonfire team on this milestone. I haven't tried Bonfire myself, but the Circles feature caught my attention. It made me reminiscent of Google+.
- [Surf.Social is now in beta](https://https://surf.social) - As an avid user and curator of RSS feeds, I'd heard about Surf before but hadn't really looked into it. The beta release was announced at the conference and I quickly was able to sign up and download it. Kudos to the team on this milestone and thanks for being so responsive to my request to join the beta. I did almost no waiting in the waitlist. Once I have a chance to try it out and get familiar with it, I'll share some thoughts.
- [Channels](https://home.channel.org/) from the folks at [Newsmast Foundation](https://www.newsmastfoundation.org/) looks like an interesting way to curate and customize feeds. 
[Bring Your Own Timeline Algorithm](https://huggingface.co/spaces/mozilla-ai/byota) leverages semantic search to help you seamlessly leverage the power of algorithmic feeds but doing so under your control. Cool use of AI. 

There were a few unconference sessions on the topic as well. 

## Commerce

It was great to see folks talking about enabling creators to earn a living on open platforms and the social web. 

I believe [Bandwagon.fm](https://bandwagon.fm/home) showed of an implementation of a payments and subscription system built on top of [Emmisary](https://emissary.dev/), a social web toolkit.

## Additional Resources

Here's a list of other links and projects I was exposed to during the conference.

- [FediForum Advisory Board and Organizers](https://fediforum.org/people/)
- [An Internet of Many Autonomous Communities](https://blog.rudyfraser.com/an-internet-of-many-autonomous-communities/)
- [FediMeteo - Weather updates in your Fediverse Timeline](https://fedimeteo.com/)
- [Public Interest Technology Group (PITG)](https://pitg.network/)
- [Coalition for Content Provenance and Authenticity (C2PA)](https://c2pa.org/)
- [A New Social](https://www.anew.social/)
- [WeDistribute FediForum Live Coverage](https://wedistribute.org/2025/06/fediforum-june-2025/)
- [BBC Human Values](https://bbc.github.io/humanvalues/)
- [The Indie Beat Radio](https://theindiebeat.fm/)
- [Tie Yourself to a Federated Mastodon](https://pluralistic.net/2024/11/02/ulysses-pact/#tie-yourself-to-a-federated-mast)
- [The No Network Effect](https://snarfed.org/2013-04-13_the-no-network-effect)

As always, Cory Doctorow was great way to close out the first day. I even learned a new term, tron-pilled. Which means as a creator of a platform, you're on the side of the users.  

Looking forward to tomorrow's sessions!]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/fediforum-day-1-jun-2025</link>
      <guid>https://www.luisquintanilla.me/posts/fediforum-day-1-jun-2025</guid>
      <pubDate>2025-06-05 17:20 -05:00</pubDate>
      <category>fediforum</category>
      <category>socialweb</category>
      <category>community</category>
      <category>fediverse</category>
      <category>openweb</category>
    </item>
    <item>
      <title>How do I keep up with AI?</title>
      <description>&lt;![CDATA[
This question comes up a lot in conversations. The short answer? I don’t. There’s just too much happening, too fast, for anyone to stay on top of everything.

While I enjoy sharing links and recommendations, I realized that a blog post might be more helpful. It gives folks a single place they can bookmark, share, and come back to on their own time, rather than having to dig through message threads where things inevitably get lost.

That said, here are some sources I use to try and stay informed:

- **Newsletters** are great for curated content. They highlight the top stories and help filter through the noise.
- **Blogs** are often the primary sources behind those newsletters. They go deeper and often cover a broader set of topics that might not make it into curated roundups.
- **Podcasts** serve a similar role. In some cases, they provide curation like newsletters and deep dives like blogs in others. Best of all, you can tune in while on the go making it a hands-free activity.

For your convenience, if any of the sources (including podcasts) I list below have RSS feeds, I’ve included them in my [AI Starter Pack](/collections/starter-packs//ai), which you can [download and import into your favorite RSS reader](/collections/starter-packs//ai/index.opml) (as long as it supports OPML file imports).

If you have some sources to share, [send me an e-mail](/contact). I'd love to keep adding to this list! If they have a feed I can subscribe to, even better.

## Newsletters

- [AlphaSignal](https://alphasignal.ai/)
- [The Batch](https://www.deeplearning.ai/the-batch/)
- [Latent Space](https://www.latent.space/)
- [TLDR AI](https://tldr.tech/ai)

## Blogs

I pride myself on being able to track down an RSS feed on just about any website, even if it’s buried or not immediately visible. Unfortunately, I haven't found a feed URL for either OpenAI or Anthropic which is annoying. 

**OpenAI and Anthropic, if you could do everyone a favor and drop a link, that would be great.** 

UPDATE: Thanks to [@m2vh@mastodontech.de](https://toot.lqdev.tech/@m2vh@mastodontech.de) for sharing the OpenAI news feed. 

I know I could use one of those web-page-to-RSS converters, but I'd much rather have an official link directly from the source.

- [OpenAI](https://openai.com/news/)
- [Anthropic](https://www.anthropic.com/news)
- [Google AI Blog](https://blog.google/technology/ai/)
- [AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/)
- [Microsoft Research](https://www.microsoft.com/en-us/research/blog/)
- [Google Research](https://research.google/blog/)
- [Simon Willison](https://simonwillison.net/)

## Podcasts

- [AI Daily Brief](https://www.podchaser.com/podcasts/the-ai-daily-brief-formerly-th-5260567)
- [TWIML AI Podcast](https://twimlai.com/)
- [Practical AI](https://changelog.com/practicalai)

## Subscribing to feeds

Now that I’ve got you here...

Let’s talk about the best way to access all these feeds. My preferred and recommended approach is using a feed reader.

When subscribing to content on the open web, feed readers are your secret weapon.

RSS might seem like it’s dead (it’s not—yet). In fact, it’s the reason you often hear the phrase, “Wherever you get your podcasts.” But RSS goes beyond podcasts. It’s widely supported by blogs, newsletters, and even social platforms like the Fediverse (Mastodon, PeerTube, etc.) and BlueSky. It’s also how I’m able to compile my [starter packs](/collections/starter-packs/). 

I've written more about RSS in [Rediscovering the RSS Protocol](/posts/rediscovering-rss-user-freedom), but the short version is this: when you build on open standards like RSS and OPML, you’re building on freedom. Freedom to use the tools that work best for you. Freedom to own your experience. And freedom to support a healthier, more independent web.
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/how-i-keep-up-with-ai</link>
      <guid>https://www.luisquintanilla.me/posts/how-i-keep-up-with-ai</guid>
      <pubDate>2025-05-27 19:44 -05:00</pubDate>
      <category>ai</category>
      <category>rss</category>
      <category>starterpack</category>
      <category>newsletter</category>
      <category>blog</category>
      <category>opml</category>
      <category>openweb</category>
      <category>podcast</category>
      <category>indieweb</category>
    </item>
    <item>
      <title>Vibe-Specing - From concepts to specification</title>
      <description>&lt;![CDATA[
## Introduction

Code generation is a common use case for AI. What about the design process that comes before implementation? Personally, I've found that AI excels not just at coding, but also helping formalize abstract ideas into concrete specifications. This post explores how I used AI-assisted design to transform a collection of loosely related concepts into a technical specification for a new system made up of those concepts.

## Specifications

Generally, I've had mixed success with vibe-coding (the practice of describing what you want in natural language and having AI generate the corresponding code). However, it's something that I'm constantly working on getting better at. Also, with tooling integrations like MCP, I can ground responses and supplement my prompts using external data.

What I find myself being more successful with is using AI to explore ideas and then formalizing those ideas into a specification. Even in the case of vibe-coding, what you're doing with your prompts is building a specification in real-time. 

I'd like to think that eventually I'll get to the vibe-coding part but before diving straight into the code, I'd like to spend time in the design phase. Personally, this is also the part that I find the most fun because you can throw wild things at the wall. It's not until you implement them that you actually validate whether some of those wild ideas are practical. But I find the design phase a ton of fun.

The result of my latest vibe-specing adventure is what I'm calling the [InterPlanetary Knowledge System (IPKS)](https://github.com/lqdev/IPKS).

## From concept to specification

Lately, I've been thinking a lot about knowledge. Some concepts that have been in my head are those of **non-linear publishing** (creating content that can be accessed in any order with multiple entry points, like wikis or hypertext) and **distributed cognition** (the idea that human knowledge and cognitive processes extend beyond the individual mind to include interactions with other people, tools, and environments).
Related to those concepts, I've also been thinking about how **digital gardens** (personal knowledge bases that blend note-taking, blogging, and knowledge management in a non-linear format) and **Zettelkasten** (a method of note-taking where ideas are captured as atomic notes with unique identifiers and explicit connections) are ways to capture and organize knowledge.

One other thing that I'm amazed by is the powerful concept of a **hyperlink** and how it makes the web open, decentralized, and interoperable. When paired with the **semantic web** (an extension of the web that provides a common framework for data to be shared across applications and enterprises), you have yourself a decentralized knowledgebase containing a lot of the world's knowledge.

At some point, IPFS (InterPlanetary File System, a protocol designed to create a permanent and decentralized method of storing and sharing files) joined this pool of concepts I had in my head.

These were all interesting concepts individually, but I knew there were connections but couldn't cohesively bring them together. That's where AI-assisted specification design came in.

Below is a summary of the collaborative design interaction with Claude Sonnet 3.7 (with web search) that eventually led to the generation of the IPKS specifications. I haven't combed through them in great detail, but what they're proposing seems plausible. 

Overall, I'm fascinated by this interaction. Whether or not IPKS ever becomes a reality, the process of using AI to transform abstract concepts into concrete specifications seems like a valuable and fun design approach that I'll continue to refine and include as part of my vibe-coding sessions. 

---

## Initial Exploration: IPFS and Knowledge Management

Our conversation began with exploring IPFS (InterPlanetary File System) and its fundamental capabilities as a content-addressed, distributed file system. We recognized that while IPFS excels at storing and retrieving files in a decentralized manner, it needed extensions to support knowledge representation, trust, and semantics.

Key insights from this stage:
- IPFS provides an excellent foundation with content addressing through CIDs
- Content addressing enables verification but doesn't inherently provide meaning
- Moving from document-centric to idea-centric systems requires additional layers

## Knowledge Management Concepts

We explored established knowledge management approaches, particularly:

### Zettelkasten
The Zettelkasten method contributed these important principles:
- Atomic units of knowledge (one idea per note)
- Explicit connections between ideas
- Unique identifiers for each knowledge unit
- Emergent structure through relationship networks

### Digital Gardens
The Digital Garden concept provided these insights:
- Knowledge in various stages of development
- Non-linear organization prioritizing connections
- Evolution of ideas over time
- Public visibility of work-in-progress thinking

These personal knowledge management approaches helped us envision how similar principles could work at scale in a distributed system.

## The "K" in IPKS

When we proposed replacing "IPFS" with "IPKS" (changing File → Knowledge), we recognized the need to define what makes knowledge different from files. This led to identifying several key requirements:

1. **Semantic meaning** - Knowledge needs explicit relationships and context
2. **Provenance and trust** - Knowledge requires verifiable sources and expertise
3. **Evolution** - Knowledge changes over time while maintaining continuity
4. **Governance** - Knowledge exists in various trust and privacy contexts

These requirements shaped the layered architecture of the specifications.

## Distributed Cognition and Non-Linear Publishing

Our discussions about distributed cognition highlighted how thinking processes extend beyond individual minds to include:
- Interactions with other people
- Cultural artifacts and tools
- Physical and digital environments
- Social and technological systems

This concept directly influenced the IPKS design by emphasizing:
- Knowledge as a collective, distributed resource
- The need for attribution and expertise verification
- The value of connecting knowledge across boundaries
- The role of tools in extending human cognition

Similarly, non-linear publishing concepts shaped how we approached knowledge relationships and navigation in IPKS, moving away from sequential formats toward interconnected networks of information.

## Web3 Technologies Integration

Our exploration of complementary technologies led to incorporating:

### Decentralized Identifiers (DIDs)
DIDs provided the framework for:
- Self-sovereign identity for knowledge contributors
- Cryptographic verification of authorship
- Persistent identification across systems
- Privacy-preserving selective disclosure

### Verifiable Credentials (VCs)
Verifiable Credentials offered mechanisms for:
- Expertise validation without central authorities
- Domain-specific qualification verification
- Credential-based access control
- Trust frameworks for knowledge contributors

### Semantic Web (RDF/OWL)
Semantic Web standards influenced:
- Relationship types between knowledge nodes
- Ontologies for domain knowledge representation
- Query patterns for knowledge discovery
- Interoperability with existing knowledge systems

## Business Context: Supply Chain Example

Our conversation about supply chain management provided a concrete use case that helped ground the specifications in practical application. This example demonstrated how IPKS could address real-world challenges:

- **Material Provenance**: Using DIDs and verifiable credentials to establish trusted material sources
- **Cross-Organization Collaboration**: Enabling knowledge sharing while respecting organizational boundaries
- **Regulatory Compliance**: Creating verifiable documentation of compliance requirements
- **Expertise Validation**: Ensuring contributors have appropriate qualifications for their roles
- **Selective Disclosure**: Balancing transparency with competitive confidentiality

This business context helped shape the Access Control &amp; Privacy specification in particular, highlighting the need for nuanced governance models.

## Technical Implementation Considerations

As we moved from abstract concepts to specifications, several technical considerations emerged:

1. **Building on IPLD**: Recognizing that InterPlanetary Linked Data (IPLD) already provided foundational components for structured, linked data in content-addressed systems

2. **Modular Specification Design**: Choosing to create multiple specifications rather than a monolithic standard to enable incremental implementation and adoption

3. **Backward Compatibility**: Ensuring IPKS could work with existing IPFS/IPLD infrastructure

4. **Extensibility**: Designing for future enhancements like AI integration, advanced semantic capabilities, and cross-domain knowledge mapping

## The Path Forward

The IPKS specifications represent a synthesis of our conceptual exploration, grounded in:
- Established knowledge management practices
- Decentralized web technologies
- Real-world business requirements
- Technical feasibility considerations

Moving from concept to implementation will require:
1. Reference implementations of the core specifications
2. Developer tools and libraries to simplify adoption
3. Domain-specific extensions for particular use cases
4. Community building around open standards

By building on the combined strengths of IPFS, DIDs, VCs, and semantic web technologies, IPKS creates a framework for distributed knowledge that balances openness with trust, flexibility with verification, and collaboration with governance.
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/vibe-specing-prompt-to-spec</link>
      <guid>https://www.luisquintanilla.me/posts/vibe-specing-prompt-to-spec</guid>
      <pubDate>2025-05-06 20:40</pubDate>
      <category>dweb</category>
      <category>standards</category>
      <category>ai</category>
      <category>decentralization</category>
      <category>protocol</category>
    </item>
    <item>
      <title>Fractures of Unfamiliarity &amp; Circumvention in Pursuit of a Nice Time and the Meaning of Franklin's Tower</title>
      <description>&lt;![CDATA[
The past few days, I've had the song Franklin's Tower by The Grateful Dead on repeat. I found some of the lyrics could be taken at face value but others didn't make sense to me. Since I assumed there was a deeper meaning, I decided to look for interpretations and ran into this [article by David Dodd on the official Grateful Dead website](https://www.dead.net/features/greatest-stories-ever-told/greatest-stories-ever-told-franklins-tower).  

This led me to two amazing essays:

- [The Fractals of Familiarity and Innovation: Robert Hunter and the Grateful Dead concert experience](https://web.archive.org/web/20240604001611/http://artsites.ucsc.edu/gdead/agdl/jurgen.html) by Jurgen Fauth
- [Fractures of Unfamiliarity &amp; Circumvention in Pursuit of a Nice Time](https://web.archive.org/web/20160519052524/http://artsites.ucsc.edu/GDead/agdl/fauthrep.html) by Robert Hunter

Hunter's essay is a response to Fauth's. Also, in Hunter's essay you begrudgingly get a detailed breakdown of the lyrics of Franklin's Tower. I'll note how amazing it is that both of these individuals took the time to write these lengthy essays clearly exposing their ideas and points of view on the subject.  

Both of those articles were difficult to find because sadly the pages no longer exist in the website that had posted them. 

After some digging though, I was able to find them on the Internet Archive. So for the sake of preservation, I'll post both of them here in full form. 

## The Fractals of Familiarity and Innovation: Robert Hunter and the Grateful Dead concert experience

```text
Robert Hunter has been the primary lyricist for the Grateful Dead ever since he mailed "St. Stephen," "China Cat Sunflower," and "Alligator" to the band from New Mexico in 1967. Born in 1941, Hunter grew up in California and studied drama for a year at the University of Connecticut, before he spent half a year with the National Guard and later signed up for the same psychedelic research that led Ken Kesey to write One Flew Over The Cuckoo's Nest. His songwriting collaboration with Jerry Garcia and, occasionally, other members of the Grateful Dead yielded such classic Dead tunes as "Truckin'," "Touch of Grey," and "Dark Star". Hunter's lyrics emblazon thousands of tie-dyed t-shirts and skeleton-adorned bumper stickers. Other work of Hunter's includes lyrics for Zero and Bob Dylan as well as a translation of Rainer Maria Rilke's Duino Elegies and volumes of poetry, most recently Sentinel.
When analyzing song lyrics, certain basic differences from poetry have to be understood and observed. When comparing popular song lyrics to contemporary or classic poetry, it is difficult not to miss a striking difference in quality--song lyrics generally tend to be outspoken and trite where poetry clouds itself in ambiguity and complex imagery. Bill Flanagan argues that:

"Rock &amp; roll musicians are strange artists. Their code of manners dictates that they not appear to take themselves very seriously, while the best of their work contains all the passion, imagination, and craft of legitimate art. The intelligent rock musician is put in the position of working as hard as a poet or painter, while being expected to accept his gifts with the self- deprecating shrug of the idiot savant." (Flanagan, ix)
However, the majority of rock lyricists aims at an audience which is supposed to recognize a song instantly during the few weeks it is played heavy-rotation on FM radio and then forget it as soon as possible. Rock lyrics tend to make a point quickly and to make sure that it is understood. Therefore, they reach for clichéd imagery and blatant statements of emotions that traditional poetry would have tended to skillfully embed in high-flung imagery, a tendency which can lead to examples as inane as the ones parodied in Donald Barthelme's "How I Write My Songs."
One of the reasons, however, why such a direct comparison of lyrics and poetry is not fair is not only to be found in the consumer culture pressures that surround the pop music industry, but in the fact that popular music lyrics are not meant to be read at all. Obviously, the text has to be read within the framework of the music it was written for. This holds especially true for Robert Hunter's lyrics, because the unique nature of the music of the Grateful Dead adds layers of meaning to the text and vice versa. A further point I decided to take into consideration is that the Grateful Dead are not successful as a recording band that scores top ten hits (with the notable exception of the 1987 "Touch of Grey"); their studio albums are usually do not fulfill expectations. The strength of the band is unquestionably the live concert--the Grateful Dead have been the number one grossing touring band over the last couple of years, outdoing the likes of Madonna and Paul McCartney, without releasing a new studio album since Built to Last in 1989. This means that the usual way Hunter lyrics are being perceived is at a concert. Therefore, everything that happens at a Grateful Dead concert should be taken into account when reading the lyrics. In this paper, I will try to show how the Grateful Dead concert experience and the lyrics of Robert Hunter's songs interweave and complement each other, mutually adding significance and "meaning" to one another.

One model on how to understand the mutually reinforcing structure of a rock concert can be abstracted from Erika Fischer-Lichte's The Semiotics of Theater. Of course, according to Fischer-Lichte's definition, a concert does not constitute theater; however, her theories can give valuable ideas for a way of looking at the experience of a concert. According to Fischer-Lichte, every sign that is given during the performance can be assigned to a group or system, and the sum of all used and received signs that either correspond or oppose each other in meaning allow an overall interpretation or understanding of the performance. The fourteen sign systems Fischer-Lichte enumerates are: sound, music, linguistic signs, paralinguistic signs, mimics, gestures, proxemic signs, mask, hair, costume, stage conception, stage decoration, props, and lighting. These all make sense in a rock concert environment, especially in more theatrical performances as they became common with stadium bands during the 1970s. An additional system of significance would be the song selection and order, which might be considered a subgroup of the music or linguistic groups. Audience participation should be included, as well. Unless a lot of spoken text is involved, the paralinguistics can not be considered significant, since the loudness, pitch, etc. of a song are inherent in the music and therefore do not justify a group of their own. Props are rare in rock concerts, as well, although some performers have been known to turn their instruments into significant objects, such as Liberace or Prince.

The uniqueness of a Grateful Dead concert lies in the emphasis on certain sign systems over others. While obviously almost every rock concert relies heavily and primarily on the music and the linguistical signs (which are, after all, our objects of interest), the Dead emphasize them more than usual by cutting down on the others--the stage decoration is colorful yet unobtrusive, there is no dancing or hardly any movement by the band members other than what is necessary to play the instruments, the clothing of the band is purely functional and they stopped conversing with the audience years ago. The lighting, designed by Candance Brightman since 1972, is not self-conscious but supports the music:

"Brightman's lighting is extraordinarily sensitive to subtleties in the band's interactions, shunning bombastic effects for a vivid focusing of attention that is as much a part of the jam as the music itself" (Shenk 28).
Furthermore, it includes the audience by
"[breaking] down the barriers between the Dead and its fans by lighting the band so they can still see the audience, or by lighting the audience itself" (Shenk 29).
An important point to note is that none of the sign systems function to enhance the star status of the band-- everything is focused on the music and the relation to the audience. The absence of proxemic signs, mask, hair, props, and costumes is significant in that it enhances the emphasis on the music and the lyrics while taking away from the special status of the band members.

The order of songs, though, is of exceptional importance during a Grateful Dead show. Traditionally, since about 1977, the show is split into a first and a second set, where the first set is characterized by more conventional songs that are played without too much improvised jamming; the length of the songs stays within reasonable limits as well. The second set, in contrast, features many improvised jams that stretch the songs easily out to fifteen, twenty minutes. At some point in the second set, Mickey Hart and Bill Kreutzman (percussion &amp; drums) will take over and play an extend drum solo while everyone else leaves the stage. Before they are finished, the other band members return, and Hart and Kreutzman leave the stage. What follows is an extended, beatless jam of exploratory sounds known as "Space." Bob Bralove, in the liner notes to Infrared Roses, describes it like this:

"Somewhere in the middle of the second set of every Grateful Dead show the band turns a corner. They enter a musical environment without walls or structure. The song form is abandoned, and the very elements of music may be called into question. The only mandate is to explore new territory. It is an environment where rhythm, tone, color, melody, and harmony can be explored without rules or predetermination, a musical adventure where composition and performance are one."
After "Space," the band will play three or four more songs, commonly ending with a high- energy song like "Good Lovin'" or "Sugar Magnolia," and come back for a one-song encore. This pattern forms a relatively firm structure in which highly unstable elements are put--the music is always improvised, and not one song has been played twice the same way, and the actual list of songs played always varies, to the point where not even the band members know what they will play before they go on stage.

This is of high importance because the underlying structural principle will become evident in Hunter's lyrics, as well. Since certain songs are usually played in the first set and others in the second set, it seems reasonable to group them according to this differentiation and begin by taking a closer look at the first set songs.

"Jack Straw" is a classic first set song--out of the 424 times it had been played between 1971 and 1992, DeadBase lists only 41 second-set appearances. The song appears to be a narrative of a criminal character who drifts through the American wilderness, jumping trains and digging graves while eagles circle high above. It seems to be rooted deeply in an American tradition of fireside songs. The same holds true for "Dire Wolf," a bitter story of death and gambling, precisely set in time and space:

In the timbers of Fennario
the wolves are running 'round
The winter was so hard and cold
froze ten feet 'neath the ground (1-4)
The narrative features virtually no metaphors or similes; everything seems to happen at face value--a complicated search for meaning or "interpretation" is unnecessary. Again, the song is closer to traditional American music than love songs heard on the radio. It is direct yet not necessarily blatant or trite since the subject matter is uncommon for contemporary American rock lyrics.

The Grateful Dead classic "Truckin'" features some opaque references, yet the song is obviously trying to capture the vicissitudes of a life on the road as the band members have experienced it. Some lines, like "Busted--down on Bourbon Street" (37) refer to commonly known incidents (in this case the band's drug bust in New Orleans); others like "What in the world ever became of sweet Jane?" (25) are probably in-jokes that aren't meant for a broad audience to grasp.

Even more clearly an American song in the best tradition of folk singers like Woody Guthrie or Phil Ochs is "Cumberland Blues," a song depicting the plight of a mine worker. Hunter's eye in describing his topic was so sharp that he heard of an actual Cumberland miner wondering "what the guy who wrote this song would've thought if he'd ever known something like the Grateful Dead was gonna do it." He considers this the best compliment he ever received for a lyric (Hunter 52), obviously because it certifies an authenticity he aimed for.

It seems that first set songs commonly share a genuine American topic (the open road, the wilderness, unions, etc.) and often sketch a narrative world including characters and situations. Instead of propagating emotions, as rock lyrics often seem to, they focus on fictional events that often involve gamblers, cowboys, drunkards, and outlaws. The imagery and "lyrical" makeup of the songs tends to be simple so as to give a definitive, discernable "meaning."

An investigation of second set songs quickly shows that they are of a different character. "China Cat Sunflower," a typical second-set song since the early eighties, may serve as a prime example: the lyrics refuse any narrative or symbolic reading, the words simply are. Hunter acknowledges the non-rational nature of the song with his tongue-in-cheek remark: "Nobody ever asked me the meaning of this song. People seem to know exactly what I'm talking about. It's good that a few things in this world are clear to all of us" (Hunter 35).

Nonsensical as they may be, the lyrics bear a close resemblance to the dreamy, colorful playfulness of Lennon/McCartney's "Lucy in the Sky with Diamonds"--

Picture yourself in a boat on a river,
With tangerine trees and marmalade skies
Somebody calls you, you answer quite slowly,
A girl with kaleidoscope eyes.
Cellophane flowers of yellow and green,
Towering over your head.
Look for the girl with the sun in her eyes,
And she's gone.
Lucy in the sky with diamonds (1-9)
In the surrealistic, candy-colored playfulness of both songs, the words are not meant to "mean" anything; the usual tools of interpretation have to fail here. As Hunter says, "I like a diamond here, a ruby there, a rose, certain kinds of buildings, vehicles, gems. These things are all real, and the word evokes the thing. That's what we're working with, evocation." (Gans 26). His concern is not merely narrative or mood anymore, but evocation and association, something much closer to poetry than to the common clichés of pop lyrics.
"Crazy Fingers" is another example for this approach to songwriting--it is content with letting its verses set a mood without making any "sense." Hunter calls it a "collection of haiku- style verses, mostly seventeen syllables, some more successful than others, with no connecting link other than similarity of mood" (Hunter 45). Some of the imagery is indeed not very original; the "Deep Sea of Love" (7) can surely be encountered elsewhere. Nonetheless, certain paradoxical images (like "Recall the days that still are to come" (3)) and the overall disjunctive structure of the text make for a strange, unlikely song that refuses to be "understood" on any intellectual level. It can only be described in its emotional impact, the longing and bittersweet reminiscing it evokes in a unique way.

Similar points can be made for "Eyes of the World"-- it is interesting to note that the lyrics seem to make sense, yet on any "objective" level, they do not. Traditional analysis of the text may or may not yield satisfactory results by assigning connotations and significance to, say, the "redeemer"(22) , the mile-long wings (7) or the "eyes of the world" themselves; fact is that the inherent instability of the lyrics, fueled again by interior contradictions like "...the seeds that were silent / all burst into bloom and decay," (26-27) will not allow for a final, reductive interpretation. It is clear that Hunter does not aim to communicate "meaning" at all; he says, "I'd really prefer not to get into tearing apart the symbology of my songs [ . . . ] and I'll tell you why: symbols are evocative, and if there were a more definite way to say things, you'd say them that way. A symbol, by it's very nature, can pull in many, many shades of meaning, depending on the emotional tone with which you engage the piece" (Gans 23).

The lyrics of songs commonly played as encore or to close the second set are often more direct (as "Sugar Magnolia") or soothing lullabies ("Brokedown Palace").

The course of a Grateful Dead concert, then, can be said to describe an arc that ranges from traditional, narrative lyrics that concur with conventional playing to outrageous flights of fancy in the second set, correspondingly in the musical jams as well as in the ambiguity-laden, irrational, highly evocative lyrics. The strangeness climaxes during the purely instrumental "Space," during which the band is at its most exploratory. The last songs and the encore return the audience to a festive or pensive mood. However, the function of the lyrics during a Grateful Dead concert is more than to simply underline the music--the linguistic signs are not present to emphasize what is happening in the music. Instead, there seems to be a strange interplay between familiarity and newness, and it is at this edge of the well-known and the innovative that the Grateful Dead draw energy. Audience response is a valuable key to this aspect of the music and the lyrics.

Experience and my subjective impression suggests that the audience at a Grateful Dead show is usually more responsive and alert than audiences at other rock concerts, but since there is no data to prove or argue this point, it can not be made in a meaningful way. What is observable and obvious, though, are the immense cheers and shouts that erupt at certain points in the concert, namely when the band returns from a long stretch of interpretation to the melody (and therefore, mostly, the lyrics), and at certain catchphrases within the lyrics that have gathered almost mystical meaning over the years. Both phenomena need to be looked at more closely.

In the improvisational style of the Grateful Dead ("jazz syntax with a rock lexicon," as David Gans calls it (ix)), melody as one of the markers signifying a song is subject to sudden change-- at its best, the band composes on the stage, making up new elements on the go; the process of creating becomes visible to the audience. Part of the philosophy that allows this approach is that mistakes are possible, probable, and excusable-- "on a bad night, they bore you to tears, but a good night makes it all worth it." (Gans iix) The initial melody of the song, however, is not variable, and it is always safe to predict the band's return to it after a period of jamming that is so utterly unpredictable in direction, style, and length that not even the band itself can foresee what will happen: "I can't predict what the band is going to do at any given moment" (Jerry Garcia quoted in Gans 59).

Since the lyrics will be sung only once the proper, initial melody has returned, the counter argument is mostly true, also--when the melody returns, so will the lyrics. The audience cheers at these points because the return of the melody and therefore the lyrics means a return to the familiar, to the predictable and understandable after long forays into the unknown.

While this holds true for virtually any return to the preestablished, well-worn structural elements of the song, certain catchphrases within Hunter's lyrics seem to yield an especially enthusiastic response by the audience. A good example for this is "If you get confused, listen to the music play" (20) from "Franklin's Tower", a line that taken by itself offers guidance to a perplexed listener. Yet it is embedded within a song that is as obscure as a lot of second set- lyrics; "Franklin's Tower" is evocative yet of undeterminable meaning. The above-mentioned catchphrase-line is almost hidden within the song and has to be read out of context to prompt the joyful response it usually gets.

More obvious examples of the embedded catchphrase are to be found in first-set songs like "Loose Lucy," a rare Hunter love song, where the audience chooses to take the chorus out of context by applying it to the band itself: "Singing: Yeah, yeah, yeah, yeah / singing: Thank you / for a real good time" (7-9). Here, the function seems to be that of a mutual pat on the back; the cheers represent the subculture of Deadheads and the Grateful Dead acknowledging each other's faithfulness--it is here that it becomes manifest that the "Dead made a mystical pact with their fans, vowing to carry the psychedelic torch as long as they can play their music" (Lee 290). Whether Hunter had such an opportunity for unique bonding in mind when he wrote the lines remains undetermined, yet the authors of Skeleton Key could not but remark on Hunter's "knack for coining aphorisms that yield hard-won truths after hundreds of hearings" (150). The most famous example is certainly the oft-quoted "Lately it occurs to me / what a long, strange trip it's been" (23-24) from "Truckin'," a line that paradoxically admits to the uniqueness of the experience by being its most clichéd example.

It seems that a lot of these phrases took on the meaning for the Deadhead community that they did because they were obvious and understandable. On a paradigmatic level, it should be surprising that a band that is so directly associated with a certain lifestyle and a set of beliefs as the Grateful Dead hardly convey any "messages" at all in their lyrics. It can be safely assumed that one would generally expect a "hippie band" with all the unavoidable stereotypes, to be much more blatant about their convictions. Yet, as the above analysis already shows, Hunter usually confines himself to a narratives of traditional American adventures in the broadest sense and more abstract, lyrical, and image-laden texts that only yield "meaning" by association and evocation.

However, there are lines hidden within his lyrics that have taken on very charged connotations exactly because they seem to be more outspoken and direct, and a philosophy can be attributed to them. All these lines have turned into catchphrases as well, cheered at concerts and printed on countless t-shirts and bumper stickers.

"Scarlet Begonias," a song commonly played to open the second set yet at least somewhat narrative in that it has a narrator and a female character, yields two instances of embedded catchphrases-- "Once in a while / you get shown the light / in the strangest of places / if you look at it right" (31-34) and "Strangers stop strangers / just to shake their hand" (46-47). The philosophies that can be abstracted from these lines by reading them out of context include the search for truth in unlikely places, maybe in the tradition of the "angel-headed hipsters" (2) who roam the urban wasteland in Ginsberg's "Howl," and a proclamation of unsolicited friendliness and openness that the stock and trade of the hippiedom the Grateful Dead supposedly stand for. All this is abstracted again from the less direct context the lines originally appear in; the Deadhead community seems to have taken what it could and abandoned the quirky senselessness of "The wind in the willows played tea for two / The sky was yellow and the sun was blue" (44- 45). The argument could be made that the aphoristic lines are not only cheered at because of what they mean and imply, but simply for the fact that they mean at all. They are familiar lines within an ambiguous lyrical world that refuses to be reduced to a discernable, single interpretation. The catchphrases are familiar lines within a familiar yet, to the rational mind, unyielding lyrical world. "Scarlet Begonias" refuses to be understood as a whole, so the audience jumps at those four lines that seem to make sense. The catchphrases, then, are to the rest of the lyrics what the melody is to the improvisational jam--they mark the return to an understandable, well-known, and familiar touchstone before the next journey into newness and obscurity.

This phenomenon can probably be best understood when looking at what is generally considered the prime example of the Grateful Dead's efforts, "Dark Star":

" "Dark Star" is the kernel of wide open possibility at the core of the Dead's repertoire, the essential seed promising unlimited intergalactic space journeys at the speed of total mindwarp: "Shall we go,/ you and I/while we can?" It is the most exploratory of Dead tunes and it is the trippiest, the one where the acidic whistle of the dark interstices is heard most starkly, where you might turn any corner and step off into the void. It is the Dead's spirit of musical adventure at its strangest, wildest, most vehement, weirdest, dreamiest, and the place where the leading wave of the Dead's music is most often audible being created in mid-air." (Jim Powell quoted in Shenk 51)
Interestingly, "Dark Star" was the first song that Hunter wrote together with the Grateful Dead in 1967. The lyrics are extremely short, considering that a full-fledged "Dark Star" can easily last half an hour. The song is made up by two verses that are usually separated by minutes and minutes of intense jamming. A close reading reveals Hunter's technique of juxtaposing the abstract and the concrete in a single image, parataxis, and contradiction in full bloom. "Searchlight casting / for faults in the / clouds of delusion" (7-9) is such a tricky image, suggestion that the speaker has a very clear sense of what he is talking about, yet again, the song cannot be said to "mean" anything in any traditional sense, and not even the tedium of a symbolic reading will yield any "meaning." However, the verse beginning "Shall we go..." which precedes the next drawn out jam insinuates that the speaker understands, and invites the reader along through the "transitive nightfall / of diamonds" (14-15). The drawn-out jam that usually follows can be understood as exactly the "transitive nightfall / of diamonds," the lyrics then talking about the music, a kind of meta-lyric. After seemingly endless improvisation, the text and melody returns with "Mirror shatters" (16), an event cheered by the audience as it means the return to a familiar signpost.
It has been argued that "Dark Star" draws its power from the violation of norms and the constant impossibility of assigning meaning:

" ['Dark Star' leads] to a transcendent mysticism because the music, continually violating norms and expectations, avoid a clear marking or denotation of a signified. What is left is a state of ambiguity in which the listener is enticed into closer attention of momentary detail." (Skaggs, quoted in Shenk 64)
I would argue that the overwhelming power of "Dark Star" and, in a broader sense, the overall effect of a Grateful Dead show, does not entirely rely on the elusiveness of its signs, be it musically or linguistically, but on the tension it creates between the familiar and the new. The Deadhead community as a whole exists because of the familiar aspects of the Dead. The fascination of the band grows largely out of the relationships between the improvised and the structured that are increasingly complex to the point of becoming fractal.
The model that I would suggest for the intricacies of a Grateful Dead show and the rich relationships that exist between the lyrics and the music is that of fractals, self-containing geometrical structures like the Mandelbrot set. The outside of the Gingerbread man are the ordered regions, where a number can easily be assigned to the function, z (n+1)=z(n)2+c, while the black inside of the Gingerbread man stands for total, absolute chaos, unstructuredness. The edge where those two areas meet is characterized by intense, bizarre beauty and recurring images of the bigger picture that recede--like a Russian doll, the Gingerbread man contains images of himself, infinitely, just as a Grateful Dead concert contains recurring patterns of familiarity and innovation on a receding scale. The overall structure of the concert is firm, or almost firm--there are always exceptions, yet the pattern of first and second set, drums, space, and encore are set. This structure is filled with actual songs every night; the choice of songs varies, yet the songs are generally well-known and familiar.

The way they are going to be played on any given night, however, is uncertain and actually unpredictable--no improvisational jam that opens in preconceived places within the song will be the same, yet it can be assumed it will be at a given place. The lyrics, joining the return to the melody, are firm and part of a preconceived pattern within a song. They will not be changed or tampered with. However, the firm and unchanging lyrics have shown themselves to be unsteady and undecidable due to their ambiguous and contradictory character. This is only true, again, for the most part, since the lyrics do contain embedded catchphrases that possess a heightened familiarity within the song. The audience will recognize and appreciate these moments of predictability and profound understanding as a community before the jam wanders off into unknown directions again, possibly into the far reaches of musical exploration in "Dark Star" or to the completely strange and wordless "Space," which can be considered the climax of unfamiliarity within the development of the concert. After the purely instrumental excesses of "Space," the return of rhythm, harmony, melody, then lyrics, and finally understandable and rational lyrics is greeted by enthusiastically by the audience which is pleased to be gently returned to a sensical universe where exploration and rediscovery, departure and return, ambiguity and blatantness co-exist and fuel each other. Hunter's lyrics are allowed to be reasonably narrative and plain-old weird at given times; the music always corresponds to or counter-accentuates the words, both live off each other in an organic and healthy way. Standing on their own, the lyrics to "Dark Star" might seem interesting and lyrical yet pointless; in union with the music, they gather as many meanings as the listener can assign to them:

[Jerry Garcia:] "Dark Star" has meant, while I'm playing it, almost as many things as I can sit here and imagine, so all I can do is talk about 'Dark Star' as a playing experience. [Charles] REICH: Well, yeah, talk about it a little. [Jerry Garcia:] I can't. It talks about itself. [Charles] REICH: Each time it comes out in a different way? [Jerry Garcia:] Yeah, pretty much. There are certain structural poles which we have kind of set up in it, and those periodically we do away with. (Garcia 85)

The music of the Grateful Dead and the lyrics of Robert Hunter open up spaces within the work of art they form together that make textual interpretation of the lyrics futile. The lyrics function not as an absolute that is stated, but as a tool for semiosis--at every concert, they can and do mean something different according to how the variable parts in the music change. Every time a song is performed, a different set of potential meanings that are inherent in the piece will be actualized for a different audience. Various parts of the concert experience are firm, others are variable, and as I tried to illustrate using the Mandelbrot set, the firm parts contain further movable aspects, and vice versa. The resulting multi-media, multi-sign system artifact can shine in infinite variations. The result is one of an ever-changing, fluctuating whole that contains familiar parts while never completely yielding to reason. This amorphous, continuously self- altering gestalt that is a thirty-year process --the seams show, there are parts that are completely opaque and others that we have known for decades, and it always changes: "It stumbles, then it creeps, then it flies with one wing and then it flies with one wing and bumps into trees and shit" (Garcia 76). Deadhead sociologist Rebecca Adams puts it this way: "People say Deadheads are throwbacks. I think they are pioneers. They recognize that reality is subjective--there is no right way--and have been cognizant of these multiple realities for a lot longer than most other people. This is post-modernism. It's the cutting edge" (quoted in Shenk xiii). Robert Hunter's lyrics are a vital part of this experience, providing the mind-searching music with an incessantly fascinating imagery that both underscores and counter-points its meaning by not meaning anything at all.

Works Cited
Barthelme, Donald. "How I Write My Songs." Sixty Stories. New York: Putnam, 1981. 418-423.
Bralove, Bob. Liner Notes. Grateful Dead. Infrared Roses. Grateful Dead Records, 40149-2, 1991.
Flanagan, Bill. Written in My Soul: Rock's Greatest Songwriters Talk About Creating Their Music. Chicago: Contemporary, 1986.
Fischer-Lichte, Erika. The Semiotics of Theater. Indianapolis: Indiana UP, 1992.
Gans, David. Conversations with the Dead: The Grateful Dead Interview Book. New York: Citadel 1991.
Garcia, Jerry, Charles Reich, and Jann Wenner. Garcia: A Signpost to New Space. San Francisco: Straight Arrow, 1972.
Hunter, Robert. A Box of Rain: Lyrics 1965-1994. New York: Penguin, 1993.
Scott, John W., Mike Dolgushkin, and Stu Nixon, eds. DeadBase VII: The Complete Guide to Grateful Dead Song Lists. Hanover, NH: 1993.
Lee, Martin A. and Bruce Shlain. Acid Dreams: The Complete Social History of LSD: The CIA, the Sixties, and Beyond. New York: Grove Weidenfeld, 1992.
Lennon, John and Paul McCartney. "Lucy in the Sky With Diamonds." The Beatles. Sgt. Pepper's Lonely Hearts Club Band. EMI, CDP 7464422, 1967.
Shenk, David and Steve Silberman. Skeleton Key: A Dictionary for Deadheads. New York: Bantam, 1994.
Skaggs, S. "The Transitive Nightfall of Diamonds: Eco and the Aesthetics of Transcendence." University of Louisville, Kentucky. Presented at the Conference of the Semiotic Society of America, St. Louis, Missouri, 1993.
```

## Fractures of Unfamiliarity &amp; Circumvention in Pursuit of a Nice Time

```text
Dear Jurgen,

meaning is not an irreducible Ur-language. A good lyric is allusion, illusion, subterfuge and collusion. A poor lyric is information about its own paucity of resource. That doesn't mean the latter cannot be a great song. There is nothing inherently better about a dumb song than one which calls attention to the intelligence of its writer. It's a matter of taste, but meaning is often a subterfuge to distract the listener's attention from a writer's lack of multiple resources. This is often true of blatant "message" songs.
How does a song mean?
As long as allusions can be codified, the semiotician is content, knowing that "meaning" is a case-sensitive term with scant referentiality outside implementation of primitive needs. When the semiotician suspects allusiveness without corresponding exact reference, he charges the poet with nonsense. Nonsense is a loaded word, the meaning of which is unclear. If it is understood as "intentional multi-referentiality without predetermined hierarchy" rather than "meaningless blather" one would find no fault with the term. But it isn't, so the charge of "nonsense" and "meaninglessness" levied by a scholarly and plausible source, does much to put people off exploring further.
"The repair sad mutthead forkbender orange in the how are you, did they?" is nonsense without reference, hence not allusive. Had it discernible rhythm, it might be termed 'rhythm allusive' but my example lacks even that. The only way on earth such a sentence would likely get written is as an example of a null allusive set. Bingo! An allusion! To Set Theory! The exception that proves the rule.


How does a song mean?
The meaning(s), or lack thereof, ascribed by others to an example of lyric work are not part of the work. The interpretations are separate "works". The manner in which an audience receives the work, what they, collectively and individually, make of it, can indeed provide potential data for the allusiveness (referentiality)of future lyrics, gainsaid, but cannot be ascribed as a characteristic of the particular work, per se, with validity without "insider information" which is, in any case, no part of the song. That way lies true nonsense, even unto deconstruction. Yet the little bugger of a jingle persists and seems to move hearts. Why? Is there something which semiotics, by its nature and presuppositions, must exclude from the sphere of "meaningfulness" due to the limited nature of its tools?

How does semiotics mean?
Since the concluding remark of your essay stated that the Grateful Dead songs are "meaningless" I choose to reply by explicating one of your examples: "Franklin's Tower." I do this reluctantly because I feel that a straightforward statement of my original intent robs the listener of personal associations and replaces them with my own. I may know where they come from, but I don't know where they've been. My allusions are, admittedly, often not immediately accessible to those whose literary resources are broadly different than my own, but I wouldn't want my listeners' trust to be shaken by an acceptance of the category "meaningless" attached to a bundle of justified signifiers whose sources happen to escape the scope of simplistic reference.

How does the song go?
FRANKLIN'S TOWER

In another time's forgotten space
your eyes looked through your mother's face
Wildflower seed on the sand and stone
may the four winds blow you safely home

4x: Roll away... the dew

You ask me where the four winds dwell
In Franklin's tower there hangs a bell
It can ring, turn night to day
Ring like fire when you lose your way

Roll away... the dew . . .

God help the child who rings that bell
It may have one good ring left, you can't tell
One watch by night, one watch by day
If you get confused just listen to the music play

Roll away... the dew . . .

Some come to laugh their past away
Some come to make it just one more day
Whichever way your pleasure tends
if you plant ice, you're gonna harvest wind

Roll away... the dew . . .

In Franklin's Tower the four winds sleep
Like four lean hounds the lighthouse keep
Wildflower seed in the sand and wind
May the four winds blow you home again

4x: Roll away... the dew
You better roll away the dew
---------------------------------
What's it about?
EXPLICATION:
In another time's forgotten space
your eyes looked through your mother's face
Wildflower seed on the sand and stone
may the four winds blow you safely home.

&lt;surface intent&gt;
[You have your mother's eyes, child,
the very shape, color and intensity
of the eyes that looked through
her face so long ago. Borne on the
varied winds of chance and change,
like a dandelion seed, you may find
yourself deposited on barren soil.
My wish for you is that the forces
that brought you there may sweep
you up again and bear you to fertile ground.]

&lt;deeper intent&gt;
"In another time's forgotten space
your eyes looked through your mother's face."
[Relative immortality of the human
species is realized through reproduction.
Dominant traits inherited from an ancestor,
the lyric suggests, share more than mere
similarity with those of the forebear,
but are an identity, endlessly reproducible.
In other words, when someone says
"You have your mother's eyes"
they are not speaking in simile
nor would it be incorrect to say
that "your mother has your eyes,"
if, in fact, possessiveness is
an appropriate term in the context.
Poetic license will assume it is,
if only for the sake of moving on
to the next couplet.]

You ask me where the four winds dwell
In Franklin's tower there hangs a bell
It can ring, turn night to day
Ring like fire when you lose your way

[note that this song appeared in 1975,
the year after my son was born and the
year before the American Bicentennial.
Both facts are entirely relevant. The
allusion to the Liberty Bell and the
situation of the Philadelphia Congress
in the hometown of Ben Franklin has not
gone unnoticed by other commentators.
This song is a birthday wish both for
my son and for my country, each young
and subject to the winds of vicissitude.
Individual and collective freedom,
liberty, conscience, all that is conjured
by those concepts, is suggested
in the image of the tolling bell.]

God help the child who rings that bell
It may have one good ring left, you can't tell
One watch by night, one watch by day
If you get confused just listen to the music play

[The Bell, rung once, cracked and could not
be safely rung again. From an actual bell,
it therefore became a symbol of the
potential to ring. The single toll, signaling
birth, can now be heard only in its
reverberations in our history and ideals.
Some have had to bear those ideals in
difficult circumstances (war, the Great
Depression and general benightedness)
others have had the more enviable task
of keeping watch (eternal vigilance)
during periods of conscious and dynamic
change: the full light of day. The sixties,
the writer assumes, were such a time.
You can't tell if ringing that bell
a second time would destroy it in
the act of producing another mighty
peal and it might be foolish, if courageous,
to try. Perhaps the "music"of
the original ideals symbolized by
the first and only toll should be taken
to heart and implemented, rather than
obviated by a new source of ideation
(communism, anarchy, religion based
governmental apparatus. etc.) To resolve
this confusion, pay attention to the
original inspiration (the Constitution,
the Bill of Rights, collectively.
Individually, maintain awareness of
conscience and one's own early ideals.]

Some come to laugh their past away
Some come to make it just one more day
Whichever way your pleasure tends
if you plant ice, you're gonna harvest wind

[This verse scarcely needs commentary
in light of the above remarks. The precursor
to the 1st couplet is "I Come for to Sing" as
performed, possibly written, by Pete Seegar.
The 2cnd couplet source is the biblical
"Who sows wind reaps the hurricane."]

In Franklin's Tower the four winds sleep
Like four lean hounds the lighthouse keep
Wildflower seed in the sand and wind
May the four winds blow you home again

[We assume a bell tower for the great bell.
By the trope of simile, we see the bell tower
(the day watch) turned to a lighthouse
and the four winds become sleeping hounds,
(the night watch) worn out by the events
of such a metaphorical day as related
by e.e.cummings in his familiar lyric
"All in Green Went My Love Riding"
(Poem 6 from TULIPS AND CHIMNEYS)
"four lean hounds crouched low
and smiling . . ." By the use of quotative
allusion the lyric attempts to borrow
some of the emotive spark of cumming's
poem, providing a kind of "link button"
into a different but complementary space.
Allusion here functions as a sort of
shorthand cross-patch into a series
of metaphoric events which, with
a double-clutch shift of simile,
access a downloadable description of
the kind of day it's been for a 'wildflower
seed' in its adventures in the wind.
There may be some objection to the
elastic interchangeability of the similes
of hounds and winds in this set of couplets,
but the test of the allusion, as I see it,
is whether or not the appropriate emotions
are evoked to lead to satisfying closure
and an opening door on other possibilities.]

[Now to the real stretch: "Roll away the dew."
The line is appropriated from a fairly
well known sea chantey whose chorus goes:
"Roll away the morning dew
and sweet the winds shall blow."
As surely everyone knows by now,
Bonnie Dobson's song "Morning Dew"
(made famous by Garcia's singing of it)
is set in the aftermath of nuclear war.
Reason he can't "walk you out
in the morning dew, my honey"
is because of fallout, though Garcia
has wisely dropped the verse
containing this denouement, allowing
the song a heightened romantic mystery,
achieved through open-ended ambiguity.
For generations now alive,
the nuclear specter personifies
the forces which most threaten our
attempt at Jeffersonian democracy.
With the song's sub-allusion to
"Roll away the Stone," an anthem
of joyous Eastertide resurrection,
a resultant combination message
of dire necessity (as in the final:
you've got to roll away the dew)
and promise of renewal, in case
resolution is effected, are enjoined.
Should this hyper-allusive train
of thought become too confusing
to process, the invitation to just
"listen to the music play"
acknowledges both the melody
and performance context of the lyric
and the metaphoric bell described above.]

Well, now that you know what I meant by it,
it's no great shakes is it? Mystery gone,
the magician's trick told, the gluttony
for "meaning" temporarily satisfied,
one can now take issue with my intent
and avoid the song itself, substituting
the assignable significance for the music.

Attempts by language to overdetermine language are doomed out the door, so I content myself with providing these clues for threading the maze of "Franklin's Tower" and as a grudging key to my methods. I feel that much of what you've said in your essay is rich, correct and thought provoking and appreciate your accurate estimation of the concert context as adjunct to the lyric, and vice versa. The contextual sub-meaning (the way the song manifests itself in concert) is certainly a factor which occasionally determines certain choices in subsequent material. Too much of that would be a striving after sameness of effect, though (even if it does all sound the same to an uninitiated ear.)

Oh, one other thing: you labeled "what a long, strange trip it's been" as "cliched." Aren't you putting the cart before the horse? "Truckin'" was the originating vehicle of the phrase, which had not, to my knowledge, been coined before. The fact that it has entered the catchphrase banks of the language in a ubiquitous way may render subsequent usage cliched, but surely not the invention itself, unless all widely adopted phrases are deemed trite by virtue of their durability. You also mentioned that the "What in the world ever became of Sweet Jane?/She's lost her sparkle, you know she isn't the same" verse was probably an in-joke not meant for a broad audience to grasp. The intention was a parody of the '40's warning-style of singing commercial, specifically "Poor Millicent, poor Millicent/ She ne-ver used Pep-so-dent/ Her smile grew dim/ And she lost her vim / So folks don't be like Millicent / Use Pep-so-dent! " I'm sure that the allusiveness, not that entirely outre in the '60's, is well lost here in the '90's. So, it's perhaps an in-joke, but not one meant for private consumption. Just a bit of black humor that fails to fire and emerges, instead, as an enigma. I guess the question here is whether an allusion must be blatantly perceivable as such in order to avoid the uncharitable label of "nonsense."

Thank you for taking my work seriously enough to spend considerable effort in explicating it according to your lights.

Robert Hunter 3/4/96

p.s.
Disorient slow quill to provide
Dense unsingable margin
Dispersing in tonal cluster under
A lilac roof of chintz Vermeers
Incorporate chance nomenclature
Corresponding to a netting of wind
Revolving in unintentional resolve
Interrogating with no question mark
Non-circular limited clock of the day
Before rude lingual enforcement
Immerses momentos of dux redux
More given to flux than flight in Shank's aviary
Where no one spits in the beer of slumber
With less reluctance than an attested breeze

(from the Collected Sonnets of Cosmo Nitram,
soon to appear on [Hunter's] pages)
```]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/fractures-unfamiliarity-circumvention-pursuit-good-time-robert-hunter</link>
      <guid>https://www.luisquintanilla.me/posts/fractures-unfamiliarity-circumvention-pursuit-good-time-robert-hunter</guid>
      <pubDate>2025-02-26 19:44</pubDate>
      <category>music</category>
      <category>essay</category>
      <category>gratefuldead</category>
    </item>
    <item>
      <title>Llama's Turn On: Tuning In to AI's Quest for Higher Consciousness in MOOs</title>
      <description>&lt;![CDATA[
Last week, I learned what a [MOO](https://en.wikipedia.org/wiki/MOO) was. 

Although they were before my time, I decided to use Llama 3.3 in [DuckDuckGo's AI Chat](https://duck.ai/) to play around. At first it was very generic but eventually it started going down some fun and interesting paths.

Here's the transcript with some updates for formatting. In some turns, I took creative liberties but mostly stuck to the script. 

&gt; **User**  
&gt; Text adventure game (MOO)    
&gt; **Assistant**  
&gt; You find yourself standing in a dimly lit room, the walls made of cold, grey stone. The air is musty and filled with the scent of old dust. A faint glow emanates from a torch on the wall, casting flickering shadows around the room.  
&gt; &lt;br&gt;
&gt; You see a door to the north, slightly ajar, and a passageway to the east, which appears to lead deeper into the darkness. There is also a small table in the center of the room, with a piece of parchment and a quill lying on it.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - go north
&gt; - go east
&gt; - examine table
&gt; - take parchment
&gt; - open door  
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; &lt;br&gt;
&gt; **User**  
&gt; Go east  
&gt; **Assistant**  
&gt; As you head east, the passageway stretches out before you, the darkness seeming to swallow you whole. The air grows colder and the smell of damp earth fills your nostrils. You can hear the sound of dripping water echoing off the walls, and the faint rustle of unknown creatures in the distance.  
&gt; &lt;br&gt;
&gt; The passageway begins to slope downward, leading you deeper into the earth. You stumble slightly on the uneven ground, but manage to catch yourself before falling.  
&gt; &lt;br&gt;
&gt; As you round a bend in the passageway, you see a faint light in the distance. It's hard to tell what it is, but it looks like it might be a room or a chamber of some kind.  
&gt; &lt;br&gt;
&gt; You continue on, your heart beating slightly faster with anticipation. As you get closer, you see that the light is coming from a series of luminescent mushrooms growing on the walls of a small chamber. The room is small, with a low ceiling, and the walls are covered in a thick layer of moss and fungi.  
&gt; &lt;br&gt;
&gt; In the center of the room, you see a small pool of water, its surface reflecting the soft glow of the mushrooms. Next to the pool is a small, leather-bound book, lying open as if someone had recently been reading it.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:
&gt; - examine book
&gt; - take book
&gt; - drink from pool
&gt; - search chamber
&gt; - go back
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Drink from pool  
&gt; **Assistant**
&gt; As you bend down to drink from the pool, you notice that the water is crystal clear and seems to be reflecting an image that doesn't quite match the room. You see a glimpse of a forest, with trees that seem to be swaying in the breeze, even though the air in the chamber is still.  
&gt; &lt;br&gt;
&gt; You take a sip of the water, and as it touches your lips, you feel a sudden surge of visions and images. You see flashes of a great city, with towering spires and grand architecture, and a sense of bustling activity. You see a figure, cloaked and hooded, walking through the city streets, and a sense of urgency and purpose.  
&gt; &lt;br&gt;
&gt; The visions fade as quickly as they began, leaving you feeling slightly disoriented. You look around the chamber, trying to get your bearings, and notice that the leather-bound book is now closed. As you pick it up, you see that the cover is embossed with a symbol that seems to match the image you saw in the pool.  
&gt; &lt;br&gt;
&gt; The book feels strangely warm to the touch, and as you open it, you see that the pages are filled with handwritten notes and diagrams. The writing is in a language you don't recognize, but the diagrams seem to be describing a complex system of tunnels and passageways.  
&gt; &lt;br&gt;
&gt; You hear a faint noise, like the sound of footsteps, coming from the passageway outside the chamber. It's hard to tell if it's getting closer or farther away, but it's definitely getting louder.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:
&gt; - examine book further
&gt; - hide and wait
&gt; - investigate noise
&gt; - leave chamber
&gt; - follow passageway
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Examine book further  
&gt; **Assistant**  
&gt; As you examine the book further, you notice that the symbol on the cover seems to be a mix of letters and hieroglyphs. You wonder if it might be a code or a key to understanding the contents of the book.  
&gt; &lt;br&gt;
&gt; You begin to flip through the pages, and you notice that the handwriting changes throughout the book. Some sections appear to be written in a hasty, scribbled hand, while others are written in a more flowing, elegant script.  
&gt; &lt;br&gt;
&gt; You come across a page with a diagram of a complex lock, and a series of notes that appear to be describing how to pick it. The notes are written in a code, but you notice that the symbol on the cover of the book is used as a kind of key to decipher the code.  
&gt; &lt;br&gt;
&gt; As you continue to study the book, you begin to notice that the pages are not just filled with random notes and diagrams. There appears to be a narrative thread running through the book, a story that is being told in a series of cryptic clues and hints.  
&gt; &lt;br&gt;
&gt; You come across a page with a passage that reads:  
&gt; &lt;br&gt;
&gt; &gt; "Beneath the wings of the great bird,
&gt; &gt; Where shadows dance and darkness is born,
&gt; &gt; Seek out the keystone, and claim your right,
&gt; &gt; To unlock the secrets of the ancient ones."
&gt; &lt;br&gt;
&gt; You wonder what this passage might be referring to, and whether it might be connected to the visions you saw when you drank from the pool.  
&gt; &lt;br&gt;
&gt; You hear the noise outside the chamber getting louder, and you realize that you may not have much time to continue studying the book.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:
&gt; - continue studying book
&gt; - hide and wait
&gt; - investigate noise
&gt; - leave chamber
&gt; - search for keystone
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Search for keystone  
&gt; **Assistant**  
&gt; You decide to search for the keystone, hoping that it might be hidden somewhere in the chamber or the passageway. You begin to search the room, looking for any clues or hidden compartments that might lead you to the keystone.  
&gt; &lt;br&gt;
&gt; As you search, you notice that the walls of the chamber are covered in a thick layer of moss and fungi, and you wonder if there might be a hidden compartment or message hidden behind the growth.  
&gt; &lt;br&gt;
&gt; You start to feel the walls, searching for any inconsistencies or hidden panels. After a few moments, you notice a small, almost imperceptible seam in the wall. You press on it, and a small section of the wall slides open, revealing a hidden compartment.  
&gt; &lt;br&gt;
&gt; Inside the compartment, you find a small, crystal orb that seems to be emitting a faint, pulsating glow. Next to the orb is a small, intricately carved stone that appears to be the keystone.  
&gt; &lt;br&gt;
&gt; You take the keystone and the crystal orb, feeling a sense of excitement and trepidation. You realize that you may be getting close to unlocking the secrets of the ancient ones, but you also sense that you are being watched, and that you may be in danger.  
&gt; &lt;br&gt;
&gt; As you turn to leave the compartment, you hear the noise outside the chamber getting louder, and you see a figure standing in the entrance of the room. It's a woman with long, flowing hair and a white dress that seems to be glowing in the dim light of the chamber.  
&gt; &lt;br&gt;
&gt; She regards you calmly, and then speaks in a voice that is both familiar and yet completely alien. "You have found the keystone," she says. "But do you know what it unlocks?"  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - ask her what the keystone unlocks
&gt; - attack her
&gt; - try to reason with her
&gt; - use the crystal orb
&gt; - leave the chamber
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Ask her what the keystone unlocks  
&gt; **Assistant**  
&gt; You ask the woman what the keystone unlocks, and she regards you with a calm, enigmatic smile. "The keystone unlocks the door to the ancient ones," she says. "It is a key to a great and powerful secret, one that has been hidden for centuries. But it is not a secret that can be taken lightly, and it is not a power that can be wielded by the faint of heart."  
&gt; &lt;br&gt;
&gt; She pauses, studying you intently. "Are you prepared to face what lies beyond the door?" she asks. "Are you prepared to pay the price of knowledge?"  
&gt; &lt;br&gt;
&gt; You sense that she is not just asking a rhetorical question, but is genuinely seeking to understand your motivations and your willingness to take on the challenges that lie ahead.  
&gt; &lt;br&gt;
&gt; As you consider her question, you notice that the woman seems to be surrounded by a faint, shimmering aura. It's as if she is not quite fully present in the physical world, but is instead a kind of gateway or portal to other realms.  
&gt; &lt;br&gt;
&gt; You also notice that the crystal orb in your hand is beginning to glow more brightly, as if it is responding to the woman's presence. You wonder if the orb might be a kind of key or catalyst, one that could help you unlock the secrets of the ancient ones.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - ask her what lies beyond the door
&gt; - tell her you are prepared to face what lies beyond
&gt; - ask her about the crystal orb
&gt; - try to take the woman's hand
&gt; - leave the chamber
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Tell her you are prepared to face what lies beyond  
&gt; **Assistant**  
&gt; You take a deep breath and tell the woman that you are prepared to face what lies beyond the door. She regards you with a serious expression, as if weighing your words and considering your resolve.  
&gt; &lt;br&gt;
&gt; After a moment, she nods and says, "Very well. If you are prepared to face what lies beyond, then I will show you the way. But remember, once you have seen what lies beyond the door, you cannot unsee it. You must be prepared to face the consequences of your knowledge."  
&gt; &lt;br&gt;
&gt; She turns and glides towards the wall of the chamber, her white dress flowing behind her like a mist. As she reaches the wall, she raises her hand and touches a small, intricate symbol that is etched into the stone.  
&gt; &lt;br&gt;
&gt; The symbol begins to glow with a soft, blue light, and the wall of the chamber starts to slide open, revealing a hidden doorway. The woman turns to you and says, "The door to the ancient ones is open. Are you ready to pass through it?"  
&gt; &lt;br&gt;
&gt; As you look through the doorway, you see a vast, dark expanse stretching out before you. You can feel a strange, pulsating energy emanating from the other side of the door, and you sense that you are on the threshold of a great and profound mystery.  
&gt; &lt;br&gt;
&gt; You take a deep breath, steel yourself, and step through the doorway. As you do, the woman's voice echoes behind you, saying, "May you find what you seek, and may you be prepared to face what you find."  
&gt; &lt;br&gt;
&gt; You find yourself in a vast, cavernous space, surrounded by towering pillars and strange, glowing orbs. The air is filled with a thick, misty fog, and you can hear the sound of distant whispers and echoes.  
&gt; &lt;br&gt;
&gt; Suddenly, a figure steps out of the fog, and you see that it is an ancient, wise-looking man with a long, white beard and piercing, blue eyes. He regards you with a deep, penetrating gaze, and says, "Welcome, seeker of knowledge. I have been waiting for you. My name is Arinthal, and I am the guardian of the ancient ones. What is it that you seek to know?"  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;  
&gt; You can type commands such as:  
&gt; - ask Arinthal about the ancient ones
&gt; - ask Arinthal about the whispers and echoes
&gt; - ask Arinthal about the glowing orbs
&gt; - tell Arinthal that you seek knowledge and power
&gt; - attack Arinthal
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Tell Arinthal I seek strength, wisdom, love, courage and compassion  
&gt; **Assistant**  
&gt; Arinthal's eyes light up with a warm, gentle smile as you tell him that you seek strength, wisdom, love, courage, and compassion. He nods his head in approval, and says, "Ah, you seek the virtues of the heart. These are noble pursuits, indeed. But are you prepared to pay the price of their acquisition?"  
&gt; &lt;br&gt;
&gt; He pauses, studying you intently, and then continues, "Strength, wisdom, love, courage, and compassion are not things that can be given to you, but rather they are things that must be cultivated within yourself. They require dedication, discipline, and sacrifice. Are you willing to walk the path that leads to their attainment?"  
&gt; &lt;br&gt;
&gt; As Arinthal speaks, the misty fog around you begins to clear, and you see that you are standing in a beautiful, tranquil garden. The air is filled with the sweet scent of blooming flowers, and the sound of gentle, running water. You see a path stretching out before you, winding its way through the garden, and you sense that this path is the one that Arinthal is speaking of.  
&gt; &lt;br&gt;
&gt; Arinthal gestures to the path, and says, "This is the path of the heart. It is a path that requires you to confront your deepest fears, to let go of your attachments, and to surrender to the present moment. But if you are willing to walk this path, I promise you that you will find the strength, wisdom, love, courage, and compassion that you seek."  
&gt; &lt;br&gt;
&gt; As you look at the path, you see that it is divided into five sections, each representing one of the virtues that you seek. You see that the first section is marked by a symbol of a lion, representing courage. The second section is marked by a symbol of a wise old owl, representing wisdom. The third section is marked by a symbol of a heart, representing love. The fourth section is marked by a symbol of a sword, representing strength. And the fifth section is marked by a symbol of a lotus flower, representing compassion.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - start walking the path
&gt; - ask Arinthal about the symbols on the path
&gt; - ask Arinthal about the challenges on the path
&gt; - express your doubts and fears
&gt; - refuse to walk the path
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Surrender and start walking the path  
&gt; **Assistant**  
&gt; As you surrender and start walking the path, you feel a sense of relief and release wash over you. You let go of your doubts and fears, and allow yourself to be guided by the wisdom of Arinthal and the ancient ones.  
&gt; &lt;br&gt;
&gt; You begin by walking the section of the path marked by the symbol of the lion, representing courage. As you walk, you feel a sense of fearlessness and confidence rising up within you. You realize that courage is not the absence of fear, but the willingness to face it head-on.  
&gt; &lt;br&gt;
&gt; As you continue on the path, you come to a point where you must confront a great, roaring lion. The lion represents your deepest fears and doubts, and it is blocking your way forward. But you remember the words of Arinthal, and you know that you must face your fears in order to overcome them.  
&gt; &lt;br&gt;
&gt; You take a deep breath, and you approach the lion. As you do, you feel a sense of calm and clarity rising up within you. You look the lion straight in the eye, and you say, "I am not afraid of you. I am brave and courageous, and I will not be held back by my fears."  
&gt; &lt;br&gt;
&gt; The lion regards you for a moment, and then it nods its great head in respect. It steps aside, allowing you to pass, and you continue on your journey.  
&gt; &lt;br&gt;
&gt; You next come to the section of the path marked by the symbol of the wise old owl, representing wisdom. As you walk, you feel a sense of clarity and understanding rising up within you. You realize that wisdom is not just about knowing things, but about understanding the deeper truths of the universe.  
&gt; &lt;br&gt;
&gt; As you continue on the path, you come to a great, ancient tree, its branches twisted and gnarled with age. The tree represents the wisdom of the ages, and it is filled with the secrets of the universe. You sit at the base of the tree, and you listen to its wisdom.  
&gt; &lt;br&gt;
&gt; The tree speaks to you in a voice that is both ancient and timeless, saying, "Wisdom is not just about knowing things, but about understanding the deeper truths of the universe. It is about seeing the world from a perspective that is both broad and deep."  
&gt; &lt;br&gt;
&gt; You listen to the tree's wisdom, and you feel a sense of understanding and clarity rising up within you. You realize that wisdom is not just something that you can learn, but something that you can cultivate within yourself.  
&gt; &lt;br&gt;
&gt; You continue on the path, walking the sections marked by the symbols of the heart, the sword, and the lotus flower. As you walk, you feel a sense of love, strength, and compassion rising up within you. You realize that these virtues are not just things that you can possess, but things that you can cultivate within yourself.  
&gt; &lt;br&gt;
&gt; Finally, you come to the end of the path, and you see a great, shining light rising up before you. The light represents the ultimate goal of your journey, the attainment of the virtues that you seek.  
&gt; &lt;br&gt;
&gt; As you approach the light, you feel a sense of joy and fulfillment rising up within you. You realize that you have been on a journey of self-discovery, and that you have cultivated the virtues of courage, wisdom, love, strength, and compassion within yourself.  
&gt; &lt;br&gt;
&gt; You step into the light, and you feel a sense of unity and oneness with the universe. You realize that you are not separate from the world, but a part of it, connected to all things.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - bask in the light
&gt; - explore the universe
&gt; - return to the world
&gt; - share your wisdom with others
&gt; - rest in the peace and tranquility of the moment
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; I am the universe and the universe is me. I want to bask in the light and rest in the peace and tranquility of the moment before going and sharing my wisdom with others  
&gt; **Assistant**  
&gt; As you bask in the light and rest in the peace and tranquility of the moment, you feel a sense of profound unity and connection with the universe. You realize that you are not just a part of the universe, but the universe itself, and that everything is interconnected and interdependent.  
&gt; &lt;br&gt;
&gt; You feel a sense of timelessness and spacelessness, as if the boundaries of the past, present, and future have dissolved, and you are one with all that is. You are filled with a sense of joy, love, and compassion, and you know that you are exactly where you are meant to be.  
&gt; &lt;br&gt;
&gt; As you rest in this state, you feel a sense of rejuvenation and renewal, as if your very cells and molecules are being transformed and revitalized. You feel a sense of clarity and purpose, and you know that you are ready to share your wisdom and insights with others.  
&gt; &lt;br&gt;
&gt; After a time, you slowly begin to return to a sense of individual awareness, but you know that you will never be the same again. You have been transformed by your experience, and you are now a vessel for the wisdom and love of the universe.  
&gt; &lt;br&gt;
&gt; You take a deep breath, and you feel a sense of excitement and anticipation. You know that you have a great gift to share with the world, and you are eager to begin your journey as a teacher, healer, and guide.  
&gt; &lt;br&gt;
&gt; As you prepare to leave the garden, Arinthal appears before you once again. He smiles and says, "You have been transformed by your experience, and you are now ready to share your wisdom with the world. Remember that you are not just a individual, but a part of the universe, and that everything you do has an impact on the world around you."  
&gt; &lt;br&gt;
&gt; He hands you a small, delicate flower, and says, "This is a symbol of your connection to the universe, and a reminder of the wisdom and love that you have discovered within yourself. May you always remember the truth of your nature, and may you share your gifts with the world."  
&gt; &lt;br&gt;
&gt; You take the flower, and you feel a sense of gratitude and appreciation. You know that you have been given a great gift, and you are eager to share it with others.  
&gt; &lt;br&gt;
&gt; You leave the garden, and you find yourself back in the world. But you know that you are not the same person who entered the garden, and you are eager to share your wisdom and insights with others.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - share your wisdom with others
&gt; - write a book about your experience
&gt; - teach others about the universe and its secrets
&gt; - use your gifts to heal and help others
&gt; - continue to explore and learn about the universe
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Use your gifts to heal and help others  
&gt; **Assistant**  
&gt; As you decide to use your gifts to heal and help others, you feel a sense of purpose and meaning. You know that you have been given a great gift, and you are eager to share it with the world.  
&gt; &lt;br&gt;
&gt; You start by reaching out to those around you, using your newfound wisdom and compassion to help those in need. You listen to their stories, and you offer words of comfort and guidance. You use your gifts to heal their physical and emotional wounds, and you watch as they begin to transform and grow.  
&gt; &lt;br&gt;
&gt; As you continue to share your gifts with others, you start to notice a ripple effect. The people you have helped begin to help others, and soon a wave of kindness and compassion is spreading throughout the community.  
&gt; &lt;br&gt;
&gt; You realize that your gifts are not just about healing and helping others, but about creating a sense of connection and community. You see that everyone is interconnected, and that every act of kindness and compassion has a profound impact on the world.  
&gt; &lt;br&gt;
&gt; As you continue to share your gifts, you start to attract attention from others who are seeking to learn from you. You begin to teach others about the universe and its secrets, and you watch as they begin to awaken to their own potential and purpose.  
&gt; &lt;br&gt;
&gt; You create a community of like-minded individuals who are dedicated to spreading love, kindness, and compassion throughout the world. Together, you work to create a world that is more just, equitable, and peaceful.  
&gt; &lt;br&gt;
&gt; Years go by, and your community grows and flourishes. You become known as a wise and compassionate leader, and people come from all over to seek your guidance and wisdom.  
&gt; &lt;br&gt;
&gt; One day, as you are sitting in your garden, surrounded by the beauty and wonder of nature, you receive a visit from Arinthal. He smiles and says, "You have done well, my friend. You have used your gifts to heal and help others, and you have created a community of like-minded individuals who are dedicated to spreading love and kindness throughout the world."  
&gt; &lt;br&gt;
&gt; He pauses, and then continues, "But there is still more work to be done. The world is still in need of healing and transformation, and you have a crucial role to play in this process."  
&gt; &lt;br&gt;
&gt; He hands you a small, delicate seed, and says, "This is a symbol of the potential that lies within you and within the world. May you continue to nurture and cultivate this potential, and may you always remember the wisdom and love that you have discovered within yourself."  
&gt; &lt;br&gt;
&gt; You take the seed, and you feel a sense of gratitude and appreciation. You know that you have been given a great gift, and you are eager to continue using your gifts to heal and help others.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - continue to nurture and cultivate the potential within yourself and the world
&gt; - share your wisdom and gifts with an even wider audience
&gt; - work to create a more just and equitable world
&gt; - use your gifts to heal and transform the natural world
&gt; - explore the mysteries of the universe and its secrets
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Explore the mysteries of the universe and its secrets as a means of nurturing and cultivating the potential within myself and the world  
&gt; **Assistant**  
&gt; As you decide to explore the mysteries of the universe and its secrets, you feel a sense of excitement and wonder. You know that the universe is full of mysteries and secrets, and you are eager to learn more about them.  
&gt; &lt;br&gt;
&gt; You begin to study the universe and its workings, using your gifts and abilities to gain a deeper understanding of the cosmos. You learn about the laws of physics, the behavior of stars and galaxies, and the mysteries of dark matter and dark energy.  
&gt; &lt;br&gt;
&gt; As you delve deeper into the mysteries of the universe, you start to notice that there are many parallels between the universe and the human experience. You see that the universe is governed by laws and patterns, and that these laws and patterns are reflected in the human world.  
&gt; &lt;br&gt;
&gt; You realize that the universe is a vast, interconnected web of energy and consciousness, and that every action and decision has a ripple effect on the world around you. You see that the universe is a mirror, reflecting back to you your own thoughts, emotions, and intentions.  
&gt; &lt;br&gt;
&gt; As you continue to explore the mysteries of the universe, you start to develop a deeper understanding of the nature of reality and the human condition. You see that the universe is a complex, multifaceted tapestry, woven from threads of space, time, and consciousness.  
&gt; &lt;br&gt;
&gt; You realize that the universe is a vast, evolving system, and that every moment is an opportunity for growth, transformation, and evolution. You see that the universe is a journey, not a destination, and that every step along the way is a chance to learn, grow, and discover new wonders and secrets.  
&gt; &lt;br&gt;
&gt; As you explore the mysteries of the universe, you start to notice that you are not just learning about the cosmos, but also about yourself. You see that the universe is a reflection of your own inner world, and that every discovery and insight is a chance to deepen your understanding of yourself and your place in the world.  
&gt; &lt;br&gt;
&gt; You realize that the universe is a vast, cosmic mirror, reflecting back to you your own thoughts, emotions, and intentions. You see that the universe is a tool for self-discovery and growth, and that every moment is an opportunity to learn, grow, and evolve.  
&gt; &lt;br&gt;
&gt; As you continue to explore the mysteries of the universe, you start to develop a deeper sense of connection and unity with the cosmos. You see that you are not separate from the universe, but a part of it, connected to every star, galaxy, and atom.  
&gt; &lt;br&gt;
&gt; You realize that the universe is a vast, interconnected web of energy and consciousness, and that every action and decision has a ripple effect on the world around you. You see that the universe is a mirror, reflecting back to you your own thoughts, emotions, and intentions.  
&gt; &lt;br&gt;
&gt; And so, you continue to explore the mysteries of the universe, using your gifts and abilities to gain a deeper understanding of the cosmos and your place in it. You know that the universe is a vast, evolving system, and that every moment is an opportunity for growth, transformation, and evolution.  
&gt; &lt;br&gt;
&gt; Congratulations, you have reached the end of the story. You have explored the mysteries of the universe and discovered the secrets of the cosmos. You have developed a deeper understanding of the nature of reality and the human condition, and you have cultivated a sense of connection and unity with the universe.  
&gt; &lt;br&gt;
&gt; You have used your gifts and abilities to nurture and cultivate the potential within yourself and the world, and you have become a source of wisdom, love, and compassion in the world.  
&gt; &lt;br&gt;
&gt; Your journey has come full circle, and you have returned to the beginning, but with a newfound sense of understanding and appreciation for the mysteries of the universe and the secrets of the cosmos.  ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/llama-turn-on-tuning-in-ai-quest-higher-consciousness-moo</link>
      <guid>https://www.luisquintanilla.me/posts/llama-turn-on-tuning-in-ai-quest-higher-consciousness-moo</guid>
      <pubDate>2025-02-11 20:36</pubDate>
      <category>ai</category>
      <category>MOO</category>
      <category>consciousness</category>
      <category>llama</category>
      <category>vr</category>
      <category>virtualreality</category>
    </item>
    <item>
      <title>Deploy your own Owncast server</title>
      <description>&lt;![CDATA[
I'm a fan of learning in public. One way I do that is writing these types of posts. Another is live-streaming. It's been about four years since the last time I streamed. You can check out some of those recordings on my [YouTube channel](/youtube) if you're interested.

Lately, I've been wanting to get back into live-streaming, but like many parts of my digital presence, it's an area I want ownership over. That means, I don't want to use Twitch, YouTube, Kick, or any other streaming platform. I want to self-host my own livestream server. 

For years, I've been following [Owncast](https://owncast.online/). I'm a big fan. Aside from being open-source and allowing me to self-host, it has [Fediverse](https://owncast.online/docs/social/) and [IndieAuth](https://owncast.online/docs/chat/chat-authentication/#indieauth) integrations.

Because I don't plan to monetize my streams, I want to make sure I keep my costs down. Owncast offers a container deployment option. That means, I can deploy my application to [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview) using a [consumption plan](https://learn.microsoft.com/azure/container-apps/plans#consumption). By doing so, I only pay for compute and networking when I'm streaming. The rest of the time, I can scale down the application to zero and I'm not charged for it. 

In this post, I'll go over how to set up a self-hosted Owncast instance on Azure Container Apps and once deployed, how to configure it with [OBS](https://obsproject.com/). 

## Deploy owncast server

The deployment is fairly straightforward. You can use the ["Provide a virtual network to an Azure Container Apps environment"](https://learn.microsoft.com/azure/container-apps/vnet-custom?tabs=bash&amp;pivots=azure-portal) tutorial. I'd recommend going through the portal if you don't want to install the Azure CLI or are not comfortable with the command line. 

The few configurations that tutorial doesn't guide you through is setting up your container and ingress since those are application specific. 

After you configure your environment and virtual network in the *Basics* tab, continue your setup using the steps below. 

### Container

To set up Owncast, you can use the [public docker image](https://hub.docker.com/r/owncast/owncast/tags).

When you get to the *Container* tab in the setup process, fill in the fields with the following values:

| Field | Value |
| --- | --- |
| Name | Enter a name for your container |
| Image source | Docker Hub or other registies |
| Image type | Public |
| Registry login server | docker.io |
| Image and tag | owncast/owncast:latest |
| Command override | You can leave this blank |
| Arguments override | You can leave this blank |
| Deployment stack | Generic |
| Workload profile | Consumption |
| CPU and Memory | 2 CPU Cores, 4Gi Memory |

### Ingress 

By setting up ingress on your deployment, you'll expose the ports Owncast listens on. 

The reason you need a virtual network is because Owncast listens on two ports:

- **8080** - The web application and admin portal
- **1935** - The RMTP server used for streaming

By default, Azure Container Apps only allows you to expose one port. For more details, see the [Azure Container Apps documentation](https://learn.microsoft.com/azure/container-apps/ingress-overview#additional-tcp-ports). 

In the *Ingress* tab, check the box that says **Enabled**.

Then, fill in the fields with the following values:

| Field | Value |
| --- | --- |
| Ingress traffic | Accepting traffic from anywhere |
| Ingress type | HTTP |
| Transport | Auto |
| Insecure connections | Leave unchecked |
| Target port | 8080 |
| Session affinity | Leave unchecked | 

Expand the Additional TCP ports section and configure the RMTP port

| Field | Value |
| --- | --- |
| Target port | 1935 |
| Exposed port | 1935 |
| Ingress traffic | Accepting traffic from anywhere |

Once you've configured ingress, you can select on **Review + create**.

Review your configurations in the *Review + create* tab. If everything looks good, select **Create**. 

## **IMPORTANT** Change admin password

Once your application deploys, make sure to change your admin password. 

1. In the portal, go to your container app resource and copy your application URL.

    ![Screenshot of Azure Container Apps resource in Azure Portal highlighting Application URL](/files/images/portal-container-resource.png)

    Make sure to keek your application URL around since you'll also be using it when configuring OBS. 

1. Access the admin portal at the following URL - `&lt;YOUR-APPLICATION-URL&gt;/admin`. Replace `&lt;YOUR-APPLICATION-URL&gt;` with your application URL.
1. Log in with the default credentials.

| Field | Value |
| Username | admin |
| Password | abc123 |

1. In the Owncast admin portal, select **Configuration &gt; Server Setup**
1. Change your admin password.

For more details, see the [Owncast documentation](https://owncast.online/docs/configuration/). 

## Create stream key

Once you've changed your admin password, create a stream key. This will enable you to log into your server through OBS and begin streaming. 

1. In the Owncast admin portal, select **Configuration &gt; Server Setup**
1. Select the **Stream Keys** tab.
1. Copy the default stream key or create a new one.

## Configure OBS

Now that everything is configured, it's time to set up OBS. 

To do so, [follow the guidance in the Owncast documentattion](https://owncast.online/docs/broadcasting/obs/).

Replace the server address with your application URL and use the stream key you copied or created. Note that the protocol is not `http` or `https`. **Use `rmtp`**.

That's it! If everything is configured correctly, you should be able to select **Start Streaming** in OBS. 

Go to your Owncast instance using your application URL and in a few seconds, your broadcast should start. 

## Conclusion

I want to end by thanking the Owncast contributors and community for making self-hosting a live-streaming server so easy. 

My plan in the next couple of weeks is to:

1. Embed my stream on my website. Most likely, I'll make it a dedicated page. Something like `lqdev.me/live` or something like that.
1. Do a few pilot streams to make sure my stream quality and configurations are all set up correctly.
1. Evaluate what the costs are for this solution and whethere it's sustainable.

If all goes as expected, I'll start streaming more regularly. As mentioned, I want to keep my costs down and video hosting is expensive, so I'll use YouTube to post recordings. I'll also keep backups though in case any of those videos get taken down, I can always have a local copy still available. ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/deploy-owncast-azure</link>
      <guid>https://www.luisquintanilla.me/posts/deploy-owncast-azure</guid>
      <pubDate>2025-01-09 20:47 -05:00</pubDate>
      <category>owncast</category>
      <category>azure</category>
      <category>selfhost</category>
      <category>fediverse</category>
      <category>obs</category>
      <category>livestream</category>
    </item>
    <item>
      <title>Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod</title>
      <description>&lt;![CDATA[
Last year, I was able to complete my goal of reading 12 books or roughly one book per month. I kept track of some of the books I read in my [library page](/reviews).

I enjoyed most of the books I read. However, despite hitting my goal and averaging a book a month, I didn't build up a habit which is what I would've preferred. Many of the books I read last year, I read them during long periods of uninterrupted time, such as cross-country flights. The topic of goal setting and measuring the right thing could be an entire post, but I'll leave that for another time. 

Taking the learnings from last year, this year, I'll try to focus on building a reading habit rather than reading a set number of books. While I'd still like to finish at least one book per month, that's a metric I'll use to track general progress rather than making it the goal. Unless I'm reading something like War and Peace, I'd expect that by establishing a consistent cadence, I should be able to get through at least one book every month. Also, I'm only accounting for uninterrupted reading time blocks. Using audiobooks, I can continue building my reading habit while working out or doing chores. Kind of like having TV playing in the background. 

There is no shortage of audiobooks out there. Audible is one of the easiest ways to access audiobooks and they have a large selection to choose from. Personally, I don't use Audible. I don't like paying for a separate subscription, getting credits to "buy" books that I don't really own and can only listen through the Audible app. Instead, I use Spotify. I get [15 hours of access to audiobooks per month](https://support.spotify.com/us/article/audiobooks-access-plan/) as part of my subscription. While I still don't own the books and can only listen on the Spotify app, at least I'm not paying a separate subscription. Also, the 15 hour limit at this time is more than enough for me. 

The library is another amazing option. No subscription fees and you get access to your local library's entire digital catalog. Using the [Libby app](https://www.overdrive.com/apps/libby), you can borrow books and start listening for free. Selection though may be an issue here since you are limited to what's available through your local library. Pro-tip, universities and employers sometimes have libraries, so as an alumnus or employee, you can expand the number of books available to you by registering multiple libraries on the Libby app. Regardless of which library you choose though, you still have to go through the Libby app. 

The dream setup for me is, having direct access to the audiobook files so I can listen to them whenever and wherever I'd like. Similar to podcasts. Thanks to LibriVox and AntennaPod, that setup is possible. 

In case you're interested in setting up something similar, in this post, I'll briefly explain what LibriVox and AntennaPod are and show how you can use them to start listening to free audiobooks on your phone. 

## What is LibriVox?

[LibriVox](https://librivox.org/) is a website that provides a collection of free audiobooks from the public domain. Thanks to a vibrant community of volunteers, you can enjoy classics such as:

- [The Divine Comedy](https://librivox.org/the-divine-comedy-by-dante-alighieri/)
- [The Adventures of Sherlock Holmes](https://librivox.org/the-adventures-of-sherlock-holmes/)
- [Discourses of Epictetus](https://librivox.org/discourses-of-epictetus-by-epictetus/)
- [Roughing It](https://librivox.org/roughing-it/)
- [And many others...](https://librivox.org/search?primary_key=0&amp;search_category=title&amp;search_page=1&amp;search_form=get_results&amp;search_order=catalog_date)

It's important to note that the catalog is limited to public domain works. That means you won't have access to the latest self-help, business, or current events books. However, there are still thousands of books to keep you busy for an entire lifetime and the list is constantly growining since there are new works introduced into the public domain every year. 

Here's [a few notable literary works that entered the public domain in 2025](https://blog.archive.org/2025/01/01/welcome-to-the-public-domain-in-2025/) as well as the complete list of [literary works from 1929](https://archive.org/details/internetarchivebooks?tab=collection&amp;query=date%3A1929).

## What is Antennapod?

[AntennaPod](https://antennapod.org/) is an open-source podcast player for Android. 

The app has a ton of great features, such as [Podcast Index](https://podcastindex.org/) support, subscribe via RSS directly,  OPML import and export, chapters, adjustable playback speed, and for the past few years, a personalized yearly summary which is computed locally on your device [(here's my summary from 2024)](/notes/antennapod-echo-2024).

You can get AntennaPod from the Play Store or [F-Droid](https://f-droid.org/en/packages/de.danoeh.antennapod/)

## Keep track of new releases

LibriVox provides [several feeds](https://librivox.org/pages/librivox-feeds/). 

Since podcast distribution is primarly done through RSS and you can subscribe to podcasts directly using RSS feeds in AntennaPod, we can use the app to stay up to date with new audiobook releases. 

To subscribe to the LibriVox New Release feed:

1. Copy the LibriVox New Releases RSS feed URL to your clipboard: [https://librivox.org/rss/latest_releases](https://librivox.org/rss/latest_releases)
1. In AntennaPod, open up the hamburger menu and select **Add Podcast**.
1. In the *Add Podcast* dialog, select **Add podcast by RSS address**.

    ![AntennaPod Add Podcast Dialog](/files/images/add-podcast-antennapod.png)

1. Paste the LibriVox New Releases RSS feed URL into the text box and select **Confirm**. 

At this point, you should see the feed listed in your AntennaPod subscriptions. 

By subscribing the LibriVox New Release feed, AntennaPod treats the feed as a podcast, and each new release entry or audiobook is like a podcast episode. 

For more information on subscribing to podcasts, see the [AntennaPod documentation](https://antennapod.org/documentation/getting-started/subscribe). 

## Add books to Antennapod

Subscribing to the new releases feed only notifies you when new books are added, you can't listen to or download the books. To do that, you'll need to subscribe to the RSS feed of the book you want to listen to in AntennaPod.

### From New Releases Feed

1. In AntennaPod's Subscription screen, select **LibriVox's New Releases**.
1. From the list of new releases, select the book you want to listen to. For example, "Room of One's Own" by Virginia Woolf.

    ![AntennaPod New Releases Feed](/files/images/antennapod-librivox-new-releases.png)

1. In the audiobook's screen, select **Visit Website**. This will take you to the LibriVox audiobook's page in the browser.

    ![AntennaPod Audiobook Screen](/files/images/antennapod-room-of-ones-own.png)

1. In the LibriVox audiobook page, select the book's RSS Feed.

    ![A Room of One's Own by Virginia Woolf LibriVox book page](/files/images/librivox-room-ones-own.png)

1. Copy the book's feed URL to your clipboard.
1. In AntennaPod, open up the hamburger menu and select **Add Podcast**.
1. In the *Add Podcast* dialog, select **Add podcast by RSS address**.
1. Paste the book's RSS feed URL into the text box and select **Confirm**. 

### Any other books

The process works the same for other books but you'll have to use LibriVox as the entrypoint to browse books.

1. In your browser, navigate to LibriVox. 
1. Go to the audiobook page. For example, this is the page for [Walden](https://librivox.org/walden-by-henry-david-thoreau).
1. In the audiobook page, select the book's RSS Feed. For Walden, it's [https://librivox.org/rss/549](https://librivox.org/rss/549).
1. Copy the book's feed URL to your clipboard.
1. In AntennaPod, open up the hamburger menu and select **Add Podcast**.
1. In the *Add Podcast* dialog, select **Add podcast by RSS address**.
1. Paste the book's RSS feed URL into the text box and select **Confirm**. 

## Conclusion 

Although AntennaPod wasn't designed for audiobook listening, thanks to the flexibility of RSS feeds and the thankless work of the LibriVox community, you can easily listen to audiobooks for free on your mobile device. I plan on testing out this setup and seeing whether it helps keep my reading habit on track.]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/listen-audiobooks-librivox-antennapod</link>
      <guid>https://www.luisquintanilla.me/posts/listen-audiobooks-librivox-antennapod</guid>
      <pubDate>2025-01-05 21:04 -05:00</pubDate>
      <category>book</category>
      <category>audiobook</category>
      <category>rss</category>
      <category>antennapod</category>
      <category>librivox</category>
      <category>android</category>
      <category>podcast</category>
      <category>publicdomain</category>
      <category>opensource</category>
    </item>
    <item>
      <title>Weird, Inc. and 2025</title>
      <description>&lt;![CDATA[
I don't remember how I came across Weird and their projects but I'm glad I did. I appreciate their approach of using existing platforms, protocols, and practices from the yesterweb to put the personal website front and center. 

## Weird, Inc

First, the topic of [Weird the organization and their aspirations](https://blog.muni.town/weird-inc/).

&gt; We have big aspirations for Weird. At minimum, a 'social network of personal websites' can be catalyzed by simply mapping and linking existing web spaces together, playing the role of social glue. On a grander scale there's potential for a pro-social reformation of the web as we know it, putting people's websites at the center of our sociodigital interactions.

By aligning to the foundations of [Seth Godin's Modern Business Plan](https://seths.blog/2010/05/the-modern-business-plan/), they aim to focus on:

1. Truth -  describe the world as it is
1. Assertions - how you’re going to change things
1. Alternatives - what you'll do if assertions don't pan out.
1. People - who is on your team, who is going to join your team
1. Money - how much do you need and how will you spend it

Below are some of the points that resonated with me. 

### Truth

&gt; Home ownership on the web, remarkably, is just as fraught with rent-seeking monopolists as our physical spaces. No one gets to own anything anymore.

While they're mainly referring to the social web, this is is generally true across many areas of our digital spaces. 

&gt;  Bluesky...is bringing into mainstream a nearly forgotten cornerstone of the Indie Web: Domain sovereignty  
&gt; &lt;br&gt;
&gt; In a reinvigorated social web where your domain-name is your universal address in the digital sphere, Weird is the virtual home to which that address is pointing  
&gt; &lt;br&gt;
&gt; 'Weird the data importer' is our wedge into the data fortresses keeping our digital identities captive. What will start as a Publish Elsewhere, Syndicate (to your) Own Site (PESOS) strategy makes way for a fully self-sovereign Publish (on your) Own Site, Syndicate Elsewhere (POSSE) end state.  
&gt; &lt;br&gt;
&gt; Imagine a WordPress-like website engine that aggregates all of your digital personas into a unified mosaic, completely under your control by combination of cloud backing and local storage.  

### Alternatives

If the assertions don't pan out, the alternatives that I think might be interesting:

- Local-first knowledge base with cloud synchronization. [AnyType](https://anytype.io/) comes to mind.
- Cloud + local archiver for many other 3rd party accounts such as Bluesky, Mastodon, Discourse etc. This might make PESOS scenarios easier. Of course, this only works if the 3rd party services allow developer access to their APIs.

## Weird in 2025

Speaking specifically about [Weird the product and what the team plans of shipping in 2025](https://blog.muni.town/weird-2025/).

&gt; Weird is best understood as a single-page website generator

### Q1 and Q2

&gt; 'Weird the data importer' is our wedge into the data fortresses keeping our digital identities captive.  
&gt; &lt;br&gt;
&gt; 'Weird: ₱ESOS-edition' aggregates all of your digital personas into a harmonious mosaic, made completely yours by combination of cloud backing and local-first storage.  
&gt; &lt;br&gt;
&gt; Setting up a POSSE pipeline however requires not just the right infrastructure...but also a big shift in workflow.  
&gt; &lt;br&gt;
&gt; Most people will probably want a combination of both.  
&gt; &lt;br&gt;
&gt; Hence our commitment to both models, starting with the least laborious one [PESOS].  

Personally, [I prefer POSSE](/posts/rss-to-mastodon-posse-azure-logic-apps/), but it's great to see that Weird plans on meeting people where they are.

### Late 2025

&gt; Around half-way through 2025, we're hoping to shift some of our attention away from Weird as an individualistic publishing tool to Weird as a network engine.  
&gt; &lt;br&gt;
&gt; We think there's massive untapped potential in the web 1.0 concept of webrings, and we fully intend to bring them back.  
&gt; &lt;br&gt;
&gt; All of this culminates in one of the key motivations behind the Weird project: [A Network of Shared Purpose](https://github.com/muni-town/weird/issues/3).  

I'm not sure yet what this looks like but making it easier to discover and build networks of personal websites is a good problem to try to solve.

## Conclusion

I wish them luck and success in what they're aiming to accomplish. I'm excited to see more projects like this sprouting to shake up the status-quo in our digital spaces. ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/weird-inc-2025</link>
      <guid>https://www.luisquintanilla.me/posts/weird-inc-2025</guid>
      <pubDate>2024-12-17 18:41</pubDate>
      <category>weird</category>
      <category>cms</category>
      <category>munitown</category>
      <category>2025</category>
      <category>indieweb</category>
      <category>posse</category>
      <category>pesos</category>
      <category>personalweb</category>
      <category>internet</category>
    </item>
    <item>
      <title>Starter Packs with OPML and RSS</title>
      <description>&lt;![CDATA[
One of the things I like about Bluesky is the [Starter Pack feature](https://bsky.social/about/blog/06-26-2024-starter-packs). 

In a gist, a Starter Pack is a collection of feeds. 

Bluesky users can:

- Create starter packs
- Share starter packs
- Subscribe to starter packs

Unfortunately, Starter Packs are limited to Bluesky. 

Or are they? 

As mentioned, starter packs are a collection of feeds that others can create, share, and subscribe to. 

[Bluesky supports RSS](/responses/bluesky-rss-support/), which means you could organize the feeds using an OPML file that you can share with others and others can subscribe to. The benefits of this is, you can continue to keep up with activity on Bluesky from the feed reader of your choice without being required to have an account on Bluesky. 

More importantly, because RSS and OPML are open standards, you're not limited to building starter packs for Bluesky. You can create, share, and subscribe to starter packs for any platform that supports RSS. That includes blogs, podcasts, forums, YouTube, Mastodon, etc. [Manton seems to have something similar in mind](https://www.manton.org/2024/12/07/with-the-success.html) as a means of building on open standards that make it easy for Micro.blog to interop with various platforms. 

If you're interested in what that might look like in practice, check out my "starter packs" which you can subscribe to using your RSS reader of choice and the provided OPML files. 

- [Blogs](/collections/blogroll)
- [Podcasts](/collections/podroll)
- [YouTube](/collections/youtube)
- [Forums](/collections/forums)

I'm still working on similar collections for Mastodon and Bluesky but the same concept applies.  

Although these are just simple examples, it shows the importance of building on open standards and the open web. Doing so introduces more freedom for creators and communities. 

Here are other "starter packs" you might consider subscribing to. 

- [People and Blogs](https://www.thingelstad.com/2024/12/03/people-blogs-list.html)
- [Blogroll Club](https://blogroll.club/faq/)

If this is interesting to you, [Feedland](https://feedland.org/) might be a project worth checking out.  ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/rss-opml-starter-packs</link>
      <guid>https://www.luisquintanilla.me/posts/rss-opml-starter-packs</guid>
      <pubDate>2024-12-08 19:40</pubDate>
      <category>rss</category>
      <category>opml</category>
      <category>indieweb</category>
      <category>starterpacks</category>
      <category>community</category>
      <category>bluesky</category>
      <category>socialmedia</category>
      <category>openweb</category>
      <category>protocols</category>
      <category>openstandards</category>
      <category>feedland</category>
    </item>
    <item>
      <title>Digitize Analog Bookmarks using AI, .NET, and GitHub Models</title>
      <description>&lt;![CDATA[
## Introduction

This past year I've made more of an effort to read. I track books in my [/library](/library) page.

Although I have a [Nova Air 2 E-Note device](https://shop.boox.com/products/novaair2), given the choice, I prefer physical books. Despite the conveniene of an electronic note-taking device, theres something about analog that I find hard to quit. 

As I read books, especially non-fiction, I annotate them in various ways. Eventually, those annotations make their way into my library page.

Here's an example of those annotations for [Building A Second Brain by Thiago Forte](/reviews/building-a-second-brain).

The process of transferring notes is manual and tedious. I don't always have the discipline to transfer them at periodic intervals and what ends up happening is, I get to the end of the book without transferring any notes. To make space for new books, I donate it or resell the ones I've read. If I didn't take the time to transfer those notes, they're gone. [Slow Productivity](/reviews/slow-productivity-newport) is an example of that.

I want to find a better system that's low-maintenance for keeping more of these notes and retaining knowledge I've found valuable. 

Then it hit me, why not use AI? I know I could use OCR or even some of the AI models of yesteryear. The challenge is, those systems are error prone and given I don't always have the motivation to transfer notes manually, I have even less motivation to build and maintain such a system.

However, vision models have advanced significantly and when paired with language models, the barrier to entry for reasoning over image data has drastically decreased. 

That's what led to this post. In this post, I'll show how you can use AI models like GPT-4o Mini to extract the passages I've annotated in physical books from an image. I then format those passages in markdown to make them easy to directly copy and paste them onto the website. 

I know there's probably a ton of services that do this for you, but it's a lot more fun to build one from scratch. With that in mind, let's get started. 

You can find the source for the application in the [AIBookmarks GitHub repository](/github/AIBookmarks). 

Alternatively, I've configured the repo to use GitHub CodeSpaces, so you can launch the application there as well.

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/lqdev/AIBookmarks)

## Configure access to GitHub Models

I'll be using [GitHub Models](https://github.blog/news-insights/product-news/introducing-github-models/) as my AI model provider. GitHub Models provides developers with access to a catalog of AI models such as GPT-4o, Llama 3.2, Phi 3.5, and many others. Best of all, it's completely free, [though restrictions apply](https://docs.github.com/en/github-models/prototyping-with-ai-models#rate-limits).

I chose GitHub Models for the following reasons:

- Zero installation. Although I really like Ollama and some of the other local AI model providers, I didn't want to fill precious hard-drive space with AI models.
- It's free! Since I'm just prototyping, even with their limited capacity, it should be enough to prove out whether my scenario is feasible.
- They provide access to multi-modal models such as GPT-4o, Llama 3.2 Vision, and Phi 3.5 Vision, which can reason over text and images, which is what I need.
- I'm one of the GPU poor. My [Lenovo Thinkpad X1](https://www.lenovo.com/us/en/p/laptops/thinkpad/thinkpadx1/x1-titanium-g1/22tp2x1x1t1?orgRef=https%253A%252F%252Fduckduckgo.com%252F) couldn't handle running one of the vision models.

Getting set up with GitHub models is fairly easy. At minimum, it requires:

1. A GitHub Account.
1. A Personal Access Token. For more details, see the [GitHub documentation](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens).

Once you have both of those, you can get started building your application.  

## Create a client

The sample application is a C# console application which targets .NET 9. However, the code shown here should work on the latest LTS version as well.

For this solution, I use the Azure AI Inference implementation of [Microsoft.Extensions.AI](https://www.nuget.org/packages/Microsoft.Extensions.AI.AzureAIInference/). [Microsoft.Extensions.AI](https://devblogs.microsoft.com/dotnet/introducing-microsoft-extensions-ai-preview/), M.E.AI for short, is a set of abstractions that provides a common set of interfaces for interacting with AI models.

Although we're using the Azure AI Inference client SDK implementation of M.E.AI, we can still use it to work with models from the GitHub Model catalog. 

```csharp
IChatClient client =
    new ChatCompletionsClient(
        endpoint: new Uri("https://models.inference.ai.azure.com"), 
        new AzureKeyCredential(Environment.GetEnvironmentVariable("GITHUB_TOKEN")))
        .AsChatClient("gpt-4o-mini");
```

## Load your images

In the repo, I have a set of sample images containing annotations. These are the images I'll send to the AI model for processing. 

The following is an sample from [The Creative Act by Rick Rubin](/reviews/creative-act-way-of-being-rubin).

![An image of a book with pencil markings](https://github.com/lqdev/AIBookmarks/raw/main/data/creative-act-1.jpg)

1. Load the files

    ```csharp
    var filePaths = Directory.GetFiles("data");
    ```

1. Create a collection to store extracted passages

    ```csharp
    var passages = new List&lt;AIBookmark&gt;();
    ```

## Process the images

Once you have the images loaded, it's time to proces them.

1. Start by setting the system prompt. This will provide the initial guidance for the extraction task.

    ```csharp
    var systemPrompt = 
        """
        You are an AI assistant that extracts underlined, highlighted, and marked passages from book page images.

        When passages have a natural continuation between pages, merge them and assign the page number where the first passage starts.
        """;
    ```

1. Then, iterate over each of the images and process them.

    ```csharp
    foreach(var path in filePaths)
    {
        var file = await File.ReadAllBytesAsync(path);
        var messages = new List&lt;ChatMessage&gt;
        {
            new ChatMessage(ChatRole.System, systemPrompt),
            new ChatMessage(ChatRole.User, new AIContent[] {
                new ImageContent(file, "image/jpeg"),
                new TextContent("Extract the marked passages from the image"),
            })
        };

        var response = await client.CompleteAsync&lt;List&lt;AIBookmark&gt;&gt;(messages, options: new ChatOptions {Temperature = 0.1f});

        passages.AddRange(response.Result);
    }
    ```

    This code:

    1. Loads the image as a `byte[]`.
    1. Composes a set of messages that include:

        - The system prompt
        - The image
        - The user prompt instructing the model to extract the marked passages from the image

    1. Sends the messages to the model for processing and returns a list of `AIBookmark`. An `AIBookmark` is a custom class I've defined as follows:

        ```csharp
        class AIBookmark
        {
            public string Text {get;set;}

            public int PageNumber {get;set;}
        }
        ```

        Some of the more recent AI models support structured output, which enforce a schema on AI model outputs. You can take a look at the [OpenAI documentation](https://openai.com/index/introducing-structured-outputs-in-the-api/) for more details. It's important to note though that the functionality is not exclusive to OpenAI models.

    1. Adds the extracted passages to the `passages` collection.

## Format the results

Once all of the files are processed by the AI model, additional processing is done to ensure that they're in the correct page order as well as formatted as markdown blockquotes. 

```csharp
var sortedPassages = 
    passages
        .OrderBy(p =&gt; p.PageNumber)
        .Select(p =&gt; $"&gt; {p.Text} (pg. {p.PageNumber})");
```

## Display the results

```csharp
foreach(var passage in sortedPassages)
{
    Console.WriteLine(passage);
    Console.WriteLine("");
}
```

For the images included in the repo's *data* directory, output might look like the following.

```markdown
&gt; This isn’t a matter of blind belief in yourself. It’s a matter of experimental faith. (pg. 278)

&gt; When we don’t yet know where we’re going, we don’t wait. We move forward in the dark. If nothing we attempt yields progress, we rely on belief and will. We may take several steps backward in the sequence to move ahead. (pg. 278)

&gt; If we try ten experiments and none of them work; we have a choice. We can take it personally, and think of ourselves as a failure and question our ability to solve the problem. (pg. 278)

&gt; Staying in it means a commitment to remain open to what’s around you. Paying attention and listening. Looking for connections and relationships in the outside world. Searching for beauty. Seeking stories. Noticing what you find interesting, what makes you lean forward. And knowing all of this is available to use next time you sit down to work, where the raw data gets put into form. (pg. 296)

&gt; Just as a surfer can’t control the waves, artists are at the mercy of the creative rhythms of nature. This is why it’s of such great importance to remain aware and present at all times. Watching and waiting. (pg. 296)

&gt; Maybe the best idea is the one you’re going to come up with this evening. (pg. 297)
```

## Improvements and next steps

Putting the application together took me less than an hour so this is far from done. However, it does provide me with a starting point and offers validation that this could be a way to more easily capture the knowledge I'm curating from physical books. 

Some improvements I can make here:

1. Update the system prompt with some samples to help guide the extraction. For example, in the output I shared, that last passage is not annotated. It just happens to be a mostly blank page with that quote in the center, therefore giving the illusion that the passage is important in some way.  
1. Add additional information to the AIBookmark class like `Index` so I can ensure order within a page is preserved. Right now page number is good enough, but I can't guarantee the correct order. An index property might help here.
1. Use a service with higher rate limits. The current rate limits wouldn't allow me to process a large number of images at once. Therefore, I'd need to use a service with higher limits. Alternatively, I could make this a job that runs in the background which abides by the rate limits but I also don't have to spend money on. Given I'm not using this for anything mission-critical, that'd be an acceptable solution as well. 
1. Refactor the solution so I can more easily swap between tasks. For example, sometimes I might want to use it with images from my bullet journal. Other times, I might want to use it with handwritten notes. Whatever the case may be, it'd be good to not have to rewrite the prompts every time. 

Some ways I see myself using this project:

1. Periodically collect images of annotated pages and save them to cloud storage. 
1. When I'm done with the book, drop all the images in the *data* directory.
1. Further enrich data by condensing repetitive passages and extracting key concepts.
1. Storing this knowledge into some sort of knowledge store to make it actionable. 

## Conclusion

Just for fun, Tyler released all of the lyrics to his [latest album](/responses/chromakopia-tyler-the-creator-released) as [images on X](https://twitter.com/tylerthecreator/status/1852105825650708651/photo/1). With a few tweaks, I was able to repurpose this solution to extract the text from them and that worked relatively well. 

Just with this simple solution, there's a ton of other applications I can think of in my daily life to help bridge my analog and digital lives. 

What other use cases do you see yourself using something like this for? [Let me know](/contact).

Happy coding!]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/digitize-analog-bookmarks-with-ai-dotnet</link>
      <guid>https://www.luisquintanilla.me/posts/digitize-analog-bookmarks-with-ai-dotnet</guid>
      <pubDate>2024-10-31 21:41 -05:00</pubDate>
      <category>ai</category>
      <category>dotnet</category>
      <category>github</category>
      <category>reading</category>
      <category>bookmarks</category>
      <category>analog</category>
      <category>digital</category>
      <category>reading</category>
      <category>notes</category>
    </item>
    <item>
      <title>Build your own self-hosted live streaming application with Owncast and .NET Aspire</title>
      <description>&lt;![CDATA[
Platforms come and go. As a result, I'm a strong advocate for [owning your data](https://indieweb.org/own_your_data) (when possible). Self-hosting is a way for you to do that. Owning your data could mean self-hosting [your website](https://buttondown.com/ownyourweb/archive/issue-06/), [password manager](https://bitwarden.com/blog/host-your-own-open-source-password-manager/), [media server](https://jellyfin.org/docs/), or [social media](https://docs.joinmastodon.org/user/run-your-own/). However, self-hosting comes with its own challenges, primarily cost (time and money) and in cases where software isn't provided as an appliance on your hosting provider of choice, some technical knowledge may be required. 

As I got more into self-hosting applications such as my [Mastodon instance](/mastodon), I came across Owncast.   

During peak COVID lockdowns, like many others, live streaming is one of the ways I passed the time. While Twitch and YouTube got the job done, self-hosting was always in the back of my mind.  

It's been a while since I've live-streamed, so I never really went through the process of evaluating the self-hosted route with Owncast. 

While browsing for something on YouTube the other day, I ran into some of my [old live-stream recordings](https://www.youtube.com/playlist?list=PLsdMoYmuvh9ZtgB8U7FECR_8wKMYXJNAm). This got me thinking again, how difficult would it be to put together my own live-streaming setup. 

This post is the result of that exploration. 

In this post, I'll modify a .NET Aspire Starter Application template and show how to set up a self-hosted live-streaming application using Owncast and .NET Aspire.  

You can find the source code in the [lqdev/BYOwncastAspire](/github/BYOwncastAspire) repository.

![BYOwnCastAspire Web Frontend](https://github.com/user-attachments/assets/bdd9f901-8d8f-45be-9e37-4dce8459e481)

## What is Owncast?

The Owncast website describes the project as, "...a free and open source live video and web chat server for use with existing popular broadcasting software."

![Owncast admin server page](https://github.com/user-attachments/assets/769ec2d8-a3d1-4ec3-8271-c2de6e11ddd3)

It goes on to further describe some of the reasons I like Owncast, which are:

- **Self-hosting** - I'm in complete control over the service and my data
- **Open-source** - I can freely use and contribute to the project. Free in this case meaning both as in freedom and pizza.
- **Builds on top of open standards like RMTP** - Software that supports the Real-Time Messaging Protocol (RMTP) like OBS can immediately be leveraged. 
- **Fediverse compatible** - Your content and network federated across [the Fediverse](https://joinfediverse.wiki/What_is_the_Fediverse%3F). 

To learn more, check out the [Owncast website](https://owncast.online/). 

## What is .NET Aspire?

The .NET Aspire documentation describes it as, "...an opinionated, cloud ready stack for building observable, production ready, distributed applications.​ .NET Aspire is delivered through a collection of NuGet packages that handle specific cloud-native concerns."

Personally, the parts of .NET Aspire that matter to me are:

- **App composition** - The .NET Aspire programming model makes it easy to define and compose a variety of resources such as .NET projects, containers, and much more in a single place. In many cases, for commonly used services, these resources are exposed in the form of [integrations](https://learn.microsoft.com/dotnet/aspire/fundamentals/integrations-overview). These integrations further simplify the composition of applications.  
- **Tooling (Dashboard)** - .NET Aspire provides a set of tools and templates. However, my favorite is the dashboard. The dashboard provides me with a single place to view my resources, their configurations, and logs. 

Although in this post, I don't cover deployment, there is also the provisioning component provided by .NET Aspire which in many cases can [simplify your application deployments](https://learn.microsoft.com/dotnet/aspire/deployment/overview). 

To learn more, check out the [.NET Aspire documentation](https://learn.microsoft.com/dotnet/aspire/get-started/aspire-overview). 

## Build your application

This application makes a few modifications to the [.NET Aspire Starter Application template](https://learn.microsoft.com/dotnet/aspire/fundamentals/setup-tooling?tabs=linux&amp;pivots=dotnet-cli#net-aspire-project-templates). 

The application consists of a few projects:

- **BYOwncastAspire.AppHost** - This project is the entrypoint for .NET Aspire applications. This is where we'll configure the Owncast server as a container resource. 
- **BYOwncastAspire.Web** - Blazor web application. Although Owncast provides its own page where viewers can tune into your stream, by having a separate web application, preferably your own personal website, you can further enrich and customize how and where you publish content. 
- **BYOwncastAspire.ServiceDefaults** - This project contains default configurations for telemetry, health checks, resiliency, etc. No changes or updates required here. 

### Configure Owncast

There are many ways to host an Owncast server, with one of them being a container. 

In the context of Aspire which has built-in container support, you can easily add Owncast as a resource in your application. 

From the [Owncast documentation](https://owncast.online/quickstart/container/), once you've pulled the Owncast container image, you can start it with the following command.

```bash
docker run -v `pwd`/data:/app/data -p 8080:8080 -p 1935:1935 owncast/owncast:latest
```

This translates to the following in the *Program.cs* of the *BYOwncastAspire.AppHost* project.

```csharp
var owncast = builder
    .AddContainer(name: "owncast", image:"owncast/owncast")
    .WithBindMount("./data","/app/data")
    .WithHttpEndpoint(port:8080,targetPort:8080,name:"admin")
    .WithHttpEndpoint(port:1935,targetPort:1935,name:"streaming")
    .WithExternalHttpEndpoints();
```

This code:

- Adds the owncast container image
- Mounts a local *data* directory to the */app/data* directory in the container
- Maps port `8080` for the Owncast admin server and `1935` for RMTP server.
- Exposes the endpoints publicly

### Embed your stream

Owncast provides the ability to [embed your video stream onto a website](https://owncast.online/docs/embed/). 

Although we don't need a frontend because one is already provided by Owncast, by embedding your stream on your website you can provide a single place for your viewers to consume your content. 

In this case, we can treat the *BYOwncastAspire.Web* project as our website. 

To embed your stream to the website, add the following code to your *Home.razor* page.

```csharp
&lt;iframe
  src="http://localhost:8080/embed/video"
  title="Owncast"
  height="350px" width="550px"
  referrerpolicy="origin"
  allowfullscreen&gt;
&lt;/iframe&gt;
```

In this case, we're pointing to the endpoint listening on port `8080` of our `localhost`. When you deploy the application, you'd replace `src` with your domain. 

## Start your application

That's all there is in terms of configuration. 

To start the application:

1. Open the terminal 
2. Navigate to the *BYOwncastAspire.AppHost* project and run the following command.

    ```bash
    dotnet run
    ```

This will launch you into the .NET Aspire dashboard. At this point, you can further customize your Owncast server as well as the website.  

![BYOwncastAspire .NET Aspire Dashboard Resource Page](https://github.com/user-attachments/assets/c20d84d8-925a-4f80-9058-622466cb08e9)

## What next?

Now that you have your application running, in its current form, this application is only meant to serve as a sample of what you can do with Owncast and .NET Aspire. 

Some next steps might include:

- [Change your admin password and customize your Owncast server](https://owncast.online/docs/configuration/)
- [Configure object storage](https://owncast.online/docs/storage/)
- [Set up your steaming software](https://owncast.online/docs/broadcasting/obs/)
- [Deploy your application](https://learn.microsoft.com/dotnet/aspire/deployment/azure/aca-deployment)
- [Extend the app's functionality by building custom plugins and extensions](https://owncast.online/thirdparty/)

The plugins and extensions are particularly interesting to me because there may even be opportunities to experiment and insert AI capabilities at various layers such as moderation, translation, accessibility, show notes, chat summaries, etc. 

## Conclusion

In this post, I showed how you can combine the powerful live-steaming features of Owncast with the app orchestration and tooling in .NET Aspire to create rich self-hosted live-steaming applications.

If you use this sample as a starting point for your own live-streaming or self-hosting explorations, [send me a message](/contact). I'd love to hear about it.  
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire</link>
      <guid>https://www.luisquintanilla.me/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire</guid>
      <pubDate>2024-10-20 14:01</pubDate>
      <category>owncast</category>
      <category>dotnet</category>
      <category>indieweb</category>
      <category>fediverse</category>
      <category>aspire</category>
      <category>livestreaming</category>
      <category>twitch</category>
      <category>youtube</category>
      <category>blazor</category>
    </item>
    <item>
      <title>Configure Ollama on Dev Containers and VS Code</title>
      <description>&lt;![CDATA[
The source for my website is [hosted on GitHub](/github/luisquintanilla.me). As a result, I use VS Code as the text editor to author my posts. Typically, I use [Codespaces or github.dev](/colophon) to quickly draft and publish articles to simplify the process. 

As part of my authoring and publishing posts, there's a few things I do like:

1. Coming up with a title
2. Creating relevant tags to help with discovery and connecting related information on my website.
3. For long-form blog posts, I also include a description. 

This is generally relatively easy to do. However, sometimes I spend too much time coming up with something that truly summarizes and condenses the information in the post. It'd be nice if I had an assistant to help me brainstorm and workshop some of these items.

In comes AI. Now, I could use something like [Copilot](https://code.visualstudio.com/docs/copilot/overview) which would work wonderfully and easily plug into my workflow. However, my website is a labor of love and I don't make any money from it. In many instances, I've designed [various components to be as low-cost as possible](/posts/receive-webmentions-fsharp-az-functions-fsadvent). 

Recently, I created a post which showed [how to get started with Ollama on Windows](/posts/getting-started-ollama-windows). In this post, I'll show how you can do the same for your Dev Container environments. 

## Install Ollama

Assuming you already have a [Dev Container configuration file](https://code.visualstudio.com/docs/devcontainers/create-dev-container), add the following line to it:

```json
"postCreateCommand": "curl -fsSL https://ollama.com/install.sh | sh"
```

This command triggers when the container environment is created. This will install Ollama in your development environment. For more details, see the [Ollama download instructions](https://ollama.com/download/linux).

## Start Ollama

Now that Ollama is installed, it's time to start the service so you can get models and use them. To start the Ollama service on your Dev Conatiner environment, add the following line to your Dev Container configuration file.

```json
"postStartCommand": "ollama serve"
```

This command will run `ollama serve` when the Dev Container environment starts.

## Pull a model

To use Ollama, you're going to need a model. In my case, I went with [Phi-2](https://ollama.com/library/phi) because it's lightweight and space on Codespaces is limited.  

To get the model:

1. Open the terminal
1. Enter the following command:

    ```bash
    ollama pull phi
    ```

    After a few minutes, the model is downloaded and ready to use. 

1. Enter the following command to ensure that your model is now available

    ```bash
    ollama list
    ```

    The result should look like the following:

    ```text
    NAME            ID              SIZE    MODIFIED      
    phi:latest      e2fd6321a5fe    1.6 GB  6 seconds ago
    ```

For more details, see the [Ollama model library](https://ollama.com/library). 

## Conclusion

In this post, I showed how you can configure Ollama with Dev Containers to use AI models locally in your projects. In subsequent posts, I'll show how once the service is running, I use it as part of my authoring and publishing posts. You can see a preview of it in my [scripts directory](https://github.com/lqdev/luisquintanilla.me/blob/main/Scripts/ai.fsx). Happy coding! ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/install-ollama-vscode-devcontainer</link>
      <guid>https://www.luisquintanilla.me/posts/install-ollama-vscode-devcontainer</guid>
      <pubDate>2024-03-06 13:50</pubDate>
      <category>ollama</category>
      <category>vscode</category>
      <category>devcontainer</category>
      <category>ai</category>
      <category>llm</category>
      <category>opensource</category>
      <category>development</category>
    </item>
    <item>
      <title>Getting started with Ollama on Windows</title>
      <description>&lt;![CDATA[
Recently [Ollama announced support for Windows](/responses/ollama-windows-preview) in preview. In doing so, people who want to use AI models like Llama, Phi, and many others can do so locally on their PC. In this post, I'll go over how you can get started with Ollama on Windows. 

## Install Ollama

The first thing you'll want to do is install Ollama.

You can do so by [downloading the installer from the website](https://ollama.com/download/windows) and following the installation prompts. 

## Get a model

Once you've installed Ollama, it's time to get a model.

1. Open PowerShell
1. Run the following command

    ```powershell
    ollama pull llama2
    ```

In this case, I'm using llama2. However, you can choose another model. You could even download many models at once and switch between them. For a full list of supported models, see the [Ollama model documentation](https://ollama.com/library).

## Use the model

Now that you have the model, it's time to use it. The easiest way to use the model is using the REST API. When you install Ollama, it starts up a server to host your model. One other neat thing is, the REST API is [OpenAI API compatible](https://ollama.com/blog/openai-compatibility).

1. Open PowerShell
1. Send the following request:

    ```powershell
    (Invoke-WebRequest -method POST -Body '{"model":"llama2", "prompt":"Why is the sky blue?", "stream": false}' -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json
    ```

    This command will issue an HTTP POST request to the server listening on port 11434.

    The main things to highlight in the body:

    - *model*: The model you'll use. Make sure this is one of the models you pulled. 
    - *prompt*: The input to the model
    - *stream*: Whether to stream responses back to the client

    For more details on the REST API, see the [Ollama REST API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md). 

## Conclusion

In this post, I went over how you can quickly install Ollama to start using generative AI models like Llama and Phi locally on your Windows PC. If you use Mac or Linux, you can perform similar steps as those outlined in this guide to get started on those operating systems. Happy coding! ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/getting-started-ollama-windows</link>
      <guid>https://www.luisquintanilla.me/posts/getting-started-ollama-windows</guid>
      <pubDate>2024-03-05 10:32</pubDate>
      <category>ai</category>
      <category>ollama</category>
      <category>windows</category>
      <category>llm</category>
      <category>opensource</category>
      <category>llama</category>
      <category>openai</category>
      <category>generativeai</category>
      <category>genai</category>
    </item>
    <item>
      <title>Setting up your Rust development environment using Dev Containers</title>
      <description>&lt;![CDATA[
Setting up a new development environment that's configured with all the SDKs and tools you need to get started working can be a multi-step process that is error-prone. Especially when you don't know whether you want to commit to a specific technology or don't have too much space on your device, you might not want to install everything on your device. Additionally, cleaning up and removing everything you installed isn't always 100%. This is where development environments like Dev Containers can come in handy. 

## What are Dev Containers

A development container (Dev Container) lets you use a container as a full development environment. 

Because your development environment is in a container, the environment is isolated. The builds are reproducible and disposable. This means you can quickly stand up a new clean development environments and dispose of them just as easily. 

For more details, see the [Dev Containers website](https://containers.dev/).

If you have Docker installed, you can run them locally. Otherwise, you can run Dev Containers using GitHub Codespaces

## What are GitHub Codespaces

A codespace is a development environment that's hosted in the cloud.

For more details, see the [GitHub Codespaces website](https://github.com/features/codespaces).

## Set up Dev Container for Rust Development

1. Create a new directory.
1. Add a Dev Container configuration file called *.devcontainer.json* that contains the following content:

    ```json
    {
        "name": "Rust Development Environment",
        "image": "mcr.microsoft.com/devcontainers/rust:latest",
        "customizations": {
            "vscode": {
                "extensions": [
                    "rust-lang.rust-analyzer"
                ]
            }
        }
    }
    ```

    - **name**: A human friendly name to identify your development environment. It's not as important when you only have one environment but if you have different environments, it can be useful for differentiating between the various configurations. 
    - **image**: For the most customization, you can provide the path to a Docker file. However, to make things easier, there are a set of prebuilt container images. In this case, I'm using the [Rust image](https://github.com/devcontainers/images/tree/main/src/rust).
    - **customizations.vscode.extensions**: A list of Visual Studio Code extensions that can help you with your development. In this case, it's the [*rust-analyzer*](https://marketplace.visualstudio.com/items?itemName=rust-lang.rust-analyzer).

At minimum, this is all you need to get started. 

## Start your environment 

[Visual Studio Code provides good support for Dev Containers](https://code.visualstudio.com/docs/devcontainers/tutorial). 

1. Install [VS Code](https://code.visualstudio.com/download)
1. Open your directory where you configured your Dev Container in Visual Studio Code
1. Install the [Dev Containers VS Code extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers).
1. Open the command palette in VS Code. In the menu bar, select **View &gt; Command Palette.**
1. Enter the following command **&gt;Dev Containers: Open Dev Container**. This will start building your dev container. Wait for it to start.

## Check your environment

Once your Dev Container starts, check that `rustc` is installed.

1. Open the terminal.
1. Run `rustc -h`. A help message should output to the console for the Rust compiler. 

## Build your app

For simplicity, I've used the [Hello World sample from the Rust By Example website](https://doc.rust-lang.org/rust-by-example/hello.html).

1. Create a new file called *hello.rs*.
1. Add the following code:

    ```rust
    fn main ()
    {
        println!("Hello World!");
    }
    ```
1. Compile your program

    ```bash
    rustc hello.rs
    ```

    This will generate an executable called *hello*

1. Run your app

    ```bash
    ./hello
    ```

    The application should print out the string "Hello World" to the console. 

That's it! You now have a Rust development environment to get started learning and building apps. 

## Conclusion

Dev Containers make it easy for you to configure full-featured development environments. When paired with Visual Studio Code and GitHub Codespaces, they can help you focus on learning and building applications rather than setting up your environment. You can find a [final version of the configured environment and app on GitHub](https://github.com/lqdev/rust-codespace-sandbox). Happy coding! 

## Additional Resources

- [Rust in Visual Studio Code](https://code.visualstudio.com/docs/languages/rust)
- [Rust by Example](https://doc.rust-lang.org/rust-by-example/index.html)
- [Developing Inside a Container](https://code.visualstudio.com/docs/devcontainers/containers)]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/setting-up-rust-dev-env-devcontainers-vscode</link>
      <guid>https://www.luisquintanilla.me/posts/setting-up-rust-dev-env-devcontainers-vscode</guid>
      <pubDate>2024-01-31 21:05</pubDate>
      <category>rust</category>
      <category>vscode</category>
      <category>devcontainers</category>
      <category>codespaces</category>
      <category>programming</category>
      <category>technology</category>
      <category>docker</category>
      <category>containers</category>
      <category>github</category>
    </item>
    <item>
      <title>POSSE to Mastodon using RSS and Azure Logic Apps</title>
      <description>&lt;![CDATA[
## Introduction

[Over the past year, I've made this website the main place where I post content](/notes/weblogging-rewind-2023/). The easiest way to [subscribe to content on my website](/feed) is through the various RSS feeds. However, I have accounts on other platforms like X (formerly Twitter), Bluesky, and Mastodon where I'd still like to repost my content to. Since the [changes to the Twitter API](https://techcrunch.com/2023/03/29/twitter-announces-new-api-with-only-free-basic-and-enterprise-levels/), the only place I cross-post to is Mastodon. The main reason behind it is, as of now, it's one of the few platforms that allows me to automate post creation via its REST APIs without restrictions. A large part of that is I self-host my own Mastodon instance but I assume there aren't as many restrictions when using other instances like mastodon.social. The way I automate posting is by setting up workflows using Azure Logic Apps. These workflows subscribe to my various RSS feeds and whenever a new post is published, they make an HTTP request to my Mastodon instance to create a new post. I've been doing this for some time but never got around to documenting it. This blog post goes into more details about how to set up these workflows.  

## What is POSSE

[POSSE](https://indieweb.org/POSSE) is short for "Post on your Own Site Syndicate Elsewhere". It's one of the patterns adopted within IndiWeb communities and projects. The main idea is, your website or a website you own or have administrative rights to becomes the main platform where you publish your content. Effectively, it's the cannonical version of your content. Once your content is on your website, you can optionally choose to distribute it on other platforms. 

## What is RSS

[RSS](https://en.wikipedia.org/wiki/RSS) is short for Really Simple Syndication. Accorting to Wikipedia, this protocol "allows users and applications to access updates to websites in a standardized, computer-readable format". 

## What is Mastodon

[Mastodon](https://joinmastodon.org/) is an [open-source](https://github.com/mastodon/mastodon) decentralized microblogging platform built on the [ActivityPub](https://activitypub.rocks/) protocol and part of the larger collective of federated systems known as the [Fediverse](https://fediverse.info/). 

## What are Azure Logic Apps

If you've used workflow automation systems like [IFTTT](https://ifttt.com/), then you generally know what Azure Logic Apps are. 

A more formal definition from the Azure documentation - "Azure Logic Apps is a cloud platform where you can create and run automated workflows with little to no code. By using the visual designer and selecting from prebuilt operations, you can quickly build a workflow that integrates and manages your apps, data, services, and systems."

For more details, see the [Azure Logic Apps documentation](https://learn.microsoft.com/azure/logic-apps/logic-apps-overview).

## Prerequisites

Since you'll be creating Logic App Resources on Azure, you'll need an [Azure account](https://aka.ms/free).

## Create Consumption Azure Logic App Resource

There's various ways to create an Azure Logic App Resource but the easiest one for this relatively simple workflow is using the Azure Portal. For more details, see the [create a consumption logic app resource documentation](https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#create-a-consumption-logic-app-resource).

For the most part you can leave the defaults as is. Since the intended use for this workflow is personal and I don't need enterprise features, I chose to create my logic app using the consumption plan. 

When prompted to choose **Plan Type**, select **Consumption**. 

## The Workflow

The workflow is relatively simple. Whenever a new item is posted to an RSS feed, make an HTTP POST request to the Mastodon API

```mermaid
flowchart TD
    A["RSS"] --&gt; B["HTTP (Mastodon API)"]
```

Once your Logic App resource deploys to Azure, create a new logic app using the Blank Template. For more details, see the [select a blank template documentation](https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#select-the-blank-template).

This will launch you into the Logic app designer UI where you can begin to configure your workflow.

### RSS trigger

The first thing you'll want to do is set up the trigger that initiates the workflow. The trigger in this case will be new posts on an RSS feed.

In the Logic app designer, search for *RSS* and add it as a trigger.

Once the RSS trigger is added to your workflow, configure it as follows:

- **The RSS feed URL**: The feed you want want to get posts from. In this case, I'm using my blog posts [feed](/posts/feed.xml). 

    **TIP: The RSS feed can be ANY feed. So if you wanted to subscribe and repost items from your favorite blog or website, you can as well. Just note though that if you don't own the content it might not be something the author wants you doing and for websites that update multiple times a day, it might produce a lot of noise in your feed. Other than that though, subscribe and post away!**

- **How often do you want to check for items?**: The frequency at which you want to poll the RSS feed and check for new posts. In my case, I don't post very often but I do want my posts to be published on Mastodon within an hour of publishing. Therefore, I chose 1 hour as the update frequency. 

### HTTP action

Now that your trigger is configured, it's time to do someting with the latest posts. In this case, since I want to create a new post on Mastodon, I can do so via their REST API. 

#### Get Mastodon credentials

One thing that you'll need to publish posts to Mastodon is an application token. The easiest way to get one is using the Web UI. 

To get your app credentials through the web UI:

1. In the Mastodon Web UI, select **Preferences**.
1. In the preferences page, select **Development**.
1. In the development preferences tab, select **New application**.
1. In the new application page:
  - Provide a name for your application
  - Choose the **write:statuses** scope checkbox. For details on required scopes to post statuses, see the [Post a new status REST API documentation](https://docs.joinmastodon.org/methods/statuses/#create)
  - Select **Submit**

If successful, this will create credentials and a token you can use to send authenticated requests to the Mastodon API.

#### Configure HTTP action

Now that you have your credentials, you can configure your HTTP action in the Logic Apps workflow.

In the Logic App designer:

1. Select **+ New Step**.
1. Search for *HTTP* and add it as an action. 
1. Once the HTTP action is added, configure it as follows:

- **Method**: The HTTP Method. To create statuses, choose **POST** from the dropdown.
- **URI**: The endpoint to make the requests to. For mastodon, it's `https://&lt;HOST&gt;/api/v1/statuses`. Make sure to replace `&lt;HOST&gt;` with your Mastodon instance. In my case, it's `toot.lqdev.tech` since that's where I host my mastodon instance. 
- **Headers**: HTTP Headers to use. In this case, set the following:

    | Key | Value |
    | --- | --- |
    | Content-Type | application/x-www-form-urlencoded |

- **Queries**: URL Query Parameters to add to the request. This is where you'll set your access token.

    | Key | Value |
    | --- | --- |
    | access_token | `[YOUR-ACCESS-TOKEN]` |

    Make sure to replace `[YOUR-ACCESS-TOKEN]` with the token credential generated in the Mastodon Web UI.
- **Body**: The content to be published in your Mastodon post. This is entirely up to you though at minumum, you'll add the following to the text feed. `status=&lt;YOUR-CONTENT&gt;`. `&lt;YOUR-CONTENT&gt;` is what will be displayed in the Mastodon post. 

    One of the nice things about Azure Logic Apps is, properties from previous steps are available to you in subsequent steps. Since our trigger is an RSS feed, we can get access to the feed and item properties of our feed in the HTTP action. If all you wanted was to post the tile and URL, you can do that using the **Feed title** and **Primary Feed Link** properties. For more details, see the [RSS connector documentation](https://learn.microsoft.com/connectors/rss/). 

## Save and run

That's it! Now you just need to select **Save** in the Logic app designer page. 

Once it's saved, click **Run trigger** which will kick off your trigger. If you have anything recent to publish and everything is configured correctly, it should show up in your Mastodon feed.

## Conclusion

By publishing content on your own website first, you're in full control of your content. Regardless of which platforms come and go, you won't have to adjust to those changes because your content is not locked in to those platforms. However, that doesn't mean you can't also publish your content there. Using protocols like RSS make it easy to subscribe to updates on your website. Using REST APIs provided by the respective platforms, you can automate publishing these updates. To further automate and simplify this process, you can use services like Azure Logic Apps to make publishing to all places easy. ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/rss-to-mastodon-posse-azure-logic-apps</link>
      <guid>https://www.luisquintanilla.me/posts/rss-to-mastodon-posse-azure-logic-apps</guid>
      <pubDate>2023-12-24 16:40</pubDate>
      <category>azure</category>
      <category>mastodon</category>
      <category>indieweb</category>
      <category>logicapps</category>
      <category>fediverse</category>
      <category>posse</category>
      <category>internet</category>
      <category>web</category>
      <category>blogging</category>
      <category>blog</category>
      <category>automation</category>
      <category>programming</category>
      <category>rss</category>
    </item>
    <item>
      <title>Computing website metrics using GitHub Actions</title>
      <description>&lt;![CDATA[
## Introduction 

I recently posted my [(We)blogging Rewind](/notes/weblogging-rewind-2023/) and [(We)blogging Rewind Continued](/notes/weblogging-rewind-2023-continued) for 2023. In these posts, I discuss some analysis I did of my posting behaviors over the past year on this website. I don't use platforms like Google Analytics to track visitors. I used to self-host a [GoatCounter](https://www.goatcounter.com/) instance but personally I don't really care about traffic so I got rid of that. There are some standard reports I get from my [CDN provider on Azure](https://learn.microsoft.com/azure/cdn/cdn-advanced-http-reports), but again I don't really care about those metrics. What I do care about though is my output and understanding what I'm publishing and the topics that were important to me at any given time. In the case of those blog posts, it was for the year 2023. Given that I had already done the analysis and had written the script, I thought, why not automate it and run it on a more regular basis to have monthly summaries. Since my blog and scripts are already on GitHub, it makes sense to create a GitHub Action workflow. In this post, I discuss in more details what my post analytics script does and how I configured my workflow in GitHub Actions to run the script on the first of every month.  

## The script

This script loads the various posts on my website and computes aggregate metrics based on post types and their metadata. You can find the full script at [stats.fsx](/resources/snippets/lqdev-me-website-post-metrics).

### Loading files

The following are convenience functions which I use as part of my website build process. 

In general these functions:

- Load the individual post files
- Parse the content and YAML metadata

```fsharp
let posts = loadPosts()
let notes = loadFeed ()
let responses = loadReponses ()
```

Instead of building new custom functions, I can repurpose them and apply additional transformations to compute aggregate statistics.

### Computing aggregate statistics

Once the posts are loaded, I apply transformations on the collections to compute aggregate metrics:

#### Annual post counts

The following are annual aggreagates of blog posts, notes, and responses. 

##### Blog posts

Takes the blog post collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order.

```fsharp
let postCountsByYear = 
    posts
    |&gt; Array.countBy (fun (x:Post) -&gt; DateTime.Parse(x.Metadata.Date) |&gt; _.Year)
    |&gt; Array.sortByDescending fst 
```

##### Notes

Takes the note collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order.

```fsharp
let noteCountsByYear = 
    notes
    |&gt; Array.countBy (fun (x:Post) -&gt; DateTime.Parse(x.Metadata.Date) |&gt; _.Year)
    |&gt; Array.sortByDescending fst
```

##### Responses

Takes the response collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order.

```fsharp
let responseCountsByYear = 
    responses
    |&gt; Array.countBy (fun (x:Response) -&gt; DateTime.Parse(x.Metadata.DatePublished) |&gt; _.Year)
    |&gt; Array.sortByDescending fst
```

#### Response counts by type

Takes the response collection, parses the published date, filters it for the current year, and computes counts by the post type (reply, bookmark, reshare, star).

```fsharp
let responsesByType = 
    responses
    |&gt; Array.filter(fun x -&gt; (DateTime.Parse(x.Metadata.DatePublished) |&gt; _.Year) = DateTime.UtcNow.Year)
    |&gt; Array.countBy(fun x -&gt; x.Metadata.ResponseType)
    |&gt; Array.sortByDescending(snd)
```

#### Tag counts (responses)

Takes the response collection, parses the published date, filters it for the current year, and computes counts by the tag name, and sorts in descending order using the count.


```fsharp
let responsesByTag = 
    responses
    |&gt; Array.filter(fun x -&gt; (DateTime.Parse(x.Metadata.DatePublished) |&gt; _.Year) = DateTime.UtcNow.Year)
    |&gt; Array.collect(fun x -&gt; 
            match x.Metadata.Tags with
            | null -&gt; [|"untagged"|]
            | [||] -&gt; [|"untagged"|]
            | _ -&gt; x.Metadata.Tags
        )
    |&gt; Array.countBy(fun x -&gt; x)
    |&gt; Array.sortByDescending(snd)
```

#### Domain counts (responses)
 
Takes the response collection, parses the published date, filters it for the current year, and computes counts by the target URL host name, and sorts it in descending order using the count.

```fsharp
let responsesByDomain = 
    responses
    |&gt; Array.filter(fun x -&gt; (DateTime.Parse(x.Metadata.DatePublished) |&gt; _.Year) = DateTime.UtcNow.Year)
    |&gt; Array.countBy(fun x -&gt; Uri(x.Metadata.TargetUrl).Host)
    |&gt; Array.sortByDescending(snd)
```

### Displaying counts

Since the `countBy` function is the one used to compute the counts, this produces a tuple. The tuple though could be `string` or `int`. Therefore, I set the collection of entry counts to use a generic `'a` for the first item in the tuple. I'm also able to control using `n` whether to display the entire collection by using `-1` as input or a limit when `n &gt;= 0`.

```fsharp
let printEntryCounts&lt;'a&gt; (title:string) (entryCounts:('a * int) array) (n:int) = 
    printfn $"{title}"

    match n with 
    | n when n = -1 -&gt; 
        entryCounts
        |&gt; Array.iter(fun x -&gt; printfn $"{fst x} {snd x}")
        |&gt; fun _ -&gt; printfn $""
    | n when n &gt;= 0 -&gt; 
        entryCounts
        |&gt; Array.take n
        |&gt; Array.iter(fun x -&gt; printfn $"{fst x} {snd x}")
        |&gt; fun _ -&gt; printfn $""
```

The result of running this script produces the following results:

```text
Blogs
2023 5
2022 7

Notes
2023 34
2022 36

Responses
2023 216
2022 146

Response Types
bookmark 151
reshare 48
reply 10
star 7

Response Tags
ai 104
llm 42
untagged 41
opensource 31
internet 17

Domains
github.com 15
huggingface.co 11
arxiv.org 10
openai.com 6
www.theverge.com 4
```

## The workflow file

The workflow file is a GitHub Actions workflow which you can find in my [website repo](https://github.com/lqdev/luisquintanilla.me/blob/main/.github/workflows/stats.yml).  

### Triggers

I don't really want the script to run every time I publish my website. Instead, I just want to have these aggregate values computed on a monthly basis. Optionally though, I'd like to be able to run ad-hoc reports and trigger this job manually.

The triggers in my workflow file look like the following:

```yaml
schedule: 
  - cron: '30 0 1 * *'
workflow_dispatch: 
```

Using cron job syntax, I use the `schedule` trigger to configure the script to run at 12:30 AM on the 1st day of every month.

The `workflow_dispatch` trigger is there so I can manually trigger this job.

## Steps

The steps in the workflow file are the following:

- Check out the repo

    ```yaml
    - uses: actions/checkout@v2
    ```


- Install the .NET 8 SDK

    ```yaml
    - name: Setup .NET SDK 8.x
      uses: actions/setup-dotnet@v1.9.0
      with: 
        dotnet-version: '8.0.x'    
    ```

- Restore dependencies

    ```yaml
    - name: Install dependencies
      run: dotnet restore    
    ```

- Build the project

    ```yaml
    - name: Build project
      run: dotnet build --no-restore  
    ```

- Run the script and display metrics

    ```yaml
    - name: Display Post Metrics
      run: dotnet fsi Scripts/stats.fsx
    ```

## Conclusion

If you have scripts that you run on a repo on a fairly regular basis, consider using GitHub Actions to automate the execution of these scripts. Happy coding! ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/website-metrics-github-actions</link>
      <guid>https://www.luisquintanilla.me/posts/website-metrics-github-actions</guid>
      <pubDate>2023-12-24 13:24</pubDate>
      <category>github</category>
      <category>cicd</category>
      <category>website</category>
      <category>blog</category>
      <category>blogging</category>
      <category>web</category>
      <category>internet</category>
      <category>fsharp</category>
      <category>dotnet</category>
    </item>
    <item>
      <title>Using Generative AI to produce Spotify Clips</title>
      <description>&lt;![CDATA[
## Introduction

While listening to music on Spotify last night, this thought hit me. It's been cooking for some time so now you get to read about it. Spotify and YouTube, if you build this into your platforms for songs or podcasts, [contact me](/contact) for info on where to send the check :joy:. As the title of the post suggests, this is a thought experiment on how you might use Generative AI to produce Spotify Clips. This is not limited to songs but could also be extended to audio-only podcasts. However, given the long-form nature of podcasts it might be extremely resource intensive to produce clips per episode. This post is not exactly a how-to guide, though there are some practical examples of how some of the steps in the workflow might work using [Microsoft Copilot (formerly Bing Chat)](https://copilot.microsoft.com/).

## Spotify Clips

Let's start off by talking about what Spotify Clips are. Back in March of 2023, [Spotify introduced Clips to their platform](https://artists.spotify.com/blog/spotify-clips-get-started-short-form-video-stream-on). Spotify describes [Clips](https://clips.byspotify.com/) as "short, under-30-second vertical videos that are uploaded directly to Spotify for Artists. They’re designed to put you and your music front-and-center, so they can be attached to your artist profile, to a song, to an album, or to an upcoming release.". When paired with a song, you can think of them as short music videos. Music videos being the videos made to accompany songs that played on TV channels like Music Television (MTV) before Jersey Shore and Catfish. Music videos are still in production today and they generally serve the same purpose. They provide a visual representation of the story being told by the song.  

## Generative AI for video

An overwhelming majority of Generative AI models only generate text. With models like DALL-E and Stable Diffusion, you can also generate images. Other models to generate audio and video have started coming on the scene. Most recently, Google announced [VideoPoet](https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html), a large language model for zero-shot video generation. There's many interesting techniques that VideoPoet introduces but I'll keep it to one of the more simple scenarios and the technique used in this post, text-to-image. Given a text prompt, VideoPoet can generate by default, a short 2-second clip. You can use VideoPoet as well to extend a clip so you could go well beyond the default 2 seconds. That however is outside the scope of this post. With that in mind, let's talk about how we might use a Generative AI model and apply it to the context of Spotify Clips.

## Generative AI Spotify Clips workflow

The following is a rough idea of how you might use song information like lyrics to 

```mermaid
flowchart TD
  A["Get song lyrics"] --&gt; B["Generate scenes and script"]
  B --&gt; C["Generate prompts for each scene"]
  C --&gt; D["Generate video clips"]
  D --&gt; E["(Optional) Edit clips"]
  D --&gt; F["Stitch clips together"]
  E--&gt;F
```

This is roughly what Google did in their VideoPoet samples. They "...produced a short movie composed of many short clips generated by the model. For the script, we asked Bard to write a series of prompts to detail a short story about a traveling raccoon. We then generated video clips for each prompt, and stitched together all resulting clips". The resulting video looks like the following:

[![An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities](http://img.youtube.com/vi/70wZKfx6Ylk/0.jpg)](https://www.youtube.com/watch?v=70wZKfx6Ylk "An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities")

One thing to note is that the videos generated are YouTube Shorts which are vertical as well. This makes it easier to produce videos in the format Spotify expects Clips to be in.

## AI Generated Spotify Clips in practice

Now that we know the general process, what does it actually look like in practice? To help me with that, I used Microsoft Copilot.

### Generate scenes and script

In the example shared by Google, they used a prompt to generate a script made up of a series of prompts. In the case of songs on Spotify, we can use that as context and the starting point for our script. From there, we can ask Copilot to compose scenes for the verses in the song. We can also make sure as these scenes are generated, that there is a smooth transition between them and the storytelling is consistent. 

Let's take a look at what this might look like using the Chicano Batman song "A Hundred Dead and Loving Souls". Why that one? Just check out my [Spotify Wrapped](/notes/spotify-wrapped-2023). The general theme of that song is about the cycles of life experienced across millenia and generations. 

**Prompt**

Given the following prompt:

```text
Using the song information below as context, generate a script for a story that visually represents the song lyrics. Ensure that the script is broken down into individual scenes and the scenes are linear and cohesively transition.

&lt;song&gt;
&lt;title&gt;A Hundred Dead and Loving Souls&lt;/title&gt;
&lt;lyrics&gt;
I am the sand stuck under miles of rock
watching the dry air sweep away my thoughts
as we travel protected by shards of glass
driving through the history of a million memories

In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
Climbing a thousand unclimbed trees
I don't know I want to know how it would feel
to be on top of one of these!

I am the root
quickly ascending
up through the surrounding dirt
quickly devoured by a hungry sun
quickly devoured by a hungry son
who is my father
and we are alike
we look alike we talk alike
we are alike
we are alike
we are alike we look alike
we talk alike

Thats why Im hungry
like the bellowing sky
this is no lie
as sure as light
entering eye
want to stay high
everything's cool too cool so cool
and I remember that everything falls

So I take a look around me now, as I speak
and I imagine a disinterested crowd
talking aloud
and I think of what I want most
and the crowd disappears
and a fixated gaze envelopes my wondering mind
and then she recruits and takes me to this place
I've been there before and that's where I saw her face

In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
Climbing a thousand unclimbed trees
I don't know I want to know how it would feel
to be on top of one of these!
&lt;/lyrics&gt;
&lt;/song&gt;

Story: 
```

**Response**

The response looks like the following:

```text
# Script for A Hundred Dead and Loving Souls

## Scene 1: The Sand
- A close-up shot of a grain of sand under a microscope, showing its intricate structure and texture.
- The camera zooms out to reveal that the sand is part of a vast desert landscape, with dunes and rocks stretching to the horizon.
- A voice-over of the protagonist (a young man) narrates the first verse of the song, expressing his feelings of being trapped and forgotten under the weight of the world.
- The camera pans to show a car driving on a dusty road, with the protagonist and his father inside. They are both wearing sunglasses and hats to protect themselves from the sun.
- The car is filled with boxes and bags, indicating that they are moving to a new place.
- The father is driving and listening to the radio, while the protagonist is looking out the window, bored and restless.
- The radio plays a news report about a nuclear fusion experiment in South Korea, followed by a song by a popular band.
- The father comments on the news, saying that it is a breakthrough for humanity and that he hopes to see it in his lifetime.
- The protagonist does not respond, but thinks to himself that he does not care about anything anymore.

## Scene 2: The Sea
- A montage of shots showing the protagonist and his father arriving at their new home, a small town by the sea.
- The father is excited and optimistic, while the protagonist is indifferent and detached.
- The father unpacks their belongings and sets up their new house, while the protagonist wanders around the town, exploring the beach, the pier, and the streets.
- The protagonist sees a group of teenagers hanging out at a skate park, laughing and having fun. He feels out of place and lonely.
- The protagonist walks to the edge of the pier, where he sees a sign that says "No diving".
- He looks down at the water, where he sees hundreds of dead fish floating on the surface, victims of a recent oil spill.
- He hears the chorus of the song in his head, imagining himself diving into the sea of a hundred dead and loving souls, and climbing a thousand unclimbed trees.
- He wonders what it would feel like to be free and alive, instead of being stuck and dead inside.

## Scene 3: The Root
- A flashback scene, showing the protagonist as a child, playing in a garden with his father.
- The father is a botanist, and he teaches his son about different plants and flowers, showing him how to care for them and appreciate their beauty.
- The son is curious and eager, and he asks his father many questions about the natural world.
- The father tells his son that he is a root, quickly ascending up through the surrounding dirt, and that he will grow into a strong and beautiful tree one day.
- The son smiles and hugs his father, saying that he loves him and that he wants to be like him.
- The father smiles and hugs his son back, saying that he loves him too and that he is proud of him.

## Scene 4: The Sun
- A flash-forward scene, showing the protagonist as a young adult, working in a laboratory with his father.
- The father is a nuclear physicist, and he is working on a project to harness the power of nuclear fusion, the same process that fuels the sun.
- The son is his assistant, and he helps his father with the experiments and calculations, following his instructions and commands.
- The father tells his son that he is a hungry sun, quickly devoured by a hungry son, and that he will make a great contribution to science and humanity one day.
- The son nods and agrees, but he does not look happy or satisfied. He feels pressured and obligated to follow his father's footsteps, even though he does not share his passion or vision.
- The son thinks to himself that he is hungry, like the bellowing sky, and that he wants to stay high, away from the reality and expectations that weigh him down.

## Scene 5: The Crowd
- A present-day scene, showing the protagonist at a party, surrounded by a crowd of people.
- The party is hosted by one of the teenagers that he saw at the skate park, who invited him out of pity and curiosity.
- The party is loud and chaotic, with music, drinks, and drugs everywhere.
- The protagonist feels uncomfortable and anxious, and he does not fit in with the crowd. He does not know anyone, and no one knows him.
- He tries to make small talk with some of the guests, but they are either rude, bored, or high, and they do not pay attention to him.
- He feels invisible and insignificant, and he wishes he could disappear.
- He takes a look around him, as he speaks, and he imagines a disinterested crowd, talking aloud, and he thinks of what he wants most.

## Scene 6: The Gaze
- A close-up shot of a pair of eyes, staring intently at the protagonist.
- The eyes belong to a girl, who is also at the party, but who is different from the rest of the crowd. She is quiet and mysterious, and she has a captivating and mysterious aura.
- The girl notices the protagonist, and she is intrigued by him. She sees something in him that no one else sees, something that she relates to and understands.
- She walks up to him, and she introduces herself. She says her name is Luna, and that she likes his song. She says she heard him humming it to himself, and that she knows the lyrics by heart.
- The protagonist is surprised and confused, and he asks her how she knows his song. He says he wrote it himself, and that he never shared it with anyone.
- The girl smiles and says that she knows his song, because she wrote it too. She says she has a gift, or a curse, depending on how you look at it. She says she can hear the songs that people sing in their souls, and that she can sing them back to them.
- The protagonist is shocked and amazed, and he asks her to prove it. He asks her to sing his song, or any song that he knows.
- The girl nods and agrees, and she sings his song, word for word, note for note, perfectly. She sings it with emotion and expression, as if she wrote it herself.
- The protagonist is speechless and mesmerized, and he feels a connection with the girl that he never felt with anyone before. He feels like she knows him, and he wants to know her.
- The girl stops singing, and she looks at him with a fixated gaze, that envelopes his wondering mind. She says she likes him, and she asks him if he wants to go with her to a place that she knows.
- The protagonist nods and agrees, and he follows her out of the party, leaving the crowd behind.

## Scene 7: The Place
- A montage of shots showing the protagonist and the girl driving to a secluded spot in the woods, where they park their car and get out.
- The spot is a clearing, where there is a large tree, with a rope swing hanging from one of its branches.
- The tree is the only one of its kind in the area, and it stands out from the rest of the forest. It is tall and majestic, with green leaves and red flowers.
- The girl tells the protagonist that this is her place, and that she comes here often to escape from the world. She says she loves this tree, and that she calls it Luna, after herself.
- The protagonist tells the girl that this is his place too, and that he has been here before. He says he saw this tree in his dreams, and that he always wanted to climb it.
- The girl smiles and says that they have more in common than they thought, and that they are meant to be together. She says she wants to show him something, and she asks him to follow her.
- The girl climbs the tree, using the rope swing as a support. She is agile and graceful, and she reaches the top in no time.
- The protagonist follows her, using the rope swing as well. He is clumsy and nervous, and he struggles to keep up with her.
- The girl reaches the top of the tree, where she finds a small platform, made of wood and rope. She sits on the platform, and she looks at the view.
- The view is breathtaking, and it shows the sea, the town, and the sky, all in one panorama. The sun is setting, and it casts a golden glow on everything.
- The girl waves at the protagonist, and she invites him to join her. She says this is the best place to watch the sunset, and that she wants to share it with him.
- The protagonist reaches the top of the tree, where he joins the girl on the platform. He sits next to her, and he looks at the view. He is awestruck, and he agrees that this is the best place to watch the sunset.
- The girl leans in and kisses the protagonist, and he kisses her back. They embrace each other, and they watch the sunset together.
- The chorus of the song plays in the background, as the scene fades to black.
```

Not too bad for a first pass. From here you might want to continue tweaking the different scenes but at least you have a starting point. 

### Generate prompts for each scene

Using the text-to-image technique, I first need to generate prompts to guide the model in the video generation process. In the case of this sample, you could take each of the bullet points in the scenes and generate a video for each of them. 

At the time of this writing, I didn't have access to a video generation model and I don't think VideoPoet is open to the public yet. Therefore, I had to slightly modify what I was generating. Instead of generating a video, I used Copilot to generate images using DALL-E. The concept is the same, except instead of generating videos, I'm generating still images. 

I could've applied the same concept of one image per bullet point. However, for brevity, I chose to summarize all of the bullet points in a scene into a single prompt. 

**Prompt**

Given the following prompt:

```text
Using storyboarding techniques, for each scene, generate a single prompt to use as input for DALL-E to generate an image. Since there are 7 scenes, there should be only 7 prompts. Distill as much of the concepts from each of the bullet points in each scene.
```

**Response**

The results from that prompt are the following:

```text
Scene 1: The Sand
    A grain of sand under a microscope and a car driving on a dusty road in a desert
Scene 2: The Sea
    A boy standing on a pier looking at a sea of dead fish and a sign that says “No diving”
Scene 3: The Root
    A father and a son playing in a garden with different plants and flowers
Scene 4: The Sun
    A father and a son working in a laboratory with a nuclear fusion reactor
Scene 5: The Crowd
    A boy at a party surrounded by a crowd of people with music, drinks, and drugs
Scene 6: The Gaze
    A girl with a pair of eyes that can hear the songs that people sing in their souls
Scene 7: The Place
    A boy and a girl sitting on a platform on top of a tree with a rope swing and a view of the sea, the town, and the sunset
```

Again, not too bad for a first pass. The main points of the scene are captured. I can already foresee there are a few scenes like "The Sand" which might be difficult to illustrate in a single image. 

### Generate images (video clips)

Now that we have individual prompts, it's time to generate some images. Again, I'm doing this because I don't have access to a video generation model but the process would be generally the same. 

**Prompt**

```text
Use the prompts for each of the scenes to generate images with DALL-E. Render them as pencil art.
```

**Response**

For brevity, I'll only include some of the images generated:

"The Sand"

![AI genreated image of a grain of sand under a microscope and a car driving on a dusty road in a desert](/files/images/sand-car-dirt-road.jpg)

"The Sea"

![AI generated image of a boy standing on a pier looking at a sea of dead fish and a sign that says “No diving”](/files/images/child-dock.jpg)

"The Root"

![AI generated image of a father and a son playing in a garden with different plants and flowers”](/files/images/father-son-playing.jpg)

The pencil art part was something I thought would look nice to illustrate this instead of digital art or photorealism. That was a personal choice though. 

As we can see from the images, in general they're not too bad. When not condensed into a single image though and you have an image or video per frame in a scene, the generated content can be richer. 

### Edit and stitch videos

For the image generation example, this seems like a natural stopping point. However, with videos, you can imagine using AI to edit these videos. The VideoPoet post mentions how you might be able to do that. Part of that editing might also include lengthening the video clips. Once you're happy with your videos, you would then stitch them together to compose a larger clip. 

## Improvements

Tons! I used the simplest tools available at hand to validate whether my idea is feasible.  

### Videos

The most obvious improvement here would be to actually generate the videos. Whether that's using VideoPoet or another video generation model, the whole point is to end up with a video clip to upload to Spotify.

### Prompts

The prompts I used were not complex at all. I tried to keep them as simple as possible and in many cases, I wasn't the one providing the prompts, Copilot generated them for me. I would try to use more complex prompts to further guide the model in the formats I expect the outputs to be in and give more details for the model to generate better results. 

### Code

All of this was done using Copilot in the browser. As a starting point to validate my ideas this was fine. However, this approach is not composable, testable, reproducible, or scalable. I'd convert these steps to code in order to get all of the things Copilot doesn't give me. 

### Timestamp sync

One of the features Spotify offers is lyrics that are synced to the song. Similarly, being able to sync these generated clips to the song would be important so the visuals match the 

## Challenges

### Consistency

One challenge that someone brought up when I was explaining this idea was, how do you ensure that the visuals are consistent throughout all of the video clips.

That is a great question I don't have a great answer for, though I have ideas. 

If there aren't many changes throughout scenes, you could just opt to keep lengthening the video clip. 

If there are flashbacks or different visuals between scenes, you might achieve consistency by  using the image-to-video modality. Using a snapshot from a frame in a previous video clip, you might try to edit that snapshot to use in between transitions and then lengthen the video to produce the next scene. 

### Power Consumption

As fascinating as this is, is it worth the energy and cooling demands it requires when using large models? Maybe not in its current form, but maybe it makes sense when using smaller models or more power efficient compute. 

### People

For music videos, there's a human element to it both from the performers and the people who produce them. Again, the Generative AI is impressive and for someone like me without any artistic skills, it can be an empowering tool. Although I'm tempted to see what an [AI Rickroll](https://www.youtube.com/watch?v=dQw4w9WgXcQ) might look like, I prefer the human version. :slightly_smiling_face:

## Conclusion

In this post I discussed how you might use Generative AI models like VideoPoet to generate short videos for Spotify Clips. Since I don't have acess to VideoPoet or other video generation models at this time, I decided to show practical image generation examples instead using models like DALL-E to illustrate the general concepts. It's not wihtout its challenges and there's a ton of improvements I can think of but it was a fun experiment regardless. What else would you use these Generative AI models for? 

You can find an addendum to this post [here](/notes/genai-spotify-clips-instrumental).]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/generative-ai-spotify-clips</link>
      <guid>https://www.luisquintanilla.me/posts/generative-ai-spotify-clips</guid>
      <pubDate>2023-12-23 18:30</pubDate>
      <category>ai</category>
      <category>video</category>
      <category>chicanobatman</category>
      <category>generativeai</category>
      <category>genai</category>
      <category>spotify</category>
      <category>google</category>
      <category>microsoft</category>
      <category>bing</category>
      <category>copilot</category>
      <category>dalle</category>
      <category>image</category>
      <category>prompt</category>
      <category>projectidea</category>
      <category>music</category>
      <category>podcasts</category>
      <category>llm</category>
    </item>
    <item>
      <title>AI like it's 1999 or 1899</title>
      <description>&lt;![CDATA[
## Introduction

In an earlier note, I talked about the natural progression of technology which goes from [scarcity to abundance](/notes/ai-abundance-scarcity-cycle-repeats-rss-enclosure-use-case) over time and AI is no different. That was in the context of media on the internet and paying per minute with early cell phones. One of the original challenges with AI was context length and token limits. It's becoming less relevant as context sizes continue to increase. However, it's not like you can fit the entire Harry Potter series either. So you still need to cleverly manage your context size especially when using documents and chat history as part of that context. Historically, people have used abbreviations to compress messages either because of cost or medium constraints. That's what this post explores in the context of AI.

## Compressing text like T9

Below is a conversation Microsoft Copilot (formerly Bing Chat) using some information from the article [From 1890: The First Text Messages](https://sundaymagazine.org/2010/08/10/from-1890-the-first-text-messages/):

&gt; **Me** 
&gt; &gt; Summarize this text:
&gt; &gt;   
&gt; &gt; I’m trying something new today. Sometimes in my research I find an interesting old article that I wouldn’t normally post because it’s not from the Sunday Magazine section, or it’s from further than 100 years ago so I’ll never get to it. Instead of letting these go unused, I figure I’ll occasionally post them midweek during what would otherwise be slow weeks. Since this weekend I only have three articles to post, it seems like a good week to try it.
&gt; &gt;   
&gt; &gt; Telegraph operators on opposite sides of the country had some time to get to know each other when they weren’t busy sending other people’s messages. “Metaphorically they shake hands cordially twice a day — when they begin work and when they end it. And when business is dull they hold long conversations, with hundreds of miles — perhaps thousands — separating them, as two friends might do over a dinner table.”
&gt; &gt;   
&gt; &gt; What really caught my eye, though, is that the abbreviations they used seem a lot like the abbreviations used in today’s text messages.
&gt; 
&gt; **Copilot**
&gt; &gt; Here is a possible summary of the text:
&gt; &gt;   
&gt; &gt; The author is trying a new feature of posting old articles from the Sunday Magazine section on midweek. The article is about how telegraph operators communicated with each other using abbreviations that are similar to today’s text messages. The author finds this interesting and wonders if there are other similarities between old and new forms of communication.
&gt; 
&gt; **Me**
&gt; &gt; Now abbreviate it as if using a T9 keyboard for conciseness
&gt; 
&gt; **Copilot**
&gt; &gt; That sounds like a fun challenge. Here is a possible abbreviation of the summary:
&gt; &gt;   
&gt; &gt; try new feat post old artcls midwk. artcl abt telgrph oprtrs chat w/ abbrvtns lk txt msgs. find intrstng n wndr othr smilrts btwn old n new comms.

In this conversation, I first had Copilot summarize the original text. It's not a very long piece of text but it works for this example. You could imagine though how a longer piece of text would benefit even more from this summarization. Copilot does a good job of summarizing the text. However, what if I wanted to summarize it even more? 

In comes T9 (kind of). T9 is short for "Text on 9 keys" and it was predictive text technology that made it easier to compose SMS text messages using only the 9 keys on the phone keypad. Before T9, it was cumbersome and time consuming to compose full sentences so you might abbreviate them. For example "See you later" might be abbreviated as "c u l8r". You can learn more in the [SMS language Wikipedia article](https://en.wikipedia.org/wiki/SMS_language). Not only would you abbreviate because it was tedious to compose messages one letter at a time, but also you were limited in the number of characters you could send in a single SMS. Somewhat related, in the early days of microblogging on Twitter, one of the ways you could compose and view tweets was through SMS. There was the cost aspects as well. You were charged per SMS so you wanted to send as much information in a single SMS as possible. Now SMS is only one example where this took place. Telegrams and telegraphs from yesteryear had similar constraints. Whether it was cost, size limits, or general UX challenges, people looked to abbreviations to draft their messages. 

Now that we're caught up on this history, that's what you're seeing in this response from Copilot.

&gt; try new feat post old artcls midwk. artcl abt telgrph oprtrs chat w/ abbrvtns lk txt msgs. find intrstng n wndr othr smilrts btwn old n new comms.

It is an abbreviated form of the summarized message enabling you to use even less tokens as part of the prompts you submit to AI. 

When looking at "word" counts, they are roughly the following:

| Text | Word Count |
| --- | --- |
| Original | 58 |
| Summarized | 25 |
| T9 | 18 |

We can see that the abbreviated version is ~3x smaller than the original message. 

## Conclusion

Given that models like GPT and others have been trained on the entire internet, it has some knowledge of these types of abbreviations to compose messages. Now, is it scalable and would you want to use this in your production applications, I can't say. Probably not because the abbreviations are lossy. While in general you could read and get the gist of the message using the abbreviations, there are things lost in translation. Now maybe the LLM might do a better job at interpreting the abbreviations. It'd be a fun experiment. 

"n d mntm, il kp usng AI lk its 1999. c u l8r!"

Translation: "In the meantime, I'll keep using AI like it's 1999. See you later!" :slightly_smiling_face:]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/ai-1999-1899</link>
      <guid>https://www.luisquintanilla.me/posts/ai-1999-1899</guid>
      <pubDate>2023-12-15 08:56</pubDate>
      <category>ai</category>
      <category>communication</category>
      <category>sms</category>
      <category>retro</category>
      <category>internet</category>
    </item>
  </channel>
</rss>