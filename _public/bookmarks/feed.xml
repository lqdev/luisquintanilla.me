<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla - Bookmarks</title>
    <link>https://www.lqdev.me/bookmarks</link>
    <description>IndieWeb bookmarks by Luis Quintanilla</description>
    <lastBuildDate>2025-09-15 19:48 -05:00</lastBuildDate>
    <language>en</language>
    <item>
      <title>How people use ChatGPT</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Despite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT’s consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10% of the world’s adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages but even faster growth in non-work-related messages, which have grown from 53% to more than 70% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation topic and find that “Practical Guidance,” “Seeking Information,” and “Writing” are the three most common topics and collectively account for nearly 80% of all conversations. Writing dominates work-related tasks, highlighting chatbots’ unique ability to generate digital outputs compared to traditional search engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://www.nber.org/system/files/working_papers/w34255/w34255.pdf"&gt;PDF&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/how-people-use-chatgpt-national-bureau-economic-research</link>
      <guid>https://www.lqdev.me/bookmarks/how-people-use-chatgpt-national-bureau-economic-research</guid>
      <pubDate>2025-09-15 19:48 -05:00</pubDate>
      <category>ai</category>
      <category>openai</category>
      <category>research</category>
      <category>chatgpt</category>
      <category>economics</category>
    </item>
    <item>
      <title>Real Simple Licensing</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The open content licensing standard for the AI-first Internet&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/real-simple-licensing-open-standard</link>
      <guid>https://www.lqdev.me/bookmarks/real-simple-licensing-open-standard</guid>
      <pubDate>2025-09-12 20:48 -05:00</pubDate>
      <category>rsl</category>
      <category>ai</category>
      <category>licensing</category>
      <category>openstandard</category>
    </item>
    <item>
      <title>Reclaim Control</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;RECLAIM CONTROL over your digital life, devices, services and data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;You have the right to control your data, devices, and services. We coordinate, develop and foster communities of Reclaimers who want digital independence. Together, let's reclaim the right to a technical future of data sovereignty, digital autonomy, responsible governance, and collective care online for all.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/reclaim-control</link>
      <guid>https://www.lqdev.me/bookmarks/reclaim-control</guid>
      <pubDate>2025-08-19 21:41 -05:00</pubDate>
      <category>digitalsovereignty</category>
      <category>reclaimcontrol</category>
    </item>
    <item>
      <title>The Opt Out Project</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;I have built up a treasure trove of knowledge, devices, and services aimed at keeping me and my family away from the evils of data-collecting algorithmic systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;That's why I started this site. I've decided to put information about these tools and alternatives in one spot for others to find and use. Many of these tools are reviewed in the tech press, others are more bespoke but there is information out there if you know where to look. My goal is to bring it all together in one place, to guide you through the process, and help you opt out as well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...there are alternatives out there, better visions of what our lives could be like. There are systems that don't require capturing and storing all our data, mining us exploitatively for profit, and serving us the kinds of misinformation and advertising that has us hating our neighbors.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/out-out-project</link>
      <guid>https://www.lqdev.me/bookmarks/out-out-project</guid>
      <pubDate>2025-08-19 20:51 -05:00</pubDate>
      <category>optout</category>
      <category>alternative</category>
      <category>tech</category>
    </item>
    <item>
      <title>HOPE 16 Conference Videos</title>
      <description>&lt;![CDATA[[bookmark] ]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/hope-16-videos</link>
      <guid>https://www.lqdev.me/bookmarks/hope-16-videos</guid>
      <pubDate>2025-08-18 18:53 -05:00</pubDate>
      <category>hope</category>
      <category>hacker</category>
      <category>conference</category>
    </item>
    <item>
      <title>FediCon 2025 Videos</title>
      <description>&lt;![CDATA[[bookmark] ]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/fedicon-2025-videos</link>
      <guid>https://www.lqdev.me/bookmarks/fedicon-2025-videos</guid>
      <pubDate>2025-08-18 18:53 -05:00</pubDate>
      <category>fediverse</category>
      <category>fedicon2025</category>
      <category>conference</category>
    </item>
    <item>
      <title>Awesome Self-Hosted</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;This is a list of Free Software network services and web applications which can be hosted on your own server(s).&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/awesome-self-hosted</link>
      <guid>https://www.lqdev.me/bookmarks/awesome-self-hosted</guid>
      <pubDate>2025-08-11 23:24 -05:00</pubDate>
      <category>selfhosting</category>
      <category>opensource</category>
      <category>FOSS</category>
    </item>
    <item>
      <title>URL Town</title>
      <description>&lt;![CDATA[[bookmark] &lt;p&gt;Love to see projects like this.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;url.town doesn’t have any overly lofty ambitions; we’re just building our own directory of really nice websites. We’re not trying to fully recreate the original Yahoo! or DMOZ directories. We’re not aiming for some astronomical number of links. This is just one space on the web, tied to a community that loves to share neat things with one another. Quality matters much more than quantity. There’s no need to share everything just for the shake of sharing it; it’s much better to share things that are useful or interesting.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/url-town</link>
      <guid>https://www.lqdev.me/bookmarks/url-town</guid>
      <pubDate>2025-08-10 23:26 -05:00</pubDate>
      <category>indieweb</category>
      <category>websites</category>
      <category>directory</category>
    </item>
    <item>
      <title>Claude Code Emacs Integration</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Claude Code IDE for Emacs provides native integration with Claude Code CLI through the Model Context Protocol (MCP). Unlike simple terminal wrappers, this package creates a bidirectional bridge between Claude and Emacs, enabling Claude to understand and leverage Emacs’ powerful features—from LSP and project management to custom Elisp functions. This transforms Claude into a true Emacs-aware AI assistant that works within your existing workflow and can interact with your entire Emacs ecosystem.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/claude-code-emacs-integration</link>
      <guid>https://www.lqdev.me/bookmarks/claude-code-emacs-integration</guid>
      <pubDate>2025-08-10 20:17 -05:00</pubDate>
      <category>emacs</category>
      <category>anthropic</category>
      <category>ai</category>
      <category>claudecode</category>
    </item>
    <item>
      <title>Resources List for the Personal Web</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;This list is to help guide and help those who are in every stage of their web-building journey on the personal web. This list is not meant to overwhelm you, but rather give you options and find tools, graphics, utilities, codes, and everything in between to help get you creating more on the independent web.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/resource-list-personal-web</link>
      <guid>https://www.lqdev.me/bookmarks/resource-list-personal-web</guid>
      <pubDate>2025-05-30 13:32 -05:00</pubDate>
      <category>32bitcafe</category>
      <category>personalweb</category>
      <category>indieweb</category>
      <category>openweb</category>
      <category>community</category>
      <category>retro</category>
    </item>
    <item>
      <title>Pocket shutting down</title>
      <description>&lt;![CDATA[[bookmark] &lt;p&gt;I haven't used Pocket in a long time but sad to hear it's shutting down. It's great they're offering the option of letting you export your data and platforms like &lt;a href="https://www.manton.org/2025/05/22/with-pocket-shutting-down-ive.html"&gt;Micro.blog&lt;/a&gt; are making it easy to host that content on your own site (assuming you're using Micro.blog).&lt;/p&gt;
&lt;p&gt;For bookmarking solutions, I've been using my website as well as messages-to-self on Element. What's on my website is the content that I really want to make sure I archive, whereas the messages to self I treat more as a read-it-later solution. Eventually, some of those make it to my website. I'm working on my mobile publishing flow to simplify my bookmarking process but overall, I'm happy with my current system.&lt;/p&gt;
&lt;p&gt;This is also another great reminder why owning your content is important.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/pocket-shutting-down</link>
      <guid>https://www.lqdev.me/bookmarks/pocket-shutting-down</guid>
      <pubDate>2025-05-22 20:21 -05:00</pubDate>
      <category>pocket</category>
      <category>mozilla</category>
      <category>bookmark</category>
    </item>
    <item>
      <title>Bukmark Club</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;The BUKMARK.CLUB is a collection of websites from across the Internet.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/bukmark-club</link>
      <guid>https://www.lqdev.me/bookmarks/bukmark-club</guid>
      <pubDate>2025-05-09 21:09 -05:00</pubDate>
      <category>bookmark</category>
      <category>internet</category>
      <category>web</category>
      <category>community</category>
    </item>
    <item>
      <title>Introducing AutoRound</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;As large language models (LLMs) and vision-language models (VLMs) continue to grow in size and complexity, deploying them efficiently becomes increasingly challenging. Quantization offers a solution by reducing model size and inference latency. Intel's AutoRound emerges as a cutting-edge quantization tool that balances accuracy, efficiency, and compatibility.&lt;br /&gt;
&lt;br&gt;
AutoRound is a weight-only post-training quantization (PTQ) method developed by Intel. It uses signed gradient descent to jointly optimize weight rounding and clipping ranges, enabling accurate low-bit quantization (e.g., INT2 - INT8) with minimal accuracy loss in most scenarios. For example, at INT2, it outperforms popular baselines by up to 2.1x higher in relative accuracy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2309.05516"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/intel/auto-round"&gt;Repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/intel-autoround</link>
      <guid>https://www.lqdev.me/bookmarks/intel-autoround</guid>
      <pubDate>2025-05-09 20:45 -05:00</pubDate>
      <category>quantization</category>
      <category>intel</category>
      <category>ai</category>
    </item>
    <item>
      <title>Parakeet TDT 0.6B V2 (En)</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;parakeet-tdt-0.6b-v2 is a 600-million-parameter automatic speech recognition (ASR) model designed for high-quality English transcription, featuring support for punctuation, capitalization, and accurate timestamp prediction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This XL variant of the FastConformer [1] architecture integrates the TDT [2] decoder and is trained with full attention, enabling efficient transcription of audio segments up to 24 minutes in a single pass.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/nvidia-parakeet-v2</link>
      <guid>https://www.lqdev.me/bookmarks/nvidia-parakeet-v2</guid>
      <pubDate>2025-05-09 20:43 -05:00</pubDate>
      <category>nvidia</category>
      <category>ai</category>
      <category>transcription</category>
    </item>
    <item>
      <title>Introducing Locate 3D</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Locate 3D transforms how AI understands physical space, delivering &lt;strong&gt;state-of-the-art object localization in complex 3D environments&lt;/strong&gt;.&lt;br /&gt;
&lt;br&gt;
Operating directly on standard sensor data, Locate 3D brings spatial intelligence to robotics and augmented reality applications in real-world settings.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://ai.meta.com/research/publications/locate-3d-real-world-object-localization-via-self-supervised-learning-in-3d/"&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We present Locate 3D, a model for localizing objects in 3D scenes from referring expressions like “the small coffee table between the sofa and the lamp.” Locate 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities. Notably, Locate 3D operates directly on sensor observation streams (posed RGB-D frames), enabling real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA, a novel self-supervised learning (SSL) algorithm applicable to sensor point clouds. It takes as input a 3D pointcloud featurized using 2D foundation models (CLIP, DINO). Subsequently, masked prediction in latent space is employed as a pretext task to aid the self-supervised learning of contextualized pointcloud features. Once trained, the 3D-JEPA encoder is finetuned alongside a language-conditioned decoder to jointly predict 3D masks and bounding boxes. Additionally, we introduce Locate 3D Dataset, a new dataset for 3D referential grounding, spanning multiple capture setups with over 130K annotations. This enables a systematic study of generalization capabilities as well as a stronger model.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/meta-locate3d</link>
      <guid>https://www.lqdev.me/bookmarks/meta-locate3d</guid>
      <pubDate>2025-05-09 20:33 -05:00</pubDate>
      <category>meta</category>
      <category>ai</category>
      <category>3d</category>
      <category>research</category>
    </item>
    <item>
      <title>ZeroSearch - Incentivize the Search Capability of LLMs without Searching</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) &lt;strong&gt;Uncontrolled Document Quality&lt;/strong&gt;: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) &lt;strong&gt;Prohibitively High API Costs&lt;/strong&gt;: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce &lt;strong&gt;ZeroSearch&lt;/strong&gt;, a reinforcement learning framework that &lt;strong&gt;enhances the search capabilities of LLMs without interacting with real search engines&lt;/strong&gt;. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model’s reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that &lt;strong&gt;ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module&lt;/strong&gt;. Remarkably, &lt;strong&gt;a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it&lt;/strong&gt;. Furthermore, it generalizes well across both base and instruction-tuned models of varying sizes and is compatible with a wide range of RL algorithms.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/zero-search-alibaba</link>
      <guid>https://www.lqdev.me/bookmarks/zero-search-alibaba</guid>
      <pubDate>2025-05-09 20:31 -05:00</pubDate>
      <category>ai</category>
      <category>search</category>
      <category>llm</category>
      <category>research</category>
    </item>
    <item>
      <title>A Survey of AI Agent Protocols</title>
      <description>&lt;![CDATA[[bookmark] &lt;p&gt;It'll be interesting to see once many of the exiting protocols begin to converge towards a standard. There's a ton of existing well-adopted protocol standards out there that when stitched together could create a compelling and native experience. I think eventually we'll get to a place where agents are native parts of the OSI stack and the world wide web.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: &lt;strong&gt;there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence.&lt;/strong&gt; In this paper, we provide the first comprehensive analysis of existing agent protocols, proposing a systematic two-dimensional classification that differentiates context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore the future landscape of agent protocols by identifying critical research directions and characteristics necessary for next-generation protocols. These characteristics include adaptability, privacy preservation, and group-based interaction, as well as trends toward layered architectures and collective intelligence infrastructures. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/survey-ai-agent-protocols</link>
      <guid>https://www.lqdev.me/bookmarks/survey-ai-agent-protocols</guid>
      <pubDate>2025-05-09 20:23 -05:00</pubDate>
      <category>ai</category>
      <category>agents</category>
      <category>protocols</category>
      <category>research</category>
    </item>
    <item>
      <title>TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. &lt;strong&gt;In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications.&lt;/strong&gt; We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at &lt;a href="https://github.com/mengyuest/TeLoGraF"&gt;this https URL&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/telograf</link>
      <guid>https://www.lqdev.me/bookmarks/telograf</guid>
      <pubDate>2025-05-05 20:03 -05:00</pubDate>
      <category>ai</category>
      <category>graphs</category>
      <category>neuralnetworks</category>
      <category>research</category>
    </item>
    <item>
      <title>CORG: Generating Answers from Complex, Interrelated Contexts</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, &lt;strong&gt;we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator.&lt;/strong&gt; Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/corg-context-organizer-framework</link>
      <guid>https://www.lqdev.me/bookmarks/corg-context-organizer-framework</guid>
      <pubDate>2025-05-05 19:53 -05:00</pubDate>
      <category>rag</category>
      <category>ai</category>
      <category>knowledge</category>
      <category>research</category>
    </item>
    <item>
      <title>Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks</title>
      <description>&lt;![CDATA[[bookmark] &lt;blockquote class="blockquote"&gt;
&lt;p&gt;Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that &lt;strong&gt;automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.lqdev.me/bookmarks/self-generate-incontext-examples-llm-agents-improve-sequential-decision-making</link>
      <guid>https://www.lqdev.me/bookmarks/self-generate-incontext-examples-llm-agents-improve-sequential-decision-making</guid>
      <pubDate>2025-05-05 19:50 -05:00</pubDate>
      <category>ai</category>
      <category>agents</category>
      <category>planning</category>
      <category>research</category>
    </item>
  </channel>
</rss>