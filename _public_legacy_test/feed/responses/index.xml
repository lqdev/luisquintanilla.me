<rss version="2.0">
  <channel>
    <title>Luis Quintanilla Response Feed</title>
    <link>https://www.luisquintanilla.me</link>
    <description>Response Feed</description>
    <lastPubDate>2025-06-30 20:21</lastPubDate>
    <language>en</language>
    <item>
      <title>Tumblr‚Äôs move to WordPress and fediverse integration is ‚Äòon hold‚Äô</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tumblr-wordpress-fediverse-integration-pause?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tumblr-wordpress-fediverse-integration-pause&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;That's unfortunate. Having Tumblr as your frontend for posting and curating feeds, with WordPress as the backend and integrated with decentralized networks like the Fediverse for distribution would've been amazing. It sounds like Tumblr is still strategic to Automattic and as more people move away from the centralized social platforms and towards their own corners of the internet, having these differente building blocks in place will be even more important.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tumblr-wordpress-fediverse-integration-pause?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tumblr-wordpress-fediverse-integration-pause</guid>
      <pubDate>2025-06-30 20:21</pubDate>
      <category>#tumblr</category>
      <category>#wordpress</category>
      <category>#fediverse</category>
      <category>#automattic</category>
    </item>
    <item>
      <title>Do not privatize federal land</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/do-not-privatize-federal-land?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/do-not-privatize-federal-land&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I‚Äôve long advocated selling off some federal land...Most of this ‚Äúpublic land‚Äù is never used by the public. Selling some of it would actually make it more accessible and useful to real people.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How about no. If privatization means, John and Jane Doe can purchase some land for their homestead is one thing. We've tried that before by the way - &lt;a href="https://en.wikipedia.org/wiki/Homestead_Acts"&gt;Homestead Act&lt;/a&gt;. In reality, it means some private equity firm or hedge fund buying the land, building expensive and shoddy cookit-cutter condos, and &lt;a href="https://www.nar.realtor/magazine/real-estate-news/commercial/build-to-rent-homes"&gt;selling you a subscription for a roof over your head&lt;/a&gt;. I say condos because it's hot and there's not a lot of water in Vegas, so keeping those brand new AI data centers cool might be a problem.&lt;/p&gt;
&lt;p&gt;Also, if you haven't seen that area that Alex highlights in his post, it seems like a deserted landscape. And yes, the terrain can be rough, but actually, it's a beautiful landscape that is best left undisturbed.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/do-not-privatize-federal-land?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/do-not-privatize-federal-land</guid>
      <pubDate>2025-06-29 17:26</pubDate>
      <category>#bureaulandmanagement</category>
      <category>#uspol</category>
      <category>#land</category>
    </item>
    <item>
      <title>Self-Adapting Language Models (SEAL)</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/self-adapting-language-models-seal?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/self-adapting-language-models-seal&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. &lt;strong&gt;We introduce Self-Adapting LLMs (SEAL) ü¶≠, a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives.&lt;/strong&gt; Given a new input, the model produces a self-edit ‚Äî a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop, using the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's generation to parameterize and control its own adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation in response to new data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We demonstrate SEAL in two domains: (1) &lt;strong&gt;Knowledge Incorporation&lt;/strong&gt;, where the model integrates new factual information by generating logical implications as synthetic data, and (2) &lt;strong&gt;Few-Shot Learning&lt;/strong&gt;, where the model autonomously selects data augmentations and training hyperparameters to adapt to new abstract reasoning tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Continual-Intelligence"&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2506.10943"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/self-adapting-language-models-seal?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/self-adapting-language-models-seal</guid>
      <pubDate>2025-06-16 20:33</pubDate>
      <category>#agent</category>
      <category>#ai</category>
      <category>#llm</category>
      <category>#mit</category>
      <category>#research</category>
    </item>
    <item>
      <title>How we built our multi-agent research system - Anthropic</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/anthropic-how-we-built-multi-agent-system?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/anthropic-how-we-built-multi-agent-system&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Great article.&lt;/p&gt;
&lt;p&gt;TLDR: These are largely engineering problems.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The journey of this multi-agent system from prototype to production taught us critical lessons about system architecture, tool design, and prompt engineering. A multi-agent system consists of multiple agents (&lt;strong&gt;LLMs autonomously using tools in a loop&lt;/strong&gt;) working together. Our Research feature involves an agent that plans a research process based on user queries, and then uses tools to create parallel agents that search for information simultaneously. Systems with multiple agents introduce new challenges in agent coordination, evaluation, and reliability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Benefits of a multi-agent system&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The essence of search is compression: distilling insights from a vast corpus. &lt;strong&gt;Subagents facilitate compression by operating in parallel&lt;/strong&gt; with their own context windows, exploring different aspects of the question simultaneously before condensing the most important tokens for the lead research agent. Each subagent also provides &lt;strong&gt;separation of concerns&lt;/strong&gt;‚Äîdistinct tools, prompts, and exploration trajectories‚Äîwhich reduces path dependency and enables thorough, independent investigations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Once intelligence reaches a threshold, multi-agent systems become a vital way to scale performance. For instance, although individual humans have become more intelligent in the last 100,000 years, human societies have become exponentially more capable in the information age because of our collective intelligence and ability to coordinate. Even generally-intelligent agents face limits when operating as individuals; &lt;strong&gt;groups of agents can accomplish far more&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our internal evaluations show that multi-agent research systems excel especially for breadth-first queries that involve pursuing multiple independent directions simultaneously.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Multi-agent systems work mainly because they help spend enough tokens to solve the problem...Multi-agent architectures effectively scale token usage for tasks that exceed the limits of single agents.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôve found that multi-agent systems excel at valuable tasks that involve heavy parallelization, information that exceeds single context windows, and interfacing with numerous complex tools.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Architecture overview&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our Research system uses a multi-agent architecture with an orchestrator-worker pattern, where a lead agent coordinates the process while delegating to specialized subagents that operate in parallel.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Traditional approaches using Retrieval Augmented Generation (RAG) use static retrieval. That is, they fetch some set of chunks that are most similar to an input query and use these chunks to generate a response. In contrast, our architecture uses a &lt;strong&gt;multi-step search that dynamically finds relevant information, adapts to new findings, and analyzes results to formulate high-quality answers&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Prompt engineering and evaluations for research agents&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Think like your agents.&lt;/strong&gt; To iterate on prompts, you must understand their effects. To help us do this, we built simulations using our Console with the exact prompts and tools from our system, then watched agents work step-by-step.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Teach the orchestrator how to delegate.&lt;/strong&gt; In our system, the lead agent decomposes queries into subtasks and describes them to subagents. Each subagent needs an &lt;strong&gt;objective&lt;/strong&gt;, an &lt;strong&gt;output format&lt;/strong&gt;, &lt;strong&gt;guidance on the tools and sources to use&lt;/strong&gt;, and &lt;strong&gt;clear task boundaries&lt;/strong&gt;. Without detailed task descriptions, agents duplicate work, leave gaps, or fail to find necessary information.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Scale effort to query complexity.&lt;/strong&gt; Agents struggle to judge appropriate effort for different tasks, so we embedded scaling rules in the prompts...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Tool design and selection are critical.&lt;/strong&gt; Agent-tool interfaces are as critical as human-computer interfaces. Using the right tool is efficient‚Äîoften, it‚Äôs strictly necessary...Bad tool descriptions can send agents down completely wrong paths, so each tool needs a distinct purpose and a clear description.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Let agents improve themselves.&lt;/strong&gt;...When given a prompt and a failure mode, they are able to diagnose why the agent is failing and suggest improvements.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Start wide, then narrow down.&lt;/strong&gt; Search strategy should mirror expert human research: explore the landscape before drilling into specifics.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Guide the thinking process.&lt;/strong&gt;...Our testing showed that extended thinking improved instruction-following, reasoning, and efficiency. Subagents also plan, then use interleaved thinking after tool results to evaluate quality, identify gaps, and refine their next query. This makes subagents more effective in adapting to any task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Parallel tool calling transforms speed and performance.&lt;/strong&gt;...For speed, we introduced two kinds of parallelization: (1) the lead agent spins up 3-5 subagents in parallel rather than serially; (2) the subagents use 3+ tools in parallel.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our prompting strategy focuses on instilling &lt;strong&gt;good heuristics rather than rigid rules&lt;/strong&gt;. We studied how skilled humans approach research tasks and encoded these strategies in our prompts‚Äîstrategies like &lt;strong&gt;decomposing difficult questions into smaller tasks&lt;/strong&gt;, &lt;strong&gt;carefully evaluating the quality of sources&lt;/strong&gt;, &lt;strong&gt;adjusting search approaches based on new information&lt;/strong&gt;, and &lt;strong&gt;recognizing when to focus on depth (investigating one topic in detail) vs. breadth (exploring many topics in parallel)&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Effective evaluation of agents&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...evaluating multi-agent systems presents unique challenges...Because we don‚Äôt always know what the right steps are, we usually can't just check if agents followed the ‚Äúcorrect‚Äù steps we prescribed in advance. Instead, we need flexible evaluation methods that judge whether agents achieved the right outcomes while also following a reasonable process.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Start evaluating immediately with small samples.&lt;/strong&gt;...it‚Äôs best to start with small-scale testing right away with a few examples, rather than delaying until you can build more thorough evals.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;LLM-as-judge evaluation scales when done well.&lt;/strong&gt;...We used an LLM judge that evaluated each output against criteria in a rubric: &lt;strong&gt;factual accuracy&lt;/strong&gt; (do claims match sources?), &lt;strong&gt;citation accuracy&lt;/strong&gt; (do the cited sources match the claims?), &lt;strong&gt;completeness&lt;/strong&gt; (are all requested aspects covered?), &lt;strong&gt;source quality&lt;/strong&gt; (did it use primary sources over lower-quality secondary sources?), and &lt;strong&gt;tool efficiency&lt;/strong&gt; (did it use the right tools a reasonable number of times?)...[we] found that a single LLM call with a single prompt outputting scores from 0.0-1.0 and a pass-fail grade was the most consistent and aligned with human judgements.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Human evaluation catches what automation misses.&lt;/strong&gt;...Adding source quality heuristics to our prompts helped resolve this issue. Even in a world of automated evaluations, manual testing remains essential.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;the best prompts for these agents are not just strict instructions, but &lt;strong&gt;frameworks for collaboration&lt;/strong&gt; that define the &lt;strong&gt;division of labor&lt;/strong&gt;, &lt;strong&gt;problem-solving approaches&lt;/strong&gt;, and &lt;strong&gt;effort budgets&lt;/strong&gt;. Getting this right relies on &lt;strong&gt;careful prompting and tool design&lt;/strong&gt;, &lt;strong&gt;solid heuristics&lt;/strong&gt;, &lt;strong&gt;observability&lt;/strong&gt;, and &lt;strong&gt;tight feedback loops&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Production reliability and engineering challenges&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Agents are stateful and errors compound.&lt;/strong&gt;...Agents can run for long periods of time, maintaining state across many tool calls. This means we need to &lt;strong&gt;durably execute code and handle errors along the way&lt;/strong&gt;. Without effective mitigations, minor system failures can be catastrophic for agents. When errors occur, we can't just restart from the beginning...We combine the adaptability of AI agents built on Claude with deterministic safeguards like retry logic and regular checkpoints.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Debugging benefits from new approaches&lt;/strong&gt;...Adding full production tracing let us diagnose why agents failed and fix issues systematically. Beyond standard observability, we monitor agent decision patterns and interaction structures‚Äîall without monitoring the contents of individual conversations, to maintain user privacy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Deployment needs careful coordination&lt;/strong&gt;...we use rainbow deployments to avoid disrupting running agents, by gradually shifting traffic from old to new versions while keeping both running simultaneously.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Synchronous execution creates bottlenecks.&lt;/strong&gt; Currently, our lead agents execute subagents synchronously, waiting for each set of subagents to complete before proceeding. This simplifies coordination, but creates bottlenecks in the information flow between agents. Asynchronous execution would enable additional parallelism: agents working concurrently and creating new subagents when needed. But this asynchronicity adds challenges in result coordination, state consistency, and error propagation across the subagents. As models can handle longer and more complex research tasks, we expect the performance gains will justify the complexity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;For all the reasons described in this post, the gap between prototype and production is often wider than anticipated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Multi-agent research systems can operate reliably at scale with &lt;strong&gt;careful engineering&lt;/strong&gt;, &lt;strong&gt;comprehensive testing&lt;/strong&gt;, &lt;strong&gt;detail-oriented prompt and tool design&lt;/strong&gt;, &lt;strong&gt;robust operational practices&lt;/strong&gt;, and &lt;strong&gt;tight collaboration between research, product, and engineering teams&lt;/strong&gt; who have a strong understanding of current agent capabilities.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/anthropic-how-we-built-multi-agent-system?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/anthropic-how-we-built-multi-agent-system</guid>
      <pubDate>2025-06-16 17:39</pubDate>
      <category>#agent</category>
      <category>#ai</category>
      <category>#anthropic</category>
      <category>#engineering</category>
    </item>
    <item>
      <title>MLS over ActivityPub Draft</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mls-over-activitypub-draft?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mls-over-activitypub-draft&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;a href="https://messaginglayersecurity.rocks/"&gt;Messaging Layer Security (MLS)&lt;/a&gt; is an IETF standard for end-to-end encrypted (E2EE) messaging. It lets people on laptops and phones communicate with each other in a secure way that no one in between can see.&lt;br /&gt;
&lt;br&gt;
MLS is designed to use pluggable lower-level protocols. This specification defines an envelope format for distributing MLS messages through the network, and an Activity Streams 2.0 profile for the packets of application data stored inside the messages.&lt;br /&gt;
&lt;br&gt;
This specification is ready for review from both ActivityPub developers and security analysts. It‚Äôs time to start making proof-of-concept implementations and testing interoperability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Source: https://socialwebfoundation.org/2025/06/13/mls-over-activitypub-draft/&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mls-over-activitypub-draft?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mls-over-activitypub-draft</guid>
      <pubDate>2025-06-14 10:47</pubDate>
      <category>#protocol</category>
      <category>#socialweb</category>
      <category>#activitypub</category>
      <category>#w3c</category>
    </item>
    <item>
      <title>Faircamp - A static site generator for audio producers</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/faircamp-static-site-audio-producers?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/faircamp-static-site-audio-producers&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This is such a cool project. Looks like it's super simple to get started with and it's packed with a ton of features so creators get to stay in their flow.&lt;/p&gt;
&lt;p&gt;I also like how creators are showcased.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/faircamp-static-site-audio-producers?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/faircamp-static-site-audio-producers</guid>
      <pubDate>2025-06-05 20:05</pubDate>
      <category>#faircamp</category>
      <category>#music</category>
      <category>#socialweb</category>
      <category>#indieweb</category>
      <category>#staticsite</category>
      <category>#technology</category>
      <category>#tools</category>
      <category>#openweb</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>State-Of-The-Art Prompting For AI Agents</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/state-of-the-art-prompting-ai-agents-y-combinator?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/state-of-the-art-prompting-ai-agents-y-combinator&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=DL82mGde6wo" title="YouTube Thumbnail for State-of-the-art prompting for ai agents"&gt;&lt;img src="http://img.youtube.com/vi/DL82mGde6wo/0.jpg" class="img-fluid" alt="YouTube Thumbnail for State-of-the-art prompting for ai agents" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/state-of-the-art-prompting-ai-agents-y-combinator?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/state-of-the-art-prompting-ai-agents-y-combinator</guid>
      <pubDate>2025-06-03 20:23</pubDate>
      <category>#ai</category>
      <category>#agents</category>
      <category>#prompting</category>
    </item>
    <item>
      <title>Introducing ElevenLabs Conversational AI 2.0</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/elevenlabs-conversational-ai-2-0?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/elevenlabs-conversational-ai-2-0&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Conversational AI 2.0 launches with advanced features and enterprise readiness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Natural turn-taking to understand the flow of conversation.&lt;/li&gt;
&lt;li&gt;Multilingual communication with integrated language detection&lt;/li&gt;
&lt;li&gt;Integrated RAG: knowledgeable agents, minimum latency, maximum privacy&lt;/li&gt;
&lt;li&gt;Multimodality&lt;/li&gt;
&lt;li&gt;Batch calls&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/elevenlabs-conversational-ai-2-0?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/elevenlabs-conversational-ai-2-0</guid>
      <pubDate>2025-06-03 20:19</pubDate>
      <category>#ai</category>
      <category>#speech</category>
      <category>#elevenlabs</category>
    </item>
    <item>
      <title>The Darwin G√∂del Machine - AI that improves itself by rewriting its own code</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/sakana-ai-gwm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/sakana-ai-gwm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A longstanding goal of AI research has been the creation of AI that can learn indefinitely. One tantalizing path toward that goal is an AI that improves itself by rewriting its own code, including any code responsible for learning. That idea, known as a G√∂del Machine, proposed by J√ºrgen Schmidhuber decades ago, is a hypothetical self-improving AI. It optimally solves problems by recursively rewriting its own code when it can mathematically prove a better strategy, making it a key concept in meta-learning or ‚Äúlearning to learn.‚Äù&lt;br /&gt;
&lt;br&gt;
While the theoretical G√∂del Machine promised provably beneficial self-modifications, its realization relied on an impractical assumption: that the AI could mathematically prove that a proposed change in its own code would yield a net improvement before adopting it. We, in collaboration with Jeff Clune‚Äôs lab at UBC, propose something more feasible: a system that harnesses the principles of open-ended algorithms like Darwinian evolution to search for improvements that empirically improve performance.&lt;/p&gt;
&lt;p&gt;We call the result the Darwin G√∂del Machine (full technical report). DGMs leverage foundation models to propose code improvements, and use recent innovations in open-ended algorithms to search for a growing library of diverse, high-quality AI agents. Our experiments show that DGMs improve themselves the more compute they are provided. In line with the clear trend that AI systems that rely on learning ultimately outperform those designed by hand, there is a potential that DGMs could soon outperform hand-designed AI systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2505.22954"&gt;Technical Report&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/sakana-ai-gwm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/sakana-ai-gwm</guid>
      <pubDate>2025-06-03 20:16</pubDate>
      <category>#ai</category>
      <category>#agent</category>
      <category>#sakana</category>
    </item>
    <item>
      <title>Resources List for the Personal Web</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/resource-list-personal-web?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/resource-list-personal-web&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This list is to help guide and help those who are in every stage of their web-building journey on the personal web. This list is not meant to overwhelm you, but rather give you options and find tools, graphics, utilities, codes, and everything in between to help get you creating more on the independent web.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/resource-list-personal-web?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/resource-list-personal-web</guid>
      <pubDate>2025-05-30 13:32</pubDate>
      <category>#32bitcafe</category>
      <category>#personalweb</category>
      <category>#indieweb</category>
      <category>#openweb</category>
      <category>#community</category>
      <category>#retro</category>
    </item>
    <item>
      <title>The stage has been set for the growth of small social media</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/stage-set-growth-small-social-media-cheapskate?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/stage-set-growth-small-social-media-cheapskate&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Last month during Meta's antitrust trial, Mark Zuckerberg admitted that his company's focus has changed radically from where it began...[his] testimony implies that Facebook (Meta) may no longer be as much of an option if we actually want to talk to each other on line. But, neither are many of the other large social media sites.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...if companies allowed us to discover great new small blogs and small social media sites, many of us would decide to spend more time there. Then, companies would be less able to advertise to us on their own sites.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...we have been told by the mainstream media for well over a decade that small social media is dead. They say the same about small blogs, yet small blogs are actually more plentiful than ever, with over &lt;a href="https://firstsiteguide.com/blogging-stat"&gt;600 million blogs&lt;/a&gt; on the Internet and about 7 million new blog articles published each day.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;If you decide to create your own forum, do yourself a favor. The way to protect yourself and your users is by &lt;strong&gt;running your social media site at a domain that you own and on a server that you control&lt;/strong&gt;, and don't agree to hand over any of your rights to anyone for any reason.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I believe large corporate-run social media sites are vulnerable in a way they have not been for twenty years. Their users have reached a state of extreme dissatisfaction with their &amp;quot;enshittified&amp;quot; platforms, and they are looking for something, anything, better.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;But each of you who are reading these words can help end this situation by creating your own small social media site.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;All you need to start your own small site is a domain name, a server that you can buy, rent, or set up on an old computer, perhaps social media software (see above), patience, a willingness to work, and ideas for ways of notifying potential users of the existence of your site.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Run your new site as you and your users see fit, and make it far superior to Facebook, Twitter (X), Reddit, or any of the other major sites whose owners think you have no other options.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/stage-set-growth-small-social-media-cheapskate?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/stage-set-growth-small-social-media-cheapskate</guid>
      <pubDate>2025-05-26 18:14</pubDate>
      <category>#social</category>
      <category>#openweb</category>
      <category>#personalweb</category>
    </item>
    <item>
      <title>RSS is (not) dead (yet)</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/rss-not-dead-yet-mcnamee?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/rss-not-dead-yet-mcnamee&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Illustrating the history of RSS. Created in collaboration with &lt;a href="https://audmcname.com/alliaservice.com"&gt;A. Service&lt;/a&gt;, and debuted at VanCAF 2023&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://audmcname.com/wp-content/uploads/rss/rss0.jpg" class="img-fluid" alt="Cover of Rss is not dead yet comic" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;audmcname.com&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/rss-not-dead-yet-mcnamee?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/rss-not-dead-yet-mcnamee</guid>
      <pubDate>2025-05-26 18:11</pubDate>
      <category>#rss</category>
      <category>#comic</category>
      <category>#art</category>
    </item>
    <item>
      <title>Pocket shutting down</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/pocket-shutting-down?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/pocket-shutting-down&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I haven't used Pocket in a long time but sad to hear it's shutting down. It's great they're offering the option of letting you export your data and platforms like &lt;a href="https://www.manton.org/2025/05/22/with-pocket-shutting-down-ive.html"&gt;Micro.blog&lt;/a&gt; are making it easy to host that content on your own site (assuming you're using Micro.blog).&lt;/p&gt;
&lt;p&gt;For bookmarking solutions, I've been using my website as well as messages-to-self on Element. What's on my website is the content that I really want to make sure I archive, whereas the messages to self I treat more as a read-it-later solution. Eventually, some of those make it to my website. I'm working on my mobile publishing flow to simplify my bookmarking process but overall, I'm happy with my current system.&lt;/p&gt;
&lt;p&gt;This is also another great reminder why owning your content is important.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/pocket-shutting-down?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/pocket-shutting-down</guid>
      <pubDate>2025-05-22 20:21</pubDate>
      <category>#pocket</category>
      <category>#mozilla</category>
      <category>#bookmark</category>
    </item>
    <item>
      <title>Agent Network Protocol - The HTTP of the Agentic Web Era</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/agent-network-protocol?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/agent-network-protocol&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;a href="https://agent-network-protocol.com/blogs/posts/mcp-anp-comparison.html"&gt;Comparison of MCP and ANP: What Kind of Communication Protocol Do Agents Need?&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;MCP and ANP differ significantly in protocol architecture, identity authentication, and information organization.&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MCP is a typical Client-Server (CS) architecture, while &lt;strong&gt;ANP is a typical Peer-to-Peer (P2P) architecture&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;MCP's identity authentication is based on the OAuth standard, facilitating client access to current internet resources. &lt;strong&gt;ANP's identity authentication is based on the W3C DID standard&lt;/strong&gt;, focusing on cross-platform interoperability among agents, enabling seamless connectivity.&lt;/li&gt;
&lt;li&gt;MCP organizes information using JSON-RPC technology, essentially API calls. &lt;strong&gt;ANP uses semantic web Linked-Data technology&lt;/strong&gt; to build a data network that is easily accessible and understandable by AI.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;ANP is agent-centric, where each agent has equal status, forming a decentralized agent collaboration network.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/agent-network-protocol?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/agent-network-protocol</guid>
      <pubDate>2025-05-17 10:56</pubDate>
      <category>#agent</category>
      <category>#ai</category>
      <category>#protocol</category>
    </item>
    <item>
      <title>Chicago Dust Storm</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/chicago-dust-storm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/chicago-dust-storm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Never change Chicago.&lt;/p&gt;
&lt;p&gt;What felt like the first week of good weather since forever ends with a dust storm.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/chicago-dust-storm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/chicago-dust-storm</guid>
      <pubDate>2025-05-16 19:49</pubDate>
      <category>#chicago</category>
      <category>#weather</category>
      <category>#duststorm</category>
    </item>
    <item>
      <title>Claude Artifacts</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/claude-artifacts-reece?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/claude-artifacts-reece&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;üíØüíØüíØ. Artifacts are great. In ChatGPT when I find myself trying to format outputs as markdown, the UI gets confused and some of the content is rendered in markdown while the rest gets rendered as part of the UI which makes it unusable and hard to copy. The best way I've found to get around it is to ask it to format in org-mode or asciidoc. Claude Artifacts make this a non-issue. I also love that in addition to copying and downloading the artifact, you can also publish it as a URL.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/claude-artifacts-reece?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/claude-artifacts-reece</guid>
      <pubDate>2025-05-11 08:42</pubDate>
      <category>#claude</category>
      <category>#chatgpt</category>
      <category>#plaintext</category>
      <category>#ai</category>
      <category>#org</category>
      <category>#emacs</category>
      <category>#asciidoc</category>
      <category>#markdown</category>
    </item>
    <item>
      <title>Announcing Distributed.Press Social Inbox 1.0</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/announcing-distributed-press-social-inbox?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/announcing-distributed-press-social-inbox&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I hadn't heard of &lt;a href="https://distributed.press/"&gt;Distributed Press&lt;/a&gt; and this is an older post (from 2023), but the idea is interesting.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Hypha and Sutty are thrilled to announce the release of the Social Inbox, a new feature of Distributed.Press that integrates a website‚Äôs comment section with federated social media platforms like Mastodon. With the Social Inbox enabled, websites obtain their own account on the Fediverse, allowing it to automatically send out new posts to followers at the time of publication. When other users reply to posts, you can approve them to be published to the site as comments. The Social Inbox allows readers to directly engage with your posts where they already are, and gives publishers the ability to incorporate public dialogue into their websites.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/announcing-distributed-press-social-inbox?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/announcing-distributed-press-social-inbox</guid>
      <pubDate>2025-05-10 20:47</pubDate>
      <category>#decentralization</category>
      <category>#indieweb</category>
      <category>#social</category>
      <category>#fediverse</category>
      <category>#dweb</category>
    </item>
    <item>
      <title>Discourse and the Fediverse</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/discourse-fediverse?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/discourse-fediverse&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Two years ago, we started working on a plugin that brings Discourse and the Fediverse closer together. Discourse communities are online spaces that facilitate open collaboration and communication. The Fediverse offers ways to expand the reach of Discourse communities and help them build bridges with people active in other spaces, all while keeping the conversation civil, meaningful and focused. This post will describe how the ActivityPub plugin works and how you can enable your Discourse community to connect with other communities or Fediverse users.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/discourse-fediverse?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/discourse-fediverse</guid>
      <pubDate>2025-05-09 22:20</pubDate>
      <category>#discourse</category>
      <category>#fediverse</category>
      <category>#social</category>
      <category>#openweb</category>
      <category>#opensource</category>
      <category>#forum</category>
    </item>
    <item>
      <title>Bukmark Club</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/bukmark-club?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/bukmark-club&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The BUKMARK.CLUB is a collection of websites from across the Internet.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/bukmark-club?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/bukmark-club</guid>
      <pubDate>2025-05-09 21:09</pubDate>
      <category>#bookmark</category>
      <category>#internet</category>
      <category>#web</category>
      <category>#community</category>
    </item>
    <item>
      <title>In Praise of Links</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/in-praise-of-links-osteophage?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/in-praise-of-links-osteophage&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Hyperlinks deserve more recognition in light of all the ways their value has been sidelined and denied.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;To that end, I present a link compilation in praise of links. It includes things I agree with entirely and things I don't, spanning from the 2020s to the early 2000s, to supply a tapestry of perspectives, context, and examples on the value and importance of links. Links may have their downsides, challenges, and vulnerabilities, but my hope is that this compilation will (re)invigorate your appreciation for linking as a technology and a social practice, all the better to understand what's at stake when links are discarded and devalued.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/in-praise-of-links-osteophage?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/in-praise-of-links-osteophage</guid>
      <pubDate>2025-05-09 21:05</pubDate>
      <category>#openweb</category>
      <category>#internet</category>
      <category>#indieweb</category>
      <category>#decentralization</category>
      <category>#web</category>
    </item>
    <item>
      <title>The Evolution of Blogging</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/evolution-of-blogging-history-web?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/evolution-of-blogging-history-web&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...around 1997, when Jorn Barger launched his website, titling it ‚ÄúThe Robot Wisdom Weblog.‚Äù Barger‚Äôs plan was to curate his favorite links from around the web, and add bits of commentary to each one. In other words, he meant to keep a log of his web experience. Hence weblog.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In their earliest days, webloggers stood as gatekeepers to the web‚Äôs ever-growing well of content. Each day, these URL pioneers would post a few new links and sprinkle in their own commentary. Blogs acted as a signpost for web users, and following a few key blogs was enough to keep track of just about everything new on the web. Many began to look at the blogging community as a brand new type of media, one that often stood far closer to an impartial truth than traditional mediums would allow for.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;1999, Peter Merholz threw up his own weblog, and in the sidebar added the tagline: &amp;quot;I‚Äôve decided to pronounce the word ‚Äúweblog‚Äù as wee‚Äô- blog. Or ‚Äúblog‚Äù for short.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In the beginning, most bloggers hand coded their sites, adding a new HTML page to their server each time they had a new entry, or just updating the homepage to include the newest links. But soon enough, tools showed up to help bloggers with the process.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...on the fringes, a new type of blog was emerging. The personal blog. These sites ditched the curated links and focused exclusively on commentary. Bloggers used their site to chronicle their personal journey, from the almost boring and banal to the weird and wonderful. This new type of blog was less an alternative media source and more akin to an online journal or diary. And these writers saw themselves not as gatekeepers to the web, but as sharers of their own identity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;New tools soon caught up with this shifting perspective. The first of the bunch to do so was LiveJournal...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Blogger came next. And it would prove to be the most popular of the early blogging tools. The platform first launched in August of 1999, but picked up steam the following year when it introduced the concept of a permalink.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;These platforms made blogging open, in every sense of the word. Visitors were greeted with a single, open textarea on a free platform that was open to all to participate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Blogger became the platform choice for a lot of writers out there. It certainly was for Mena Trott&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Trott had the feeling that platforms like Blogger did not go nearly far enough.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Trott wanted her own site to stand out from the crowd. To be quintessentially hers. Blogger wouldn‚Äôt let her do that. So together with her husband, a Perl programmer, she built Movable Type...And with that move, Trott ushered in a third generation of blogging.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Movable Type allowed users to set up their blog on their own server, and have complete control over its look and feel.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Blog (the noun and verb) became a word familiar not just to web geeks, but to everyone. Lots of imitators and innovators followed in the wake of Moveable Type. Open source platforms like WordPress rose to meet the rising demand. But for a long time, Moveable Type was the standard. It fed its community with a constant stream of new features that promoted openness and freedom.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Blogging, as a community and a practice, continues to be refined to this day thanks to the dedication of web users everywhere. But its sharpest point was made the day Mena Trott decided that &lt;strong&gt;a blog could be more than just some text on a page. It could be one of a kind.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/evolution-of-blogging-history-web?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/evolution-of-blogging-history-web</guid>
      <pubDate>2025-05-09 20:54</pubDate>
      <category>#blogging</category>
      <category>#history</category>
      <category>#web</category>
      <category>#internet</category>
    </item>
    <item>
      <title>socialweb.network</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/socialweb-network?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/socialweb-network&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Your gateway to decentralized social networking protocols, resources, and community&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/socialweb-network?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/socialweb-network</guid>
      <pubDate>2025-05-09 20:49</pubDate>
      <category>#fediverse</category>
      <category>#social</category>
      <category>#openweb</category>
      <category>#decentralized</category>
      <category>#community</category>
      <category>#indieweb</category>
    </item>
    <item>
      <title>Introducing AutoRound</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/intel-autoround?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/intel-autoround&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;As large language models (LLMs) and vision-language models (VLMs) continue to grow in size and complexity, deploying them efficiently becomes increasingly challenging. Quantization offers a solution by reducing model size and inference latency. Intel's AutoRound emerges as a cutting-edge quantization tool that balances accuracy, efficiency, and compatibility.&lt;br /&gt;
&lt;br&gt;
AutoRound is a weight-only post-training quantization (PTQ) method developed by Intel. It uses signed gradient descent to jointly optimize weight rounding and clipping ranges, enabling accurate low-bit quantization (e.g., INT2 - INT8) with minimal accuracy loss in most scenarios. For example, at INT2, it outperforms popular baselines by up to 2.1x higher in relative accuracy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2309.05516"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/intel/auto-round"&gt;Repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/intel-autoround?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/intel-autoround</guid>
      <pubDate>2025-05-09 20:45</pubDate>
      <category>#quantization</category>
      <category>#intel</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Parakeet TDT 0.6B V2 (En)</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nvidia-parakeet-v2?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nvidia-parakeet-v2&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;parakeet-tdt-0.6b-v2 is a 600-million-parameter automatic speech recognition (ASR) model designed for high-quality English transcription, featuring support for punctuation, capitalization, and accurate timestamp prediction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This XL variant of the FastConformer [1] architecture integrates the TDT [2] decoder and is trained with full attention, enabling efficient transcription of audio segments up to 24 minutes in a single pass.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nvidia-parakeet-v2?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nvidia-parakeet-v2</guid>
      <pubDate>2025-05-09 20:43</pubDate>
      <category>#nvidia</category>
      <category>#ai</category>
      <category>#transcription</category>
    </item>
    <item>
      <title>Fediverse House 2025 Roundup</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/fediverse-house-2025-roundup?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/fediverse-house-2025-roundup&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;No Walls, Just Vibes at SXSW‚Äôs Fediverse House&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Fediverse House was SXSW‚Äôs first physical gathering dedicated entirely to the constellation of interoperable social platforms powered by protocols like ActivityPub and AT Protocol. (We know ‚ÄúFediverse‚Äù isn‚Äôt quite accurate to describe the whole ecosystem; in this case, it was just a convenient, fun way to name the space.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It didn‚Äôt matter which platform you preferred or which protocol you were building on; everyone was focused on the singular goal of building a better internet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the real power move for creators is ownership and control of their work and livelihoods.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...identity portability is a radical shift, and...decentralization could lead to more humane social spaces.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://dot-social.simplecast.com/"&gt;DotSocial Podcast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://flipboard.video/c/fediverse.house/videos"&gt;Fediverse House PeerTube Channel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/fediverse-house-2025-roundup?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/fediverse-house-2025-roundup</guid>
      <pubDate>2025-05-09 20:36</pubDate>
      <category>#fediverse</category>
      <category>#conference</category>
      <category>#indieweb</category>
      <category>#openweb</category>
    </item>
    <item>
      <title>Introducing Locate 3D</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/meta-locate3d?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/meta-locate3d&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Locate 3D transforms how AI understands physical space, delivering &lt;strong&gt;state-of-the-art object localization in complex 3D environments&lt;/strong&gt;.&lt;br /&gt;
&lt;br&gt;
Operating directly on standard sensor data, Locate 3D brings spatial intelligence to robotics and augmented reality applications in real-world settings.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://ai.meta.com/research/publications/locate-3d-real-world-object-localization-via-self-supervised-learning-in-3d/"&gt;Paper&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We present Locate 3D, a model for localizing objects in 3D scenes from referring expressions like ‚Äúthe small coffee table between the sofa and the lamp.‚Äù Locate 3D sets a new state-of-the-art on standard referential grounding benchmarks and showcases robust generalization capabilities. Notably, Locate 3D operates directly on sensor observation streams (posed RGB-D frames), enabling real-world deployment on robots and AR devices. Key to our approach is 3D-JEPA, a novel self-supervised learning (SSL) algorithm applicable to sensor point clouds. It takes as input a 3D pointcloud featurized using 2D foundation models (CLIP, DINO). Subsequently, masked prediction in latent space is employed as a pretext task to aid the self-supervised learning of contextualized pointcloud features. Once trained, the 3D-JEPA encoder is finetuned alongside a language-conditioned decoder to jointly predict 3D masks and bounding boxes. Additionally, we introduce Locate 3D Dataset, a new dataset for 3D referential grounding, spanning multiple capture setups with over 130K annotations. This enables a systematic study of generalization capabilities as well as a stronger model.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/meta-locate3d?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/meta-locate3d</guid>
      <pubDate>2025-05-09 20:33</pubDate>
      <category>#meta</category>
      <category>#ai</category>
      <category>#3d</category>
      <category>#research</category>
    </item>
    <item>
      <title>ZeroSearch - Incentivize the Search Capability of LLMs without Searching</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/zero-search-alibaba?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/zero-search-alibaba&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) &lt;strong&gt;Uncontrolled Document Quality&lt;/strong&gt;: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) &lt;strong&gt;Prohibitively High API Costs&lt;/strong&gt;: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce &lt;strong&gt;ZeroSearch&lt;/strong&gt;, a reinforcement learning framework that &lt;strong&gt;enhances the search capabilities of LLMs without interacting with real search engines&lt;/strong&gt;. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model‚Äôs reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that &lt;strong&gt;ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module&lt;/strong&gt;. Remarkably, &lt;strong&gt;a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it&lt;/strong&gt;. Furthermore, it generalizes well across both base and instruction-tuned models of varying sizes and is compatible with a wide range of RL algorithms.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/zero-search-alibaba?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/zero-search-alibaba</guid>
      <pubDate>2025-05-09 20:31</pubDate>
      <category>#ai</category>
      <category>#search</category>
      <category>#llm</category>
      <category>#research</category>
    </item>
    <item>
      <title>A Survey of AI Agent Protocols</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/survey-ai-agent-protocols?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/survey-ai-agent-protocols&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;It'll be interesting to see once many of the exiting protocols begin to converge towards a standard. There's a ton of existing well-adopted protocol standards out there that when stitched together could create a compelling and native experience. I think eventually we'll get to a place where agents are native parts of the OSI stack and the world wide web.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: &lt;strong&gt;there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence.&lt;/strong&gt; In this paper, we provide the first comprehensive analysis of existing agent protocols, proposing a systematic two-dimensional classification that differentiates context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore the future landscape of agent protocols by identifying critical research directions and characteristics necessary for next-generation protocols. These characteristics include adaptability, privacy preservation, and group-based interaction, as well as trends toward layered architectures and collective intelligence infrastructures. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/survey-ai-agent-protocols?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/survey-ai-agent-protocols</guid>
      <pubDate>2025-05-09 20:23</pubDate>
      <category>#ai</category>
      <category>#agents</category>
      <category>#protocols</category>
      <category>#research</category>
    </item>
    <item>
      <title>Resisting the urge to rewrite the website</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/resisting-urge-rewrite-website-santala?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/resisting-urge-rewrite-website-santala&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I have half-jokingly been entertaining an idea of building a new blog: for every blog post published, you get to add one new feature. The first blog post needs to be written in a plain txt file and deployed as-is to a server. With the second post, you can choose to add something you want: maybe a bit of HTML navigation to be able to have links between the blog posts. The third post could come with Markdown support so your posts are not plain text anymore but rendered into HTML. And so on.&lt;br /&gt;
&lt;br&gt;
The idea would is that you get to ‚Äúearn‚Äù feature development by writing blog posts. If you want a fancy site with bells and whistles and good tooling, you have to write and publish a lot. That way, you cannot get lost into the rabbit hole of new shiny feature development at the expense of writing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I like this suggestion. My rewrite has been ongoing but slow. I tried vibe-coding my way to success but didn't have much luck there, so it's taking more time to do it manually. If I was doing it from scratch it wouldn't be a challenge. The challenge is making all the posts and current structure fit into what I want to website to eventually look like.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I use different tools for writing and coding. I can write my blog posts on my iPad when I‚Äôm in the library or local pub and at that time, there‚Äôs no way for me to tinker with the tools or the website itself.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another great suggestion. A lot of my desire to rewrite stems from frictions in my current workflow as well as being able to support new and different post types. Back when I had my &lt;a href="/feed/surface-duo-blogging-github-dev/"&gt;Duo&lt;/a&gt;, I didn't mind the mobile VS Code authoring experience, but now that I'm back to a single screen device, not being able to publish from my phone creates a lot of friction.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/resisting-urge-rewrite-website-santala?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/resisting-urge-rewrite-website-santala</guid>
      <pubDate>2025-05-05 20:06</pubDate>
      <category>#website</category>
      <category>#indieweb</category>
      <category>#blogging</category>
    </item>
    <item>
      <title>TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/telograf?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/telograf&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. &lt;strong&gt;In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications.&lt;/strong&gt; We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at &lt;a href="https://github.com/mengyuest/TeLoGraF"&gt;this https URL&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/telograf?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/telograf</guid>
      <pubDate>2025-05-05 20:03</pubDate>
      <category>#ai</category>
      <category>#graphs</category>
      <category>#neuralnetworks</category>
      <category>#research</category>
    </item>
    <item>
      <title>Just Put It On Your Blog</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/just-post-on-your-blog-shellsharks?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/just-post-on-your-blog-shellsharks&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It‚Äôs great to have a place to share your thoughts. A place you can go back to when you want to remember something you had written or thought about before. A place you can refer people to when they have questions you‚Äôve answered in the past. A place to be you. So, get a blog, and put all the things there.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is great to carve out your own space in the interwebs. If you run out of ideas of what to do with your website, here's &lt;a href="https://jamesg.blog/2024/02/19/personal-website-ideas"&gt;James' blog with 100+ ideas&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/just-post-on-your-blog-shellsharks?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/just-post-on-your-blog-shellsharks</guid>
      <pubDate>2025-05-05 19:57</pubDate>
      <category>#blogging</category>
      <category>#indieweb</category>
      <category>#website</category>
    </item>
    <item>
      <title>CORG: Generating Answers from Complex, Interrelated Contexts</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/corg-context-organizer-framework?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/corg-context-organizer-framework&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, &lt;strong&gt;we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator.&lt;/strong&gt; Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/corg-context-organizer-framework?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/corg-context-organizer-framework</guid>
      <pubDate>2025-05-05 19:53</pubDate>
      <category>#rag</category>
      <category>#ai</category>
      <category>#knowledge</category>
      <category>#research</category>
    </item>
    <item>
      <title>Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/self-generate-incontext-examples-llm-agents-improve-sequential-decision-making?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/self-generate-incontext-examples-llm-agents-improve-sequential-decision-making&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that &lt;strong&gt;automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/self-generate-incontext-examples-llm-agents-improve-sequential-decision-making?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/self-generate-incontext-examples-llm-agents-improve-sequential-decision-making</guid>
      <pubDate>2025-05-05 19:50</pubDate>
      <category>#ai</category>
      <category>#agents</category>
      <category>#planning</category>
      <category>#research</category>
    </item>
    <item>
      <title>Running RAG with ONNX Runtime GenAI for On-Prem Windows</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/rag-onnx-runtime-genai-windows?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/rag-onnx-runtime-genai-windows&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Really great to see these case studies and comparisons.&lt;/p&gt;
&lt;p&gt;A while back we published a blog post showcasing how experiences like the &lt;a href="https://devblogs.microsoft.com/dotnet/introducing-ai-dev-gallery-gateway-to-local-ai-development/"&gt;AI Dev Gallery&lt;/a&gt; make use of ONNX Runtime and the various AI building blocks in .NET to enable a diverse set of scenarios.&lt;/p&gt;
&lt;p&gt;For more ONNX Runtime GenAI focused C# content, you can also reference the post &lt;a href="https://devblogs.microsoft.com/dotnet/using-phi3-csharp-with-onnx-for-text-and-vision-samples-md/"&gt;Using Phi-3 &amp;amp; C# with ONNX for text and vision samples&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/rag-onnx-runtime-genai-windows?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/rag-onnx-runtime-genai-windows</guid>
      <pubDate>2025-05-02 15:19</pubDate>
      <category>#onnxruntime</category>
      <category>#windows</category>
      <category>#dotnet</category>
    </item>
    <item>
      <title>ALL about RSS</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/all-about-rss?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/all-about-rss&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A list of RSS related stuff: tools, services, communities and tutorials, etc.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/all-about-rss?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/all-about-rss</guid>
      <pubDate>2025-04-28 21:07</pubDate>
      <category>#rss</category>
      <category>#indieweb</category>
      <category>#feeds</category>
    </item>
    <item>
      <title>GPT Image 1 - Image Generation API</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/gpt-image-1-api?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/gpt-image-1-api&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we‚Äôre bringing the natively multimodal model that powers this experience in ChatGPT to the API via &lt;code&gt;gpt-image-1&lt;/code&gt;, enabling developers and businesses to easily integrate high-quality, professional-grade image generation directly into their own tools and platforms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The &lt;code&gt;gpt-image-1&lt;/code&gt; model is now available globally via the Images API, with support in the Responses API coming soon.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/gpt-image-1-api?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/gpt-image-1-api</guid>
      <pubDate>2025-04-24 19:50</pubDate>
      <category>#openai</category>
      <category>#ai</category>
      <category>#gpt</category>
    </item>
    <item>
      <title>The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/llama-4-herd?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/llama-4-herd&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôre introducing Llama 4 Scout and Llama 4 Maverick, the first open-weight natively multimodal models with unprecedented context length support and our first built using a mixture-of-experts (MoE) architecture. We‚Äôre also previewing Llama 4 Behemoth, one of the smartest LLMs in the world and our most powerful yet to serve as a teacher for our new models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;After tinkering with Mistral Small 3.1, I'm excited to try Llama 4 Scout once it lands in GitHub Models and Ollama.&lt;/p&gt;
&lt;p&gt;I don't think that Mistral Small 3.1 uses MoE architecture which should help a ton considering the 10 million token context size.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/llama-4-herd?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/llama-4-herd</guid>
      <pubDate>2025-04-08 08:30</pubDate>
      <category>#llama</category>
      <category>#meta</category>
      <category>#ai</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Gen Z Leading Tumblr Revival</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/gen-z-tumblr-revival?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/gen-z-tumblr-revival&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Thanks to Gen Z, the site has found new life. As of 2025, Gen Z makes up 50% of Tumblr‚Äôs active monthly users and accounts for 60% of new sign-ups&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Don't call it a comeback.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;User numbers spiked in January during the near-ban of TikTok and jumped again last year when Brazil temporarily banned X.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;To keep up with the momentum, Tumblr introduced Reddit-style Communities in December, letting users connect over shared interests like photography and video games. In January, it debuted Tumblr TV‚Äîa TikTok-like feature that serves as both a GIF search engine and a short-form video platform.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;But perhaps Tumblr‚Äôs greatest strength is that it isn‚Äôt TikTok or Facebook.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;For its users...that‚Äôs part of the appeal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Tumblr's been through a rough ride, but at the core there's a lot of nice things to like about it. In previous posts, I talked about some opportunities for Tumblr as a result of &lt;a href="/feed/shipping-wordpress-tumblr"&gt;replatforming on top of WordPress&lt;/a&gt; and its expressed intent for &lt;a href="/feed/tumblr-still-working-fediverse-integration"&gt;ActivityPub integration&lt;/a&gt;. In doing so, not only would it enable an intuitive front-end for publishing different kinds of posts, but it would also provide opportunities for self-hosting and a more federated and decentralized social platform.&lt;/p&gt;
&lt;p&gt;I'm excited to see where this goes.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.businessinsider.com/gen-z-flocking-tumblr-millennials-musk-zuckerberg-safe-space-2025-4"&gt;Related post from Business Insider's Amanda Hoover&lt;/a&gt; on the topic.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/gen-z-tumblr-revival?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/gen-z-tumblr-revival</guid>
      <pubDate>2025-04-08 06:05</pubDate>
      <category>#tumblr</category>
      <category>#automattic</category>
      <category>#socialmedia</category>
      <category>#genz</category>
      <category>#activitypub</category>
      <category>#fediverse</category>
    </item>
    <item>
      <title>Bringing Static Sites to the Fediverse - Enhancements and Implementations</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/bringing-static-sites-fediverse-enhancements-implementations-maho?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/bringing-static-sites-fediverse-enhancements-implementations-maho&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Awesome post from my friend &lt;a href="https://maho.dev/"&gt;maho.dev&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I‚Äôve introduced customizable templates for note generation. This enhancement allows the full content of a blog post to be included directly in ActivityPub notes, moving beyond mere link sharing‚Äîa practice often associated with bots‚Äîand leveraging the full potential of the Fediverse.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is cool. I'd like something better for my POSSE setup. Might give this a try.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/bringing-static-sites-fediverse-enhancements-implementations-maho?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/bringing-static-sites-fediverse-enhancements-implementations-maho</guid>
      <pubDate>2025-03-18 21:08</pubDate>
      <category>#fediverse</category>
      <category>#indieweb</category>
      <category>#activitypub</category>
    </item>
    <item>
      <title>Introducing Gemma 3</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-gemma-3?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-gemma-3&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we're introducing Gemma 3, a collection of lightweight, state-of-the-art open models built from the same research and technology that powers our Gemini 2.0 models. These are our most advanced, portable and responsibly developed open models yet. They are designed to run fast, directly on devices ‚Äî from phones and laptops to workstations ‚Äî helping developers create AI applications, wherever people need them. Gemma 3 comes in a range of sizes (1B, 4B, 12B and 27B), allowing you to choose the best model for your specific hardware and performance needs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Build with the world's best single-accelerator model: Gemma 3 delivers state-of-the-art performance for its size, outperforming Llama3-405B, DeepSeek-V3 and o3-mini in preliminary human preference evaluations on LMArena‚Äôs leaderboard. This helps you to &lt;strong&gt;create engaging user experiences that can fit on a single GPU or TPU host.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Create AI with &lt;strong&gt;advanced text and visual reasoning capabilities&lt;/strong&gt;: Easily build applications that analyze images, text, and short videos, opening up new possibilities for interactive and intelligent applications.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Handle complex tasks with an expanded context window: Gemma 3 offers a &lt;strong&gt;128k-token context window&lt;/strong&gt; to let your applications process and understand vast amounts of information.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Create AI-driven workflows using function calling: &lt;strong&gt;Gemma 3 supports function calling and structured output&lt;/strong&gt; to help you automate tasks and build agentic experiences.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-gemma-3?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-gemma-3</guid>
      <pubDate>2025-03-18 20:55</pubDate>
      <category>#gemma</category>
      <category>#google</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Introducing Command A</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-command-a-cohere?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-command-a-cohere&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we‚Äôre introducing Command A, a new state-of-the-art generative model optimized for demanding enterprises that require fast, secure, and high-quality AI. Command A delivers maximum performance with minimal hardware costs when compared to leading proprietary and open-weights models, such as GPT-4o and DeepSeek-V3. For private deployments, Command A excels on business-critical agentic and multilingual tasks, while being deployable on just two GPUs, compared to other models that typically require as many as 32.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Its 256k context length (2x most leading models) can handle much longer enterprise documents. Other key features include Cohere‚Äôs advanced retrieval-augmented generation (RAG) with verifiable citations, agentic tool use, enterprise-grade security, and strong multilingual performance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The next generation of Cohere models will help power a range of AI applications for customers across industries like finance, healthcare, manufacturing, energy, and the public sector. In particular, they will seamlessly integrate with North, our secure AI agents platform to unlock the full potential of your company data and people with AI agents.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-command-a-cohere?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-command-a-cohere</guid>
      <pubDate>2025-03-18 20:45</pubDate>
      <category>#cohere</category>
      <category>#ai</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Simon Willison's AI-Generated Tools Colophon</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/willison-tools-colophon?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/willison-tools-colophon&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The tools on tools.simonwillison.net were mostly built using AI-assisted programming.&lt;br /&gt;
&lt;br&gt;
This page lists the commit messages for each tool, many of which link to the LLM transcript used to produce the code.&lt;br /&gt;
&lt;br&gt;
The descriptions for each of the tools were generated using Claude 3.7 Sonnet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is such a cool use of a website. Using AI, Simon has generated a set of small utilities that are useful to him. If they're useful to others though, he's made them avaiable on his website.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/willison-tools-colophon?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/willison-tools-colophon</guid>
      <pubDate>2025-03-18 20:35</pubDate>
      <category>#colophon</category>
      <category>#indieweb</category>
      <category>#ai</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Mistral Small 3.1</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mistral-small-3-1?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mistral-small-3-1&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Building on Mistral Small 3, this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini, while delivering inference speeds of 150 tokens per second.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Lightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mistral-small-3-1?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mistral-small-3-1</guid>
      <pubDate>2025-03-18 20:31</pubDate>
      <category>#mistral</category>
      <category>#ai</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Just pre-ordered by PebbleOS Core Time 2 smartwatch</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/preordered-pebbleos-watch?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/preordered-pebbleos-watch&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Just pre-ordered my Pebble Core Time 2 Duo! Sadly it won't be here until December.&lt;/p&gt;
&lt;p&gt;Go to &lt;a href="https://store.repebble.com/"&gt;https://store.repebble.com/&lt;/a&gt; to get yours.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/preordered-pebbleos-watch?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/preordered-pebbleos-watch</guid>
      <pubDate>2025-03-18 20:01</pubDate>
      <category>#pebbleos</category>
      <category>#smartwatch</category>
      <category>#opensource</category>
      <category>#pebble</category>
    </item>
    <item>
      <title>Introducing AX: Why Agent Experience Matters</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ax-why-agent-experience-matters?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ax-why-agent-experience-matters&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Interesting post from &lt;a href="https://biilmann.blog/"&gt;Matt Biilman&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the largest disruption from the current evolution of AI will come from bringing agency to computers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Computers will perceive their environment and take actions autonomously in order to achieve goals.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Companies discovered that improving the DX of their products would empower and incentivize developers to extend their product with new capabilities and lead to huge competitive advantages. For developer tool companies, DX became a key competitive differentiator.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We need to start focusing on AX or ‚Äúagent experience‚Äù ‚Äî the holistic experience AI agents will have as the user of a product or platform.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Too many companies are focusing on adding shallow AI features all over their products or building yet another AI agent. The real breakthrough will be thinking about how your customers‚Äô favorite agents can help them derive more value from your product. This requires thinking deeply about agents as a persona your team is building and developing for.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;As AI agents start becoming useful and commonplace, we‚Äôre broadly going to see two approaches enabling agents to interact with the software we depend on:&lt;br /&gt;
&lt;br&gt;
A closed vertical approach, where companies tightly integrate their own agents into their own software. An open approach, where companies focus on making their software accessible to external agents.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Leaning into AX as a strategy, means embracing a vision of an open agent world. This vision aligns with the original ethos of the open web: a place where many diverse competing agents (built by different people or companies) can seamlessly interact with software on behalf of their users. Prioritizing AX makes it as simple as possible for any agent a user prefers, to deliver outcomes on their behalf.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Software designed for AI agents has the potential to deliver exponential value. As an industry we must collectively focus on building an open agent ecosystem and designing thoughtful AX to create a better, more open, and connected digital world.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ax-why-agent-experience-matters?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ax-why-agent-experience-matters</guid>
      <pubDate>2025-03-05 21:49</pubDate>
      <category>#ai</category>
      <category>#agents</category>
      <category>#ax</category>
    </item>
    <item>
      <title>Introducing Mercury, the first commercial-scale diffusion large language model</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mercury-first-commercial-scale-llm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mercury-first-commercial-scale-llm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are announcing the Mercury family of diffusion large language models (dLLMs), a new generation of LLMs that push the frontier of fast, high-quality text generation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mercury is up to 10x faster than frontier speed-optimized LLMs. Our models run at over 1000 tokens/sec on NVIDIA H100s, a speed previously possible only using custom chips.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mercury-first-commercial-scale-llm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mercury-first-commercial-scale-llm</guid>
      <pubDate>2025-02-26 20:10</pubDate>
      <category>#diffusion</category>
      <category>#llm</category>
      <category>#ai</category>
    </item>
    <item>
      <title>The Algorithm - J. Cole has a blog?</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/inevitable-jcole-blog?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/inevitable-jcole-blog&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This is just a place for me to share. I been wanting a lil blog for years. Somewhere to post random shit I fuck with where the audience is way smaller than it is on the social media platforms. Finally pulled the trigger, bare with us as we still developing this page and the layout.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is cool!&lt;/p&gt;
&lt;p&gt;I ran into this while reading the post &lt;a href="https://herbertlui.net/blogging-for-smaller-audiences-and-deeper-connections/"&gt;Blogging for smaller audiences and deeper connections&lt;/a&gt; by &lt;a href="https://herbertlui.net/"&gt;Herbert Lui&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Looking at &lt;a href="https://www.inevitable.live/algorithm"&gt;other posts&lt;/a&gt;, it looks like he or at least someone is actively posting.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/inevitable-jcole-blog?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/inevitable-jcole-blog</guid>
      <pubDate>2025-02-25 21:34</pubDate>
      <category>#music</category>
      <category>#blog</category>
      <category>#indieweb</category>
      <category>#creativity</category>
      <category>#art</category>
    </item>
    <item>
      <title>Your Site Is a Home</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/your-site-is-a-home-hamid?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/your-site-is-a-home-hamid&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;While going through a backlog of personal blogs I haven't read in a few weeks, I'm noticing a trend in posts that express similar sentiments to those in Naz's post.&lt;/p&gt;
&lt;p&gt;I'm happy to see more people choosing to have a place that gives them more autonomy and freedom to express themselves in whichever form makes the most sense for them.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Your site is a home.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Eventually, social networks were created...novelty and the promise of interconnectedness by gathering in a common town square to blast out whatever was going on in our lives eventually won out.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;But you might have still had your website, your home to return to...the place you built.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Things started to change and instead of going home, you, and everybody else started to live in the town square.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;There is a shape to your physical home. Arranged and organized in the ways that make sense for you, the reward is a space that works for you.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A platform or network doesn‚Äôt allow for much configuration. The town square isn‚Äôt owned by you.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;People became absolutely reliant on the same gathering place...the convenience of seeing friends (sometimes) outweighed your other neighbors spouting garbage and hate. You came to rely on this place for everything.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;You can still have a home.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I have very little at the town square, because it‚Äôs not a public one. It‚Äôs a walled-off town square, whose rules and borders change at the whims of those who created it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Thanks for visiting my home. I‚Äôm glad you dropped by.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I‚Äôd love to see yours sometime.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/your-site-is-a-home-hamid?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/your-site-is-a-home-hamid</guid>
      <pubDate>2025-02-25 20:49</pubDate>
      <category>#personalweb</category>
      <category>#indieweb</category>
      <category>#socialmedia</category>
    </item>
    <item>
      <title>How to Multistream Using Azure, Azure VM, MonaServer 2, and FFmpeg with OBS Studio</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/multistream-azure-ffmpeg-obs-studio-maho?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/multistream-azure-ffmpeg-obs-studio-maho&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In this guide, we will set up a multistreaming workflow using Azure, an Azure VM, MonaServer 2, and FFmpeg to broadcast to multiple platforms like LinkedIn Live and YouTube Live. We‚Äôll use OBS Studio as the main streaming software. Let‚Äôs get started.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Love this detailed writeup from &lt;a href="https://maho.dev/"&gt;Maho&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When I originally experimented with something similar using my Owncast instance, I was running ffmpeg locally but I like his solution better.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/multistream-azure-ffmpeg-obs-studio-maho?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/multistream-azure-ffmpeg-obs-studio-maho</guid>
      <pubDate>2025-02-25 20:33</pubDate>
      <category>#livestream</category>
      <category>#azure</category>
      <category>#obs</category>
    </item>
    <item>
      <title>Unplatform</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/unplatform-indieweb?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/unplatform-indieweb&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Unplatform is an interactive guidebook, online library, and recommendations database intended to help you escape social media and join the indie web.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Love this guide.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/unplatform-indieweb?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/unplatform-indieweb</guid>
      <pubDate>2025-02-25 20:20</pubDate>
      <category>#indieweb</category>
      <category>#socialmedia</category>
      <category>#personalweb</category>
    </item>
    <item>
      <title>s1: Simple test-time scaling</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/s1-simple-test-time-scaling?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/s1-simple-test-time-scaling&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending &amp;quot;Wait&amp;quot; multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at &lt;a href="https://github.com/simplescaling/s1"&gt;this https URL&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/s1-simple-test-time-scaling?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/s1-simple-test-time-scaling</guid>
      <pubDate>2025-02-24 20:54</pubDate>
      <category>#ai</category>
      <category>#research</category>
      <category>#s1</category>
    </item>
    <item>
      <title>Nomic Embed Text V2: An Open Source, Multilingual, Mixture-of-Experts Embedding Model</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nomic-embed-text-v2?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nomic-embed-text-v2&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we're excited to announce Nomic Embed Text V2, our next-generation embedding model that brings the Mixture of Experts (MoE) architecture to text embeddings on a new expanded multilingual training dataset.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Personally, I found the part on MoE to be the most interesting about this release.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Rather than a dense model which uses all parameters on an input, the MoE architecture dynamically routes to different &amp;quot;experts&amp;quot; - sparse subsets of parameters at each layer - activating, ideally, only the parameters especially needed to process the input. This approach allows for more efficient use of compute when generating embeddings.&lt;br /&gt;
&lt;br&gt;
In our experiments, we found that alternating MoE layers with 8 experts and top-2 routing provides the optimal balance between performance and efficiency. This results in 475M total parameters in the model, but only 305M active during training and inference.&lt;br /&gt;
&lt;br&gt;
Research into embedding model architecture has significant practical implications for working with text embeddings in production:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lower latency for high-volume applications of embeddings like retrieval&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Reduced deployment costs through more efficient parameter usage&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;More accessibility to embeddings in settings with constrained compute&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nomic-embed-text-v2?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nomic-embed-text-v2</guid>
      <pubDate>2025-02-24 20:47</pubDate>
      <category>#nomic</category>
      <category>#ai</category>
      <category>#embeddings</category>
    </item>
    <item>
      <title>Claude 3.7 Sonnet and Claude Code</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/anthropic-claude-3-7-sonnet-code?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/anthropic-claude-3-7-sonnet-code&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we‚Äôre announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model can think for.
&lt;br&gt;
Claude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development. Along with the model, we‚Äôre also introducing a command line tool for agentic coding, Claude Code. Claude Code is available as a limited research preview, and enables developers to delegate substantial engineering tasks to Claude directly from their terminal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Claude 3.7 Sonnet and Claude Code mark an important step towards AI systems that can truly augment human capabilities. With their ability to reason deeply, work autonomously, and collaborate effectively, they bring us closer to a future where AI enriches and expands what humans can achieve.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/anthropic-claude-3-7-sonnet-code?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/anthropic-claude-3-7-sonnet-code</guid>
      <pubDate>2025-02-24 20:43</pubDate>
      <category>#ai</category>
      <category>#anthropic</category>
      <category>#claude</category>
    </item>
    <item>
      <title>The Ultra-Scale Playbook: Training LLMs on GPU Clusters</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ultrascale-playbook-training-llms-gpu-clusters?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ultrascale-playbook-training-llms-gpu-clusters&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Gold.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;All the techniques we'll cover in this book tackle one or several of the following three key challenges, which we'll keep bumping into throughout the book:&lt;/p&gt;
&lt;hr&gt;
  1. **Memory Usage**: it's a hard limitation - if a training step doesn't fit in memory, training cannot proceed  
  2. **Compute Efficiency**: we want our hardware to spend most time computing, so we need to reduce time spent on data transfers or waiting for other GPUs to perform work.  
  3. **Communication overhead**: we want to minimize communication overhead as it keeps GPUs idle. To archieve this we will try to make best use of intra-node (fast) and inter-node (slower) bandwidths as well as overlap communication with compute as much as possible.  
&lt;hr&gt;
In many places we'll see that we can trade one of these (computation, communication, memory) for another (e.g. recomputation or Tensor Parallelism). Finding the right balance is key to scaling training.
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src="https://nanotron-ultrascale-playbook.static.hf.space/dist/assets/images/ultra-cheatsheet.svg" class="img-fluid" alt="Ultra-Scale Playbook Cheatsheet" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ultrascale-playbook-training-llms-gpu-clusters?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ultrascale-playbook-training-llms-gpu-clusters</guid>
      <pubDate>2025-02-19 22:03</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#training</category>
    </item>
    <item>
      <title>Dream Job - Google Super Bowl 2025 Ad</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/google-dream-job-2025-sb-ad?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/google-dream-job-2025-sb-ad&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I didn't watch the Super Bowl this year so I didn't get a chance to watch the ads until a few days later.&lt;/p&gt;
&lt;p&gt;From the ones I saw, I think Google's was my favorite.&lt;/p&gt;
&lt;p&gt;Apart from the touching story, what stood out to me the most was its focus on humanity, with technology playing a supporting role rather than being the main character.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-7e6g11BJc0" title="Thumbnail Google Dream Job Super Bowl 2025 Ad"&gt;&lt;img src="http://img.youtube.com/vi/-7e6g11BJc0/0.jpg" class="img-fluid" alt="Thumbnail Google Dream Job Super Bowl 2025 Ad" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/google-dream-job-2025-sb-ad?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/google-dream-job-2025-sb-ad</guid>
      <pubDate>2025-02-17 16:00</pubDate>
      <category>#google</category>
      <category>#ad</category>
      <category>#superbowl</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Languages &amp; Runtime Community Standup - Tensors in .NET</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/dotnet-language-runtime-standup-tensors?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/dotnet-language-runtime-standup-tensors&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Great session on Tensors in .NET by &lt;a href="https://bsky.app/profile/tannergooding.bsky.social"&gt;Tanner&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tensors are the foundational data structure powering man AI workloads today.&lt;/p&gt;
&lt;p&gt;Introduced in .NET 9, &lt;a href="https://learn.microsoft.com/en-us/dotnet/core/whats-new/dotnet-9/overview#tensort"&gt;Tensors&lt;/a&gt; builds on top of earlier work like &lt;a href="https://devblogs.microsoft.com/dotnet/announcing-ml-net-3-0/#tensor-primitives-integration"&gt;TensorPrimitives&lt;/a&gt;, &lt;a href="https://devblogs.microsoft.com/dotnet/hardware-intrinsics-in-net-core/"&gt;hardware intrinsics&lt;/a&gt;, and &lt;a href="https://learn.microsoft.com/en-us/dotnet/standard/generics/math"&gt;generic math&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can check out the recording below to learn more.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=VOEeNffChSg" title="Language Runtime Community Standup Tensors in .NET Thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/VOEeNffChSg/0.jpg" class="img-fluid" alt="Language Runtime Community Standup Tensors in .NET Thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/dotnet-language-runtime-standup-tensors?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/dotnet-language-runtime-standup-tensors</guid>
      <pubDate>2025-02-15 15:12</pubDate>
      <category>#ai</category>
      <category>#tensor</category>
      <category>#dotnet</category>
      <category>#runtime</category>
    </item>
    <item>
      <title>Generative AI for Beginners (.NET) is now available</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/generative-ai-for-beginners-dotnet-course?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/generative-ai-for-beginners-dotnet-course&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;If you're a .NET developer looking to get started with Generative AI, there's &lt;a href="https://devblogs.microsoft.com/dotnet/announcing-generative-ai-for-beginners-dotnet/"&gt;a new course for beginners&lt;/a&gt; that just launched.&lt;/p&gt;
&lt;p&gt;Through a series of self-paced lessons, you'll learn the foundations to help you start using AI to augment the capabilities of your .NET applications.&lt;/p&gt;
&lt;p&gt;Check out the &lt;a href="https://aka.ms/genainet"&gt;course&lt;/a&gt; and give us feedback!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/generative-ai-for-beginners-dotnet-course?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/generative-ai-for-beginners-dotnet-course</guid>
      <pubDate>2025-02-15 12:14</pubDate>
      <category>#ai</category>
      <category>#dotnet</category>
      <category>#course</category>
      <category>#generativeai</category>
    </item>
    <item>
      <title>AI Dev Gallery now in the Microsoft Store</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ai-dev-gallery-windows-preview-ms-store?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ai-dev-gallery-windows-preview-ms-store&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;So excited to see the &lt;a href="https://apps.microsoft.com/detail/9n9pn1mm3bd5"&gt;AI Dev Gallery is now in the Microsoft Store&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Through various interactive samples, developers get to see how they might use AI for different tasks like text classification, object detection, and many others.&lt;/p&gt;
&lt;p&gt;Best of all, the models are all running locally and you get to see and export the C# code powering the samples to Visual Studio so you can continue tinkering on your own and integrate into your own applications.&lt;/p&gt;
&lt;p&gt;A few months ago, the team came on the .NET AI Community Standup to showcase the app. Since then, it's only kept improving and introducing new scenarios.&lt;/p&gt;
&lt;p&gt;You can check out the recording from that stream here.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=5H9TxzCQfNo" title="Thumbnail of AI Community Standup AI Dev Gallery"&gt;&lt;img src="http://img.youtube.com/vi/5H9TxzCQfNo/0.jpg" class="img-fluid" alt="Thumbnail of AI Community Standup AI Dev Gallery" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ai-dev-gallery-windows-preview-ms-store?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ai-dev-gallery-windows-preview-ms-store</guid>
      <pubDate>2025-02-15 12:02</pubDate>
      <category>#ai</category>
      <category>#windows</category>
      <category>#dotnet</category>
      <category>#microsoft</category>
      <category>#onnx</category>
    </item>
    <item>
      <title>HuggingFace AI Agents Course</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/huggingface-ai-agents-course?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/huggingface-ai-agents-course&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This free course will take you on a journey, from beginner to expert, in understanding, using and building AI agents.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;At the end of this course you‚Äôll understand how Agents work and how to build your own Agents using the latest libraries and tools.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/huggingface-ai-agents-course?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/huggingface-ai-agents-course</guid>
      <pubDate>2025-02-10 17:20</pubDate>
      <category>#huggingface</category>
      <category>#ai</category>
      <category>#agents</category>
      <category>#course</category>
    </item>
    <item>
      <title>Why Blog If Nobody Reads It?</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/why-blog-nobody-reads-andy?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/why-blog-nobody-reads-andy&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;There is a hidden value in blogging... You do it not for the applause but because it needs doing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;You write because you think, because you observe, because you need to put it somewhere.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;If someone reads it? Bonus. If not? The work still got done.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;üíØüíØüíØ&lt;/p&gt;
&lt;p&gt;I agree with many of the points in Andy's post.&lt;/p&gt;
&lt;p&gt;I don't have analytics on my website, so I couldn't tell you who or how many people are visiting it.&lt;/p&gt;
&lt;p&gt;And that's okay.&lt;/p&gt;
&lt;p&gt;When someone &lt;a href="https://malici.ous.computer/@shellsharks/statuses/01JKGDJCE0WZYAWRB816J8JFG2"&gt;links to content they found on my website and tags me&lt;/a&gt; or &lt;a href="/feed/cdn-issue-fixed-redirects-working"&gt;sends me an e-mail letting me know something on my website is broken&lt;/a&gt; is when I know there's at least one person.&lt;/p&gt;
&lt;p&gt;Many of the content on here serves as a reminder of how I've solved problems for myself. The microposts and responses are observations that capture a moment in time. If the messages reach someone on the other end and either solve their problem or resonate with them, I'm happy that happens, but I see that as a bonus.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/why-blog-nobody-reads-andy?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/why-blog-nobody-reads-andy</guid>
      <pubDate>2025-02-10 17:02</pubDate>
      <category>#blogging</category>
      <category>#expression</category>
      <category>#creativity</category>
      <category>#writing</category>
    </item>
    <item>
      <title>Bringing Pebble Back</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/bringing-pebble-back?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/bringing-pebble-back&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;
We‚Äôre making a new Pebble-style smartwatch.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is amazing news! By the time I decided to get a Pebble, they'd already been sold to Google.&lt;/p&gt;
&lt;p&gt;I agree with Eric's points here:&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I've tried every single smart watch out there, but none do it for me. No one makes a smartwatch with the core set of features I want:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Always-on e-paper screen&lt;/strong&gt; (it‚Äôs reflective rather than emissive. Sunlight readable. Glanceable. Not distracting to others like a bright wrist)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Long battery life&lt;/strong&gt; (one less thing to charge. It‚Äôs annoying to need extra cables when traveling)
Simple and beautiful user experience around a core set of features I use regularly (telling time, notifications, music control, alarms, weather, calendar, sleep/step tracking)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Buttons!&lt;/strong&gt; (to play/pause/skip music on my phone without looking at the screen)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hackable&lt;/strong&gt; (apparently you can‚Äôt even write your own watchfaces for Apple Watch? That is wild. There were &amp;gt;16k watchfaces on the Pebble appstore!)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Garmin is the closest I've come to it at least on the battery life and buttons front. Pebble was one-of-a-kind.&lt;/p&gt;
&lt;p&gt;I already signed up to stay up to date with the project and plan on being a day one customer. You can too at &lt;a href="https://repebble.com/"&gt;repebble.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;P.S. While not surprised, I didn't know Eric was behind another amazing project I'm a fan of, &lt;a href="https://www.beeper.com/"&gt;Beeper&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/bringing-pebble-back?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/bringing-pebble-back</guid>
      <pubDate>2025-02-06 19:42</pubDate>
      <category>#eink</category>
      <category>#pebble</category>
      <category>#smartwatch</category>
      <category>#google</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Build a link blog</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/build-a-link-blog-xuanwo?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/build-a-link-blog-xuanwo&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Great to see more people sharing knowledge and linking to others from their own websites / platforms.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I decided to follow simon's approach to creating a link blog, where I can share interesting links I find on the internet along with my own comments and thoughts about them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...this is an excellent way to share knowledge while also keeping a personal record. It‚Äôs much better than simply saving something to Readwise to read later and leaving a few highlights or dull comments like &amp;quot;interesting.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/build-a-link-blog-xuanwo?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/build-a-link-blog-xuanwo</guid>
      <pubDate>2025-02-04 22:29</pubDate>
      <category>#blogging</category>
      <category>#indieweb</category>
      <category>#linkblog</category>
      <category>#personalweb</category>
      <category>#openweb</category>
    </item>
    <item>
      <title>Master the Art of the Product Manager 'No'</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/master-art-of-product-manager?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/master-art-of-product-manager&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;üòÇüòÇüòÇ&lt;/p&gt;
&lt;p&gt;This website is amazing.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We already finalized our OKRs for H1 but if you throw it in the backlog maybe we can get it prioritized in H2 planning&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This could be part of a larger initiative, but let‚Äôs hold off for now&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We should validate that with users first&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/master-art-of-product-manager?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/master-art-of-product-manager</guid>
      <pubDate>2025-02-03 20:26</pubDate>
      <category>#fun</category>
      <category>#business</category>
      <category>#productmanager</category>
    </item>
    <item>
      <title>The Illustrated DeepSeek-R1</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/illustrated-deepseek-r1?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/illustrated-deepseek-r1&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Just like most existing LLMs, DeepSeek-R1 generates one token at a time, except it excels at solving math and reasoning problems because it is able to spend more time processing a problem through the process of generating thinking tokens that explain its chain of thought.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed7fd8c3-7654-497c-a8e2-1f2e7930992e_3302x1438.png" class="img-fluid" alt="DeepSeek R1 Training Recipe" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;newsletter.languagemodels.co&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/illustrated-deepseek-r1?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/illustrated-deepseek-r1</guid>
      <pubDate>2025-02-03 20:20</pubDate>
      <category>#deepseek</category>
      <category>#ai</category>
      <category>#visualization</category>
      <category>#generativeai</category>
      <category>#genai</category>
      <category>#llm</category>
      <category>#learning</category>
    </item>
    <item>
      <title>Introducing deep research</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openai-deep-research?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openai-deep-research&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;An agent that uses reasoning to synthesize large amounts of online information and complete multi-step research tasks for you.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Powered by a version of the upcoming OpenAI o3 model that‚Äôs optimized for web browsing and data analysis, it leverages reasoning to search, interpret, and analyze massive amounts of text, images, and PDFs on the internet, pivoting as needed in reaction to information it encounters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;How it works&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Deep research was trained using end-to-end reinforcement learning on hard browsing and reasoning tasks across a range of domains. Through that training, it learned to plan and execute a multi-step trajectory to find the data it needs, backtracking and reacting to real-time information where necessary. The model is also able to browse over user uploaded files, plot and iterate on graphs using the python tool, embed both generated graphs and images from websites in its responses, and cite specific sentences or passages from its sources. As a result of this training, it reaches new highs on a number of public evaluations focused on real-world problems.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openai-deep-research?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openai-deep-research</guid>
      <pubDate>2025-02-03 20:17</pubDate>
      <category>#ai</category>
      <category>#openai</category>
      <category>#agent</category>
      <category>#research</category>
    </item>
    <item>
      <title>A Guide to Implementing ActivityPub in a Static Site (or Any Website) - Part 8</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/guide-implementing-activitypub-static-site-pt-8?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/guide-implementing-activitypub-static-site-pt-8&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Part 8 of &lt;a href="https://www.maho.dev/2024/02/a-guide-to-implement-activitypub-in-a-static-site-or-any-website/"&gt;&amp;quot;A guide to implement ActivityPub in a static site (or any website)&amp;quot;&lt;/a&gt; by &lt;a href="https://www.maho.dev/"&gt;maho.dev&lt;/a&gt; recently dropped.&lt;/p&gt;
&lt;p&gt;This part goes over how to add replies and comments to your static site.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/guide-implementing-activitypub-static-site-pt-8?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/guide-implementing-activitypub-static-site-pt-8</guid>
      <pubDate>2025-02-02 18:23</pubDate>
      <category>#fediverse</category>
      <category>#indieweb</category>
      <category>#activitypub</category>
      <category>#personalweb</category>
      <category>#internet</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Buc-ee's plans first Wisconsin travel center</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/bucees-wisconsin?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/bucees-wisconsin&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Buc-ee's, the popular Texas-based travel center chain, is set to open its first Wisconsin location in Oak Creek, city officials announced.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Time to get my banana pudding fix.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/bucees-wisconsin?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/bucees-wisconsin</guid>
      <pubDate>2025-01-28 20:25</pubDate>
      <category>#bucees</category>
      <category>#wisconsin</category>
      <category>#travel</category>
    </item>
    <item>
      <title>Short Long Form</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/short-long-form-moreale?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/short-long-form-moreale&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This is something I've been dealing with on my site. I separate my &lt;a href="/posts/1"&gt;long-form&lt;/a&gt; from &lt;a href="/feed"&gt;short-form&lt;/a&gt; posts. However, I don't have a hard limit that clearly defines which is which. An initial reason for the separation was, I wanted to have a feed that was easy to scroll through. That was harder to do if I was rendering an entire long-form post as part of the feed. In the end though, most of my posts are all markdown files and their main distinction is how they are rendered.&lt;/p&gt;
&lt;p&gt;As I work on my website redesign, I want to change how I think about posts. I want to separate the post content from the rendering. A post is a post regardless of content and length. Regardless of whether it's a response, image, video, note, or anything in between, I don't want to constrain the content of my post. I can choose to render posts differently depending on whether it's being viewed in a feed or individual post page. The content driving those different views though shouldn't change.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/short-long-form-moreale?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/short-long-form-moreale</guid>
      <pubDate>2025-01-20 21:21</pubDate>
      <category>#instagram</category>
      <category>#posts</category>
      <category>#attention</category>
    </item>
    <item>
      <title>Semantic Search and On-Device ML in Emacs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/semantic-search-on-device-ml-emacs?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/semantic-search-on-device-ml-emacs&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This is a cool walk-through of how to add semantic search to Emacs using local ML models.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;With ONNX.el you can run any ML model inside Emacs, including ones for images, audios, etc. In case you are running embedding models, combine this with sem.el to perform semantic searches. Depending on your input modality, you might have to figure out preprocessing. For text, tokenizers.el should cover all of what you need in modern NLP for preprocessing, though you will have to do something on your own for anything non-text or multimodal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;By default sem runs a local all-MiniLM-L6-v2 model for text embeddings. This model is small and runs fast on CPU. Additionally, we load an O2 optimized variant which helps further. The vectors are stored in a lancedb database on file system which runs without a separate process. I was originally writing the vector db core myself to learn more of Rust, but then stopped doing that since lancedb gives all that I needed, out of the box.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/semantic-search-on-device-ml-emacs?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/semantic-search-on-device-ml-emacs</guid>
      <pubDate>2025-01-20 21:09</pubDate>
      <category>#onnx</category>
      <category>#emacs</category>
      <category>#ai</category>
      <category>#ml</category>
    </item>
    <item>
      <title>Digital soverignity is more important than ever</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/digital-sovereignty-white?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/digital-sovereignty-white&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...you needed to move anything you care about online to a space you control. Digital sovereignty is more important than ever.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;üíØüíØüíØüíØüíØ&lt;/p&gt;
&lt;p&gt;Best time was yesterday. Next best time is now.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/digital-sovereignty-white?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/digital-sovereignty-white</guid>
      <pubDate>2025-01-20 20:46</pubDate>
      <category>#personalweb</category>
      <category>#digitalsoverignty</category>
      <category>#indieweb</category>
    </item>
    <item>
      <title>Making space for a handmade web</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/making-space-handmade-web?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/making-space-handmade-web&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Love this article.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Before the web was more commercialized, it was personal...A website was a way to represent yourself and connect to others, a space to tend and call your very own.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The imperfect, handmade character of websites has been sanded away in favor of efficiency, and social media platforms have risen in the stead of amateur sites. Many of us conform to these containers and retreat from expressing our authentic selves publicly...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;If the idea of a more handcrafted internet resonates with you, the best way to be part of the movement is simply to make your own website. This may seem intimidating if we expect webpages to be a holistic reflection of ourselves, like a resume, portfolio, or blog. We limit the web as a medium when we expect that a website must fulfill a host of needs, or serve any function at all. &lt;strong&gt;A website doesn‚Äôt need to be anything but your own&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A more personal web becomes possible once we turn away from our preconceived notions of what a website must be. Let‚Äôs use our tools to continuously push at the boundaries of the web. Doing so will create a more creative, expansive environment that allows us to reclaim our agency and reinvigorate our collective understanding of software as an artisanal craft. Instead of concentrating on a few, monolithic sites, what if we visited many, more personalized sites? We should explore the far reaches of the web and invite our friends to join in: After all, &lt;strong&gt;we make the internet, and only we can build the web we want&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/making-space-handmade-web?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/making-space-handmade-web</guid>
      <pubDate>2025-01-18 19:25</pubDate>
      <category>#indieweb</category>
      <category>#personalweb</category>
      <category>#internet</category>
      <category>#websites</category>
      <category>#handmadeweb</category>
    </item>
    <item>
      <title>R.I.P David Lynch</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/rip-david-lynch?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/rip-david-lynch&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;David Lynch, the filmmaker celebrated for his uniquely dark and dreamlike vision in such movies as ‚ÄúBlue Velvet‚Äù and ‚ÄúMulholland Drive‚Äù and the TV series ‚ÄúTwin Peaks,‚Äù has died just days before his 79th birthday.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I didn't understand all of his work, but I appreciate his originality and overall aesthetic.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/rip-david-lynch?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/rip-david-lynch</guid>
      <pubDate>2025-01-16 19:11</pubDate>
      <category>#davidlynch</category>
      <category>#twinpeaks</category>
    </item>
    <item>
      <title>Swarm navigation of cyborg-insects in unknown obstructed soft terrain</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/swarm-cyborg-insects-agents?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/swarm-cyborg-insects-agents&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I'm going to tell future generations, this is how we built multi-agent systems back in my day.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Cyborg insects refer to hybrid robots that integrate living insects with miniature electronic controllers to enable robotic-like programmable control. These creatures exhibit advantages over conventional robots in adaption to complex terrain and sustained energy efficiency. Nevertheless, there is a lack of literature on the control of multi-cyborg systems. This research gap is due to the difficulty in coordinating the movements of a cyborg system under the presence of insects' inherent individual variability in their reactions to control input. Regarding this issue, we propose a swarm navigation algorithm and verify it under experiments. This research advances swarm robotics by integrating biological organisms with control theory to develop intelligent autonomous systems for real-world applications.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/swarm-cyborg-insects-agents?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/swarm-cyborg-insects-agents</guid>
      <pubDate>2025-01-15 21:02</pubDate>
      <category>#robotics</category>
      <category>#ai</category>
      <category>#insects</category>
      <category>#agents</category>
    </item>
    <item>
      <title>AI Subtitles Are Coming to VLC</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ai-subtitles-coming-vlc?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ai-subtitles-coming-vlc&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Shown off at CES 2025 is VLC's upcoming automatic subtitling feature, which is local in nature, running entirely on a user's machine without needing to connect to some distant server.&lt;br /&gt;
&lt;br&gt;
According to Jean-Baptiste Kempf, the creator and lead developer of VLC, this new feature is powered by open source AI models, which carry out subtitle generation tasks in two stages.&lt;br /&gt;
&lt;br&gt;
One aspect is automatic generation of subtitles from the video, and the other is translation of those subtitles into over 100 languages.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Unfortunately, there's no word on when this feature will be arriving on VLC.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ai-subtitles-coming-vlc?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ai-subtitles-coming-vlc</guid>
      <pubDate>2025-01-15 20:49</pubDate>
      <category>#vlc</category>
      <category>#ai</category>
      <category>#opensource</category>
      <category>#local</category>
    </item>
    <item>
      <title>The Future of Calm Technology with Amber Case</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/calm-tech-interview?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/calm-tech-interview&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Interesting interview and introduction to &lt;a href="https://calmtech.com/"&gt;Calm Technology&lt;/a&gt; and the &lt;a href="https://www.calmtech.institute/"&gt;Calm Tech Institute&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/calm-tech-interview?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/calm-tech-interview</guid>
      <pubDate>2025-01-14 21:13</pubDate>
      <category>#digitalminimalism</category>
      <category>#technology</category>
    </item>
    <item>
      <title>Stop renting space for your notes and start being your own landlord</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/landlord-of-your-notes?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/landlord-of-your-notes&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;When you choose to keep your notes in locally stored plain text files, you‚Äôre choosing to be your own landlord. Keeping notes in a proprietary app is the same as renting: the land you‚Äôre building on belongs to someone else, with no guarantee of continuing access.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is so true for many other areas of our digital lives as well.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/landlord-of-your-notes?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/landlord-of-your-notes</guid>
      <pubDate>2025-01-13 21:54</pubDate>
      <category>#localfirst</category>
      <category>#notetaking</category>
      <category>#pkm</category>
    </item>
    <item>
      <title>A Year of Curiosity</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/year-of-curiosity?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/year-of-curiosity&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;When we encounter something new and interesting, our brains release dopamine ‚Äì the same neurotransmitter associated with reward and pleasure. This creates a simple pattern: the more we learn, the more we want to learn.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This drive to explore and understand isn‚Äôt just nice to have ‚Äì it‚Äôs essential to who we are as humans.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;There‚Äôs no single ‚Äúright‚Äù way to be curious. Learning practical skills like coding or cooking, diving into topics like history or science, joining groups of people who share your interests‚Ä¶ These are all great ways to inject more curiosity into your life.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Designing a year of curiosity&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Monthly:&lt;/strong&gt; Design one tiny experiment at the beginning of each month This could be as simple as exploring a topic you know nothing about, trying a new hobby, or doing something that pushes you out of your comfort zone...&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weekly:&lt;/strong&gt; Every week, take 10 to 15 minutes to conduct a weekly review. What went well this week? What didn‚Äôt go as planned? What will you focus on next week? This could mean doubling down on what worked or tweaking something that didn‚Äôt...&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Daily:&lt;/strong&gt; Create at least one moment of curiosity in your day, no matter how small. Experiment with a new recipe or tool, have one meaningful conversation, try a journaling prompt, or take a different route to work. Even one minute of curiosity a day can add up to a much richer life.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/year-of-curiosity?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/year-of-curiosity</guid>
      <pubDate>2025-01-08 21:27</pubDate>
      <category>#innovation</category>
      <category>#exploration</category>
      <category>#curiosity</category>
      <category>#growth</category>
    </item>
    <item>
      <title>Agents</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/agents-huyen?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/agents-huyen&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The unprecedented capabilities of foundation models have opened the door to agentic applications that were previously unimaginable. These new capabilities make it finally possible to develop autonomous, intelligent agents to act as our assistants, coworkers, and coaches. They can help us create a website, gather data, plan a trip, do market research, manage a customer account, automate data entry, prepare us for interviews, interview our candidates, negotiate a deal, etc. The possibilities seem endless, and the potential economic value of these agents is enormous.&lt;br /&gt;
&lt;br&gt;
This section will start with an overview of agents and then continue with two aspects that determine the capabilities of an agent: tools and planning. Agents, with their new modes of operations, have new modes of failure. This section will end with a discussion on how to evaluate agents to catch these failures.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/agents-huyen?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/agents-huyen</guid>
      <pubDate>2025-01-07 11:23</pubDate>
      <category>#ai</category>
      <category>#agents</category>
    </item>
    <item>
      <title>Agents Whitepaper</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/kaggle-agents-whitepaper?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/kaggle-agents-whitepaper&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Humans are fantastic at messy pattern recognition tasks. However, they often rely on tools - like books, Google Search, or a calculator - to supplement their prior knowledge before arriving at a conclusion. Just like humans, Generative AI models can be trained to use tools to access real-time information or suggest a real-world action. For example, a model can leverage a database retrieval tool to access specific information, like a customer's purchase history, so it can generate tailored shopping recommendations. Alternatively, based on a user's query, a model can make various API calls to send an email response to a colleague or complete a financial transaction on your behalf. To do so, the model must not only have access to a set of external tools, it needs the ability to plan and execute any task in a self- directed fashion. This combination of reasoning, logic, and access to external information that are all connected to a Generative AI model invokes the concept of an agent, or a program that extends beyond the standalone capabilities of a Generative AI model. This whitepaper dives into all these and associated aspects in more detail.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/kaggle-agents-whitepaper?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/kaggle-agents-whitepaper</guid>
      <pubDate>2025-01-06 21:50</pubDate>
      <category>#agents</category>
      <category>#ai</category>
      <category>#whitepaper</category>
    </item>
    <item>
      <title>Welcome to the Public Domain in 2025</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/internet-archive-public-domain-2025?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/internet-archive-public-domain-2025&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;On January 1, 2025, we celebrate published works from 1929 and published sound recordings from 1924 entering the public domain! The passage of these works into the public domain celebrates our shared cultural heritage. The ability to breathe new life into long forgotten works, remix the most popular and enduring works of the time, and to better circulate the oddities we find in thrift stores, attics, and on random pockets of the internet are now freely available for us all.&lt;br /&gt;
&lt;br&gt;
While not at the same blockbuster level as 2024 with Steamboat Willie‚Äôs passage into the public domain, works from 1929 still inhabit strong cultural significance today. The works of 1929 continue to capture the Lost Generation‚Äôs voice, the rise of sound film, and the emerging modern moment of the 1920s.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/internet-archive-public-domain-2025?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/internet-archive-public-domain-2025</guid>
      <pubDate>2025-01-03 15:53</pubDate>
      <category>#internetarchive</category>
      <category>#publicdomain</category>
    </item>
    <item>
      <title>LangChain State of AI 2024 Report</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/langchain-state-of-ai-2024?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/langchain-state-of-ai-2024&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In 2024, developers leaned into complexity with multi-step agents, sharpened efficiency by doing more with fewer LLM calls, and added quality checks to their apps using methods of feedback and evaluation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://blog.langchain.dev/content/images/size/w1600/2024/12/Top-10-LLM-Providers-bar-chart.png" class="img-fluid" alt="Bar Chart Showing Top 10 LLMs Used 2024" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;LangChain Blog&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://blog.langchain.dev/content/images/size/w1600/2024/12/Top-10-Retrievers--1-.png" class="img-fluid" alt="Graphic showing Top 10 Retrievers / Vector Stores 2024" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;LangChain Blog&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://blog.langchain.dev/content/images/size/w1600/2024/12/Top-evaluation-metrics--1-.png" class="img-fluid" alt="Graphic showing Top LLM Evaluation Metrics 2024" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;LangChain Blog&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/langchain-state-of-ai-2024?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/langchain-state-of-ai-2024</guid>
      <pubDate>2024-12-29 12:12</pubDate>
      <category>#ai</category>
      <category>#2024</category>
      <category>#report</category>
      <category>#langchain</category>
      <category>#llm</category>
    </item>
    <item>
      <title>MarkItDown - Convert files to Markdown</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/microsoft-markitdown?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/microsoft-markitdown&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;MarkItDown is a utility for converting various files to Markdown (e.g., for indexing, text analysis, etc). It supports:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PDF&lt;/li&gt;
&lt;li&gt;PowerPoint&lt;/li&gt;
&lt;li&gt;Word&lt;/li&gt;
&lt;li&gt;Excel&lt;/li&gt;
&lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt;
&lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt;
&lt;li&gt;HTML&lt;/li&gt;
&lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt;
&lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/microsoft-markitdown?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/microsoft-markitdown</guid>
      <pubDate>2024-12-18 21:43</pubDate>
      <category>#markdown</category>
      <category>#etl</category>
      <category>#data</category>
      <category>#python</category>
      <category>#ai</category>
      <category>#microsoft</category>
    </item>
    <item>
      <title>Agentic Fediverse</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/agentic-fediverse?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/agentic-fediverse&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The &amp;quot;Agentic Fediverse&amp;quot; is the idea of a new kind of network federation, a complementary iteration on the concept of federation currently established with ActivityPub. It's an experiment and something that we in Muni Town are still trying to define more concretely.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Tenets&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are still working on defining the core tenets of the agentic fediverse, but here is what we have so far.&lt;br /&gt;
&lt;br&gt;
An agentic fediverse is...&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A fediverse of agents; agent-centric, as opposed to server-centric.&lt;/li&gt;
&lt;li&gt;Local-first; solve local problems for local people.&lt;/li&gt;
&lt;li&gt;Accessible by default; inaccessibility is a bug.&lt;/li&gt;
&lt;li&gt;Systematically consensual; architected on the basis of informed consent.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/agentic-fediverse?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/agentic-fediverse</guid>
      <pubDate>2024-12-17 19:44</pubDate>
      <category>#fediverse</category>
      <category>#munitown</category>
      <category>#agents</category>
      <category>#agentic</category>
      <category>#activitypub</category>
      <category>#protocols</category>
      <category>#socialweb</category>
    </item>
    <item>
      <title>A New Social Web - Hello, Social Web</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/hello-social-web?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/hello-social-web&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We're A New Social, a new non-profit organization focused on building cross-protocol tools and services for the open social web.&lt;br /&gt;
&lt;br&gt;
Our mission is to liberate people's networks from their platforms, enabling &lt;a href="https://www.augment.ink/bridges-the-last-network-effect/?ref=anew.social"&gt;The Last Network Effect&lt;/a&gt; and leveling the playing field across the open social web.&lt;br /&gt;
&lt;br&gt;
The first project we'll take on to accomplish this mission is &lt;a href="https://fed.brid.gy/?ref=anew.social"&gt;Bridgy Fed&lt;/a&gt;, a service that enables users of ActivityPub-based platforms like Mastodon, ATProto-based platforms like Bluesky, and websites to interact and engage across ecosystems.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/hello-social-web?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/hello-social-web</guid>
      <pubDate>2024-12-17 19:39</pubDate>
      <category>#anewsocial</category>
      <category>#fediverse</category>
      <category>#socialmedia</category>
      <category>#protocols</category>
      <category>#atproto</category>
      <category>#activitypub</category>
      <category>#socialweb</category>
      <category>#personalweb</category>
    </item>
    <item>
      <title>StreamBuilder: our open-source framework for powering your dashboard</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/streambuilder-tumblr-framework-building-feed?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/streambuilder-tumblr-framework-building-feed&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Interesting post. Whether building your feeds or plugging into existing platforms like &lt;a href="https://bsky.social/about/blog/3-30-2023-algorithmic-choice"&gt;Bluesky&lt;/a&gt;, this framework could serve as a good starting point.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we‚Äôre abnormally jazzed to announce that we‚Äôre open-sourcing the custom framework we built to power your dashboard on Tumblr. We call it StreamBuilder, and we‚Äôve been using it for many years.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;StreamBuilder has a lot going on. The primary architecture centers around ‚Äústreams‚Äù of content: whether posts from a blog, a list of blogs you‚Äôre following, posts using a specific tag, or posts relating to a search. These are separate kinds of streams, which can be mixed together, filtered based on certain criteria, ranked for relevancy or engagement likelihood, and more.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;So, what‚Äôs included in the box?&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The full framework library of code that we use today, on Tumblr, to power almost every feed of content you see on the platform.&lt;/li&gt;
&lt;li&gt;A YAML syntax for composing streams of content, and how to filter, inject, and rank them.&lt;/li&gt;
&lt;li&gt;Abstractions for programmatically composing, filtering, ranking, injecting, and debugging streams.&lt;/li&gt;
&lt;li&gt;Abstractions for composing streams together‚Äîsuch as with carousels, for streams-within-streams.&lt;/li&gt;
&lt;li&gt;An abstraction for cursor-based pagination for complex stream templates.&lt;/li&gt;
&lt;li&gt;Unit tests covering the public interface for the library and most of the underlying code.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/Automattic/stream-builder"&gt;GitHub Repo&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/streambuilder-tumblr-framework-building-feed?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/streambuilder-tumblr-framework-building-feed</guid>
      <pubDate>2024-12-17 19:03</pubDate>
      <category>#tumblr</category>
      <category>#algorithms</category>
      <category>#feed</category>
      <category>#socialmedia</category>
    </item>
    <item>
      <title>Openverse</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openverse?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openverse&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Openverse is a tool that allows openly licensed and public domain works to be discovered and used by everyone.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Openverse searches across more than 800 million images and audio tracks from open APIs and the Common Crawl dataset. We aggregate works from multiple public repositories, and facilitate reuse through features like one-click attribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Openverse is the successor to CC Search which was launched by Creative Commons in 2019&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openverse?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openverse</guid>
      <pubDate>2024-12-17 16:53</pubDate>
      <category>#openverse</category>
      <category>#opensource</category>
      <category>#creativecommons</category>
    </item>
    <item>
      <title>State of the Word 2024</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/state-of-the-word-2024-wordpress?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/state-of-the-word-2024-wordpress&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Cool retrospective and announcements from this WordPress' State of the Word 2024.&lt;/p&gt;
&lt;p&gt;Focused mode and templates look interesting.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=KLybH5YvIPQ" title="State of the Word 2024 YouTube Thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/KLybH5YvIPQ/0.jpg" class="img-fluid" alt="State of the Word 2024 YouTube Thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/state-of-the-word-2024-wordpress?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/state-of-the-word-2024-wordpress</guid>
      <pubDate>2024-12-17 16:50</pubDate>
      <category>#wordpress</category>
      <category>#conference</category>
      <category>#blogging</category>
      <category>#website</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Tumblr adds Reddit-like ‚Äòcommunities‚Äô</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tumblr-adding-reddit-like-communities?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tumblr-adding-reddit-like-communities&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Tumblr is introducing a new Community feature ‚Äî in-app groups organized by topic or interest.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Communities are similar to subreddits or Facebook groups and had previously been in beta.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Given recent announcements like &lt;a href="/feed/shipping-wordpress-tumblr"&gt;Tumblr replatforming on top of WordPress&lt;/a&gt; and &lt;a href="/feed/wordpress-activitypub-plugin"&gt;WordPress adding support for ActivityPub&lt;/a&gt;, I could see communities becoming not only an alternative to Reddit, but also Lemmy in the Fediverse.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tumblr-adding-reddit-like-communities?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tumblr-adding-reddit-like-communities</guid>
      <pubDate>2024-12-16 22:00</pubDate>
      <category>#tumblr</category>
      <category>#reddit</category>
      <category>#communities</category>
      <category>#automattic</category>
      <category>#wordpress</category>
      <category>#activitypub</category>
      <category>#fediverse</category>
      <category>#lemmy</category>
    </item>
    <item>
      <title>Build a YouTube chat app with .NET</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/youtube-chat-app-ai-dotnet?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/youtube-chat-app-ai-dotnet&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Great post from &lt;a href="https://jordanmatthiesen.me/"&gt;jordanmatthiesen.me&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Recently on a trip for a tech conference I created a YouTube chat app using .NET and AI. This is part of my exploration into creating a larger app for chatting about .NET AI development (leveraging docs, presentations, and sample code my team has been working on at Microsoft).&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/youtube-chat-app-ai-dotnet?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/youtube-chat-app-ai-dotnet</guid>
      <pubDate>2024-12-16 21:22</pubDate>
      <category>#dotnet</category>
      <category>#ai</category>
      <category>#youtube</category>
      <category>#chat</category>
      <category>#microsoftextensionsai</category>
      <category>#openai</category>
    </item>
    <item>
      <title>Sora is here</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/sora-released?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/sora-released&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our video generation model is rolling out at &lt;a href="https://sora.com"&gt;sora.com‚Å†&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Users can generate videos up to 1080p resolution, up to 20 sec long, and in widescreen, vertical or square aspect ratios. You can bring your own assets to extend, remix, and blend, or generate entirely new content from text.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The version of Sora we are deploying has many limitations. It often generates unrealistic physics and struggles with complex actions over long durations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôre introducing our video generation technology now to give society time to explore its possibilities and co-develop norms and safeguards that ensure it‚Äôs used responsibly as the field advances.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/sora-released?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/sora-released</guid>
      <pubDate>2024-12-09 21:34</pubDate>
      <category>#sora</category>
      <category>#openai</category>
      <category>#ai</category>
    </item>
    <item>
      <title>How I turned my blog into a social media hub</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/how-i-turned-website-social-media-hub-newman?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/how-i-turned-website-social-media-hub-newman&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...my blog has become a central hub for almost everything I write online, including nearly all my social media posts, links to my published work on other sites, and the occasional freestanding blog post. While this approach has a few trade-offs, I feel better about posting in the first place knowing that I‚Äôm doing it on my own terms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;There‚Äôs also just something deeply satisfying about seeing a running feed of my own writing, on my own website, with my own design and presentation. A social network alone can‚Äôt replicate that, but the POSSE approach offers the best of both worlds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I've enjoyed seeing over the past few weeks, as people seek new online platforms, the personal website is being rediscovered.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/how-i-turned-website-social-media-hub-newman?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/how-i-turned-website-social-media-hub-newman</guid>
      <pubDate>2024-12-05 19:22</pubDate>
      <category>#indieweb</category>
      <category>#social</category>
      <category>#microblog</category>
      <category>#blogging</category>
      <category>#internet</category>
      <category>#web</category>
      <category>#socialmedia</category>
    </item>
    <item>
      <title>Jimmy The Gambler</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/jimmy-gambler-psu?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/jimmy-gambler-psu&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;üòÇ üòÇ üòÇ&lt;/p&gt;
&lt;p&gt;I don't think I've ever heard James Franklin called a gambler.&lt;/p&gt;
&lt;p&gt;The game against Minnesota was too close for comfort, but a win is a win. In many ways, thanks to Jimmy, The Gambler.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/jimmy-gambler-psu?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/jimmy-gambler-psu</guid>
      <pubDate>2024-11-24 19:36</pubDate>
      <category>#pennstate</category>
      <category>#ncaaf</category>
      <category>#collegefootball</category>
      <category>#psu</category>
    </item>
    <item>
      <title>Implement ActivityPub on a static site series by maho.dev</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/implement-activitypub-static-site-series-maho?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/implement-activitypub-static-site-series-maho&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I've been following along with this amazing series from &lt;a href="https://maho.dev/"&gt;Maho&lt;/a&gt; on implementing ActivityPub on static websites.&lt;/p&gt;
&lt;p&gt;As I think about what I want out of my website and the way I engage in the Fediverse, there's a lot of overlap.&lt;/p&gt;
&lt;p&gt;Today, what I mainly use my Mastodon instance for is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Having a Fediverse presence on a self-hosted Mastodon instance.&lt;/li&gt;
&lt;li&gt;&lt;a href="/posts/rss-to-mastodon-posse-azure-logic-apps/"&gt;Cross-posting posts on by website using the POSSE pattern&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Although I have learned a ton from self-hosting my own Mastodon instance, neither of the points listed above require me self-hosting or even having an account on someone else's instance.&lt;/p&gt;
&lt;p&gt;Yesterday, &lt;a href="/feed/using-domain-mastodon-discovery/"&gt;I took the first step in linking my Fediverse presence to my domain&lt;/a&gt;. That fulfills my first requirement.&lt;/p&gt;
&lt;p&gt;I rarely post original content on Mastodon and for consumption, &lt;a href="/feed/subscribed-to-1042-feeds-newsblur"&gt;I already subscribe to accounts and tags via RSS&lt;/a&gt;. Therefore, the second requirement is one that can naturally occur without having to use a Mastodon instance as an intermediary. I can just post on my website and because my posts can show up in my outbox, there's no need to POSSE. I don't want my presence to be limited to Mastodon though, but since I plan on supporting media, reviews, and other types of posts, I expect my posts to be accessible across other platforms on the Fediverse like Pixelfed, Bookwyrm, and many others.&lt;/p&gt;
&lt;p&gt;Since my website and website features are built using .NET, Maho's guide simplifies my implementation. Because &lt;a href="/colophon"&gt;I'm not using any of the existing static site generators and rolled my own&lt;/a&gt;, that means that there's going to be some effort and customizations required on my end. It would no different from my &lt;a href="/posts/receive-webmentions-fsharp-az-functions-fsadvent/"&gt;custom Webmentions implementation&lt;/a&gt; though. My hope is that I will save time I often spend maintaining the server as well as money since I no longer need to rent a server to host my instance. At the same time, I get to learn and contribute to building a more open, decentralized, and personal web.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/implement-activitypub-static-site-series-maho?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/implement-activitypub-static-site-series-maho</guid>
      <pubDate>2024-11-19 10:03</pubDate>
      <category>#acvititypub</category>
      <category>#fediverse</category>
      <category>#staticsite</category>
      <category>#blogging</category>
      <category>#indieweb</category>
      <category>#openweb</category>
      <category>#personalweb</category>
      <category>#dotnet</category>
      <category>#internet</category>
    </item>
    <item>
      <title>Digital minimalism: A moral duty?</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/digital-minimalism-moral-duty-deep-questions?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/digital-minimalism-moral-duty-deep-questions&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I saw a reference to this paper yesterday and added it to my Read-It-Later list.&lt;/p&gt;
&lt;p&gt;Here's a link to the original paper: &lt;a href="https://philpapers.org/archive/AYLITA.pdf"&gt;https://philpapers.org/archive/AYLITA.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Chances are, I wasnt going to get to it until way later with the backlog of other things I want to read later.&lt;/p&gt;
&lt;p&gt;Lucky for me, that was the subject of the latest Deep Questions episode. Cal does a nice job breaking down the main arguments of the paper.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=KK9ktzbBDPE" title="A thumbnail of Deep Questions Podcast Episode on Digital Minimalism"&gt;&lt;img src="http://img.youtube.com/vi/KK9ktzbBDPE/0.jpg" class="img-fluid" alt="A thumbnail of Deep Questions Podcast Episode on Digital Minimalism" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The main takeaway for me was, technology is a great tool. However, when it robs you of your rational decision making abilities and pushes you to compulsive behaviors that move you away from the habits and practices that fuel your growth, it's a problem. For example, lets say you want to build a habit of reading or working out. Unfortunately, when you &lt;INSERT TIME SUCKING DISTRACTION OF CHOICE&gt;, time, attention, and energy that would've otherwise been spent on those habits is now diverted towards the distraction. That is irrational and goes against your desire to grow, yet making the rational choice is where most of us struggle. Although the paper focuses on our digital lives, I can see other ways similar arguments can be applied.&lt;/p&gt;
&lt;p&gt;I've also included the audio version as well: &lt;a href="https://www.buzzsprout.com/1121972/episodes/16124495-ep-327-would-kant-use-tiktok"&gt;https://www.buzzsprout.com/1121972/episodes/16124495-ep-327-would-kant-use-tiktok&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/digital-minimalism-moral-duty-deep-questions?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/digital-minimalism-moral-duty-deep-questions</guid>
      <pubDate>2024-11-19 09:56</pubDate>
      <category>#digitalminimalism</category>
      <category>#postcast</category>
      <category>#deepquestions</category>
      <category>#philosophy</category>
      <category>#calnewport</category>
    </item>
    <item>
      <title>Why YOU Should Make A Website</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/why-you-should-make-a-website-luvstarkei?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/why-you-should-make-a-website-luvstarkei&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This video showed up in my feed yesterday. &lt;a href="https://luvstarkei.com/"&gt;LuvstarKei&lt;/a&gt; makes some great points for building your own space on the web.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=uNlZ50b6wSs" title="LuvstarKei Thumbnail Why You Should Make A Website YouTube Video"&gt;&lt;img src="http://img.youtube.com/vi/uNlZ50b6wSs/0.jpg" class="img-fluid" alt="LuvstarKei Thumbnail Why You Should Make A Website YouTube Video" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With the most recent Twitter / X exodus, it's great to see folks moving to places built on more open protocols like AT (Bluesky) or ActivityPub (Fediverse).&lt;/p&gt;
&lt;p&gt;That said, those platforms are not your own, even in cases where you self-host. If tomorrow Threads, Bluesky, or Mastodon stop hosting your instance or stop development altogether, what happens to the content and your community? In the case of the Fediverse, many projects are being built in the open and there are other providers you could switch to. The switching costs though may not make it worth bothering to export and keep your content as is usually the case. That's not to say with a personal website you might not have similar issues (i.e. switching from WordPress to Ghost). However, HTML is HTML and you can rehost that anywhere on the internet.&lt;/p&gt;
&lt;p&gt;Everyone is looking for different things from the social platforms, and that's not to say they don't have their value. However, from an ownership, permanence, and creative freedom perspective, nothing beats having your own website.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/why-you-should-make-a-website-luvstarkei?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/why-you-should-make-a-website-luvstarkei</guid>
      <pubDate>2024-11-16 18:08</pubDate>
      <category>#website</category>
      <category>#indieweb</category>
      <category>#socialmedia</category>
      <category>#personalweb</category>
    </item>
    <item>
      <title>.NET Conf 2024 - Session Recordings</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/dotnet-conf-2024-session-playlist?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/dotnet-conf-2024-session-playlist&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;The recordings are out for .NET Conf 2024 in case you missed any of the sessions.&lt;/p&gt;
&lt;p&gt;Here are the recordings from the sessions I participated in.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://youtu.be/7Rw_ciSh2Wk"&gt;Building AI Applications From Scratch: A Hands-On Guide for .NET Developers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://youtu.be/MdM8G2e7jOg"&gt;Building the Foundation: AI Fundamentals in .NET&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, there's a &lt;a href="https://www.youtube.com/playlist?list=PLdo4fOcmZ0oXU8g5XIKiOPX4tgIqG2Ddc"&gt;Premier Bonus playlist&lt;/a&gt; which has a ton of amazing bonus content.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/dotnet-conf-2024-session-playlist?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/dotnet-conf-2024-session-playlist</guid>
      <pubDate>2024-11-15 11:52 -05:00</pubDate>
      <category>#dotnet</category>
      <category>#dotnetconf</category>
      <category>#net9</category>
      <category>#dotnet9</category>
      <category>#programming</category>
      <category>#dev</category>
    </item>
    <item>
      <title>Thundercat on Tyler's Chromakopia album?</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/thundercat-chromakopia?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/thundercat-chromakopia&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Love this new &amp;quot;making of&amp;quot; video. I was surprised to learn Thundercat is on the new album. I didn't catch which track it's on, but will be listening for it.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=KvR_QlXaAfU" title="Mask Is Off Video Thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/KvR_QlXaAfU/0.jpg" class="img-fluid" alt="Mask Is Off Video Thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/thundercat-chromakopia?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/thundercat-chromakopia</guid>
      <pubDate>2024-11-13 02:11</pubDate>
      <category>#tylerthecreator</category>
      <category>#chromakopia</category>
      <category>#thundercat</category>
      <category>#music</category>
    </item>
    <item>
      <title>Day 1 of .NET Conf</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/dotnetconf-2024-day-1-success?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/dotnetconf-2024-day-1-success&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Day 1 of .NET Conf was a lot of fun. Tons of great sessions. Here's the recording of all the sessions.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=hM4ifrqF_lQ" title=".NET Conf Day 1 LiveStream"&gt;&lt;img src="http://img.youtube.com/vi/hM4ifrqF_lQ/0.jpg" class="img-fluid" alt=".NET Conf Day 1 LiveStream" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you're interested in checking out my session with Jeremy, Building AI Applications From Scratch, here's a link to it.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/live/hM4ifrqF_lQt=23407" title=".NET Conf Day 1 LiveStream Countdown"&gt;&lt;img src="http://img.youtube.com/vi/hM4ifrqF_lQ/1.jpg" class="img-fluid" alt=".NET Conf Day 1 LiveStream Countdown" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tomorrow, I'll also get a chance to share the stage with Tarek and Tanner to talk &lt;a href="https://www.dotnetconf.net/agenda#:%7E:text=Building%20the%20Foundation:%20AI%20Fundamentals%20in%20.NET"&gt;AI Fundamentals&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/dotnetconf-2024-day-1-success?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/dotnetconf-2024-day-1-success</guid>
      <pubDate>2024-11-13 01:47</pubDate>
      <category>#dotnet</category>
      <category>#dotnetconf</category>
      <category>#dotnet9</category>
      <category>#net9</category>
      <category>#conference</category>
      <category>#ai</category>
      <category>#aspire</category>
      <category>#microsoft</category>
    </item>
    <item>
      <title>Leo.fm move to micro.blog</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/leo-fm-moves-to-microblog?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/leo-fm-moves-to-microblog&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;So cool to see you posting more on your own platform.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/leo-fm-moves-to-microblog?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/leo-fm-moves-to-microblog</guid>
      <pubDate>2024-11-10 14:27 -05:00</pubDate>
      <category>#indieweb</category>
      <category>#microblog</category>
      <category>#twit</category>
    </item>
    <item>
      <title>Microsoft Research: Introducing DRIFT Search</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-drift-search?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-drift-search&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h2&gt;Introducing DRIFT Search: Combining global and local search methods to improve quality and efficiency&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;DRIFT search (Dynamic Reasoning and Inference with Flexible Traversal)...builds upon Microsoft‚Äôs GraphRAG technique, combining characteristics of both global and local search to generate detailed responses in a method that balances computational costs with quality outcomes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;DRIFT Search: A step-by-step process&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Primer&lt;/strong&gt;: When a user submits a query, DRIFT compares it to the top K most semantically relevant community reports. This generates an initial answer along with several follow-up questions, which act as a lighter version of global search. To do this, we expand the query using Hypothetical Document Embeddings (HyDE), to increase sensitivity (recall), embed the query, look up the query against all community reports, select the top K and then use the top K to try to answer the query. The aim is to leverage high-level abstractions to guide further exploration.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Follow-Up&lt;/strong&gt;: With the primer in place, DRIFT executes each follow-up using a local search variant. This yields additional intermediate answers and follow-up questions, creating a loop of refinement that continues until the search engine meets its termination criteria, which is currently configured for two iterations (further research will investigate reward functions to guide terminations). This phase represents a globally informed query refinement. Using global data structures, DRIFT navigates toward specific, relevant information within the knowledge graph even when the initial query diverges from the indexing persona. This follow-up process enables DRIFT to adjust its approach based on emerging information.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output Hierarchy&lt;/strong&gt;: The final output is a hierarchy of questions and answers ranked on their relevance to the original query. This hierarchical structure can be customized to fit specific user needs. During benchmark testing, a naive map-reduce approach aggregated all intermediate answers, with each answer weighted equally.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-drift-search?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-drift-search</guid>
      <pubDate>2024-11-04 21:59 -05:00</pubDate>
      <category>#ai</category>
      <category>#search</category>
      <category>#rag</category>
      <category>#llm</category>
      <category>#microsoft</category>
      <category>#research</category>
      <category>#msr</category>
      <category>#graphrag</category>
    </item>
    <item>
      <title>Microsoft Research Focus: Week of October 28, 2024</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/microsoft-research-focus-oct-28-2024?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/microsoft-research-focus-oct-28-2024&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h2&gt;METAREFLECTION: Learning Instructions for Language Agents using Past Reflections&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Language agents are AI systems that can understand, reason and respond in natural language to complete various tasks. While the latest LLMs are capable enough to power reasonably good language agents, the closed-API model makes it hard to improve them when they perform sub-optimally. Recent studies have explored using techniques like self-reflection and prompt optimization to improve performance. Unfortunately, self-reflection can be used only during the agent‚Äôs current run, while contemporary prompt optimization techniques are designed and tested to work on simple single-step agents.&lt;/p&gt;
&lt;p&gt;In a recent paper: METAREFLECTION: Learning Instructions for Language Agents using Past Reflections, researchers from Microsoft introduce a novel offline reinforcement learning technique that enhances the performance of language agents by augmenting a semantic memory based on experiential learnings from past trials. They demonstrate the efficacy of METAREFLECTION across multiple domains, including complex logical reasoning, biomedical semantic similarity, open world question answering, and vulnerability threat detection, in Infrastructure-as-Code, spanning different agent designs. METAREFLECTION boosts language agents‚Äô performance by 4% to 16.82% over the baseline agent implementations and performs on par with existing state-of-the-art prompt optimization techniques while requiring fewer LLM calls.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Generative AI applications rely on large, foundation models, particularly LLMs. LLMs often have tens to hundreds of billions of parameters, making them too large for a single graphics processing unit (GPU) to handle in terms of both memory and computation. Because of their size, training these models requires distributing the workload across hundreds or even thousands of GPUs. This can lead to significant communication overhead, a challenge that arises when data needs to be shared between different GPUs.&lt;/p&gt;
&lt;p&gt;In a recent paper: Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping, researchers from Microsoft introduce a system designed to enhance the efficiency of LLM training by reducing the time lost to communication between GPUs.&lt;/p&gt;
&lt;p&gt;Domino breaks down data dependencies in a single batch of training into smaller, independent pieces. These smaller pieces are processed in parallel, and communication between GPUs happens simultaneously with computation, minimizing delays.&lt;/p&gt;
&lt;p&gt;Test results comparing Domino to Megatron-LM show that Domino speeds up the training process by up to 1.3x on Nvidia DGX-H100 GPUs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;OmniParser for pure vision-based GUI agent&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large vision-language models (VLMs) such as GPT-4V and GPT-4o show promise in driving intelligent agent systems that operate within user interfaces (UI). However, VLMs‚Äô full potential remains underexplored in real-world applications, particularly when it comes to acting as general agents across diverse operating systems and applications with only vision input. One limiting factor is the absence of a robust technique for screen parsing which is capable of 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associating the intended action with the corresponding region on the screen.&lt;/p&gt;
&lt;p&gt;In a recent article: OmniParser for pure vision-based GUI agent, researchers from Microsoft present a compact screen parsing module that can convert UI screenshots into structured elements. OmniParser can be used with a variety of models to create agents capable of taking actions on UIs. When used with GPT-4V, OmniParser significantly improves the agent capability to generate precisely grounded actions for interface regions.&lt;/p&gt;
&lt;p&gt;OmniParser with GPT-4V agent achieved the best performance on the recently released ‚ÄØWindowsAgentArena (opens in new tab)‚ÄØbenchmark.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/microsoft-research-focus-oct-28-2024?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/microsoft-research-focus-oct-28-2024</guid>
      <pubDate>2024-11-04 21:39 -05:00</pubDate>
      <category>#ai</category>
      <category>#microsoft</category>
      <category>#research</category>
      <category>#msr</category>
      <category>#microsoftresearch</category>
    </item>
    <item>
      <title>In-Context LoRA for Diffusion Transformers</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/in-context-lora?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/in-context-lora&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Recent research arXiv:2410.15027 has explored the use of diffusion transformers (DiTs) for task-agnostic image generation by simply concatenating attention tokens across images. However, despite substantial computational resources, the fidelity of the generated images remains suboptimal. In this study, we reevaluate and streamline this framework by hypothesizing that text-to-image DiTs inherently possess in-context generation capabilities, requiring only minimal tuning to activate them. Through diverse task experiments, we qualitatively demonstrate that existing text-to-image DiTs can effectively perform in-context generation without any tuning. Building on this insight, we propose a remarkably simple pipeline to leverage the in-context abilities of DiTs: (1) concatenate images instead of tokens, (2) perform joint captioning of multiple images, and (3) apply task-specific LoRA tuning using small datasets (e.g., 20‚àº100 samples) instead of full-parameter tuning with large datasets. We name our models In-Context LoRA (IC-LoRA). This approach requires no modifications to the original DiT models, only changes to the training data. Remarkably, our pipeline generates high-fidelity image sets that better adhere to prompts. While task-specific in terms of tuning data, our framework remains task-agnostic in architecture and pipeline, offering a powerful tool for the community and providing valuable insights for further research on product-level task-agnostic generation systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Repo: https://github.com/ali-vilab/In-Context-LoRA&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/in-context-lora?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/in-context-lora</guid>
      <pubDate>2024-11-04 21:28 -05:00</pubDate>
      <category>#ai</category>
      <category>#transformers</category>
      <category>#diffusers</category>
      <category>#imagegeneration</category>
      <category>#lora</category>
      <category>#finetuning</category>
    </item>
    <item>
      <title>Thinking LLMs: General Instruction Following with Thought Generation</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/thinking-llms-instruction-following-thought-generation?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/thinking-llms-instruction-following-thought-generation&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. We propose a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. We show that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning &amp;amp; problem-solving tasks.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/thinking-llms-instruction-following-thought-generation?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/thinking-llms-instruction-following-thought-generation</guid>
      <pubDate>2024-11-04 21:24 -05:00</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#research</category>
      <category>#meta</category>
      <category>#chainofthought</category>
      <category>#training</category>
      <category>#finetuning</category>
    </item>
    <item>
      <title>Haves and Choices</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/haves-choices-brown-nielsen?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/haves-choices-brown-nielsen&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The ‚Äúhave to‚Äù narrative positions us as repositories of instructions made elsewhere, as if we were just programs following the code we‚Äôve been given...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The ‚Äúchoose to‚Äù narrative has no illusions about our power and recognizes that we are small players in a bigger, and certainly unjust, world. But we are not machines. And maybe we don‚Äôt like the choices available to us, maybe we wish there were others within reach. But once we accept that there are choices to make, we may notice where we have some room to maneuver, some space to play with, some opportunity or avenue or loophole we can exploit.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Refusing your own agency time and again is like disconnecting from a power source‚Äîthe energy is still there, latent and ready, but the plug dangles inches from the outlet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...accepting that you have some agency might hurt: you bump up against the systems that constrain your choices; you see more clearly how other people‚Äôs choices limit (or expand) your own. But it keeps you connected to that source, that font of energy that is yours and no one else‚Äôs. It keeps you hooked up to who you are, and to what you want.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Interesting excerpts from &lt;a href="https://everythingchanges.us/blog/haves-and-choices/"&gt;Mandy Brown's &amp;quot;Haves and Choices&amp;quot; article&lt;/a&gt; to think about.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/haves-choices-brown-nielsen?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/haves-choices-brown-nielsen</guid>
      <pubDate>2024-11-01 09:37</pubDate>
      <category>#life</category>
      <category>#thoughts</category>
    </item>
    <item>
      <title>NotebookLlama: An Open Source version of NotebookLM</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/notebook-llama-oss-notebook-lm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/notebook-llama-oss-notebook-lm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This is cool! Already looking at how I can repurpose this sample for a project I have in mind.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/notebook-llama-oss-notebook-lm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/notebook-llama-oss-notebook-lm</guid>
      <pubDate>2024-11-01 09:32</pubDate>
      <category>#ai</category>
      <category>#sample</category>
      <category>#postcast</category>
      <category>#texttospeech</category>
      <category>#programming</category>
      <category>#sample</category>
      <category>#tutorial</category>
    </item>
    <item>
      <title>Art the Clown Will Ring the Nasdaq Closing Bell in Times Square on Halloween!</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/art-clown-nasdaq-bell-halloween?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/art-clown-nasdaq-bell-halloween&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;With Terrifier 3 now playing in theaters and just passing $50 million worldwide, Art the Clown‚Äôs domination of the Halloween season continues with a takeover of the Nasdaq MarketSite in Times Square!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...Art the Clown and star Lauren LaVera will ring the Nasdaq closing bell!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is so cool! Hopefully that's all he does.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/art-clown-nasdaq-bell-halloween?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/art-clown-nasdaq-bell-halloween</guid>
      <pubDate>2024-10-31 09:05</pubDate>
      <category>#art</category>
      <category>#terrifier</category>
      <category>#halloween</category>
      <category>#movie</category>
      <category>#horror</category>
      <category>#newyork</category>
      <category>#ny</category>
      <category>#nasdaq</category>
    </item>
    <item>
      <title>New Release: Spirit Box by Flying Lotus</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/spirit-box-flying-lotus?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/spirit-box-flying-lotus&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;He's back! A day after &lt;a href="/feed/chromakopia-tyler-the-creator-released"&gt;Tyler&lt;/a&gt; too.&lt;/p&gt;
&lt;p&gt;I already loved Garmonbozia but Ajhussi is my favorite track.&lt;/p&gt;
&lt;p&gt;Although on this album you hear some of the early FlyLo sound, it still feels new and fresh.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=OLAK5uy_kQDB4vsLJy0bmjuYsCXKa-eMO-W33na2w" title="Flying Lotus Spirit Box Album Playlist Thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/E-BbWldXfJk/0.jpg" class="img-fluid" alt="Flying Lotus Spirit Box Album Playlist Thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/spirit-box-flying-lotus?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/spirit-box-flying-lotus</guid>
      <pubDate>2024-10-30 08:45</pubDate>
      <category>#music</category>
      <category>#flyinglotus</category>
      <category>#spiritbox</category>
      <category>#newmusic</category>
      <category>#album</category>
    </item>
    <item>
      <title>Chromakopia by Tyler The Creator Released</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/chromakopia-tyler-the-creator-released?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/chromakopia-tyler-the-creator-released&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I did a first pass earlier today and I generally enjoyed this new album from Tyler.&lt;/p&gt;
&lt;p&gt;Like many previous albums, it's highly introspective. I loved the instrumentals as well.&lt;/p&gt;
&lt;p&gt;Some first impressions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Track 10 is not a double track as it's generally been the case of his previous albums.&lt;/li&gt;
&lt;li&gt;&amp;quot;Like Him&amp;quot; adds a new contrasting dimension to Tyler's relationship with his father.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'll need some more time to digest it and read through the lyrics but overall, great album.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=hCcwCv3G1FQ&amp;amp;list=OLAK5uy_nt1Nw4wT6I7VlzNknxTiIz3hfED0ttO8Q" title="Tyler The Creator Chromakopia Playlist Thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/hCcwCv3G1FQ/0.jpg" class="img-fluid" alt="Tyler The Creator Chromakopia Playlist Thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/chromakopia-tyler-the-creator-released?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/chromakopia-tyler-the-creator-released</guid>
      <pubDate>2024-10-28 19:59</pubDate>
      <category>#chromakopia</category>
      <category>#music</category>
      <category>#tylerthecreator</category>
      <category>#newmusic</category>
      <category>#album</category>
      <category>#rap</category>
      <category>#hiphop</category>
    </item>
    <item>
      <title>R.I.P 399 - Queen of the Tetons</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/rip-399-queen-tetons?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/rip-399-queen-tetons&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Sad news from the Tetons this week. Grizzly 399 passed away after being struck by a vehicle.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9gXa-bs_9i0" title="PBS 399 Queen of the Tetons Documentary"&gt;&lt;img src="http://img.youtube.com/vi/9gXa-bs_9i0/0.jpg" class="img-fluid" alt="PBS 399 Queen of the Tetons Documentary" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Grizzly No. 399 died Tuesday night on a highway in Snake River Canyon south of Jackson...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;At 28 years old, No. 399 was the oldest known reproducing female grizzly in the Yellowstone ecosystem. Each spring, wildlife enthusiasts eagerly awaited her emergence from her den to see how many cubs she had birthed over the winter ‚Äî then quickly shared the news online.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The grizzly lived through a time of strife over her species in the region...&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/rip-399-queen-tetons?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/rip-399-queen-tetons</guid>
      <pubDate>2024-10-25 21:59</pubDate>
      <category>#399</category>
      <category>#grandteton</category>
      <category>#grizzly</category>
      <category>#teton</category>
    </item>
    <item>
      <title>Tiny Desk Concerts - Kamasi Washington</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/kamasi-washington-tiny-desk-concert-npr?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/kamasi-washington-tiny-desk-concert-npr&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I think this is Kamasi's first time on Tiny Desk. Great performance. I didn't realize Brandon Coleman was on keys. The encore, &amp;quot;Asha The First&amp;quot;, was really good.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=x8WTPgeVPjg" title="Thumbnail of Kamasi Washington performing on Tiny Desk Concerts"&gt;&lt;img src="http://img.youtube.com/vi/x8WTPgeVPjg/0.jpg" class="img-fluid" alt="Thumbnail of Kamasi Washington performing on Tiny Desk Concerts" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/kamasi-washington-tiny-desk-concert-npr?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/kamasi-washington-tiny-desk-concert-npr</guid>
      <pubDate>2024-10-22 09:30</pubDate>
      <category>#kamasiwashington</category>
      <category>#music</category>
      <category>#tinydesk</category>
      <category>#npr</category>
      <category>#jazz</category>
      <category>#brainfeeder</category>
    </item>
    <item>
      <title>Noid by Tyler The Creator</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/noid-tyler-the-creator-chromakopia?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/noid-tyler-the-creator-chromakopia&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I'm ready for this new album. It reminds me of Cherry Bomb in some ways. Love the instrumentals.&lt;/p&gt;
&lt;iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/1tnZxHryc2wWtjUZC1LQw5?utm_source=generator" width="100%" height="352" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"&gt;&lt;/iframe&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/noid-tyler-the-creator-chromakopia?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/noid-tyler-the-creator-chromakopia</guid>
      <pubDate>2024-10-21 20:15</pubDate>
      <category>#music</category>
      <category>#chromakopia</category>
      <category>#tylerthecreator</category>
      <category>#noid</category>
      <category>#newmusic</category>
    </item>
    <item>
      <title>Chromakopia - Tyler The Creator</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/chromakopia-tyler-the-creator?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/chromakopia-tyler-the-creator&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;2024 just keeps delivering. Most of the artists I follow have dropped an album this year. Now I'm just waiting on FlyLo. Hopefully he releases his album by end of year.&lt;/p&gt;
&lt;p&gt;I don't know what to expect from Chromakopia, but I can't wait after watching this trailer.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=gkZ4dLMH-B8" title="Thumbnail of St. Chroma Chromakopia album anouncement video"&gt;&lt;img src="http://img.youtube.com/vi/gkZ4dLMH-B8/0.jpg" class="img-fluid" alt="Thumbnail of St. Chroma Chromakopia album anouncement video" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/chromakopia-tyler-the-creator?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/chromakopia-tyler-the-creator</guid>
      <pubDate>2024-10-18 09:28</pubDate>
      <category>#music</category>
      <category>#tylerthecreator</category>
      <category>#chromakopia</category>
      <category>#newmusic</category>
    </item>
    <item>
      <title>You should be using an RSS reader</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/you-should-be-using-rss-reader-pluralistic?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/you-should-be-using-rss-reader-pluralistic&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Yes, you should!&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the conduit through which I experience Molly's excellent work is totally enshittification-proof, and the more I use it, the easier it is for everyone to be less enshittified.&lt;br /&gt;
&lt;/br&gt;
This conduit is anti-lock-in, it works for nearly the whole internet. It is surveillance-resistant, far more accessible than the web or any mobile app interface. It is my secret super-power.&lt;br /&gt;
&lt;/br&gt;
It's RSS.&lt;br /&gt;
&lt;br&gt;
...But RSS isn't just good for the news! It's good for everything.&lt;br /&gt;
&lt;/br&gt;
Your RSS reader doesn't (necessarily) have an algorithm. By default, you'll get everything as it appears, in reverse-chronological order.&lt;br /&gt;
&lt;/br&gt;
Now, you sign up to so many feeds that you're feeling overwhelmed and you want an algorithm to prioritize posts ‚Äì or recommend content. Lots of RSS readers have some kind of algorithm and recommendation system...&lt;br /&gt;
&lt;/br&gt;
But you control the algorithm, you control the recommendations. And if a new RSS reader pops up with an algorithm you're dying to try, you can export all the feeds you follow with a single click, which will generate an OPML file. Then, with one click, you can import that OPML file into any other RSS reader in existence and all your feeds will be seamlessly migrated there. You can delete your old account, or you can even use different readers for different purposes.&lt;br /&gt;
&lt;/br&gt;
RSS basically works like social media should work. Using RSS is a chance to visit a utopian future in which the platforms have no power, and all power is vested in publishers, who get to decide what to publish, and in readers, who have total control over what they read and how, without leaking any personal information through the simple act of reading.&lt;br /&gt;
&lt;/br&gt;
And here's the best part: every time you use RSS, you bring that world closer into being!&lt;br /&gt;
&lt;/br&gt;
Unlike those largely useless, performative boycotts of widely used platforms, switching to RSS doesn't require that you give anything up. Not only does switching to RSS let you continue to follow all the newsletters, webpages and social media accounts you're following now, it makes doing so better: more private, more accessible, and less enshittified.&lt;br /&gt;
&lt;/br&gt;
Using RSS to follow the stuff that matters to you will have an immediate, profoundly beneficial impact on your own digital life ‚Äì and it will appreciably, irreversibly nudge the whole internet towards a better state.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Great article by &lt;a href="https://craphound.com/bio/"&gt;Cory Doctorow&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I've written at length on my website as to &lt;a href="/posts/rediscovering-rss-user-freedom"&gt;why I enjoy RSS&lt;/a&gt;. My use goes back to the days of Google Reader. Like many, once that was disbanded, my consumption pretty much shifted to social media for a few years. After growing dissatisfaction with the limitations and lack of control over my feed, I started my journey back to RSS readers eventually landing on &lt;a href="/feed/subscribed-to-1042-feeds-newsblur"&gt;NewsBlur&lt;/a&gt; and elfeed as my readers of choice.&lt;/p&gt;
&lt;p&gt;On this website, RSS plays an important role in a few areas.&lt;/p&gt;
&lt;p&gt;RSS is how I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.lqdev.me/feed/website-feeds-opml"&gt;Enable discovery of my own feeds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/subscribe"&gt;Enable visitors to subscribe and get the latest content&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/posts/rss-to-mastodon-posse-azure-logic-apps"&gt;Syndicate posts to Mastodon&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="/feed/blogroll-discovery-implemented"&gt;Enable feed discovery of websites I subscribe to&lt;/a&gt; through my:
&lt;ul&gt;
&lt;li&gt;&lt;a href="/feed/blogroll"&gt;Blogroll&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/feed/podroll"&gt;Podroll (podcasts)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/feed/youtube"&gt;YouTubeRoll?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/feed/forums"&gt;ForumRoll?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So how can you get started?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn &lt;a href="https://aboutfeeds.com/"&gt;about feeds&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Check out the many &lt;a href="https://matthiasott.com/notes/we-love-rss"&gt;getting started guides&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Browse through &lt;a href="https://blogroll.club/"&gt;blogrolls and feed catalogs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.theverge.com/24036427/rss-feed-reader-best"&gt;Explore various feed readers&lt;/a&gt; to find the one that works best for you.&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/you-should-be-using-rss-reader-pluralistic?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/you-should-be-using-rss-reader-pluralistic</guid>
      <pubDate>2024-10-18 08:54</pubDate>
      <category>#rss</category>
      <category>#pluralistic</category>
      <category>#indieweb</category>
      <category>#protocols</category>
      <category>#openweb</category>
      <category>#freeweb</category>
      <category>#internet</category>
      <category>#smallweb</category>
    </item>
    <item>
      <title>DEFCON Media Server</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/defcon-media-server?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/defcon-media-server&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Videos, Slides, Presentations, and assets from DEFCON.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/defcon-media-server?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/defcon-media-server</guid>
      <pubDate>2024-10-10 20:41</pubDate>
      <category>#defcon</category>
      <category>#media</category>
      <category>#persentations</category>
      <category>#videos</category>
      <category>#slides</category>
    </item>
    <item>
      <title>Fighting for our web - A talk from XOXO</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/fighting-for-our-web-white?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/fighting-for-our-web-white&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I haven't watched the entire talk, but here are some excerpts from the transcript that resonated with me.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=MTaeVVAvk-c" title="Thumbnail of Molly White Speaking at XOXO Conference"&gt;&lt;img src="http://img.youtube.com/vi/MTaeVVAvk-c/0.jpg" class="img-fluid" alt="Thumbnail of Molly White Speaking at XOXO Conference" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;For a lot of people, the web feels worse than it used to. Much of our time is spent on a handful of giant social networks that do everything they can to keep people in their apps for as long as possible, even if it has detrimental effects on the people who are using them, or on the social networks themselves.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Websites outside of these handful of social networks are harder and harder to even find, and partially because of that, they have a harder and harder time sustaining themselves.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;What is left for those of us who saw the web as an infinite canvas, a tool to reach those who we never could have dreamed of reaching before, a medium that could stretch the limits of what was even possible in an analog world?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...having said all this, would it surprise you to hear that now more than ever, I feel that same burning feeling of excitement around what‚Äôs possible?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;That‚Äôs because what really sucks about the web these days, what has us feeling despair and anger, has everything to do with the industry that has formed around the web, but not the web itself. The web is still just a substrate on which anything can be built. Most importantly, the web is the people who use it, not the companies that have established themselves around it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;And the widespread disillusionment that we‚Äôre seeing may actually be a good thing. More people than ever have realized that the utopian dreams of a web that could only bring about positive and wonderful things might have been misguided.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;With this knowledge comes power. The power to shape the web that we want to see, while fighting against the one that we don‚Äôt.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;My experience in fighting this fight has helped to convince me of just how much power we, everyday normal people, have‚Äîeven when we‚Äôre staring down massive platforms and billion-dollar companies.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I ended up doing what I enjoy doing‚Äîbuilding something cool, mostly for the sake of building it, and writing down what I saw.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...this simple act of building something interesting and somewhat different had an impact that I could not have anticipated. It turned out that people were starving for it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;And even though I was a nobody, it had a major impact.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The lesson was clear. The tech industry, even with its billions of dollars, is not an indomitable force. Though they can and will ignore what people want from them, they cannot control what those people think. And the tide can turn against them. And it doesn‚Äôt always take a job at The New York Times or a huge pre-established platform to become one of the voices speaking up, helping to turn that tide. Sometimes you just have to make something cool. And using the very same technology that enabled crypto guys to sell their scam tokens, or the boosterist journalists to publish obsequious descriptions of companies that were really selling vaporware, or the crypto community to spam social media with promotions for their NFTs, I was able to do something a little different to push back against that very same phenomenon.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;What they don‚Äôt seem to realize is that in doing so, by reducing the web only to the types of expression that can happen within their cramped boxes‚Äîwhere you can‚Äôt write more than 280 characters, or you can‚Äôt publish your cool JavaScript-based art project, or you can‚Äôt say the things that you want to say without getting de-boosted by the engagement maximization machine, or you can‚Äôt read what your friends are posting without the platform interjecting offensive troll posts or soulless AI-generated meme images‚Äîthey‚Äôre creating a thirst for everything outside of those boxes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A thirst for what the web really is‚Äîa medium, a conduit, a tool that is used by readers and artists and creators and explorers, not a gatekeeper that seems to be in an ever more adversarial relationship with everyone who uses it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The platforms I talk about are deeply entrenched to the point where many people barely use the web outside of them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;But the platforms do not exist without the people, and there are a lot more of us than there are of them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...it has never been easier to do cool things on the web. What used to be expensive and require a great deal of technical expertise is now becoming more and more accessible, both financially and technologically, by the day. More people than ever have access to the web, both in terms of access to devices, but also in terms of access to internet connectivity and software. What used to be the realm of the nerds and those with the financial wherewithal to purchase expensive home computers, connectivity, and software packages, is now home to people from all walks of life, who bring new ideas, perspectives, and experiences that we often forget were in short supply during what some of us think of as the ‚Äúgood old days‚Äù of the internet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;And so, we‚Äîall of us‚Äîcan build the projects we want to see, write the software that the big platforms won‚Äôt, and create the services that people need. We can wire everything together, whether the platforms like it or not, to tear down the walls that they put up. We can modify the software, reverse engineer their systems, and wrestle back control of how we experience the web, even through these very platforms. We can share information and help people understand what is happening around them. We can teach others to build the things they want to build, and share with one another the important and meaningful work that is being done.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We can build the web that we want to see, and we can return to that place where the web is a place of wonder, where all of us feel that same burning feeling of excitement as we push the web back towards the wonderful, beautiful, joyful place it ought to be.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/fighting-for-our-web-white?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/fighting-for-our-web-white</guid>
      <pubDate>2024-10-10 20:20</pubDate>
      <category>#internet</category>
      <category>#indieweb</category>
      <category>#protocolsnotplatforms</category>
      <category>#humanweb</category>
      <category>#smallweb</category>
      <category>#web30</category>
    </item>
    <item>
      <title>Marvin Gaye - What's Going On</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/marvin-gaye-whats-going-on?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/marvin-gaye-whats-going-on&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This is such a beautiful album. It seems I'm not the only one that thinks so since it's rated as the &lt;a href="https://www.rollingstone.com/music/music-lists/best-albums-of-all-time-1062063/marvin-gaye-whats-going-on-4-1063232/"&gt;#1 album of all time by Rolling Stone&lt;/a&gt;. The powerful messages in the lyrics still resonate to this day and the composition is such a perfect match for Marvin Gaye's voice. 50 years later it feels like this album has transcended space and time.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=o5TmORitlKk" title="Marvin Gaye What's Going On Album Playlist Thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/o5TmORitlKk/0.jpg" class="img-fluid" alt="Marvin Gaye What's Going On Album Playlist Thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Over the past few days, I've listened to it from beginning to end at least 3 times.&lt;/p&gt;
&lt;p&gt;Of course there's the classics like &lt;em&gt;What's Going On&lt;/em&gt; and &lt;em&gt;Mercy Mercy Me&lt;/em&gt; that many people know. However, the ones that have quickly become my favorites are:&lt;/p&gt;
&lt;h2&gt;&lt;em&gt;Save The Children&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=E1KqO8YtXlY" title="Save The Children Marvin Gaye Thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/E1KqO8YtXlY/0.jpg" class="img-fluid" alt="Save The Children Marvin Gaye Thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;em&gt;God Is Love&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=2OIOoTJwEVg" title="God Is Love Marvin Gaye Thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/2OIOoTJwEVg/0.jpg" class="img-fluid" alt="God Is Love Marvin Gaye Thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/marvin-gaye-whats-going-on?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/marvin-gaye-whats-going-on</guid>
      <pubDate>2024-09-25 20:19</pubDate>
      <category>#music</category>
      <category>#marvingaye</category>
      <category>#whatsgoingon</category>
      <category>#soul</category>
      <category>#rnb</category>
      <category>#album</category>
      <category>#war</category>
      <category>#nature</category>
      <category>#environment</category>
      <category>#conservation</category>
      <category>#love</category>
    </item>
    <item>
      <title>Listening to the Sun by Andre 3000</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/andre-3000-listening-to-the-sun?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/andre-3000-listening-to-the-sun&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Simply amazing.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=2nOxSeBhQAA" title="Andre 3000 Listening to the Sun Thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/2nOxSeBhQAA/0.jpg" class="img-fluid" alt="Andre 3000 Listening to the Sun Thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/andre-3000-listening-to-the-sun?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/andre-3000-listening-to-the-sun</guid>
      <pubDate>2024-09-14 22:08 -05:00</pubDate>
      <category>#andre3000</category>
      <category>#music</category>
      <category>#instrumental</category>
      <category>#experimental</category>
      <category>#ambient</category>
      <category>#jazz</category>
      <category>#creative</category>
      <category>#instrumental</category>
    </item>
    <item>
      <title>Cosmohedron by Duncan Hatch</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/cosmohedron-duncan-hatch?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/cosmohedron-duncan-hatch&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I love the visuals on this video.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=3pxrECZYEAA" title="Cosmohedron Animated Short Thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/3pxrECZYEAA/0.jpg" class="img-fluid" alt="Cosmohedron Animated Short Thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/cosmohedron-duncan-hatch?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/cosmohedron-duncan-hatch</guid>
      <pubDate>2024-09-14 22:06 -05:00</pubDate>
      <category>#short</category>
      <category>#animation</category>
      <category>#psychidelic</category>
      <category>#duncanhatch</category>
      <category>#video</category>
      <category>#youtube</category>
    </item>
    <item>
      <title>Shipping Tumblr and WordPress</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/shipping-wordpress-tumblr?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/shipping-wordpress-tumblr&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Since Automattic acquired Tumblr we‚Äôve made it more efficient, grown its revenue, and worked to improve the platform. But there‚Äôs one part of the plan that we haven‚Äôt yet started, which is to run Tumblr on WordPress. I‚Äôm pleased to say we‚Äôre kicking off that project now!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A little late, but I can't express how much I love this.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We love Tumblr‚Äôs streamlined posting experience and its current product direction. We‚Äôre not changing that. We‚Äôre talking about running Tumblr‚Äôs backend on WordPress. You won‚Äôt even notice a difference from the outside.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Although it sounds outside the scope of the original plan, it would be amazing if the shared backend as well as the Tumblr publishing front-end made their way into the open-source version of WordPress.&lt;/p&gt;
&lt;p&gt;That would open the door to self-hosted personal websites that also make it significantly easier to post content in smaller chunks like a microblog.&lt;/p&gt;
&lt;p&gt;Pair that with ActivityPub integrations and you now have a connected web of individuals where the first place they post content to is their own website and for discovery / broader reach can also federate and post to other platforms that also support protocols like ActivityPub.&lt;/p&gt;
&lt;p&gt;Can't wait to see how this project develops.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Do you yearn for the days when people owned their corner of the internet and expressed themselves in wild and wacky ways? Do you want to see an internet focused on creativity, art, and ideas instead of debating and dividing? Do you think content and data should be owned by authors and artists, instead of getting locked behind the closed platform of a mega-corporation? Do you want to build an internet where anyone with a story can tell it, and anyone with a product can sell it, regardless of income, gender, politics, language, or where they live in the world?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The answer to all of those is YES.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/shipping-wordpress-tumblr?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/shipping-wordpress-tumblr</guid>
      <pubDate>2024-09-05 22:15</pubDate>
      <category>#wordpress</category>
      <category>#tumblr</category>
      <category>#personalweb</category>
      <category>#blogging</category>
      <category>#indieweb</category>
      <category>#microblog</category>
      <category>#posse</category>
      <category>#smallweb</category>
      <category>#automattic</category>
    </item>
    <item>
      <title>Join me at DEVintersection in Las Vegas - September 10-12</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/devintersection-vegas-sept-2024?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/devintersection-vegas-sept-2024&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I'm excited to be at DEVintersection again this year where I'll get a chance to meet old and new friends.&lt;/p&gt;
&lt;p&gt;I have a few sessions where I'll be talking about some of my favorite things, .NET and AI.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.devintersection.com/#!/session/An%20Introduction%20to%20AI%20in%20.NET/6788"&gt;Introduction to AI in .NET&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.devintersection.com/#!/session/Building%20AI%20Applications%20from%20Scratch:%20A%20Hands-On%20Guide%20for%20.NET%20Developers/6789"&gt;Building AI Applications from Scratch: A Hands-On Guide for .NET Developers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.devintersection.com/#!/session/Ask%20The%20Experts%20-%20Starting%20out%20on%20your%20.NET%20&amp;amp;%20AI%20explorations/7023"&gt;Ask The Experts - Starting out on your .NET &amp;amp; AI explorations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other sessions I recommend as well:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.devintersection.com/#!/session/Navigating%20the%20World%20of%20AI%20Models%20in%20.NET:%20From%20Local%20Development%20to%20the%20Cloud/7022"&gt;Navigating the World of AI Models in .NET: From Local Development to the Cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.devintersection.com/#!/session/KEYNOTE%20-%20Practical%20Real-World%20AI%20for%20Developers/6986"&gt;KEYNOTE - Practical Real-World AI for Developers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.devintersection.com/#!/session/Making%20Web%20Applications%20Intelligent%20and%20Performant%20with%20Redis/6982"&gt;Making Web Applications Intelligent and Performant with Redis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.devintersection.com/#!/session/Building%20AI%20Copilots:%20Integrate%20Semantic%20Kernel,%20Azure%20OpenAI,%20and%20Azure%20Cosmos%20DB%20with%20.NET%20Aspire/6960"&gt;Building AI Copilots: Integrate Semantic Kernel, Azure OpenAI, and Azure Cosmos DB with .NET Aspire&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.devintersection.com/#!/session/Introduction%20to%20.NET%20Aspire/6964"&gt;Introduction to .NET Aspire&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There's so many more, but in the interest of not listing them all out, check out the &lt;a href="https://www.devintersection.com/#!/sessions"&gt;schedule&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See you there!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/devintersection-vegas-sept-2024?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/devintersection-vegas-sept-2024</guid>
      <pubDate>2024-09-04 20:10 -05:00</pubDate>
      <category>#devintersection</category>
      <category>#conference</category>
      <category>#dotnet</category>
      <category>#ai</category>
      <category>#aspnet</category>
      <category>#las</category>
      <category>#lasvegas</category>
      <category>#desert</category>
    </item>
    <item>
      <title>NPR Tiny Desk Concerts: Bob Weir, Wolf Bros, Mikaela Davis</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/npr-tiny-desk-bob-weir-wolf-bros-mikaela-davis?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/npr-tiny-desk-bob-weir-wolf-bros-mikaela-davis&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I came across this gem over the weekend. &lt;a href="https://www.mikaeladavis.com/"&gt;Mikaela Davis&lt;/a&gt; on the harp just takes Bird Song and Ripple to the next level.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=4l_gUwdPrNY" title="NPR Tiny Desk Concert Bob Weir Wolf Bros video thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/4l_gUwdPrNY/0.jpg" class="img-fluid" alt="NPR Tiny Desk Concert Bob Weir Wolf Bros video thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/npr-tiny-desk-bob-weir-wolf-bros-mikaela-davis?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/npr-tiny-desk-bob-weir-wolf-bros-mikaela-davis</guid>
      <pubDate>2024-08-26 19:27 -05:00</pubDate>
      <category>#music</category>
      <category>#tinydesk</category>
      <category>#gratefuldead</category>
      <category>#bobweir</category>
      <category>#npr</category>
      <category>#wolfbros</category>
      <category>#mikaeladavis</category>
      <category>#concert</category>
      <category>#deadhead</category>
    </item>
    <item>
      <title>Bringing Llama 3 to life</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/meta-bringing-llama-3-to-life?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/meta-bringing-llama-3-to-life&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;At AI Infra @ Scale 2024, Meta engineers discussed every step of how we built and brought Llama 3 to life, from data and training to inference.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=ELIcy6flgQI" title="Bringing Llama 3 to life talk video thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/ELIcy6flgQI/0.jpg" class="img-fluid" alt="Bringing Llama 3 to life talk video thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/meta-bringing-llama-3-to-life?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/meta-bringing-llama-3-to-life</guid>
      <pubDate>2024-08-22 16:21 -05:00</pubDate>
      <category>#ai</category>
      <category>#meta</category>
      <category>#llama3</category>
      <category>#opensource</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Transformers in music recommendation</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/transformers-music-recommendation-google?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/transformers-music-recommendation-google&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We present a music recommendation ranking system that uses Transformer models to better understand the sequential nature of user actions based on the current user context.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/transformers-music-recommendation-google?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/transformers-music-recommendation-google</guid>
      <pubDate>2024-08-21 20:16 -05:00</pubDate>
      <category>#google</category>
      <category>#ai</category>
      <category>#transformers</category>
      <category>#deeplearning</category>
      <category>#neuralneworks</category>
      <category>#recommendation</category>
      <category>#research</category>
      <category>#music</category>
    </item>
    <item>
      <title>New Chicano Batman: Tanto Arriba Como Abajo</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/new-chicano-batman-tanto-arriba-como-abajo?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/new-chicano-batman-tanto-arriba-como-abajo&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;As above, so below.&lt;/p&gt;
&lt;p&gt;Some new Chicano Batman was a pleasant surprise this morning. Even better, an instrumental track.&lt;/p&gt;
&lt;p&gt;You can hear some of their older sound in this track, which I really enjoyed.&lt;/p&gt;
&lt;iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/518ruJoGWraifuVpTBKr5a" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"&gt;&lt;/iframe&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/new-chicano-batman-tanto-arriba-como-abajo?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/new-chicano-batman-tanto-arriba-como-abajo</guid>
      <pubDate>2024-08-20 20:24 -05:00</pubDate>
      <category>#music</category>
      <category>#chicanobatman</category>
      <category>#instrumental</category>
      <category>#newmusic</category>
    </item>
    <item>
      <title>Pain and sorrow on repeat</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/pain-and-sorrow-on-repeat?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/pain-and-sorrow-on-repeat&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I've had this song on repeat since&lt;a href="/feed/new-flying-lotus-garmonbozia/"&gt;Friday&lt;/a&gt;. Also, I didn't know the name of the song is a &lt;a href="https://twinpeaks.fandom.com/wiki/Garmonbozia"&gt;Twin Peaks&lt;/a&gt; reference. Given his recent focus as a composer, I hope this means there's a bigger Twin Peaks project or collaboration in the works.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=CfB0Uqd2aRE" title="Garmonbozia by Flying Lotus visualizer video thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/CfB0Uqd2aRE/0.jpg" class="img-fluid" alt="Garmonbozia by Flying Lotus visualizer video thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/pain-and-sorrow-on-repeat?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/pain-and-sorrow-on-repeat</guid>
      <pubDate>2024-08-19 21:47 -05:00</pubDate>
      <category>#music</category>
      <category>#flyinglotus</category>
      <category>#twinpeaks</category>
      <category>#davidlynch</category>
      <category>#flylo</category>
      <category>#tv</category>
    </item>
    <item>
      <title>Thundercat + Yo Gabba Gabba</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/thundercat-yo-gabba-gabba?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/thundercat-yo-gabba-gabba&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;a href="https://x.com/DatDaDatty/status/1824251654813704281"&gt;&lt;img src="https://pbs.twimg.com/ext_tw_video_thumb/1824251568817844225/pu/img/25k7XExQAqNaL1kB.jpg" class="img-fluid" alt="Image of Thundercat playing the bass on Yo Gabba Gabba Show" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here's something I didn't know I needed in my life. Perfect way to start a Saturday morning.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/thundercat-yo-gabba-gabba?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/thundercat-yo-gabba-gabba</guid>
      <pubDate>2024-08-17 11:06 -05:00</pubDate>
      <category>#music</category>
      <category>#thundercat</category>
      <category>#yogabbagabba</category>
      <category>#fun</category>
    </item>
    <item>
      <title>Organic Maps removed from Google Play Store</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/organic-maps-removed-play-store?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/organic-maps-removed-play-store&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Last night Organic Maps was removed from the Play Store without any warnings or additional details due to &amp;quot;not meeting the requirements for the Family Program&amp;quot;. Compared to Google Maps and other maps apps rated for 3+ age, there are no ads or in-app purchases in Organic Maps. We have asked for an appeal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The app is still available on &lt;a href="https://f-droid.org/en/packages/app.organicmaps/"&gt;F-Droid&lt;/a&gt;. Much better place for getting apps in my opinion.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/organic-maps-removed-play-store?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/organic-maps-removed-play-store</guid>
      <pubDate>2024-08-17 11:00 -05:00</pubDate>
      <category>#organicmaps</category>
      <category>#google</category>
      <category>#play</category>
      <category>#fdroid</category>
      <category>#opensource</category>
      <category>#osm</category>
      <category>#openstreetmaps</category>
      <category>#maps</category>
      <category>#app</category>
      <category>#store</category>
    </item>
    <item>
      <title>New Flying Lotus: Garmonbozia</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/new-flying-lotus-garmonbozia?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/new-flying-lotus-garmonbozia&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Good way to start a Friday. New drop from FlyLo. I like it.&lt;/p&gt;
&lt;iframe style="border: 0; width: 100%; height: 120px;" src="https://bandcamp.com/EmbeddedPlayer/track=1672253486/size=large/bgcol=ffffff/linkcol=0687f5/tracklist=false/artwork=small/transparent=true/" seamless&gt;&lt;a href="https://flyinglotus.bandcamp.com/track/garmonbozia"&gt;Garmonbozia by Flying Lotus&lt;/a&gt;&lt;/iframe&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/new-flying-lotus-garmonbozia?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/new-flying-lotus-garmonbozia</guid>
      <pubDate>2024-08-16 09:52 -05:00</pubDate>
      <category>#music</category>
      <category>#flyinglotus</category>
      <category>#garmonbozia</category>
      <category>#newmusic</category>
      <category>#flylo</category>
      <category>#bandcamp</category>
    </item>
    <item>
      <title>Transformer Explainer: Interactive Learning of Text-Generative Models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/transformer-explainer-interactive-learning-text-generative-models?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/transformer-explainer-interactive-learning-text-generative-models&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Transformers have revolutionized machine learning, yet their inner workings remain opaque to many. We present Transformer Explainer, an interactive visualization tool designed for non-experts to learn about Transformers through the GPT-2 model. Our tool helps users understand complex Transformer concepts by integrating a model overview and enabling smooth transitions across abstraction levels of mathematical operations and model structures. It runs a live GPT-2 instance locally in the user's browser, empowering users to experiment with their own input and observe in real-time how the internal components and parameters of the Transformer work together to predict the next tokens. Our tool requires no installation or special hardware, broadening the public's education access to modern generative AI techniques. Our open-sourced tool is available at this https URL. A video demo is available at this https URL.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://poloclub.github.io/transformer-explainer/"&gt;Tool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://youtu.be/ECR4oAwocjs"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/transformer-explainer-interactive-learning-text-generative-models?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/transformer-explainer-interactive-learning-text-generative-models</guid>
      <pubDate>2024-08-14 21:51 -05:00</pubDate>
      <category>#ai</category>
      <category>#transformer</category>
      <category>#neuralnetwork</category>
      <category>#learning</category>
      <category>#gpt</category>
    </item>
    <item>
      <title>HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/hybridrag-knowledge-graphs-vector-rag-info-extraction?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/hybridrag-knowledge-graphs-vector-rag-info-extraction&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&amp;amp;A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&amp;amp;A format, and hence provide a natural set of pairs of ground-truth Q&amp;amp;As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/hybridrag-knowledge-graphs-vector-rag-info-extraction?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/hybridrag-knowledge-graphs-vector-rag-info-extraction</guid>
      <pubDate>2024-08-14 21:49 -05:00</pubDate>
      <category>#ai</category>
      <category>#rag</category>
      <category>#graph</category>
      <category>#database</category>
      <category>#knowledgegraph</category>
      <category>#kb</category>
      <category>#vector</category>
    </item>
    <item>
      <title>The US government wants to make it easier for you to click the 'unsubscribe' button</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/us-govt-fcc-easier-unsubscribe?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/us-govt-fcc-easier-unsubscribe&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Given my &lt;a href="/feed/subscription-dark-patterns/"&gt;recent experience unsubscribing from content&lt;/a&gt;, there's definitely room for improvement.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/us-govt-fcc-easier-unsubscribe?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/us-govt-fcc-easier-unsubscribe</guid>
      <pubDate>2024-08-12 21:31 -05:00</pubDate>
      <category>#regulation</category>
      <category>#fcc</category>
      <category>#subscriptions</category>
      <category>#uspol</category>
      <category>#government</category>
      <category>#us</category>
      <category>#darkpatterns</category>
    </item>
    <item>
      <title>GPT-4o System Card</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/gpt4o-system-card?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/gpt4o-system-card&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;GPT-4o is an autoregressive omni model, which accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It\u2019s trained end-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network.&lt;br /&gt;
&lt;br&gt;
GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window)2 in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.&lt;br /&gt;
&lt;br&gt;
In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House3, we are sharing the GPT-4o System Card, which includes our Preparedness Framework(opens in a new window)5 evaluations. In this System Card, we provide a detailed look at GPT-4o\u2019s capabilities, limitations, and safety evaluations across multiple categories, with a focus on speech-to-speech (voice)A while also evaluating text and image capabilities, and the measures we\u2019ve taken to enhance safety and alignment. We also include third party assessments on general autonomous capabilities, as well as discussion of potential societal impacts of GPT-4o text and vision capabilities.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/gpt4o-system-card?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/gpt4o-system-card</guid>
      <pubDate>2024-08-08 20:16 -05:00</pubDate>
      <category>#ai</category>
      <category>#openai</category>
      <category>#gpt-4o</category>
      <category>#documentation</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Bluesky Welcomes Mike Masnick to Board of Directors</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mike-masnick-bluesky-board-of-directors?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mike-masnick-bluesky-board-of-directors&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Nice! I remember reading &lt;a href="https://knightcolumbia.org/content/protocols-not-platforms-a-technological-approach-to-free-speech"&gt;Protocols, Not Platforms&lt;/a&gt; many years ago and felt inspired to seek out a better web.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mike-masnick-bluesky-board-of-directors?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mike-masnick-bluesky-board-of-directors</guid>
      <pubDate>2024-08-07 22:14 -05:00</pubDate>
      <category>#bluesky</category>
      <category>#protocols</category>
      <category>#platforms</category>
      <category>#socialmedia</category>
      <category>#decentralization</category>
      <category>#distributedweb</category>
      <category>#indieweb</category>
      <category>#smallweb</category>
      <category>#humanweb</category>
      <category>#web</category>
    </item>
    <item>
      <title>Jackson Hole's new Instagram filter warns you when you're dangerously close to wildlife</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/jackson-hole-instagram-filter-wildlife?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/jackson-hole-instagram-filter-wildlife&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;img src="https://i.ytimg.com/vi/F8Knnumz6vQ/hqdefault.jpg" class="img-fluid" alt="Chapelle Show Meme Modern Problems Require Modern Solutions" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/jackson-hole-instagram-filter-wildlife?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/jackson-hole-instagram-filter-wildlife</guid>
      <pubDate>2024-08-07 21:59 -05:00</pubDate>
      <category>#wildlife</category>
      <category>#wyoming</category>
      <category>#tourons</category>
      <category>#instagram</category>
      <category>#socialmedia</category>
      <category>#fun</category>
      <category>#jacksonhole</category>
      <category>#teton</category>
      <category>#nationalpark</category>
    </item>
    <item>
      <title>CNET purchased for JUST $100 million</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/cnet-bought-for-just-100-million?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/cnet-bought-for-just-100-million&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;JUST $100 million. That's pocket change üôÉ&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Red Ventures paid $500 million for the tech property once valued at $1.8 billion.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Given previous valuations though, I see why the headline was phrased that way.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/cnet-bought-for-just-100-million?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/cnet-bought-for-just-100-million</guid>
      <pubDate>2024-08-06 09:06 -05:00</pubDate>
      <category>#cnet</category>
      <category>#tech</category>
      <category>#acquisition</category>
      <category>#news</category>
    </item>
    <item>
      <title>Blogroll Club - A blog directory</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/blogroll-club-blog-directory?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/blogroll-club-blog-directory&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;A Blog Directory is a cool project from &lt;a href="https://notes.jeddacp.com/"&gt;JC (Probably)&lt;/a&gt; and &lt;a href="https://louplummer.lol/"&gt;Lou Plummer&lt;/a&gt;. &lt;a href="https://blogroll.club/"&gt;Check it out&lt;/a&gt;!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/blogroll-club-blog-directory?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/blogroll-club-blog-directory</guid>
      <pubDate>2024-08-05 20:53 -05:00</pubDate>
      <category>#blog</category>
      <category>#rss</category>
      <category>#indieweb</category>
      <category>#blogroll</category>
      <category>#community</category>
      <category>#web</category>
      <category>#smallweb</category>
      <category>#personalweb</category>
    </item>
    <item>
      <title>Congratulations on ten years of Overcast</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/congratulations-overcast-ten-years?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/congratulations-overcast-ten-years&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I no longer have an iPhone, but whenever anyone asks for a podcast app recommendation, Overcast is the first one I mention. Congrats on 10 years. Here's to many more.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/congratulations-overcast-ten-years?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/congratulations-overcast-ten-years</guid>
      <pubDate>2024-08-04 17:07 -05:00</pubDate>
      <category>#podcast</category>
      <category>#app</category>
      <category>#overcast</category>
      <category>#anniversary</category>
      <category>#milestone</category>
      <category>#apple</category>
      <category>#ios</category>
      <category>#rss</category>
    </item>
    <item>
      <title>Reddit says companies must pay for data access</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/reddit-companies-pay-data-access?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/reddit-companies-pay-data-access&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;img src="https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExejNzaDF5aHZ6dHRlenl6bnh4c2h6YWoxbnRuOXBqZnlmOGVlbWlwOCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/xT9DPzhNGA8MKjxwFG/giphy.gif" class="img-fluid" alt="GIF of Snoopy laughing" /&gt;&lt;/p&gt;
&lt;p&gt;I'm all for compensation. Remind me again, how much are the communities that create the content and make Reddit what it is getting out of these licensing deals?&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/reddit-companies-pay-data-access?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/reddit-companies-pay-data-access</guid>
      <pubDate>2024-07-31 16:23 -05:00</pubDate>
      <category>#ai</category>
      <category>#reddit</category>
      <category>#data</category>
      <category>#licensing</category>
      <category>#microsoft</category>
      <category>#anthropic</category>
      <category>#perplexity</category>
    </item>
    <item>
      <title>.well-known feeds</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/well-known-feeds?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/well-known-feeds&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This is a clever use of &lt;em&gt;.well-known&lt;/em&gt; and OPML.&lt;/p&gt;
&lt;p&gt;Definitely something I want to experiment with and implement on my site even if it's not widely adopted.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/well-known-feeds?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/well-known-feeds</guid>
      <pubDate>2024-07-31 09:01 -05:00</pubDate>
      <category>#standards</category>
      <category>#feeds</category>
      <category>#rss</category>
      <category>#web</category>
      <category>#indieweb</category>
      <category>#smallweb</category>
    </item>
    <item>
      <title>LongROPE: Extending LLM Context Window Beyond 2 Million Tokens</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/longrope-extending-llm-context-2m-tokens?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/longrope-extending-llm-context-2m-tokens&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/jshuadvd/LongRoPE"&gt;GitHub Repo&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/longrope-extending-llm-context-2m-tokens?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/longrope-extending-llm-context-2m-tokens</guid>
      <pubDate>2024-07-30 14:16 -05:00</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#longrope</category>
      <category>#microsoft</category>
      <category>#research</category>
      <category>#msr</category>
    </item>
    <item>
      <title>Tensors from scratch series</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tensors-from-scratch-blog-series?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tensors-from-scratch-blog-series&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I've been enjoying reading through this Tensors series.&lt;/p&gt;
&lt;p&gt;If you're interested, here's also the link to &lt;a href="https://maharshi.bearblog.dev/tensors-from-scratch-part-1/"&gt;part 1&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tensors-from-scratch-blog-series?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tensors-from-scratch-blog-series</guid>
      <pubDate>2024-07-29 22:26 -05:00</pubDate>
      <category>#tensors</category>
      <category>#math</category>
      <category>#ai</category>
      <category>#c</category>
      <category>#machinelearning</category>
      <category>#ml</category>
      <category>#artificialintelligence</category>
    </item>
    <item>
      <title>Meta AI's Segment Anything Model (SAM) 2</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/meta-ai-segment-anything-model-2?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/meta-ai-segment-anything-model-2&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;h2&gt;Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Following up on the success of the Meta Segment Anything Model (SAM) for images, we're releasing SAM 2, a unified model for real-time promptable object segmentation in images and videos that achieves state-of-the-art performance.&lt;/li&gt;
&lt;li&gt;In keeping with our approach to open science, we're sharing the code and model weights with a permissive Apache 2.0 license.&lt;/li&gt;
&lt;li&gt;We're also sharing the SA-V dataset, which includes approximately 51,000 real-world videos and more than 600,000 masklets (spatio-temporal masks).&lt;/li&gt;
&lt;li&gt;SAM 2 can segment any object in any video or image - even for objects and visual domains it has not seen previously, enabling a diverse range of use cases without custom adaptation.&lt;/li&gt;
&lt;li&gt;SAM 2 has many potential real-world applications. For example, the outputs of SAM 2 can be used with a generative video model to create new video effects and unlock new creative applications. SAM 2 could also aid in faster annotation tools for visual data to build better computer vision systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/meta-ai-segment-anything-model-2?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/meta-ai-segment-anything-model-2</guid>
      <pubDate>2024-07-29 22:22 -05:00</pubDate>
      <category>#meta</category>
      <category>#ai</category>
      <category>#computervision</category>
      <category>#sam2</category>
      <category>#segmentanythingmodel</category>
      <category>#aimodel</category>
      <category>#cv</category>
    </item>
    <item>
      <title>Nyxt Emacs Hacks</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nyxt-emacs-hacks?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nyxt-emacs-hacks&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Saving this guide for future reference as I set up my elfeed / Nyxt capture workflows for the website.&lt;/p&gt;
&lt;p&gt;Additional articles that might be helpful.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ag91.github.io/blog/2021/07/09/org-capture-in-nyxt-taking-notes-while-browsing/"&gt;Org capture in Nyxt: Taking Notes While Browsing&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nyxt-emacs-hacks?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nyxt-emacs-hacks</guid>
      <pubDate>2024-07-28 16:53 -05:00</pubDate>
      <category>#nyxt</category>
      <category>#emacs</category>
      <category>#orgmode</category>
      <category>#capture</category>
      <category>#templates</category>
      <category>#workflow</category>
      <category>#lisp</category>
    </item>
    <item>
      <title>Zombie Internet</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/facebook-zombie-internet-willison?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/facebook-zombie-internet-willison&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In my experience, the supermajority of engagement on viral AI Facebook pages is just as artificially-generated as the content they publish.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Whether it's a child transforming into a water bottle cyborg, a three-armed flight attendant rescuing Tiger Jesus from a muddy plane crash, or a hybrid human-monkey baby being stung to death by giant hornets, all tend to have copy+pasted captions, reactions &amp;amp; comments which usually make no sense in the observed context.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I've noticed similar patterns on YouTube. Sometime the comments include timestamp links which makes them seem more credible, but upon further inspection, it's all bot activity.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/facebook-zombie-internet-willison?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/facebook-zombie-internet-willison</guid>
      <pubDate>2024-07-16 11:48 -05:00</pubDate>
      <category>#bots</category>
      <category>#web</category>
      <category>#comments</category>
      <category>#ai</category>
      <category>#facebook</category>
      <category>#internet</category>
      <category>#personalweb</category>
    </item>
    <item>
      <title>Test response using org capture templates</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/test-emacs-capture-response-org?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/test-emacs-capture-response-org&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Testing org-capture template generated response file&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/test-emacs-capture-response-org?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/test-emacs-capture-response-org</guid>
      <pubDate>2024-07-14 13:47 -05:00</pubDate>
      <category>#org</category>
      <category>#emacs</category>
      <category>#blogging</category>
      <category>#automation</category>
      <category>#orgmode</category>
      <category>#templates</category>
      <category>#capturetemplates</category>
      <category>#website</category>
      <category>#personalweb</category>
      <category>#capture</category>
    </item>
    <item>
      <title>Home-Cooked Software and Barefoot Developers</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/home-cooked-software-barefoot-developers-appleton?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/home-cooked-software-barefoot-developers-appleton&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The emerging golden age of home-cooked software, barefoot developers, and why the local-first community should help build it&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This is a talk I presented Local-first Conference in Berlin, May 2024. It's specifically directed at the local-first
community, but its relevant to anyone involved in building software.&lt;br /&gt;
&lt;/br&gt;
For the last ~year I've been keeping a close eye on how language models capabilities meaningfully change the speed, ease, and accessibility of software development. The slightly bold theory I put forward in this talk is that we're on a verge of a golden age of local, home-cooked software and a new kind of developer ‚Äì what I've called the barefoot developer.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/home-cooked-software-barefoot-developers-appleton?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/home-cooked-software-barefoot-developers-appleton</guid>
      <pubDate>2024-07-06 22:02</pubDate>
      <category>#localfirst</category>
      <category>#smallweb</category>
      <category>#sofware</category>
      <category>#llm</category>
      <category>#programming</category>
      <category>#talk</category>
      <category>#indieweb</category>
    </item>
    <item>
      <title>Meta Large Language Model Compiler: Foundation Models of Compiler Optimization</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/meta-llm-compiler?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/meta-llm-compiler&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://huggingface.co/collections/facebook/llm-compiler-667c5b05557fe99a9edd25cb"&gt;HuggingFace Collection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scontent-lga3-1.xx.fbcdn.net/v/t39.2365-6/448997590_1496256481254967_2304975057370160015_n.pdf?_nc_cat=106&amp;amp;ccb=1-7&amp;amp;_nc_sid=3c67a6&amp;amp;_nc_ohc=4Yn8V9DFdbsQ7kNvgHtvfLI&amp;amp;_nc_ht=scontent-lga3-1.xx&amp;amp;oh=00_AYDYp657HzWrcs2CZ6ZBjStwB03bo760w9voXwJorfXA_w&amp;amp;oe=6683F28D"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/meta-llm-compiler?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/meta-llm-compiler</guid>
      <pubDate>2024-06-27 22:39</pubDate>
      <category>#ai</category>
      <category>#compiler</category>
      <category>#meta</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Deep Questions - Debunking AI Model Capabilities / Distributed Webs of Trust</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/deep-questions-debunking-genai-rss-blogroll?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/deep-questions-debunking-genai-rss-blogroll&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=OvlfCW3Ec1g" title="Deep Questions Podcast Debunking AI Episode Thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/OvlfCW3Ec1g/0.jpg" class="img-fluid" alt="Deep Questions Podcast Debunking AI Episode Thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Nice job by Cal debunking misconceptions about AI model capabilities. The segment highlights a few points I cover in my unpublished &lt;a href="https://github.com/lqdev/luisquintanilla.me/blob/main/_scratch/nolm-not-only-language-models.md"&gt;NoLM - Not Only Language Models&lt;/a&gt; blog post. Specifically the fact that Language Models on their own can't do much and need to be connected to data sources and other systems. Complex AI systems will be built with more specialized roles and leverage various components for their planning and execution. In the end though, models will require integration into existing systems. Those integrations need to be done by people, meaning humans are still in control of the AI-assisted system capabilities.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=OvlfCW3Ec1g&amp;amp;t=4082" title="Deep Questions Podcast Debunking AI Episode Thumbnail"&gt;&lt;img src="http://img.youtube.com/vi/OvlfCW3Ec1g/0.jpg" class="img-fluid" alt="Same Deep Questions Podcast Debunking AI Episode Thumbnail" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Later in the podcast, Cal takes a question about distributed webs of trust. I agree with Cal's point of using existing open standards like RSS for content consumption. It's the reason you often hear the phrase, &amp;quot;or wherever you get your podcasts&amp;quot;. Assuming you have a program that can read an RSS feed, you can follow all types of content. On the topic of discovery, Cal makes the suggestion of using distributed webs of trust. Using domain names and linking as ways of discovering content. While blogrolls were not directly called out, it's one of the benefits a curated set of links provides.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/deep-questions-debunking-genai-rss-blogroll?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/deep-questions-debunking-genai-rss-blogroll</guid>
      <pubDate>2024-06-24 20:40</pubDate>
      <category>#ai</category>
      <category>#deepquestions</category>
      <category>#rss</category>
      <category>#calnewport</category>
      <category>#podcast</category>
      <category>#blogroll</category>
      <category>#opml</category>
      <category>#social</category>
      <category>#socialmedia</category>
      <category>#distributedweb</category>
    </item>
    <item>
      <title>Affirmations for bloggers</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ntietz-blogging-affirmations?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ntietz-blogging-affirmations&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;h2&gt;The affirmations&lt;/h2&gt;
&lt;p&gt;Here are the things I've seen and learned. Each of these will be expanded in its own section.&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have things to write about.&lt;/li&gt;
&lt;li&gt;Your perspective matters.&lt;/li&gt;
&lt;li&gt;You are good enough.&lt;/li&gt;
&lt;li&gt;Posts don't have to be novel.&lt;/li&gt;
&lt;li&gt;People will read it.&lt;/li&gt;
&lt;li&gt;Mistakes are okay!&lt;/li&gt;
&lt;li&gt;It's okay to ask for things.&lt;/li&gt;
&lt;li&gt;You can get started quickly.&lt;/li&gt;
&lt;li&gt;You can write on a schedule.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src="https://i.imgflip.com/1otri4.jpg" class="img-fluid" alt="Meme of man pointing at himself in the mirror" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ntietz-blogging-affirmations?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ntietz-blogging-affirmations</guid>
      <pubDate>2024-06-11 21:42</pubDate>
      <category>#blogging</category>
      <category>#writing</category>
      <category>#personalweb</category>
      <category>#socialweb</category>
      <category>#indieweb</category>
    </item>
    <item>
      <title>The cassette tape is making a comeback</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/npr-cassette-tapes-making-comeback?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/npr-cassette-tapes-making-comeback&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Despite the odds, cassette tapes are making a comeback. And one family-owned company in Springfield, Missouri is a leader in the revival.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hopefully this also means that players are also making a comeback because you need something to play them on. Having visited a museum recently full of old radio recordings on cassette tapes, I was sad to find out I couldn't listen to them because the tape player was broken. It's hard enough finding a decent MP3 player, I'm sure it must be just as hard if not harder finding a tape player.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/npr-cassette-tapes-making-comeback?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/npr-cassette-tapes-making-comeback</guid>
      <pubDate>2024-06-11 21:31</pubDate>
      <category>#cassette</category>
      <category>#tapes</category>
      <category>#retro</category>
      <category>#music</category>
      <category>#comeback</category>
    </item>
    <item>
      <title>Deep Dive into Ownership in Mojo</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/deep-dive-ownership-mojo?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/deep-dive-ownership-mojo&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In the second part of the ownership series in Mojo, we built on the mental model developed in the &lt;a href="http://www.modular.com/blog/what-ownership-is-really-about-a-mental-model-approach"&gt;first part&lt;/a&gt; and provided practical examples to illustrate how ownership works in Mojo. We covered the different kinds of values (BValue, LValue, and RValue) and how they propagate through expressions. We also explained the function argument conventions (borrowed, inout, owned) and demonstrated how these conventions help manage memory safely and efficiently. We concluded with three fundamental rules:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Rule 1&lt;/strong&gt;: Owned arguments take RValue on the caller side but are LValue on the callee side.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rule 2&lt;/strong&gt;: Owned arguments own the type if the transfer operator ^ is used; otherwise, they copy the type if it is Copyable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rule 3&lt;/strong&gt;: Copy operations are optimized to move operations if the type is Copyable and Movable and isn‚Äôt used anymore, reducing unnecessary overhead.&lt;br /&gt;
&lt;br&gt;
Lastly, we emphasized that the main goals of ownership in Mojo are:&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory Safety&lt;/strong&gt;: Enforcing exclusive ownership and proper lifetimes to prevent memory errors such as use-after-free and double-free.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance Optimization&lt;/strong&gt;: Converting unnecessary copy operations into move operations to reduce overhead and enhance performance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ease of Use&lt;/strong&gt;: Automating memory management through ownership rules and the transfer operator, simplifying development.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compile-Time Guarantees&lt;/strong&gt;: Providing strong compile-time guarantees through type-checking and dataflow lifetime analysis, catching errors early in the development process.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/deep-dive-ownership-mojo?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/deep-dive-ownership-mojo</guid>
      <pubDate>2024-06-11 21:20</pubDate>
      <category>#mojo</category>
      <category>#python</category>
      <category>#c</category>
      <category>#cpp</category>
      <category>#c++</category>
      <category>#pl</category>
      <category>#programming</category>
      <category>#programminglanguage</category>
    </item>
    <item>
      <title>EFF Presents - The Encryptids - Bigfoot</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/eff-the-encryptids-bigfoot?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/eff-the-encryptids-bigfoot&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I‚Äôm proud to share the first post in a series from our friends, &lt;a href="https://www.eff.org/Encryptids"&gt;The Encryptids&lt;/a&gt;‚Äîthe rarely-seen enigmas who inspire campfire lore. But this time, they‚Äôre spilling secrets about how they survive this ever-digital world. We begin by checking in with the legendary Bigfoot de la Sasquatch...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;People say I'm the most famous of The Encryptids, but sometimes I don't want the spotlight. They all want a piece of me: exes, ad trackers, scammers, even the government. A picture may be worth a thousand words, but my digital profile is worth cash (to skeezy data brokers). I can‚Äôt hit a city block without being captured by doorbell cameras, CCTV, license plate readers, and a maze of street-level surveillance. It can make you want to give up on privacy altogether. Honey, no. Why should you have to hole up in some dank, busted forest for freedom and respect? You don‚Äôt.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Privacy isn't about hiding. It's about revealing what you want to who you want on your terms. It's your basic right to dignity.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/eff-the-encryptids-bigfoot?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/eff-the-encryptids-bigfoot</guid>
      <pubDate>2024-06-11 21:12</pubDate>
      <category>#eff</category>
      <category>#privacy</category>
      <category>#digitalrights</category>
      <category>#campaign</category>
    </item>
    <item>
      <title>Mapping the Mind of a Large Language Model</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mapping-mind-large-language-model-anthropic?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mapping-mind-large-language-model-anthropic&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we report a significant advance in understanding the inner workings of AI models. We have identified how millions of concepts are represented inside Claude Sonnet, one of our deployed large language models. This is the first ever detailed look inside a modern, production-grade large language model. This interpretability discovery could, in future, help us make AI models safer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Previously, we made some progress matching patterns of neuron activations, called features, to human-interpretable concepts. We used a technique called &amp;quot;dictionary learning&amp;quot;, borrowed from classical machine learning, which isolates patterns of neuron activations that recur across many different contexts. In turn, any internal state of the model can be represented in terms of a few active features instead of many active neurons. Just as every English word in a dictionary is made by combining letters, and every sentence is made by combining words, every feature in an AI model is made by combining neurons, and every internal state is made by combining features.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In October 2023, we reported success applying dictionary learning to a very small &amp;quot;toy&amp;quot; language model and found coherent features corresponding to concepts like uppercase text, DNA sequences, surnames in citations, nouns in mathematics, or function arguments in Python code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We used the same scaling law philosophy that predicts the performance of larger models from smaller ones to tune our methods at an affordable scale before launching on Sonnet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We successfully extracted millions of features from the middle layer of Claude 3.0 Sonnet, (a member of our current, state-of-the-art model family, currently available on claude.ai), providing a rough conceptual map of its internal states halfway through its computation. This is the first ever detailed look inside a modern, production-grade large language model.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mapping-mind-large-language-model-anthropic?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mapping-mind-large-language-model-anthropic</guid>
      <pubDate>2024-06-11 21:03</pubDate>
      <category>#ai</category>
      <category>#interpretability</category>
      <category>#anthropic</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Claude's Character</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/claudes-character-anthropic?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/claudes-character-anthropic&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Companies developing AI models generally train them to avoid saying harmful things and to avoid assisting with harmful tasks. The goal of this is to train models to behave in ways that are &amp;quot;harmless&amp;quot;. But when we think of the character of those we find genuinely admirable, we don‚Äôt just think of harm avoidance. We think about those who are curious about the world, who strive to tell the truth without being unkind, and who are able to see many sides of an issue without becoming overconfident or overly cautious in their views. We think of those who are patient listeners, careful thinkers, witty conversationalists, and many other traits we associate with being a wise and well-rounded person.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;AI models are not, of course, people. But as they become more capable, we believe we can‚Äîand should‚Äîtry to train them to behave well in this much richer sense. Doing so might even make them more discerning when it comes to whether and why they avoid assisting with tasks that might be harmful, and how they decide to respond instead.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Claude 3 was the first model where we added &amp;quot;character training&amp;quot; to our alignment finetuning process: the part of training that occurs after initial model training, and the part that turns it from a predictive text model into an AI assistant. The goal of character training is to make Claude begin to have more nuanced, richer traits like curiosity, open-mindedness, and thoughtfulness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Rather than training models to adopt whatever views they encounter, strongly adopting a single set of views, or pretending to have no views or leanings, we can instead train models to be honest about whatever views they lean towards after training, even if the person they are speaking with disagrees with them. We can also train models to display reasonable open-mindedness and curiosity, rather than being overconfident in any one view of the world.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In order to steer Claude‚Äôs character and personality, we made a list of many character traits we wanted to encourage the model to have...We don‚Äôt want Claude to treat its traits like rules from which it never deviates. We just want to nudge the model‚Äôs general behavior to exemplify more of those traits.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Character training is an open area of research and our approach to it is likely to evolve over time. It raises complex questions like whether AI models should have unique and coherent characters or should be more customizable, as well as what responsibilities we have when deciding which traits AI models should and shouldn‚Äôt have.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/claudes-character-anthropic?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/claudes-character-anthropic</guid>
      <pubDate>2024-06-11 20:56</pubDate>
      <category>#anthropic</category>
      <category>#claude</category>
      <category>#alignment</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Ultravox - An open, fast, and extensible multimodal LLM</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ultravox-multimodal-llm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ultravox-multimodal-llm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Ultravox is a new kind of multimodal LLM that can understand text as well as human speech, without the need for a separate Audio Speech Recognition (ASR) stage. Building on research like AudioLM, SeamlessM4T, Gazelle, SpeechGPT, and others, we've extended Meta's Llama 3 model with a multimodal projector that converts audio directly into the high-dimensional space used by Llama 3. This direct coupling allows Ultravox to respond much more quickly than systems that combine separate ASR and LLM components. In the future this will also allow Ultravox to natively understand the paralinguistic cues of timing and emotion that are omnipresent in human speech.&lt;br /&gt;
&lt;br&gt;
The current version of Ultravox (v0.1), when invoked with audio content, has a time-to-first-token (TTFT) of approximately 200ms, and a tokens-per-second rate of ~100, all using a Llama 3 8B backbone. While quite fast, we believe there is considerable room for improvement in these numbers. We look forward to working with LLM hosting providers to deliver state-of-the-art performance for Ultravox.&lt;br /&gt;
&lt;br&gt;
Ultravox currently takes in audio and emits streaming text. As we evolve the model, we'll train it to be able to emit a stream of speech tokens that can then be converted directly into raw audio by an appropriate unit vocoder. We're interested in working with interested parties to build this functionality!&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ultravox-multimodal-llm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ultravox-multimodal-llm</guid>
      <pubDate>2024-06-11 20:51</pubDate>
      <category>#ai</category>
      <category>#multimodal</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Apple - Private Cloud Compute</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/apple-private-cloud-compute?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/apple-private-cloud-compute&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We set out to build Private Cloud Compute with a set of core requirements:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Stateless computation on personal user data.&lt;/strong&gt; ...we want a strong form of stateless data processing where personal data leaves no trace in the PCC system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enforceable guarantee.&lt;/strong&gt; Security and privacy guarantees are strongest when they are entirely technically enforceable, which means it must be possible to constrain and analyze all the components that critically contribute to the guarantees of the overall Private Cloud Compute system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No privileged runtime access.&lt;/strong&gt; Private Cloud Compute must not contain privileged interfaces that would enable Apple‚Äôs site reliability staff to bypass PCC privacy guarantees, even when working to resolve an outage or other severe incident.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-targetability.&lt;/strong&gt; An attacker should not be able to attempt to compromise personal data that belongs to specific, targeted Private Cloud Compute users without attempting a broad compromise of the entire PCC system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Verifiable transparency.&lt;/strong&gt; Security researchers need to be able to verify, with a high degree of confidence, that our privacy and security guarantees for Private Cloud Compute match our public promises.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/apple-private-cloud-compute?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/apple-private-cloud-compute</guid>
      <pubDate>2024-06-11 20:42</pubDate>
      <category>#ai</category>
      <category>#pcc</category>
      <category>#privacy</category>
      <category>#cloud</category>
      <category>#privatecloudcompute</category>
      <category>#wwdc</category>
    </item>
    <item>
      <title>Introducing Apple‚Äôs On-Device and Server Foundation Models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-apple-on-device-server-foundational-models?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-apple-on-device-server-foundational-models&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Apple Intelligence is comprised of multiple highly-capable generative models that are specialized for our users‚Äô everyday tasks, and can adapt on the fly for their current activity. The foundation models built into Apple Intelligence have been fine-tuned for user experiences such as writing and refining text, prioritizing and summarizing notifications, creating playful images for conversations with family and friends, and taking in-app actions to simplify interactions across apps.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In the following overview, we will detail how two of these models ‚Äî a ~3 billion parameter on-device language model, and a larger server-based language model available with Private Cloud Compute and running on Apple silicon servers ‚Äî have been built and adapted to perform specialized tasks efficiently, accurately, and responsibly. These two foundation models are part of a larger family of generative models created by Apple to support users and developers; this includes a coding model to build intelligence into Xcode, as well as a diffusion model to help users express themselves visually, for example, in the Messages app.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Pre-training&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our foundation models are trained on Apple's AXLearn framework, an open-source project we released in 2023. It builds on top of JAX and XLA, and allows us to train the models with high efficiency and scalability on various training hardware and cloud platforms, including TPUs and both cloud and on-premise GPUs. We used a combination of data parallelism, tensor parallelism, sequence parallelism, and Fully Sharded Data Parallel (FSDP) to scale training along multiple dimensions such as data, model, and sequence length.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Post-training&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We find that data quality is essential to model success, so we utilize a hybrid data strategy in our training pipeline, incorporating both human-annotated and synthetic data, and conduct thorough data curation and filtering procedures. We have developed two novel algorithms in post-training: (1) a rejection sampling fine-tuning algorithm with teacher committee, and (2) a reinforcement learning from human feedback (RLHF) algorithm with mirror descent policy optimization and a leave-one-out advantage estimator. We find that these two algorithms lead to significant improvement in the model‚Äôs instruction-following quality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Optimization&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Both the on-device and server models use grouped-query-attention. We use shared input and output vocab embedding tables to reduce memory requirements and inference cost. These shared embedding tensors are mapped without duplications. The on-device model uses a vocab size of 49K, while the server model uses a vocab size of 100K, which includes additional language and technical tokens.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;For on-device inference, we use low-bit palletization, a critical optimization technique that achieves the necessary memory, power, and performance requirements. To maintain model quality, we developed a new framework using LoRA adapters that incorporates a mixed 2-bit and 4-bit configuration strategy ‚Äî averaging 3.5 bits-per-weight ‚Äî to achieve the same accuracy as the uncompressed models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Additionally, we use an interactive model latency and power analysis tool, Talaria, to better guide the bit rate selection for each operation. We also utilize activation quantization and embedding quantization, and have developed an approach to enable efficient Key-Value (KV) cache update on our neural engines.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Model Adaptation&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our foundation models are fine-tuned for users‚Äô everyday activities, and can dynamically specialize themselves on-the-fly for the task at hand...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We represent the values of the adapter parameters using 16 bits, and for the ~3 billion parameter on-device model, the parameters for a rank 16 adapter typically require 10s of megabytes. The adapter models can be dynamically loaded, temporarily cached in memory, and swapped ‚Äî giving our foundation model the ability to specialize itself on the fly for the task at hand while efficiently managing memory and guaranteeing the operating system's responsiveness.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Performance and Evaluation&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We compare our models with both open-source models (Phi-3, Gemma, Mistral, DBRX) and commercial models of comparable size (GPT-3.5-Turbo, GPT-4-Turbo)1. We find that our models are preferred by human graders over most comparable competitor models. On this benchmark, our on-device model, with ~3B parameters, outperforms larger models including Phi-3-mini, Mistral-7B, and Gemma-7B. Our server model compares favorably to DBRX-Instruct, Mixtral-8x22B, and GPT-3.5-Turbo while being highly efficient.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;To further evaluate our models, we use the Instruction-Following Eval (IFEval) benchmark to compare their instruction-following capabilities with models of comparable size. The results suggest that both our on-device and server model follow detailed instructions better than the open-source and commercial models of comparable size.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-apple-on-device-server-foundational-models?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-apple-on-device-server-foundational-models</guid>
      <pubDate>2024-06-11 20:26</pubDate>
      <category>#ai</category>
      <category>#apple</category>
      <category>#slm</category>
      <category>#edge</category>
      <category>#wwdc</category>
    </item>
    <item>
      <title>Introducing Apple Intelligence</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-apple-intelligence?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-apple-intelligence&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Apple today introduced Apple Intelligence, the personal intelligence system for iPhone, iPad, and Mac that combines the power of generative models with personal context to deliver intelligence that‚Äôs incredibly useful and relevant.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Apple Intelligence unlocks new ways for users to enhance their writing and communicate more effectively. With brand-new systemwide Writing Tools built into iOS 18, iPadOS 18, and macOS Sequoia, users can rewrite, proofread, and summarize text nearly everywhere they write...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Apple Intelligence powers exciting image creation capabilities to help users communicate and express themselves in new ways. With Image Playground, users can create fun images in seconds, choosing from three styles: Animation, Illustration, or Sketch...All images are created on device, giving users the freedom to experiment with as many images as they want.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Powered by Apple Intelligence, Siri becomes more deeply integrated into the system experience. With richer language-understanding capabilities, Siri is more natural, more contextually relevant, and more personal, with the ability to simplify and accelerate everyday tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A cornerstone of Apple Intelligence is on-device processing, and many of the models that power it run entirely on device. To run more complex requests that require more processing power, Private Cloud Compute extends the privacy and security of Apple devices into the cloud to unlock even more intelligence.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Apple is integrating ChatGPT access into experiences within iOS 18, iPadOS 18, and macOS Sequoia, allowing users to access its expertise ‚Äî as well as its image- and document-understanding capabilities ‚Äî without needing to jump between tools.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-apple-intelligence?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-apple-intelligence</guid>
      <pubDate>2024-06-11 20:16</pubDate>
      <category>#ai</category>
      <category>#apple</category>
      <category>#appleintelligence</category>
    </item>
    <item>
      <title>The Verge - Apple WWDC 2024 keynote in 18 minutes</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/verge-wwdc-2024-18-minute-recap-video?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/verge-wwdc-2024-18-minute-recap-video&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I always look forward to watching these recap videos from the Verge.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=sBXdyUA6A88" title="Thumbnail of Verge WWDC 2024 Keynote recap video"&gt;&lt;img src="http://img.youtube.com/vi/sBXdyUA6A88/0.jpg" class="img-fluid" alt="Thumbnail of Verge WWDC 2024 Keynote recap video" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/verge-wwdc-2024-18-minute-recap-video?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/verge-wwdc-2024-18-minute-recap-video</guid>
      <pubDate>2024-06-11 20:05</pubDate>
      <category>#apple</category>
      <category>#wwdc</category>
      <category>#keynote</category>
      <category>#wwdc2024</category>
      <category>#ai</category>
      <category>#ios</category>
      <category>#ipad</category>
      <category>#iphone</category>
      <category>#watchos</category>
      <category>#ipados</category>
      <category>#macos</category>
      <category>#visionos</category>
    </item>
    <item>
      <title>Andrej Karpathy - Let's reproduce GPT-2 (124M)</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/karpathy-reproduce-gpt-2-124m-tutorial?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/karpathy-reproduce-gpt-2-124m-tutorial&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=l8pRSuU81PU" title="YouTube thumbnail of Karpathy Let's Preproduce GPT video tutorial"&gt;&lt;img src="http://img.youtube.com/vi/l8pRSuU81PU/0.jpg" class="img-fluid" alt="YouTube thumbnail of Karpathy Let's Preproduce GPT video tutorial" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/karpathy-reproduce-gpt-2-124m-tutorial?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/karpathy-reproduce-gpt-2-124m-tutorial</guid>
      <pubDate>2024-06-11 19:59</pubDate>
      <category>#ai</category>
      <category>#gpt</category>
      <category>#transformer</category>
      <category>#tutorial</category>
    </item>
    <item>
      <title>TinyAgent: Function Calling at the Edge</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/TinyAgent-function-calling-edge?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/TinyAgent-function-calling-edge&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;img src="https://bair.berkeley.edu/static/blog/tiny-agent/Figure2.png" class="img-fluid" alt="High level conceptual diagram of TinyAgent system" /&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The ability of LLMs to execute commands through plain language (e.g. English) has enabled agentic systems that can complete a user query by orchestrating the right set of tools...recent multi-modal efforts such as the GPT-4o or Gemini-1.5 model, has expanded the realm of possibilities with AI agents. While this is quite exciting, the large model size and computational requirements of these models often requires their inference to be performed on the cloud. This can create several challenges for their widespread adoption. First and foremost, uploading data such as video, audio, or text documents to a third party vendor on the cloud, can result in privacy issues. Second, this requires cloud/Wi-Fi connectivity which is not always possible...latency could also be an issue as uploading large amounts of data to the cloud and waiting for the response could slow down response time, resulting in unacceptable time-to-solution. These challenges could be solved if we deploy the LLM models locally at the edge.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...current LLMs like GPT-4o or Gemini-1.5 are too large for local deployment. One contributing factor is that a lot of the model size ends up memorizing general information about the world into its parametric memory which may not be necessary for a specialized downstream application.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...this leads to an intriguing research question:&lt;br /&gt;
&lt;br&gt;
Can a smaller language model with significantly less parametric memory emulate such emergent ability of these larger language models?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Achieving this would significantly reduce the computational footprint of agentic systems and thus enable efficient and privacy-preserving edge deployment. Our study demonstrates that this is feasible for small language models through training with specialized, high-quality data that does not require recalling generic world knowledge.&lt;br /&gt;
&lt;br&gt;
Such a system could particularly be useful for semantic systems where the AI agent‚Äôs role is to understand the user query in natural language and, instead of responding with a ChatGPT-type question answer response, orchestrate the right set of tools and APIs to accomplish the user‚Äôs command. For example, in a Siri-like application, a user may ask a language model to create a calendar invite with particular attendees. If a predefined script for creating calendar items already exists, the LLM simply needs to learn how to invoke this script with the correct input arguments (such as attendees‚Äô email addresses, event title, and time). This process does not require recalling/memorization of world knowledge from sources like Wikipedia, but rather requires reasoning and learning to call the right functions and to correctly orchestrate them.&lt;br /&gt;
&lt;br&gt;
Our goal is to develop Small Language Models (SLM) that are capable of complex reasoning that could be deployed securely and privately at the edge. Here we will discuss the research directions that we are pursuing to that end. First, we discuss how we can enable small open-source models to perform accurate function calling, which is a key component of agentic systems. It turns out that off-the-shelf small models have very low function calling capabilities. We discuss how we address this by systematically curating high-quality data for function calling, using a specialized Mac assistant agent as our driving application. We then show that fine-tuning the model on this high quality curated dataset, can enable SLMs to even exceed GPT-4-Turbo‚Äôs function calling performance. We then show that this could be further improved and made efficient through a new Tool RAG method. Finally, we show how the final models could be deployed efficiently at the edge with real time responses.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/TinyAgent-function-calling-edge?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/TinyAgent-function-calling-edge</guid>
      <pubDate>2024-06-11 19:43</pubDate>
      <category>#agent</category>
      <category>#ai</category>
      <category>#slm</category>
      <category>#function</category>
      <category>#edge</category>
      <category>#local</category>
      <category>#languagemodel</category>
      <category>#smalllanguagemodel</category>
      <category>#research</category>
    </item>
    <item>
      <title>Tierra Whack - NPR Tiny Desk Concert</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tierra-whack-npr-tiny-desk-2024?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tierra-whack-npr-tiny-desk-2024&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Great performance.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=uRG6IWYVazM" title="Thumbnail of Tierra Whack NPR Tiny Desk Concert"&gt;&lt;img src="http://img.youtube.com/vi/uRG6IWYVazM/0.jpg" class="img-fluid" alt="Thumbnail of Tierra Whack NPR Tiny Desk Concert" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tierra-whack-npr-tiny-desk-2024?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tierra-whack-npr-tiny-desk-2024</guid>
      <pubDate>2024-06-08 11:55</pubDate>
      <category>#music</category>
      <category>#tinydesk</category>
      <category>#tierrawhack</category>
      <category>#hiphop</category>
      <category>#rap</category>
      <category>#livemusic</category>
      <category>#performance</category>
      <category>#npr</category>
    </item>
    <item>
      <title>Generative AI Handbook: A Roadmap for Learning Resources</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/genai-handbook-learning-resource-brown?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/genai-handbook-learning-resource-brown&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This document aims to serve as a handbook for learning the key concepts underlying modern artificial intelligence systems. Given the speed of recent development in AI, there really isn‚Äôt a good textbook-style source for getting up-to-speed on the latest-and-greatest innovations in LLMs or other generative models, yet there is an abundance of great explainer resources (blog posts, videos, etc.) for these topics scattered across the internet. My goal is to organize the ‚Äúbest‚Äù of these resources into a textbook-style presentation, which can serve as a roadmap for filling in the prerequisites towards individual AI-related learning goals. My hope is that this will be a ‚Äúliving document‚Äù, to be updated as new innovations and paradigms inevitably emerge, and ideally also a document that can benefit from community input and contribution. This guide is aimed at those with a technical background of some kind, who are interested in diving into AI either out of curiosity or for a potential career. I‚Äôll assume that you have some experience with coding and high-school level math, but otherwise will provide pointers for filling in any other prerequisites.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/genai-handbook-learning-resource-brown?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/genai-handbook-learning-resource-brown</guid>
      <pubDate>2024-06-06 22:18</pubDate>
      <category>#generativeai</category>
      <category>#genai</category>
      <category>#handbook</category>
      <category>#learning</category>
      <category>#resource</category>
    </item>
    <item>
      <title>Omakub - An Omakase Developer Setup for Ubuntu 24.04</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/omakub-ubuntu-developer-desktop?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/omakub-ubuntu-developer-desktop&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Cool project.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Turn a fresh Ubuntu installation into a fully-configured, beautiful, and modern web development system by running a single command.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Omakub is an opinionated take on what Linux can be at its best.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Omakub includes a curated set of applications and tools that one might discover through hours of watching YouTube, reading blogs, or just stumbling around Linux internet. All so someone coming straight from a platform like Windows or the Mac can immediately start enjoying a ready-made system, without having to do any configuration and curation legwork at all.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/omakub-ubuntu-developer-desktop?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/omakub-ubuntu-developer-desktop</guid>
      <pubDate>2024-06-06 22:14</pubDate>
      <category>#linux</category>
      <category>#developer</category>
      <category>#ubuntu</category>
      <category>#desktop</category>
      <category>#37signals</category>
    </item>
    <item>
      <title>My Permadomain</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/my-permadomain-rscottjones?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/my-permadomain-rscottjones&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I'm guilty of owning way too many domains but this short-hand (lqdev.me) and other permadomain (luisquintanilla.me) are the ones I use mostly.&lt;/p&gt;
&lt;p&gt;A potential happy medium I've seen is using subdomains (i.e. project.lqdev.me).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.jim-nielsen.com/2018/pathnames-to-subdomains/"&gt;Jim Nielsen has great examples&lt;/a&gt; of how he's doing that today. Though that's an older post, he also addressed it most recently in the post &lt;a href="https://blog.jim-nielsen.com/2023/domain-sins-of-my-youth/"&gt;Domain Sins of My Youth&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;That somehow feels better than the URL you get when you use free hosting like GitHub Pages. As a bonus, it's already tied to your identity (if using your personal domain). Also, you save some money.&lt;/p&gt;
&lt;p&gt;There's a few ways I practice this today. In addition to the previously mentioned domains, I also own lqdev.tech. Services and projects I host live there.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mastodon Instance (toot.lqdev.tech)&lt;/li&gt;
&lt;li&gt;Matrix server (matrix.lqdev.tech)&lt;/li&gt;
&lt;li&gt;Webmentions service (webmentions.lqdev.tech)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So far it's been working well.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/my-permadomain-rscottjones?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/my-permadomain-rscottjones</guid>
      <pubDate>2024-05-30 21:41</pubDate>
      <category>#domains</category>
      <category>#personalweb</category>
      <category>#permadomain</category>
      <category>#indieweb</category>
      <category>#projects</category>
      <category>#selfhosting</category>
      <category>#hosting</category>
      <category>#self-hosting</category>
    </item>
    <item>
      <title>Micro.blog - Podcast hosting for $5</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/microblog-podcast-hosting?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/microblog-podcast-hosting&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This is cool! The Micro.blog premium plan has tons of great features as well.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Six years ago, we launched our $10/month plan with podcast hosting. Since then we‚Äôve added several big features to the plan...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, I want to bring the podcast feature to more people, so we‚Äôre moving it down to the standard $5/month plan.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/microblog-podcast-hosting?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/microblog-podcast-hosting</guid>
      <pubDate>2024-05-30 21:27</pubDate>
      <category>#microblog</category>
      <category>#podcast</category>
      <category>#indieweb</category>
      <category>#hosting</category>
      <category>#blog</category>
      <category>#blogging</category>
    </item>
    <item>
      <title>Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/repro-gpt-2-llm-c-90-min-20-dollars-karpathy?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/repro-gpt-2-llm-c-90-min-20-dollars-karpathy&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the TLDR is that we're training a 12-layer GPT-2 (124M), from scratch, on 10B tokens of FineWeb, with max sequence length of 1024 tokens.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The 124M model is the smallest model in the GPT-2 series released by OpenAI in 2019, and is actually quite accessible today, even for the GPU poor. With llm.c, which is quite efficient at up to ~60% model flops utilization, reproducing this model on one 8X A100 80GB SXM node takes ~90 minutes. For example, on Lambda this node goes for ~$14/hr, so the total cost of reproducing this model today is about $20. You can train the model with a single GPU too, it would just take proportionally longer (e.g. ~4-24 hours depending on the GPU).&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/repro-gpt-2-llm-c-90-min-20-dollars-karpathy?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/repro-gpt-2-llm-c-90-min-20-dollars-karpathy</guid>
      <pubDate>2024-05-28 20:33</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#gpt</category>
      <category>#gpt2</category>
      <category>#llmc</category>
      <category>#c</category>
      <category>#slm</category>
    </item>
    <item>
      <title>21 Years of WordPress</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/21-years-of-wordpress?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/21-years-of-wordpress&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;21 years since Mike and I did the first release of WordPress, forking Michel‚Äôs work on b2/caf√©log.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I‚Äôve been thinking a lot about elements that made WordPress successful in its early years that we should keep in mind as we build this year and beyond. Here‚Äôs 11 opinions:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Simple things should be easy and intuitive, and complex things possible.&lt;br /&gt;
&lt;br&gt;...&lt;/li&gt;
&lt;li&gt;Wikis are amazing, and our documentation should be wiki-easy to edit.&lt;br /&gt;
&lt;br&gt;...&lt;/li&gt;
&lt;li&gt;It‚Äôs important that we all do support, go to meetups and events, anything we can to stay close to regular end-users of what we make.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Congrats to WordPress and the team on 21 years. I think the following are some good elements to keep building on.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/21-years-of-wordpress?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/21-years-of-wordpress</guid>
      <pubDate>2024-05-27 21:05</pubDate>
      <category>#wordpress</category>
      <category>#blogging</category>
      <category>#web</category>
      <category>#anniversary</category>
      <category>#opensource</category>
      <category>#openweb</category>
    </item>
    <item>
      <title>The Daylight DC1 is a $729 attempt to build a calmer computer</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/daylight-dc-1-tablet-computer?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/daylight-dc-1-tablet-computer&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Like many in the comments, I like the idea of this device and what it's aiming to do.&lt;/p&gt;
&lt;p&gt;The &amp;quot;LivePaper&amp;quot; display technology seems interesting.&lt;/p&gt;
&lt;p&gt;At $729 though, it seems a little steep. I paid a fraction of that for my &lt;a href="/uses"&gt;Onyx Boox Nova Air 2&lt;/a&gt; and I have most of the functionality of this device. As an e-reader, note-taking device, and tablet that can install any Android app, it works perfectly fine for my use cases and I'm very happy with it.&lt;/p&gt;
&lt;p&gt;Still, I'm interested in seeing whether this takes off or whether it ends up being another Rabbit R1 or Humane Ai Pin.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/daylight-dc-1-tablet-computer?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/daylight-dc-1-tablet-computer</guid>
      <pubDate>2024-05-26 16:48</pubDate>
      <category>#eink</category>
      <category>#digitalminimalism</category>
      <category>#daylight</category>
      <category>#tablet</category>
      <category>#technology</category>
      <category>#gadgets</category>
    </item>
    <item>
      <title>The IndieWeb's Next Stage</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/indieweb-next-steps-durnell?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/indieweb-next-steps-durnell&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;span&gt;üôã&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Great post.&lt;/p&gt;
&lt;p&gt;One of the challenges I've found, even when using appliances like those from hosting providers like &lt;a href="https://www.linode.com/marketplace/apps/linode/wordpress/"&gt;Linode&lt;/a&gt; or tools like &lt;a href="https://yunohost.org/"&gt;YunoHost&lt;/a&gt; is connecting them to your domain.&lt;/p&gt;
&lt;p&gt;Usually, that's specific to your domain name provider and usually a manual process.&lt;/p&gt;
&lt;p&gt;In cases where that's easy, you're often overcharged for the convenience. Compared to &amp;quot;free&amp;quot; social media and publishing websites, it makes using your own domain a less desirable option.&lt;/p&gt;
&lt;p&gt;Maybe advocating for a multi-staged approach like &lt;a href="https://indiewebify.me/"&gt;IndieWebify.Me&lt;/a&gt; could make the journey more approachable. More importantly, making it a journey, rather than a destination could help with meeting folks where they are and guide them closer towards their goals.&lt;/p&gt;
&lt;p&gt;In any case, the proposed list of goals seem like a great start towards helping people create their own place on the web.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/indieweb-next-steps-durnell?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/indieweb-next-steps-durnell</guid>
      <pubDate>2024-05-25 21:54</pubDate>
      <category>#indieweb</category>
      <category>#community</category>
      <category>#personalweb</category>
    </item>
    <item>
      <title>RIP Kabosu</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/rip-kabosu-doge?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/rip-kabosu-doge&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The face of one of the defining memes of the 2010s, the doge meme, died on Friday. Kabosu, the shiba inu with the knowing face that launched a million internet jokes, was 18 when she died&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/rip-kabosu-doge?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/rip-kabosu-doge</guid>
      <pubDate>2024-05-25 19:55</pubDate>
      <category>#shiba</category>
      <category>#doge</category>
      <category>#meme</category>
      <category>#dog</category>
    </item>
    <item>
      <title>Erykah Badu + Reggie Watts + Marc Rebillet</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/badu-watts-rebillet?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/badu-watts-rebillet&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I never knew I needed this in my life.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=AtR1yVmCCvw" title="Erykah Badu, Reggie Watts, Marc Rebillet Jamming"&gt;&lt;img src="http://img.youtube.com/vi/AtR1yVmCCvw/0.jpg" class="img-fluid" alt="Erykah Badu, Reggie Watts, Marc Rebillet Jamming" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/badu-watts-rebillet?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/badu-watts-rebillet</guid>
      <pubDate>2024-05-18 23:50</pubDate>
      <category>#badu</category>
      <category>#watts</category>
      <category>#rebillet</category>
      <category>#music</category>
    </item>
    <item>
      <title>One year with the Light Phone 2</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/one-year-retrospect-lightphone-2-verge?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/one-year-retrospect-lightphone-2-verge&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;img src="https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExZXVrN2kxamptbnZtdTVwZnM1ZndnOWFydnBlOTdja2FpMTI1cjdnZSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3o6ZtfZp8ID54YUne0/giphy.gif" class="img-fluid" alt="GIF image with man signing into microphone with caption don't call it a comeback" /&gt;&lt;/p&gt;
&lt;p&gt;Between this article and &lt;a href="/feed/dumbphone-boom-real"&gt;The Dumbphone Boom is Real&lt;/a&gt;, I'm seeing more publications on the subject. Far from a comeback, but still cool to see.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/one-year-retrospect-lightphone-2-verge?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/one-year-retrospect-lightphone-2-verge</guid>
      <pubDate>2024-04-28 20:43</pubDate>
      <category>#dumbphone</category>
      <category>#retro</category>
      <category>#technology</category>
      <category>#lightphone</category>
      <category>#minimalism</category>
      <category>#digitalminimalism</category>
    </item>
    <item>
      <title>New Album: Kamasi Washington - Fearless Movement</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/new-kamasi-washington-album-fearless-movement?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/new-kamasi-washington-album-fearless-movement&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;2024 just keeps getting better in terms of new music releases.&lt;/p&gt;
&lt;p&gt;Kamasi Washington has a new album coming out this week called Fearless Movement.&lt;/p&gt;
&lt;p&gt;The most recent single, Dream State, with Andre 3000 is great.&lt;/p&gt;
&lt;iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/1nLU0aUyHaza8hZm1Jp6lu?utm_source=generator" width="100%" height="152" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" loading="lazy"&gt;&lt;/iframe&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/new-kamasi-washington-album-fearless-movement?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/new-kamasi-washington-album-fearless-movement</guid>
      <pubDate>2024-04-28 20:34</pubDate>
      <category>#newmusic</category>
      <category>#kamasiwashington</category>
      <category>#jazz</category>
      <category>#album</category>
      <category>#music</category>
    </item>
    <item>
      <title>The Dumbphone Boom Is Real</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/dumbphone-boom-real?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/dumbphone-boom-real&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Will Stults spent too much time on his iPhone, doom-scrolling the site formerly known as Twitter and tweeting angrily at Elon Musk as if the billionaire would actually notice. Stults‚Äôs partner, Daisy Krigbaum, was addicted to Pinterest and YouTube, bingeing videos on her iPhone before going to sleep. Two years ago, they both tried Apple‚Äôs Screen Time restriction tool and found it too easy to disable, so the pair decided to trade out their iPhones for more low-tech devices. They‚Äôd heard about so-called dumbphones, which lacked the kinds of bells and whistles‚Äîa high-resolution screen, an app store, a video camera‚Äîthat made smartphones so addictive. But they found the process of acquiring one hard to navigate. ‚ÄúThe information on it was kind of disparate and hard to get to. A lot of people who know the most about dumbphones spend the least time online,‚Äù Krigbaum said. A certain irony presented itself: figuring out a way to be less online required aggressive online digging.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The growing dumbphone fervor may be motivated, in part, by the discourse around child safety online. Parents are increasingly confronted with evidence that sites like Instagram and TikTok intentionally try to hook their children. Using those sites can increase teens‚Äô anxiety and lower their self-esteem, according to some studies, and smartphones make it so that kids are logged on constantly. Why should this situation be any healthier for adults? After almost two decades with iPhones, the public seems to be experiencing a collective ennui with digital life. So many hours of each day are lived through our portable, glowing screens, but the Internet isn‚Äôt even fun anymore. We lack the self-control to wean ourselves off, so we crave devices that actively prevent us from getting sucked into them. That means opting out of the prevailing technology and into what Cal Newport, a contributing writer for The New Yorker, has called a more considered ‚Äúdigital minimalism.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While dumbphones aren't a cure-all for unhealthy technology habits, as a dumbphone user, I can relate to the frustrations that come from the lack of device availability and support. Even when new devices hit the market, they tend to be targeted towards non-US markets.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/dumbphone-boom-real?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/dumbphone-boom-real</guid>
      <pubDate>2024-04-25 23:38</pubDate>
      <category>#dumbphone</category>
      <category>#technology</category>
      <category>#culture</category>
      <category>#digitalminimalism</category>
      <category>#minimalism</category>
    </item>
    <item>
      <title>Introducing Snowflake Arctic</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/snowflake-arctic-enterprise-ai-llm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/snowflake-arctic-enterprise-ai-llm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, the Snowflake AI Research Team is thrilled to introduce Snowflake Arctic, a top-tier enterprise-focused LLM that pushes the frontiers of cost-effective training and openness. Arctic is efficiently intelligent and truly open.&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently Intelligent: Arctic excels at enterprise tasks such as SQL generation, coding and instruction following benchmarks even when compared to open source models trained with significantly higher compute budgets. In fact, it sets a new baseline for cost-effective training to enable Snowflake customers to create high-quality custom models for their enterprise needs at a low cost.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Truly Open: Apache 2.0 license provides ungated access to weights and code. In addition, we are also open sourcing all of our data recipes and research insights.&lt;br /&gt;
&lt;br&gt;
Snowflake Arctic is available from Hugging Face, NVIDIA API catalog and Replicate today or via your model garden or catalog of choice, including Snowflake Cortex, Amazon Web Services (AWS), Microsoft Azure, Lamini, Perplexity and Together over the coming days.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/snowflake-arctic-enterprise-ai-llm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/snowflake-arctic-enterprise-ai-llm</guid>
      <pubDate>2024-04-25 23:34</pubDate>
      <category>#snowflake</category>
      <category>#ai</category>
      <category>#llm</category>
      <category>#enterprise</category>
    </item>
    <item>
      <title>‚ÄòPBS Retro‚Äô is coming to Roku as a FAST channel</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/pbs-retro-roku-channel?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/pbs-retro-roku-channel&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;PBS is making child edutainment classics like Zoboomafoo, Mister Rogers‚Äô Neighborhood, and Reading Rainbow available for free on a new ‚ÄòPBS Retro‚Äô channel on Roku.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is cool! Although I'm kind of bummed the stuff I grew up watching is now considered &amp;quot;retro&amp;quot;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/pbs-retro-roku-channel?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/pbs-retro-roku-channel</guid>
      <pubDate>2024-04-25 23:31</pubDate>
      <category>#pbs</category>
      <category>#roku</category>
      <category>#tv</category>
      <category>#fast</category>
      <category>#retro</category>
      <category>#education</category>
    </item>
    <item>
      <title>We Need To Rewild The Internet</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/rewild-the-internet-farrell-berjon?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/rewild-the-internet-farrell-berjon&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The internet has become an extractive and fragile monoculture. But we can revitalize it using lessons learned by ecologists.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our online spaces are not ecosystems, though tech firms love that word. They‚Äôre plantations; highly concentrated and controlled environments...&lt;br /&gt;
&lt;br&gt;
We all know this. We see it each time we reach for our phones. But what most people have missed is how this concentration reaches deep into the internet‚Äôs infrastructure ‚Äî the pipes and protocols, cables and networks, search engines and browsers. These structures determine how we build and use the internet, now and in the future.&lt;br /&gt;
&lt;br&gt;
They‚Äôve concentrated into a series of near-planetary duopolies.&lt;br /&gt;
&lt;br&gt;
Two kinds of everything may be enough to fill a fictional ark and repopulate a ruined world, but can‚Äôt run an open, global ‚Äúnetwork of networks‚Äù where everyone has the same chance to innovate and compete.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The internet made the tech giants possible. Their services have scaled globally, via its open, interoperable core. But for the past decade, they‚Äôve also worked to enclose the varied, competing and often open-source or collectively provided services the internet is built on into their proprietary domains. Although this improves their operational efficiency, it also ensures that the flourishing conditions of their own emergence aren‚Äôt repeated by potential competitors. For tech giants, the long period of open internet evolution is over. Their internet is not an ecosystem. It‚Äôs a zoo.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Up close, internet concentration seems too intricate to untangle; from far away, it seems too difficult to deal with. But what if we thought of the internet not as a doomsday ‚Äúhyperobject,‚Äù but as a damaged and struggling ecosystem facing destruction? What if we looked at it not with helpless horror at the eldritch encroachment of its current controllers, but with compassion, constructiveness and hope?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Rewilding ‚Äúaims to restore healthy ecosystems by creating wild, biodiverse spaces,‚Äù according to the International Union for Conservation of Nature. More ambitious and risk-tolerant than traditional conservation, it targets entire ecosystems to make space for complex food webs and the emergence of unexpected interspecies relations. It‚Äôs less interested in saving specific endangered species. Individual species are just ecosystem components, and focusing on components loses sight of the whole. Ecosystems flourish through multiple points of contact between their many elements, just like computer networks. And like in computer networks, ecosystem interactions are multifaceted and generative.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Whatever we do, the internet isn‚Äôt returning to old-school then-common interfaces like FTP and Gopher, or organizations operating their own mail servers again instead of off-the-shelf solutions like G-Suite. But some of what we need is already here, especially on the web. Look at the resurgence of RSS feeds, email newsletters and blogs, as we discover (yet again) that relying on one app to host global conversations creates a single point of failure and control. New systems are growing, like the Fediverse with its federated islands, or Bluesky with algorithmic choice and composable moderation.&lt;br /&gt;
&lt;br&gt;
We don‚Äôt know what the future holds. Our job is to keep open as much opportunity as we can, trusting that those who come later will use it. Instead of setting purity tests for which kind of internet is most like the original, we can test changes against the values of the original design. Do new standards protect the network‚Äôs ‚Äúgenerality,‚Äù i.e. its ability to support multiple uses, or is functionality limited to optimize efficiency for the biggest tech firms?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...our internet took off because it was designed as a general-purpose network, built to connect anyone.&lt;br /&gt;
&lt;br&gt;
Our internet was built to be complex and unbiddable, to do things we cannot yet imagine.&lt;br /&gt;
&lt;br&gt;
Internet infrastructure is a degraded ecosystem, but it‚Äôs also a built environment, like a city. Its unpredictability makes it generative, worthwhile and deeply human.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We need to stop thinking of internet infrastructure as too hard to fix. It‚Äôs the underlying system we use for nearly everything we do.&lt;br /&gt;
&lt;br&gt;
Rewilding the internet connects and grows what people are doing across regulation, standards-setting and new ways of organizing and building infrastructure, to tell a shared story of where we want to go. It‚Äôs a shared vision with many strategies. The instruments we need to shift away from extractive technological monocultures are at hand or ready to be built.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/rewild-the-internet-farrell-berjon?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/rewild-the-internet-farrell-berjon</guid>
      <pubDate>2024-04-25 23:11</pubDate>
      <category>#internet</category>
      <category>#technology</category>
      <category>#openweb</category>
      <category>#personalweb</category>
      <category>#online</category>
    </item>
    <item>
      <title>Calculus Made Easy</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/calculus-made-easy?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/calculus-made-easy&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Calculus Made Easy is a book on calculus originally published in 1910 by Silvanus P. Thompson, considered a classic and elegant introduction to the subject.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.gutenberg.org/ebooks/33283"&gt;Project Gutenberg PDF&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/calculus-made-easy?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/calculus-made-easy</guid>
      <pubDate>2024-04-25 23:01</pubDate>
      <category>#calculus</category>
      <category>#book</category>
      <category>#math</category>
    </item>
    <item>
      <title>The Internet Used To Be Fun</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/internet-used-to-be-fun-kwon?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/internet-used-to-be-fun-kwon&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I‚Äôve been meaning to write some kind of Important Thinkpiece‚Ñ¢ on the glory days of the early internet, but every time I sit down to do it, I find another, better piece that someone else has already written. So for now, here‚Äôs a collection of articles that to some degree answer the question ‚ÄúWhy have a personal website?‚Äù with ‚ÄúBecause it‚Äôs fun, and the internet used to be fun.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a great catalog of posts about the personal web. Courtesy of &lt;a href="https://kwon.nyc/"&gt;Rachel Kwon&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/internet-used-to-be-fun-kwon?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/internet-used-to-be-fun-kwon</guid>
      <pubDate>2024-04-25 22:56</pubDate>
      <category>#internet</category>
      <category>#indieweb</category>
      <category>#personalweb</category>
      <category>#online</category>
      <category>#blogging</category>
      <category>#blogs</category>
    </item>
    <item>
      <title>Grow Your Own Services</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/grow-your-own-services?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/grow-your-own-services&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This is a site encouraging non-technical people and organisations to create their own online services such as websites, social networks, personal clouds, instant messaging etc.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/grow-your-own-services?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/grow-your-own-services</guid>
      <pubDate>2024-04-25 22:53</pubDate>
      <category>#selfhosting</category>
      <category>#indieweb</category>
      <category>#openweb</category>
      <category>#internet</category>
      <category>#community</category>
      <category>#cloud</category>
      <category>#messaging</category>
      <category>#website</category>
      <category>#socialnetworks</category>
      <category>#websites</category>
      <category>#online</category>
    </item>
    <item>
      <title>SAMMO: A general-purpose framework for prompt optimization</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/sammo-prompt-optimization-framework?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/sammo-prompt-optimization-framework&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large language models (LLMs) have revolutionized a wide range of tasks and applications that were previously reliant on manually crafted machine learning (ML) solutions, streamlining through automation. However, despite these advances, a notable challenge persists: the need for extensive prompt engineering to adapt these models to new tasks. New generations of language models like GPT-4 and Mixtral 8x7B advance the capability to process long input texts. This progress enables the use of longer inputs, providing richer context and detailed instructions to language models. A common technique that uses this enhanced capacity is the Retrieval Augmented Generation (RAG) approach. RAG dynamically incorporates information into the prompt based on the specific input example.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;To address these challenges, we developed the Structure-Aware Multi-objective Metaprompt Optimization (SAMMO) framework. SAMMO is a new open-source tool that streamlines the optimization of prompts, particularly those that combine different types of structural information like in the RAG example above. It can make structural changes, such as removing entire components or replacing them with different ones. These features enable AI practitioners and researchers to efficiently refine their prompts with little manual effort.&lt;br /&gt;
&lt;br&gt;
Central to SAMMO‚Äôs innovation is its approach to treating prompts not just as static text inputs but as dynamic, programmable entities‚Äîmetaprompts. SAMMO represents these metaprompts as function graphs, where individual components and substructures can be modified to optimize performance, similar to the optimization process that occurs during traditional program compilation.&lt;br /&gt;
&lt;br&gt;
The following key features contribute to SAMMO‚Äôs effectiveness:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Structured optimization: Unlike current methods that focus on text-level changes, SAMMO focuses on optimizing the structure of metaprompts. This granular approach facilitates precise modifications and enables the straightforward integration of domain knowledge, for instance, through rewrite operations targeting specific stylistic objectives.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Multi-objective search: SAMMO‚Äôs flexibility enables it to simultaneously address multiple objectives, such as improving accuracy and computational efficiency. Our paper illustrates how SAMMO can be used to compress prompts without compromising their accuracy.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;General purpose application: SAMMO has proven to deliver significant performance improvements across a variety of tasks, including instruction tuning, RAG, and prompt compression.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/sammo-prompt-optimization-framework?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/sammo-prompt-optimization-framework</guid>
      <pubDate>2024-04-25 22:48</pubDate>
      <category>#ai</category>
      <category>#microsoft</category>
      <category>#research</category>
      <category>#prompt</category>
      <category>#optimization</category>
      <category>#sammo</category>
    </item>
    <item>
      <title>Remembering the 90's Video Store</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/remembering-90s-video-store-plummer?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/remembering-90s-video-store-plummer&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Reading this post and attached image brings back so many memories.&lt;/p&gt;
&lt;p&gt;I remember in the last few years of Blockbuster and early days of Netflix, for about $20/month you could rent unlimited movies (I think it was up to three at a time).&lt;/p&gt;
&lt;p&gt;If you binge watched them or weren't happy with the choices you made, you could just drive down to your local store, return them, and grab a new set of movies.&lt;/p&gt;
&lt;p&gt;Initially you had a return period, but towards the end, there were none so you basically got to keep some movies as long as you liked.&lt;/p&gt;
&lt;p&gt;Good times.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/remembering-90s-video-store-plummer?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/remembering-90s-video-store-plummer</guid>
      <pubDate>2024-04-25 22:31</pubDate>
      <category>#nostalgia</category>
      <category>#90s</category>
      <category>#videos</category>
      <category>#blockbuster</category>
    </item>
    <item>
      <title>Ghost is federating over ActivityPub</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ghost-activitypub-integration-announcement?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ghost-activitypub-integration-announcement&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In 2024, Ghost is adopting ActivityPub and connecting with other federated platforms across the web.&lt;br /&gt;
&lt;br&gt;
This means that, soon, Ghost publishers will be able to follow, like and interact with one another in the same way that you would normally do on a social network ‚Äî but on your own website.&lt;br /&gt;
&lt;br&gt;
The difference, of course, is that you‚Äôll also be able to follow, like, and interact with users on Mastodon, Threads, Flipboard, Buttondown, WriteFreely, Tumblr, WordPress, PeerTube, Pixelfed... or any other platform that has adopted ActivityPub, too. You don‚Äôt need to limit yourself to following people who happen to use the same platform as you.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;For the past few years the choice has been difficult. Either participate in closed networks at the mercy of algorithms, or set up an independent website at the expense of your growth.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Email gave us private messaging technology that isn‚Äôt owned by a single company.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;ActivityPub is doing the same for social technology.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The open web is coming back, and with it returns diversity. You can both publish independently and grow faster than ever before with followers from all over the world &amp;amp; the web.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I can't express how much I love this. Personally I don't use Ghost, but given platforms like WordPress and now Ghost are adding support for ActivityPub, it empowers people to build their own platforms.&lt;/p&gt;
&lt;p&gt;That said, this still doesn't address the challenges of building your own website, which as mentioned in the post are one of the appealing aspects of current closed networks.&lt;/p&gt;
&lt;p&gt;Still though, there is a vast number of creators, businesses, and company websites or blogs that can benefit from this today. When paired with RSS, it gives people choice and autonomy in how they create and consume content as I mentioned in a previous post, &lt;a href="/posts/rediscovering-rss-user-freedom"&gt;Rediscovering the RSS protocol&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ghost-activitypub-integration-announcement?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ghost-activitypub-integration-announcement</guid>
      <pubDate>2024-04-25 22:11</pubDate>
      <category>#ghost</category>
      <category>#openweb</category>
      <category>#activitypub</category>
      <category>#fediverse</category>
      <category>#blogging</category>
      <category>#websites</category>
      <category>#indieweb</category>
    </item>
    <item>
      <title>Google Penzai</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/google-jax-penzai?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/google-jax-penzai&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A JAX research toolkit for building, editing, and visualizing neural networks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Penzai is a JAX library for writing models as legible, functional pytree data structures, along with tools for visualizing, modifying, and analyzing them. Penzai focuses on making it easy to do stuff with models after they have been trained, making it a great choice for research involving reverse-engineering or ablating model components, inspecting and probing internal activations, performing model surgery, debugging architectures, and more. (But if you just want to build and train a model, you can do that too!)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://github.com/google-deepmind/penzai/raw/main/docs/_static/readme_teaser.png" class="img-fluid" alt="Screenshot of Google Penzai Neural Network Visualization" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;github.com&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/google-jax-penzai?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/google-jax-penzai</guid>
      <pubDate>2024-04-25 22:06</pubDate>
      <category>#google</category>
      <category>#jax</category>
      <category>#ai</category>
      <category>#visualization</category>
    </item>
    <item>
      <title>Introducing Phi-3</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-phi-3?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-phi-3&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Starting today, Phi-3-mini, a 3.8B language model is available on Microsoft Azure AI Studio, Hugging Face, and Ollama.&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Phi-3-mini is available in two context-length variants‚Äî4K and 128K tokens. It is the first model in its class to support a context window of up to 128K tokens, with little impact on quality.&lt;/li&gt;
&lt;li&gt;It is instruction-tuned, meaning that it‚Äôs trained to follow different types of instructions reflecting how people normally communicate. This ensures the model is ready to use out-of-the-box.&lt;/li&gt;
&lt;li&gt;It is available on Azure AI to take advantage of the deploy-eval-finetune toolchain, and is available on Ollama for developers to run locally on their laptops.&lt;/li&gt;
&lt;li&gt;It has been optimized for ONNX Runtime with support for Windows DirectML along with cross-platform support across graphics processing unit (GPU), CPU, and even mobile hardware.&lt;/li&gt;
&lt;li&gt;It is also available as an NVIDIA NIM microservice with a standard API interface that can be deployed anywhere. And has been optimized for NVIDIA GPUs.&lt;br /&gt;
&lt;br&gt;
In the coming weeks, additional models will be added to Phi-3 family to offer customers even more flexibility across the quality-cost curve. Phi-3-small (7B) and Phi-3-medium (14B) will be available in the Azure AI model catalog and other model gardens shortly.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-phi-3?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-phi-3</guid>
      <pubDate>2024-04-23 22:47</pubDate>
      <category>#microsoft</category>
      <category>#phi3</category>
      <category>#ai</category>
      <category>#slm</category>
      <category>#llm</category>
      <category>#genai</category>
    </item>
    <item>
      <title>Introducing Llama 3</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-llama-3?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-llama-3&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Build the future of AI with Meta Llama 3&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Now available with both 8B and 70B pretrained and instruction-tuned versions to support a wide range of applications&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Llama 3 models take data and scale to new heights. It‚Äôs been trained on our two recently announced custom-built 24K GPU clusters on over 15T token of data ‚Äì a training dataset 7x larger than that used for Llama 2, including 4x more code. This results in the most capable Llama model yet, which supports a 8K context length that doubles the capacity of Llama 2.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-llama-3?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-llama-3</guid>
      <pubDate>2024-04-18 21:01</pubDate>
      <category>#meta</category>
      <category>#ai</category>
      <category>#llama3</category>
      <category>#llama</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Rock AlterLatino from Radio Bilingue</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/radio-bilingue-rock-alterlatino?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/radio-bilingue-rock-alterlatino&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;While listening to &lt;a href="https://891khol.org/"&gt;KHOL&lt;/a&gt; earlier today, they were rebroadcasting a recording of &lt;a href="http://radiobilingue.org/rb-programas/alterlatino/"&gt;A Todo Pulmon&lt;/a&gt;, a radio show from &lt;a href="radiobilingue.org/"&gt;Radio Bilingue&lt;/a&gt; in Fresno, CA. Good stuff.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/radio-bilingue-rock-alterlatino?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/radio-bilingue-rock-alterlatino</guid>
      <pubDate>2024-04-14 15:18</pubDate>
      <category>#radio</category>
      <category>#music</category>
      <category>#rock</category>
      <category>#alternative</category>
      <category>#fresno</category>
      <category>#radiobilingue</category>
      <category>#khol</category>
    </item>
    <item>
      <title>Proton and Standard Notes are joining forces </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/proton-acquires-standard-notes?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/proton-acquires-standard-notes&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...today, we‚Äôre happy to announce that Standard Notes will also join us to advance our shared mission.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Both Proton and Standard Notes share a strong commitment to our communities, so Standard Notes will remain open source, freely available, and fully supported. Prices are not changing, and if you have a current subscription to Standard Notes, it will continue to be honored. Proton aspires to do the right thing and be a responsible home for open-source projects, and just as we did with SimpleLogin, we are committed to preserving what makes Standard Notes special and much loved.&lt;br /&gt;
&lt;br&gt;
In the coming months, we hope to find ways to make Standard Notes more easily accessible to the Proton community. This way, in addition to protecting your email, calendar, files, passwords, and online activity, you can also protect your notes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is another exciting acquisition! I mainly use org-mode in Emacs for note taking. However, I love the ecosystem Proton is building with their security and privacy focused set of collaborative software offerings.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/proton-acquires-standard-notes?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/proton-acquires-standard-notes</guid>
      <pubDate>2024-04-10 22:35</pubDate>
      <category>#proton</category>
      <category>#standardnotes</category>
      <category>#notetaking</category>
      <category>#security</category>
      <category>#privacy</category>
      <category>#email</category>
      <category>#notes</category>
    </item>
    <item>
      <title>Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/griffin-mix-linear-recurrence-local-attention-language-models?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/griffin-mix-linear-recurrence-local-attention-language-models&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/griffin-mix-linear-recurrence-local-attention-language-models?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/griffin-mix-linear-recurrence-local-attention-language-models</guid>
      <pubDate>2024-04-10 22:02</pubDate>
      <category>#griffin</category>
      <category>#ai</category>
      <category>#research</category>
      <category>#architecture</category>
      <category>#rnn</category>
      <category>#attention</category>
      <category>#transformers</category>
      <category>#llm</category>
    </item>
    <item>
      <title>DE-COP: Detecting Copyrighted Content in Language Models Training Data</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/decop-detecting-copyright-llm-training-data?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/decop-detecting-copyright-llm-training-data&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give ‚âà 4% accuracy. Our code and datasets are available at this https URL&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/avduarte333/DE-COP_Method"&gt;Repo&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/decop-detecting-copyright-llm-training-data?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/decop-detecting-copyright-llm-training-data</guid>
      <pubDate>2024-04-10 22:00</pubDate>
      <category>#ai</category>
      <category>#copyright</category>
      <category>#research</category>
      <category>#llm</category>
      <category>#data</category>
    </item>
    <item>
      <title>Using LLM to select the right SQL Query from candidates</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/llm-text-to-sql-right-candidates?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/llm-text-to-sql-right-candidates&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Text-to-SQL models can generate a list of candidate SQL queries, and the best query is often in the candidate list, but not at the top of the list. An effective re-rank method can select the right SQL query from the candidate list and improve the model's performance. Previous studies on code generation automatically generate test cases and use them to re-rank candidate codes. However, automatic test case generation for text-to-SQL is an understudied field. We propose an automatic test case generation method that first generates a database and then uses LLMs to predict the ground truth, which is the expected execution results of the ground truth SQL query on this database. To reduce the difficulty for LLMs to predict, we conduct experiments to search for ways to generate easy databases for LLMs and design easy-to-understand prompts. Based on our test case generation method, we propose a re-rank method to select the right SQL query from the candidate list. Given a candidate list, our method can generate test cases and re-rank the candidate list according to their pass numbers on these test cases and their generation probabilities. The experiment results on the validation dataset of Spider show that the performance of some state-of-the-art models can get a 3.6% improvement after applying our re-rank method.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/llm-text-to-sql-right-candidates?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/llm-text-to-sql-right-candidates</guid>
      <pubDate>2024-04-10 21:59</pubDate>
      <category>#ai</category>
      <category>#research</category>
      <category>#sql</category>
      <category>#llm</category>
    </item>
    <item>
      <title>RecurrentGemma - Open weights language model from Google DeepMind, based on Griffin.</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/google-deepmind-recurrent-gemma?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/google-deepmind-recurrent-gemma&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;RecurrentGemma is a family of open-weights Language Models by Google DeepMind, based on the novel Griffin architecture. This architecture achieves fast inference when generating long sequences by replacing global attention with a mixture of local attention and linear recurrences.&lt;br /&gt;
&lt;br&gt;
This repository contains the model implementation and examples for sampling and fine-tuning. We recommend most users adopt the Flax implementation, which is highly optimized. We also provide an un-optimized PyTorch implementation for reference.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/google-deepmind-recurrent-gemma?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/google-deepmind-recurrent-gemma</guid>
      <pubDate>2024-04-10 21:57</pubDate>
      <category>#ai</category>
      <category>#gemma</category>
      <category>#google</category>
      <category>#llm</category>
      <category>#opensource</category>
      <category>#slm</category>
      <category>#griffin</category>
      <category>#neuralnetwork</category>
    </item>
    <item>
      <title>Hello OLMo: A truly open LLM</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/allen-ai-olmo-llm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/allen-ai-olmo-llm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, The Allen Institute for AI (AI2) has released OLMo 7B, a truly open, state-of-the-art large language model released alongside the pre-training data and training code. This empowers researchers and developers to use the best and open models to advance the science of language models collectively.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://huggingface.co/allenai/OLMo-7B"&gt;OLMo-7B on HuggingFace&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/allen-ai-olmo-llm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/allen-ai-olmo-llm</guid>
      <pubDate>2024-04-09 22:17</pubDate>
      <category>#allenai</category>
      <category>#llm</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>LLM training in simple, raw C/CUDA </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/llm-c-karpathy?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/llm-c-karpathy&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;LLM training in simple, pure C/CUDA. There is no need for 245MB of PyTorch or 107MB of cPython. For example, training GPT-2 (CPU, fp32) is ~1,000 lines of clean code in a single file. It compiles and runs instantly, and exactly matches the PyTorch reference implementation. I chose GPT-2 as the first working example because it is the grand-daddy of LLMs, the first time the modern stack was put together.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/llm-c-karpathy?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/llm-c-karpathy</guid>
      <pubDate>2024-04-09 22:15</pubDate>
      <category>#llm</category>
      <category>#gpt</category>
      <category>#c</category>
      <category>#programming</category>
      <category>#learning</category>
      <category>#tutorial</category>
    </item>
    <item>
      <title>Beeper was just acquired by Automattic</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/beeper-acquired-automattic?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/beeper-acquired-automattic&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Beeper, the upstart messaging app that attempts to corral all your messaging services into one inbox, is being acquired by Automattic, the giant that runs Wordpress.com, Tumblr, and a number of other hugely popular web properties&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is exciting especially given some of the recent developments in the EU. What's most interesting to me is how Beeper leverages open protocols like Matrix and for bridging capabilities where possible to provide secure messaging.&lt;/p&gt;
&lt;p&gt;With more people moving to smaller spaces to communicate with their communities, being able to do so in a single place without everyone being on the same platform like in the early days of the internet is a welcome development.&lt;/p&gt;
&lt;p&gt;Additional coverage from the &lt;a href="https://blog.beeper.com/2024/04/09/beeper-is-now-available/"&gt;Beeper blog&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;What we‚Äôre announcing today‚Ä¶&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No more waitlist ‚Äì Beeper is now available to everyone!&lt;/li&gt;
&lt;li&gt;Beeper has been acquired by Automattic&lt;/li&gt;
&lt;li&gt;Our new Android app is out of beta&lt;/li&gt;
&lt;li&gt;We‚Äôre renaming Beeper Cloud ‚Üí Beeper (sorry for the confusion)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;and &lt;a href="https://ma.tt/2024/04/beeper-texts/"&gt;Matt Mullenweg's blog&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today the announcement went out that we‚Äôre combining the best technology from Beeper and Texts to create a great private, secure, and open source messaging client for people to have control of their communications. We‚Äôre going to use the Beeper brand, because it‚Äôs fun. This is not unlike how browsers have evolved, where solid tech and encryption on top of an open ecosystem has created untold value for humanity.&lt;br /&gt;
&lt;br&gt;
A lot of people are asking about iMessage on Android‚Ä¶ I have zero interest in fighting with Apple, I think instead it‚Äôs best to focus on messaging networks that want more engagement from power-user clients. This is an area I‚Äôm excited to work on when I return from my sabbatical next month.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/beeper-acquired-automattic?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/beeper-acquired-automattic</guid>
      <pubDate>2024-04-09 22:06</pubDate>
      <category>#beeper</category>
      <category>#messaging</category>
      <category>#automattic</category>
      <category>#matrix</category>
    </item>
    <item>
      <title>ARAGOG: Advanced RAG Output Grading</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/aragog-advanced-rag-output-grading?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/aragog-advanced-rag-output-grading&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary Index as a competent retrieval approach. All resources related to this research are publicly accessible for further investigation through our GitHub repository ARAGOG (this https URL). We welcome the community to further this exploratory study in RAG systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/predlico/ARAGOG"&gt;GitHub repo&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/aragog-advanced-rag-output-grading?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/aragog-advanced-rag-output-grading</guid>
      <pubDate>2024-04-09 00:32</pubDate>
      <category>#rag</category>
      <category>#ai</category>
      <category>#research</category>
      <category>#llm</category>
      <category>#knowledge</category>
      <category>#retrieval</category>
      <category>#retrievalaugmentedgeneration</category>
    </item>
    <item>
      <title>Far Out Guides</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/far-out-guides-hiking-app?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/far-out-guides-hiking-app&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Outdoor nagivation app for long-distance trails&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/far-out-guides-hiking-app?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/far-out-guides-hiking-app</guid>
      <pubDate>2024-04-07 19:38</pubDate>
      <category>#outdoors</category>
      <category>#app</category>
      <category>#hiking</category>
      <category>#technology</category>
    </item>
    <item>
      <title>Molly White's Blogroll</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/blogroll-molly-white?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/blogroll-molly-white&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Bookmarking for reference.&lt;/p&gt;
&lt;p&gt;I'm already subscribed to many of these websites and publications. However, there's several new ones I found that I think will eventually make their rotation into my &lt;a href="/feed/blogroll"&gt;blogroll&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/blogroll-molly-white?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/blogroll-molly-white</guid>
      <pubDate>2024-04-07 17:50</pubDate>
      <category>#blogroll</category>
      <category>#indieweb</category>
      <category>#rss</category>
      <category>#blogging</category>
      <category>#web</category>
      <category>#internet</category>
      <category>#smallweb</category>
      <category>#community</category>
    </item>
    <item>
      <title>OpenAI - Introducing improvements to the fine-tuning API and expanding our custom models program</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openai-introducing-fine-tuning-improvements?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openai-introducing-fine-tuning-improvements&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h2&gt;New fine-tuning API features&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we‚Äôre introducing new features to give developers even more control over their fine-tuning jobs, including:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Epoch-based Checkpoint Creation: Automatically produce one full fine-tuned model checkpoint during each training epoch, which reduces the need for subsequent retraining, especially in the cases of overfitting&lt;/li&gt;
&lt;li&gt;Comparative Playground: A new side-by-side Playground UI for comparing model quality and performance, allowing human evaluation of the outputs of multiple models or fine-tune snapshots against a single prompt&lt;/li&gt;
&lt;li&gt;Third-party Integration: Support for integrations with third-party platforms (starting with Weights and Biases this week) to let developers share detailed fine-tuning data to the rest of their stack&lt;/li&gt;
&lt;li&gt;Comprehensive Validation Metrics: The ability to compute metrics like loss and accuracy over the entire validation dataset instead of a sampled batch, providing better insight on model quality&lt;/li&gt;
&lt;li&gt;Hyperparameter Configuration: The ability to configure available hyperparameters from the Dashboard (rather than only through the API or SDK)&lt;/li&gt;
&lt;li&gt;Fine-Tuning Dashboard Improvements: Including the ability to configure hyperparameters, view more detailed training metrics, and rerun jobs from previous configurations&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Expanding our Custom Models Program&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Assisted Fine-Tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we are formally announcing our assisted fine-tuning offering as part of the Custom Model program. Assisted fine-tuning is a collaborative effort with our technical teams to leverage techniques beyond the fine-tuning API, such as additional hyperparameters and various parameter efficient fine-tuning (PEFT) methods at a larger scale. It‚Äôs particularly helpful for organizations that need support setting up efficient training data pipelines, evaluation systems, and bespoke parameters and methods to maximize model performance for their use case or task.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Custom-Trained Model&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In some cases, organizations need to train a purpose-built model from scratch that understands their business, industry, or domain. Fully custom-trained models imbue new knowledge from a specific domain by modifying key steps of the model training process using novel mid-training and post-training techniques. Organizations that see success with a fully custom-trained model often have large quantities of proprietary data‚Äîmillions of examples or billions of tokens‚Äîthat they want to use to teach the model new knowledge or complex, unique behaviors for highly specific use cases.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openai-introducing-fine-tuning-improvements?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openai-introducing-fine-tuning-improvements</guid>
      <pubDate>2024-04-04 22:58</pubDate>
      <category>#openai</category>
      <category>#finetuning</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Introducing Command R+: A Scalable LLM Built for Business</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-cohere-comandr-plus?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-cohere-comandr-plus&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Command R+ is a state-of-the-art RAG-optimized model designed to tackle enterprise-grade workloads, and is available first on Microsoft Azure&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Command R+, like our recently launched Command R model, features a 128k-token context window and is designed to offer best-in-class:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Advanced Retrieval Augmented Generation (RAG) with citation to reduce hallucinations&lt;/li&gt;
&lt;li&gt;Multilingual coverage in 10 key languages to support global business operations&lt;/li&gt;
&lt;li&gt;Tool Use to automate sophisticated business processes&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-cohere-comandr-plus?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-cohere-comandr-plus</guid>
      <pubDate>2024-04-04 22:46</pubDate>
      <category>#cohere</category>
      <category>#llm</category>
      <category>#comandr</category>
      <category>#azure</category>
      <category>#comandrplus</category>
    </item>
    <item>
      <title>Large Language Models Are Zero-Shot Time Series Forecasters</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/llm-time-series-forecasting?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/llm-time-series-forecasting&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side information, and answer questions to help explain predictions. While we find that increasing model size generally improves performance on time series, we show GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers, and poor uncertainty calibration, which is likely the result of alignment interventions such as RLHF.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/ngruver/llmtime"&gt;Github Repo&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/llm-time-series-forecasting?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/llm-time-series-forecasting</guid>
      <pubDate>2024-04-04 11:07</pubDate>
      <category>#forecasting</category>
      <category>#llm</category>
      <category>#ai</category>
      <category>#research</category>
    </item>
    <item>
      <title>IPEX-LLM</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/intel-ipex-llm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/intel-ipex-llm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/intel-ipex-llm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/intel-ipex-llm</guid>
      <pubDate>2024-04-04 11:05</pubDate>
      <category>#intel</category>
      <category>#llm</category>
      <category>#pytorch</category>
      <category>#cpu</category>
      <category>#gpu</category>
    </item>
    <item>
      <title>ReALM: Reference Resolution As Language Modeling</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/apple-realm-research-paper?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/apple-realm-research-paper&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of references, with our smallest model obtaining absolute gains of over 5% for on-screen references. We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/apple-realm-research-paper?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/apple-realm-research-paper</guid>
      <pubDate>2024-04-04 11:04</pubDate>
      <category>#ai</category>
      <category>#research</category>
      <category>#apple</category>
      <category>#gpt</category>
    </item>
    <item>
      <title>Introducing Stable Audio 2.0</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-stable-audio-2-0?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-stable-audio-2-0&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable Audio 2.0 sets a new standard in AI-generated audio, producing high-quality, full tracks with coherent musical structure up to three minutes in length at 44.1kHz stereo.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The new model introduces audio-to-audio generation by allowing users to upload and transform samples using natural language prompts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable Audio 2.0 was exclusively trained on a licensed dataset from the AudioSparx music library, honoring opt-out requests and ensuring fair compensation for creators.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-stable-audio-2-0?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-stable-audio-2-0</guid>
      <pubDate>2024-04-04 11:01</pubDate>
      <category>#stabilityai</category>
      <category>#ai</category>
      <category>#audio</category>
      <category>#nlp</category>
    </item>
    <item>
      <title>The Matrix is coming back for a fifth movie</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/matrix-five-announced?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/matrix-five-announced&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Deadline reports that The Martian writer Drew Goddard has been tapped to pen and direct another Matrix movie executive produced by Lana Wachowski. Currently, the new film has no title or projected premiere date, and there‚Äôs been no announcement as to whether franchise stars like Keanu Reeves, Carrie-Anne Moss, Laurence Fishburne, Yahya Abdul-Mateen II, or Jessica Henwick will return.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Not sure how to feel about this, but I'll end up watching anyway.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/matrix-five-announced?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/matrix-five-announced</guid>
      <pubDate>2024-04-04 10:56</pubDate>
      <category>#matrix</category>
      <category>#movie</category>
      <category>#scifi</category>
      <category>#neo</category>
      <category>#theone</category>
    </item>
    <item>
      <title>Google Podcasts is gone ‚Äî and so is my faith in Google</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/google-podcasts-gone-youtube-cant-replace-it-verge?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/google-podcasts-gone-youtube-cant-replace-it-verge&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Good article. I felt this way when Google Reader and a few other services were shut down.&lt;/p&gt;
&lt;p&gt;That being said, this is kind of a good thing.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Luckily, there are plenty of good podcast apps out there, like Pocket Casts, Overcast, Antennapod, and even Apple Podcasts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This line basically says it all. Podcasts, like blogging, continue to be an open ecosystem and where the saying,&amp;quot;wherever you get your podcasts&amp;quot;, is still going strong.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/google-podcasts-gone-youtube-cant-replace-it-verge?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/google-podcasts-gone-youtube-cant-replace-it-verge</guid>
      <pubDate>2024-04-02 22:54</pubDate>
      <category>#google</category>
      <category>#podcasts</category>
      <category>#youtube</category>
      <category>#audio</category>
      <category>#opensource</category>
      <category>#rss</category>
    </item>
    <item>
      <title>Start using ChatGPT instantly</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/start-using-chatgpt-seamlessly-no-signup?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/start-using-chatgpt-seamlessly-no-signup&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôre making it easier for people to experience the benefits of AI without needing to sign up.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We may use what you provide to ChatGPT to improve our models for everyone. If you‚Äôd like, you can turn this off through your Settings - whether you create an account or not.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôve also introduced additional content safeguards for this experience, such as blocking prompts and generations in a wider range of categories.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/start-using-chatgpt-seamlessly-no-signup?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/start-using-chatgpt-seamlessly-no-signup</guid>
      <pubDate>2024-04-01 20:44</pubDate>
      <category>#openai</category>
      <category>#chatgpt</category>
      <category>#ai</category>
      <category>#llm</category>
    </item>
    <item>
      <title>T-Mobile Sidekick‚Äôs Jump button made mobile multitasking easy</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tmobile-sidekick-jump-button-mobile-productivity?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tmobile-sidekick-jump-button-mobile-productivity&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Bring back the Sidekick! &lt;a href="/feed/ayaneo-slide/"&gt;Ayaneo Slide&lt;/a&gt; is probably the closest to this today. Would love to see a &lt;a href="/feed/windows-12-mobile-concept/"&gt;smaller version of it running on Windows on ARM-based Snapdragon processors&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Before the iPhone, before Android, before webOS, a revolutionary soap bar of a phone made it incredibly easy to get shit done. The Danger Hiptop, better known as the T-Mobile Sidekick, made the internet portable and affordable like no phone before.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tmobile-sidekick-jump-button-mobile-productivity?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tmobile-sidekick-jump-button-mobile-productivity</guid>
      <pubDate>2024-04-01 20:27</pubDate>
      <category>#tmobile</category>
      <category>#nostalgia</category>
      <category>#sidekick</category>
      <category>#tmobile</category>
      <category>#phones</category>
      <category>#smartphones</category>
      <category>#productivity</category>
      <category>#mobile</category>
      <category>#cellphones</category>
      <category>#handhelds</category>
    </item>
    <item>
      <title>Announcing DBRX: A new standard for efficient open source LLMs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/announcing-dbrx-llm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/announcing-dbrx-llm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we are excited to advance our mission by open sourcing DBRX, a general purpose large language model (LLM) built by our Mosaic Research team that outperforms all established open source models on standard benchmarks. We believe that pushing the boundary of open source models enables generative AI for all enterprises that is customizable and transparent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are excited about DBRX for three distinct reasons. First, it handily beats open source models, such as, LLaMA2-70B, Mixtral, and Grok-1 on language understanding, programming, math, and logic...&lt;br /&gt;
&lt;br&gt;
Second, DBRX beats GPT-3.5 on most benchmarks...&lt;br /&gt;
&lt;br&gt;
Third, DBRX is a Mixture-of-Experts (MoE) model built on the MegaBlocks research and open source project, making the model extremely fast in terms of tokens/second.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/announcing-dbrx-llm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/announcing-dbrx-llm</guid>
      <pubDate>2024-03-31 20:26</pubDate>
      <category>#databricks</category>
      <category>#ai</category>
      <category>#llm</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>One-step Diffusion with Distribution Matching Distillation</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/one-step-diffusion-distribution-matching-distillation?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/one-step-diffusion-distribution-matching-distillation&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Our one-step generator achieves comparable image quality with StableDiffusion v1.5 while being 30x faster.&lt;/strong&gt;&lt;br /&gt;
&lt;br&gt;
Diffusion models are known to approximate the score function of the distribution they are trained on. In other words, an unrealistic synthetic image can be directed toward higher probability density region through the denoising process (see SDS). Our core idea is training two diffusion models to estimate not only the score function of the target real distribution, but also that of the fake distribution. We construct a gradient update to our generator as the difference between the two scores, essentially nudging the generated images toward higher realism as well as lower fakeness (see VSD). Our method is similar to GANs in that a critic is jointly trained with the generator to minimize a divergence between the real and fake distributions, but differs in that our training does not play an adversarial game that may cause training instability, and our critic can fully leverage the weights of a pretrained diffusion model. Combined with a simple regression loss to match the output of the multi-step diffusion model, our method outperforms all published few-step diffusion approaches, reaching 2.62 FID on ImageNet 64x64 and 11.49 FID on zero-shot COCO-30k, comparable to Stable Diffusion but orders of magnitude faster. Utilizing FP16 inference, our model generates images at 20 FPS on modern hardware.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2311.18828"&gt;Paper&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/one-step-diffusion-distribution-matching-distillation?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/one-step-diffusion-distribution-matching-distillation</guid>
      <pubDate>2024-03-26 21:44</pubDate>
      <category>#ai</category>
      <category>#genai</category>
      <category>#diffusion</category>
      <category>#dmd</category>
      <category>#stablediffusion</category>
      <category>#dalle</category>
    </item>
    <item>
      <title>404 Media Now Has a Full Text RSS Feed</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/404-media-full-text-rss?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/404-media-full-text-rss&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We paid for the development of full text RSS feeds for Ghost-based publishers. Now we can offer them to our paid subscribers, and other Ghost sites can use the service too.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our friends Anil Dash and Ernie Smith have recently written passionately and persuasively about the importance of RSS to the open web, and about how a technology that turns 25 years old this month remains both subversive and quite versatile. RSS-based distribution underpins a podcasting ecosystem that has allowed for shows to be distributed not just on Apple Podcasts but on Spotify, Google Podcasts, Pocket Casts, Overcast, and whatever other podcast player you might want to listen on. ‚ÄúBeing able to say, ‚Äòwherever you get your podcasts‚Äô is a radical statement,‚Äù Dash wrote. ‚ÄúBecause what it represents is the triumph of exactly the kind of technology that's supposed to be impossible: open, empowering tech that's not owned by any one company, that can't be controlled by any one company, and that allows people to have ownership over their work and their relationship with their audience.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;RSS has empowered podcasters, but that it needs a ‚Äúcreator economy rethink‚Äù for text.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/404-media-full-text-rss?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/404-media-full-text-rss</guid>
      <pubDate>2024-03-26 21:16</pubDate>
      <category>#rss</category>
      <category>#404media</category>
      <category>#ghost</category>
      <category>#404</category>
      <category>#media</category>
      <category>#protocols</category>
      <category>#opensource</category>
      <category>#technology</category>
    </item>
    <item>
      <title>Stability CEO Resigns</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/stability-ai-ceo-resigns?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/stability-ai-ceo-resigns&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Earlier today, Emad Mostaque resigned from his role as CEO of Stability AI and from his position on the Board of Directors of the company to pursue decentralized AI.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/stability-ai-ceo-resigns?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/stability-ai-ceo-resigns</guid>
      <pubDate>2024-03-23 11:49</pubDate>
      <category>#stability</category>
      <category>#ai</category>
      <category>#news</category>
    </item>
    <item>
      <title>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mamba-linear-time-sequence-selective-state-spaces?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mamba-linear-time-sequence-selective-state-spaces&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5√ó higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mamba-linear-time-sequence-selective-state-spaces?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mamba-linear-time-sequence-selective-state-spaces</guid>
      <pubDate>2024-03-21 20:44</pubDate>
      <category>#ai</category>
      <category>#models</category>
      <category>#mamba</category>
      <category>#llm</category>
      <category>#compute</category>
    </item>
    <item>
      <title>Releasing Common Corpus: the largest public domain dataset for training LLMs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/common-corpus-llm-training-dataset?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/common-corpus-llm-training-dataset&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We announce today the release of Common Corpus on HuggingFace:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Common Corpus is the largest public domain dataset released for training LLMs.&lt;/li&gt;
&lt;li&gt;Common Corpus includes 500 billion words from a wide diversity of cultural heritage initiatives.&lt;/li&gt;
&lt;li&gt;Common Corpus is multilingual and the largest corpus to date in English, French, Dutch, Spanish, German and Italian.&lt;/li&gt;
&lt;li&gt;Common Corpus shows it is possible to train fully open LLMs on sources without copyright concerns.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/common-corpus-llm-training-dataset?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/common-corpus-llm-training-dataset</guid>
      <pubDate>2024-03-20 22:12</pubDate>
      <category>#ai</category>
      <category>#data</category>
      <category>#llm</category>
      <category>#huggingface</category>
      <category>#nlp</category>
    </item>
    <item>
      <title>Machine Learning for Games Course - HuggingFace</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/machine-learning-for-games-course-huggingface?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/machine-learning-for-games-course-huggingface&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Welcome to the course that will teach you the most fascinating topic in game development: how to use powerful AI tools and models to create unique game experiences.&lt;br /&gt;
&lt;br&gt;
New AI models are revolutionizing the Game Industry in two impactful ways:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On how we make games:
&lt;ul&gt;
&lt;li&gt;Generate textures using AI&lt;/li&gt;
&lt;li&gt;Using AI voice actors for the voices.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How we create gameplay:
&lt;ul&gt;
&lt;li&gt;Crafting smart Non-Playable Characters (NPCs) using large language models.&lt;br /&gt;
&lt;br&gt;
This course will teach you:&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;How to integrate AI models for innovative gameplay, featuring intelligent NPCs.&lt;/li&gt;
&lt;li&gt;How to use AI tools to help your game development pipeline.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/machine-learning-for-games-course-huggingface?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/machine-learning-for-games-course-huggingface</guid>
      <pubDate>2024-03-19 22:35</pubDate>
      <category>#machinelearning</category>
      <category>#games</category>
      <category>#huggingface</category>
      <category>#course</category>
      <category>#ai</category>
      <category>#ml</category>
    </item>
    <item>
      <title>The Online Local Chronicle</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/online-local-chronicle-searls?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/online-local-chronicle-searls&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In the same way that every little place in America used to have a printed newspaper, every little place in America could have an online local chronicle.&lt;br /&gt;
&lt;br&gt;
Broadly speaking, an online local chronicle is a collection of facts organized mostly in chronological order. The ‚Äúpages‚Äù of the chronicle can be thought of as subsets of a community‚Äôs universal timeline of events. These online local chronicles could become the backbone of local news operations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Nice project. Unfortunately, it's rare you get local news. I like publications / websites like &lt;a href="https://www.hobokengirl.com/"&gt;Hoboken Girl&lt;/a&gt; and &lt;a href="https://blockclubchicago.org/"&gt;Block Club Chicago&lt;/a&gt;. I wish there were more of them in more cities and towns. I know they're there in some forms like Facebook Groups. Even better, it'd be great to have the websites for these publications be the main source of truth that then syndicated their content to the various platforms out there.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/online-local-chronicle-searls?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/online-local-chronicle-searls</guid>
      <pubDate>2024-03-19 22:17</pubDate>
      <category>#community</category>
      <category>#news</category>
      <category>#local</category>
      <category>#knowledgebase</category>
      <category>#information</category>
      <category>#web</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Quanto: a PyTorch quantization toolkit </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/quanto-pytorch-quantization-toolkit?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/quanto-pytorch-quantization-toolkit&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Quantization is a technique to reduce the computational and memory costs of evaluating Deep Learning Models by representing their weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we are excited to introduce quanto, a versatile pytorch quantization toolkit, that provides several unique features:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;available in eager mode (works with non-traceable models)&lt;/li&gt;
&lt;li&gt;quantized models can be placed on any device (including CUDA and MPS),&lt;/li&gt;
&lt;li&gt;automatically inserts quantization and dequantization stubs,&lt;/li&gt;
&lt;li&gt;automatically inserts quantized functional operations,&lt;/li&gt;
&lt;li&gt;automatically inserts quantized modules (see below the list of supported modules),&lt;/li&gt;
&lt;li&gt;provides a seamless workflow for a float model, going from a dynamic to a static quantized model,&lt;/li&gt;
&lt;li&gt;supports quantized model serialization as a state_dict,&lt;/li&gt;
&lt;li&gt;supports not only int8 weights, but also int2 and int4,&lt;/li&gt;
&lt;li&gt;supports not only int8 activations, but also float8.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/quanto-pytorch-quantization-toolkit?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/quanto-pytorch-quantization-toolkit</guid>
      <pubDate>2024-03-19 22:00</pubDate>
      <category>#huggingface</category>
      <category>#quantization</category>
      <category>#pytorch</category>
      <category>#tools</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Demystifying Embedding Spaces using Large Language Models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/demistifying-embedding-spaces-llms?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/demistifying-embedding-spaces-llms&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing Large Language Models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decoding user preferences in recommender systems. Our work couples the immense information potential of embeddings with the interpretative power of LLMs.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/demistifying-embedding-spaces-llms?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/demistifying-embedding-spaces-llms</guid>
      <pubDate>2024-03-19 21:39</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#embedding</category>
      <category>#interpretability</category>
    </item>
    <item>
      <title>The Tokenizer Playground</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tokenizer-playground?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tokenizer-playground&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Experiment with different tokenizers (running locally in your browser).
I really love doing this thing&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tokenizer-playground?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tokenizer-playground</guid>
      <pubDate>2024-03-18 22:35</pubDate>
      <category>#ai</category>
      <category>#tokenizer</category>
      <category>#huggingface</category>
      <category>#playground</category>
      <category>#aiplayground</category>
    </item>
    <item>
      <title>Introducing Stable Video 3D: Quality Novel View Synthesis and 3D Generation from Single Images</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-stable-video-3d?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-stable-video-3d&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we are releasing Stable Video 3D (SV3D), a generative model based on Stable Video Diffusion, advancing the field of 3D technology and delivering greatly improved quality and view-consistency.&lt;br /&gt;
&lt;br&gt;
This release features two variants: SV3D_u and SV3D_p. SV3D_u generates orbital videos based on single image inputs without camera conditioning. SV3D_p extends the capability by accommodating both single images and orbital views, allowing for the creation of 3D video along specified camera paths.&lt;br /&gt;
&lt;br&gt;
Stable Video 3D can be used now for commercial purposes with a Stability AI Membership. For non-commercial use, you can download the model weights on Hugging Face and view our research paper here.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-stable-video-3d?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-stable-video-3d</guid>
      <pubDate>2024-03-18 21:29</pubDate>
      <category>#stabilityai</category>
      <category>#ai</category>
      <category>#video</category>
      <category>#3d</category>
      <category>#computervision</category>
    </item>
    <item>
      <title>Nvidia reveals Blackwell B200 GPU</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nvidia-blackwell-b200-gpu?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nvidia-blackwell-b200-gpu&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Nvidia reveals Blackwell B200 GPU, the ‚Äòworld‚Äôs most powerful chip‚Äô for AI&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;‚ÄòBuilt to democratize trillion-parameter AI.‚Äô&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Nvidia says the new B200 GPU offers up to 20 petaflops of FP4 horsepower from its 208 billion transistors. Also, it says, a GB200 that combines two of those GPUs with a single Grace CPU can offer 30 times the performance for LLM inference workloads while also potentially being substantially more efficient. It ‚Äúreduces cost and energy consumption by up to 25x‚Äù over an H100, says Nvidia.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nvidia-blackwell-b200-gpu?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nvidia-blackwell-b200-gpu</guid>
      <pubDate>2024-03-18 21:09</pubDate>
      <category>#ai</category>
      <category>#nvidia</category>
      <category>#gpu</category>
      <category>#hardware</category>
      <category>#pchardware</category>
      <category>#pc</category>
    </item>
    <item>
      <title>MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mm1-multimodal-llm-pretraining?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mm1-multimodal-llm-pretraining&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, consisting of both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mm1-multimodal-llm-pretraining?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mm1-multimodal-llm-pretraining</guid>
      <pubDate>2024-03-18 21:02</pubDate>
      <category>#ai</category>
      <category>#mm1</category>
      <category>#llm</category>
      <category>#multimodal</category>
      <category>#apple</category>
    </item>
    <item>
      <title>Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/enhace-rag-application-accuracy-knowledge-graphs?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/enhace-rag-application-accuracy-knowledge-graphs&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A practical guide to constructing and retrieving information from knowledge graphs in RAG applications with Neo4j and LangChain&lt;br /&gt;
&lt;br&gt;
Graph retrieval augmented generation (Graph RAG) is gaining momentum and emerging as a powerful addition to traditional vector search retrieval methods. This approach leverages the structured nature of graph databases, which organize data as nodes and relationships, to enhance the depth and contextuality of retrieved information.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/enhace-rag-application-accuracy-knowledge-graphs?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/enhace-rag-application-accuracy-knowledge-graphs</guid>
      <pubDate>2024-03-17 21:29</pubDate>
      <category>#knowledgegraph</category>
      <category>#rag</category>
      <category>#genai</category>
      <category>#langchain</category>
      <category>#generativeai</category>
      <category>#retrievalaugmentedgeneration</category>
      <category>#patterns</category>
      <category>#applicationpatterns</category>
    </item>
    <item>
      <title>LaVague - Large Action Model framework</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/lavague-large-action-model?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/lavague-large-action-model&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Redefining internet surfing by transforming natural language instructions into seamless browser interactions.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/lavague-large-action-model?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/lavague-large-action-model</guid>
      <pubDate>2024-03-17 21:22</pubDate>
      <category>#ai</category>
      <category>#genai</category>
      <category>#lavague</category>
      <category>#selenium</category>
      <category>#web</category>
      <category>#lam</category>
      <category>#largeactionmodel</category>
    </item>
    <item>
      <title>Algorithms for Modern Hardware</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/algorithms-modern-hardware-book?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/algorithms-modern-hardware-book&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This is an upcoming high performance computing book titled ‚ÄúAlgorithms for Modern Hardware‚Äù by Sergey Slotin.&lt;br /&gt;
&lt;br&gt;
Its intended audience is everyone from performance engineers and practical algorithm researchers to undergraduate computer science students who have just finished an advanced algorithms course and want to learn more practical ways to speed up a program than by going from O(nlog‚Å°n)O(nlogn) to O(nlog‚Å°log‚Å°n)O(nloglogn).&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/algorithms-modern-hardware-book?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/algorithms-modern-hardware-book</guid>
      <pubDate>2024-03-17 21:20</pubDate>
      <category>#book</category>
      <category>#algorithms</category>
      <category>#hardware</category>
    </item>
    <item>
      <title>Spreadsheets are all you need</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/spreadsheets-are-all-you-need?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/spreadsheets-are-all-you-need&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A low-code way to learn AI - Learn how AI works from a real LLM implemented entirely in Excel&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/spreadsheets-are-all-you-need?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/spreadsheets-are-all-you-need</guid>
      <pubDate>2024-03-17 21:18</pubDate>
      <category>#excel</category>
      <category>#tutorial</category>
      <category>#ai</category>
      <category>#genai</category>
      <category>#tokenizers</category>
      <category>#transformers</category>
    </item>
    <item>
      <title>Building Meta‚Äôs GenAI Infrastructure</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/building-meta-genai-infrastructure?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/building-meta-genai-infrastructure&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;ul&gt;
&lt;li&gt;Marking a major investment in Meta‚Äôs AI future, we are announcing two 24k GPU clusters. We are sharing details on the hardware, network, storage, design, performance, and software that help us extract high throughput and reliability for various AI workloads. We use this cluster design for Llama 3 training.&lt;/li&gt;
&lt;li&gt;We are strongly committed to open compute and open source. We built these clusters on top of Grand Teton, OpenRack, and PyTorch and continue to push open innovation across the industry.&lt;/li&gt;
&lt;li&gt;This announcement is one step in our ambitious infrastructure roadmap. By the end of 2024, we‚Äôre aiming to continue to grow our infrastructure build-out that will include 350,000 NVIDIA H100 GPUs as part of a portfolio that will feature compute power equivalent to nearly 600,000 H100s.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/building-meta-genai-infrastructure?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/building-meta-genai-infrastructure</guid>
      <pubDate>2024-03-17 21:14</pubDate>
      <category>#meta</category>
      <category>#ai</category>
      <category>#infrastraucture</category>
      <category>#generativeai</category>
      <category>#genai</category>
      <category>#facebook</category>
    </item>
    <item>
      <title>OpenAI Transformer Debugger</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openai-transformer-debugger?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openai-transformer-debugger&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Transformer Debugger (TDB) is a tool developed by OpenAI's Superalignment team with the goal of supporting investigations into specific behaviors of small language models. The tool combines automated interpretability techniques with sparse autoencoders.&lt;br /&gt;
&lt;br&gt;
TDB enables rapid exploration before needing to write code, with the ability to intervene in the forward pass and see how it affects a particular behavior. It can be used to answer questions like, &amp;quot;Why does the model output token A instead of token B for this prompt?&amp;quot; or &amp;quot;Why does attention head H attend to token T for this prompt?&amp;quot; It does so by identifying specific components (neurons, attention heads, autoencoder latents) that contribute to the behavior, showing automatically generated explanations of what causes those components to activate most strongly, and tracing connections between components to help discover circuits.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openai-transformer-debugger?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openai-transformer-debugger</guid>
      <pubDate>2024-03-17 21:05</pubDate>
      <category>#openai</category>
      <category>#ai</category>
      <category>#transformer</category>
      <category>#debugger</category>
      <category>#tools</category>
    </item>
    <item>
      <title>Diffusion Models From Scratch</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/diffusion-models-from-scratch-duan?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/diffusion-models-from-scratch-duan&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Here, we'll cover the derivations from scratch to provide a rigorous understanding of the core ideas behind diffusion. What assumptions are we making? What properties arise as a result?&lt;br /&gt;
&lt;br&gt;
A reference [codebase] is written from scratch, which provides minimalist re-production of the MNIST example below. It clocks in at under 500 lines of code.&lt;br /&gt;
&lt;br&gt;
Each page takes up to an hour to read thoroughly. Approximately a lecture each.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/diffusion-models-from-scratch-duan?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/diffusion-models-from-scratch-duan</guid>
      <pubDate>2024-03-17 21:03</pubDate>
      <category>#diffusion</category>
      <category>#ai</category>
      <category>#concepts</category>
      <category>#models</category>
    </item>
    <item>
      <title>Diffusion models from scratch, from a new theoretical perspective</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/diffusion-models-from-scratch-yuan?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/diffusion-models-from-scratch-yuan&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This tutorial aims to introduce diffusion models from an optimization perspective as introduced in our paper (joint work with Frank Permenter). It will go over both theory and code, using the theory to explain how to implement diffusion models from scratch. By the end of the tutorial, you will learn how to implement training and sampling code for a toy dataset, which will also work for larger datasets and models.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/diffusion-models-from-scratch-yuan?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/diffusion-models-from-scratch-yuan</guid>
      <pubDate>2024-03-17 21:02</pubDate>
      <category>#diffusion</category>
      <category>#ai</category>
      <category>#concepts</category>
      <category>#models</category>
    </item>
    <item>
      <title>Fast Inner-Product Algorithms and Architectures for Deep Neural Network Accelerators</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/fast-inner-product-dnn-accelerators?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/fast-inner-product-dnn-accelerators&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We introduce a new algorithm called the Free-pipeline Fast Inner Product (FFIP) and its hardware architecture that improve an under-explored fast inner-product algorithm (FIP) proposed by Winograd in 1968. Unlike the unrelated Winograd minimal filtering algorithms for convolutional layers, FIP is applicable to all machine learning (ML) model layers that can mainly decompose to matrix multiplication, including fully-connected, convolutional, recurrent, and attention/transformer layers. We implement FIP for the first time in an ML accelerator then present our FFIP algorithm and generalized architecture which inherently improve FIP's clock frequency and, as a consequence, throughput for a similar hardware cost. Finally, we contribute ML-specific optimizations for the FIP and FFIP algorithms and architectures. We show that FFIP can be seamlessly incorporated into traditional fixed-point systolic array ML accelerators to achieve the same throughput with half the number of multiply-accumulate (MAC) units, or it can double the maximum systolic array size that can fit onto devices with a fixed hardware budget. Our FFIP implementation for non-sparse ML models with 8 to 16-bit fixed-point inputs achieves higher throughput and compute efficiency than the best-in-class prior solutions on the same type of compute platform.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/trevorpogue/algebraic-nnhw"&gt;GitHub&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/fast-inner-product-dnn-accelerators?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/fast-inner-product-dnn-accelerators</guid>
      <pubDate>2024-03-17 20:47</pubDate>
      <category>#ai</category>
      <category>#math</category>
      <category>#acceleration</category>
      <category>#neuralnetworks</category>
      <category>#dnn</category>
    </item>
    <item>
      <title>Grok-1</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/grok-1-open-release?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/grok-1-open-release&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This repository contains JAX example code for loading and running the Grok-1 open-weights model.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/grok-1-open-release?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/grok-1-open-release</guid>
      <pubDate>2024-03-17 20:45</pubDate>
      <category>#grok</category>
      <category>#ai</category>
      <category>#x</category>
      <category>#github</category>
      <category>#opensource</category>
      <category>#moe</category>
    </item>
    <item>
      <title>Ollama now supports AMD graphics cards</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ollama-supports-amd-gpus?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ollama-supports-amd-gpus&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Ollama now supports AMD graphics cards in preview on Windows and Linux. All the features of Ollama can now be accelerated by AMD graphics cards on Ollama for Linux and Windows.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ollama-supports-amd-gpus?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ollama-supports-amd-gpus</guid>
      <pubDate>2024-03-17 20:43</pubDate>
      <category>#ollama</category>
      <category>#gpu</category>
      <category>#amd</category>
      <category>#ai</category>
      <category>#llm</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Apple Vision Pro Perspectives - Hugo Barra</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/apple-vision-pro-barra?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/apple-vision-pro-barra&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Friends and colleagues have been asking me to share my perspective on the Apple Vision Pro as a product.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This started as blog post and became an essay before too long, so I‚Äôve structured my writing in multiple sections each with a clear lead to make it a bit easier to digest ‚Äî peppered with my own ‚Äòtakes‚Äô. I‚Äôve tried to stick to original thoughts for the most part and link to what others have said where applicable.&lt;br /&gt;
&lt;br&gt;
Some of the topics I touch on:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why I believe Vision Pro may be an over-engineered ‚Äúdevkit‚Äù&lt;/li&gt;
&lt;li&gt;The genius &amp;amp; audacity behind some of Apple‚Äôs hardware decisions&lt;/li&gt;
&lt;li&gt;Gaze &amp;amp; pinch is an incredible UI superpower and major industry ah-ha moment&lt;/li&gt;
&lt;li&gt;Why the Vision Pro software/content story is so dull and unimaginative&lt;/li&gt;
&lt;li&gt;Why most people won‚Äôt use Vision Pro for watching TV/movies&lt;/li&gt;
&lt;li&gt;Apple‚Äôs bet in immersive video is a total game-changer for Live Sports&lt;/li&gt;
&lt;li&gt;Why I returned my Vision Pro‚Ä¶ and my Top 10 wishlist to reconsider&lt;/li&gt;
&lt;li&gt;Apple‚Äôs VR debut is the best thing that ever happened to Oculus/Meta&lt;/li&gt;
&lt;li&gt;My unsolicited product advice to Meta for Quest Pro 2 and beyond&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/apple-vision-pro-barra?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/apple-vision-pro-barra</guid>
      <pubDate>2024-03-15 21:12</pubDate>
      <category>#apple</category>
      <category>#visionpro</category>
      <category>#vr</category>
      <category>#ar</category>
    </item>
    <item>
      <title>Bluesky‚Äôs Stackable Approach to Moderation</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/bluesky-stackable-approach-moderation?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/bluesky-stackable-approach-moderation&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Bluesky was created to put users and communities in control of their social spaces online. The first generation of social media platforms connected the world, but ended up consolidating power in the hands of a few corporations and their leaders. Our online experience doesn‚Äôt have to depend on billionaires unilaterally making decisions over what we see. On an open social network like Bluesky, you can shape your experience for yourself.&lt;br /&gt;
&lt;br&gt;
Today, we‚Äôre excited to announce that we‚Äôre &lt;a href="https://github.com/bluesky-social/ozone"&gt;open-sourcing Ozone&lt;/a&gt;, our collaborative moderation tool. With Ozone, individuals and teams can work together to review and label content across the network. Later this week, we‚Äôre opening up the ability for you to run your own independent moderation services, seamlessly integrated into the Bluesky app. This means that you'll be able to create and subscribe to additional moderation services on top of what Bluesky requires, giving you unprecedented control over your social media experience.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/bluesky-stackable-approach-moderation?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/bluesky-stackable-approach-moderation</guid>
      <pubDate>2024-03-15 20:53</pubDate>
      <category>#moderation</category>
      <category>#community</category>
      <category>#bluesky</category>
      <category>#atprotocol</category>
      <category>#socialmedia</category>
      <category>#internet</category>
      <category>#web</category>
    </item>
    <item>
      <title>Introducing the Proton Mail desktop app </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-protonmail-desktop-app?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-protonmail-desktop-app&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we‚Äôre excited to broaden the horizons of secure communication by launching the Proton Mail desktop app. Anyone can now use the new Proton Mail desktop app for Windows and macOS, with a Linux version in beta.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;With the new Proton Mail desktop apps, you get a dedicated email experience, allowing you to enjoy all the productivity innovations of our web app, allowing you to go through your emails and events faster without the potential distractions that pop up anytime you open your browser. And, of course, your privacy remains protected at all times.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-protonmail-desktop-app?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-protonmail-desktop-app</guid>
      <pubDate>2024-03-15 20:06</pubDate>
      <category>#email</category>
      <category>#proton</category>
      <category>#desktop</category>
      <category>#app</category>
      <category>#privacy</category>
      <category>#security</category>
    </item>
    <item>
      <title>What I learned from looking at 900 most popular open source AI tools</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/learned-from-looking-900-oss-ai-tools-huyen?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/learned-from-looking-900-oss-ai-tools-huyen&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;So many cool ideas are being developed by the community. Here are some of my favorites.&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Batch inference optimization: FlexGen, llama.cpp&lt;/li&gt;
&lt;li&gt;Faster decoder with techniques such as Medusa, LookaheadDecoding&lt;/li&gt;
&lt;li&gt;Model merging: mergekit&lt;/li&gt;
&lt;li&gt;Constrained sampling: outlines, guidance, SGLang&lt;/li&gt;
&lt;li&gt;Seemingly niche tools that solve one problem really well, such as einops and safetensors.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/learned-from-looking-900-oss-ai-tools-huyen?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/learned-from-looking-900-oss-ai-tools-huyen</guid>
      <pubDate>2024-03-14 22:13</pubDate>
      <category>#ai</category>
      <category>#tools</category>
      <category>#analysis</category>
      <category>#openai</category>
      <category>#github</category>
    </item>
    <item>
      <title>You can now train a 70b language model at home</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/answer-ai-train-70b-llm-at-home?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/answer-ai-train-70b-llm-at-home&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we‚Äôre releasing Answer.AI‚Äôs first project: a fully open source system that, for the first time, can efficiently train a 70b large language model on a regular desktop computer with two or more standard gaming GPUs (RTX 3090 or 4090). This system, which combines FSDP and QLoRA, is the result of a collaboration between Answer.AI, Tim Dettmers (U Washington), and Hugging Face‚Äôs Titus von Koeller and Sourab Mangrulkar.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/answer-ai-train-70b-llm-at-home?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/answer-ai-train-70b-llm-at-home</guid>
      <pubDate>2024-03-08 12:13</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#qlora</category>
      <category>#gpu</category>
      <category>#nvidia</category>
      <category>#finetuning</category>
    </item>
    <item>
      <title>Levels of Complexity: RAG Applications</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/levels-of-complexity-rag-applications?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/levels-of-complexity-rag-applications&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This post comprehensive guide to understanding and implementing RAG applications across different levels of complexity. Whether you're a beginner eager to learn the basics or an experienced developer looking to deepen your expertise, you'll find valuable insights and practical knowledge to help you on your journey. Let's embark on this exciting exploration together and unlock the full potential of RAG applications.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/levels-of-complexity-rag-applications?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/levels-of-complexity-rag-applications</guid>
      <pubDate>2024-03-07 22:08</pubDate>
      <category>#rag</category>
      <category>#ai</category>
      <category>#llm</category>
      <category>#architecture</category>
      <category>#app</category>
    </item>
    <item>
      <title>5 reasons why desktop Linux is finally growing in popularity</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/year-of-linux-desktop-4-percent?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/year-of-linux-desktop-4-percent&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;StatCounter reported that desktop Linux reached over 4% market share for the first time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Why is Linux finally growing?&lt;br /&gt;
&lt;br&gt;
While Windows is the king of the hill with 72.13% and MacOS comes in a distant second at 15.46%, it's clear that Linux is making progress.&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Microsoft isn't that interested in Windows&lt;/li&gt;
&lt;li&gt;Linux gaming, thanks to Steam, is also growing&lt;/li&gt;
&lt;li&gt;Users are finally figuring out that some Linux distros are easy to use&lt;/li&gt;
&lt;li&gt;Finding and installing Linux desktop software is easier than ever&lt;/li&gt;
&lt;li&gt;The Linux desktop is growing in popularity in India&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/year-of-linux-desktop-4-percent?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/year-of-linux-desktop-4-percent</guid>
      <pubDate>2024-03-07 21:27</pubDate>
      <category>#linux</category>
      <category>#pc</category>
      <category>#desktop</category>
      <category>#desktop</category>
    </item>
    <item>
      <title>Inflection-2.5: meet the world's best personal AI</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/inflection-2-5?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/inflection-2-5&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;At Inflection, our mission is to create a personal AI for everyone. Last May, we released Pi‚Äîa personal AI, designed to be empathetic, helpful, and safe. In November we announced a new major foundation model, Inflection-2, the second best LLM in the world at the time.&lt;br /&gt;
&lt;br&gt;
Now we are adding IQ to Pi‚Äôs exceptional EQ.&lt;br /&gt;
&lt;br&gt;
We are launching Inflection-2.5, our upgraded in-house model that is competitive with all the world's leading LLMs like GPT-4 and Gemini. It couples raw capability with our signature personality and unique empathetic fine-tuning. Inflection-2.5 is available to all Pi's users today, at pi.ai, on iOS, on Android, or our new desktop app.&lt;br /&gt;
&lt;br&gt;
We achieved this milestone with incredible efficiency: Inflection-2.5 approaches GPT-4‚Äôs performance, but used only 40% of the amount of compute for training.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/inflection-2-5?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/inflection-2-5</guid>
      <pubDate>2024-03-07 21:22</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#inflection</category>
    </item>
    <item>
      <title>The surprising connection between after-hours work and decreased productivity</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/surprising-connection-after-hours-work-productivity-slack?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/surprising-connection-after-hours-work-productivity-slack&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Quick take: How do you spend your time at work and what is it costing you? Slack‚Äôs Workforce Index, based on survey responses from more than 10,000 desk workers around the globe, uncovers new findings on how to structure the workday to maximize productivity and strengthen employee well-being and satisfaction.&lt;br /&gt;
&lt;br&gt;
Key learnings include:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Employees who log off at the end of the workday register 20% higher productivity scores than those who feel obligated to work after hours.&lt;/li&gt;
&lt;li&gt;Making time for breaks during the workday improves employee productivity and well-being, and yet half of all desk workers say they rarely or never take breaks.&lt;/li&gt;
&lt;li&gt;On average, desk workers say that the ideal amount of focus time is around four hours a day, and more than two hours a day in meetings is the tipping point at which a majority of workers feel overburdened by meetings.&lt;/li&gt;
&lt;li&gt;Three out of every four desk workers report working in the 3 to 6pm timeframe, but of those, only one in four consider these hours highly productive.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/surprising-connection-after-hours-work-productivity-slack?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/surprising-connection-after-hours-work-productivity-slack</guid>
      <pubDate>2024-03-07 21:14</pubDate>
      <category>#slack</category>
      <category>#productivity</category>
      <category>#work</category>
    </item>
    <item>
      <title>Training great LLMs entirely from ground up in the wilderness as a startup</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/training-llms-ground-up-wilderness-startup?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/training-llms-ground-up-wilderness-startup&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Given that we‚Äôve successfully trained pretty strong multimodal language models at Reka, many people have been particularly curious about the experiences of building infrastructure and training large language &amp;amp; multimodal models from scratch from a completely clean slate.&lt;br /&gt;
&lt;br&gt;
I complain a lot about external (outside Google) infrastructure and code on my social media, leading people to really be curious about what are the things I miss and what I hate/love in the wilderness. So here‚Äôs a post (finally). This blogpost sheds light on the challenges and lessons learned.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Figuring out things in the wilderness was an interesting experience. It was unfortunately not painless. Compute scarcity and also unreliable compute providers made things significantly harder than expected but we‚Äôre glad we pulled through with brute technical strength.&lt;br /&gt;
&lt;br&gt;
All in all, this is only a small part of the story of how we started a company, raised some money, bought some chips and matched Gemini pro/GPT 3.5 and outperformed many others in less than a year having to build everything from scratch.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/training-llms-ground-up-wilderness-startup?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/training-llms-ground-up-wilderness-startup</guid>
      <pubDate>2024-03-07 21:10</pubDate>
      <category>#llm</category>
      <category>#ai</category>
      <category>#startup</category>
      <category>#compute</category>
      <category>#gpu</category>
      <category>#training</category>
    </item>
    <item>
      <title>The missing graph datatype already exists. It was invented in the '70s</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/datalog-the-missing-graph-data-type-already-exists?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/datalog-the-missing-graph-data-type-already-exists&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The datatype for a graph is a relation, and graph algorithms are queries on the relation. But modern languages need better support for the relational model.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This post is a response to/inspired by &lt;a href="https://www.hillelwayne.com/post/graph-types/"&gt;The Hunt for the Missing Data Type (HN) by Hillel Wayne&lt;/a&gt;. I suggest reading his article first.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I claim the reason why it is so difficult to support graphs in languages nowadays is because the imperative/structured programming model of modern programming languages is ill-suited for graph algorithms. As Wayne correctly points out, the core problem is that when you write a graph algorithm in an imperative language like Python or Rust, you have to choose some explicit representation for the graph. Then, your traversal algorithm is dependent on the representation you chose. If you find out later that your representation is no longer efficient, it is a lot of work to adapt your algorithms for a new representation.&lt;br /&gt;
&lt;br&gt;
So what if we just, like, didn‚Äôt do this?&lt;br /&gt;
&lt;br&gt;
We already have a declarative programming language where expressing graph algorithms is extremely natural‚ÄîDatalog, whose semantics are based on* the relational algebra, which was developed in the 1970s.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Wonderful! Except for the ‚Äúwriting Datalog‚Äù part.&lt;br /&gt;
&lt;br&gt;
If Datalog is so great, why hasn‚Äôt it seen more adoption?&lt;br /&gt;
&lt;br&gt;
The short answer is that Datalog is relatively esoteric outside of academia and some industry applications and, as a result, is not a great language from a ‚Äúsoftware engineering‚Äù perspective. It is hard for programmers accustomed to imperative code to write Datalog programs, and large Datalog programs can be hard to write and understand.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/datalog-the-missing-graph-data-type-already-exists?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/datalog-the-missing-graph-data-type-already-exists</guid>
      <pubDate>2024-03-06 21:39</pubDate>
      <category>#datalog</category>
      <category>#programming</category>
      <category>#graphs</category>
      <category>#datatypes</category>
      <category>#programminglanguages</category>
      <category>#software</category>
      <category>#database</category>
      <category>#algorithms</category>
    </item>
    <item>
      <title>Apple Podcasts now includes transcripts</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/apple-podcasts-transcript-17-4?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/apple-podcasts-transcript-17-4&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Apple Podcasts will auto-generate transcripts for podcasts beginning today, thanks to the 17.4 update for iPhones and iPads. Transcripts will automatically appear for new podcast episodes shortly after their publication, while Apple will transcribe podcast back catalogs over time.
&lt;br&gt;
The podcast transcripts are searchable, allowing users to type in a specific word or phrase and skip to that part of an episode. Users can find transcripts for individual podcast episodes on the bottom-left corner of the ‚ÄúNow Playing‚Äù screen.
&lt;br&gt;
Podcasters who don‚Äôt want to use Apple‚Äôs automated transcription can opt to upload their own transcripts via RSS tags or in Apple Podcasts Connect for premium episodes, or they can download and edit Apple‚Äôs transcript before reuploading.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is cool and great for accessibility.&lt;/p&gt;
&lt;p&gt;I recently chose to &lt;a href="/feed/decoder-nilay-patel-why-websites-are-the-future"&gt;read the latest episode of Decoder&lt;/a&gt; instead of listening to it. One of the advantages this also provided was that I could reference direct quotes in my post from the episode.&lt;/p&gt;
&lt;p&gt;I could see Apple taking this further by making it easier to generate show notes / descriptions based on the episode using AI.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/apple-podcasts-transcript-17-4?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/apple-podcasts-transcript-17-4</guid>
      <pubDate>2024-03-05 20:52</pubDate>
      <category>#iphone</category>
      <category>#apple</category>
      <category>#podcasts</category>
      <category>#ios</category>
      <category>#ipados</category>
    </item>
    <item>
      <title>Microsoft to end its Android apps on Windows 11 subsystem in 2025</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/windows-subsystem-for-android-end-of-support-2025?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/windows-subsystem-for-android-end-of-support-2025&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Microsoft is ending support for its Android subsystem in Windows 11 next year. The software giant first announced it was bringing Android apps to Windows 11 with Amazon‚Äôs Appstore nearly three years ago, but this Windows Subsystem for Android will now be deprecated starting March 5th, 2025.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's unfortunate considering the new lineup of ARM-based PCs expected later this year. It would've been nice to have a &lt;a href="/feed/windows-12-mobile-concept"&gt;mobile PC with 5G support&lt;/a&gt; that could run mobile apps for scenarios where there are no web / native PC apps.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/windows-subsystem-for-android-end-of-support-2025?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/windows-subsystem-for-android-end-of-support-2025</guid>
      <pubDate>2024-03-05 20:47</pubDate>
      <category>#windows</category>
      <category>#android</category>
      <category>#wsa</category>
      <category>#pc</category>
    </item>
    <item>
      <title>Stable Diffusion 3: Research Paper</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/stable-diffusion-3-research-paper?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/stable-diffusion-3-research-paper&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Key Takeaways:&lt;br /&gt;
&lt;br&gt;
Today, we‚Äôre publishing our research paper that dives into the underlying technology powering Stable Diffusion 3.&lt;br /&gt;
&lt;br&gt;
Stable Diffusion 3 outperforms state-of-the-art text-to-image generation systems such as DALL¬∑E 3, Midjourney v6, and Ideogram v1 in typography and prompt adherence, based on human preference evaluations.&lt;br /&gt;
&lt;br&gt;
Our new Multimodal Diffusion Transformer (MMDiT) architecture uses separate sets of weights for image and language representations, which improves text understanding and spelling capabilities compared to previous versions of SD3.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/stable-diffusion-3-research-paper?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/stable-diffusion-3-research-paper</guid>
      <pubDate>2024-03-05 12:55</pubDate>
      <category>#stabilityai</category>
      <category>#stablediffusion</category>
      <category>#ai</category>
      <category>#generativeai</category>
      <category>#genai</category>
      <category>#images</category>
    </item>
    <item>
      <title>Wix‚Äôs new AI chatbot builds websites in seconds based on prompts</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/wix-website-builder-ai-chatbot?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/wix-website-builder-ai-chatbot&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;You can now build a website, images and all, using only prompts in Wix‚Äôs new AI website builder. Creating a website is free, but you‚Äôll have to upgrade to one of Wix‚Äôs premium plans if you want to do things like accept payments or don‚Äôt want to be limited to using a Wix domain name.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;You‚Äôd probably need to delve into Wix‚Äôs advanced editing features and know things about actual web development for that. But it was very easy to use the basic AI generator to create something that looks close to a legitimate site to start with, making it much easier to get to a basic starting point.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/wix-website-builder-ai-chatbot?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/wix-website-builder-ai-chatbot</guid>
      <pubDate>2024-03-04 23:15</pubDate>
      <category>#wix</category>
      <category>#ai</category>
      <category>#websites</category>
      <category>#internet</category>
      <category>#web</category>
    </item>
    <item>
      <title>Today‚Äôs smart homes: the hopes and the realities</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/todays-smart-homes-hopes-realities-verge?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/todays-smart-homes-hopes-realities-verge&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Verge team and others share their experiences of how smart technologies affect their lives ‚Äî how it can often help and sometimes frustrate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In these articles, we‚Äôve concentrated on how our own experiences, and the experiences of others, have affected how we regard smart home tech. We‚Äôve got personal accounts by one reporter who decided to put together a brand-new smart home and another whose brother moved into a home haunted by the ghosts of someone else‚Äôs smart tech. Several of our staffers wax enthusiastically about their favorite devices and automations. A writer describes how smart tech makes his home more accessible. Our smart home reviewer tells how she uses technology to keep her varied pets (and she has a lot of them) happy and healthy. We talk to people who use smart devices to help them care for their parents ‚Äî and more.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/todays-smart-homes-hopes-realities-verge?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/todays-smart-homes-hopes-realities-verge</guid>
      <pubDate>2024-03-04 23:13</pubDate>
      <category>#iot</category>
      <category>#verge</category>
      <category>#smarthome</category>
      <category>#technology</category>
    </item>
    <item>
      <title>Guest host Hank Green makes Nilay Patel explain why websites have a future</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/decoder-nilay-patel-why-websites-are-the-future?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/decoder-nilay-patel-why-websites-are-the-future&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the last platform on the web of any scale or influence is Google Search. And so, over time, webpages have become dramatically optimized for Google Search. And that means the kinds of things people write about, the containers that we write in, are mostly designed to be optimized for Google Search. They‚Äôre not designed for, ‚ÄúI need to just quickly tell you about this and move on.‚Äù Our little insight was, ‚ÄúWell, what if we just don‚Äôt do that? What if we only write for the people who come directly to our website instead of the people who find our articles through Search or Google Discover or whatever other Google platforms are in the world?‚Äù And so we just made these little blog posts, and the idea was, if you just come to our website one more time a day because there‚Äôs one more thing to look at that you‚Äôll like, we will be fine.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;more and more people are starting to realize, ‚ÄúOh, we should just make the websites more valuable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...if you start writing for other people, which is the heart of what a blog post really is: it‚Äôs you trying to entertain yourself and trying to entertain just a handful of other people, you‚Äôre going to go really much farther than trying to satisfy the robot.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Why am I writing in the text box that pays money to Elon and Mark [Zuckerberg] and not my text box?&lt;/strong&gt;&lt;br /&gt;
&lt;br&gt;
Why do we all work for free? Look, we want to talk about the platform era and media. Why do we all work for free?&lt;br /&gt;
&lt;br&gt;
...It‚Äôs very confusing, and there are a lot of reasons. If you just sit back and think about why, there are a million reasons why.&lt;br /&gt;
&lt;br&gt;
One, the software is nicer to use than most CMSes. You just pick one. Name a company that makes a CMS. They‚Äôre like, ‚ÄúIs this as fun to use as Twitter?‚Äù And the answer is no. Flatly no. Even the one we have now for quick posts is not as fun to use as Twitter was in its heyday. Will this immediately bring me the dopamine hit of immediate feedback? No.&lt;br /&gt;
&lt;br&gt;
[When redesigning the website]...the first instinct was, ‚ÄúLet‚Äôs at least make it easier to publish. Let‚Äôs at least remove the barriers to entry to getting on the website, and then we can do comments, and then we can think about how we can distribute in different ways.‚Äù So that is working. My team is happier. We did not know that the Twitter thing would happen, but the Twitter thing happened, and our desire to publish in the boxes we controlled went up as a group. And then, on top of it, our audience saw that we were having fun. And once you are having fun anywhere on the internet, people sort of gravitate to you. So traffic has gone up.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The distribution actually just creates the work or creates the pressures that force all the work to be the same. And I think over time that‚Äôs what drives the audiences away. So there‚Äôs a real change in how these platforms work, where, over time, they just become more and more of the same thing and the creators become more and more the same. And that‚Äôs a little exhausting. And every place where you see open distribution, you see a huge variety of creators and content.&lt;br /&gt;
&lt;br&gt;
Podcasts have basically open distribution. Like podcast or distributor RSS feeds, that means people kind of own their distribution, there‚Äôs a vast array of podcast creators. There‚Äôs a vast array of podcast formats. They don‚Äôt all sound like the beginning of YouTube videos or whatever. And I hate to keep picking on YouTube; you can pick any algorithmic platform, and it‚Äôs the same. TikTokers are more the same than different. Podcasters are more different than the same. The web is distributed largely through websites and through RSS. There‚Äôs a huge variety of websites and the way websites look. But then you see the algorithmic search pressure push web design kind of all under the same box.&lt;br /&gt;
&lt;br&gt;
Newsletters distributed by email: open distribution. The newsletter economy is full of a huge variety of creators doing a huge variety of things. They‚Äôre more different than the same. So all I see with the fediverse is, ‚ÄúOh, this is going to open social distribution up a little bit.‚Äù It‚Äôs going to allow us to control our distribution networks. It‚Äôs going to say, ‚ÄúI‚Äôm not on Twitter, but people on Twitter can follow my website, and I can go promote that follow anywhere I want in different ways and build an audience outside of the pressures of the algorithm.‚Äù To me, just that, that ability to try, is 1 percent better.
&lt;br&gt;
If you‚Äôre me and you run a big website and you are thinking, ‚ÄúHow can I redistribute this website, how can I reach people more directly?‚Äù my brain is lit up. You should be able to follow me at TheVerge.com and see all my quick posts in your Threads account when Threads federates.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/decoder-nilay-patel-why-websites-are-the-future?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/decoder-nilay-patel-why-websites-are-the-future</guid>
      <pubDate>2024-03-04 22:45</pubDate>
      <category>#websites</category>
      <category>#internet</category>
      <category>#podcast</category>
      <category>#decoder</category>
      <category>#rss</category>
      <category>#platforms</category>
      <category>#blogs</category>
      <category>#technology</category>
    </item>
    <item>
      <title>Gemma PyTorch</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/gemma-pytorch?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/gemma-pytorch&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Gemma is a family of lightweight, state-of-the art open models built from research and technology used to create Google Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This is the official PyTorch implementation of Gemma models. We provide model and inference implementations using both PyTorch and PyTorch/XLA, and support running inference on CPU, GPU and TPU.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/gemma-pytorch?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/gemma-pytorch</guid>
      <pubDate>2024-03-04 21:29</pubDate>
      <category>#google</category>
      <category>#gemma</category>
      <category>#pytorch</category>
      <category>#ai</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Introducing the next generation of Claude</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/announcing-claude-3-model-family?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/announcing-claude-3-model-family&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus. Each successive model offers increasingly powerful performance, allowing users to select the optimal balance of intelligence, speed, and cost for their specific application.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Claude 3 Opus is our most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Opus shows us the outer limits of what‚Äôs possible with generative AI.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Claude 3 Sonnet strikes the ideal balance between intelligence and speed‚Äîparticularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Claude 3 Haiku is our fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with unmatched speed. Users will be able to build seamless AI experiences that mimic human interactions.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/announcing-claude-3-model-family?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/announcing-claude-3-model-family</guid>
      <pubDate>2024-03-04 21:25</pubDate>
      <category>#ai</category>
      <category>#anthropic</category>
      <category>#claude</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Ente - Private cloud storage for photos, videos, etc.</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ente-private-media-cloud-storage?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ente-private-media-cloud-storage&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Store, share, and rediscover your memories with absolute privacy&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ente-private-media-cloud-storage?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ente-private-media-cloud-storage</guid>
      <pubDate>2024-03-02 15:49</pubDate>
      <category>#e2ee</category>
      <category>#photos</category>
      <category>#cloud</category>
      <category>#storage</category>
      <category>#product</category>
    </item>
    <item>
      <title>GGUF, the long way around</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/gguf-long-way-around?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/gguf-long-way-around&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôve been on a whirlwind adventure to build up our intuition of how machine learning models work, what artifacts they produce, how the machine learning artifact storage story has changed over the past couple years, and finally ended up in GGUF‚Äôs documentation to better understand the log that is presented to us when we perform local inference on artifacts in GGUF.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/gguf-long-way-around?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/gguf-long-way-around</guid>
      <pubDate>2024-02-29 21:49</pubDate>
      <category>#ai</category>
      <category>#genai</category>
      <category>#gguf</category>
      <category>#llm</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Predictive Human Preference: From Model Ranking to Model Routing</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/predictive-human-preference-model-ranking-routing?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/predictive-human-preference-model-ranking-routing&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Human preference has emerged to be both the Northstar and a powerful tool for AI model development. Human preference guides post-training techniques including RLHF and DPO. Human preference is also used to rank AI models, as used by LMSYS‚Äôs Chatbot Arena.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Chatbot Arena aims to determine which model is generally preferred. I wanted to see if it‚Äôs possible to do predictive human preference: determine which model is preferred for each query.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This post first discusses the correctness of Chatbot Arena, which will then be used as a baseline to evaluate the correctness of preference predictions. It then discusses how to build a preference predictor and the initial results.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/predictive-human-preference-model-ranking-routing?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/predictive-human-preference-model-ranking-routing</guid>
      <pubDate>2024-02-29 10:43</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#evaluation</category>
      <category>#ml</category>
    </item>
    <item>
      <title>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/era-1-bit-llms?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/era-1-bit-llms&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/era-1-bit-llms?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/era-1-bit-llms</guid>
      <pubDate>2024-02-28 22:03</pubDate>
      <category>#llm</category>
      <category>#ai</category>
      <category>#bitnet</category>
      <category>#research</category>
    </item>
    <item>
      <title>Tubi‚Äôs new redesign wants to push you down the rabbit hole</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tubi-redesign-down-the-rabbit-hole?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tubi-redesign-down-the-rabbit-hole&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Tubi says its new ‚Äòturple‚Äô-forward brand identity is all about encouraging viewers to fall down rabbit holes to find exciting shows and movies to watch.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I've often found &lt;a href="/feed/streaming-fatigue-collecting-dvds"&gt;good content to watch on Tubi&lt;/a&gt;. Sure, they're not the latest blockbusters but there are decades worth of movies out there. Just like today, not all movies that come out are good but there's still tons of great content. Sometimes, there's even &lt;a href="/feed/dread-central-overlook-motel"&gt;overlooked gems&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tubi-redesign-down-the-rabbit-hole?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tubi-redesign-down-the-rabbit-hole</guid>
      <pubDate>2024-02-28 21:51</pubDate>
      <category>#tv</category>
      <category>#movies</category>
      <category>#tubi</category>
      <category>#streaming</category>
    </item>
    <item>
      <title>Hypertext Gardens: Delightful Vistas</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/hypertext-gardens-delightful-vistas?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/hypertext-gardens-delightful-vistas&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The time, care, and expense devoted to creating and promoting a hypertext are lost if readers arrive, glance around, and click elsewhere. How can the craft of hypertext invite readers to stay, to explore, and to reflect?&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/hypertext-gardens-delightful-vistas?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/hypertext-gardens-delightful-vistas</guid>
      <pubDate>2024-02-28 21:44</pubDate>
      <category>#openweb</category>
      <category>#digitalgarden</category>
      <category>#indieweb</category>
    </item>
    <item>
      <title>What a bunch of A-list celebs taught me about how to use my phone</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/celebs-how-to-use-phone?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/celebs-how-to-use-phone&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...[phone-free celebs are] not trying to disconnect from everyone, but they are trying to get away from that feeling of being tapped constantly on the shoulder by all the calls, texts, and emails.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;So many celebrities ditch their phone, disconnect from their social media, log off entirely.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A few years ago, Ed Sheeran shared a strategy...He hasn‚Äôt had a phone since 2015...Being phoneless hadn‚Äôt cut his contact to the world, Sheeran said, just reduced it ‚Äî and that was the point. ‚ÄúI have friends email and people email, and every few days I‚Äôll sit down and open up my laptop, and I‚Äôll answer 10 emails at a time,‚Äù he said. ‚ÄúI‚Äôll send them off, and I close my laptop, and then that‚Äôll be it. And then I‚Äôll go back to living my life, and I don‚Äôt feel overwhelmed by it.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Read and watch enough celebrity interviews, and the lesson becomes obvious: that the most powerful and connected device in your life shouldn‚Äôt be within arm‚Äôs reach at all times. All that does is invite distraction and makes it too easy to disengage from your life every time you get bored or sad or curious even for a second.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It sounds a little like I‚Äôm advocating for the return of the ‚Äô90s, when the computer was a giant box that lived in a central room of your home and the only way to use it was to go to it. And to some extent, I am! I‚Äôm increasingly convinced that my primary computer should be a device I use on purpose ‚Äî that I sit down at, operate, and then extract myself from until the next time. Whether it‚Äôs a laptop on a desk or an iPad on your nightstand, your computer should be a place as much as it is a device. And when you‚Äôre not in that place, you‚Äôre somewhere else. The computer doesn‚Äôt come along.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Over the last few weeks, as an experiment, I‚Äôve moved as many apps as possible ‚Äî the obviously distracting social media stuff but also anything I can live without on a minute-to-minute basis ‚Äî off my phone and onto my tablet and my laptop...&lt;br /&gt;
&lt;br&gt;
So far, it‚Äôs been great. I‚Äôm realizing how much of a crutch my phone really has become: I would open up TikTok just to keep me company on the walk to the kitchen or scroll through Threads while I waited for the microwave to finish. Now, I‚Äôm not sure I‚Äôm doing any less of those things in aggregate, but at least I‚Äôm doing them on purpose. I‚Äôve turned time-wasting into a deliberate activity ‚Äî I sit in my scrolling chair and scroll away, then I get up, and the scrolling stays put. And best of all, when I leave the house, there‚Äôs nothing to scroll at all.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;There has always been talk in tech about removing friction: the obsessive corporate desire to make everything easier, faster, fewer clicks, fewer chances for you to decide not to click that ad or buy that thing or like that post or upload that photo...It should be a little harder for someone to distract me while I‚Äôm eating dinner with my wife or hanging out with my kid.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It‚Äôs not about ditching technology, just about doing technology on purpose.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/celebs-how-to-use-phone?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/celebs-how-to-use-phone</guid>
      <pubDate>2024-02-28 10:24</pubDate>
      <category>#technology</category>
      <category>#communication</category>
      <category>#phones</category>
      <category>#smartphones</category>
      <category>#dumbphones</category>
      <category>#email</category>
      <category>#celebrities</category>
    </item>
    <item>
      <title>Tumblr and WordPress to Sell Users‚Äô Data to Train AI Tools</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/automattic-ai-data-tumblr-wordpress?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/automattic-ai-data-tumblr-wordpress&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Tumblr and WordPress.com are preparing to sell user data to Midjourney and OpenAI, according to a source with internal knowledge about the deals and internal documentation referring to the deals&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The internal documentation details a messy and controversial process within Tumblr itself. One internal post made by Cyle Gage, a product manager at Tumblr, states that a query made to prepare data for OpenAI and Midjourney compiled a huge number of user posts that it wasn‚Äôt supposed to. It is not clear from Gage‚Äôs post whether this data has already been sent to OpenAI and Midjourney, or whether Gage was detailing a process for scrubbing the data before it was to be sent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I generally enjoy what Automattic does for the web as a whole. However, if these claims are true, it's unfortunate. I believe there's a way to opt-out, but I'd love to learn more before jumping to conclusions.&lt;/p&gt;
&lt;p&gt;That said, WordPress (.com not .org) and Tumblr are platforms just like Reddit, Twitter, and the Meta set of offerings. I'm sure somewhere in their Terms of Service, there's some clauses around their ownserhip of the data you published on their platforms and just like it's sold to data brokers and advertisers, they can also sell it to companies training AI models.&lt;/p&gt;
&lt;p&gt;To counter these types of moves from platforms, I wish it were as easy as saying &lt;a href="https://indieweb.org/own_your_data"&gt;&amp;quot;build your own platform&amp;quot;&lt;/a&gt;. Doing so can be as &amp;quot;simple&amp;quot; as setting up a website using &lt;a href="https://indieweb.org/own_your_links"&gt;your own domain&lt;/a&gt;. Unfortunately, today, it's still not as easy to do and one of the products / companies that help you do that is WordPress. I think it's important though to note the distinction there between WordPress the company and WordPress the technology. Another piece that complicates building your own site is, there's still other ways for companies training AI models to use data that's publicly available on the internet. These are the arguments that are currently being litigated in several legal cases. Maybe there are opportunities to explore &lt;a href="/feed/verge-ai-robots-txt"&gt;a robots.txt for AI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;AI models need high quality data that's representative and as close as possible to the real world in order to improve. There is a role here for &lt;a href="/feed/cosmopedia-ai-synthetic-dataset"&gt;synthetic data&lt;/a&gt;. High quality synthetic data is behind &lt;a href="/feed/phi-2-huggingface"&gt;groundbreaking models like Microsoft's Phi&lt;/a&gt;. Instincts tell me that synthetic data can only go so far though and real data is still needed. In that case, as an AI consumer who makes use of these AI models, but don't want to contribute my data, do I have a responsibility to contribute my data to improve the systems that I use? Piracy aside, in some ways it reminds me of torrenting. You usually run into scenarios where there are multiple people downloading a file. However, there's only a handful of seeders who once obtained, make the file available for others to download. There are also additional considerations such as how people are compensated for contributing their data to these systems. It's important to note that this is not a new problem and people had been thinking about this though in different contexts. Maybe it's time to reconsider ideas like &lt;a href="https://www.microsoft.com/research/wp-content/uploads/2016/02/Inverse11.pdf"&gt;inverse privacy&lt;/a&gt; and &lt;a href="https://hbr.org/2018/09/a-blueprint-for-a-better-digital-society"&gt;data dignity&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are no clear answers here and there are a lot of things to consider. However, it's comforting that as a society we're having these conversations.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/automattic-ai-data-tumblr-wordpress?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/automattic-ai-data-tumblr-wordpress</guid>
      <pubDate>2024-02-27 21:09</pubDate>
      <category>#tumblr</category>
      <category>#automattic</category>
      <category>#wordpress</category>
      <category>#ai</category>
    </item>
    <item>
      <title>The latest Microsoft Copilot update on Android makes me mourn the death of Cortana</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/copilot-android-update-mourn-cortana-death?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/copilot-android-update-mourn-cortana-death&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Microsoft Copilot will soon be able to be your default assistant app on Android.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It's a shame that Cortana never worked out for Microsoft. If things had lined up differently, we may have seen Copilot gain access to smart devices and commands like Gemini has with Google Assistant (though that setup isn't perfect). While Copilot has a place on a computer, I think an assistant on your smartphone needs to be able to do more day-to-day tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So many things were ahead of their time. I just want a &lt;a href="/feed/windows-12-mobile-concept"&gt;Windows Mobile PC (Windows Phone?) with an LLM-backed Cortana&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/copilot-android-update-mourn-cortana-death?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/copilot-android-update-mourn-cortana-death</guid>
      <pubDate>2024-02-27 21:04</pubDate>
      <category>#cortana</category>
      <category>#copilot</category>
      <category>#ai</category>
      <category>#microsoft</category>
      <category>#virtualassistant</category>
      <category>#assistant</category>
      <category>#chatbot</category>
    </item>
    <item>
      <title>Physical media week - Exploring the importance of discs and cartridges in an increasingly digital age</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/verge-physical-media-week?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/verge-physical-media-week&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The bright promise of streaming and digital stores has given way to a darker reality: we rarely have ownership over the art we love, and much is getting lost in the process. Only a fraction of movies released over the last century are available on streaming services, while a staggering 90 percent of classic video games are considered ‚Äúcritically endangered‚Äù by archivists. As these platforms continue to dominate the media landscape, a whole lot of cultural history is being abandoned.&lt;br /&gt;
&lt;br&gt;
In this special issue, The Verge will explore how physical media factors into this and its importance in keeping art alive and accessible. That could mean boutique publishers releasing beautiful special editions of games and movies, foundations dedicated to preserving the physical history of video games, or musicians releasing their latest albums on floppy discs. We‚Äôll also be looking at some cautionary tales in the shift to subscription services and offering tips on building bookshelf-worthy collections.&lt;br /&gt;
&lt;br&gt;
Cartridges and discs have been hurtling toward obsolescence ‚Äî but it turns out, they may be more important than ever.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/verge-physical-media-week?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/verge-physical-media-week</guid>
      <pubDate>2024-02-27 12:21</pubDate>
      <category>#media</category>
      <category>#verge</category>
      <category>#disc</category>
      <category>#dvd</category>
      <category>#cd</category>
      <category>#floppy</category>
      <category>#tech</category>
      <category>#technology</category>
    </item>
    <item>
      <title>JRE - Now available wherever you get your podcasts</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/jre-wherever-you-get-your-podcasts?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/jre-wherever-you-get-your-podcasts&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Since Joe Rogan went exclusive with Spotify, I maybe listened to a handful of episodes. The main reason for it is, I don't use Spotify to listen to podcasts. Periodically, I'd scroll through the feed to see if he had any intereting guests on. As part of his new contract, the podcast is now available on other platforms. That means you can listen &lt;a href="https://podbay.fm/p/the-joe-rogan-experience/about"&gt;wherever you get your podcasts&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/jre-wherever-you-get-your-podcasts?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/jre-wherever-you-get-your-podcasts</guid>
      <pubDate>2024-02-26 18:29</pubDate>
      <category>#rss</category>
      <category>#podcast</category>
      <category>#protocols</category>
      <category>#jre</category>
      <category>#joerogan</category>
      <category>#audio</category>
      <category>#spotify</category>
    </item>
    <item>
      <title>My streaming fatigue got so bad I started collecting DVDs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/streaming-fatigue-collecting-dvds?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/streaming-fatigue-collecting-dvds&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;After spending years reassuring myself that I don‚Äôt need physical copies of movies because of streaming, DVDs have officially reentered my life.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Walmart...Thrift stores, flea markets, the library, and even my local mall‚Äôs FYE have also become places I frequent to get my hands on oft-ignored discs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It makes sense to subscribe to all these services if you‚Äôre into the exclusive content on each one and have the patience to sift through their massive libraries. However, all I‚Äôve been watching lately is the junk on Discovery Plus, simply because I‚Äôm too tired to find anything else ‚Äî especially when the extremely specific shows and movies I want to watch keep switching services or just aren‚Äôt available. One of the most devastating examples of this was when both The Office and Parks and Recreation moved from Netflix to Peacock, disrupting the casual binge-watching sessions that I would default to when I was done with work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Within the past year, nearly every streaming service has raised its prices, including Netflix, Disney Plus, Hulu, Paramount Plus, Discovery Plus, and Apple TV Plus.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I‚Äôm not saying DVDs are flawless: there‚Äôs a reason no one wants them anymore!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Despite this, it‚Äôs still nice to have something that you physically own and don‚Äôt even need an internet connection to use. So when Best Buy confirmed it would stop selling DVDs this year and rumors emerged that Walmart would do the same, I was pretty disappointed. I can‚Äôt imagine Walmart without its bin of DVDs, nor can I even see Best Buy without its already-shrunken selection of movies.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It‚Äôs 2024, and I‚Äôm not ready to say goodbye to DVDs ‚Äî in fact, I‚Äôm just getting started.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Great article from Emma.&lt;/p&gt;
&lt;p&gt;Personally, I've been doing the same. Just a few weekends ago, I got something like 8-10 DVDs for ~$25 at my local thrift store. That haul included 3 seasons of The Sopranos.&lt;/p&gt;
&lt;p&gt;With streaming services taking back control of their content and putting it on their own platforms, I don't want to have to keep signing up for a new service just to watch the shows and movies I enjoy. Also, that's assuming you can find the content to begin with (i.e. Westworld).&lt;/p&gt;
&lt;p&gt;Parks and Recreation, The Office, and Breaking Bad were some of the first shows I started collecting and have slowly been building up my collection. To save on space, I've ditched the cases and have the DVDs organized in a CD case. I've not only limited myself to DVDs but have also started collecting CDs as well.&lt;/p&gt;
&lt;p&gt;Whenever I want variety, I just use one of the free streaming services like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.justwatch.com/us/provider/freevee"&gt;Freevee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pluto.tv/"&gt;Pluto TV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://tubitv.com/"&gt;Tubi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;YouTube&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Yes, there are ads but at least I'm not paying for it and I know that's part of the deal. There are a ton of good older (and sometimes original) TV shows and movies on those platforms to keep me entertained. The most recent ones being Stargate and &lt;a href="/feed/vampires-kiss-movie"&gt;Vampire's Kiss&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/streaming-fatigue-collecting-dvds?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/streaming-fatigue-collecting-dvds</guid>
      <pubDate>2024-02-26 15:44</pubDate>
      <category>#streaming</category>
      <category>#media</category>
      <category>#dvds</category>
      <category>#movies</category>
      <category>#tv</category>
      <category>#physicalmedia</category>
    </item>
    <item>
      <title>Announcing Mistral Large</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/announcing-mistral-large?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/announcing-mistral-large&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mistral Large is our flagship model, with top-tier reasoning capacities. It is also available on Azure.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mistral Large comes with new capabilities and strengths:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is natively fluent in English, French, Spanish, German, and Italian, with a nuanced understanding of grammar and cultural context.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Its 32K tokens context window allows precise information recall from large documents.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Its precise instruction-following enables developers to design their moderation policies ‚Äì we used it to set up the system-level moderation of le Chat.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;It is natively capable of function calling. This, along with constrained output mode, implemented on la Plateforme, enables application development and tech stack modernisation at scale.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;At Mistral, our mission is to make frontier AI ubiquitous. This is why we‚Äôre announcing today that we‚Äôre bringing our open and commercial models to Azure.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/announcing-mistral-large?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/announcing-mistral-large</guid>
      <pubDate>2024-02-26 15:39</pubDate>
      <category>#ai</category>
      <category>#mistral</category>
      <category>#llm</category>
      <category>#opensource</category>
      <category>#azure</category>
    </item>
    <item>
      <title>The direct influence of Twin Peaks on Zelda</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/direct-influence-twin-peaks-zelda?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/direct-influence-twin-peaks-zelda&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In a 2010 interview, Link‚Äôs Awakening director Takashi Tezuka revealed the inspiration for this memorably bizarre cast of characters. ‚ÄúAt the time, Twin Peaks was rather popular. The drama was all about a small number of characters in a small town,‚Äù Tezuka said. ‚ÄúSo I wanted to make something like that, while it would be small enough in scope to easily understand, it would have deep and distinctive characteristics.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;... [Mark] Frost reveals in an interview with The Verge, he actually spoke with Nintendo about the Zelda franchise. ‚ÄúI don‚Äôt want to overstate it. It was a single conversation. But it was fun,‚Äù he tells me.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;‚ÄúThey were talking to me about a Twin Peaks game, and they mentioned Zelda at the time,‚Äù says Frost. ‚ÄúThey said, ‚ÄòOne of the things we love about your show is how there‚Äôs all sorts of sideways associations that can drive the story forward.‚Äô They asked me about that as they were thinking about expanding the Zelda universe.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Though he‚Äôd never played a Zelda game, Frost had enough experience with fantasy storytelling that he had some suggestions. ‚ÄúI‚Äôd played lots of Dungeons &amp;amp; Dragons when I was young, so I was familiar with the kind of story they were thinking about,‚Äù he says. ‚ÄúI think I said, ‚ÄòDon‚Äôt be afraid to use dreamlike, Jungian symbolism. Things can connect thematically without having to connect concretely.‚Äô It was things like that that I was urging them [to consider].‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/direct-influence-twin-peaks-zelda?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/direct-influence-twin-peaks-zelda</guid>
      <pubDate>2024-02-24 12:14</pubDate>
      <category>#twinpeaks</category>
      <category>#nintendo</category>
      <category>#zelda</category>
      <category>#tv</category>
      <category>#culture</category>
      <category>#gaming</category>
    </item>
    <item>
      <title>GPT in 500 lines of SQL</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/gpt-500-lines-sql?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/gpt-500-lines-sql&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;a href="https://github.com/quassnoi/explain-extended-2024"&gt;GitHub Repo&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/gpt-500-lines-sql?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/gpt-500-lines-sql</guid>
      <pubDate>2024-02-24 12:06</pubDate>
      <category>#sql</category>
      <category>#gpt</category>
      <category>#ai</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Ali vs. Frazier I</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ali-frazier-I?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ali-frazier-I&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=eIm2eK5uuVA" title="Boxing match between Joe Frazier and Muhammad Ali I"&gt;&lt;img src="http://img.youtube.com/vi/eIm2eK5uuVA/0.jpg" class="img-fluid" alt="Boxing match between Joe Frazier and Muhammad Ali I" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Such a good fight. Frazier had no defense but just kept coming forward.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ali-frazier-I?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ali-frazier-I</guid>
      <pubDate>2024-02-24 00:19</pubDate>
      <category>#boxing</category>
      <category>#ali</category>
      <category>#frazier</category>
    </item>
    <item>
      <title>Jim Cramer says McDonald‚Äôs embracing AI at drive-thrus is good news for Nvidia</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mcdonalds-ai-drive-thru-nvidia?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mcdonalds-ai-drive-thru-nvidia&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I wish they'd use AI to keep the ice cream machines from breaking instead.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mcdonalds-ai-drive-thru-nvidia?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mcdonalds-ai-drive-thru-nvidia</guid>
      <pubDate>2024-02-24 00:02</pubDate>
      <category>#mcdonalds</category>
      <category>#ai</category>
      <category>#nvidia</category>
    </item>
    <item>
      <title>Salami Rose Joe Louis Live Band at Bandcamp - May 2023</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/salami-rose-joe-louis-live-bandcamp-2023?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/salami-rose-joe-louis-live-bandcamp-2023&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=VrnEQ3TqZGE" title="Salami Rose Jose Louis and a band playing a live concert"&gt;&lt;img src="http://img.youtube.com/vi/VrnEQ3TqZGE/0.jpg" class="img-fluid" alt="Salami Rose Jose Louis and a band playing a live concert" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Great performance. I remember the first time I listened to Salami was when she opened for Flying Lotus and she was amazing.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/salami-rose-joe-louis-live-bandcamp-2023?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/salami-rose-joe-louis-live-bandcamp-2023</guid>
      <pubDate>2024-02-22 22:38</pubDate>
      <category>#brainfeeder</category>
      <category>#salamirosejoelouis</category>
      <category>#music</category>
      <category>#bandcamp</category>
    </item>
    <item>
      <title>Bluesky: An Open Social Web</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/bluesky-open-social-web?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/bluesky-open-social-web&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we‚Äôre excited to announce that the Bluesky network is federating and opening up in a way that allows you to host your own data. What does this mean?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Your data, such as your posts, likes, and follows, needs to be stored somewhere. With traditional social media, your data is stored by the social media company whose services you've signed up for. If you ever want to stop using that company's services, you can do that‚Äîbut you would have to leave that social network and lose your existing connections.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It doesn't have to be this way! An alternative model is how the internet itself works. Anyone can put up a website on the internet&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We think social media should work the same way. When you register on Bluesky, by default we'll suggest that Bluesky will store your data. But if you'd like to let another company store it, or even store it yourself, you can do that. You'll also be able to change your mind at any point, moving your data to another provider without losing any of your existing posts, likes, or follows. From your followers' perspective, your profile is always available at your handle‚Äîno matter where your information is actually stored, or how many times it has been moved.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I don't spend a lot of time on Bluesky, but I love what they're doing.&lt;/p&gt;
&lt;p&gt;They now:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://bsky.social/about/blog/02-06-2024-join-bluesky"&gt;Made their service generally available&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Have over &lt;a href="https://bsky.app/profile/bsky.app/post/3klzrudt4uk2z"&gt;5 million users&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="/feed/bluesky-rss-support"&gt;Support RSS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now you can self-host your data. I'm excited.&lt;/p&gt;
&lt;p&gt;The other piece of this that's interesting is their feature which enables you to &lt;a href="https://bsky.social/about/blog/7-05-2023-namecheap"&gt;use your domain as a custom handle&lt;/a&gt;. Not only is your identity portable, but also your data. I'd be interested to see how this works in practice given you can already do some of this on the Fediverse on platforms like Mastodon. Again, that portable identity component to me is crucial. That's one of the challenges with Mastodon today. While you can move instances, your identity changes and while most of your data comes with you, there's tihngs that still dont transfer over. The other part that I'd be interested in seeing is whether or not they can be efficient in the storage of federated data. One of the challenges with Mastodon is, your server quickly fills up because of data from other instances (when you federate). &lt;a href="feed/mastodon-4-1-0-release"&gt;It's gotten better&lt;/a&gt;, but this is an area I spend most of my time when maintaining my own self-hosted instance.&lt;/p&gt;
&lt;p&gt;I'm excited to tinker and self-host my own data. Maybe I'll also &lt;a href="/posts/rss-to-mastodon-posse-azure-logic-apps"&gt;syndicate to Bluesky just like I do with Mastodon today&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the meantime, you can find me on Bluesky &lt;a href="/bluesky"&gt;@lqdev.me&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/bluesky-open-social-web?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/bluesky-open-social-web</guid>
      <pubDate>2024-02-22 21:03</pubDate>
      <category>#bluesky</category>
      <category>#socialmedia</category>
      <category>#web</category>
      <category>#internet</category>
      <category>#community</category>
    </item>
    <item>
      <title>Stable Diffusion 3 - Early Preview</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/stable-diffusion-3-early-preview?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/stable-diffusion-3-early-preview&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Announcing Stable Diffusion 3 in early preview, our most capable text-to-image model with greatly improved performance in multi-subject prompts, image quality, and spelling abilities.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Stable Diffusion 3 suite of models currently range from 800M to 8B parameters.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable Diffusion 3 combines a diffusion transformer architecture and flow matching.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/stable-diffusion-3-early-preview?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/stable-diffusion-3-early-preview</guid>
      <pubDate>2024-02-22 20:59</pubDate>
      <category>#stabilityai</category>
      <category>#ai</category>
      <category>#stablediffusion</category>
      <category>#imagegeneration</category>
      <category>#ml</category>
      <category>#image</category>
      <category>#genai</category>
      <category>#generativeai</category>
    </item>
    <item>
      <title>The AI Study Guide: Azure Machine Learning Edition</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ai-study-guide-azure-ml-edition?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ai-study-guide-azure-ml-edition&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The AI Study Guide: Discover Machine Learning with these free Azure resources&lt;br /&gt;
&lt;br&gt;
Welcome to the February edition of the Azure AI Study Guide. Every month I‚Äôll be spilling the tea on the best and newest tools for skilling up on Azure AI. This month we‚Äôre putting on our thinking caps to investigate Azure Machine Learning (ML). I‚Äôll give you a quick breakdown of what it is, then we‚Äôll explore a four-week roadmap of our top FREE resources for you to continue your AI learning journey!‚ÄØAnd as a bonus, stay tuned to the end to see what makes machine learning and generative AI a dynamic duo.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ai-study-guide-azure-ml-edition?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ai-study-guide-azure-ml-edition</guid>
      <pubDate>2024-02-21 15:18</pubDate>
      <category>#azure</category>
      <category>#tutorial</category>
      <category>#machinelearning</category>
      <category>#ml</category>
      <category>#ai</category>
      <category>#azureml</category>
    </item>
    <item>
      <title>On Blogs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/on-blogs-searls?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/on-blogs-searls&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Thoughts I jotted down on Mastodon:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Blogs are newsletters that don‚Äôt require subscriptions.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Blogrolls are lists of blogs.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;Both require the lowest possible cognitive and economic overhead.&lt;br /&gt;
&lt;br&gt;&lt;/li&gt;
&lt;li&gt;That‚Äôs why they are coming back.&lt;br /&gt;
&lt;br&gt;
I know, they never left. But you get my point.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/on-blogs-searls?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/on-blogs-searls</guid>
      <pubDate>2024-02-21 15:14</pubDate>
      <category>#blogs</category>
      <category>#writing</category>
      <category>#newsletters</category>
      <category>#blogroll</category>
    </item>
    <item>
      <title>Windows 12 Mobile - Concept</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/windows-12-mobile-concept?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/windows-12-mobile-concept&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Cool concept.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=XhwFp6lMOcQ" title="Windows 12 Mobile Concept Video"&gt;&lt;img src="http://img.youtube.com/vi/XhwFp6lMOcQ/0.jpg" class="img-fluid" alt="Windows 12 Mobile Concept Video" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The likelihood of it happening is low, but there's a lot of really great opportunities here especially with &lt;a href="/posts/quick-thoughts-snapdragon-summit-2023"&gt;the new wave or ARM PCs coming&lt;/a&gt;. I don't know what the device form factor looks like, but I wouldn't mind carrying around a pocket PC - a true mobile computer. Already with the Windows Store, you have access to tons of apps. For the apps that aren't in the Store, there's the browser. That seemed to be good enough for &lt;a href="/feed/vision-pros-most-important-app-safari"&gt;Apple's Vision Pro&lt;/a&gt;. Taking it a step further would the app gap matter as much if you have Copilot as your concierge orchestrating tasks for you using the various services? Better yet, what if these services had their own assistants / GPTs Copilot could talk to and coordinate on your behalf?&lt;/p&gt;
&lt;p&gt;At some point, I might just use &lt;a href="/feed/openai-sora"&gt;OpenAI's Sora model&lt;/a&gt; to live vicariously through an AI-generated video depicting this alternate reality where Windows Phone exists...&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/windows-12-mobile-concept?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/windows-12-mobile-concept</guid>
      <pubDate>2024-02-21 14:59</pubDate>
      <category>#windows</category>
      <category>#mobile</category>
      <category>#windowsphone</category>
      <category>#pc</category>
    </item>
    <item>
      <title>The killer app of Gemini Pro 1.5 is video</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/gemini-pro-video-willison?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/gemini-pro-video-willison&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I‚Äôve been playing with Gemini Pro 1.5 for a few days, and I think the most exciting feature isn‚Äôt so much the token count... it‚Äôs the ability to use video as an input.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The ability to extract structured content from text is already one of the most exciting use-cases for LLMs. GPT-4 Video and LLaVA expanded that to images. And now Gemini Pro 1.5 expands that to video.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The ability to analyze video like this feels SO powerful. Being able to take a 20 second video of a bookshelf and get back a JSON array of those books is just the first thing I thought to try.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...as always with modern AI, there are still plenty of challenges to overcome...But this really does feel like another one of those glimpses of a future that‚Äôs suddenly far closer then I expected it to be.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/gemini-pro-video-willison?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/gemini-pro-video-willison</guid>
      <pubDate>2024-02-21 14:45</pubDate>
      <category>#google</category>
      <category>#ai</category>
      <category>#gemini</category>
    </item>
    <item>
      <title>HuggingChat</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/huggingchat?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/huggingchat&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The goal of this app is to showcase that it is now possible to build an open source alternative to ChatGPT.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/huggingface/chat-ui"&gt;GitHub repo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/huggingface/text-generation-inference"&gt;Text Generation Inference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/huggingchat?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/huggingchat</guid>
      <pubDate>2024-02-21 14:38</pubDate>
      <category>#huggingface</category>
      <category>#assistants</category>
      <category>#ai</category>
      <category>#chat</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Gemma: Introducing new state-of-the-art open models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/gemma-google-ai-models?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/gemma-google-ai-models&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. Developed by Google DeepMind and other teams across Google, Gemma is inspired by Gemini, and the name reflects the Latin gemma, meaning ‚Äúprecious stone.‚Äù Accompanying our model weights, we‚Äôre also releasing tools to support developer innovation, foster collaboration, and guide responsible use of Gemma models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b"&gt;HuggingFace Gemma Release&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/gemma-google-ai-models?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/gemma-google-ai-models</guid>
      <pubDate>2024-02-21 14:30</pubDate>
      <category>#ai</category>
      <category>#opensource</category>
      <category>#google</category>
      <category>#model</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Podcast - localfirst.fm</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/localfirstfm-podcast?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/localfirstfm-podcast&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A podcast about local-first software development&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/localfirstfm-podcast?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/localfirstfm-podcast</guid>
      <pubDate>2024-02-20 21:38</pubDate>
      <category>#development</category>
      <category>#software</category>
      <category>#podcast</category>
      <category>#tech</category>
      <category>#podcast</category>
      <category>#localfirst</category>
      <category>#local</category>
    </item>
    <item>
      <title>Keep your phone number private with Signal usernames</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/signal-usernames?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/signal-usernames&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Signal‚Äôs mission and sole focus is private communication. For years, Signal has kept your messages private, your profile information (like your name and profile photo) private, your contacts private, and your groups private ‚Äì among much else. Now we‚Äôre taking that one step further, by making your phone number on Signal more private.&lt;br /&gt;
&lt;br&gt;
Here‚Äôs how:&lt;br /&gt;
&lt;br&gt;
New default: Your phone number will no longer be visible to everyone in Signal...&lt;br /&gt;
&lt;br&gt;
Connect without sharing your phone number...&lt;br /&gt;
&lt;br&gt;
Control who can find you on Signal by phone number...&lt;br /&gt;
&lt;br&gt;
Right now, these options are in beta, and will be rolling out to everyone in the coming weeks.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/signal-usernames?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/signal-usernames</guid>
      <pubDate>2024-02-20 17:01</pubDate>
      <category>#signal</category>
      <category>#privacy</category>
      <category>#security</category>
      <category>#osint</category>
      <category>#messaging</category>
    </item>
    <item>
      <title>Cosmopedia v0.1</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/cosmopedia-ai-synthetic-dataset?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/cosmopedia-ai-synthetic-dataset&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Cosmopedia is a dataset of synthetic textbooks, blogposts, stories, posts and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1.The dataset contains over 30 million files and 25 billion tokens, making it the largest open synthetic dataset to date.&lt;br /&gt;
&lt;br&gt;
It covers a variety of topics; we tried to map world knowledge present in Web datasets like RefinedWeb and RedPajama, and generate synthetic content that covers them. This is the v0.1 of Cosmopedia, with ample room for improvement and topics to be more comprehensively covered. We hope this dataset will help the community's research efforts in the increasingly intriguing domain of synthetic data.&lt;br /&gt;
&lt;br&gt;
This work is inspired by the great work of Phi1.5.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/cosmopedia-ai-synthetic-dataset?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/cosmopedia-ai-synthetic-dataset</guid>
      <pubDate>2024-02-20 16:50</pubDate>
      <category>#cosmopedia</category>
      <category>#dataset</category>
      <category>#huggingface</category>
      <category>#mixtral</category>
      <category>#ai</category>
      <category>#genai</category>
      <category>#llm</category>
    </item>
    <item>
      <title>MLX Swift - On-device ML research with MLX and Swift</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/swift-mlx-ml-research?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/swift-mlx-ml-research&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Swift programming language has a lot of potential to be used for machine learning research because it combines the ease of use and high-level syntax of a language like Python with the speed of a compiled language like C++.&lt;br /&gt;
&lt;br&gt;
&lt;a href="https://github.com/ml-explore/mlx"&gt;MLX&lt;/a&gt; is an array framework for machine learning research on Apple silicon. MLX is intended for research and not for production deployment of models in apps.&lt;br /&gt;
&lt;br&gt;
&lt;a href="https://github.com/ml-explore/mlx-swift/"&gt;MLX Swift&lt;/a&gt; expands MLX to the Swift language, making experimentation on Apple silicon easier for ML researchers.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/swift-mlx-ml-research?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/swift-mlx-ml-research</guid>
      <pubDate>2024-02-20 16:25</pubDate>
      <category>#apple</category>
      <category>#mlx</category>
      <category>#swift</category>
      <category>#opensource</category>
      <category>#ml</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Ollama - Windows Preview</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ollama-windows-preview?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ollama-windows-preview&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Ollama is now available on Windows in preview, making it possible to pull, run and create large language models in a new native Windows experience. Ollama on Windows includes built-in GPU acceleration, access to the full model library, and the Ollama API including OpenAI compatibility.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://ollama.com/download/windows"&gt;Download (https://ollama.com/download/windows)&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ollama-windows-preview?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ollama-windows-preview</guid>
      <pubDate>2024-02-19 16:13</pubDate>
      <category>#ollama</category>
      <category>#windows</category>
      <category>#llm</category>
      <category>#opensource</category>
      <category>#localmodels</category>
      <category>#ml</category>
      <category>#ai</category>
    </item>
    <item>
      <title>StatiCrypt: Protected Posts on a Static Site</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/staticrypt-protected-posts-static-site-jayless?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/staticrypt-protected-posts-static-site-jayless&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...I was longing for a way to do friends-only blog posts on the open web, today I came across Stati¬≠Crypt‚Äâ, an open-source utility that lets you encrypt static HTML pages behind a password.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/staticrypt-protected-posts-static-site-jayless?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/staticrypt-protected-posts-static-site-jayless</guid>
      <pubDate>2024-02-17 12:27</pubDate>
      <category>#staticsite</category>
      <category>#web</category>
      <category>#blogs</category>
      <category>#community</category>
      <category>#openweb</category>
      <category>#security</category>
      <category>#privacy</category>
    </item>
    <item>
      <title>HuggingFace - Open Source AI Cookbook</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/open-source-ai-cookbook-hf?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/open-source-ai-cookbook-hf&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Open-Source AI Cookbook is a collection of notebooks illustrating practical aspects of building AI applications and solving various machine learning tasks using open-source tools and models.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/open-source-ai-cookbook-hf?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/open-source-ai-cookbook-hf</guid>
      <pubDate>2024-02-16 22:36</pubDate>
      <category>#ai</category>
      <category>#tutorial</category>
      <category>#huggingface</category>
      <category>#opensource</category>
      <category>#ml</category>
    </item>
    <item>
      <title>Observable 2.0 - Announcing Observable Framework</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/observable-2-0?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/observable-2-0&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we‚Äôre launching Observable 2.0 with a bold new vision: an open-source static site generator for building fast, beautiful data apps, dashboards, and reports.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our mission is to help teams communicate more effectively with data. Effective presentation of data is critical for deep insight, nuanced understanding, and informed decisions. Observable notebooks are great for ephemeral, ad hoc data exploration. But notebooks aren‚Äôt well-suited for polished dashboards and apps.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/observable-2-0?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/observable-2-0</guid>
      <pubDate>2024-02-15 20:48</pubDate>
      <category>#data</category>
      <category>#analytics</category>
      <category>#observable</category>
    </item>
    <item>
      <title>V-JEPA: The next step toward Yann LeCun‚Äôs vision of advanced machine intelligence (AMI)</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/v-jepa-meta?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/v-jepa-meta&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we‚Äôre publicly releasing the Video Joint Embedding Predictive Architecture (V-JEPA) model, a crucial step in &lt;a href="https://openreview.net/pdf?id=BZ5a1r-kVsf"&gt;advancing machine intelligence&lt;/a&gt; with a more grounded understanding of the world.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This early example of a physical world model excels at detecting and understanding highly detailed interactions between objects.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In the spirit of responsible open science, we‚Äôre releasing this model under a Creative Commons NonCommercial license for researchers to further explore.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/v-jepa-meta?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/v-jepa-meta</guid>
      <pubDate>2024-02-15 20:20</pubDate>
      <category>#meta</category>
      <category>#ai</category>
      <category>#ami</category>
      <category>#ml</category>
      <category>#opensource</category>
      <category>#multimodal</category>
    </item>
    <item>
      <title>Magic.dev</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/magic-dev?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/magic-dev&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Magic is working on frontier-scale code models to build a coworker, not just a copilot.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/magic-dev?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/magic-dev</guid>
      <pubDate>2024-02-15 20:18</pubDate>
      <category>#ai</category>
      <category>#copilot</category>
      <category>#magic</category>
      <category>#code</category>
      <category>#agi</category>
    </item>
    <item>
      <title>OpenAI Sora - Creating video from text</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openai-sora?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openai-sora&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Sora is an AI model that can create realistic and imaginative scenes from text instructions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Sora can generate videos up to a minute long while maintaining visual quality and adherence to the user‚Äôs prompt.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openai-sora?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openai-sora</guid>
      <pubDate>2024-02-15 20:15</pubDate>
      <category>#openai</category>
      <category>#multimodal</category>
      <category>#ai</category>
      <category>#texttovideo</category>
    </item>
    <item>
      <title>Introducing Gemini 1.5</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/gemini-1-5-google?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/gemini-1-5-google&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we‚Äôre announcing our next-generation model: Gemini 1.5.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Gemini 1.5 delivers dramatically enhanced performance. It represents a step change in our approach, building upon research and engineering innovations across nearly every part of our foundation model development and infrastructure. This includes making Gemini 1.5 more efficient to train and serve, with a new Mixture-of-Experts (MoE) architecture.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Gemini 1.5 Pro comes with a standard 128,000 token context window. But starting today, a limited group of developers and enterprise customers can try it with a context window of up to 1 million tokens via AI Studio and Vertex AI in private preview.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/gemini-1-5-google?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/gemini-1-5-google</guid>
      <pubDate>2024-02-15 20:11</pubDate>
      <category>#ai</category>
      <category>#gemini</category>
      <category>#lm</category>
      <category>#google</category>
    </item>
    <item>
      <title>Club TWiT Shows Now Open to Everyone!</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/club-twit-shows-available-to-all?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/club-twit-shows-available-to-all&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are thrilled to announce that our Club TWiT shows are now available to everyone in audio form. That's right, you can now listen to your favorite shows anytime, anywhere, and it's all starting as early as the end of this week.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/club-twit-shows-available-to-all?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/club-twit-shows-available-to-all</guid>
      <pubDate>2024-02-14 22:37</pubDate>
      <category>#twit</category>
      <category>#podcast</category>
      <category>#tech</category>
      <category>#community</category>
    </item>
    <item>
      <title>TWiT's Lesser Known RSS Feeds</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/twit-lesser-known-rss-feeds?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/twit-lesser-known-rss-feeds&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Subscribed!&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Many people are unaware that TWiT also has RSS feeds designed for news aggregators like Feedly, NetNewsWire, Mozilla Thunderbird, and Akregator. These feeds are not meant for podcast apps but are specifically designed for news aggregators. You can copy any of the RSS feed links below into your RSS feed reader of choice and get updates on the latest TWiT blog posts, articles, or podcasts as soon as they are published.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/twit-lesser-known-rss-feeds?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/twit-lesser-known-rss-feeds</guid>
      <pubDate>2024-02-14 22:35</pubDate>
      <category>#twit</category>
      <category>#rss</category>
      <category>#tech</category>
      <category>#news</category>
      <category>#community</category>
    </item>
    <item>
      <title>We ‚ù§Ô∏è RSS</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/we-love-rss-ott?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/we-love-rss-ott&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;What makes RSS so powerful is that it is an open format. RSS is one of the reasons the blogosphere grew so rapidly and it is the reason why podcasting exploded: because this open format allowed everyone to participate by simply publishing a feed anywhere on the web, without being restricted by platform requirements, closed APIs, and paywalls. And this superpower is also why RSS is having a renaissance today: it allows everyone to subscribe to, share, syndicate, and cross-post content on the open web. And it also enables creative automations using tools like Zapier, IFTTT, Huggin, or n8n.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;There is no denying that RSS is having a moment again. Not only because it allows us all to improve the discoverability of our work and explore online content in a personalized and deliberate way, but also because it remains one of the most powerful and influential technologies of the open web. RSS already is the cornerstone of many open technology systems like podcasting, which can‚Äôt be owned and controlled by any one company. As Anil Dash notes, this alone is radical, because it is the triumph of exactly the kind of technology that's supposed to be impossible: open and empowering tech that allows people to have ownership over their work and their relationship with their audience.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/we-love-rss-ott?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/we-love-rss-ott</guid>
      <pubDate>2024-02-14 22:27</pubDate>
      <category>#rss</category>
      <category>#web</category>
      <category>#internet</category>
      <category>#oss</category>
      <category>#openweb</category>
      <category>#indieweb</category>
    </item>
    <item>
      <title>The text file that runs the internet</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/verge-ai-robots-txt?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/verge-ai-robots-txt&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;For three decades, a tiny text file has kept the internet from chaos. This text file has no particular legal or technical authority, and it‚Äôs not even particularly complicated. It represents a handshake deal between some of the earliest pioneers of the internet to respect each other‚Äôs wishes and build the internet in a way that benefitted everybody. It‚Äôs a mini constitution for the internet, written in code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It‚Äôs called robots.txt and is usually located at yourwebsite.com/robots.txt. That file allows anyone who runs a website ‚Äî big or small, cooking blog or multinational corporation ‚Äî to tell the web who‚Äôs allowed in and who isn‚Äôt. Which search engines can index your site? What archival projects can grab a version of your page and save it? Can competitors keep tabs on your pages for their own files? You get to decide and declare that to the web.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It‚Äôs not a perfect system, but it works. Used to, anyway. For decades, the main focus of robots.txt was on search engines; you‚Äôd let them scrape your site and in exchange they‚Äôd promise to send people back to you. Now AI has changed the equation: companies around the web are using your site and its data to build massive sets of training data, in order to build models and products that may not acknowledge your existence at all.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The robots.txt file governs a give and take; AI feels to many like all take and no give. But there‚Äôs now so much money in AI, and the technological state of the art is changing so fast that many site owners can‚Äôt keep up. And the fundamental agreement behind robots.txt, and the web as a whole ‚Äî which for so long amounted to ‚Äúeverybody just be cool‚Äù ‚Äî may not be able to keep up either.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/verge-ai-robots-txt?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/verge-ai-robots-txt</guid>
      <pubDate>2024-02-14 20:47</pubDate>
      <category>#ai</category>
      <category>#internet</category>
      <category>#web</category>
    </item>
    <item>
      <title>Introducing Sudo for Windows</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-sudo-windows?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-sudo-windows&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Sudo for Windows is a new way for users to run elevated commands directly from an unelevated console session. It is an ergonomic and familiar solution for users who want to elevate a command without having to first open a new elevated console.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are also excited to announce that we are open-sourcing this project here on GitHub!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/microsoft/sudo"&gt;GitHub Repo&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-sudo-windows?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-sudo-windows</guid>
      <pubDate>2024-02-13 21:51</pubDate>
      <category>#windows</category>
      <category>#sudo</category>
      <category>#system</category>
    </item>
    <item>
      <title>GraphRAG: Unlocking LLM discovery on narrative private data </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ms-research-graphrag?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ms-research-graphrag&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Perhaps the greatest challenge ‚Äì and opportunity ‚Äì of LLMs is extending their powerful capabilities to solve problems beyond the data on which they have been trained, and to achieve comparable results with data the LLM has never seen.  This opens new possibilities in data investigation, such as identifying themes and semantic concepts with context and grounding on datasets.  In this post, we introduce GraphRAG, created by Microsoft Research, as a significant advance in enhancing the capability of LLMs.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ms-research-graphrag?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ms-research-graphrag</guid>
      <pubDate>2024-02-13 21:50</pubDate>
      <category>#research</category>
      <category>#ai</category>
      <category>#rag</category>
      <category>#llm</category>
      <category>#data</category>
      <category>#microsoft</category>
    </item>
    <item>
      <title>Every Noise at Once</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/every-noise-at-once?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/every-noise-at-once&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Every Noise at Once is an ongoing attempt at an algorithmically-generated, readability-adjusted scatter-plot of the musical genre-space, based on data tracked and analyzed for 6,291 genre-shaped distinctions by Spotify as of 2023-11-19. The calibration is fuzzy, but in general down is more organic, up is more mechanical and electric; left is denser and more atmospheric, right is spikier and bouncier.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/every-noise-at-once?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/every-noise-at-once</guid>
      <pubDate>2024-02-13 21:19</pubDate>
      <category>#music</category>
      <category>#genres</category>
      <category>#discovery</category>
      <category>#spotify</category>
    </item>
    <item>
      <title>NVIDIA Chat with RTX</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nvidia-chat-rtx?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nvidia-chat-rtx&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Chat With RTX is a demo app that lets you personalize a GPT large language model (LLM) connected to your own content‚Äîdocs, notes, videos, or other data. Leveraging retrieval-augmented generation (RAG), TensorRT-LLM, and RTX acceleration, you can query a custom chatbot to quickly get contextually relevant answers. And because it all runs locally on your Windows RTX PC or workstation, you‚Äôll get fast and secure results.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nvidia-chat-rtx?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nvidia-chat-rtx</guid>
      <pubDate>2024-02-13 21:08</pubDate>
      <category>#nvidia</category>
      <category>#chat</category>
      <category>#ai</category>
      <category>#rag</category>
      <category>#chatbot</category>
      <category>#gpu</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Stable Cascade</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/stability-ai-stable-cascade?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/stability-ai-stable-cascade&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable Cascade consists of three models: Stage A, Stage B and Stage C, representing a cascade for generating images, hence the name &amp;quot;Stable Cascade&amp;quot;. Stage A &amp;amp; B are used to compress images, similarly to what the job of the VAE is in Stable Diffusion. However, as mentioned before, with this setup a much higher compression of images can be achieved. Furthermore, Stage C is responsible for generating the small 24 x 24 latents given a text prompt. The following picture shows this visually. Note that Stage A is a VAE and both Stage B &amp;amp; C are diffusion models.&lt;br /&gt;
&lt;br&gt;
For this release, we are providing two checkpoints for Stage C, two for Stage B and one for Stage A. Stage C comes with a 1 billion and 3.6 billion parameter version, but we highly recommend using the 3.6 billion version, as most work was put into its finetuning. The two versions for Stage B amount to 700 million and 1.5 billion parameters. Both achieve great results, however the 1.5 billion excels at reconstructing small and fine details. Therefore, you will achieve the best results if you use the larger variant of each. Lastly, Stage A contains 20 million parameters and is fixed due to its small size.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/stability-ai-stable-cascade?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/stability-ai-stable-cascade</guid>
      <pubDate>2024-02-13 21:02</pubDate>
      <category>#stabilityai</category>
      <category>#ai</category>
      <category>#stablediffusion</category>
    </item>
    <item>
      <title>Memory and new controls for ChatGPT</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openai-chatgpt-memory?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openai-chatgpt-memory&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôre testing memory with ChatGPT. Remembering things you discuss across all chats saves you from having to repeat information and makes future conversations more helpful.&lt;br /&gt;
&lt;br&gt;
You're in control of ChatGPT's memory. You can explicitly tell it to remember something, ask it what it remembers, and tell it to forget conversationally or through settings. You can also turn it off entirely.&lt;br /&gt;
&lt;br&gt;
We are rolling out to a small portion of ChatGPT free and Plus users this week to learn how useful it is. We will share plans for broader roll out soon.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openai-chatgpt-memory?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openai-chatgpt-memory</guid>
      <pubDate>2024-02-13 21:00</pubDate>
      <category>#openai</category>
      <category>#chatgpt</category>
      <category>#memory</category>
      <category>#ai</category>
      <category>#llm</category>
      <category>#gpt</category>
    </item>
    <item>
      <title>SQL for Data Scientists in 100 Queries</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/sql-for-data-scientists-100-queries?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/sql-for-data-scientists-100-queries&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;notes and working examples that instructors can use to perform a lesson&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/sql-for-data-scientists-100-queries?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/sql-for-data-scientists-100-queries</guid>
      <pubDate>2024-02-06 20:07</pubDate>
      <category>#sql</category>
      <category>#datascience</category>
      <category>#tutorial</category>
    </item>
    <item>
      <title>Here‚Äôs how WhatsApp plans to interoperate with other messaging apps</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/how-whatsapp-plans-interoperate-other-messaging-services?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/how-whatsapp-plans-interoperate-other-messaging-services&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;WhatsApp, like many other major tech platforms, will have to make some significant changes to comply with the European Union‚Äôs Digital Markets Act (DMA). One of those changes is interoperability with other messaging platforms...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The shift toward interoperability will first include text messages, images, voice messages, videos, and files sent from one person to another. In theory, this would allow users to chat with people on WhatsApp through third-party apps, like iMessage, Telegram, Google Messages, and Signal, and vice versa.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;As noted by Wired, WhatsApp wants the messaging services it connects with to use the same Signal Protocol to encrypt messages. Meta is also open to apps using alternate encryption protocols so long as companies can prove ‚Äúthey reach the security standards that WhatsApp outlines in its guidance.‚Äù The third-party services will also have to sign a contract with Meta before they plug into WhatsApp, with more details about the agreement coming in March.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/how-whatsapp-plans-interoperate-other-messaging-services?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/how-whatsapp-plans-interoperate-other-messaging-services</guid>
      <pubDate>2024-02-06 20:03</pubDate>
      <category>#whatsapp</category>
      <category>#messaging</category>
      <category>#interoperability</category>
      <category>#signal</category>
    </item>
    <item>
      <title>Introducing Nomic Embed: A Truly Open Embedding Model</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nomic-embed-open-embedding-model?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nomic-embed-open-embedding-model&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We're excited to announce the release of Nomic Embed, the first&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open source&lt;/li&gt;
&lt;li&gt;Open data&lt;/li&gt;
&lt;li&gt;Open training code&lt;/li&gt;
&lt;li&gt;Fully reproducible and auditable&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nomic-embed-open-embedding-model?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nomic-embed-open-embedding-model</guid>
      <pubDate>2024-02-03 20:37</pubDate>
      <category>#embedding</category>
      <category>#ai</category>
      <category>#opensource</category>
      <category>#ml</category>
      <category>#model</category>
    </item>
    <item>
      <title>LangChain - OpenGPTs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/langchain-opengpts?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/langchain-opengpts&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A little over two months ago, on the heels of OpenAI dev day, we launched OpenGPTs: a take on what an open-source GPT store may look like. It was powered by an early version of LangGraph - an extension of LangChain aimed at building agents as graphs. At the time, we did not highlight this new package much, as we had not publicly launched it and were still figuring out the interface. We finally got around to launching LangGraph two weeks ago, and over the past weekend we updated OpenGPTs to fully use LangGraph (as well as added some new features). We figure now is as good of time as any to do a technical deep-dive on OpenGPTs and what powers it.&lt;br /&gt;
&lt;br&gt;
In this blog, we will talk about:&lt;br /&gt;
&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MessageGraph: A particular type of graph that OpenGPTs runs on&lt;/li&gt;
&lt;li&gt;Cognitive architectures: What the 3 different types of cognitive architectures OpenGPTs supports are, and how they differ&lt;/li&gt;
&lt;li&gt;Persistence: How persistence is baked in OpenGPTs via LangGraph checkpoints.&lt;/li&gt;
&lt;li&gt;Configuration: How we use LangChain primitives to configure all these different bots.&lt;/li&gt;
&lt;li&gt;New models: what new models we support&lt;/li&gt;
&lt;li&gt;New tools: what new tools we support&lt;/li&gt;
&lt;li&gt;astream_events: How we are using this new method to stream tokens and intermediate steps&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/langchain-opengpts?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/langchain-opengpts</guid>
      <pubDate>2024-01-31 21:30</pubDate>
      <category>#langchain</category>
      <category>#gpt</category>
      <category>#ai</category>
      <category>#opengpt</category>
      <category>#langgraph</category>
      <category>#messagegraph</category>
    </item>
    <item>
      <title>UNREDACTED Magazine Issue 006</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/unredacted-magazine-issue-006?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/unredacted-magazine-issue-006&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://inteltechniques.com/magazine.html"&gt;https://inteltechniques.com/magazine.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://inteltechniques.com/issues/006.pdf"&gt;https://inteltechniques.com/issues/006.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/unredacted-magazine-issue-006?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/unredacted-magazine-issue-006</guid>
      <pubDate>2024-01-31 21:24</pubDate>
      <category>#osint</category>
      <category>#magazine</category>
      <category>#unredacted</category>
      <category>#privacy</category>
      <category>#security</category>
    </item>
    <item>
      <title>Tiny Desk Premiere: Thee Sacred Souls</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/thee-sacred-souls-npr-tiny-desk-concerts?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/thee-sacred-souls-npr-tiny-desk-concerts&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;San Diego-based trio &lt;a href="https://www.npr.org/artists/1106773743/thee-sacred-souls"&gt;Thee Sacred Souls&lt;/a&gt; made its mark at the Tiny Desk with satin vocals and vintage melodies. Paying homage to southern California Latino culture meeting American soul roots, the group's sweet fusion melodies brought history and love into the space.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/thee-sacred-souls-npr-tiny-desk-concerts?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/thee-sacred-souls-npr-tiny-desk-concerts</guid>
      <pubDate>2024-01-31 19:50</pubDate>
      <category>#theesacredsouls</category>
      <category>#music</category>
      <category>#concert</category>
      <category>#npr</category>
      <category>#tinydeskconcert</category>
    </item>
    <item>
      <title>Eagle 7B : Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages (RWKV-v5)</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/eagle-7b-rkwv?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/eagle-7b-rkwv&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Eagle 7B is a 7.52B parameter model that:&lt;br /&gt;
&lt;br&gt;
Built on the &lt;a href="https://wiki.rwkv.com/"&gt;RWKV-v5 architecture&lt;/a&gt; (a linear transformer with 10-100x+ lower inference cost)&lt;br /&gt;
&lt;br&gt;
Ranks as the &lt;a href="https://blog.rwkv.com/p/the-worlds-greenest-ai-model-rwkvs"&gt;world‚Äôs greenest 7B model (per token)&lt;/a&gt;&lt;br /&gt;
&lt;br&gt;
Trained on 1.1 Trillion Tokens across 100+ languages&lt;br /&gt;
&lt;br&gt;
Outperforms all 7B class models in multi-lingual benchmarks&lt;br /&gt;
&lt;br&gt;
Approaches Falcon (1.5T), LLaMA2 (2T), Mistral (&amp;gt;2T?) level of performance in English evals&lt;br /&gt;
&lt;br&gt;
Trade blows with MPT-7B (1T) in English evals&lt;br /&gt;
&lt;br&gt;
&lt;a href="https://www.isattentionallyouneed.com/"&gt;All while being an ‚ÄúAttention-Free Transformer‚Äù&lt;/a&gt;&lt;br /&gt;
&lt;br&gt;
Is a foundation model, with a very small instruct tune - further fine-tuning is required for various use cases!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are releasing RWKV-v5 Eagle 7B, &lt;a href="https://blog.rwkv.com/p/rwkv-joins-the-linux-foundation-as"&gt;licensed as Apache 2.0 license, under the Linux Foundation&lt;/a&gt;, and can be used personally or commercially without restrictions&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://huggingface.co/RWKV/v5-Eagle-7B/"&gt;Download from HuggingFace&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/eagle-7b-rkwv?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/eagle-7b-rkwv</guid>
      <pubDate>2024-01-29 20:27</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#rwkv</category>
      <category>#deeplearning</category>
      <category>#neuralnetwork</category>
    </item>
    <item>
      <title>Getting Started With CUDA for Python Programmers</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/get-started-cuda-python-programmers?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/get-started-cuda-python-programmers&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=nOxKexn3iBo" title="A YouTube Video Tutorial by Jeremy Howard to help Python programmers get started with CUDA"&gt;&lt;img src="http://img.youtube.com/vi/nOxKexn3iBo/0.jpg" class="img-fluid" alt="A YouTube Video Tutorial by Jeremy Howard to help Python programmers get started with CUDA" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In this comprehensive video tutorial, Jeremy Howard from answer.ai demystifies the process of programming NVIDIA GPUs using CUDA, and simplifies the perceived complexities of CUDA programming. Jeremy emphasizes the accessibility of CUDA, especially when combined with PyTorch's capabilities, allowing for programming directly in notebooks rather than traditional compilers and terminals. To make CUDA more approachable to Python programmers, Jeremy shows step by step how to start with Python implementations, and then convert them largely automatically to CUDA. This approach, he argues, simplifies debugging and development.&lt;br /&gt;
&lt;br&gt;
The tutorial is structured in a hands-on manner, encouraging viewers to follow along in a Colab notebook. Jeremy uses practical examples, starting with converting an RGB image to grayscale using CUDA, demonstrating the process step-by-step. He further explains the memory layout in GPUs, emphasizing the differences from CPU memory structures, and introduces key CUDA concepts like streaming multi-processors and CUDA cores.&lt;br /&gt;
&lt;br&gt;
Jeremy then delves into more advanced topics, such as matrix multiplication, a critical operation in deep learning. He demonstrates how to implement matrix multiplication in Python first and then translates it to CUDA, highlighting the significant performance gains achievable with GPU programming. The tutorial also covers CUDA's intricacies, such as shared memory, thread blocks, and optimizing CUDA kernels.&lt;br /&gt;
&lt;br&gt;
The tutorial also includes a section on setting up the CUDA environment on various systems using Conda, making it accessible for a wide range of users.&lt;br /&gt;
&lt;br&gt;
This is lecture 3 of the &amp;quot;CUDA Mode&amp;quot; series (but you don't need to watch the others first). The notebook is available in the lecture3 folder here: &lt;a href="https://github.com/cuda-mode/lecture2"&gt;https://github.com/cuda-mode/lecture2&lt;/a&gt;...&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/get-started-cuda-python-programmers?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/get-started-cuda-python-programmers</guid>
      <pubDate>2024-01-29 20:23</pubDate>
      <category>#gpu</category>
      <category>#cuda</category>
      <category>#python</category>
      <category>#programming</category>
    </item>
    <item>
      <title>Ollama - Python &amp; JavaScript Libraries</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/python-js-ollama-libraries?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/python-js-ollama-libraries&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The initial versions of the Ollama Python and JavaScript libraries are now available:&lt;br /&gt;
&lt;br&gt;
&lt;a href="https://github.com/ollama/ollama-python"&gt;Ollama Python Library&lt;/a&gt;&lt;br /&gt;
&lt;a href="https://github.com/ollama/ollama-js"&gt;Ollama JavaScript Library&lt;/a&gt;&lt;br /&gt;
&lt;br&gt;
Both libraries make it possible to integrate new and existing apps with Ollama in a few lines of code, and share the features and feel of the Ollama REST API.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/python-js-ollama-libraries?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/python-js-ollama-libraries</guid>
      <pubDate>2024-01-25 21:26</pubDate>
      <category>#ollama</category>
      <category>#ai</category>
      <category>#llama</category>
      <category>#python</category>
      <category>#javascript</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Google‚Äôs Hugging Face deal puts ‚Äòsupercomputer‚Äô power behind open-source AI</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/google-cloud-huggingface-deal?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/google-cloud-huggingface-deal&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Google Cloud‚Äôs new partnership with AI model repository Hugging Face is letting developers build, train, and deploy AI models without needing to pay for a Google Cloud subscription. Now, outside developers using Hugging Face‚Äôs platform will have ‚Äúcost-effective‚Äù access to Google‚Äôs tensor processing units (TPU) and GPU supercomputers, which will include thousands of Nvidia‚Äôs in-demand and export-restricted H100s.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Google said that Hugging Face users can begin using the AI app-building platform Vertex AI and the Kubernetes engine that helps train and fine-tune models ‚Äúin the first half of 2024.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://www.googlecloudpresscorner.com/2024-01-25-Google-Cloud-and-Hugging-Face-Announce-Strategic-Partnership-to-Accelerate-Generative-AI-and-ML-Development"&gt;Press Release&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/google-cloud-huggingface-deal?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/google-cloud-huggingface-deal</guid>
      <pubDate>2024-01-25 21:23</pubDate>
      <category>#google</category>
      <category>#huggingface</category>
      <category>#ai</category>
      <category>#cloud</category>
      <category>#llm</category>
      <category>#ml</category>
      <category>#developers</category>
    </item>
    <item>
      <title>New OpenAI embedding models and API updates</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/new-openai-text-embedding-models-3?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/new-openai-text-embedding-models-3&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are launching a new generation of embedding models, new GPT-4 Turbo and moderation models, new API usage management tools, and soon, lower pricing on GPT-3.5 Turbo.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/new-openai-text-embedding-models-3?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/new-openai-text-embedding-models-3</guid>
      <pubDate>2024-01-25 21:20</pubDate>
      <category>#openai</category>
      <category>#llm</category>
      <category>#embedding</category>
      <category>#openai</category>
      <category>#gpt</category>
    </item>
    <item>
      <title>FOSDEM 2024 Schedule</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/fosdem-2024-schedule?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/fosdem-2024-schedule&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Lots of great sessions. I'm looking forward to the sessions on the following topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Matrix&lt;/li&gt;
&lt;li&gt;AI&lt;/li&gt;
&lt;li&gt;Nix / NixOS&lt;/li&gt;
&lt;li&gt;Software Defined Radio (SDR) &amp;amp; Amateur Radio&lt;/li&gt;
&lt;li&gt;Modern Email&lt;/li&gt;
&lt;li&gt;Collaboration &amp;amp; Content Management&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/fosdem-2024-schedule?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/fosdem-2024-schedule</guid>
      <pubDate>2024-01-23 22:19</pubDate>
      <category>#fosdem</category>
      <category>#opensource</category>
      <category>#foss</category>
      <category>#floss</category>
      <category>#ai</category>
      <category>#matrix</category>
      <category>#nixos</category>
    </item>
    <item>
      <title>OpenAI Microscope</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openai-microscope?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openai-microscope&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôre introducing OpenAI Microscope, a collection of visualizations of every significant layer and neuron of eight vision ‚Äúmodel organisms‚Äù which are often studied in interpretability. Microscope makes it easier to analyze the features that form inside these neural networks, and we hope it will help the research community as we move towards understanding these complicated systems.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openai-microscope?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openai-microscope</guid>
      <pubDate>2024-01-23 22:15</pubDate>
      <category>#openai</category>
      <category>#interpretability</category>
      <category>#ai</category>
      <category>#generativeai</category>
      <category>#visualization</category>
    </item>
    <item>
      <title>UNREDACTED Magazine Status</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/unredacted-magazine-status-jan-2024?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/unredacted-magazine-status-jan-2024&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the magazine is not 'dead'. Much like the podcast, it is simply on a hiatus. Many people falsely report online that the podcast and magazine are officially never coming back, which is contradictory to my &lt;a href="https://inteltechniques.com/blog/2023/11/20/my-irish-exit/"&gt;previous post&lt;/a&gt;. The reason there have been no issues of the magazine is simply a lack of submissions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The magazine is a community-driven product. Without the community driving it, it will go nowhere. If you would like to submit an article, please email it to staff@unredactedmagazine.com.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Sponsors are lined up to pay the costs and keep the content free, but there lies other problems. We received constant complaints about having sponsors. Most readers demanded free content without ads, which is unrealistic.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We have considered a small fee per issue, but the credit card fraud which comes with that is an even bigger issue. What is the solution? I do not know yet. If the articles pour in, I will figure it out.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/unredacted-magazine-status-jan-2024?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/unredacted-magazine-status-jan-2024</guid>
      <pubDate>2024-01-23 21:34</pubDate>
      <category>#magazine</category>
      <category>#unredacted</category>
      <category>#osint</category>
    </item>
    <item>
      <title>Microsoft is Dead (2007)</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/graham-microsoft-dead?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/graham-microsoft-dead&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;img src="https://media.giphy.com/media/b6iVj3IM54Abm/giphy.gif" class="img-fluid" alt="GIF of WWE Undertaker sitting up" /&gt;&lt;/p&gt;
&lt;p&gt;Interesting points.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/graham-microsoft-dead?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/graham-microsoft-dead</guid>
      <pubDate>2024-01-23 21:12</pubDate>
      <category>#google</category>
      <category>#microsoft</category>
      <category>#web</category>
    </item>
    <item>
      <title>NightShade</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nightshade-ai-model-poisoning-tool?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nightshade-ai-model-poisoning-tool&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Nightshade, a tool that turns any image into a data sample that is unsuitable for model training. More precisely, Nightshade transforms images into &amp;quot;poison&amp;quot; samples, so that models training on them without consent will see their models learn unpredictable behaviors that deviate from expected norms, e.g. a prompt that asks for an image of a cow flying in space might instead get an image of a handbag floating in space.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://nightshade.cs.uchicago.edu/whatis.html"&gt;What is NightShade?&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nightshade-ai-model-poisoning-tool?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nightshade-ai-model-poisoning-tool</guid>
      <pubDate>2024-01-23 20:57</pubDate>
      <category>#research</category>
      <category>#tools</category>
      <category>#ai</category>
      <category>#generativeai</category>
      <category>#llm</category>
      <category>#computervision</category>
      <category>#cv</category>
    </item>
    <item>
      <title>Introducing Shared Journals for Day One</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-shared-journals-day-one?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-shared-journals-day-one&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Shared Journals are a private space for your closest friends and family to shared life updates and memories. Shared Journals introduce a new dimension to journaling, offering a unique way to share your personal stories and experiences with up to 30 selected individuals, while keeping your individual entries private and secure.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Awesome! When I was writing the post &lt;a href="/feed/private-social-media"&gt;Private Social Media&lt;/a&gt; yesterday, I wasn't aware that these had already launched. I knew they were in beta but it's great to see they're now generally available. I'll have to give them a try.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-shared-journals-day-one?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-shared-journals-day-one</guid>
      <pubDate>2024-01-23 20:26</pubDate>
      <category>#dayone</category>
      <category>#automattic</category>
      <category>#journal</category>
    </item>
    <item>
      <title>Introducing Stable LM 2 1.6B</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-stable-lm-2-1-6B?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-stable-lm-2-1-6B&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable LM 2 1.6B is a state-of-the-art 1.6 billion parameter small language model trained on multilingual data in English, Spanish, German, Italian, French, Portuguese, and Dutch.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This model's compact size and speed lower hardware barriers, allowing more developers to participate in the generative AI ecosystem.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In addition to the pre-trained and instruction-tuned version, we release the last checkpoint before the pre-training cooldown. We include optimizer states to facilitate developers in fine-tuning and experimentation. Data details will be provided in the upcoming technical report.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable LM 2 1.6B can be used now both commercially and non-commercially with a Stability AI Membership &amp;amp; you can test the model on &lt;a href="https://huggingface.co/spaces/stabilityai/stablelm-2-1_6b-zephyr"&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-stable-lm-2-1-6B?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-stable-lm-2-1-6B</guid>
      <pubDate>2024-01-23 09:36</pubDate>
      <category>#ai</category>
      <category>#stabilityai</category>
      <category>#slm</category>
      <category>#llm</category>
      <category>#smalllanguagemodel</category>
    </item>
    <item>
      <title>The top five RSS readers for keeping up with your news feeds</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/top-five-rss-feed-readers-verge?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/top-five-rss-feed-readers-verge&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;RSS readers allow you to collect the articles of specific sources in one app, making it a lot easier to find the content you‚Äôre interested in without crawling through a lot of noise.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Whatever RSS feed reader you choose, it‚Äôs worth it to try at least one or two. This way, you can keep up with news from your favorite sources without depending on the chaos that is your email account or the random opinions from TikTok.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Great overview of the various RSS feed readers out there.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/top-five-rss-feed-readers-verge?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/top-five-rss-feed-readers-verge</guid>
      <pubDate>2024-01-22 20:49</pubDate>
      <category>#rss</category>
      <category>#news</category>
      <category>#feeds</category>
      <category>#indieweb</category>
      <category>#protocols</category>
    </item>
    <item>
      <title>The Vision Pro‚Äôs first killer app is the web, whether Apple likes it or not</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/vision-pros-most-important-app-safari?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/vision-pros-most-important-app-safari&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;it‚Äôs increasingly clear that the early success of the Vision Pro, and much of the answer to the question of what this headset is actually for, will come from a single app: Safari.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;That‚Äôs right, friends. Web browsers are back.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...at least at first, the open web is Apple‚Äôs best chance to make its headset a winner. Because at least so far, it seems developers are not exactly jumping to build new apps for Apple‚Äôs new platform.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Some of the high-profile companies that have announced they‚Äôre not yet building apps for the Vision Pro and its visionOS platform ‚Äî Netflix, Spotify, YouTube, and others ‚Äî are the very same ones that have loudly taken issue with how Apple runs the App Store.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;But what if you don‚Äôt need the App Store to reach Apple users anymore? All this corporate infighting has the potential to completely change the way we use our devices, starting with the Vision Pro.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...we‚Äôve all stopped opening websites and started tapping app icons, but the age of the URL might be coming back.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;If you believe the open web is a good thing, and that developers should spend more time on their web apps and less on their native ones, this is a big win for the future of the internet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The problem is, it‚Äôs happening after nearly two decades of mobile platforms systematically downgrading and ignoring their browsing experience...Mobile platforms treat browsers like webpage viewers, not app platforms, and it shows.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;There are some reasons for hope, though...the company appears to be still invested in making Safari work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Safari for visionOS will also come with some platform-specific features: you‚Äôll be able to open multiple windows at the same time and move them all around in virtual space.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;With a good browser and powerful PWAs, many users might mostly not notice the difference between opening the Spotify app and going to Spotify.com. That‚Äôs a win for the whole web.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;here‚Äôs the real question for Apple: which is more important, getting the Vision Pro off to a good start or protecting the sanctity of its App Store control at all costs? As Apple tries to create a platform shift to face computers, I‚Äôm not sure it can have it both ways.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Great article by &lt;a href="https://www.theverge.com/authors/david-pierce"&gt;David Pierce&lt;/a&gt;. As part of my website stats, I should probably start also counting authors I reference since many of the articles from the Verge I've previously linked to are written by David.&lt;/p&gt;
&lt;p&gt;As someone who &lt;a href="/feed/replacing-apps-websites-kingett"&gt;accesses services - &amp;quot;apps&amp;quot; - primarily through the web browser on desktop&lt;/a&gt;, this is exciting to see. While native apps have their advantages, the types of cross-platform connected experiences that can be delivered through the browser can't be ignored. First-class support for browsers in various platforms can only make these experiences even better. With more folks building their own platforms on the web on top of open standards that have been around for decades, I'm excited for the future of the web.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/vision-pros-most-important-app-safari?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/vision-pros-most-important-app-safari</guid>
      <pubDate>2024-01-20 13:29</pubDate>
      <category>#apple</category>
      <category>#safari</category>
      <category>#visionpro</category>
      <category>#web</category>
      <category>#browser</category>
      <category>#vr</category>
      <category>#xr</category>
      <category>#mr</category>
      <category>#ar</category>
      <category>#indieweb</category>
      <category>#openweb</category>
    </item>
    <item>
      <title>How Discord Serves 15-Million Users on One Server</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/how-discord-serves-15-million-users-using-one-server?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/how-discord-serves-15-million-users-using-one-server&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In early summer 2022, the Discord operations team noticed unusually high activity on their dashboards. They thought it was a bot attack, but it was legitimate traffic from MidJourney - a new, fast-growing community for generating AI images from text prompts.
&lt;br&gt;
To use MidJourney, you need a Discord account. Most MidJourney users join one main Discord server. This server grew so quickly that it soon hit Discord‚Äôs old limit of around 1 million users per server.
&lt;br&gt;
This is the story of how the Discord team creatively solved this challenge.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Discord‚Äôs real-time messaging backend is built with Elixir. Elixir runs on the BEAM virtual machine. BEAM was created for Erlang - a language optimized for large real-time systems requiring rock-solid reliability and uptime.
&lt;br&gt;
A key capability BEAM provides is extremely lightweight parallel processes. This enables a single server to efficiently run tens or hundreds of thousands of processes concurrently.
&lt;br&gt;
Elixir brings friendlier, Ruby-inspired syntax to the battle-tested foundation of BEAM. Combined they make it much easier to program massively scalable, fault-tolerant systems.
&lt;br&gt;
So by leveraging BEAM's lightweight processes, the Elixir code powering Discord can &amp;quot;fan out&amp;quot; messages to hundreds of thousands of users around the world concurrently. However, limits emerge as communities grow larger.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/how-discord-serves-15-million-users-using-one-server?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/how-discord-serves-15-million-users-using-one-server</guid>
      <pubDate>2024-01-17 20:40</pubDate>
      <category>#beam</category>
      <category>#erlang</category>
      <category>#elixir</category>
      <category>#discord</category>
      <category>#scale</category>
      <category>#enterprise</category>
      <category>#chat</category>
      <category>#communities</category>
    </item>
    <item>
      <title>Willow Protocol Specifications</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/willow-protocol?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/willow-protocol&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A protocol for peer-to-peer data stores. The best parts? Fine-grained permissions, a keen approach to privacy, destructive edits, and a dainty bandwidth and memory footprint.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/willow-protocol?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/willow-protocol</guid>
      <pubDate>2024-01-17 20:38</pubDate>
      <category>#willow</category>
      <category>#protocol</category>
      <category>#p2p</category>
      <category>#privacy</category>
      <category>#data</category>
    </item>
    <item>
      <title>Stable Code 3B - Coding on the Edge</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/stable-code-3b-stability?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/stable-code-3b-stability&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable Code 3B is a 3 billion parameter Large Language Model (LLM), allowing accurate and responsive code completion at a level on par with models such as CodeLLaMA 7b that are 2.5x larger.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Operates offline even without a GPU on common laptops such as a MacBook Air.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/stable-code-3b-stability?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/stable-code-3b-stability</guid>
      <pubDate>2024-01-17 20:36</pubDate>
      <category>#ai</category>
      <category>#stabilityai"</category>
      <category>#code</category>
      <category>#software</category>
      <category>#softwaredevelopment</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Sampling for Text Generation</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/sampling-text-generation-huyen?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/sampling-text-generation-huyen&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;ML models are probabilistic. Imagine that you want to know what‚Äôs the best cuisine in the world. If you ask someone this question twice, a minute apart, their answers both times should be the same. If you ask a model the same question twice, its answer can change.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This probabilistic nature makes AI great for creative tasks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;However, this probabilistic nature also causes inconsistency and hallucinations. It‚Äôs fatal for tasks that depend on factuality. Recently, I went over 3 months‚Äô worth of customer support requests of an AI startup I advise and found that ‚Öï of the questions are because users don‚Äôt understand or don‚Äôt know how to work with this probabilistic nature.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;To understand why AI‚Äôs responses are probabilistic, we need to understand how models generate responses, a process known as sampling (or decoding). This post consists of 3 parts.
&lt;br&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Sampling: sampling strategies and sampling variables including temperature, top-k, and top-p.
2. Test time sampling: sampling multiple outputs to help improve a model‚Äôs performance.
3. Structured outputs: how to get models to generate outputs in a certain format.
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/sampling-text-generation-huyen?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/sampling-text-generation-huyen</guid>
      <pubDate>2024-01-17 20:32</pubDate>
      <category>#ai</category>
      <category>#generativeai</category>
      <category>#statistics</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Podcasting is in its YouTube era</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/podcast-youtube-era?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/podcast-youtube-era&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, YouTube at very long last debuts RSS integration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This means more hosts saying &amp;quot;...or wherever you get your podcasts.&amp;quot; üôÇ&lt;/p&gt;
&lt;p&gt;&lt;a href="https://support.google.com/youtube/answer/13973017"&gt;Support Post&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/podcast-youtube-era?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/podcast-youtube-era</guid>
      <pubDate>2024-01-17 20:24</pubDate>
      <category>#rss</category>
      <category>#podcasts</category>
      <category>#youtube</category>
    </item>
    <item>
      <title>Each Facebook User is Monitored by Thousands of Companies</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/facebook-users-monitored-thousands-companies-markup?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/facebook-users-monitored-thousands-companies-markup&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;By now most internet users know their online activity is constantly tracked.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;But what is the scale of this surveillance? Judging from data collected by Facebook and &lt;a href="https://innovation.consumerreports.org/wp-content/uploads/2024/01/CR_Who-Shares-Your-Information-With-Facebook.pdf"&gt;newly described in a unique study&lt;/a&gt; by non-profit consumer watchdog Consumer Reports, it‚Äôs massive, and examining the data may leave you with more questions than answers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Using a panel of 709 volunteers who shared archives of their Facebook data, Consumer Reports found that a total of 186,892 companies sent data about them to the social network. On average, each participant in the study had their data sent to Facebook by 2,230 companies. That number varied significantly, with some panelists‚Äô data listing over 7,000 companies providing their data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;h2&gt;What Exactly Does This Data Contain?&lt;/h2&gt;
&lt;br&gt;
The data examined by Consumer Reports in this study comes from two types of collection: events and custom audiences. Both categories include information about what people do outside of Meta‚Äôs platforms.
&lt;br&gt;
Custom audiences allow advertisers to upload customer lists to Meta, often including identifiers like email addresses and mobile advertising IDs...
&lt;br&gt;
The other category of data collection, ‚Äúevents,‚Äù describes interactions that the user had with a brand, which can occur outside of Meta‚Äôs apps and in the real world. Events can include visiting a page on a company‚Äôs website, leveling up in a game, visiting a physical store, or purchasing a product...
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;h2&gt;How Can I See My Data?&lt;/h2&gt;
&lt;br&gt;
Facebook users can browse through the list of companies that have sent their data to Facebook by going to: [https://accountscenter.facebook.com/info_and_permissions](https://accountscenter.facebook.com/info_and_permissions)
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/facebook-users-monitored-thousands-companies-markup?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/facebook-users-monitored-thousands-companies-markup</guid>
      <pubDate>2024-01-17 20:09</pubDate>
      <category>#socialmedia</category>
      <category>#privacy</category>
      <category>#facebook</category>
      <category>#meta</category>
      <category>#surveillance</category>
    </item>
    <item>
      <title>Talking about Open Source LLMs on Oxide and Friends</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/talking-open-source-llms-oxide-friends-willison?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/talking-open-source-llms-oxide-friends-willison&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I recorded &lt;a href="https://oxide.computer/podcasts/oxide-and-friends/1692510"&gt;an episode&lt;/a&gt; of the Oxide and Friends podcast on Monday, talking with Bryan Cantrill and Adam Leventhal about Open Source LLMs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;h2&gt;Too important for a small group to control...&lt;/h2&gt;
&lt;br&gt;
This technology is clearly extremely important to the future of all sorts of things that we want to do.
&lt;br&gt;
I am totally on board with it. There are people who will tell you that it‚Äôs all hype and bluster. I‚Äôm over that. This stuff‚Äôs real. It‚Äôs really useful.
&lt;br&gt;
It is far too important for a small group of companies to completely control this technology. That would be genuinely disastrous. And I was very nervous that was going to happen, back when it was just OpenAI and Anthropic that had the only models that were any good, that was really nerve-wracking.
&lt;br&gt;
Today I‚Äôm not afraid of that at all, because there are dozens of organizations now that have managed to create one of these things...
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;h2&gt;On LLMs for learning...&lt;/h2&gt;
&lt;p&gt;One of the most exciting things for me about this technology is that it‚Äôs a teaching assistant that is always available to you.
&lt;br&gt;
You know that thing where you‚Äôre learning‚Äîespecially in a classroom environment‚Äîand you miss one little detail and you start falling further and further behind everyone else because there was this one little thing you didn‚Äôt quite catch, and you don‚Äôt want to ask stupid questions?
&lt;br&gt;
You can ask stupid questions of ChatGPT anytime you like and it can help guide you through to the right answer.
&lt;br&gt;
That‚Äôs kind of a revelation.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/talking-open-source-llms-oxide-friends-willison?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/talking-open-source-llms-oxide-friends-willison</guid>
      <pubDate>2024-01-17 20:03</pubDate>
      <category>#ai</category>
      <category>#podcast</category>
      <category>#opensource</category>
      <category>#llm</category>
    </item>
    <item>
      <title>SingSong - Generating musical accompaniments from singing</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/singsong-ai-music-accompaniment?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/singsong-ai-music-accompaniment&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We present SingSong, a system that generates instrumental music to accompany input vocals, potentially offering musicians and non-musicians alike an intuitive new way to create music featuring their own voice. To accomplish this, we build on recent developments in musical source separation and audio generation. Specifically, we apply a state-of-the-art source separation algorithm to a large corpus of music audio to produce aligned pairs of vocals and instrumental sources. Then, we adapt AudioLM (Borsos et al., 2022) -- a state-of-the-art approach for unconditional audio generation -- to be suitable for conditional &amp;quot;audio-to-audio&amp;quot; generation tasks, and train it on the source-separated (vocal, instrumental) pairs. In a pairwise comparison with the same vocal inputs, listeners expressed a significant preference for instrumentals generated by SingSong compared to those from a strong retrieval baseline.
Sound examples at &lt;a href="https://g.co/magenta/singsong"&gt;this https URL&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;AI can now help you create a backing track to all the songs you make up about your pets.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/singsong-ai-music-accompaniment?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/singsong-ai-music-accompaniment</guid>
      <pubDate>2024-01-17 19:39</pubDate>
      <category>#ai</category>
      <category>#generativeai</category>
      <category>#music</category>
    </item>
    <item>
      <title>Notion‚Äôs new calendar app is designed to keep your meetings organized</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/notion-cron-calendar-app?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/notion-cron-calendar-app&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;After acquiring Cron in 2022, Notion is bringing the calendar app fully into its all-in-one workspace.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The big new feature coming with the rebranding is Notion integration. If you or your company uses Notion, you‚Äôll be able to create or link Notion documents inside a calendar invite. If you have a database filled with due dates, you can add that as a calendar to Notion Calendar. It sounds like a much better way to handle agendas and notes than sending them around before and after a meeting or hunting for them in your Slack. Putting everything in the calendar event is a good move.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is one of the reasons I like org-mode in Emacs. Being able to annotate documents with timestamps and deadlines that show up and you can organize inside the Agenda view is so powerful. The integrations and learning curve is steeper compared to a tool like Notion but I find it simple and powerful enough for GTD-style workflows, I'd have a hard time moving. I have yet to use AnyType, so maybe &lt;a href="/feed/anytype-local-only"&gt;after trying that&lt;/a&gt;, I choose to shift some of my workflows there.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/notion-cron-calendar-app?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/notion-cron-calendar-app</guid>
      <pubDate>2024-01-17 19:33</pubDate>
      <category>#calendar</category>
      <category>#notion</category>
      <category>#productivity</category>
      <category>#gtd</category>
      <category>#software</category>
      <category>#app</category>
      <category>#pkm</category>
    </item>
    <item>
      <title>OpenMentions</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openmentions?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openmentions&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;OpenMentions is a project designed to use Webmentions and ActivityPub for topical content discovery. The site is organised along the lines of a hierarchy of topics going from broad to fine. This we call OpenTopic ‚Äì the idea being that many sites could host the full collection of topics so that the loss of any one site is not the loss of all topics.&lt;br /&gt;
&lt;br&gt;
The intention is that this site should own nothing and that topic hierarchies are organic and discoverable.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openmentions?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openmentions</guid>
      <pubDate>2024-01-16 20:21</pubDate>
      <category>#webmentions</category>
      <category>#indieweb</category>
      <category>#activitypub</category>
      <category>#online</category>
    </item>
    <item>
      <title>LeftoverLocals: Listening to LLM responses through leaked GPU local memory</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/lefoverlocals-llm-responses-leaked-gpu-memory?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/lefoverlocals-llm-responses-leaked-gpu-memory&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are disclosing LeftoverLocals: a vulnerability that allows recovery of data from GPU local memory created by another process on Apple, Qualcomm, AMD, and Imagination GPUs. LeftoverLocals impacts the security posture of GPU applications as a whole, with particular significance to LLMs and ML models run on impacted GPU platforms. By recovering local memory‚Äîan optimized GPU memory region‚Äîwe were able to build a PoC where an attacker can listen into another user‚Äôs interactive LLM session (e.g., llama.cpp) across process or container boundaries&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/lefoverlocals-llm-responses-leaked-gpu-memory?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/lefoverlocals-llm-responses-leaked-gpu-memory</guid>
      <pubDate>2024-01-16 20:16</pubDate>
      <category>#security</category>
      <category>#llm</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Easy Ways to Improve Your Privacy &amp; Security in 2024</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/easy-ways-improve-privacy-security-2024?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/easy-ways-improve-privacy-security-2024&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Every year, I like to remind everyone to go back to the basics. For those who are new to privacy and security and may be trying to create some new, positive habits, this serves as a great entry point. For veteran privacy enthusiasts, the basics form our foundation for more advanced techniques later, making it imperative to ensure we cover all those bases. So in that spirit, let‚Äôs all pause ‚Äì wherever we are in our privacy journeys ‚Äì to do a quick check and make sure we‚Äôve got the basics covered. If you‚Äôre one of those new people I mentioned, welcome! But also know that this post is packed with information, so try not to get overwhelmed. Maybe bookmark this post and do one thing per day or something like that.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Strong Passwords...&lt;br /&gt;
&lt;br&gt;
Multi-Factor Authentication (MFA)...&lt;br /&gt;
&lt;br&gt;
Regular Software Updates...&lt;br /&gt;
&lt;br&gt;
Secure Your Wi-Fi Network...&lt;br /&gt;
&lt;br&gt;
Be Cautious with Communications...&lt;br /&gt;
&lt;br&gt;&lt;br /&gt;
Review App Permissions...&lt;br /&gt;
&lt;br&gt;
Review Your Account Settings...&lt;br /&gt;
&lt;br&gt;
Secure Browsing Habits...&lt;br /&gt;
&lt;br&gt;
Device Security...&lt;br /&gt;
&lt;br&gt;
Review Financial Statements...&lt;br /&gt;
&lt;br&gt;
Educate Yourself...&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/easy-ways-improve-privacy-security-2024?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/easy-ways-improve-privacy-security-2024</guid>
      <pubDate>2024-01-15 16:54</pubDate>
      <category>#privacy</category>
      <category>#security</category>
    </item>
    <item>
      <title>AI for economists - prompts and resources</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ai-for-economists-prompts?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ai-for-economists-prompts&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This page contains example prompts and responses intended to showcase how generative AI, namely LLMs like GPT-4, can benefit economists.
&lt;br&gt;
Example prompts are shown from six domains: ideation and feedback; writing; background research; coding; data analysis; and mathematical derivations.
&lt;br&gt;
The framework as well as some of the prompts and related notes come from Korinek, A. 2023. &lt;a href="https://www.aeaweb.org/articles?id=10.1257/jel.20231736"&gt;‚ÄúGenerative AI for Economic Research: Use Cases and Implications for Economists‚Äú&lt;/a&gt;, Journal of Economic Literature, 61 (4): 1281‚Äì1317.
&lt;br&gt;
Each application area includes 1-3 prompts and responses from an LLM, often from the field of development economics, along with brief notes. The prompts will be updated periodically.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ai-for-economists-prompts?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ai-for-economists-prompts</guid>
      <pubDate>2024-01-15 16:49</pubDate>
      <category>#ai</category>
      <category>#economy</category>
      <category>#promptengineering</category>
      <category>#llm</category>
      <category>#gpt</category>
    </item>
    <item>
      <title>Hands-on - Clicks for iPhone</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/hands-on-clicks-iphone?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/hands-on-clicks-iphone&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Smartphones and physical keyboards aren‚Äôt a combination we think of often, but Clicks for iPhone is trying to bring that back with a new keyboard case that‚Äôs extremely good.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Wishful thinking on my end but I'd buy a Blackberry-like device. Not a &amp;quot;smartphone&amp;quot;, but an internet-connected device with all the phone capabilities that has a physical keyboard.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/hands-on-clicks-iphone?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/hands-on-clicks-iphone</guid>
      <pubDate>2024-01-15 16:45</pubDate>
      <category>#smarphone</category>
      <category>#keyboard</category>
      <category>#iphone</category>
      <category>#featurephone</category>
    </item>
    <item>
      <title>Goblin - Tumblr meets the Fediverse</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/goblin-tumblr-fediverse?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/goblin-tumblr-fediverse&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Goblin band is an attempt to replicate the colletive creative energy that happens on tumblr and take it to the fediverse&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Repo&lt;/strong&gt;: &lt;a href="https://github.com/johnHackworth/goblin"&gt;https://github.com/johnHackworth/goblin&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/goblin-tumblr-fediverse?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/goblin-tumblr-fediverse</guid>
      <pubDate>2024-01-15 16:14</pubDate>
      <category>#tumblr</category>
      <category>#fediverse</category>
      <category>#activitypub</category>
      <category>#automattic</category>
      <category>#firefish</category>
      <category>#socialmedia</category>
      <category>#opensource</category>
      <category>#community</category>
      <category>#decentralization</category>
      <category>#federation</category>
    </item>
    <item>
      <title>Clarkesworld - Sci-fi and Fantasy Magazine</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/clarkesworld-sci-fi-fantasy-magazine?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/clarkesworld-sci-fi-fantasy-magazine&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Clarkesworld is a monthly science fiction and fantasy magazine first published in October 2006. Each issue contains interviews, thought-provoking articles, and between six and eight works of original fiction.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/clarkesworld-sci-fi-fantasy-magazine?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/clarkesworld-sci-fi-fantasy-magazine</guid>
      <pubDate>2024-01-15 15:00</pubDate>
      <category>#magazine</category>
      <category>#scifi</category>
      <category>#fantasy</category>
      <category>#online</category>
      <category>#fiction</category>
      <category>#nonfiction</category>
      <category>#art</category>
    </item>
    <item>
      <title>Fly - New Chicano Batman out 1/23</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/chicano-batman-fly-01-2024?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/chicano-batman-fly-01-2024&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Starting off the year right with new &lt;a href="https://chicanobatman.com/"&gt;Chicano Batman&lt;/a&gt;!&lt;/p&gt;
&lt;figure class="figure"&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;‚ÄúFly‚Äù is yours 1/23 üïä And this is just the beginning!&lt;/p&gt;
&lt;p&gt;&lt;img src="https://pbs.twimg.com/media/GDqtSgWa8AAf7by?format=jpg&amp;amp;name=small" class="img-fluid" alt="A closeup image of a hand wearing a ring that says FLY" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;&lt;a href="https://twitter.com/ChicanoBatman"&gt;Chicano Batman on X&lt;/a&gt;&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/chicano-batman-fly-01-2024?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/chicano-batman-fly-01-2024</guid>
      <pubDate>2024-01-15 12:36</pubDate>
      <category>#music</category>
      <category>#chicanobatman</category>
    </item>
    <item>
      <title>FLOSS Weekly is now on Hackaday!</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/floss-weekly-now-on-hackaday?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/floss-weekly-now-on-hackaday&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôre excited to announce that Hackaday is the new home of FLOSS Weekly, a long-running podcast about free, libre, and open-source software! The TWiT network hosted the podcast for an incredible seventeen years, but due to some changes on their end, they recently had to wind things down. They were gracious enough to let us pick up the torch, with Jonathan Bennett now taking over hosting duties.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That didn't take long. Last month &lt;a href="/feed/floss-weekly-ends-twit-start-of-an-era"&gt;I learned FLOSS Weekly was ending on the TWiT network&lt;/a&gt;. It's great to see it has found a new home in Hackaday! Time to update the RSS feeds and &lt;a href="/feed/podroll"&gt;podroll&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/floss-weekly-now-on-hackaday?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/floss-weekly-now-on-hackaday</guid>
      <pubDate>2024-01-15 12:15</pubDate>
      <category>#podcast</category>
      <category>#floss</category>
      <category>#hackaday</category>
      <category>#opensource</category>
      <category>#technology</category>
      <category>#internet</category>
    </item>
    <item>
      <title>Every - Daily Newsletter</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/every-newsletter?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/every-newsletter&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Every is a daily newsletter founded in 2020. Every day, we publish a long-form essay to make you smarter about technology, productivity, and AI.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/every-newsletter?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/every-newsletter</guid>
      <pubDate>2024-01-15 11:08</pubDate>
      <category>#newsletter</category>
      <category>#technology</category>
      <category>#productivity</category>
      <category>#ai</category>
      <category>#essay</category>
      <category>#internet</category>
    </item>
    <item>
      <title>Migrating from Substack to self-hosted Ghost</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/migrating-substack-self-hosted-ghost-white?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/migrating-substack-self-hosted-ghost-white&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I have found myself with a roughly $103/mo setup...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;However, more important to me than the exact price is the degree of control I have over my own not-a-platform...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I realize that this is...a lot. If you are a newsletter writer looking to flee the Substack ship, please don't let this discourage you.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Love seeing this! Whether through self-hosted or hosted options, I hope more people get to experience the benefits of owning their own platform.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/migrating-substack-self-hosted-ghost-white?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/migrating-substack-self-hosted-ghost-white</guid>
      <pubDate>2024-01-14 16:11</pubDate>
      <category>#substack</category>
      <category>#selfhost</category>
      <category>#indieweb</category>
      <category>#ghost</category>
    </item>
    <item>
      <title>Cool URIs don't change</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/cool-uris-dont-change?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/cool-uris-dont-change&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Keeping URIs so that they will still be around in 2, 20 or 200 or even 2000 years is clearly not as simple as it sounds. However, all over the Web, webmasters are making decisions which will make it really difficult for themselves in the future. Often, this is because they are using tools whose task is seen as to present the best site in the moment, and no one has evaluated what will happen to the links when things change. The message here is, however, that many, many things can change and your URIs can and should stay the same. They only can if you think about how you design them.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/cool-uris-dont-change?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/cool-uris-dont-change</guid>
      <pubDate>2024-01-13 10:23</pubDate>
      <category>#url</category>
      <category>#http</category>
      <category>#standard</category>
      <category>#protocol</category>
      <category>#web</category>
      <category>#internet</category>
    </item>
    <item>
      <title>Where is all of the Fediverse?</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/where-is-all-fediverse?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/where-is-all-fediverse&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Here we can see that Fastly and Cloudflare make up over 50% of the entire fediverse network.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...for the population using Cloudflare, a fair number of them (30%) appear to be hosting the instances behind a home broadband connection.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the German hosting provider Hetzner hosts over 51% of the entire network!&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/where-is-all-fediverse?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/where-is-all-fediverse</guid>
      <pubDate>2024-01-13 10:15</pubDate>
      <category>#fediverse</category>
      <category>#analytics</category>
      <category>#web</category>
    </item>
    <item>
      <title>More than an OpenAI Wrapper: Perplexity Pivots to Open Source</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/perplexity-ai-pivots-open-source?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/perplexity-ai-pivots-open-source&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...Perplexity has become a surprisingly strong player in a market otherwise dominated by OpenAI, Microsoft, Google and Meta.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;At its core, Perplexity is a search engine.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...over the past year, Perplexity has evolved rapidly. It now has its own search index and has built its own LLMs based on open source models. They‚Äôve also begun to combine their proprietary technology products. At the end of November, Perplexity announced two new ‚Äúonline LLMs‚Äù ‚Äî LLMs combined with a search index ‚Äî called pplx-7b-online and pplx-70b-online. They were built on top of the open source models mistral-7b and llama2-70b.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Using open source models has been critical for the growth of Perplexity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the default Perplexity model still relies on GPT 3.5 (and a dash of LLaMA-2). But the intention is to move away from that long-standing reliance on OpenAI for its base model.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/perplexity-ai-pivots-open-source?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/perplexity-ai-pivots-open-source</guid>
      <pubDate>2024-01-13 09:43</pubDate>
      <category>#ai</category>
      <category>#perplexity</category>
      <category>#opensource</category>
      <category>#search</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Marimo</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/marimo-notebook?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/marimo-notebook&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;marimo is an open-source reactive notebook for Python ‚Äî reproducible, git-friendly, executable as a script, and shareable as an app.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/marimo-notebook?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/marimo-notebook</guid>
      <pubDate>2024-01-13 09:38</pubDate>
      <category>#notebook</category>
      <category>#programming</category>
      <category>#literateprogramming</category>
    </item>
    <item>
      <title>IndieWeb assimilation</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/indieweb-assimilation-shellshark?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/indieweb-assimilation-shellshark&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Principle Mechanics&lt;br /&gt;
&lt;br&gt;&lt;br /&gt;
This section does not provide exhaustive coverage of how to implement IndieWeb functionality. Instead, I simply summarize five core primitives which I feel comprise an IndieWeb site. For a more official gauge on where a site scores within the IndieWeb spectrum, consider leveraging IndieMark!&lt;br /&gt;
&lt;br&gt;
Hosting: You need a place to host your site and store your content. There are a lot of great options out there. Ideally, choose one that allows you the ability to make some under-the-hood changes and does not limit your content portability.&lt;br /&gt;
&lt;br&gt;
Syndication: Share your content with the world! There are two preferred methods for syndication, PESOS and POSSE. This resource does a great job explaining both! For more examples of how this is done, check this and this out. RSS is a great starting point for helping others subscribe to new content on your site.&lt;br /&gt;
&lt;br&gt;
Writing: Though your site could simply serve as a more static point/identity on the web, with little to no ‚Äúcontent‚Äù being regularly added, I recommend writing!&lt;br /&gt;
&lt;br&gt;
Interactivity: One of the more advanced concepts within the IndieWeb world, the ability to bake in native comments, replies, likes, etc is a greay way to build community. This interactivity helps mitigate reliance on centralized social networks for communication within Indie communities. One example of IndieWeb interactivity is Webmentions.&lt;br /&gt;
&lt;br&gt;
Identity: Make it unique, make it fun, make it yours. The corporate web is sterile and suffocating. Let‚Äôs bring back the whimsy of the old web.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/indieweb-assimilation-shellshark?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/indieweb-assimilation-shellshark</guid>
      <pubDate>2024-01-13 09:05</pubDate>
      <category>#indieweb</category>
      <category>#guide</category>
      <category>#catalog</category>
    </item>
    <item>
      <title>My AI Timelines Have Sped Up (Again)</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ai-pipelines-sped-up-again-alexirpan?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ai-pipelines-sped-up-again-alexirpan&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...computers are useful, ML models are useful, and even if models fail to scale, people will want to fit GPT-4 sized models on their phone. It seems reasonable to assume the competing factions will figure something out.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Data seems like the harder question. (Or at least the one I feel qualified talking about.) We have already crossed the event horizon of trying to train on everything on the Internet. It‚Äôs increasingly difficult for labs to differentiate themselves on publicly available data. Differentiation is instead coming from non-public high-quality data to augment public low-quality data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;All the scaling laws have followed power laws so far, including dataset size. Getting more data by hand doesn‚Äôt seem good enough to cross to the next thresholds. We need better means to get good data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A long time ago, when OpenAI still did RL in games / simulation, they were very into self-play. You run agents against copies of themselves, score their interactions, and update the models towards interactions with higher reward. Given enough time, they learn complex strategies through competition.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I think it‚Äôs possible we‚Äôre at the start of a world where self-play or self-play-like ideas work to improve LLM capabilities. Drawing an analogy, the environment is the dialogue, actions are text generated from an LLM, and the reward is from whatever reward model you have. Instead of using ground truth data, our models may be at a point where they can generate data that‚Äôs good enough to train on.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ai-pipelines-sped-up-again-alexirpan?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ai-pipelines-sped-up-again-alexirpan</guid>
      <pubDate>2024-01-11 10:07</pubDate>
      <category>#ai</category>
      <category>#predictions</category>
      <category>#agi</category>
      <category>#data</category>
      <category>#llm</category>
      <category>#technology</category>
      <category>#ml</category>
      <category>#computervision</category>
    </item>
    <item>
      <title>Introducing the GPT Store</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-gpt-store?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-gpt-store&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It‚Äôs been two months since we announced GPTs, and users have already created over 3 million custom versions of ChatGPT. Many builders have shared their GPTs for others to use. Today, we're starting to roll out the GPT Store to ChatGPT Plus, Team and Enterprise users so you can find useful and popular GPTs.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-gpt-store?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-gpt-store</guid>
      <pubDate>2024-01-11 10:01</pubDate>
      <category>#openai</category>
      <category>#gpt</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Python 3.13 gets a JIT</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/python-3-13-gets-jit?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/python-3-13-gets-jit&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In late December 2023 (Christmas Day to be precise), CPython core developer &lt;a href="https://github.com/python/cpython/pull/113465"&gt;Brandt Bucher submitted a little pull-request&lt;/a&gt; to the Python 3.13 branch adding a JIT compiler.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/python-3-13-gets-jit?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/python-3-13-gets-jit</guid>
      <pubDate>2024-01-09 09:53</pubDate>
      <category>#python</category>
      <category>#programming</category>
      <category>#compiler</category>
    </item>
    <item>
      <title>Replacing apps with websites</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/replacing-apps-websites-kingett?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/replacing-apps-websites-kingett&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I woke up today...and decided I was going to delete all the apps off my iPhone.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;If I didn‚Äôt need it to do a specialized function? Gone, poof. I decided to just use websites and bookmarks instead.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The websites worked just fine.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...websites are just badly designed these days, especially with using a screen reader on a mobile device. I can‚Äôt quite describe it, because mobile web browsers aren‚Äôt really well designed either, so it makes the website worse because it doesn‚Äôt even render all elements as smoothly as on desktop.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Even though there are problems, I‚Äôm honestly glad I deleted almost all my apps off my phone and started to pin websites to my home screen more. For one thing, it cuts down on notifications. it‚Äôs super freeing to not get a random notification because you didn‚Äôt open the app in a day, so the app pings you to say hey I‚Äôm still here please pay attention to me, I feel lonely, and nobody will give me animals to snuggle with.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;With pinned websites, my phone is faster and my home screen is much more organized as well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I did this a few years ago. Websites work just fine for almost everything I need to do. The main benefits I noticed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Less distractions&lt;/li&gt;
&lt;li&gt;More real estate when viewing websites on laptop / desktop&lt;/li&gt;
&lt;li&gt;Less apps to drain the battery when running on the background&lt;/li&gt;
&lt;li&gt;More restricted access / permissions (i.e. websites don't need unrestricted access to my contacts or other information on my phone)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Better PWA support on mobile platforms would go a long way here to better balance between apps and websites. In the meantime though, pinning websites and websites overall work just fine for most things I need to do day-to-day.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/replacing-apps-websites-kingett?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/replacing-apps-websites-kingett</guid>
      <pubDate>2024-01-09 09:37</pubDate>
      <category>#apps</category>
      <category>#websites</category>
      <category>#internet</category>
      <category>#mobile</category>
    </item>
    <item>
      <title>TIOBE Index for January 2024</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tiobe-index-january-2024?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tiobe-index-january-2024&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;For the first time in the history of the TIOBE index, C# has won the programming language of the year award. Congratulations! C# has been a top 10 player for more than 2 decades and now that it is catching up with the big 4 languages, it won the well-deserved award by being the language with the biggest uptick in one year (+1.43%).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Exciting to see F# almost break into the Top 20 at number 22 with 0.77%.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tiobe-index-january-2024?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tiobe-index-january-2024</guid>
      <pubDate>2024-01-07 22:52</pubDate>
      <category>#programming</category>
      <category>#dotnet</category>
      <category>#technology</category>
      <category>#fsharp</category>
      <category>#csharp</category>
      <category>#programminglanguages</category>
    </item>
    <item>
      <title>A blog post can be any length</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/blog-posts-can-be-any-length-matt?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/blog-posts-can-be-any-length-matt&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://github.com/lqdev/luisquintanilla.me/assets/11130940/d42cc57e-b882-4042-b2be-e68810105c3d" class="img-fluid" alt="A comment thread between Luis and Matt" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;&lt;a href="https://ma.tt/"&gt;Matt Mullenweg&lt;/a&gt;&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;img src="https://media.giphy.com/media/26FLgGTPUDH6UGAbm/giphy.gif" class="img-fluid" alt="A GIF of a man dressed in black pointing up to blinking overhead text &amp;quot;THIS&amp;quot;" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/blog-posts-can-be-any-length-matt?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/blog-posts-can-be-any-length-matt</guid>
      <pubDate>2024-01-07 22:37</pubDate>
      <category>#writing</category>
      <category>#blog</category>
      <category>#web</category>
      <category>#internet</category>
    </item>
    <item>
      <title>Own Your RSS Links</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/own-your-rss-links?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/own-your-rss-links&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...owning the address where your audience finds you is important. It allows you to be mobile, nimble, and without attached strings. It helps you show off all the things and places you want folks to see because you can put all these URLs on your /feeds page. It‚Äôs user-friendly in more ways than one (pretty cool how you can make all those URLs human-readable, huh?).&lt;/p&gt;
&lt;p&gt;...it means your audience never has to think about how they‚Äôre going to get your stuff.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a great idea. I've written before about &lt;a href="/feed/own-your-links-domain-verification"&gt;owning your links&lt;/a&gt;. Today, that's how I expose many of my links in the &lt;a href="/contact"&gt;contact page&lt;/a&gt;. For example, to access my Mastodon profile, instead of going to the actual URL, you can just visit &lt;a href="https://lqdev.me/mastodon"&gt;lqdev.me/mastodon&lt;/a&gt; which redirects to the actual URL. If tomorrow I choose to change where and how my Mastodon presence is hosted, the URL doesn't have to change. However, I haven't done the same for my RSS links. Recently I've been thinking about restructuring my website, specifically my microblog feed which includes notes and responses. Today, the RSS urls are coupled to the folder structure on my website which is subject to change and isn't flexible. By setting up more user-friendly and stable RSS urls through redirection, that wouldn't be an issue and readers wouldn't have to change the RSS URL they use.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/own-your-rss-links?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/own-your-rss-links</guid>
      <pubDate>2024-01-07 15:48</pubDate>
      <category>#rss</category>
      <category>#indieweb</category>
    </item>
    <item>
      <title>AdventureTaco - Favorite Photos 2023 Edition</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/favorite-photos-2023-adventuretaco?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/favorite-photos-2023-adventuretaco&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://live.staticflickr.com/65535/53444961109_ca25a7de02_h.jpg" class="img-fluid" alt="A Toyota Tacoma Driving on a dirt road towards mountains in the background" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;&lt;a href="https://adventuretaco.com/"&gt;AdventureTaco&lt;/a&gt;&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/favorite-photos-2023-adventuretaco?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/favorite-photos-2023-adventuretaco</guid>
      <pubDate>2024-01-06 21:33</pubDate>
      <category>#landscapes</category>
      <category>#photography</category>
      <category>#overlanding</category>
      <category>#adventure</category>
      <category>#photos</category>
      <category>#desert</category>
      <category>#mountain</category>
    </item>
    <item>
      <title>Birthday Gift - I want you to blog</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/i-want-you-to-blog-mullenweg?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/i-want-you-to-blog-mullenweg&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;the gift I most want for my 40th is something everyone can do.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I want you to blog.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Publish a post. About anything! It can be long or short, a photo or a video, maybe a quote or a link to something you found interesting. Don‚Äôt sweat it. Just blog. Share something you created, or amplify something you enjoyed. It doesn‚Äôt take much. The act of publishing will be a gift for you and me.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;That‚Äôs it! No wrapping paper or bows. Just blogs and blogs and blogs, each unique and beautiful in its own way.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/i-want-you-to-blog-mullenweg?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/i-want-you-to-blog-mullenweg</guid>
      <pubDate>2024-01-03 00:16</pubDate>
      <category>#blogging</category>
      <category>#wordpress</category>
      <category>#writing</category>
      <category>#community</category>
    </item>
    <item>
      <title>The Year For Blogging To Pump Up The Volume</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/blogging-pump-up-volume?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/blogging-pump-up-volume&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;There‚Äôs been a lot of pontificating lately that the web is ripe for a blogging renaissance, wishing for it to be true. Much of it from people who don‚Äôt seem to notice that‚Äôs it‚Äôs already begun. Maybe they don‚Äôt anymore know quite where to look. Maybe the sorts of blogging they‚Äôre seeing isn‚Äôt what they mean. (To the blognoscenti, do things like ‚Äúwordvomits‚Äù count?) If you haven‚Äôt seen it, either, that‚Äôs okay. All you have to do is choose to be a part of it. There‚Äôs never been a better time: those who managed to monopolize our attentions and keep too many of us chattering for a few hundred characters at a time to the benefit of advertisers are losing their relevance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I‚Äôm not one for making personal resolutions, but let me suggest one on behalf of the blogosphere: this is the year we pump up the volume.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/blogging-pump-up-volume?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/blogging-pump-up-volume</guid>
      <pubDate>2024-01-02 18:36</pubDate>
      <category>#blogging</category>
      <category>#community</category>
      <category>#writing</category>
      <category>#movie</category>
      <category>#radio</category>
    </item>
    <item>
      <title>Blogs + RSS - best decentralized social media we ever had</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/blogs-rss-decentralized-social-media-lemmer-webber?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/blogs-rss-decentralized-social-media-lemmer-webber&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;figure class="figure"&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Bring back self-hosted blogs, reinstall a feed reader, make your feed icon prominent on your blog.  Blogs + Atom/RSS is the best decentralized social media system we've ever had!&lt;br /&gt;
&lt;br&gt;
And yes I am saying that as co-author of ActivityPub: self hosted blogs is the best decentralized social networking we've had&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;a href="https://octodon.social/@cwebber"&gt;Christine Lemmer-Webber (@cwebber@octodon.social)&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;üíØ üíØ üíØ üíØ üíØ üíØ&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/blogs-rss-decentralized-social-media-lemmer-webber?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/blogs-rss-decentralized-social-media-lemmer-webber</guid>
      <pubDate>2023-12-30 00:30</pubDate>
      <category>#rss</category>
      <category>#blog</category>
      <category>#decentralized</category>
      <category>#socialmedia</category>
      <category>#blogging</category>
      <category>#mastodon</category>
      <category>#fediverse</category>
      <category>#indieweb</category>
      <category>#atom</category>
      <category>#feed</category>
    </item>
    <item>
      <title>What are farm animals thinking?</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/what-farm-animals-think-orwell?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/what-farm-animals-think-orwell&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I think I've read about this somewhere before...&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;‚ÄúAll animals are equal, but some animals are more equal than others.‚Äù
‚Äï George Orwell, Animal Farm&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;‚ÄúFour legs good, two legs bad.‚Äù
‚Äï George Orwell, Animal Farm&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/what-farm-animals-think-orwell?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/what-farm-animals-think-orwell</guid>
      <pubDate>2023-12-27 14:57</pubDate>
      <category>#science</category>
      <category>#animals</category>
      <category>#farm</category>
      <category>#orwell</category>
      <category>#animallfarm</category>
      <category>#meme</category>
    </item>
    <item>
      <title>Ferret: Refer and Ground Anything Anywhere at Any Granularity</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/apple-ml-ferret?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/apple-ml-ferret&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination. Code and data will be available at this https URL&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/apple/ml-ferret"&gt;Code&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/apple-ml-ferret?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/apple-ml-ferret</guid>
      <pubDate>2023-12-25 20:12</pubDate>
      <category>#ai</category>
      <category>#ml</category>
      <category>#llm</category>
      <category>#mllm</category>
      <category>#largelanguagemodel</category>
      <category>#multimodal</category>
      <category>#multimodallargelanguagemodel</category>
      <category>#apple</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>A Purple Life - 2023 Goals and Accomplishments</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/2023-goals-accomplishments-purple-life?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/2023-goals-accomplishments-purple-life&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;My posts have been...including more reviews of fancy travel hacked flights, tours and slow travel locations. Possibly as a result of this shift in topic ‚Äì or possibly simply because blogging seems to be on its way out according to a few of my blogging peers ‚Äì my comments section has been quieter lately. I talked about this in one of my monthly recaps with the spin that I didn‚Äôt realize I had come to rely on getting at least one comment per post to know that people (and not just bots üôÇ ) were reading my words and they weren‚Äôt floating into an abyss.&lt;br /&gt;
&lt;br&gt;
I didn‚Äôt want to be reliant on external validation when I had written this blog without it being public for years, and hadn‚Äôt realized I had come to rely on anything but the joy I get from writing it. So I was trying to grow after realizing that not receiving comments on multiple posts in a row bothered me for some reason. I‚Äôm going to do my best to not rely on that kind of feedback going forward and will continue to blog for the main reason I always have: for myself üôÇ .&lt;br /&gt;
&lt;br&gt;
I also followed a reader suggestion to add a ‚ÄúLike‚Äù button at the bottom of my posts (it‚Äôs after the ‚ÄúShare This‚Äù section and before the ‚ÄúRelated‚Äù articles section) because readers said they don‚Äôt necessarily have something they want to comment, but that a Like button would help show there is still a human reading. All fair üôÇ .&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;üôã‚Äç‚ôÇÔ∏è there's still a human reading your posts on this end üôÇ&lt;/p&gt;
&lt;p&gt;Also, today I learned there's a Guineafowl Pufferfish.&lt;/p&gt;
&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://apurplelife.com/wp-content/uploads/2023/12/GFPuffer-copy.png" class="img-fluid" alt="Picture of a Guineafowl Pufferfish" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;&lt;a href="https://apurplelife.com/"&gt;A Purple Life&lt;/a&gt;&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/2023-goals-accomplishments-purple-life?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/2023-goals-accomplishments-purple-life</guid>
      <pubDate>2023-12-24 18:05</pubDate>
      <category>#blogging</category>
      <category>#review</category>
      <category>#goals</category>
    </item>
    <item>
      <title>Bluesky now supports RSS</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/bluesky-rss-support?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/bluesky-rss-support&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I'm not as active on there, but feel free to &lt;a href="https://bsky.app/profile/did:plc:pme7qquljcdx6i4zyawoxypd/rss"&gt;subscribe to my Bluesky feed&lt;/a&gt; wherever you subscribe to feeds.&lt;/p&gt;
&lt;p&gt;Also, if you have any feed recommendations, &lt;a href="/contact"&gt;let me know&lt;/a&gt;.&lt;/p&gt;
&lt;figure class="figure"&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;üì¢¬†1.60 is rolling out now (3/5)&lt;/p&gt;
&lt;p&gt;RSS feeds for profiles!&lt;/p&gt;
&lt;p&gt;Access your posts via RSS by pasting your profile link into your RSS feed reader and it will automatically be discovered.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;Bluesky on Bluesky&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/bluesky-rss-support?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/bluesky-rss-support</guid>
      <pubDate>2023-12-23 20:59</pubDate>
      <category>#bluesky</category>
      <category>#rss</category>
      <category>#socialmedia</category>
    </item>
    <item>
      <title>Conor McGregor pitching Zune and Windows Phone to Cristiano Ronaldo</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mcgregor-ronaldo-zune-windows-phone-meme?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mcgregor-ronaldo-zune-windows-phone-meme&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://pbs.twimg.com/amplify_video_thumb/1738661229822967808/img/nmYicHlHX6lR_pha.jpg" class="img-fluid" alt="Conor McGregor speaking to someone with Cristiano Ronaldo next to him" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;&lt;a href="https://twitter.com/HappyPunch/status/1738661281236672833"&gt;@HappyPunch on X&lt;/a&gt;&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;What was probably being said according to Copilot üòÇ&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Cristiano, you need to listen to me, mate. You're wasting your time and money on those Apple products. They're overpriced, overrated, and outdated. They're nothing compared to the real champions of technology: Windows Phone and Zune. Trust me, I know what I'm talking about. I've used them both, and they're the best phone and MP3 player ever made. Let me tell you why. First of all, they had a sleek design, a smooth interface, and a unique tile system that was way ahead of its time. They were also compatible with Microsoft's ecosystem, which gave you access to Office, Xbox, and Bing. And don't get me started on the Zune Pass, which let you stream and download unlimited music for a low monthly fee. You can't beat that, mate. Second of all, they had no apps, no games, no support, and no market share. Wait, what? No, no, no, that's not a bad thing. That's a good thing. That means they were exclusive, elite, and original. They were not for the masses, they were for the class. They were not for the sheep, they were for the lions. They were not for the followers, they were for the leaders. They were not for the weak, they were for the strong. They were not for you, they were for me. And they can be for you too, if you have the courage, the vision, and the taste to switch to Windows Phone and Zune. I'm telling you, Cristiano, you're missing out on the best phone and MP3 player ever made. And I'm willing to prove it to you. I'll bet you a million euros that if you try them for a week, you'll never go back to your Apple products. What do you say? Are you man enough to take the challenge? Or are you too scared to face the truth? Come on, Cristiano, don't be a coward. Don't be a fool. Don't be a loser. Be a winner. Be a legend. Be a Windows Phone and Zune user.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mcgregor-ronaldo-zune-windows-phone-meme?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mcgregor-ronaldo-zune-windows-phone-meme</guid>
      <pubDate>2023-12-23 20:23</pubDate>
      <category>#windowsphone</category>
      <category>#zune</category>
      <category>#ai</category>
      <category>#copilot</category>
      <category>#meme</category>
    </item>
    <item>
      <title>VideoPoet: A large language model for zero-shot video generation</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/videopoet-llm-zero-shot-video-generation?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/videopoet-llm-zero-shot-video-generation&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;VideoPoet is a simple modeling method that can convert any autoregressive language model or large language model (LLM) into a high-quality video generator. It contains a few simple components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A pre-trained MAGVIT V2 video tokenizer and a SoundStream audio tokenizer transform images, video, and audio clips with variable lengths into a sequence of discrete codes in a unified vocabulary. These codes are compatible with text-based language models, facilitating an integration with other modalities, such as text.&lt;/li&gt;
&lt;li&gt;An autoregressive language model learns across video, image, audio, and text modalities to autoregressively predict the next video or audio token in the sequence.&lt;/li&gt;
&lt;li&gt;A mixture of multimodal generative learning objectives are introduced into the LLM training framework, including text-to-video, text-to-image, image-to-video, video frame continuation, video inpainting and outpainting, video stylization, and video-to-audio. Furthermore, such tasks can be composed together for additional zero-shot capabilities (e.g., text-to-audio).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This simple recipe shows that language models can synthesize and edit videos with a high degree of temporal consistency. VideoPoet demonstrates state-of-the-art video generation, in particular in producing a wide range of large, interesting, and high-fidelity motions. The VideoPoet model supports generating videos in square orientation, or portrait to tailor generations towards short-form content, as well as supporting audio generation from a video input.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html"&gt;Blog Post&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/videopoet-llm-zero-shot-video-generation?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/videopoet-llm-zero-shot-video-generation</guid>
      <pubDate>2023-12-23 19:06</pubDate>
      <category>#ai</category>
      <category>#genai</category>
      <category>#videopoet</category>
      <category>#generativeai</category>
      <category>#google</category>
    </item>
    <item>
      <title>Classic Palmtop</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/classic-palmtop-concept?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/classic-palmtop-concept&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://cdna.artstation.com/p/assets/images/images/070/172/228/large/michal-kalisz-classicpalmtop-xen200-mk-scene07.jpg?1701906705" class="img-fluid" alt="Cyberdeck radio concept art rendering with display closed front and back profile" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;Michal Kalisz&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://cdnb.artstation.com/p/assets/images/images/070/172/345/large/michal-kalisz-classicpalmtop-xen200-mk-bright-03.jpg?1701907028" class="img-fluid" alt="Cyberdeck radio concept art rendering with display open and keyboard showing" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;Michal Kalisz&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;img src="https://media.giphy.com/media/cALkoAIov3Y9a/giphy.gif" class="img-fluid" alt="Robert Downey Jr. as Tony Stark saying I need it" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/classic-palmtop-concept?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/classic-palmtop-concept</guid>
      <pubDate>2023-12-22 09:28</pubDate>
      <category>#concept</category>
      <category>#tech</category>
      <category>#cyberpunk</category>
    </item>
    <item>
      <title>Midjourney v6</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/midjourney-v6-release?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/midjourney-v6-release&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Dev Team gonna let the community test an alpha-version of Midjourney v6 model over the winter break, starting tonight, December 21st, 2023.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;What‚Äôs new with the Midjourney v6 base model?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Much more accurate prompt following as well as longer prompts,&lt;/li&gt;
&lt;li&gt;Improved coherence, and model knowledge,&lt;/li&gt;
&lt;li&gt;Improved image prompting and remix mode,&lt;/li&gt;
&lt;li&gt;Minor text drawing ability (you must write your text in ‚Äúquotations‚Äù and --style raw or lower --stylize values may help)&lt;/li&gt;
&lt;li&gt;/imagine a photo of the text &amp;quot;Hello World!&amp;quot; written with a marker on a sticky note --ar 16:9 --v 6&lt;/li&gt;
&lt;li&gt;Improved upscalers, with both 'subtle‚Äò and 'creative‚Äò modes (increases resolution by 2x) (you‚Äôll see buttons for these under your images after clicking U1/U2/U3/U4)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/midjourney-v6-release?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/midjourney-v6-release</guid>
      <pubDate>2023-12-22 08:57</pubDate>
      <category>#computervision</category>
      <category>#ai</category>
      <category>#midjourney</category>
    </item>
    <item>
      <title>LangChain State of AI 2023</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/langchain-state-of-ai-2023?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/langchain-state-of-ai-2023&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h2&gt;What are people building?&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Retrieval has emerged as the dominant way to combine your data with LLMs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...42% of complex queries involve retrieval&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...about 17% of complex queries are part of an agent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Most used LLM Providers&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;OpenAI has emerged as the leading LLM provider of 2023, and Azure (with more enterprise guarantees) has seized that momentum well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;On the open source model side, we see Hugging Face (4th), Fireworks AI (6th), and Ollama (7th) emerge as the main ways users interact with those models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;OSS Model Providers&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A lot of attention recently has been given to open source models, with more and more providers racing to host them at cheaper and cheaper costs. So how exactly are developers accessing these open source models?&lt;/p&gt;
&lt;p&gt;We see that the people are mainly running them locally, with options to do so like Hugging Face, LlamaCpp, Ollama, and GPT4All ranking high.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Most used vector stores&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Vectorstores are emerging as the primary way to retrieve relevant context.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...local vectorstores are the most used, with Chroma, FAISS, Qdrant and DocArray all ranking in the top 5.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Of the hosted offerings, Pinecone leads the pack as the only hosted vectorstore in the top 5. Weaviate follows next, showing that vector-native databases are currently more used than databases that add in vector functionality.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Of databases that have added in vector functionality, we see Postgres (PGVector), Supabase, Neo4j, Redis, Azure Search, and Astra DB leading the pack.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Most used embeddings&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;OpenAI reigns supreme&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Open source providers are more used, with Hugging Face coming in 2nd most use&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;On the hosted side, we see that Vertex AI actually beats out AzureOpenAI&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Top Advanced Retrieval Strategies&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;the most common retrieval strategy we see is not a built-in one but rather a custom one.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;After that, we see more familiar names popping up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Self Query&lt;/strong&gt; - which extracts metadata filters from user's questions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hybrid Search&lt;/strong&gt; - mainly through provider specific integrations like Supabase and Pinecone&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contextual Compression&lt;/strong&gt; - which is postprocessing of base retrieval results&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi Query&lt;/strong&gt; - transforming a single query into multiple, and then retrieving results for all&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TimeWeighted VectorStore&lt;/strong&gt; - give more preference to recent documents&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2&gt;How are people testing?&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;83% of test runs have some form of feedback associated with them. Of the runs with feedback, they average 2.3 different types of feedback, suggesting that developers are having difficulty finding a single metric to rely entirely on, and instead use multiple different metrics to evaluate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the majority of them use an LLM to evaluate the outputs. While some have expressed concern and hesitation around this, we are bullish on this as an approach and see that in practice it has emerged as the dominant way to test.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...nearly 40% of evaluators are custom evaluators. This is in line with the fact that we've observed that evaluation is often really specific to the application being worked on, and there's no one-size-fits-all evaluator to rely on.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;What are people testing?&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...most people are still primarily concerned with the correctness of their application (as opposed to toxicity, prompt leakage, or other guardrails&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...low usage of Exact Matching as an evaluation technique [suggests] that judging correctness is often quite complex (you can't just compare the output exactly as is)&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/langchain-state-of-ai-2023?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/langchain-state-of-ai-2023</guid>
      <pubDate>2023-12-22 08:32</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#opensource</category>
      <category>#report</category>
    </item>
    <item>
      <title>Eigensolutions: composability as the antidote to overfit </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/eigensolutions-composability-antidote-overfitting-verou?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/eigensolutions-composability-antidote-overfitting-verou&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;tl;dr: Overfitting happens when solutions don‚Äôt generalize sufficiently and is a hallmark of poor design. Eigensolutions are the opposite: solutions that generalize so much they expose links between seemingly unrelated use cases. Designing eigensolutions takes a mindset shift from linear design to composability.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The eigensolution is a solution that addresses several key use cases, that previously appeared unrelated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...it takes a mindset shift, from the linear Use case ‚Üí Idea ‚Üí Solution process to composability. Rather than designing a solution to address only our driving use cases, step back and ask yourself: can we design a solution as a composition of smaller, more general features, that could be used together to address a broader set of use cases?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Contrary to what you may expect, eigensolutions can actually be quite hard to push to stakeholders:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Due to their generality, they often require significantly higher engineering effort to implement. Quick-wins are easier to sell: they ship faster and add value sooner. In my 11 years designing web technologies, I have seen many beautiful, elegant eigensolutions be vetoed due to implementation difficulties in favor of far more specific solutions ‚Äî and often this was the right decision, it‚Äôs all about the cost-benefit.&lt;/li&gt;
&lt;li&gt;Eigensolutions tend to be lower level primitives, which are more flexible, but can also involve higher friction to use than a solution that is tailored to a specific use case.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Eigensolutions tend to be lower level primitives. They enable a broad set of use cases, but may not be the most learnable or efficient way to implement all of them, compared to a tailored solution. In other words, they make complex things possible, but do not necessarily make common things easy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Instead of implementing tailored solutions ad-hoc (risking overfitting), they can be implemented as shortcuts: higher level abstractions using the lower level primitive. Done well, shortcuts provide dual benefit: not only do they reduce friction for common cases, they also serve as teaching aids for the underlying lower level feature. This offers a very smooth ease-of-use to power curve: if users need to go further than what the shortcut provides, they can always fall back on the lower level primitive to do so.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In an ideal world, lower level primitives and higher level abstractions would be designed and shipped together. However, engineering resources are typically limited, and it often makes sense to ship one before the other, so we can provide value sooner.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This can happen in either direction:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lower level primitive first. Shortcuts to make common cases easy can ship at a later stage, and demos and documentation to showcase common ‚Äúrecipes‚Äù can be used as a stopgap meanwhile. This prioritizes use case coverage over optimal UX, but it also allows collecting more data, which can inform the design of the shortcuts implemented.&lt;/li&gt;
&lt;li&gt;Higher level abstraction first, as an independent, ostensibly ad hoc feature. Then later, once the lower level primitive ships, it is used to ‚Äúexplain‚Äù the shortcut, and make it more powerful. This prioritizes optimal UX over use case coverage: we‚Äôre not covering all use cases, but for the ones we are covering, we‚Äôre offering a frictionless user experience.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...despite the name eigensolution, it‚Äôs still all about the use cases: eigensolutions just expose links between use cases that may have been hard to detect, but seem obvious in retrospect...Requiring all use cases to precede any design work can be unnecessarily restrictive, as frequently solving a problem improves our understanding of the problem.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/eigensolutions-composability-antidote-overfitting-verou?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/eigensolutions-composability-antidote-overfitting-verou</guid>
      <pubDate>2023-12-21 22:38</pubDate>
      <category>#design</category>
      <category>#product</category>
    </item>
    <item>
      <title>Proton Mail versus Tuta (Tutanota) encryption</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/proton-mail-vs-tutanota-encryption?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/proton-mail-vs-tutanota-encryption&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;img src="https://media.giphy.com/media/gl0mkIZOW6Nwc/giphy.gif" class="img-fluid" alt="GIF of comedian Bill Hader eating popcorn" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/proton-mail-vs-tutanota-encryption?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/proton-mail-vs-tutanota-encryption</guid>
      <pubDate>2023-12-21 21:18</pubDate>
      <category>#protonmail</category>
      <category>#tutanota</category>
      <category>#encryption</category>
    </item>
    <item>
      <title>Phi-2 now on HuggingFace</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/phi-2-huggingface?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/phi-2-huggingface&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;When &lt;a href="/feed/microsoft-phi-2"&gt;Phi-2 initially released&lt;/a&gt;, it was on the Azure AI Studio Model Catalog. It's nice to see it's now in HuggingFace as well.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/phi-2-huggingface?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/phi-2-huggingface</guid>
      <pubDate>2023-12-20 18:37</pubDate>
      <category>#opensource</category>
      <category>#ai</category>
      <category>#phi</category>
      <category>#languagemodel</category>
      <category>#smalllanguagemodel</category>
      <category>#huggingface</category>
      <category>#transformers</category>
      <category>#microsoft</category>
    </item>
    <item>
      <title>LLM in a flash: Efficient Large Language Model Inference with Limited Memory</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/llm-flash-inference-limited-memory?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/llm-flash-inference-limited-memory&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their intensive computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters on flash memory but bringing them on demand to DRAM. Our method involves constructing an inference cost model that harmonizes with the flash memory behavior, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this flash memory-informed framework, we introduce two principal techniques. First, &amp;quot;windowing'&amp;quot; strategically reduces data transfer by reusing previously activated neurons, and second, &amp;quot;row-column bundling&amp;quot;, tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/llm-flash-inference-limited-memory?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/llm-flash-inference-limited-memory</guid>
      <pubDate>2023-12-20 18:30</pubDate>
      <category>#llm</category>
      <category>#ai</category>
      <category>#hardware</category>
    </item>
    <item>
      <title>It‚Äôs Time to Dismantle the Technopoly</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/dismantle-technology-newport?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/dismantle-technology-newport&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...according to [Neil Postman], we no longer live in a technocratic era. We now inhabit what he calls technopoly. In this third technological age, Postman argues, the fight between invention and traditional values has been resolved, with the former emerging as the clear winner. The result is the ‚Äúsubmission of all forms of cultural life to the sovereignty of technique and technology.‚Äù Innovation and increased efficiency become the unchallenged mechanisms of progress, while any doubts about the imperative to accommodate the shiny and new are marginalized. ‚ÄúTechnopoly eliminates alternatives to itself in precisely the way Aldous Huxley outlined in Brave New World,‚Äù Postman writes. ‚ÄúIt does not make them illegal. It does not make them immoral. It does not even make them unpopular. It makes them invisible and therefore irrelevant.‚Äù Technopoly, he concludes, ‚Äúis totalitarian technocracy.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;What I didn‚Äôt realize back in 2016, however, was that, although the grip of technopoly was strong, it was also soon to weaken.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A major source of this destabilization was the Trump-Clinton election cycle...Where once they had seen platforms like Facebook as useful and in some sense mandatory, they started treating them more warily.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This emerging resistance to the technopoly mind-set doesn‚Äôt fall neatly onto a spectrum with techno-optimism at one end and techno-skepticism at the other. Instead, it occupies an orthogonal dimension we might call techno-selectionism. This is a perspective that accepts the idea that innovations can significantly improve our lives but also holds that we can build new things without having to accept every popular invention as inevitable. Techno-selectionists believe that we should continue to encourage and reward people who experiment with what comes next. But they also know that some experiments end up causing more bad than good. Techno-selectionists can be enthusiastic about artificial intelligence, say, while also taking a strong stance on settings where we should block its use. They can marvel at the benefits of the social Internet without surrendering their kids‚Äô mental lives to TikTok.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/dismantle-technology-newport?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/dismantle-technology-newport</guid>
      <pubDate>2023-12-20 10:12</pubDate>
      <category>#technology</category>
      <category>#culture</category>
      <category>#media</category>
      <category>#internet</category>
    </item>
    <item>
      <title>OpenAI - Prompt engineering</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openai-prompting-guide?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openai-prompting-guide&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This guide shares strategies and tactics for getting better results from large language models (sometimes referred to as GPT models) like GPT-4. The methods described here can sometimes be deployed in combination for greater effect. We encourage experimentation to find the methods that work best for you.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openai-prompting-guide?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openai-prompting-guide</guid>
      <pubDate>2023-12-19 19:37</pubDate>
      <category>#openai</category>
      <category>#llm</category>
      <category>#promptengineering</category>
      <category>#ai</category>
      <category>#guide</category>
    </item>
    <item>
      <title>Spritely and Veilid: Exciting Projects Building the Peer-to-Peer Web</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/spritely-veilid-p2p-web?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/spritely-veilid-p2p-web&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;While there is a surge in federated social media sites, like Bluesky and Mastodon, some technologists are hoping to take things further than this model of decentralization with fully peer-to-peer applications. Two leading projects, Spritely and Veilid, hint at what this could look like.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Spritely is a framework for building distributed apps that don‚Äôt even have to know that they‚Äôre distributed. The project is spearheaded by Christine Lemmer-Webber, who was one of the co-authors of the ActivityPub spec that drives the fediverse. She is taking the lessons learned from that work, combining them with security and privacy minded object capabilities models, and mixing it all up into a model for peer to peer computation that could pave the way for a generation of new decentralized tools.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Veilid project was released at DEFCON 31 in August and has a number of promising features that could lead to it being a fundamental tool in future decentralized systems. Described as a cross between TOR and Interplanetary File System (IPFS), Veilid is a framework and protocol that offers two complementary tools. The first is private routing, which, much like TOR, can construct an encrypted private tunnel over the public internet allowing two devices to communicate with each other without anyone else on the network knowing who is talking to whom...The second tool that Veilid offers is a Distributed Hash Table (DHT), which lets anyone look up a bit of data associated with a specific key, wherever that data lives on the network.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Public interest in decentralized tools and services is growing, as people realize that there are downsides to centralized control over the platforms that connect us all. The past year has seen interest in networks like the fediverse and Bluesky explode and there‚Äôs no reason to expect that to change. Projects like Spritely and Veilid are pushing the boundaries of how we might build apps and services in the future. The things that they are making possible may well form the foundation of social communication on the internet in the next decade, making our lives online more free, secure, and resilient.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Additional Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://spritely.institute/"&gt;Spritely Institute Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://veilid.com/"&gt;Veilid Website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/spritely-veilid-p2p-web?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/spritely-veilid-p2p-web</guid>
      <pubDate>2023-12-19 11:47</pubDate>
      <category>#p2p</category>
      <category>#veilid</category>
      <category>#spritely</category>
      <category>#openweb</category>
      <category>#web</category>
      <category>#internet</category>
      <category>#decentralization</category>
    </item>
    <item>
      <title>2023 in social media: the case for the fediverse</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/2023-social-media-case-fediverse?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/2023-social-media-case-fediverse&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Good article from David Pierce. I'd add that many of these platforms (i.e. Mastodon, Lemmy, PeerTube, WordPress) have strong RSS support which offer another degree of freedom from opting out of signing up for any of the platforms but still being able to follow the people and topics you care about. Sure, the experience may not be as rich but it's yet another way for people to participate in the ecosystem.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A new kind of social internet is currently forming. Right now it might still look like ‚ÄúTwitter and Reddit, only different,‚Äù but that‚Äôs only the very beginning of what‚Äôs to come. Hopefully.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I‚Äôm convinced we‚Äôll be better off with a hundred different apps for Snapchat or Instagram or X instead of just one...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It doesn‚Äôt make sense that we have a dozen usernames, a dozen profiles, a dozen sets of fans and friends. All that stuff should belong to me, and I should be able to access it and interact with it anywhere and everywhere.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Decentralizing social media can sound like a sort of kumbaya anti-capitalist manifesto: ‚ÄúIt‚Äôs about openness and sharing, not capitalism, man!‚Äù In practice it‚Äôs the opposite: it‚Äôs a truly free market approach to social networking.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...in a fediverse-dominated world, the way to win is not to achieve excellent lock-in and network effects. The only way to win is to build the best product.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...so far we‚Äôre mostly in the ‚Äúpopular app, but federated‚Äù phase of this transition.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Almost everything in the fediverse is a one-to-one competitor to an existing platform...Some of these apps are very good! But nearly all of them are differentiated only in that they‚Äôre federated.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Let‚Äôs be super clear about this: the point of the fediverse is not that it‚Äôs federated...Making the ‚ÄúIt‚Äôs federated!‚Äù argument is like making the ‚ÄúIt‚Äôs better for privacy!‚Äù argument: it makes you feel good, and at best it‚Äôs a useful tiebreaker, but it doesn‚Äôt actually matter. All that matters is the product.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;2023 was the year ‚Äúfediverse‚Äù became a buzzword, 2024 will be the year it becomes an industry. (Hopefully one with a better name, but I‚Äôll get over that.) We‚Äôve spent too long living our lives online in someone else‚Äôs spaces. What‚Äôs next will belong to all of us. All that‚Äôs left to do is start posting.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/2023-social-media-case-fediverse?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/2023-social-media-case-fediverse</guid>
      <pubDate>2023-12-19 09:26</pubDate>
      <category>#fediverse</category>
      <category>#socialmedia</category>
      <category>#activitypub</category>
      <category>#internet</category>
      <category>#web</category>
      <category>#openweb</category>
      <category>#smallweb</category>
    </item>
    <item>
      <title>"Microblogging" prior art</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/microblogging-telegram-jwz?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/microblogging-telegram-jwz&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Just published a blog post, &lt;a href="/posts/ai-1999-1899"&gt;AI like it's 1999 or 1899&lt;/a&gt;, inspired by this post from jwz, among other things.&lt;/p&gt;
&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://files.mastodon.social/media_attachments/files/111/583/682/973/573/497/original/1d93c25bbdc52784.png" class="img-fluid" alt="A meme of a telegram saying F**k You Strong Letter to Follow" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;&lt;a href="https://mastodon.social/@jwz"&gt;@jwz@mastodon.social&lt;/a&gt;&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/microblogging-telegram-jwz?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/microblogging-telegram-jwz</guid>
      <pubDate>2023-12-15 11:18</pubDate>
      <category>#microblogging</category>
      <category>#telegram</category>
      <category>#meme</category>
    </item>
    <item>
      <title>FLOSS Weekly ended its run on the TWiT network</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/floss-weekly-ends-twit-start-of-an-era?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/floss-weekly-ends-twit-start-of-an-era&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;After 17 years and 761 episodes, FLOSS Weekly ended its run on the TWiT network yesterday.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Nooo! So sad to hear that FLOSS Weekly is ending, especially after learning that &lt;a href="/feed/farewell-privacy-security-osint-podcast"&gt;The Privacy, Security, and OSINT Show with Michael Bazzell ended as well&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At least there's some hope at the end of Doc's post which hints at it living on in some form.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;By the way, FLOSS Weekly has not slipped below the waves. I expect it will be picked up somewhere else on the Web, and wherever you get your podcasts. (I love that expression because it means podcasting isn‚Äôt walled into some giant‚Äôs garden.) When that happens, I‚Äôll point to it here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In any case, it was good while it lasted. Also, there's still &lt;a href="https://www.reality2cast.com/"&gt;Reality 2.0&lt;/a&gt; where the guests and topics are just as interesting and entertaining.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/floss-weekly-ends-twit-start-of-an-era?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/floss-weekly-ends-twit-start-of-an-era</guid>
      <pubDate>2023-12-14 21:10</pubDate>
      <category>#podcast</category>
      <category>#floss</category>
      <category>#twit</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Solo - an AI website builder for solopreneurs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/solo-mozilla-ai-website-builder?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/solo-mozilla-ai-website-builder&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we are excited to introduce a new Mozilla Innovation Project, Solo, an AI website builder for solopreneurs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;If you scour Yelp, it appears a third of businesses lack a website. However, building a website not only provides you with a presence that you own and control but it is also good for business.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our survey data shows that the majority of solopreneurs rely upon their ‚Äútech buddy‚Äù to help build their website. As a result, the websites become stale and harder to maintain as it relies on a call to their buddy. Others without a ‚Äútech buddy‚Äù try popular website authoring tools and then abandon because it‚Äôs simply too hard to author and curate content.&lt;/p&gt;
&lt;p&gt;Using AI to generate the content of your site and source your images, which a solopreneur can then revise into their own unique voice and style levels the playing field. Solo takes this a step further and can also scrape your existing business Yelp or other page so you have an online presence that is totally authentic to you.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/solo-mozilla-ai-website-builder?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/solo-mozilla-ai-website-builder</guid>
      <pubDate>2023-12-14 10:10</pubDate>
      <category>#ai</category>
      <category>#mozilla</category>
      <category>#web</category>
      <category>#internet</category>
    </item>
    <item>
      <title>Farewell to The Privacy, Security, and OSINT Show</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/farewell-privacy-security-osint-podcast?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/farewell-privacy-security-osint-podcast&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...a farewell episode was released on November 20th, 2023 entitled ‚ÄúMy Irish Exit‚Äù. It was finally officially confirmed that [The Privacy, Security, and OSINT Show with Michael Bazzell]...has reached an end.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's unfortunate. I really enjoyed listening to this show and even had it listed in my &lt;a href="/feed/podroll"&gt;podroll&lt;/a&gt;. The UNREDACTED magazine had great content as well.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/farewell-privacy-security-osint-podcast?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/farewell-privacy-security-osint-podcast</guid>
      <pubDate>2023-12-13 20:00</pubDate>
      <category>#podcast</category>
      <category>#privacy</category>
      <category>#osint</category>
    </item>
    <item>
      <title>Introducing Journaling Suggestions for Day One</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/journaling-suggestions-day-one?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/journaling-suggestions-day-one&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;See journaling recommendations inspired by your photos, locations, activities and more. Exclusively for iPhone.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I've been tinkering with Day One the past few months. When paired with their templates, this is a nice addition. Too bad it's iPhone exclusive. Hopefully it makes its way to Android at some point.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/journaling-suggestions-day-one?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/journaling-suggestions-day-one</guid>
      <pubDate>2023-12-12 20:49</pubDate>
      <category>#journaling</category>
      <category>#dayone</category>
    </item>
    <item>
      <title>Aoudaghost, unveiling the lost economic hub of the Sahara</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/aoudaghost-aprin?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/aoudaghost-aprin&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Since 2001, the site has been on the UNESCO World Heritage Tentative List.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, Aoudaghost is in a state of complete abandonment. The remains of the once-thriving town are concentrated in the area most protected by the wind and sand, with several walls and fortifications yet to be fully englobed by the desert. From the adjacent cliff, the current state of Aoudaghost can be seen in its entirety, but only the mind can imagine the Aoudaghost that served as an economic and cultural hub for the Sahara&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/aoudaghost-aprin?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/aoudaghost-aprin</guid>
      <pubDate>2023-12-12 19:45</pubDate>
      <category>#travel</category>
      <category>#sahara</category>
    </item>
    <item>
      <title>MemoryCache - Augmenting Local AI with Browser Data</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/memorycache-personal-ai?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/memorycache-personal-ai&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;MemoryCache, a Mozilla Innovation Project, is an early exploration project that augments an on-device, personal model with local files saved from the browser to reflect a more personalized and tailored experience through the lens of privacy and agency.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Additional resources&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://memorycache.ai/"&gt;https://memorycache.ai/&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/memorycache-personal-ai?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/memorycache-personal-ai</guid>
      <pubDate>2023-12-12 19:37</pubDate>
      <category>#ai</category>
      <category>#web</category>
      <category>#privacy</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Mozilla Innovation Week - Explore the Future of AI with Mozilla</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mozilla-innovation-week-dec-2023?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mozilla-innovation-week-dec-2023&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mozilla‚Äôs Innovation Week is a journey into the future of technology, where AI is not just a buzzword, but a reality we're actively shaping. Here, we're not just talking about innovation ‚Äì we're living it through a series of AI-driven explorations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;With that in mind, Innovation Week is more than a showcase. It's a platform for collaboration and inspiration. It's about bringing together ideas, people, and technology to pave the way for a more open and responsible future.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mozilla-innovation-week-dec-2023?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mozilla-innovation-week-dec-2023</guid>
      <pubDate>2023-12-12 19:34</pubDate>
      <category>#ai</category>
      <category>#opensource</category>
      <category>#mozilla</category>
    </item>
    <item>
      <title>Bash One-Liners for LLMs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/llamafile-llm-oneliners-tunney?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/llamafile-llm-oneliners-tunney&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I spent the last month working with Mozilla to launch an open source project called llamafile which is the new best way to run an LLM on your own computer. So far things have been going pretty smoothly. The project earned 5.6k stars on GitHub, 1073 upvotes on Hacker News, and received press coverage from Hackaday. Yesterday I cut a 0.3 release so let's see what it can do.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/llamafile-llm-oneliners-tunney?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/llamafile-llm-oneliners-tunney</guid>
      <pubDate>2023-12-12 19:21</pubDate>
      <category>#llama</category>
      <category>#ai</category>
      <category>#llm</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Mixtral 8x7B on Apple Silicon with MLX</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mlx-mixtral-8x7b?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mlx-mixtral-8x7b&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Run the Mixtral1 8x7B mixture-of-experts (MoE) model in MLX on Apple silicon.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mlx-mixtral-8x7b?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mlx-mixtral-8x7b</guid>
      <pubDate>2023-12-12 19:19</pubDate>
      <category>#mlx</category>
      <category>#ai</category>
      <category>#frameworks</category>
    </item>
    <item>
      <title>State of the Word 2023</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/state-of-the-word-2023?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/state-of-the-word-2023&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;State of the Word is the annual keynote address delivered by the WordPress project‚Äôs co-founder, Matt Mullenweg, celebrating the progress of the open source project and offering a glimpse into its future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=c7M4mBVgP3Y" title="State of the Word 2023"&gt;&lt;img src="http://img.youtube.com/vi/c7M4mBVgP3Y/0.jpg" class="img-fluid" alt="State of the Word 2023" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/state-of-the-word-2023?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/state-of-the-word-2023</guid>
      <pubDate>2023-12-12 19:17</pubDate>
      <category>#wordpress</category>
      <category>#blogging</category>
      <category>#web</category>
      <category>#opensource</category>
      <category>#community</category>
    </item>
    <item>
      <title>Steering at the Frontier: Extending the Power of Prompting </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/medprompt-promptbase-extending-power-prompting?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/medprompt-promptbase-extending-power-prompting&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...steering GPT-4 with a modified version of Medprompt achieves the highest score ever achieved on the complete MMLU.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;To achieve a new SoTA on MMLU, we extended Medprompt to Medprompt+ by adding a simpler prompting method and formulating a policy for deriving a final answer by integrating outputs from both the base Medprompt strategy and the simple prompts. The synthesis of a final answer is guided by a control strategy governed by GPT-4 and inferred confidences of candidate answers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;While systematic prompt engineering can yield maximal performance, we continue to explore the out-of-the-box performance of frontier models with simple prompts. It‚Äôs important to keep an eye on the native power of GPT-4 and how we can steer the model with zero- or few-shot prompting strategies.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/medprompt-promptbase-extending-power-prompting?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/medprompt-promptbase-extending-power-prompting</guid>
      <pubDate>2023-12-12 19:14</pubDate>
      <category>#ai</category>
      <category>#promptengineering</category>
    </item>
    <item>
      <title>promptbase</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/microsoft-promptbase?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/microsoft-promptbase&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;promptbase is an evolving collection of resources, best practices, and example scripts for eliciting the best performance from foundation models like GPT-4.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/microsoft-promptbase?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/microsoft-promptbase</guid>
      <pubDate>2023-12-12 19:13</pubDate>
      <category>#ai</category>
      <category>#promptengineering</category>
    </item>
    <item>
      <title>LLM360: Towards Fully Transparent Open-Source LLMs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/llm360-transparent-open-source-llm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/llm360-transparent-open-source-llm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at this &lt;a href="https://www.llm360.ai/"&gt;https URL&lt;/a&gt;). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Additional Resources&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.llm360.ai/"&gt;https://www.llm360.ai/&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/llm360-transparent-open-source-llm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/llm360-transparent-open-source-llm</guid>
      <pubDate>2023-12-12 19:02</pubDate>
      <category>#ai</category>
      <category>#research</category>
    </item>
    <item>
      <title>Phi-2: The surprising power of small language models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/microsoft-phi-2?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/microsoft-phi-2&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are now releasing Phi-2, a 2.7 billion-parameter language model that demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters. On complex benchmarks Phi-2 matches or outperforms models up to 25x larger, thanks to new innovations in model scaling and training data curation.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/microsoft-phi-2?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/microsoft-phi-2</guid>
      <pubDate>2023-12-12 19:00</pubDate>
      <category>#ai</category>
      <category>#slm</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Answer.AI - A new old kind of R&amp;D lab</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/answer-ai-rd-lab?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/answer-ai-rd-lab&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Jeremy Howard (founding CEO, previously co-founder of Kaggle and fast.ai) and Eric Ries (founding director, previously creator of Lean Startup and the Long-Term Stock Exchange) today launched Answer.AI, a new kind of AI R&amp;amp;D lab which creates practical end-user products based on foundational research breakthroughs. The creation of Answer.AI is supported by an investment of USD10m from Decibel VC. Answer.AI will be a fully-remote team of deep-tech generalists‚Äîthe world‚Äôs very best, regardless of where they live, what school they went to, or any other meaningless surface feature.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/answer-ai-rd-lab?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/answer-ai-rd-lab</guid>
      <pubDate>2023-12-12 18:57</pubDate>
      <category>#ai</category>
      <category>#research</category>
      <category>#vc</category>
    </item>
    <item>
      <title>Introducing Stable LM Zephyr 3B</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-stable-lm-zephyr-3b?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-stable-lm-zephyr-3b&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable LM Zephyr 3B is a 3 billion parameter Large Language Model (LLM), 60% smaller than 7B models, allowing accurate, and responsive output on a variety of devices without requiring high-end hardware.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-stable-lm-zephyr-3b?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-stable-lm-zephyr-3b</guid>
      <pubDate>2023-12-11 20:46</pubDate>
      <category>#llm</category>
      <category>#ai</category>
      <category>#edge</category>
    </item>
    <item>
      <title>Tumblr‚Äôs ‚Äòfediverse‚Äô integration is still being worked on</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tumblr-still-working-fediverse-integration?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tumblr-still-working-fediverse-integration&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Despite delays, the plan to connect Tumblr‚Äôs blogging site to the wider world of decentralized social media, also known as the ‚Äúfediverse,‚Äù is still on, it seems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...Mullenweg explained that despite the re-org, which will see many Tumblr employees move to other projects at the end of the year, Automattic did switch someone over to Tumblr to work on the fediverse integration, which will continue in the new year.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;‚ÄúI remain a huge believer in open standards and user freedom, though I don‚Äôt claim to have the truth on which particular standard is better or best, to serve our customers we will support everything we can in good faith to give users more freedom, choice, and avoid lock-in,‚Äù [Matt Mullenweg] also said in his AMA.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mullunweg also noted that a larger effort to migrate Tumblr‚Äôs half a billion blogs to WordPress on the backend is something he‚Äôs also contemplating in the new year.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tumblr-still-working-fediverse-integration?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tumblr-still-working-fediverse-integration</guid>
      <pubDate>2023-12-11 20:29</pubDate>
      <category>#tumblr</category>
      <category>#socialmedia</category>
      <category>#fediverse</category>
    </item>
    <item>
      <title>The Geometry of Truth: Dataexplorer</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/geometry-of-truth-data-explorer?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/geometry-of-truth-data-explorer&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This page contains interactive charts for exploring how large language models represent truth. It accompanies the paper &lt;a href="https://arxiv.org/abs/2310.06824"&gt;The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets&lt;/a&gt; by Samuel Marks and Max Tegmark.&lt;/p&gt;
&lt;p&gt;To produce these visualizations, we first extract &lt;a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/"&gt;LLaMA-13B&lt;/a&gt; representations of factual statements. These representations live in a 5120-dimensional space, far too high-dimensional for us to picture, so we use &lt;a href="https://en.wikipedia.org/wiki/Principal_component_analysis"&gt;PCA&lt;/a&gt; to select the two directions of greatest variation for the data. This allows us to produce 2-dimensional pictures of 5120-dimensional data. See this footnote for more details.&lt;a href="https://saprmarks.github.io/geometry-of-truth/dataexplorer/#fn:1"&gt;1&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/geometry-of-truth-data-explorer?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/geometry-of-truth-data-explorer</guid>
      <pubDate>2023-12-11 20:19</pubDate>
      <category>#ai</category>
      <category>#interpretability</category>
      <category>#llm</category>
    </item>
    <item>
      <title>State of AI Report - 2023</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/2023-state-of-ai-report?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/2023-state-of-ai-report&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Key themes in the 2023 Report include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPT-4 is the master of all it surveys (for now)&lt;/strong&gt;, beating every other LLM on both classic benchmarks and exams designed to evaluate humans, validating the power of proprietary architectures and reinforcement learning from human feedback.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efforts are growing to try to clone or surpass proprietary performance&lt;/strong&gt;, through smaller models, better datasets, and longer context. These could gain new urgency, amid concerns that human-generated data may only be able to sustain AI scaling trends for a few more years.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LLMs and diffusion models continue to drive real-world breakthroughs&lt;/strong&gt;, especially in the life sciences, with meaningful steps forward in both molecular biology and drug discovery.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compute is the new oil&lt;/strong&gt;, with NVIDIA printing record earnings and startups wielding their GPUs as a competitive edge. As the US tightens its restrictions on trade restrictions on China and mobilizes its allies in the chip wars, NVIDIA, Intel, and AMD have started to sell export-control proof chips at scale.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GenAI saves the VC world&lt;/strong&gt;, as amid a slump in tech valuations, AI startups focused on generative AI applications (including video, text, and coding), raised over $18 billion from VC and corporate investors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The safety debate has exploded into the mainstream&lt;/strong&gt;, prompting action from governments and regulators around the world. However, this flurry of activity conceals profound divisions within the AI community and a lack of concrete progress towards global governance, as governments around the world pursue conflicting approaches.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Challenges mount in evaluating state of the art models&lt;/strong&gt;, as standard LLMs often struggle with robustness. Considering the stakes, as ‚Äúvibes-based‚Äù approach isn‚Äôt good enough.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Additional resources&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.stateof.ai/"&gt;State of AI Website&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/2023-state-of-ai-report?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/2023-state-of-ai-report</guid>
      <pubDate>2023-12-11 20:13</pubDate>
      <category>#ai</category>
    </item>
    <item>
      <title>OnnxStream - Stable Diffusion XL 1.0 Base on a Raspberry Pi Zero 2</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/onnxstream?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/onnxstream&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Generally major machine learning frameworks and libraries are focused on minimizing inference latency and/or maximizing throughput, all of which at the cost of RAM usage. So I decided to write a super small and hackable inference library specifically focused on minimizing memory consumption: OnnxStream.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;OnnxStream is based on the idea of decoupling the inference engine from the component responsible of providing the model weights, which is a class derived from WeightsProvider. A WeightsProvider specialization can implement any type of loading, caching and prefetching of the model parameters. For example a custom WeightsProvider can decide to download its data from an HTTP server directly, without loading or writing anything to disk (hence the word &amp;quot;Stream&amp;quot; in &amp;quot;OnnxStream&amp;quot;). Three default WeightsProviders are available: DiskNoCache, DiskPrefetch and Ram.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;OnnxStream can consume even 55x less memory than OnnxRuntime with only a 50% to 200% increase in latency (on CPU, with a good SSD, with reference to the SD 1.5's UNET - see the Performance section below).&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/onnxstream?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/onnxstream</guid>
      <pubDate>2023-12-11 20:11</pubDate>
      <category>#ai</category>
      <category>#onnx</category>
    </item>
    <item>
      <title>Evaluating LLMs is a minefield</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/evaluating-llms-minefield?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/evaluating-llms-minefield&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...many things can go wrong when we are trying to evaluate LLMs‚Äô performance on a certain task or behavior in a certain scenario.&lt;/p&gt;
&lt;p&gt;It has big implications for reproducibility: both for research on LLMs and research that uses LLMs to answer a question in social science or any other field.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/evaluating-llms-minefield?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/evaluating-llms-minefield</guid>
      <pubDate>2023-12-11 20:08</pubDate>
      <category>#ai</category>
      <category>#llms</category>
      <category>#evaluation</category>
    </item>
    <item>
      <title>Vector databases: A comparison and guide for 2023</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/vector-db-comparison-guide-2023?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/vector-db-comparison-guide-2023&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Picking a vector database can be hard. Scalability, latency, costs, and even compliance hinge on this choice. For those navigating this terrain, I've embarked on a journey to sieve through the noise and compare the leading vector databases of 2023. I‚Äôve included the following vector databases in the comparision: Pinecone, Weviate, Milvus, Qdrant, Chroma, Elasticsearch and PGvector. The data behind the comparision comes from &lt;a href="https://ann-benchmarks.com/"&gt;ANN Benchmarks&lt;/a&gt;, the docs and internal benchmarks of each vector database and from digging in open source github repos.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/vector-db-comparison-guide-2023?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/vector-db-comparison-guide-2023</guid>
      <pubDate>2023-12-11 20:05</pubDate>
      <category>#vector</category>
      <category>#databases</category>
    </item>
    <item>
      <title>MemGPT - Towards LLMs as Operating Systems</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/memgpt-llm-operating-system?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/memgpt-llm-operating-system&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Teach LLMs to manage their own memory for unbounded context!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In MemGPT, a fixed-context LLM processor is augmented with a tiered memory system and a set of functions that allow it to manage its own memory. Main context is the (fixed-length) LLM input. MemGPT parses the LLM text ouputs at each processing cycle, and either yields control or executes a function call, which can be used to move data between main and external context. When the LLM generates a function call, it can request immediate return of execution to chain together functions. In the case of a yield, the LLM will not be run again until the next external event trigger (e.g. a user message or scheduled interrupt).&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/memgpt-llm-operating-system?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/memgpt-llm-operating-system</guid>
      <pubDate>2023-12-11 19:58</pubDate>
      <category>#ai</category>
      <category>#os</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Best Practices for LLM Evaluation of RAG Applications</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/best-practices-rag-application-evaluation-databricks?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/best-practices-rag-application-evaluation-databricks&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This blog represents the first in a series of investigations we‚Äôre running at Databricks to provide learnings on LLM evaluation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Recently, the LLM community has been exploring the use of ‚ÄúLLMs as a judge‚Äù for automated evaluation with many using powerful LLMs such as GPT-4 to do the evaluation for their LLM outputs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Using the Few Shots prompt with GPT-4 didn‚Äôt make an obvious difference in the consistency of results.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Including few examples for GPT-3.5-turbo-16k significantly improves the consistency of the scores, and makes the result usable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...evaluation results can‚Äôt be transferred between use cases and we need to build use-case-specific benchmarks in order to properly evaluate how good a model can meet customer needs.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/best-practices-rag-application-evaluation-databricks?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/best-practices-rag-application-evaluation-databricks</guid>
      <pubDate>2023-12-11 19:54</pubDate>
      <category>#ai</category>
      <category>#rag</category>
      <category>#evaluation</category>
      <category>#llm</category>
    </item>
    <item>
      <title>MLflow 2.8 with LLM-as-a-judge metrics and Best Practices for LLM Evaluation of RAG Applications</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mlflow-2-8-llm-as-judge-rag-evaluation?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mlflow-2-8-llm-as-judge-rag-evaluation&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;LLM-as-a-judge is one promising tool in the suite of evaluation techniques necessary to measure the efficacy of LLM-based applications.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mlflow-2-8-llm-as-judge-rag-evaluation?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mlflow-2-8-llm-as-judge-rag-evaluation</guid>
      <pubDate>2023-12-11 19:51</pubDate>
      <category>#mlflow</category>
      <category>#ai</category>
      <category>#llm</category>
      <category>#evaluation</category>
    </item>
    <item>
      <title>Metadata-Curated Language-Image Pre-training (MetaCLIP) - Demystifying CLIP Data</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/metaclip-demistifying-clip-data?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/metaclip-demistifying-clip-data&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outperforms CLIP's data on multiple standard benchmarks. In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy, surpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintaining the same training budget, attains 72.4%. Our observations hold across various model sizes, exemplified by ViT-H achieving 80.5%, without any bells-and-whistles. Curation code and training data distribution on metadata is made available at this &lt;a href="https://github.com/facebookresearch/MetaCLIP"&gt;https URL&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Repository&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/facebookresearch/MetaCLIP"&gt;MetaCLIP&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/metaclip-demistifying-clip-data?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/metaclip-demistifying-clip-data</guid>
      <pubDate>2023-12-11 19:41</pubDate>
      <category>#ai</category>
      <category>#opensource</category>
      <category>#clip</category>
    </item>
    <item>
      <title>OpenAgents: An Open Platform for Language Agents in the Wild</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openagents-platform-language-agents?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openagents-platform-language-agents&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs). Current language agent frameworks aim to facilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level designs. We present OpenAgents, an open platform for using and hosting language agents in the wild of everyday life. OpenAgents includes three agents: (1) Data Agent for data analysis with Python/SQL and data tools; (2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web browsing. OpenAgents enables general users to interact with agent functionalities through a web user interface optimized for swift responses and common failures while offering developers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate the challenges and opportunities, aspiring to set a foundation for future research and development of real-world language agents.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openagents-platform-language-agents?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openagents-platform-language-agents</guid>
      <pubDate>2023-12-11 19:39</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#agents</category>
    </item>
    <item>
      <title>The Foundation Model Transparency Index</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/foundational-model-transparency-index-stanford?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/foundational-model-transparency-index-stanford&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A comprehensive assessment of the transparency of foundation model developers&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;. Foundation models like GPT-4 and Llama 2 are used by millions of people. While the societal impact of these models is rising, transparency is on the decline. If this trend continues, foundation models could become just as opaque as social media platforms and other previous technologies, replicating their failure modes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Design&lt;/strong&gt;. We introduce the Foundation Model Transparency Index to assess the transparency of foundation model developers. We design the Index around 100 transparency indicators, which codify transparency for foundation models, the resources required to build them, and their use in the AI supply chain.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;Execution&lt;/strong&gt;. For the 2023 Index, we score 10 leading developers against our 100 indicators. This provides a snapshot of transparency across the AI ecosystem. All developers have significant room for improvement that we will aim to track in the future versions of the Index.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/foundational-model-transparency-index-stanford?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/foundational-model-transparency-index-stanford</guid>
      <pubDate>2023-12-11 19:37</pubDate>
      <category>#ai</category>
      <category>#opensource</category>
      <category>#transparency</category>
    </item>
    <item>
      <title>The New Kings of Open Source AI (Oct 2023 Recap)</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/new-kings-open-source-ai?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/new-kings-open-source-ai&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mistral 7B, released at the tail end of Sept 2023, is both Apache 2.0 and smaller but better than Llama 2, and is now rumored to be raising $400m at $2.5b valuation from a16z.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/new-kings-open-source-ai?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/new-kings-open-source-ai</guid>
      <pubDate>2023-12-11 19:23</pubDate>
      <category>#ai</category>
      <category>#opensource</category>
      <category>#newsletter</category>
    </item>
    <item>
      <title>Mixtral of experts</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mixtral-of-experts?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mixtral-of-experts&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.&lt;/p&gt;
&lt;p&gt;Mixtral has the following capabilities.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It gracefully handles a context of 32k tokens.&lt;/li&gt;
&lt;li&gt;It handles English, French, Italian, German and Spanish.&lt;/li&gt;
&lt;li&gt;It shows strong performance in code generation.&lt;/li&gt;
&lt;li&gt;It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the ‚Äúexperts‚Äù) to process the token and combine their output additively.&lt;/p&gt;
&lt;p&gt;This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mixtral-of-experts?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mixtral-of-experts</guid>
      <pubDate>2023-12-11 18:50</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>The Future of RSS is Textcasting</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/future-rss-textcasting?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/future-rss-textcasting&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Here‚Äôs the philosophy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The goal is interop between social media apps and the features writers need.&lt;/li&gt;
&lt;li&gt;What we‚Äôre doing: Moving documents between networked apps. We need a set of common features in order for it to work.&lt;/li&gt;
&lt;li&gt;The features are motivated by the needs of writers. Not by programmers or social media company execs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It‚Äôs a proposal to build, using technologies we already have and understand very well, a very simple social media protocol that is completely agnostic about what editor you use to write your posts and what viewer you choose to read it. Writer/authors would have more control over styling, links, media enclosures, etc., and readers would have more control over how and where they consume it. It‚Äôs decentralized social media, but without the need to peer through ActivityPub or anybody else‚Äôs API and squeeze our toothpaste through its tubes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Additional resources&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://textcasting.org/"&gt;Textcasting.org&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/future-rss-textcasting?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/future-rss-textcasting</guid>
      <pubDate>2023-12-11 18:40</pubDate>
      <category>#rss</category>
      <category>#openweb</category>
      <category>#internet</category>
      <category>#socialmedia</category>
    </item>
    <item>
      <title>omg.lol: an oasis on the internet</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/omg-lol-internet-oasis-watson?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/omg-lol-internet-oasis-watson&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The main thing you are getting with omg.lol is one or more subdomains, which are referred to as addresses.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Email forwarding&lt;/strong&gt;: You get an email address, you@omg.lol, which you can forward to any email address.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Web Page&lt;/strong&gt;: This is your link-in-bio one-pager to do whatever you want with. By default this is where your main address (eg, you.omg.lol) points. It‚Äôs the flagship feature of omg.lol. It comes with a markdown editor that has some fancy features baked into it. You get a selection of built-in themes but you also have the freedom to go wild with your own CSS.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DNS&lt;/strong&gt;: You have the ability to use your omg.lol subdomain however you wish by way of a friendly DNS panel.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Now Page&lt;/strong&gt;: This is a type of page you can use to let people know what‚Äôs going on in your life. It‚Äôs broader than a social media post but more immediately relevant than an about page. It comes with the same fancy markdown editor and you can optionally appear in omg.lol‚Äôs Now Garden.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Statuslog&lt;/strong&gt;: This is a place to post statuses. It‚Äôs really just a fun, silly alternative to other social media platforms but without follows and likes and such. These can cross-post to Mastodon if you want.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Weblog&lt;/strong&gt;: A full-fledge blogging platform. I‚Äôm not aware of all its features but it‚Äôs pretty powerful. It comes with fancy markdown support and has all the bloggy things you need like tags and RSS. A good example of a very custom blog on omg.lol is Apple Annie‚Äôs Weblog. But it‚Äôs worth noting you use it right out of the box without design customization if you want.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pastebin&lt;/strong&gt;: It‚Äôs just a pastebin for storing text snippets. Super simple and friendly like all of the omg.lol services.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pics&lt;/strong&gt;: It‚Äôs an image hosting service labeled as being ‚Äúsuper-beta‚Äù as of the time of this writing. But it does what it says on the tin. You can host images there and they also show up on the some.pics image feed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PURLs&lt;/strong&gt;: Persistent uniform resource locators. This is a URL redirection service. You get you.omg.lol/whatever and you.url.lol/whatever. You can use these the way you would use similar services and they come with a basic hit counter and way to preview the URL before following it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Switchboard&lt;/strong&gt;: This is a powerful routing system that lets you point the variants of your address wherever you want, be it a destination on the omg.lol platform or an external website. Most omg.lol services have their own domain so you end up with a variety of options. Just as an example, you get a tilde address (ie, omg.lol/~you). Mine points to my tilde.club webpage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keys&lt;/strong&gt;: A place to store public keys‚ÄîSSH, PGP, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proofs&lt;/strong&gt;: A service for verifying ownership or control of a particular web property at a particular moment in time. For example, here is proof that I controlled blakewatson.com as of December 10, 2023.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;API access&lt;/strong&gt;: Most, if not all, omg.lol services have an API you can use to interact with them. Total nerd freedom. ü§Ø&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/omg-lol-internet-oasis-watson?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/omg-lol-internet-oasis-watson</guid>
      <pubDate>2023-12-11 08:51</pubDate>
      <category>#web</category>
      <category>#community</category>
    </item>
    <item>
      <title>Amazing Newsletters</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/amazing-newsletters?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/amazing-newsletters&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Find the best newsletters to subscribe to!&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/amazing-newsletters?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/amazing-newsletters</guid>
      <pubDate>2023-12-10 10:29</pubDate>
      <category>#newsletters</category>
    </item>
    <item>
      <title>To Everyone Who Asks For ‚ÄòJust A Little‚Äô Of Your Time: Here‚Äôs What It Costs To Say Yes</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/what-it-cost-to-say-yes-holiday?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/what-it-cost-to-say-yes-holiday&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Makers...need to have large blocks of uninterrupted, unscheduled time to do what they do. To create and think.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I keep a maker‚Äôs schedule because I believe that anything else is anathema to deep work or creativity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Seneca writes that if all the geniuses in history were to get together, none would be able explain our baffling relationship with time. He says,&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;No person would give up even an inch of their estate, and the slightest dispute with a neighbor can mean hell to pay; yet we easily let others encroach on our lives‚Äîworse, we often pave the way for those who will take it over. No person hands out their money to passers-by, but to how many do each of us hand out our lives! We‚Äôre tight-fisted with property and money, yet think too little of wasting time, the one thing about which we should all be the toughest misers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Time? Time is our most irreplaceable asset‚Äîwe cannot buy more of it. We cannot get a second of it back. We can only hope to waste as little as possible. Yet somehow we treat it as most renewable of all resources.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/what-it-cost-to-say-yes-holiday?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/what-it-cost-to-say-yes-holiday</guid>
      <pubDate>2023-12-09 10:08</pubDate>
      <category>#productivity</category>
      <category>#timemanagement</category>
    </item>
    <item>
      <title>Practical Ways To Increase Product Velocity</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/practical-ways-increase-product-velocity?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/practical-ways-increase-product-velocity&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h2&gt;Remove Dependencies&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Create a culture that favors begging forgiveness (and reversing decisions quickly) rather than asking permission. Invest in infrastructure such as progressive / cancellable rollouts. Use asynchronous written docs to get people aligned (‚Äúcomment in this doc by Friday if you disagree with the plan‚Äù) rather than meetings (‚Äúwe‚Äôll get approval at the next weekly review meeting‚Äù).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Demand Clear Narratives&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Unclear thinking is a reliable cause of slowness, and gets revealed under a microscope.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Bonus points for documenting plans in writing. One of the largest advantages of a strong writing culture is that it forces much clearer narratives than meetings, powerpoint, or five Slack threads spread over 8 business days.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Get Your Deployment and Incident Metrics In Shape&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;No matter what your job function is, part of your role is ensuring that your engineering team has enough time to get their vital metrics in order. Especially if you‚Äôre a product leader, it‚Äôs essential that you resist the temptation to push relentlessly for more features and give your engineering counterparts the room to get fit.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Find Trusted Engineering Guides&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...it‚Äôs especially important to build a strong relationship with all of your engineering partners, and especially these trusted guides.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/practical-ways-increase-product-velocity?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/practical-ways-increase-product-velocity</guid>
      <pubDate>2023-12-09 10:03</pubDate>
      <category>#product</category>
      <category>#productivity</category>
    </item>
    <item>
      <title>SatCLIP - A Global, General-Purpose Geographic Location Encoder</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/microsoft-satclip?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/microsoft-satclip&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;SatCLIP trains location and image encoders via contrastive learning, by matching images to their corresponding locations. This is analogous to the CLIP approach, which matches images to their corresponding text. Through this process, the location encoder learns characteristics of a location, as represented by satellite imagery. For more details, check out our &lt;a href="https://arxiv.org/abs/2311.17179"&gt;paper&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/microsoft-satclip?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/microsoft-satclip</guid>
      <pubDate>2023-12-08 07:40</pubDate>
      <category>#ai</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>DOS_deck</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/dos-deck?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/dos-deck&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;DOS_deck is built upon the foundation of JS-DOS, which, in turn, relies on DOSBox. Together, they breathe new life into MS-DOS games by bringing them to your browser. However, there's a twist. Games from that era were designed for keyboard and mouse input, without established standards for interaction or control patterns. Here at DOS_deck, a tremendous effort was put into creating a seamless experience, enabling you to effortlessly navigate and play these games, ideally with the comfort of a controller in hand.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/dos-deck?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/dos-deck</guid>
      <pubDate>2023-12-08 07:36</pubDate>
      <category>#gaming</category>
      <category>#dos</category>
      <category>#retro</category>
      <category>#web</category>
    </item>
    <item>
      <title>Awesome App Defaults</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/app-defaults-catalog?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/app-defaults-catalog&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Aggregated list of App Defaults blog posts inspired by &lt;a href="https://listen.hemisphericviews.com/097"&gt;Hemispheric Views 097 - Duel of the Defaults!&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/app-defaults-catalog?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/app-defaults-catalog</guid>
      <pubDate>2023-12-06 21:53</pubDate>
      <category>#community</category>
      <category>#blogging</category>
      <category>#apps</category>
    </item>
    <item>
      <title>RSS Club</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/rss-club?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/rss-club&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;RSS Club is a collection of blogs (personal and otherwise) committed to providing RSS-only content. It‚Äôs like a newsletter delivered to your feed reader in order to celebrate the medium of RSS and breakaway from social media.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/rss-club?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/rss-club</guid>
      <pubDate>2023-12-06 21:37</pubDate>
      <category>#rss</category>
      <category>#community</category>
    </item>
    <item>
      <title>Long context prompting for Claude 2.1</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/claude-long-context-prompting?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/claude-long-context-prompting&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;ul&gt;
&lt;li&gt;Claude 2.1 recalls information very well across its 200,000 token context window&lt;/li&gt;
&lt;li&gt;However, the model can be reluctant to answer questions based on an individual sentence in a document, especially if that sentence has been injected or is out of place&lt;/li&gt;
&lt;li&gt;A minor prompting edit removes this reluctance and results in excellent performance on these tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;What can users do if Claude is reluctant to respond to a long context retrieval question? We‚Äôve found that a minor prompt update produces very different outcomes in cases where Claude is capable of giving an answer, but is hesitant to do so. When running the same evaluation internally, adding just one sentence to the prompt resulted in near complete fidelity throughout Claude 2.1‚Äôs 200K context window&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We achieved significantly better results on the same evaluation by adding the sentence ‚ÄúHere is the most relevant sentence in the context:‚Äù to the start of Claude‚Äôs response. This was enough to raise Claude 2.1‚Äôs score from 27% to 98% on the original evaluation.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/claude-long-context-prompting?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/claude-long-context-prompting</guid>
      <pubDate>2023-12-06 21:07</pubDate>
      <category>#prompts</category>
      <category>#ai</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Blogging is where it's at, again</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/blogging-where-its-at-again-mcleod?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/blogging-where-its-at-again-mcleod&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;the blog is the ‚Äúnatural form‚Äù of posting on the web: a site of your own, that you control[1] and set your own rules on content and discussion; where you can post whatever you like without worrying about ‚ÄúThe Algorithm‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;For better or for worse, social media opened up the web to a lot more people for a number of reasons...But deep down I feel having your own site is better. For the web, and for you: the writer and the reader.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...stumbling into such a trove of active blogs has enthused me about blogging as a medium again. It‚Äôs sparked a thought that through a combination of increased blogging activity, declining platforms, and increasing adoption of open standards to glue everything together, that maybe ‚Äî just maybe ‚Äî we can swing the web back towards the blog again.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Agree with many of the points. Also, TIL you could subscribe to OPML feeds.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/blogging-where-its-at-again-mcleod?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/blogging-where-its-at-again-mcleod</guid>
      <pubDate>2023-12-06 21:02</pubDate>
      <category>#blogging</category>
      <category>#socialmedia</category>
      <category>#rss</category>
      <category>#community</category>
    </item>
    <item>
      <title>Introducing Gemini</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-gemini-google?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-gemini-google&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research. It was built from the ground up to be multimodal, which means it can generalize and seamlessly understand, operate across and combine different types of information including text, code, audio, image and video.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Gemini is also our most flexible model yet ‚Äî able to efficiently run on everything from data centers to mobile devices. Its state-of-the-art capabilities will significantly enhance the way developers and enterprise customers build and scale with AI.&lt;/p&gt;
&lt;p&gt;We‚Äôve optimized Gemini 1.0, our first version, for three different sizes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gemini Ultra ‚Äî our largest and most capable model for highly complex tasks.&lt;/li&gt;
&lt;li&gt;Gemini Pro ‚Äî our best model for scaling across a wide range of tasks.&lt;/li&gt;
&lt;li&gt;Gemini Nano ‚Äî our most efficient model for on-device tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We designed Gemini to be natively multimodal, pre-trained from the start on different modalities. Then we fine-tuned it with additional multimodal data to further refine its effectiveness. This helps Gemini seamlessly understand and reason about all kinds of inputs from the ground up, far better than existing multimodal models ‚Äî and its capabilities are state of the art in nearly every domain.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our first version of Gemini can understand, explain and generate high-quality code in the world‚Äôs most popular programming languages, like Python, Java, C++, and Go.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;On TPUs, Gemini runs significantly faster than earlier, smaller and less-capable models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Starting today, Bard will use a fine-tuned version of Gemini Pro for more advanced reasoning, planning, understanding and more.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôre also bringing Gemini to Pixel. Pixel 8 Pro is the first smartphone engineered to run Gemini Nano, which is powering new features like Summarize in the Recorder app and rolling out in Smart Reply in Gboard, starting with WhatsApp ‚Äî with more messaging apps coming next year.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Starting on December 13, developers and enterprise customers can access Gemini Pro via the Gemini API in Google AI Studio or Google Cloud Vertex AI.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-gemini-google?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-gemini-google</guid>
      <pubDate>2023-12-06 20:57</pubDate>
      <category>#ai</category>
      <category>#lmm</category>
    </item>
    <item>
      <title>Introducing llamafile</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introducing-llamafile?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introducing-llamafile&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we‚Äôre announcing the first release of llamafile and inviting the open source community to participate in this new project.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;llamafile lets you turn large language model (LLM) weights into executables.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We achieved all this by combining two projects that we love: llama.cpp (a leading open source LLM chatbot framework) with Cosmopolitan Libc (an open source project that enables C programs to be compiled and run on a large number of platforms and architectures). It also required solving several interesting and juicy problems along the way, such as adding GPU and dlopen() support to Cosmopolitan;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introducing-llamafile?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introducing-llamafile</guid>
      <pubDate>2023-12-05 21:54</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>PeerTube v6 is out</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/peertube-v6-released?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/peertube-v6-released&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The sixth major version is being released today and we are very proud !&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Protect your videos with passwords !&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Video storyboard : preview what‚Äôs coming !&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Upload a new version of your video !&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Get chapters in your videos !&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stress tests, performance and config recommandations&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;‚Ä¶and there‚Äôs always more !&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/peertube-v6-released?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/peertube-v6-released</guid>
      <pubDate>2023-12-05 21:51</pubDate>
      <category>#fediverse</category>
      <category>#peertube</category>
      <category>#video</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>How I Take and Publish Notes</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/take-and-publish-notes-nielsen?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/take-and-publish-notes-nielsen&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;99% of the time, this is how my note-taking process goes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I‚Äôm catching up on my RSS feed (on my phone in the Reeder app)&lt;/li&gt;
&lt;li&gt;I read something that strikes me as interesting, novel, or insightful.&lt;/li&gt;
&lt;li&gt;I copy/paste it as an blockquote into a new, plain-text note in iA writer.&lt;/li&gt;
&lt;li&gt;I copy/paste the link of the article into iA writer.&lt;/li&gt;
&lt;li&gt;I finish reading the article and copy/paste anything else in the article that strikes me.&lt;/li&gt;
&lt;li&gt;I add my own comments in the note as they pop into my head.&lt;/li&gt;
&lt;li&gt;I move on to the next article in my RSS feed.&lt;/li&gt;
&lt;li&gt;Repeat.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Kind of meta but somewhat similar process for me. To publish the different content found on my &lt;a href="/feed/responses/"&gt;response feed&lt;/a&gt;, I:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go through articles on my RSS feed (NewsBlur on both desktop and mobile).&lt;/li&gt;
&lt;li&gt;Copy URL and block quotes from article and paste them somewhere. When I have time like now, I create a post like this one, usually in VS Code. If I don't have time though, I've been experimenting with using a messaging app like Element and E-mail as a read-it-later service. At minimum, I create a message with the link and send it to myself for later review. Later on when I have time, I create the post with additional comments and content from the article.&lt;/li&gt;
&lt;li&gt;(Optional) Add some of my own comments.&lt;/li&gt;
&lt;li&gt;Publish the notes.&lt;/li&gt;
&lt;li&gt;Repeat.&lt;/li&gt;
&lt;/ol&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/take-and-publish-notes-nielsen?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/take-and-publish-notes-nielsen</guid>
      <pubDate>2023-12-05 21:42</pubDate>
      <category>#blogging</category>
      <category>#notetaking</category>
    </item>
    <item>
      <title>AI and Mass Spying</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ai-mass-spying-schneier?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ai-mass-spying-schneier&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Surveillance facilitates social control, and spying will only make this worse. Governments around the world already use mass surveillance; they will engage in mass spying as well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mass surveillance ushered in the era of personalized advertisements; mass spying will supercharge that industry...The tech monopolies that are currently keeping us all under constant surveillance won‚Äôt be able to resist collecting and using all of that data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We could limit this capability. We could prohibit mass spying. We could pass strong data-privacy rules. But we haven‚Äôt done anything to limit mass surveillance. Why would spying be any different?&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ai-mass-spying-schneier?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ai-mass-spying-schneier</guid>
      <pubDate>2023-12-05 21:12</pubDate>
      <category>#ai</category>
      <category>#privacy</category>
      <category>#internet</category>
      <category>#surveillance</category>
    </item>
    <item>
      <title>AI Alliance Launches</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ai-alliance?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ai-alliance&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;AI Alliance Launches as an International Community of Leading Technology Developers, Researchers, and Adopters Collaborating Together to Advance Open, Safe, Responsible AI&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ai-alliance?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ai-alliance</guid>
      <pubDate>2023-12-05 21:03</pubDate>
      <category>#ai</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Where this flower blooms</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tyler-ted-talk?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tyler-ted-talk&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Preach &lt;span&gt;üôå&lt;/span&gt;&lt;/p&gt;
&lt;figure class="figure"&gt;
&lt;p&gt;&lt;a href="https://twitter.com/studiosipu/status/1723582194432794638/video/1" title="Tyler The Creator Speech at Camp FlagGnow"&gt;&lt;img src="https://pbs.twimg.com/ext_tw_video_thumb/1723582065843769344/pu/img/JSrlq95E1Hr5RzY_.jpg" class="img-fluid" alt="Tyler The Creator Speech at Camp FlagGnow" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;&lt;a href="https://twitter.com/studiosipu/status/1723582194432794638"&gt;@studiosipu on X&lt;/a&gt;&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tyler-ted-talk?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tyler-ted-talk</guid>
      <pubDate>2023-11-12 11:53</pubDate>
      <category>#tedtalk</category>
      <category>#festival</category>
    </item>
    <item>
      <title>You Need Feeds</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/you-need-feeds?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/you-need-feeds&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;YOU NEED FEEDS.&lt;/p&gt;
&lt;p&gt;A web feed is a special listing of the latest content from your favourite site.  News, music, video and more - whatever is new, web feeds will show you.  What's more, you can combine your favourite feeds using a feed reader application - and suddenly the whole web comes to you.&lt;/p&gt;
&lt;p&gt;You don't have to do the work of staying on top any more.  You can now visit a single site, or use a single app, and see everything that's new and interesting.  You choose the content.  You're in control.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/you-need-feeds?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/you-need-feeds</guid>
      <pubDate>2023-11-11 17:49</pubDate>
      <category>#rss</category>
      <category>#feeds</category>
      <category>#guides</category>
    </item>
    <item>
      <title>Texts Joins Automattic</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/texts-joins-automattic?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/texts-joins-automattic&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Using an all-in-one messaging app is a real game-changer for productivity and keeping up with things.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This is obviously a tricky area to navigate, as in the past the networks have blocked third-party clients, but I think with the current anti-trust and regulatory environments this is actually something the big networks will appreciate: it maintains the same security as their clients, opens them up in a way consumers will love and is very user-centric, and because we‚Äôre committed to supporting all their features it can actually increase engagement and usage of their platforms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I can relate to the feeling of wanting to have one inbox expressed in the video. Coincidentally, I've been playing with &lt;a href="https://delta.chat/"&gt;Delta Chat&lt;/a&gt; and by building on top of e-mail, some of the issues with the siloed platforms are alleviated. Also, &lt;a href="https://www.emailisnotdead.com/"&gt;e-mail isn't dead&lt;/a&gt; and despite some of its shortcomings, it's still broadly used to sign up and sign into platforms.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/texts-joins-automattic?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/texts-joins-automattic</guid>
      <pubDate>2023-10-24 17:13</pubDate>
      <category>#messaging</category>
      <category>#interop</category>
      <category>#decentralization</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Automattic is acquiring Texts and betting big on the future of messaging</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/automattic-acquires-texts-messaging?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/automattic-acquires-texts-messaging&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I'm really liking the recent acquisitions from Automattic. I'm just starting to use Day One and really enjoy it. Pocket Casts is a fantastic podcast app, though I prefer to use AntennaPod. WordPress is also starting to make it easy to plug into the Fediverse using your blog. I'm excited for Texts and what that might offer in the current siloed messaging landscape.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Automattic, the company that runs WordPress.com, Tumblr, Pocket Casts, and a number of other popular web properties, just made a different kind of acquisition: it‚Äôs buying Texts, a universal messaging app, for $50 million.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Texts is an app for all your messaging apps. You can use it to log in to WhatsApp, Instagram, LinkedIn, Signal, iMessage, and more and see and respond to all your messages in one place.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...Mullenweg says he‚Äôs bullish on solutions like Matrix, which offers a decentralized and open-source messaging network, and other up-and-coming standards for messaging. He‚Äôs already thinking about how Texts might gently nudge people toward more open protocols over time.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mullenweg and Automattic see a big future for messaging, as more online interaction shifts away from public-first social networks and toward things like group chats. Hardly anyone has figured out how to build a meaningful and sustainable business from chat, but Mullenweg thinks it‚Äôs possible. And he thinks it starts with making your messaging a little less messy.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/automattic-acquires-texts-messaging?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/automattic-acquires-texts-messaging</guid>
      <pubDate>2023-10-24 09:16</pubDate>
      <category>#messaging</category>
      <category>#interop</category>
      <category>#decentralization</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>POSSE: A Better Way to Post on Social Networks</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/posse-better-way-to-post-social-networks?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/posse-better-way-to-post-social-networks&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The platform era is ending. Rather than build new Twitters and Facebooks, we can create a stuff-posting system that works better for everybody.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In a POSSE world, everybody owns a domain name, and everybody has a blog. (I‚Äôm defining ‚Äúblog‚Äù pretty loosely here ‚Äî just as a place on the internet where you post your stuff and others consume it.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;But there are some big challenges to the idea...The most immediate question...is simply how to build a POSSE system that works. POSSE‚Äôs problems start at the very beginning: it requires owning your own website, which means buying a domain and worrying about DNS records and figuring out web hosts, and by now, you‚Äôve already lost the vast majority of people who would rather just type a username and password into some free Meta platform...Even those willing and able to do the technical work can struggle to make POSSE work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;When I ask Doctorow why he believed in POSSE, he describes the tension every poster feels on the modern internet. ‚ÄúI wanted to find a way to stand up a new platform in this moment,‚Äù he says, ‚Äúwhere, with few exceptions, everyone gets their news and does their reading through the silos that then hold you to ransom. And I wanted to use those silos to bring in readers and to attract and engage with an audience, but I didn‚Äôt want to become beholden to them.‚Äù &lt;strong&gt;The best of both worlds is currently a lot of work. But the poster‚Äôs paradise might not be so far away.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/posse-better-way-to-post-social-networks?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/posse-better-way-to-post-social-networks</guid>
      <pubDate>2023-10-23 18:55</pubDate>
      <category>#indieweb</category>
      <category>#posse</category>
      <category>#socialnetworks</category>
      <category>#internet</category>
      <category>#community</category>
    </item>
    <item>
      <title>Understanding Deep Learning</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/understanding-deep-learning-book?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/understanding-deep-learning-book&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The title of this book is ‚ÄúUnderstanding Deep Learning‚Äù to distinguish it from volumes that cover coding and other practical aspects. This text is primarily about the ideas that underlie deep learning. The first part of the book introduces deep learning models and discusses how to train them, measure their performance, and improve this performance. The next part considers architectures that are specialized to images, text, and graph data. These chapters require only introductory linear algebra, calculus, and probability and should be accessible to any second-year undergraduate in a quantitative discipline. Subsequent parts of the book tackle generative models and reinforcement learning. These chapters require more knowledge of probability and calculus and target more advanced students.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/understanding-deep-learning-book?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/understanding-deep-learning-book</guid>
      <pubDate>2023-10-14 21:35</pubDate>
      <category>#book</category>
      <category>#deeplearning</category>
      <category>#ai</category>
    </item>
    <item>
      <title>The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/dawn-lmms-early-explorations-gtp4v?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/dawn-lmms-early-explorations-gtp4v&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI's innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution: &lt;a href="https://cdn.openai.com/contributions/gpt-4v.pdf"&gt;this https URL&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/dawn-lmms-early-explorations-gtp4v?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/dawn-lmms-early-explorations-gtp4v</guid>
      <pubDate>2023-10-14 21:32</pubDate>
      <category>#ai</category>
      <category>#lmm</category>
    </item>
    <item>
      <title>Chain-of-Verification Reduces Hallucination in Large Language Models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/chain-of-verification-prompting-technique?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/chain-of-verification-prompting-technique&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (COVE) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show COVE decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/chain-of-verification-prompting-technique?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/chain-of-verification-prompting-technique</guid>
      <pubDate>2023-10-14 21:09</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#promptengineering</category>
    </item>
    <item>
      <title>Engage a Wider Audience With ActivityPub on WordPress.com</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/activitypub-plugin-available-hosted-wordpress?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/activitypub-plugin-available-hosted-wordpress&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Exciting times are here for all WordPress.com users! The revolutionary ActivityPub feature is now available across all WordPress.com plans, unlocking a world of engagement and interaction for your blog. Your blogs can now be part of the rapidly expanding fediverse, which enables you to connect with a broader audience and attract more followers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I can't believe I missed these news but so exciting!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/activitypub-plugin-available-hosted-wordpress?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/activitypub-plugin-available-hosted-wordpress</guid>
      <pubDate>2023-10-14 20:57</pubDate>
      <category>#activitypub</category>
      <category>#mastodon</category>
      <category>#wordpress</category>
      <category>#blogging</category>
    </item>
    <item>
      <title>Multimodality and Large Multimodal Models (LMMs)</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/multimodality-large-multimodal-models-lmm-huyen?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/multimodality-large-multimodal-models-lmm-huyen&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This post covers multimodal systems in general, including LMMs. It consists of 3 parts.&lt;/p&gt;
&lt;p&gt;Part 1 covers the context for multimodality, including why multimodal, different data modalities, and types of multimodal tasks.&lt;br /&gt;
Part 2 discusses the fundamentals of a multimodal system, using the examples of CLIP, which lays the foundation for many future multimodal systems, and Flamingo, whose impressive performance gave rise to LMMs.&lt;br /&gt;
Part 3 discusses some active research areas for LMMs, including generating multimodal outputs and adapters for more efficient multimodal training, covering newer multimodal systems such as BLIP-2, LLaVA, LLaMA-Adapter V2, LAVIN, etc.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/multimodality-large-multimodal-models-lmm-huyen?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/multimodality-large-multimodal-models-lmm-huyen</guid>
      <pubDate>2023-10-14 18:51</pubDate>
      <category>#ai</category>
      <category>#lmm</category>
    </item>
    <item>
      <title>HuggingFace: Text Embeddings Inference</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/hf-text-embedding-inference?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/hf-text-embedding-inference&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;A blazing fast inference solution for text embeddings models.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/hf-text-embedding-inference?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/hf-text-embedding-inference</guid>
      <pubDate>2023-10-14 12:54</pubDate>
      <category>#ai</category>
      <category>#embeddings</category>
      <category>#text</category>
      <category>#inference</category>
      <category>#ml</category>
    </item>
    <item>
      <title>Radiooooo, The Musical Time Machine</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/radiooooo?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/radiooooo&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Radiooooo is a project born in 2013, dreamt up by a little family of friends, both djs and music lovers, who decided to share their record collections and the fruit of many years of research, for all to enjoy.&lt;/p&gt;
&lt;p&gt;¬´ Sharing and discovering ¬ª , ¬´ curiosity and pleasure ¬ª these are the foundations of this musical time machine.&lt;/p&gt;
&lt;p&gt;Radiooooo is a collaborative website, whose goal is to open each and everyone‚Äôs horizons through culture and beauty.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/radiooooo?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/radiooooo</guid>
      <pubDate>2023-10-06 09:33</pubDate>
      <category>#music</category>
      <category>#radio</category>
    </item>
    <item>
      <title>Scaffolded LLMs as natural language computers</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/scaffolded-llms-natural-language-computers?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/scaffolded-llms-natural-language-computers&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43C3igfmMrE9Qoyfe/paczewfk3yzexgdaokto" class="img-fluid" alt="ReACT LLM Pattern Image" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;beren.io&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43C3igfmMrE9Qoyfe/sehstj9d7rfgcv6ifg2y" class="img-fluid" alt="Image of high-level CPU architecture" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;beren.io&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/scaffolded-llms-natural-language-computers?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/scaffolded-llms-natural-language-computers</guid>
      <pubDate>2023-09-28 21:00</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#cpu</category>
    </item>
    <item>
      <title>Creating the First Confidential GPUs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/creating-first-confidential-gpus?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/creating-first-confidential-gpus&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The team at NVIDIA brings confidentiality and integrity to user code and data for accelerated computing.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/creating-first-confidential-gpus?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/creating-first-confidential-gpus</guid>
      <pubDate>2023-09-28 20:59</pubDate>
      <category>#ai</category>
      <category>#security</category>
      <category>#privacy</category>
      <category>#gpu</category>
    </item>
    <item>
      <title>The AI Attack Surface Map v1.0</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ai-attack-surface-map-v1?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ai-attack-surface-map-v1&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/29651bf8-3dd7-4c94-90e4-0bab3398db60/ai-attack-surface-categories-miessler-1.png" class="img-fluid" alt="AI Surface Attack Map" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;danielmiessler.com&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ai-attack-surface-map-v1?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ai-attack-surface-map-v1</guid>
      <pubDate>2023-09-28 20:45</pubDate>
      <category>#ai</category>
      <category>#security</category>
      <category>#cybersecurity</category>
    </item>
    <item>
      <title>Mistral 7B Model</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mistral-ai?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mistral-ai&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Mistral-7B-v0.1 is a small, yet powerful model adaptable to many use-cases. Mistral 7B is better than Llama 2 13B on all benchmarks, has natural coding abilities, and 8k sequence length. It‚Äôs released under Apache 2.0 licence. We made it easy to deploy on any cloud, and of course on your gaming GPU.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mistral-ai?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mistral-ai</guid>
      <pubDate>2023-09-28 10:16</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Meta announces AI experiences in Facebook, Instagram, WhatsApp</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/meta-announces-ai-experiences-fb-whatsapp-ig?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/meta-announces-ai-experiences-fb-whatsapp-ig&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;ul&gt;
&lt;li&gt;We‚Äôre starting to roll out AI stickers across our apps, and soon you‚Äôll be able to edit your images or even co-create them with friends on Instagram using our new AI editing tools, restyle and backdrop.&lt;/li&gt;
&lt;li&gt;We‚Äôre introducing Meta AI in beta, an advanced conversational assistant that‚Äôs available on WhatsApp, Messenger, and Instagram, and is coming to Ray-Ban Meta smart glasses and Quest 3. Meta AI can give you real-time information and generate photorealistic images from your text prompts in seconds to share with friends. (Available in the US only)&lt;/li&gt;
&lt;li&gt;We‚Äôre also launching 28 more AIs in beta, with unique interests and personalities. Some are played by cultural icons and influencers, including Snoop Dogg, Tom Brady, Kendall Jenner, and Naomi Osaka.&lt;/li&gt;
&lt;li&gt;Over time, we‚Äôre making AIs for businesses and creators available, and releasing our AI studio for people and developers to build their own AIs.&lt;/li&gt;
&lt;li&gt;These new AI experiences also come with a new set of challenges for our industry. We‚Äôre rolling out our new AIs slowly and have built in safeguards.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/meta-announces-ai-experiences-fb-whatsapp-ig?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/meta-announces-ai-experiences-fb-whatsapp-ig</guid>
      <pubDate>2023-09-28 10:12</pubDate>
      <category>#meta</category>
      <category>#ai</category>
      <category>#facebook</category>
    </item>
    <item>
      <title>Introducing Raspberry Pi 5</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/raspberry-pi-5?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/raspberry-pi-5&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The everything computer. Optimised.&lt;/p&gt;
&lt;p&gt;With 2‚Äì3√ó the speed of the previous generation, and featuring silicon designed in‚Äëhouse for the best possible performance, we‚Äôve redefined the Raspberry Pi experience.&lt;/p&gt;
&lt;p&gt;Coming October 2023&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/raspberry-pi-5?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/raspberry-pi-5</guid>
      <pubDate>2023-09-28 10:10</pubDate>
      <category>#raspberrypi</category>
      <category>#selfhosting</category>
    </item>
    <item>
      <title>Carton - Run any ML model from any programming language</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/carton-ml-models?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/carton-ml-models&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Carton makes it easy to run any ML model from any programming language.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/carton-ml-models?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/carton-ml-models</guid>
      <pubDate>2023-09-28 10:03</pubDate>
      <category>#ai</category>
      <category>#ml</category>
      <category>#mlops</category>
    </item>
    <item>
      <title>BOOX Palma</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/boox-palma-eink-phone?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/boox-palma-eink-phone&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I really like my Boox e-reader but having a more pocketable device would be amazing. It's unforunate you can't also use it for handwritten notes but at this size it makes sense.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=K_n5tYz6DfM" title="Boox Palma Promo Video"&gt;&lt;img src="http://img.youtube.com/vi/K_n5tYz6DfM/0.jpg" class="img-fluid" alt="Boox Palma Promo Video" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/boox-palma-eink-phone?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/boox-palma-eink-phone</guid>
      <pubDate>2023-09-26 21:22</pubDate>
      <category>#eink</category>
      <category>#ereader</category>
      <category>#mp3</category>
      <category>#minimalism</category>
      <category>#simplicity</category>
    </item>
    <item>
      <title>FlowiseAI - Drag and drop for LLM flows</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/flowiseai-visual-llm-flow?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/flowiseai-visual-llm-flow&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;img src="https://github.com/FlowiseAI/Flowise/raw/main/images/flowise.gif" class="img-fluid" alt="GIF of FlowiseAI LLM visual tool" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/flowiseai-visual-llm-flow?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/flowiseai-visual-llm-flow</guid>
      <pubDate>2023-09-25 20:54</pubDate>
      <category>#ai</category>
      <category>#tools</category>
    </item>
    <item>
      <title>Why Open Source AI Will Win</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/why-open-source-ai-will-win-varun?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/why-open-source-ai-will-win-varun&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/why-open-source-ai-will-win-varun?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/why-open-source-ai-will-win-varun</guid>
      <pubDate>2023-09-25 20:48</pubDate>
      <category>#opensource</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Distilling step-by-step: Outperforming larger language models with less training data and smaller model sizes</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/distilling-step-x-step-outperforming-large-models-small-data?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/distilling-step-x-step-outperforming-large-models-small-data&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In &lt;a href="https://arxiv.org/abs/2305.02301"&gt;‚ÄúDistilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes‚Äù&lt;/a&gt;, presented at ACL2023, we set out to tackle this trade-off between model size and training data collection cost. We introduce distilling step-by-step, a new simple mechanism that allows us to train smaller task-specific models with much less training data than required by standard fine-tuning or distillation approaches that outperform few-shot prompted LLMs‚Äô performance. We demonstrate that the distilling step-by-step mechanism enables a 770M parameter T5 model to outperform the few-shot prompted 540B PaLM model using only 80% of examples in a benchmark dataset, which demonstrates a more than 700x model size reduction with much less training data required by standard approaches.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/distilling-step-x-step-outperforming-large-models-small-data?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/distilling-step-x-step-outperforming-large-models-small-data</guid>
      <pubDate>2023-09-25 20:45</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#slm</category>
      <category>#optimization</category>
    </item>
    <item>
      <title>How I rewired my brain in six weeks</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/rewired-brain-six-weeks-bbc?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/rewired-brain-six-weeks-bbc&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;There is growing evidence that simple, everyday changes to our lives can alter our brains and change how they work. Melissa Hogenboom put herself into a scanner to find out.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I was surprised that something as simple as mindfulness can play such a crucial role in keeping our minds healthy. Research has shown that mindfulness is a simple but powerful way to enhance several cognitive functions. It can improve attention, relieve pain and reduce stress. Research has found that after only a few months of mindfulness training, certain depression and anxiety symptoms can ease ‚Äì though as with any complex mental health problem, this may of course vary depending on individual circumstances.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/rewired-brain-six-weeks-bbc?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/rewired-brain-six-weeks-bbc</guid>
      <pubDate>2023-09-25 20:42</pubDate>
      <category>#neuroscience</category>
      <category>#habits</category>
      <category>#mindfulness</category>
      <category>#cognition</category>
    </item>
    <item>
      <title>vim + llm = üî•</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/vim-llm-hacks-thacker?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/vim-llm-hacks-thacker&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;... I learned that you could be inside vim, but manipulate the entire file as if you were piping the contents of the file into a command. The output of the command does in-line replacement of the entire file with those changes. That sounds confusing, but it just means you can be inside a vim file and do :%!grep test and it‚Äôll remove all lines that don‚Äôt contain test, for example.&lt;/p&gt;
&lt;p&gt;This post is a simple showcase of taking that concept, but throwing an llm into the mix to add more dynamic functionality.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/vim-llm-hacks-thacker?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/vim-llm-hacks-thacker</guid>
      <pubDate>2023-09-25 20:39</pubDate>
      <category>#vi</category>
      <category>#linux</category>
      <category>#llm</category>
      <category>#ai</category>
      <category>#softwaredevelopment</category>
    </item>
    <item>
      <title>Next-Gen CPU Acceleration: AVX For Generative AI</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/next-gen-cpu-acceleration-avx-gen-ai?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/next-gen-cpu-acceleration-avx-gen-ai&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The future is AVX10, so says Intel. Recently a document was released showcasing a post-AVX512 world, and to explain why this matters, I've again invited the Chips And Cheese crew onto the channel. Chester and George answer my questions on AVX10 and why it matters! Visit &lt;a href="http://www.chipsandcheese.com"&gt;http://www.chipsandcheese.com&lt;/a&gt; to learn more!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=bskEGP0r3hE" title="YouTube video Next-Gen CPU Acceleration: AVX For Generative AI"&gt;&lt;img src="http://img.youtube.com/vi/bskEGP0r3hE/0.jpg" class="img-fluid" alt="YouTube video Next-Gen CPU Acceleration: AVX For Generative AI" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/next-gen-cpu-acceleration-avx-gen-ai?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/next-gen-cpu-acceleration-avx-gen-ai</guid>
      <pubDate>2023-09-25 20:34</pubDate>
      <category>#ai</category>
      <category>#hardware</category>
      <category>#hpc</category>
    </item>
    <item>
      <title>PSA: Fat Bear Week Oct 4-10 2023</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/fat-bear-week-2023?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/fat-bear-week-2023&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Fat Bear Week - an annual celebration of success. All bears are winners but only one true champion will emerge. Held over the course of seven days and concluding on the Fat Bear Tuesday, people chose which bear to crown in this tournament style bracket where bears are pitted against each other for your vote.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/fat-bear-week-2023?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/fat-bear-week-2023</guid>
      <pubDate>2023-09-25 20:31</pubDate>
      <category>#nature</category>
      <category>#bears</category>
      <category>#nps</category>
    </item>
    <item>
      <title>ChatGPT can now see, hear, and speak</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/chatgpt-voice-image-capabilities?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/chatgpt-voice-image-capabilities&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are beginning to roll out new voice and image capabilities in ChatGPT. They offer a new, more intuitive type of interface by allowing you to have a voice conversation or show ChatGPT what you‚Äôre talking about.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/chatgpt-voice-image-capabilities?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/chatgpt-voice-image-capabilities</guid>
      <pubDate>2023-09-25 10:12</pubDate>
      <category>#ai</category>
      <category>#chatgpt</category>
      <category>#openai</category>
      <category>#llm</category>
      <category>#machinelearning</category>
      <category>#ml</category>
    </item>
    <item>
      <title>Amazon and Anthropic announce strategic collaboration to advance generative AI </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/aws-anthropic-partnership?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/aws-anthropic-partnership&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Anthropic selects AWS as its primary cloud provider and will train and deploy its future foundation models on AWS Trainium and Inferentia chips, taking advantage of AWS‚Äôs high-performance, low-cost machine learning accelerators.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/aws-anthropic-partnership?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/aws-anthropic-partnership</guid>
      <pubDate>2023-09-25 10:11</pubDate>
      <category>#ai</category>
      <category>#cloud</category>
    </item>
    <item>
      <title>Tracy Durnell 20th anniversary of blogging</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tracy-durnell-20-blogging-anniversary?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tracy-durnell-20-blogging-anniversary&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Happy 20th anniversary! Also, thanks for the generous linking. Lots of new folks to read and subscribe to.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tracy-durnell-20-blogging-anniversary?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tracy-durnell-20-blogging-anniversary</guid>
      <pubDate>2023-09-23 21:38</pubDate>
      <category>#blogs</category>
      <category>#blogging</category>
      <category>#indieweb</category>
    </item>
    <item>
      <title>DALL¬∑E 3</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/dall-e-3?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/dall-e-3&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;DALL¬∑E 3 understands significantly more nuance and detail than our previous systems, allowing you to easily translate your ideas into exceptionally accurate images.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/dall-e-3?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/dall-e-3</guid>
      <pubDate>2023-09-22 10:18</pubDate>
      <category>#dalle</category>
      <category>#ai</category>
      <category>#openai</category>
      <category>#images</category>
      <category>#llm</category>
      <category>#ml</category>
    </item>
    <item>
      <title>Matrix 2.0: The Future of Matrix</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/matrix-2-0?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/matrix-2-0&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;TL;DR: If you want to play with a shiny new Matrix 2.0 client, head over to &lt;a href="https://element.io/blog/element-x-ignition/"&gt;Element X&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/matrix-2-0?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/matrix-2-0</guid>
      <pubDate>2023-09-22 10:10</pubDate>
      <category>#matrix</category>
      <category>#messaging</category>
      <category>#communication</category>
      <category>#opensource</category>
      <category>#protocols</category>
      <category>#decentralization</category>
      <category>#openweb</category>
      <category>#web</category>
      <category>#internet</category>
    </item>
    <item>
      <title>The Atlantic‚Äôs Guide to Privacy</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/atlantic-privacy-guide-2023?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/atlantic-privacy-guide-2023&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Sponsored post, but it's still a good list with guidance and suggestions for common questions.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/atlantic-privacy-guide-2023?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/atlantic-privacy-guide-2023</guid>
      <pubDate>2023-09-17 13:03</pubDate>
      <category>#privacy</category>
      <category>#security</category>
      <category>#internet</category>
    </item>
    <item>
      <title>Optimizing your LLM in production </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/optimizing-llm-production-hf?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/optimizing-llm-production-hf&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In this blog post, we will go over the most effective techniques at the time of writing this blog post to tackle these challenges for efficient LLM deployment:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Lower Precision:&lt;/strong&gt; Research has shown that operating at reduced numerical precision, namely 8-bit and 4-bit, can achieve computational advantages without a considerable decline in model performance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;2. Flash Attention:&lt;/strong&gt; Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Architectural Innovations:&lt;/strong&gt; Considering that LLMs are always deployed in the same way during inference, namely autoregressive text generation with a long input context, specialized model architectures have been proposed that allow for more efficient inference. The most important advancement in model architectures hereby are Alibi, Rotary embeddings, Multi-Query Attention (MQA) and Grouped-Query-Attention (GQA).&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/optimizing-llm-production-hf?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/optimizing-llm-production-hf</guid>
      <pubDate>2023-09-17 12:58</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#production</category>
      <category>#engineering</category>
      <category>#software</category>
      <category>#mlops</category>
      <category>#aiops</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Software¬≤</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/software-squared-jiang?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/software-squared-jiang&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A new generation of AIs that become increasingly general by producing their own training data&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are currently at the cusp of transitioning from ‚Äúlearning from data‚Äù to ‚Äúlearning what data to learn from‚Äù as the central focus of AI research.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;If deep learning can be described as ‚ÄúSoftware 2.0‚Äù‚Äîsoftware that programs itself based on example inputs/output pairs, then this promising, data-centric paradigm, in which software effectively improves itself by searching for its own training data, can be described as a kind of ‚ÄúSoftware¬≤‚Äù. This paradigm inherits the benefits of Software 2.0 while improving on its core, data-bound weaknesses: While deep learning (Software 2.0) requires the programmer to manually provide training data for each new task, Software¬≤ recasts data as software that models or searches the world to produce its own, potentially unlimited, training tasks and data.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/software-squared-jiang?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/software-squared-jiang</guid>
      <pubDate>2023-09-17 12:54</pubDate>
      <category>#ai</category>
      <category>#software</category>
      <category>#data</category>
    </item>
    <item>
      <title>PointLLM: Empowering Large Language Models to Understand Point Clouds</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/point-llm-point-clouds?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/point-llm-point-clouds&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We introduce PointLLM, a multi-modal large language model capable of understanding colored point clouds of objects. It perceives object types, geometric structures, and appearance without concerns for ambiguous depth, occlusion, or viewpoint dependency. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establish two benchmarks: Generative 3D Object Classification and 3D Object Captioning, assessed through three different evaluation methods.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/point-llm-point-clouds?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/point-llm-point-clouds</guid>
      <pubDate>2023-09-17 12:52</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#generativeai</category>
      <category>#multimodal</category>
      <category>#research</category>
    </item>
    <item>
      <title>How consumers are using Generative AI</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/a16z-how-consumers-using-generative-ai?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/a16z-how-consumers-using-generative-ai&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;1. Most leading products are built from the ‚Äúground up‚Äù around generative AI&lt;/strong&gt;&lt;br /&gt;
Like ChatGPT, the majority of products on this list didn‚Äôt exist a year ago‚Äî80% of these websites are new
Of the 50 companies on the list, only 5 are products of, or acquisitions by, pre-existing big tech companies...
Of the remaining list members, a whopping 48% are completely bootstrapped, with no outside funding, according to PitchBook data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;2. ChatGPT has a massive lead, for now‚Ä¶&lt;/strong&gt;&lt;br /&gt;
ChatGPT represents 60% of monthly traffic to the entire top 50 list, with an estimated 1.6 billion monthly visits and 200 million monthly users (as of June 2023). This makes ChatGPT the 24th most visited website globally.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;3. LLM assistants (like ChatGPT) are dominant, but companionship and creative tools are on the rise&lt;/strong&gt;&lt;br /&gt;
General LLM chatbots represent 68% of total consumer traffic to the top 50 list.
However, two other categories have started to drive significant usage in recent months‚ÄîAI companions (such as CharacterAI) and content generation tools (such as Midjourney and ElevenLabs). Within the broader content generation category, image generation is the top use case with 41% of traffic, followed by prosumer writing tools at 26%, and video generation at 8%.
Another category worth mentioning? Model hubs. There are only 2 on the list, but they drive significant traffic‚ÄîCivitai (for images) and Hugging Face both rank in the top 10. This is especially impressive because consumers are typically visiting these sites to download models to run locally, so web traffic is likely an underestimate of actual usage.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;4. Early ‚Äúwinners‚Äù have emerged, but most product categories are up for grabs&lt;/strong&gt;&lt;br /&gt;
Good news for builders: despite the surge in interest in generative AI, in many categories there is not yet a runway success.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;5. Acquisition for top products is entirely organic‚Äîand consumers are willing to pay!&lt;/strong&gt;&lt;br /&gt;
The majority of companies on this list have no paid marketing (at least, that SimilarWeb is able to attribute). There is significant free traffic ‚Äúavailable‚Äù via X, Reddit, Discord, and email, as well as word of mouth and referral growth.
And consumers are willing to pay for GenAI. 90% of companies on the list are already monetizing, nearly all of them via a subscription model. The average product on the list makes $21/month (for users on monthly plans)‚Äîyielding $252 annually.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;strong&gt;6. Mobile apps are still emerging as a GenAI platform&lt;/strong&gt;&lt;br /&gt;
Consumer AI products have, thus far, been largely browser-first, rather than app-first. Even ChatGPT took 6 months to launch a mobile app!
Why aren‚Äôt more AI companies building on mobile? The browser is a natural starting place to reach the broadest base of consumers. Many AI companies have small teams and likely don‚Äôt want to fragment their focus and resources across Web, iOS, and Android.
Given that the average consumer now spends 36 minutes more per day on mobile than desktop (4.1 hours vs. 3.5 hours), we expect to see more mobile-first GenAI products emerge as the technology matures.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/a16z-how-consumers-using-generative-ai?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/a16z-how-consumers-using-generative-ai</guid>
      <pubDate>2023-09-17 12:40</pubDate>
      <category>#ai</category>
      <category>#generativeai</category>
      <category>#product</category>
      <category>#business</category>
      <category>#venturecapital</category>
      <category>#vc</category>
    </item>
    <item>
      <title>WordPress ActivityPub Plugin</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/wordpress-activitypub-plugin?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/wordpress-activitypub-plugin&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I can't believe I missed this announcement. This is great to see!&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Enter the fediverse with ActivityPub, broadcasting your blog to a wider audience! Attract followers, deliver updates, and receive comments from a diverse user base of ActivityPub-compliant platforms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;With the ActivityPub plugin installed, your WordPress blog itself function as a federated profile, along with profiles for each author. For instance, if your website is example.com, then the blog-wide profile can be found at @example.com@example.com, and authors like Jane and Bob would have their individual profiles at @jane@example.com and @bobz@example.com, respectively.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/wordpress-activitypub-plugin?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/wordpress-activitypub-plugin</guid>
      <pubDate>2023-09-17 12:32</pubDate>
      <category>#wordpress</category>
      <category>#activitypub</category>
      <category>#fediverse</category>
      <category>#openweb</category>
      <category>#decentralization</category>
      <category>#internet</category>
      <category>#blogging</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Coqui üê∏ XTTS</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/coqui-xtts-text-to-speech?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/coqui-xtts-text-to-speech&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;a href="https://huggingface.co/coqui/XTTS-v1"&gt;XTTS&lt;/a&gt; is a Voice generation model that lets you clone voices into different languages by using just a quick 3-second audio clip.&lt;/p&gt;
&lt;p&gt;XTTS is built on previous research, like Tortoise, with additional architectural innovations and training to make cross-language voice cloning and multilingual speech generation possible.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/coqui-xtts-text-to-speech?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/coqui-xtts-text-to-speech</guid>
      <pubDate>2023-09-17 12:25</pubDate>
      <category>#ai</category>
      <category>#generativeai</category>
      <category>#speech</category>
      <category>#huggingface</category>
      <category>#ml</category>
      <category>#tts</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Introducing W√ºrstchen: Fast Diffusion for Image Generation </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/wurstchen-fast-image-generation?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/wurstchen-fast-image-generation&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;W√ºrstchen is a diffusion model, whose text-conditional component works in a highly compressed latent space of images. Why is this important? Compressing data can reduce computational costs for both training and inference by orders of magnitude. Training on 1024√ó1024 images is way more expensive than training on 32√ó32. Usually, other works make use of a relatively small compression, in the range of 4x - 8x spatial compression. W√ºrstchen takes this to an extreme. Through its novel design, it achieves a 42x spatial compression! This had never been seen before, because common methods fail to faithfully reconstruct detailed images after 16x spatial compression. W√ºrstchen employs a two-stage compression, what we call Stage A and Stage B. Stage A is a VQGAN, and Stage B is a Diffusion Autoencoder (more details can be found in the  paper). Together Stage A and B are called the Decoder, because they decode the compressed images back into pixel space. A third model, Stage C, is learned in that highly compressed latent space. This training requires fractions of the compute used for current top-performing models, while also allowing cheaper and faster inference. We refer to Stage C as the Prior.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/wurstchen-fast-image-generation?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/wurstchen-fast-image-generation</guid>
      <pubDate>2023-09-17 12:15</pubDate>
      <category>#ai</category>
      <category>#generativeai</category>
      <category>#imagegeneration</category>
      <category>#huggingface</category>
      <category>#diffusion</category>
      <category>#ml</category>
    </item>
    <item>
      <title>Efficient Controllable Generation for SDXL with T2I-Adapters </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/t2i-adapters-efficient-controllable-generation?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/t2i-adapters-efficient-controllable-generation&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;a href="https://huggingface.co/papers/2302.08453"&gt;T2I-Adapter&lt;/a&gt; is an efficient plug-and-play model that provides extra guidance to pre-trained text-to-image models while freezing the original large text-to-image models. T2I-Adapter aligns internal knowledge in T2I models with external control signals. We can train various adapters according to different conditions and achieve rich control and editing effects.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Over the past few weeks, the Diffusers team and the T2I-Adapter authors have been collaborating to bring the support of T2I-Adapters for Stable Diffusion XL (SDXL) in diffusers. In this blog post, we share our findings from training T2I-Adapters on SDXL from scratch, some appealing results, and, of course, the T2I-Adapter checkpoints on various conditionings (sketch, canny, lineart, depth, and openpose)!&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/t2i-adapters-efficient-controllable-generation?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/t2i-adapters-efficient-controllable-generation</guid>
      <pubDate>2023-09-17 12:11</pubDate>
      <category>#ai</category>
      <category>#generativeai</category>
      <category>#imagegeneration</category>
      <category>#deeplearning</category>
      <category>#ml</category>
      <category>#huggingface</category>
    </item>
    <item>
      <title>Spread Your Wings: Falcon 180B is here </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/falcon-180b-announcement?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/falcon-180b-announcement&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we're excited to welcome TII's Falcon 180B to HuggingFace! Falcon 180B sets a new state-of-the-art for open models. It is the largest openly available language model, with 180 billion parameters, and was trained on a massive 3.5 trillion tokens using TII's RefinedWeb dataset. This represents the longest single-epoch pretraining for an open model.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/falcon-180b-announcement?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/falcon-180b-announcement</guid>
      <pubDate>2023-09-12 10:35</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#opensource</category>
      <category>#huggingface</category>
    </item>
    <item>
      <title>The history and legacy of Visual Basic</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/visual-basic-history-retool?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/visual-basic-history-retool&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;How Visual Basic became the world's most dominant programming environment, its sudden fall from grace, and why its influence is still shaping the future of software development.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/visual-basic-history-retool?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/visual-basic-history-retool</guid>
      <pubDate>2023-09-12 10:33</pubDate>
      <category>#visualbasic</category>
      <category>#windows</category>
    </item>
    <item>
      <title>Modular: Mojoüî• - It‚Äôs finally here!</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mojo-available-local-download?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mojo-available-local-download&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today, we‚Äôre excited to announce the next big step in Mojo‚Äôs evolution: Mojo is now available for &lt;a href="https://developer.modular.com/"&gt;local download&lt;/a&gt; ‚Äì beginning with Linux systems, and adding Mac and Windows in coming releases.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mojo-available-local-download?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mojo-available-local-download</guid>
      <pubDate>2023-09-10 10:37</pubDate>
      <category>#ai</category>
      <category>#python</category>
      <category>#programminglanguages</category>
    </item>
    <item>
      <title>textual-web</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/textualize-textual-web?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/textualize-textual-web&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Textual Web publishes Textual apps and terminals on the web.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/textualize-textual-web?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/textualize-textual-web</guid>
      <pubDate>2023-09-10 10:35</pubDate>
      <category>#tui</category>
      <category>#terminal</category>
      <category>#web</category>
    </item>
    <item>
      <title>Dread Central - The Overlook Motel</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/dread-central-overlook-motel?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/dread-central-overlook-motel&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I'm a fan of finding hidden gems and overlooked films, especially when it comes to horror. It's how I came across some of my favorites like Hell House and Terrifier. That's why I was excited to run into The Overlook Motel series from Dread Central which spotlights these kinds of films.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=erM90Tf5wIo&amp;amp;list=PLh5in8jMKw0Q9LbTqEothBHBnJZXWSnLI" title="Overlook Motel Dread Central Playlist"&gt;&lt;img src="http://img.youtube.com/vi/erM90Tf5wIo/0.jpg" class="img-fluid" alt="Overlook Motel Dread Central Playlist" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/dread-central-overlook-motel?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/dread-central-overlook-motel</guid>
      <pubDate>2023-09-08 22:30</pubDate>
      <category>#movies</category>
      <category>#horror</category>
    </item>
    <item>
      <title>Tailscale has partnered with Mullvad</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tailscale-partners-mullvad?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tailscale-partners-mullvad&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we announce a partnership with Tailscale that allows you to use both in conjunction through the Tailscale app. This functionality is not available through the Mullvad VPN app.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This partnership allows customers of Tailscale to make use of our WireGuard VPN servers as ‚Äúexit nodes‚Äù. This means that whilst connected to Tailscale, you can access your devices across Tailscale‚Äôs mesh network, whilst still connecting outbound through Mullvad VPN WireGuard servers in any location.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tailscale-partners-mullvad?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tailscale-partners-mullvad</guid>
      <pubDate>2023-09-08 21:26</pubDate>
      <category>#privacy</category>
      <category>#vpn</category>
      <category>#security</category>
    </item>
    <item>
      <title>Plato's Cave Regrets to Inform You It Will Be Raising Its Rent</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/platos-cave-raising-rent?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/platos-cave-raising-rent&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;If you are receiving this letter, it means you have been designated a tenant of the cave‚Äîi.e., you are chained to the wall, you are forced to watch shadows for all eternity, you are projecting said shadow puppets, and/or you are a philosopher who was able to break free and understand the true shackles of reality (PhD candidates about to argue their thesis).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We do not undertake this lightly. As the costs of maintaining a cave meant to trap you in your ignorance increases year after year, we want you to know, from the bottom of our hearts, that we, too, are suffering. We get that times are tough, and we hope you can extend that sympathy to us, the managers of your cave.&lt;/p&gt;
&lt;p&gt;Please rest assured that cave costs are increasing everywhere. We manage many other caves like that of Polyphemus the Cyclops and the childhood home of Zeus. So, trust us: we know caves.&lt;/p&gt;
&lt;p&gt;We hope you will continue to enjoy living in our cave. We believe you are a valued part of the Plato's Cave community. Credit, cash, or Venmo all work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Original Source: &lt;a href="https://www.mcsweeneys.net/articles/platos-cave-regrets-to-inform-you-it-will-be-raising-its-rent"&gt;https://www.mcsweeneys.net/articles/platos-cave-regrets-to-inform-you-it-will-be-raising-its-rent&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://media.giphy.com/media/zHd8x7Pik0Ftm/giphy.gif" class="img-fluid" alt="Boy laughing and then crying" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/platos-cave-raising-rent?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/platos-cave-raising-rent</guid>
      <pubDate>2023-09-07 21:25</pubDate>
      <category>#simulation</category>
    </item>
    <item>
      <title>Generative AI and .NET - Part 2 SDK</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/generative-ai-dotnet-pt-2-powell?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/generative-ai-dotnet-pt-2-powell&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It‚Äôs time to have a look at how we can build the basics of an application using Azure OpenAI Services and the .NET SDK.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/generative-ai-dotnet-pt-2-powell?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/generative-ai-dotnet-pt-2-powell</guid>
      <pubDate>2023-09-06 09:29</pubDate>
      <category>#ai</category>
      <category>#dotnet</category>
      <category>#generativeai</category>
      <category>#llm</category>
      <category>#azure</category>
    </item>
    <item>
      <title>Set up Your Onion Service</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/set-up-onion-service?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/set-up-onion-service&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This guide shows you how to set up an Onion Service for your website. For the technical details of how the Onion Service protocol works, see our Onion Service protocol page.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/set-up-onion-service?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/set-up-onion-service</guid>
      <pubDate>2023-09-06 00:14</pubDate>
      <category>#tor</category>
      <category>#internet</category>
      <category>#privacy</category>
      <category>#guide</category>
    </item>
    <item>
      <title>Cory Doctorow: Interoperability Can Save the Open Web</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/interoperability-can-save-open-web-doctorow?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/interoperability-can-save-open-web-doctorow&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In his new book &lt;a href="https://www.versobooks.com/products/3035-the-internet-con"&gt;The Internet Con: How to Seize the Means of Computation&lt;/a&gt;, author &lt;a href="https://www.eff.org/about/staff/cory-doctorow"&gt;Cory Doctorow&lt;/a&gt; presents a strong case for disrupting Big Tech. While the dominance of Internet platforms like Twitter, Facebook, Instagram, or Amazon is often taken for granted, Doctorow argues that these walled gardens are fenced in by legal structures, not feats of engineering. Doctorow proposes forcing interoperability‚Äîany given platform‚Äôs ability to interact with another‚Äîas a way to break down those walls and to make the Internet freer and more democratic.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/interoperability-can-save-open-web-doctorow?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/interoperability-can-save-open-web-doctorow</guid>
      <pubDate>2023-09-06 00:09</pubDate>
      <category>#openweb</category>
      <category>#interoperability</category>
      <category>#internet</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>Rethinking trust in direct messages in the AI era</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/rethinking-trust-dm-ai-era?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/rethinking-trust-dm-ai-era&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This blog post is a part of a series exploring our research in privacy, security, and cryptography. For the previous post, see &lt;a href="https://www.microsoft.com/en-us/research/blog/research-trends-in-privacy-security-and-cryptography"&gt;https://www.microsoft.com/en-us/research/blog/research-trends-in-privacy-security-and-cryptography&lt;/a&gt;. While AI has the potential to massively increase productivity, this power can be used equally well for malicious purposes, for example, to automate the creation of sophisticated scam messages. In this post, we explore threats AI can pose for online communication ecosystems and outline a high-level approach to mitigating these threats.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/rethinking-trust-dm-ai-era?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/rethinking-trust-dm-ai-era</guid>
      <pubDate>2023-09-06 00:05</pubDate>
      <category>#security</category>
      <category>#ai</category>
      <category>#privacy</category>
      <category>#cryptography</category>
      <category>#communication</category>
      <category>#messaging</category>
      <category>#socialmedia</category>
      <category>#trust</category>
    </item>
    <item>
      <title>Perplexity: Interactive LLM visualization</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/perplexity-interactive-llm-visualizations?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/perplexity-interactive-llm-visualizations&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I built this little tool to help me understand what it's like to be an autoregressive language model. For any given passage of text, it augments the original text with highlights and annotations that tell me how &amp;quot;surprising&amp;quot; each token is to the model, and which other tokens the model thought were most likely to occur in its place. Right now, the LM I'm using is the smallest version of GPT-2, with 124M parameters.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/perplexity-interactive-llm-visualizations?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/perplexity-interactive-llm-visualizations</guid>
      <pubDate>2023-09-06 00:02</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#visualization</category>
      <category>#tools</category>
    </item>
    <item>
      <title>Can LLMs learn from a single example?</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/can-llms-learn-single-example?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/can-llms-learn-single-example&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Summary: recently while fine-tuning a large language model (LLM) on multiple-choice science exam questions, we observed some highly unusual training loss curves. In particular, it appeared the model was able to rapidly memorize examples from the dataset after seeing them just once. This astonishing feat contradicts most prior wisdom about neural network sample efficiency. Intrigued by this result, we conducted a series of experiments to validate and better understand this phenomenon. It‚Äôs early days, but the experiments support the hypothesis that the models are able to rapidly remember inputs. This might mean we have to re-think how we train and use LLMs.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/can-llms-learn-single-example?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/can-llms-learn-single-example</guid>
      <pubDate>2023-09-05 23:59</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#finetuning</category>
      <category>#fastai</category>
    </item>
    <item>
      <title>Doing Laundry on Campus Without a Phone</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/doing-laundry-without-phone?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/doing-laundry-without-phone&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This article reminds me of this gem üòÇ&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=mzb355qT8RI" title="Dennis Orders a Boba Tea - Scene | It's Always Sunny in Philadelphia | FX"&gt;&lt;img src="http://img.youtube.com/vi/mzb355qT8RI/0.jpg" class="img-fluid" alt="Dennis Orders a Boba Tea - Scene | It's Always Sunny in Philadelphia | FX" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/doing-laundry-without-phone?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/doing-laundry-without-phone</guid>
      <pubDate>2023-09-02 18:45</pubDate>
      <category>#technology</category>
      <category>#tv</category>
    </item>
    <item>
      <title>Teaching with AI</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/teching-with-ai-openai?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/teching-with-ai-openai&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôre releasing a guide for teachers using ChatGPT in their classroom‚Äîincluding suggested prompts, an explanation of how ChatGPT works and its limitations, the efficacy of AI detectors, and bias.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/teching-with-ai-openai?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/teching-with-ai-openai</guid>
      <pubDate>2023-09-01 18:32</pubDate>
      <category>#ai</category>
      <category>#education</category>
      <category>#openai</category>
      <category>#guide</category>
    </item>
    <item>
      <title>Announcement - Solo by Pixelfed</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/pixelfed-solo-announcement?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/pixelfed-solo-announcement&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Solo by Pixelfed&lt;/p&gt;
&lt;p&gt;We've been secretly building a single user federated photo sharing server (based on Pixelfed), with minimal setup, and built-in import&lt;/p&gt;
&lt;p&gt;Solo is simple, download the code, drag your photos to the media directory and open your browser&lt;/p&gt;
&lt;p&gt;So simple, yet super smart, Solo won't get in your way, but it will impress&lt;/p&gt;
&lt;p&gt;Launching Oct 2023&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Very excited about this!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/pixelfed-solo-announcement?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/pixelfed-solo-announcement</guid>
      <pubDate>2023-09-01 10:55</pubDate>
      <category>#fediverse</category>
      <category>#pixelfed</category>
      <category>#photos</category>
      <category>#socialmedia</category>
    </item>
    <item>
      <title>Generative AI and .NET - Part 1 Intro</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/generative-ai-dotnet-pt-1-powell?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/generative-ai-dotnet-pt-1-powell&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;over this series I‚Äôm going to share my learnings on the APIs, SDKs, and the like. The goal here isn‚Äôt to ‚Äúbuild something‚Äù but rather to share what I‚Äôve learnt, the mistakes I‚Äôve made, the things I‚Äôve found confusing, and the code I‚Äôve had to rewrite umpteen times because ‚Äúoh, that‚Äôs a better way to do it‚Äù.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/generative-ai-dotnet-pt-1-powell?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/generative-ai-dotnet-pt-1-powell</guid>
      <pubDate>2023-09-01 10:46</pubDate>
      <category>#ai</category>
      <category>#dotnet</category>
      <category>#generativeai</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Llama 2 7B/13B are now available in Web LLM</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/web-llm-llama-2?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/web-llm-llama-2&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Llama 2 7B/13B are now available in Web LLM!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Llama 2 70B is also supported.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This project brings large-language model and LLM-based chatbot to web browsers. Everything runs inside the browser with no server support and accelerated with WebGPU. This opens up a lot of fun opportunities to build AI assistants for everyone and enable privacy while enjoying GPU acceleration.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/web-llm-llama-2?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/web-llm-llama-2</guid>
      <pubDate>2023-08-31 18:57</pubDate>
      <category>#ai</category>
      <category>#webgpu</category>
      <category>#llama</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Supporting the Open Source AI Community</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/supporting-open-source-ai-community-az?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/supporting-open-source-ai-community-az&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We believe artificial intelligence has the power to save the world‚Äîand that a thriving open source ecosystem is essential to building this future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;To help close this resource gap, we‚Äôre announcing today the a16z Open Source AI Grant program. We‚Äôll support a small group of open source developers through grant funding (not an investment or SAFE note), giving them the opportunity to continue their work without the pressure to generate financial returns.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/supporting-open-source-ai-community-az?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/supporting-open-source-ai-community-az</guid>
      <pubDate>2023-08-31 18:48</pubDate>
      <category>#ai</category>
      <category>#opensource</category>
      <category>#machinelearning</category>
      <category>#llm</category>
    </item>
    <item>
      <title>WordPress at 25 and the Indie Web</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/podcast-wordpress-indieweb-reality2?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/podcast-wordpress-indieweb-reality2&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Good discussion on the open web, self-hosting, radio, and the indie web.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/podcast-wordpress-indieweb-reality2?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/podcast-wordpress-indieweb-reality2</guid>
      <pubDate>2023-08-30 21:02</pubDate>
      <category>#podcast</category>
      <category>#indieweb</category>
      <category>#wordpress</category>
      <category>#selfhost</category>
      <category>#radio</category>
      <category>#openweb</category>
      <category>#fediverse</category>
    </item>
    <item>
      <title>Making Large Language Models Work For You</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/making-llms-work-for-you-willison?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/making-llms-work-for-you-willison&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;My goal today is to provide practical, actionable advice for getting the most out of Large Language Models‚Äîboth for personal productivity but also as a platform that you can use to build things that you couldn‚Äôt build before.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/making-llms-work-for-you-willison?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/making-llms-work-for-you-willison</guid>
      <pubDate>2023-08-29 19:57</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#wordpress</category>
      <category>#presentation</category>
    </item>
    <item>
      <title>We Don‚Äôt Need a New Twitter</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/no-new-twitter-newport?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/no-new-twitter-newport&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;If Meta can succeed in capturing some of this peak-Twitter magic, while avoiding late-stage Twitter‚Äôs struggles, the company will perhaps even reclaim some of the cultural gravity that it squandered a decade ago when Facebook took its turn toward crazy-uncle irrelevance. But can Meta possibly succeed in building a saner, nicer Twitter?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Breaking news can spread quickly, as can clips that are funny in an original or strange way‚Äîbut these innocuous trends feel serendipitous, like a rainbow spanning storm clouds. To reach the Twitter masses, conspiracy, demagoguery, and cancellation are much more likely to succeed. The result is a Faustian bargain for our networked era: trusting the wisdom of crowds to identify what‚Äôs interesting can create an intensely compelling stream of shared content, but this content is likely to arrive drenched in rancor.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The obvious way Meta can attempt to escape this bargain is by moving Threads away from retransmission-based curation and toward algorithmic ranking. This will give the company more control over which discussions are amplified, but, in doing so, they will also lose the human-powered selectivity that makes Twitter so engaging.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;If we look past this narrow discussion of Threads‚Äô challenges, however, a broader question arises: Why is it so important to create a better version of Twitter in the first place? Ignored amid the hand-wringing about the toxic turn taken by large-scale conversation platforms are the many smaller, less flashy sites and services that have long been supporting a more civilized form of digital interaction.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;‚ÄúThe Internet has become the ultimate narrowcasting vehicle: everyone from UFO buffs to New York Yankee fans has a Website (or dozen) to call his own,‚Äù the journalist Richard Zoglin wrote in 1996. ‚ÄúA dot-com in every pot.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôve gone from Zoglin‚Äôs dot-com in every pot to the social-media age‚Äôs vision of every pot being filled with slop &lt;a href="https://www.newyorker.com/culture/cultural-comment/tiktok-and-the-fall-of-the-social-media-giants"&gt;from the same platforms&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/no-new-twitter-newport?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/no-new-twitter-newport</guid>
      <pubDate>2023-08-28 20:46</pubDate>
      <category>#twitter</category>
      <category>#socialmedia</category>
      <category>#indieweb</category>
      <category>#internet</category>
      <category>#culture</category>
    </item>
    <item>
      <title>Against Waldenponding</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/against-waldenponding-rao?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/against-waldenponding-rao&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Waldenponding (after Thoreau's Walden Pond experiment on which Walden is based). The crude caricature is &amp;quot;smash your smart phone and go live in a log cabin to reclaim your attention and your life from being hacked by evil social media platforms.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/against-waldenponding-rao?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/against-waldenponding-rao</guid>
      <pubDate>2023-08-24 21:43</pubDate>
      <category>#internet</category>
      <category>#society</category>
      <category>#technology</category>
    </item>
    <item>
      <title>Increasing the surface area of blogging</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/increasing-surface-area-blogging-critchlow?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/increasing-surface-area-blogging-critchlow&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;RSS is kind of an invisible technology. People call RSS dead because you can‚Äôt see it. There‚Äôs no feed, no login, no analytics. RSS feels subsurface.&lt;/p&gt;
&lt;p&gt;Come to think of it - all the interesting bits of blogging are invisible. The discussion has moved to Twitter, or discords, or DMs. Trackbacks aren‚Äôt a thing anymore. So when you see someone blogging all you see is the blog post. The branching replies and conversations are either invisible or hard to track down.&lt;/p&gt;
&lt;p&gt;But I believe we‚Äôre living in a golden age of RSS. Blogging is booming. My feed reader has 280 feeds in it.&lt;/p&gt;
&lt;p&gt;How do we increase the surface area of RSS and blogging?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I think there‚Äôs something quietly radical about making your feed reader open by default. It increases the surface area of RSS so others can discover content more easily. It makes blogging more visible.&lt;/p&gt;
&lt;p&gt;The nice thing about RSS and OPML is that‚Äôs a very extensible spec. The file format is flexible, you can define your own schema and fields. This might open up new kinds of publishing.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/increasing-surface-area-blogging-critchlow?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/increasing-surface-area-blogging-critchlow</guid>
      <pubDate>2023-08-24 21:25</pubDate>
      <category>#rss</category>
      <category>#internet</category>
      <category>#blog</category>
      <category>#postcast</category>
      <category>#feeds</category>
      <category>#openweb</category>
      <category>#openstandards</category>
    </item>
    <item>
      <title>About Feeds</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/about-feeds?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/about-feeds&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;About Feeds is a free site from Matt Webb. I made this site because using web feeds for the first time is hard, and we can fix that.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/about-feeds?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/about-feeds</guid>
      <pubDate>2023-08-24 21:21</pubDate>
      <category>#rss</category>
      <category>#internet</category>
      <category>#blog</category>
      <category>#openweb</category>
      <category>#openstandards</category>
    </item>
    <item>
      <title>Consciousness is a Big Suitcase</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/consciousness-big-suitcase-minsky?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/consciousness-big-suitcase-minsky&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Marvin Minsky is the leading light of AI‚Äîartificial intelligence, that is. He sees the brain as a myriad of structures. Scientists who, like Minsky, take the strong AI view believe that a computer model of the brain will be able to explain what we know of the brain's cognitive abilities. Minsky identifies consciousness with high-level, abstract thought, and believes that in principle machines can do everything a conscious human being can do.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/consciousness-big-suitcase-minsky?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/consciousness-big-suitcase-minsky</guid>
      <pubDate>2023-08-24 21:01</pubDate>
      <category>#ai</category>
      <category>#consciousness</category>
      <category>#computerscience</category>
    </item>
    <item>
      <title>SerenityOS</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/serenity-os?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/serenity-os&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A graphical Unix-like operating system for desktop computers!&lt;/p&gt;
&lt;p&gt;SerenityOS is a love letter to '90s user interfaces with a custom Unix-like core. It flatters with sincerity by stealing beautiful ideas from various other systems.&lt;/p&gt;
&lt;p&gt;Roughly speaking, the goal is a marriage between the aesthetic of late-1990s productivity software and the power-user accessibility of late-2000s *nix.&lt;/p&gt;
&lt;p&gt;This is a system by us, for us, based on the things we like.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/serenity-os?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/serenity-os</guid>
      <pubDate>2023-08-24 18:57</pubDate>
      <category>#os</category>
      <category>#linux</category>
      <category>#90s</category>
      <category>#serenityos</category>
      <category>#retro</category>
    </item>
    <item>
      <title>Introducing Code Llama, a state-of-the-art large language model for coding</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/meta-code-llama?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/meta-code-llama&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Takeaways&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Code Llama is a state-of-the-art LLM capable of generating code, and natural language about code, from both code and natural language prompts.&lt;/li&gt;
&lt;li&gt;Code Llama is free for research and commercial use.&lt;/li&gt;
&lt;li&gt;Code Llama is built on top of Llama 2 and is available in three models:
&lt;ul&gt;
&lt;li&gt;Code Llama, the foundational code model;&lt;/li&gt;
&lt;li&gt;Codel Llama - Python specialized for Python;&lt;/li&gt;
&lt;li&gt;and Code Llama - Instruct, which is fine-tuned for understanding natural language instructions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In our own benchmark testing, Code Llama outperformed state-of-the-art publicly available LLMs on code tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/meta-code-llama?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/meta-code-llama</guid>
      <pubDate>2023-08-24 12:43</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#llama</category>
      <category>#code</category>
      <category>#opensource</category>
      <category>#meta</category>
    </item>
    <item>
      <title>Announcing Python in Excel</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/announcing-python-in-excel?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/announcing-python-in-excel&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we‚Äôre announcing a significant evolution in the analytical capabilities available within Excel by releasing a Public Preview of Python in Excel. Python in Excel makes it possible to natively combine Python and Excel analytics within the same workbook - with no setup required. With Python in Excel, you can type Python directly into a cell, the Python calculations run in the Microsoft Cloud, and your results are returned to the worksheet, including plots and visualizations.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/announcing-python-in-excel?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/announcing-python-in-excel</guid>
      <pubDate>2023-08-22 17:38</pubDate>
      <category>#python</category>
      <category>#excel</category>
      <category>#dataanalytics</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Large Language Models with Semantic Search</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/llm-semantic-search-deeplearningai-course?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/llm-semantic-search-deeplearningai-course&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Keyword search has been a common method for search for many years. But for content-rich websites like news media sites or online shopping platforms, the keyword search capability can be limiting. Incorporating large language models (LLMs) into your search can significantly enhance the user experience by allowing them to ask questions and find information in a much easier way.&lt;/p&gt;
&lt;p&gt;This course teaches the techniques needed to leverage LLMs into search.&lt;/p&gt;
&lt;p&gt;Throughout the lessons, you‚Äôll explore key concepts like dense retrieval, which elevates the relevance of retrieved information, leading to improved search results beyond traditional keyword search, and reranking, which injects the intelligence of LLMs into your search system, making it faster and more effective.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/llm-semantic-search-deeplearningai-course?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/llm-semantic-search-deeplearningai-course</guid>
      <pubDate>2023-08-16 22:43</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#course</category>
      <category>#retrievalaugmentedgeneration</category>
    </item>
    <item>
      <title>Patterns for Building LLM-based Systems and Products</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/patterns-for-building-llm-systems-products-yan?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/patterns-for-building-llm-systems-products-yan&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;There are seven key patterns. They‚Äôre also organized along the spectrum of improving performance vs. reducing cost/risk, and closer to the data vs. closer to the user.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evals: To measure performance&lt;/li&gt;
&lt;li&gt;RAG: To add recent, external knowledge&lt;/li&gt;
&lt;li&gt;Fine-tuning: To get better at specific tasks&lt;/li&gt;
&lt;li&gt;Caching: To reduce latency &amp;amp; cost&lt;/li&gt;
&lt;li&gt;Guardrails: To ensure output quality&lt;/li&gt;
&lt;li&gt;Defensive UX: To anticipate &amp;amp; manage errors gracefully&lt;/li&gt;
&lt;li&gt;Collect user feedback: To build our data flywheel&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Addendum: &lt;a href="https://eugeneyan.com/writing/llm-problems/"&gt;how to match these LLM patterns to potential problems&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/patterns-for-building-llm-systems-products-yan?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/patterns-for-building-llm-systems-products-yan</guid>
      <pubDate>2023-08-16 22:20</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#designpatterns</category>
    </item>
    <item>
      <title>HuggingFace Candle</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/huggingface-candle?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/huggingface-candle&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) and ease of use.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/huggingface-candle?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/huggingface-candle</guid>
      <pubDate>2023-08-16 21:54</pubDate>
      <category>#ai</category>
      <category>#machinelearning</category>
      <category>#opensource</category>
      <category>#huggingface</category>
    </item>
    <item>
      <title>Open challenges in LLM research</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/open-challenges-llm-research-huyen?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/open-challenges-llm-research-huyen&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;ol&gt;
&lt;li&gt;Reduce and measure hallucinations&lt;/li&gt;
&lt;li&gt;Optimize context length and context construction&lt;/li&gt;
&lt;li&gt;Incorporate other data modalities&lt;/li&gt;
&lt;li&gt;Make LLMs faster and cheaper&lt;/li&gt;
&lt;li&gt;Design a new model architecture&lt;/li&gt;
&lt;li&gt;Develop GPU alternatives&lt;/li&gt;
&lt;li&gt;Make agents usable&lt;/li&gt;
&lt;li&gt;Improve learning from human preference&lt;/li&gt;
&lt;li&gt;Improve the efficiency of the chat interface&lt;/li&gt;
&lt;li&gt;Build LLMs for non-English languages&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/open-challenges-llm-research-huyen?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/open-challenges-llm-research-huyen</guid>
      <pubDate>2023-08-16 21:47</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#research</category>
    </item>
    <item>
      <title>Jupyter AI Brings Generative AI to Notebooks</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/jupyter-ai-notebooks?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/jupyter-ai-notebooks&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The open-source Project Jupyter, used by millions for data science and machine learning, has released Jupyter AI, a free tool bringing powerful generative AI capabilities to Jupyter notebooks.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://jupyter-ai.readthedocs.io/en/latest/"&gt;https://jupyter-ai.readthedocs.io/en/latest/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Jupyter AI, which brings generative AI to Jupyter. Jupyter AI provides a user-friendly and powerful way to explore generative AI models in notebooks and improve your productivity in JupyterLab and the Jupyter Notebook. More specifically, Jupyter AI offers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;An %%ai magic that turns the Jupyter notebook into a reproducible generative AI playground. This works anywhere the IPython kernel runs (JupyterLab, Jupyter Notebook, Google Colab, VSCode, etc.).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A native chat UI in JupyterLab that enables you to work with generative AI as a conversational assistant.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Support for a wide range of generative model providers and models (AI21, Anthropic, Cohere, Hugging Face, OpenAI, SageMaker, etc.).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/jupyter-ai-notebooks?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/jupyter-ai-notebooks</guid>
      <pubDate>2023-08-16 21:43</pubDate>
      <category>#ai</category>
      <category>#notebooks</category>
      <category>#development</category>
    </item>
    <item>
      <title>Retrostream TV</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/restrostrange-tv?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/restrostrange-tv&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Interesting project.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;RetroStrange is part independent media experiment, part handmade exaltation of vintage weirdness and the public domain. It is produced by Noah Maher and Phil Nelson.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/restrostrange-tv?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/restrostrange-tv</guid>
      <pubDate>2023-07-23 23:51</pubDate>
      <category>#livestream</category>
    </item>
    <item>
      <title>WordPress Playground</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/wordpress-playground?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/wordpress-playground&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Not a WordPress user but this is cool, especially this - &lt;em&gt;&amp;quot;Build an entire site, save it, host it&amp;quot;&lt;/em&gt;&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;WordPress Playground makes WordPress instantly accessible for users, learners, extenders, and contributors. You can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Try a block, a theme, or a plugin&lt;/li&gt;
&lt;li&gt;Build an entire site, save it, host it&lt;/li&gt;
&lt;li&gt;Test your plugin with many specific WordPress and PHP versions&lt;/li&gt;
&lt;li&gt;Embed a real, interactive WordPress site in your tutorial or course&lt;/li&gt;
&lt;li&gt;Showcase a plugin or theme on your website&lt;/li&gt;
&lt;li&gt;Preview pull requests from your repository&lt;/li&gt;
&lt;li&gt;‚Ä¶or even run WordPress locally using the VisualStudio Code plugin or a CLI tool called wp-now&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/wordpress-playground?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/wordpress-playground</guid>
      <pubDate>2023-07-15 00:27</pubDate>
      <category>#indieweb</category>
      <category>#wordpress</category>
      <category>#openweb</category>
    </item>
    <item>
      <title>Tildeverse</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tildeverse?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tildeverse&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;a loose association of like-minded tilde communities.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;tildes are pubnixes in the spirit of &lt;a href="http://tilde.club/"&gt;tilde.club&lt;/a&gt;, which was created in 2014 by &lt;a href="https://medium.com/message/tilde-club-i-had-a-couple-drinks-and-woke-up-with-1-000-nerds-a8904f0a2ebf"&gt;paul ford&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Public Access UNIX Systems (PAUS) are a type of server that provide various services to a multi-user community.  They first began in the early 1980's and continue today. Early servers ran various flavors of UNIX, hence the name Public Access &amp;quot;UNIX&amp;quot; Systems, but later generations saw a large mix of Unix-variants and, of course, GNU/Linux. To recognize the many different operating systems online today, these systems are increasingly referred to generically as &amp;quot;pubnixes&amp;quot;. - &lt;a href="https://github.com/cwmccabe/pubnixhist"&gt;Pubnix Hist&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tildeverse?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tildeverse</guid>
      <pubDate>2023-07-12 22:25</pubDate>
      <category>#linux</category>
      <category>#community</category>
      <category>#retrocomputing</category>
    </item>
    <item>
      <title>Hacking the Planet (with Notcurses) - A Guide to TUIs and Character Graphics</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/hack-the-planet-notcurses-tuis?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/hack-the-planet-notcurses-tuis&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A TUI (text user interface) is a holistic model, view, and controller implemented using character graphics.
TUIs, like WIMP3 GUIs, freely move the cursor around their rectilinear display, as opposed to line-oriented
CLIs and their ineluctable marches through the scrolling region.
Given the same interactive task&lt;/p&gt;
&lt;p&gt;‚Ä¢ A TUI implementation is almost certainly a smaller memory and disk footprint than a GUI,&lt;br /&gt;
‚Ä¢ a good TUI implementation might introduce less latency, and&lt;br /&gt;
‚Ä¢ a properly-done TUI implementation can often be significantly more portable.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/hack-the-planet-notcurses-tuis?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/hack-the-planet-notcurses-tuis</guid>
      <pubDate>2023-07-12 22:21</pubDate>
      <category>#linux</category>
      <category>#terminal</category>
      <category>#tui</category>
    </item>
    <item>
      <title>A slack clone in 5 lines of bash</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/dam-slack-clone-bash?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/dam-slack-clone-bash&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;suc provides Slack, Mattermost, etc.‚Äôs core features:&lt;/p&gt;
&lt;p&gt;Real-time, rich-text chat,&lt;br /&gt;
File sharing,&lt;br /&gt;
Fine-grained access control,&lt;br /&gt;
Straightforward automation and integration with other tools,&lt;br /&gt;
Data encryption in transit&lt;br /&gt;
and optionally at rest,&lt;br /&gt;
state-of-the-art user authentication.&lt;/p&gt;
&lt;p&gt;This paper shows how suc implements those features. suc stays small by leveraging the consistent and composable primitives offered by modern UNIX implementations&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/dam-slack-clone-bash?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/dam-slack-clone-bash</guid>
      <pubDate>2023-07-12 22:07</pubDate>
      <category>#linux</category>
      <category>#bash</category>
      <category>#communication</category>
    </item>
    <item>
      <title>Kaz Nejatian (COO, Shopify): Why Shopify Elevated the Non-Manager Career Path and Ditched Meetings</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/interview-shopify-coo-career-path-meetings?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/interview-shopify-coo-career-path-meetings&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h2&gt;The difference between crafters and managers&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The difference is in what you spend time on.&lt;/p&gt;
&lt;p&gt;&amp;quot;Most people get satisfaction from building ‚Äî from actually creating things.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;As companies scale, optics start playing a larger role. People start spending more time on internal docs than actually talking to customers. How do you prevent this from happening at Shopify?&lt;/p&gt;
&lt;p&gt;In most product reviews, product managers spend way too much time preparing the perfect presentation for execs.&lt;/p&gt;
&lt;p&gt;At Shopify, our approach to product reviews is different. We want to see how the product actually works by playing with the demo or diving into the code.&lt;/p&gt;
&lt;p&gt;We want our PMs to be extremely user-focused, to take full ownership over problems, and to have a high tolerance for risk.&lt;/p&gt;
&lt;p&gt;If these attributes aren't present, product managers tend to become &amp;quot;keepers of strategy.‚Äù You end up with smart, highly credentialed individuals spending all their time writing strategy memos to increase their team size so that they can write even more strategy memos.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;How Shopify rages against meetings&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In early 2023, Shopify initiated operation ‚ÄúChaos Monkey‚Äù to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Cancel all meetings with 3+ people&lt;/li&gt;
&lt;li&gt;Reinstate ‚Äúno meeting Wednesdays‚Äù&lt;/li&gt;
&lt;li&gt;Remove needless Slack channels&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Does Shopify have a strong writing culture to help people communicate without meetings?&lt;/p&gt;
&lt;p&gt;Yes, we try to make async decisions as much as we can. We do this in a few ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;One of our mantras is ‚ÄúDo things, tell people.‚Äù You‚Äôll see this plastered on our walls if you come to Shopify‚Äôs office.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We built an operating system called GSD (get shit done). This internal tool emphasizes frequent written updates, which are much easier to digest than constant meetings.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A meeting is a bug that some other process didn‚Äôt work out.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We focus on the mission. We want to be the all-in-one commerce platform for people to start and grow businesses. We try to avoid getting distracted by other side quests.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The main thing is to keep the main thing the main thing.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/interview-shopify-coo-career-path-meetings?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/interview-shopify-coo-career-path-meetings</guid>
      <pubDate>2023-07-12 20:20</pubDate>
      <category>#career</category>
      <category>#productivity</category>
      <category>#product</category>
    </item>
    <item>
      <title>Announcing xAI </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/x-ai-launch?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/x-ai-launch&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we announce the formation of xAI.&lt;/p&gt;
&lt;p&gt;The goal of xAI is to understand the true nature of the universe.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/x-ai-launch?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/x-ai-launch</guid>
      <pubDate>2023-07-12 20:15</pubDate>
      <category>#ai</category>
    </item>
    <item>
      <title>Introducing Keras Core: Keras for TensorFlow, JAX, and PyTorch.</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/keras-core-announcement?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/keras-core-announcement&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We're excited to share with you a new library called Keras Core, a preview version of the future of Keras. In Fall 2023, this library will become Keras 3.0. Keras Core is a full rewrite of the Keras codebase that rebases it on top of a modular backend architecture. It makes it possible to run Keras workflows on top of arbitrary frameworks ‚Äî starting with TensorFlow, JAX, and PyTorch.&lt;/p&gt;
&lt;p&gt;Keras Core is also a drop-in replacement for tf.keras, with near-full backwards compatibility with tf.keras code when using the TensorFlow backend. In the vast majority of cases you can just start importing it via import keras_core as keras in place of from tensorflow import keras and your existing code will run with no issue ‚Äî and generally with slightly improved performance, thanks to XLA compilation.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/keras-core-announcement?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/keras-core-announcement</guid>
      <pubDate>2023-07-11 16:22</pubDate>
      <category>#deeplearning</category>
      <category>#ai</category>
      <category>#keras</category>
      <category>#jax</category>
      <category>#tensorflow</category>
      <category>#pytorch</category>
    </item>
    <item>
      <title>GPT-4 API general availability</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/gpt-4-ga?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/gpt-4-ga&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;GPT-4 API general availability and deprecation of older models in the Completions API. GPT-3.5 Turbo, DALL¬∑E and Whisper APIs are also generally available, and we are releasing a deprecation plan for older models of the Completions API, which will retire at the beginning of 2024.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/gpt-4-ga?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/gpt-4-ga</guid>
      <pubDate>2023-07-06 22:08</pubDate>
      <category>#openai</category>
      <category>#gpt4</category>
      <category>#llm</category>
      <category>#ai</category>
    </item>
    <item>
      <title>512KB Club</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/512kb-club?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/512kb-club&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The 512KB Club is a collection of performance-focused web pages from across the Internet. To qualify your website must satisfy both of the following requirements:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It must be an actual site that contains a reasonable amount of information, not just a couple of links on a page (more info here).&lt;/li&gt;
&lt;li&gt;Your total UNCOMPRESSED web resources must not exceed 512KB.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/512kb-club?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/512kb-club</guid>
      <pubDate>2023-06-29 10:56</pubDate>
      <category>#web</category>
      <category>#smallweb</category>
      <category>#community</category>
    </item>
    <item>
      <title>1MB Club</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/1mb-club?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/1mb-club&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;1MB Club is a growing collection of performance-focused web pages weighing less than 1 megabyte.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/1mb-club?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/1mb-club</guid>
      <pubDate>2023-06-29 10:53</pubDate>
      <category>#web</category>
      <category>#smallweb</category>
      <category>#community</category>
    </item>
    <item>
      <title>Introducing Proton Pass</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/protonpass-release?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/protonpass-release&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôre happy to announce the global launch of Proton Pass...a password manager, one of the most highly demanded services from the Proton community in our annual surveys since we first launched Proton Mail...&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/protonpass-release?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/protonpass-release</guid>
      <pubDate>2023-06-29 10:32</pubDate>
      <category>#security</category>
    </item>
    <item>
      <title>fsharpConf 2023</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/fsharp-conf-2023?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/fsharp-conf-2023&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I'm really enjoying the sessions. Kudos to the team who put it together and presenters delivering great content.&lt;/p&gt;
&lt;p&gt;If you're interested, check out the stream at &lt;a href="http://fsharpconf.com/"&gt;http://fsharpconf.com/&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/fsharp-conf-2023?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/fsharp-conf-2023</guid>
      <pubDate>2023-06-26 13:02</pubDate>
      <category>#fsharp</category>
      <category>#fsharpconf</category>
      <category>#dotnet</category>
      <category>#programming</category>
      <category>#community</category>
      <category>#opensource</category>
    </item>
    <item>
      <title>LGR review of the Book 8088 and Hand 386 Retro DOS PCs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/book-8088-hand-386-dos-pcs?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/book-8088-hand-386-dos-pcs&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;a href="https://yewtu.be/embed/6bODiZ5bP84" title="Book 8088 and Hand 386 Retro DOS PCs"&gt;&lt;img src="https://yewtu.be/vi/6bODiZ5bP84/maxres.jpg" class="img-fluid" alt="Book 8088 and Hand 386 Retro DOS PCs" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://media.giphy.com/media/l0HFkA6omUyjVYqw8/giphy.gif" class="img-fluid" alt="GIF of baby throwing cash out window" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/book-8088-hand-386-dos-pcs?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/book-8088-hand-386-dos-pcs</guid>
      <pubDate>2023-06-20 23:29</pubDate>
      <category>#retrocomputing</category>
      <category>#review</category>
      <category>#dos</category>
      <category>#pcs</category>
    </item>
    <item>
      <title>Reclaim Open 2023 Resources</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/reclaim-open-2023-resources?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/reclaim-open-2023-resources&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;List of resources from the Reclaim Open 2023 Conference.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://reclaimopen.com/"&gt;Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://watch.reclaimed.tech/reclaim-open"&gt;Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=WzogFtdBVg8"&gt;KEYNOTE: We Have Never Been Social: Web 2.0 and What Went Wrong&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://digitalmediacookbook.com/"&gt;Digital Media Cookbook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://minicomp.github.io/wiki/"&gt;Minimal Computing (Minicomp) Wiki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://go-dh.github.io/mincomp/thoughts/2016/10/03/tldr/#minimal-connectivity"&gt;Minimal Definitions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://press.uchicago.edu/ucp/books/book/chicago/F/bo3773600.html"&gt;Book: From Counterculture to Cyberculture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://darcyd.com/fragmented_future.pdf"&gt;Article: Fragmented Future&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/reclaim-open-2023-resources?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/reclaim-open-2023-resources</guid>
      <pubDate>2023-06-19 22:04</pubDate>
      <category>#reclaimopen</category>
      <category>#opensource</category>
      <category>#openweb</category>
      <category>#edtech</category>
    </item>
    <item>
      <title>Orca: Progressive Learning from Complex Explanation Traces of GPT-4</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/orca-lm-progressive-learning-gpt-4?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/orca-lm-progressive-learning-gpt-4&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Recent research has focused on enhancing the capability of smaller models
through imitation learning, drawing on the outputs generated by large
foundation models (LFMs). A number of issues impact the quality of these
models, ranging from limited imitation signals from shallow LFM outputs;
small scale homogeneous training data; and most notably a lack of rigorous
evaluation resulting in overestimating the small model‚Äôs capability as they
tend to learn to imitate the style, but not the reasoning process of LFMs. To
address these challenges, we develop Orca, a 13-billion parameter model
that learns to imitate the reasoning process of LFMs. Orca learns from
rich signals from GPT-4 including explanation traces; step-by-step thought
processes; and other complex instructions, guided by teacher assistance from
ChatGPT. To promote this progressive learning, we tap into large-scale and
diverse imitation data with judicious sampling and selection. Orca surpasses
conventional state-of-the-art instruction-tuned models such as Vicuna-13B
by more than 100% in complex zero-shot reasoning benchmarks like Big-
Bench Hard (BBH) and 42% on AGIEval. Moreover, Orca reaches parity
with ChatGPT on the BBH benchmark and shows competitive performance
(4 pts gap with optimized system message) in professional and academic
examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot
settings without CoT; while trailing behind GPT-4. Our research indicates
that learning from step-by-step explanations, whether these are generated
by humans or more advanced AI models, is a promising direction to improve
model capabilities and skills.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/orca-lm-progressive-learning-gpt-4?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/orca-lm-progressive-learning-gpt-4</guid>
      <pubDate>2023-06-06 23:29</pubDate>
      <category>#ai</category>
      <category>#finetuning</category>
      <category>#gpt</category>
    </item>
    <item>
      <title>Gorilla: Large Language Model Connected with Massive APIs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/gorilla-apis-llm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/gorilla-apis-llm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/ShishirPatil/gorilla"&gt;Code&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/gorilla-apis-llm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/gorilla-apis-llm</guid>
      <pubDate>2023-06-03 22:37</pubDate>
      <category>#ai</category>
      <category>#opensource</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/draggan?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/draggan&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object's rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/XingangPan/DragGAN"&gt;Code&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/draggan?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/draggan</guid>
      <pubDate>2023-06-01 22:52</pubDate>
      <category>#ai</category>
      <category>#computer-vision</category>
    </item>
    <item>
      <title>StarCoder: A State-of-the-Art LLM for Code </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/hf-starcoder-llm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/hf-starcoder-llm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;StarCoder and StarCoderBase are Large Language Models for Code (Code LLMs) trained on permissively licensed data from GitHub, including from 80+ programming languages, Git commits, GitHub issues, and Jupyter notebooks. Similar to LLaMA, we trained a ~15B parameter model for 1 trillion tokens. We fine-tuned StarCoderBase model for 35B Python tokens, resulting in a new model that we call StarCoder.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/hf-starcoder-llm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/hf-starcoder-llm</guid>
      <pubDate>2023-06-01 22:50</pubDate>
      <category>#ai</category>
      <category>#code</category>
    </item>
    <item>
      <title>LoRA: Low-Rank Adaptation of Large Language Models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/microsoft-lora-llm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/microsoft-lora-llm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;LoRA reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights. This vastly reduces the storage requirement for large language models adapted to specific tasks and enables efficient task-switching during deployment all without introducing inference latency. LoRA also outperforms several other adaptation methods including adapter, prefix-tuning, and fine-tuning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2106.09685"&gt;Paper&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/microsoft-lora-llm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/microsoft-lora-llm</guid>
      <pubDate>2023-06-01 22:45</pubDate>
      <category>#ai</category>
      <category>#finetune</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Shap-E: Generating Conditional 3D Implicit Functions</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openai-shap-e?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openai-shap-e&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://github.com/openai/shap-e"&gt;Code&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openai-shap-e?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openai-shap-e</guid>
      <pubDate>2023-06-01 22:41</pubDate>
      <category>#ai</category>
      <category>#openai</category>
      <category>#3d</category>
    </item>
    <item>
      <title>WordPress - Newsletter paid subscriptions</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/wordpress-paid-newsletter?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/wordpress-paid-newsletter&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...we‚Äôre introducing a big update ‚Äî the ability to add paid subscriptions and premium content, whatever plan you‚Äôre on. Including the Free plan.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Paid subscriptions let your fans support your art, writing, or project directly.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/wordpress-paid-newsletter?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/wordpress-paid-newsletter</guid>
      <pubDate>2023-06-01 21:00</pubDate>
      <category>#web</category>
      <category>#blogging</category>
      <category>#wordpress</category>
      <category>#newsletters</category>
    </item>
    <item>
      <title>Massively Multilingual Speech (MMS)</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/meta-mms-tts-stt?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/meta-mms-tts-stt&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Massively Multilingual Speech (MMS) project expands speech technology from about 100 languages to over 1,000 by building a single multilingual speech recognition model supporting over 1,100 languages (more than 10 times as many as before), language identification models able to identify over 4,000 languages (40 times more than before), pretrained models supporting over 1,400 languages, and text-to-speech models for over 1,100 languages. Our goal is to make it easier for people to access information and to use devices in their preferred language.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms"&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/meta-mms-tts-stt?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/meta-mms-tts-stt</guid>
      <pubDate>2023-05-22 23:43</pubDate>
      <category>#ai</category>
      <category>#nlp</category>
      <category>#speech-to-text</category>
      <category>#text-to-speech</category>
    </item>
    <item>
      <title>Implement DNS in a weekends</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/implement-dns-weekend?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/implement-dns-weekend&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/implement-dns-weekend?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/implement-dns-weekend</guid>
      <pubDate>2023-05-14 00:39</pubDate>
      <category>#internet</category>
      <category>#dns</category>
      <category>#www</category>
      <category>#zine</category>
    </item>
    <item>
      <title>PaLM 2</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/PaLM-2?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/PaLM-2&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;PaLM 2 is our next generation large language model that builds on Google‚Äôs legacy of breakthrough research in machine learning and responsible AI.&lt;/p&gt;
&lt;p&gt;It excels at advanced reasoning tasks, including code and math, classification and question answering, translation and multilingual proficiency, and natural language generation better than our previous state-of-the-art LLMs, including PaLM. It can accomplish these tasks because of the way it was built ‚Äì bringing together compute-optimal scaling, an improved dataset mixture, and model architecture improvements.&lt;/p&gt;
&lt;p&gt;PaLM 2 is grounded in Google‚Äôs approach to building and deploying AI responsibly. It was evaluated rigorously for its potential harms and biases, capabilities and downstream uses in research and in-product applications. It‚Äôs being used in other state-of-the-art models, like Med-PaLM 2 and Sec-PaLM, and is powering generative AI features and tools at Google, like Bard and the PaLM API.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://ai.google/static/documents/palm2techreport.pdf"&gt;Technical Report&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/PaLM-2?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/PaLM-2</guid>
      <pubDate>2023-05-11 20:39</pubDate>
      <category>#ai</category>
    </item>
    <item>
      <title>Transformers Agent</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/huggingface-transformer-agents?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/huggingface-transformer-agents&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Transformers Agent...provides a natural language API on top of transformers: we define a set of curated tools and design an agent to interpret natural language and to use these tools. It is extensible by design; we curated some relevant tools, but we‚Äôll show you how the system can be extended easily to use any tool developed by the community.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/huggingface-transformer-agents?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/huggingface-transformer-agents</guid>
      <pubDate>2023-05-11 20:35</pubDate>
      <category>#transformers</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Copilot for Docs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/copilot-docs?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/copilot-docs&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Whether you‚Äôre learning a new library or API or you‚Äôve been using it for years, it can feel like the documentation gets in your way more than it helps. Maybe the tutorials are too basic, or the reference manual is too sketchy, or the relevant information is split across multiple pages full of irrelevant details.&lt;/p&gt;
&lt;p&gt;We‚Äôre exploring a way to get you the information you need, faster. By surfacing the most relevant content for questions with tailored summaries that help connect the dots, Copilot for docs saves developers from scouring reams of documentation.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/copilot-docs?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/copilot-docs</guid>
      <pubDate>2023-05-11 20:32</pubDate>
      <category>#ai</category>
      <category>#documentation</category>
      <category>#developers</category>
    </item>
    <item>
      <title>ChatGPT Prompt Engineering for Developers</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/chatgpt-prompt-engineering-developers?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/chatgpt-prompt-engineering-developers&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In ChatGPT Prompt Engineering for Developers, you will learn how to use a large language model (LLM) to quickly build new and powerful applications.  Using the OpenAI API, you‚Äôll be able to quickly build capabilities that learn to innovate and create value in ways that were cost-prohibitive, highly technical, or simply impossible before now.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/chatgpt-prompt-engineering-developers?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/chatgpt-prompt-engineering-developers</guid>
      <pubDate>2023-05-11 20:29</pubDate>
      <category>#ai</category>
      <category>#gpt</category>
      <category>#promptengineering</category>
    </item>
    <item>
      <title>ImageBind: One Embedding Space To Bind Them All</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/imagebind?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/imagebind&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...ImageBind, the first AI model capable of binding information from six modalities. The model learns a single embedding, or shared representation space, not just for text, image/video, and audio, but also for sensors that record depth (3D), thermal (infrared radiation), and inertial measurement units (IMU), which calculate motion and position.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://dl.fbaipublicfiles.com/imagebind/imagebind_final.pdf"&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://imagebind.metademolab.com/"&gt;Demo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/facebookresearch/ImageBind"&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/imagebind?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/imagebind</guid>
      <pubDate>2023-05-09 21:02</pubDate>
      <category>#ai</category>
      <category>#embeddings</category>
    </item>
    <item>
      <title>Language models can explain neurons in language models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/large-language-models-explain-neural-network-neurons?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/large-language-models-explain-neural-network-neurons&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This paper applies automation to the problem of scaling an interpretability technique to all the neurons in a large language model. Our hope is that building on this approach of automating interpretability will enable us to comprehensively audit the safety of models before deployment.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our technique seeks to explain what patterns in text cause a neuron to activate. It consists of three steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Explain the neuron's activations using GPT-4&lt;/li&gt;
&lt;li&gt;Simulate activations using GPT-4, conditioning on the explanation&lt;/li&gt;
&lt;li&gt;Score the explanation by comparing the simulated and real activations&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/large-language-models-explain-neural-network-neurons?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/large-language-models-explain-neural-network-neurons</guid>
      <pubDate>2023-05-09 20:59</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#deeplearning</category>
    </item>
    <item>
      <title>How generative AI is changing the way developers work</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/generative-ai-developer-work?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/generative-ai-developer-work&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/generative-ai-developer-work?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/generative-ai-developer-work</guid>
      <pubDate>2023-04-20 22:11</pubDate>
      <category>#ai</category>
      <category>#engineering</category>
      <category>#software</category>
    </item>
    <item>
      <title>PEFT: Parameter-Efficient Fine-Tuning of Billion-Scale Models on Low-Resource Hardware</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/peft-parameter-efficient-fine-tuning?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/peft-parameter-efficient-fine-tuning&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...as models get larger and larger, full fine-tuning becomes infeasible to train on consumer hardware. In addition, storing and deploying fine-tuned models independently for each downstream task becomes very expensive, because fine-tuned models are the same size as the original pretrained model. Parameter-Efficient Fine-tuning (PEFT) approaches are meant to address both problems!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs. This also overcomes the issues of catastrophic forgetting, a behaviour observed during the full finetuning of LLMs. PEFT approaches have also shown to be better than fine-tuning in the low-data regimes and generalize better to out-of-domain scenarios. It can be applied to various modalities, e.g., image classification and stable diffusion dreambooth.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/peft-parameter-efficient-fine-tuning?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/peft-parameter-efficient-fine-tuning</guid>
      <pubDate>2023-04-20 21:32</pubDate>
      <category>#ai</category>
      <category>#finetuning</category>
    </item>
    <item>
      <title>Wikipedia embeddings dataset</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/wikipedia-embeddings?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/wikipedia-embeddings&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/wikipedia-embeddings?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/wikipedia-embeddings</guid>
      <pubDate>2023-04-20 21:23</pubDate>
      <category>#ai</category>
      <category>#dataset</category>
      <category>#embeddings</category>
    </item>
    <item>
      <title>Offline Is Just Online With Extreme Latency</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nielsen-offline-online-latency?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nielsen-offline-online-latency&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;you can think of online/offline as part of the same continuum just different measurements of latency. There are gradations of latency when you‚Äôre ‚Äúonline‚Äù, and ‚Äúoffline‚Äù is merely at the slowest end of that spectrum.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nielsen-offline-online-latency?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nielsen-offline-online-latency</guid>
      <pubDate>2023-04-20 11:00</pubDate>
      <category>#web</category>
      <category>#offline</category>
    </item>
    <item>
      <title>LLaVA: Large Language and Vision Assistant</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/llava-language-vision-assistant?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/llava-language-vision-assistant&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;LLaVA represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks in the language domain, but the idea is less explored in the multimodal field.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Multimodal Instruct Data. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data.
2. LLaVA Model. We introduce LLaVA (Large Language-and-Vision Assistant), an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.
3. Performance. Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%.
4. Open-source. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/llava-language-vision-assistant?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/llava-language-vision-assistant</guid>
      <pubDate>2023-04-18 21:34</pubDate>
      <category>#ai</category>
      <category>#language</category>
      <category>#vision</category>
    </item>
    <item>
      <title>WebGPU API</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/webgpu-api?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/webgpu-api&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The WebGPU API enables web developers to use the underlying system's GPU (Graphics Processing Unit) to carry out high-performance computations and draw complex images that can be rendered in the browser.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/webgpu-api?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/webgpu-api</guid>
      <pubDate>2023-04-15 22:57</pubDate>
      <category>#ai</category>
      <category>#web</category>
      <category>#openstandards</category>
    </item>
    <item>
      <title>Consistency Models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openai-consistency-models?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openai-consistency-models&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Paper: https://arxiv.org/abs/2303.01469&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Diffusion models have made significant breakthroughs in image, audio, and video generation, but they depend on an iterative generation process that causes slow sampling speed and caps their potential for real-time applications. To overcome this limitation, we propose consistency models, a new family of generative models that achieve &lt;strong&gt;high sample quality without adversarial training&lt;/strong&gt;. They &lt;strong&gt;support fast one-step generation&lt;/strong&gt; by design, while still allowing for few-step sampling to trade compute for sample quality. They also &lt;strong&gt;support zero-shot data editing&lt;/strong&gt;, like image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either as a way to distill pre-trained diffusion models, or as standalone generative models. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step generation. For example, we achieve the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained as standalone generative models, consistency models also outperform single-step, non-adversarial generative models on standard benchmarks like CIFAR-10, ImageNet 64x64 and LSUN 256x256.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openai-consistency-models?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openai-consistency-models</guid>
      <pubDate>2023-04-14 14:22</pubDate>
      <category>#openai</category>
      <category>#ai</category>
    </item>
    <item>
      <title>Free Dolly</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/free-dolly-llm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/free-dolly-llm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/free-dolly-llm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/free-dolly-llm</guid>
      <pubDate>2023-04-12 23:06</pubDate>
      <category>#ai</category>
      <category>#llm</category>
      <category>#oss</category>
    </item>
    <item>
      <title>Generative Agents: Interactive Simulacra of Human Behavior</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/llm-generative-agents?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/llm-generative-agents&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In this paper, we introduce generative agents‚Äîcomputational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent‚Äôs experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/llm-generative-agents?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/llm-generative-agents</guid>
      <pubDate>2023-04-11 11:21</pubDate>
      <category>#ai</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Koala: A Dialogue Model for Academic Research</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/koala-dialogue-model?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/koala-dialogue-model&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...Koala, a chatbot trained by fine-tuning Meta‚Äôs LLaMA on dialogue data gathered from the web.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Many of the most capable LLMs require huge computational resources to train, and oftentimes use large and proprietary datasets. This suggests that in the future, highly capable LLMs will be largely controlled by a small number of organizations, and both users and researchers will pay to interact with these models without direct access to modify and improve them on their own. On the other hand, recent months have also seen the release of increasingly capable freely available or (partially) open-source models, such as LLaMA. These systems typically fall short of the most capable closed models, but their capabilities have been rapidly improving. This presents the community with an important question: will the future see increasingly more consolidation around a handful of closed-source models, or the growth of open models with smaller architectures that approach the performance of their larger but closed-source cousins?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our results suggest that learning from high-quality datasets can mitigate some of the shortcomings of smaller models, maybe even matching the capabilities of large closed-source models in the future.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/koala-dialogue-model?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/koala-dialogue-model</guid>
      <pubDate>2023-04-05 23:28</pubDate>
      <category>#ai</category>
      <category>#llm</category>
    </item>
    <item>
      <title>Testing tagged responses</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/test-tagged-response?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/test-tagged-response&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;My response&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/test-tagged-response?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/test-tagged-response</guid>
      <pubDate>2023-04-04 23:30</pubDate>
      <category>#webdev</category>
    </item>
    <item>
      <title>From Deep Learning Foundations to Stable Diffusion</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/deep-learning-foundations-stable-diffusion?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/deep-learning-foundations-stable-diffusion&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;From Deep Learning Foundations to Stable Diffusion...is part 2 of Practical Deep Learning for Coders.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In this course, containing over 30 hours of video content, we implement the astounding Stable Diffusion algorithm from scratch!&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/deep-learning-foundations-stable-diffusion?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/deep-learning-foundations-stable-diffusion</guid>
      <pubDate>2023-04-04 21:35</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Schillace Laws of Semantic AI</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/schillace-laws-semantic-ai?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/schillace-laws-semantic-ai&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The &amp;quot;Schillace Laws&amp;quot; were formulated after working with a variety of Large Language Model (LLM) AI systems to date. Knowing them will accelerate your journey into this exciting space of reimagining the future of software engineering.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/schillace-laws-semantic-ai?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/schillace-laws-semantic-ai</guid>
      <pubDate>2023-03-29 21:01</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Darklang is going all-in on AI</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/darklang-gpt?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/darklang-gpt&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;On February 1, we stopped working on what we're now calling &amp;quot;darklang-classic&amp;quot;, and are fully heads down on building &amp;quot;darklang-gpt&amp;quot;, which is the same core Darklang but redesigned to have AI as the primary (or possibly only) way of writing code.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/darklang-gpt?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/darklang-gpt</guid>
      <pubDate>2023-03-28 22:24</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>OpenFlamingo: An open-source framework for training vision-language models with in-context learning</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/laion-openflamingo-vision-llm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/laion-openflamingo-vision-llm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...OpenFlamingo, an open-source reproduction of DeepMind's Flamingo model. At its core, OpenFlamingo is a framework that enables training and evaluation of large multimodal models (LMMs).&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/laion-openflamingo-vision-llm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/laion-openflamingo-vision-llm</guid>
      <pubDate>2023-03-28 22:00</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/cerebras-gpt?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/cerebras-gpt&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Cerebras open sources seven GPT-3 models from 111 million to 13 billion parameters. Trained using the Chinchilla formula, these models set new benchmarks for accuracy and compute efficiency.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today‚Äôs release is designed to be used by and reproducible by anyone. All models, weights, and checkpoints are available on Hugging Face and GitHub under the Apache 2.0 license. Additionally, we provide detailed information on our training methods and performance results in our forthcoming paper. The Cerebras CS-2 systems used for training are also available on-demand via Cerebras Model Studio.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/cerebras-gpt?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/cerebras-gpt</guid>
      <pubDate>2023-03-28 21:53</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Prompt Engineering</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/prompt-engineering-weng?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/prompt-engineering-weng&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/prompt-engineering-weng?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/prompt-engineering-weng</guid>
      <pubDate>2023-03-21 21:47</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>GPT-4</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openai-gpt-4?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openai-gpt-4&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;GPT-4 can accept images as inputs and generate captions, classifications, and analyses.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;GPT-4 is capable of handling over 25,000 words of text, allowing for use cases like long form content creation, extended conversations, and document search and analysis.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openai-gpt-4?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openai-gpt-4</guid>
      <pubDate>2023-03-14 13:59</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Alpaca: A Strong Open-Source Instruction-Following Model</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/alpaca-model-stanford?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/alpaca-model-stanford&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We are releasing our findings about an instruction-following language model, dubbed Alpaca, which is fine-tuned from Meta‚Äôs LLaMA 7B model. We train the Alpaca model on 52K instruction-following demonstrations generated in the style of self-instruct using text-davinci-003. Alpaca shows many behaviors similar to OpenAI‚Äôs text-davinci-003, but is also surprisingly small and easy/cheap to reproduce.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/alpaca-model-stanford?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/alpaca-model-stanford</guid>
      <pubDate>2023-03-14 11:06</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>SPQA: The AI-based Architecture That‚Äôll Replace Most Existing Software</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/spqa-ai-based-architecture-replace-software?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/spqa-ai-based-architecture-replace-software&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;AI-based applications will be completely different than those we have today. The new architecture will be a far more elegant, four-component structure based around GPTs: State, Policy, Questions, and Action.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A security program using SPQA&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Choose the base model&lt;/strong&gt; ‚Äî You start with the latest and greatest overall GPT model from OpenAI, Google, Meta, McKinsey, or whoever. Lots of companies will have one. Let‚Äôs call it OpenAI‚Äôs GPT-6. It already knows so incredibly much about security, biotech, project management, scheduling, meetings, budgets, incident response, and audit preparedness that you might be able to survive with it alone. But you need more personalized context.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Train your custom model&lt;/strong&gt; ‚Äî Then you train your custom model which is based on your own data, which will stack on top of GPT-6. This is all the stuff in the STATE section above. It‚Äôs your company‚Äôs telemetry and context. Logs. Docs. Finances. Chats. Emails. Meeting transcripts. Everything. It‚Äôs a small company and there are compression algorithms as part of the Custom Model Generation (CMG) product we use, so it‚Äôs a total of 312TB of data. You train your custom model on that.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Train your policy model&lt;/strong&gt; ‚Äî Now you train another model that‚Äôs all about your company‚Äôs desires. The mission, the goals, your anti-goals, your challenges, your strategies. This is the guidance that comes from humans that we‚Äôre using to steer the ACTION part of the architecture. When we ask it to make stuff for us, and build out our plans, it‚Äôll do so using the guardrails captured here in the POLICY.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Tell the system to take the following actions&lt;/strong&gt; ‚Äî Now the models are combined. We have GPT-6, stacked with our STATE model, also stacked with our POLICY model, and together they know us better than we know ourselves.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/spqa-ai-based-architecture-replace-software?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/spqa-ai-based-architecture-replace-software</guid>
      <pubDate>2023-03-14 10:52</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Large language models are having their Stable Diffusion moment</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/large-language-models-stable-diffusion-moment?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/large-language-models-stable-diffusion-moment&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The race is on to release the first fully open language model that gives people ChatGPT-like capabilities on their own devices.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/large-language-models-stable-diffusion-moment?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/large-language-models-stable-diffusion-moment</guid>
      <pubDate>2023-03-14 10:43</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Access Your Bitwarden Vault Without a Password</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/access-bitwarden-vault-passwordless?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/access-bitwarden-vault-passwordless&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Bitwarden expands the Log in with device option that lets you use a second device to authenticate your Bitwarden vault login instead of using your Bitwarden password.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/access-bitwarden-vault-passwordless?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/access-bitwarden-vault-passwordless</guid>
      <pubDate>2023-02-23 11:20</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Introduction to Data-Centric AI</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/introduction-data-centric-ai-mit?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/introduction-data-centric-ai-mit&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Data-Centric AI (DCAI) is an emerging science that studies techniques to improve datasets, which is often the best way to improve performance in practical ML applications. While good data scientists have long practiced this manually via ad hoc trial/error and intuition, DCAI considers the improvement of data as a systematic engineering discipline.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This is the first-ever course on DCAI. This class covers algorithms to find and fix common issues in ML data and to construct better datasets, concentrating on data used in supervised learning tasks like classification. All material taught in this course is highly practical, focused on impactful aspects of real-world ML applications, rather than mathematical details of how particular models work. You can take this course to learn practical techniques not covered in most ML classes, which will help mitigate the ‚Äúgarbage in, garbage out‚Äù problem that plagues many real-world ML applications.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/introduction-data-centric-ai-mit?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/introduction-data-centric-ai-mit</guid>
      <pubDate>2023-02-23 10:25</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Software 2.0</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/software-2-0-karpathy?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/software-2-0-karpathy&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The ‚Äúclassical stack‚Äù of Software 1.0 is what we‚Äôre all familiar with ‚Äî it is written in languages such as Python, C++, etc...Software 2.0 is written in much more abstract, human unfriendly language, such as the weights of a neural network.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Benefits of Software 2.0&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Computationally homogeneous&lt;/li&gt;
&lt;li&gt;Simple to bake into silicon&lt;/li&gt;
&lt;li&gt;Constant running time&lt;/li&gt;
&lt;li&gt;Constant memory use&lt;/li&gt;
&lt;li&gt;Highly portable&lt;/li&gt;
&lt;li&gt;Agile&lt;/li&gt;
&lt;li&gt;Modules can meld into an optimal whole&lt;/li&gt;
&lt;li&gt;It is better than you&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Limitations of Software 2.0&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;At the end of the optimization we‚Äôre left with large networks that work well, but it‚Äôs very hard to tell how. Across many applications areas, we‚Äôll be left with a choice of using a 90% accurate model we understand, or 99% accurate model we don‚Äôt.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The 2.0 stack can fail in unintuitive and embarrassing ways ,or worse, they can ‚Äúsilently fail‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the existence of adversarial examples and attacks highlights the unintuitive nature of this stack.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Programming Software 2.0&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;If you recognize Software 2.0 as a new and emerging programming paradigm instead of simply treating neural networks as a pretty good classifier in the class of machine learning techniques, the extrapolations become more obvious, and it‚Äôs clear that there is much more work to do.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In particular, we‚Äôve built up a vast amount of tooling that assists humans in writing 1.0 code, such as powerful IDEs with features like syntax highlighting, debuggers, profilers, go to def, git integration, etc. In the 2.0 stack, the programming is done by accumulating, massaging and cleaning datasets. Who is going to develop the first Software 2.0 IDEs, which help with all of the workflows in accumulating, visualizing, cleaning, labeling, and sourcing datasets?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Github is a very successful home for Software 1.0 code. Is there space for a Software 2.0 Github? In this case repositories are datasets and commits are made up of additions and edits of the labels.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Traditional package managers and related serving infrastructure like pip, conda, docker, etc. help us more easily deploy and compose binaries. How do we effectively deploy, share, import and work with Software 2.0 binaries? What is the conda equivalent for neural networks?&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/software-2-0-karpathy?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/software-2-0-karpathy</guid>
      <pubDate>2023-02-23 10:14</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Zero-shot image-to-text generation with BLIP-2</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/blip-2-zero-shot-image-to-text-generation?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/blip-2-zero-shot-image-to-text-generation&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This guide introduces BLIP-2 from Salesforce Research that enables a suite of state-of-the-art visual-language models that are now available in ü§ó Transformers. We'll show you how to use it for image captioning, prompted image captioning, visual question-answering, and chat-based prompting.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;BLIP-2 bridges the modality gap between vision and language models by adding a lightweight Querying Transformer (Q-Former) between an off-the-shelf frozen pre-trained image encoder and a frozen large language model. Q-Former is the only trainable part of BLIP-2; both the image encoder and language model remain frozen.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/blip-2-zero-shot-image-to-text-generation?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/blip-2-zero-shot-image-to-text-generation</guid>
      <pubDate>2023-02-15 14:04</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>What Is ChatGPT Doing‚Ä¶and Why Does It Work?</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/what-is-chatgpt-why-it-works-wolfram?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/what-is-chatgpt-why-it-works-wolfram&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/what-is-chatgpt-why-it-works-wolfram?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/what-is-chatgpt-why-it-works-wolfram</guid>
      <pubDate>2023-02-14 23:30</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Attacking Machine Learning Systems</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/attacking-machine-learning-systems?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/attacking-machine-learning-systems&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;At their core, modern ML systems have complex mathematical models that use training data to become competent at a task. And while there are new risks inherent in the ML model, all of that complexity still runs in software. Training data are still stored in memory somewhere. And all of that is on a computer, on a network, and attached to the Internet. Like everything else, these systems will be hacked through vulnerabilities in those more conventional parts of the system.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/attacking-machine-learning-systems?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/attacking-machine-learning-systems</guid>
      <pubDate>2023-02-07 10:54</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>LMOps - Enabling AI capabilities with LLMs and Generative AI models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/microsoft-lmops-github?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/microsoft-lmops-github&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;LMOps is a research initiative on fundamental research and technology for building AI products w/ foundation models, especially on the general technology for enabling AI capabilities w/ LLMs and Generative AI models.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Better Prompts: Promptist, Extensible prompts&lt;/li&gt;
&lt;li&gt;Longer Context: Structured prompting, Length-Extrapolatable Transformers&lt;/li&gt;
&lt;li&gt;Knowledge Augmentation (TBA)&lt;/li&gt;
&lt;li&gt;Fundamentals&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/microsoft-lmops-github?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/microsoft-lmops-github</guid>
      <pubDate>2023-02-07 09:49</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Skype 8.93 Released</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/skype-8-93-release?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/skype-8-93-release&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;A few updates I found interesting.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Beauty of languages: You can now use your own voice during a translated call in Skype.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Universal translator: During a Skype call, if a participant speaks different languages, Skype Translator will automatically detect the languages and translate it for you.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Extra! Extra! Read all about it: Stay up-to-date with the latest news and trends.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Hit me up sometime: You can easily add contacts in Skype on mobile using a unique QR code to get connected.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I have a few thoughts on the &lt;a href="https://support.skype.com/faq/FA34951/what-is-the-today-tab-in-skype-and-how-do-i-use-it"&gt;Today&lt;/a&gt; feature and overall updates Skype has been receiving over the past few months but I'll leave those for another post.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/skype-8-93-release?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/skype-8-93-release</guid>
      <pubDate>2023-01-30 11:07</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Install Manjaro WSL - Windows 10 &amp; 11</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/windows-central-install-manjaro-wsl-guide?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/windows-central-install-manjaro-wsl-guide&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/windows-central-install-manjaro-wsl-guide?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/windows-central-install-manjaro-wsl-guide</guid>
      <pubDate>2023-01-29 14:56</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>WebmentionFs Send Test</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/webmentionfs-send-test?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/webmentionfs-send-test&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Testing WebmentionFs send integration. From Website.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/webmentionfs-send-test?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/webmentionfs-send-test</guid>
      <pubDate>2023-01-28 11:24</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Microsoft Edge 'Phoenix' is an internal reimagining of the Edge web browser with a new UI and more features</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/microsoft-edge-phoenix-browser?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/microsoft-edge-phoenix-browser&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;As someone who spends a significant amount of time in the browser, making Edge, more specifically the browser the platform for applications, this looks appealing. Especially when you consider Progressive Web App (PWA) support and integrations with the Windows Store.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/microsoft-edge-phoenix-browser?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/microsoft-edge-phoenix-browser</guid>
      <pubDate>2023-01-26 11:26</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Eternal Tumblr</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/hon-eternal-tumblr?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/hon-eternal-tumblr&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Automattic, the current owner of tumblr, one of the biggest meme mines in the world, has said they‚Äôre going to implement ActivityPub, which is the underlying protocol on which Mastodon operates. There are boring ways this could go, and then interesting ways this could go.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Interesting points. I'm looking forward to how this shakes out.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/hon-eternal-tumblr?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/hon-eternal-tumblr</guid>
      <pubDate>2023-01-25 19:26</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Mathematics for Machine Learning and Data Science</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/math-machine-learning-data-science-mooc?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/math-machine-learning-data-science-mooc&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Master the Toolkit of AI and Machine Learning. Mathematics for Machine Learning and Data Science is a beginner-friendly specialization where you‚Äôll learn the fundamental mathematics toolkit of machine learning: calculus, linear algebra, statistics, and probability.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/math-machine-learning-data-science-mooc?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/math-machine-learning-data-science-mooc</guid>
      <pubDate>2023-01-25 17:02</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>The Best Time to Own a Domain Was 20 Years Ago; The Second Best Time Is Today</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nielsen-best-time-to-own-domain?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nielsen-best-time-to-own-domain&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;if own your domain, create value there, and drive people to it, you‚Äôre paying ~$10 a year to build unbounded value over the years ‚Äî value you control.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;That is why owning a domain (and publishing your content there) is like planting a tree: it‚Äôs value that starts small and grows. The best time to own a domain and publish your content there was 20 years ago. The second best time is today.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I love a technology like &lt;code&gt;rel=me&lt;/code&gt; which pushes the idea of domain ownership into broader and broader spheres of society ‚Äî ‚ÄúHow do I get that nice little green checkmark on my profile?‚Äù It reinforces the value of owning your own domain (which you can verify elsewhere) and encourages citizens of the internet people to build value in their own little corners of the world wide web.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nielsen-best-time-to-own-domain?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nielsen-best-time-to-own-domain</guid>
      <pubDate>2023-01-25 10:29</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Bitwarden Open Source Security Explained</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/bitwarden-open-source-security-explained?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/bitwarden-open-source-security-explained&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This article answers three common questions about how being open source strengthens Bitwarden security, transparency, and privacy.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/bitwarden-open-source-security-explained?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/bitwarden-open-source-security-explained</guid>
      <pubDate>2023-01-25 10:26</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>BioProviders - .NET library for accessing and manipulating bioinformatic datasets</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/github-fsharp-bioproviders?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/github-fsharp-bioproviders&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/github-fsharp-bioproviders?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/github-fsharp-bioproviders</guid>
      <pubDate>2023-01-24 10:08</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Remote Sensing Datasets Repo</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ml-remote-sensing-datasets?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ml-remote-sensing-datasets&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ml-remote-sensing-datasets?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ml-remote-sensing-datasets</guid>
      <pubDate>2023-01-24 10:01</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>How to implement Q&amp;A against your documentation with GPT3, embeddings and Datasette</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/implement-q-a-documentation-gpt-willison?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/implement-q-a-documentation-gpt-willison&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Here's how to do this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run a text search (or a semantic search, described later) against your documentation to find content that looks like it could be relevant to the user's question.&lt;/li&gt;
&lt;li&gt;Grab extracts of that content and glue them all together into a blob of text.&lt;/li&gt;
&lt;li&gt;Construct a prompt consisting of that text followed by &amp;quot;Given the above content, answer the following question: &amp;quot; and the user's question&lt;/li&gt;
&lt;li&gt;Send the whole thing through the GPT-3 API and see what comes back&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/implement-q-a-documentation-gpt-willison?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/implement-q-a-documentation-gpt-willison</guid>
      <pubDate>2023-01-19 14:19</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>A Visual Guide to SSH Tunnels</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/visual-guide-ssh-tunnels?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/visual-guide-ssh-tunnels&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/visual-guide-ssh-tunnels?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/visual-guide-ssh-tunnels</guid>
      <pubDate>2023-01-18 10:52</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Karpathy - Let's build GPT: from scratch, in code, spelled out</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/karpathy-lets-build-gpt-from-scratch-video?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/karpathy-lets-build-gpt-from-scratch-video&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" title="Let's build GPT"&gt;&lt;img src="https://yewtu.be/vi/kCc8FmEb1nY/maxres.jpg" class="img-fluid" alt="Let's build GPT" /&gt;&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/karpathy-lets-build-gpt-from-scratch-video?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/karpathy-lets-build-gpt-from-scratch-video</guid>
      <pubDate>2023-01-18 10:45</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>A Big Pile of Personal Developer &amp; Designer Blogs in an OPML File</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/coyier-big-list-dev-designer-blogs-opml-file?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/coyier-big-list-dev-designer-blogs-opml-file&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;OPML File: &lt;a href="https://chriscoyier.s3.amazonaws.com/personal-developer-blogs.xml"&gt;https://chriscoyier.s3.amazonaws.com/personal-developer-blogs.xml&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/coyier-big-list-dev-designer-blogs-opml-file?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/coyier-big-list-dev-designer-blogs-opml-file</guid>
      <pubDate>2023-01-11 15:38</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Subscribe Wherever You Get Your Content</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/subscribe-wherever-you-get-your-content-nielsen?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/subscribe-wherever-you-get-your-content-nielsen&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Wouldn‚Äôt it be amazing if a similar phrase could enter the larger public consciousness for blogs or even people who you follow online?&lt;/p&gt;
&lt;p&gt;Instead of:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;‚ÄúFollow me on Twitter.‚Äù  
‚ÄúI‚Äôm @realChuckieCheese on Instagram.‚Äù  
‚ÄúSubscribe to my videos on YouTube.‚Äù  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You‚Äôd hear some popular influencer saying:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;‚ÄúFollow me wherever you follow people online.‚Äù  
‚ÄúFind me and subscribe wherever you get your content.‚Äù
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In a world like this, perhaps &lt;a href="https://twitter.com/jimniels/status/1597654715873267713"&gt;apex domains could become the currency of online handles&lt;/a&gt;: follow me &lt;code&gt;@jim-nielsen.com&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/subscribe-wherever-you-get-your-content-nielsen?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/subscribe-wherever-you-get-your-content-nielsen</guid>
      <pubDate>2023-01-11 10:43</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Building a website like it's 1999... in 2022</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/building-website-like-1999-in-2022-localghost?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/building-website-like-1999-in-2022-localghost&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Now the only challenges for me are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Should the song on my website match my ringtone?&lt;/li&gt;
&lt;li&gt;Where do I place the flames GIF?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src="/images/flames.gif" class="img-fluid" alt="Flames GIF" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/building-website-like-1999-in-2022-localghost?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/building-website-like-1999-in-2022-localghost</guid>
      <pubDate>2023-01-11 10:18</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Bring Back Blogging Directory</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/bring-back-blogs-directory?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/bring-back-blogs-directory&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;If you're looking for new folks follow, here's a list of blogs on a variety of topics.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/bring-back-blogs-directory?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/bring-back-blogs-directory</guid>
      <pubDate>2023-01-04 21:03</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Intel launches new entry-level Core i3 N-series mobile CPUs</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ces-intel-entry-level-n-series-processor?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ces-intel-entry-level-n-series-processor&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://cdn.mos.cms.futurecdn.net/kkWgG7UZh3ySjrniGmYmDk-1200-80.jpg" class="img-fluid" alt="Overview of Intel N-Series Processor Features" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;Intel&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The article suggests these processors are for the education market. Hopefully a few of the devices with these processors in a Surface Go form-factor are made generally available as well. I think the N200 series is the sweet spot in terms of performance and battery life with only 6W of max turbo power. The i3 which draws a max of 15W may be too much. At that point, some of the newly announced &lt;a href="https://www.windowscentral.com/hardware/computers-desktops/13th-gen-intel-core-mobile-cpus-announced-ces-2023"&gt;U-series processors&lt;/a&gt; might make more sense.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ces-intel-entry-level-n-series-processor?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ces-intel-entry-level-n-series-processor</guid>
      <pubDate>2023-01-04 00:28</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Personal Websites and Online Identities</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/personal-websites-and-online-identifies-hd?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/personal-websites-and-online-identifies-hd&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Great post.&lt;/p&gt;
&lt;p&gt;For me, I've found POSSE to Twitter and Mastodon using RSS and IFTTT-like services has been a relatively low-effort thing to do. The downside though is any follow-up conversations take place on those platforms. That hasn't been an issue for me yet though so until it is, I'll continue to use my relatively low-effort solution.&lt;/p&gt;
&lt;p&gt;I agree with you on likes / reposts. My current implementation of those posts is focused on Webmentions, which means it's virtually meaningless since few people know about let alone integrate Webmentions into their website. I would argue replies tend to fall into that same space as well. I have however found great use in bookmark posts to which I try and add some content from the source document to help me quickly identfy why I thought that post / website was interesting or relevant to a specific topic at that time.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/personal-websites-and-online-identifies-hd?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/personal-websites-and-online-identifies-hd</guid>
      <pubDate>2023-01-03 23:14</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>nanoGPT - The simplest, fastest repository for training/finetuning medium-sized GPTs.</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nanoGPT?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nanoGPT&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs. It's a re-write of minGPT, which I think became too complicated, and which I am hesitant to now touch. Still under active development, currently working to reproduce GPT-2 on OpenWebText dataset. The code itself aims by design to be plain and readable: train.py is a ~300-line boilerplate training loop and model.py a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nanoGPT?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nanoGPT</guid>
      <pubDate>2023-01-03 22:35</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Petals - Easy way to run 100B+ language models at home</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/petals-ml-model-inference?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/petals-ml-model-inference&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Run 100B+ language models at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/petals-ml-model-inference?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/petals-ml-model-inference</guid>
      <pubDate>2023-01-03 22:34</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Bring Back Personal Blogging</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/bring-back-personal-blogging?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/bring-back-personal-blogging&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In the beginning, there were blogs, and they were the original social web. We built community. We found our people. We wrote personally. We wrote frequently. We self-policed, and we linked to each other so that newbies could discover new and good blogs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The biggest reason personal blogs need to make a comeback is a simple one: we should all be in control of our own platforms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;People built entire communities around their favorite blogs, and it was a good thing. You could find your people, build your tribe, and discuss the things your collective found important.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Buy that domain name. Carve your space out on the web. Tell your stories, build your community, and talk to your people.¬†It doesn‚Äôt have to be big. It doesn‚Äôt have to be fancy. You don‚Äôt have to reinvent the wheel.¬†It doesn‚Äôt need to duplicate any space that already exists on the web ‚Äî in fact, it shouldn‚Äôt. This is your creation. It‚Äôs your expression. It should reflect you.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/bring-back-personal-blogging?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/bring-back-personal-blogging</guid>
      <pubDate>2023-01-03 22:20</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>OpenAI Text Embedding Ada 002 Model</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openai-text-embedding-ada-002?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openai-text-embedding-ada-002&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The new model, &lt;code&gt;text-embedding-ada-002&lt;/code&gt;, replaces five separate models for text search, text similarity, and code search, and outperforms our previous most capable model, Davinci, at most tasks, while being priced 99.8% lower.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openai-text-embedding-ada-002?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openai-text-embedding-ada-002</guid>
      <pubDate>2023-01-02 13:48</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>SWYX - Fave Podcasts 2022</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/swyx-favorite-podcasts-2022?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/swyx-favorite-podcasts-2022&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/swyx-favorite-podcasts-2022?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/swyx-favorite-podcasts-2022</guid>
      <pubDate>2023-01-02 12:44</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>ThinkPad Buyer's Guide</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/used-thinkpad-buyers-guide?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/used-thinkpad-buyers-guide&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/used-thinkpad-buyers-guide?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/used-thinkpad-buyers-guide</guid>
      <pubDate>2022-12-29 16:44</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>It‚Äôs Time to Get Back Into RSS</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/miessler-time-get-back-rss?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/miessler-time-get-back-rss&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We all mourned when Reader died and took RSS with it, but it's time to return to what made it great&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/miessler-time-get-back-rss?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/miessler-time-get-back-rss</guid>
      <pubDate>2022-12-28 10:50</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Umbrel - Personal Server OS</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/umbrel-personal-server-os-selfhosting?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/umbrel-personal-server-os-selfhosting&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Really cool project. Don't really care for the crypto stuff but the rest looks very interesting.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/umbrel-personal-server-os-selfhosting?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/umbrel-personal-server-os-selfhosting</guid>
      <pubDate>2022-12-28 10:27</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Transformers from Scratch</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/e2eml-transformers-from-scratch?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/e2eml-transformers-from-scratch&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/e2eml-transformers-from-scratch?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/e2eml-transformers-from-scratch</guid>
      <pubDate>2022-12-28 10:03</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Matrix Holiday Update</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/matrix-holiday-edition-2022?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/matrix-holiday-edition-2022&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;These updates are exciting!&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;a href="https://macaw.social/@wongmjane/109529583352532543"&gt;Reddit appears to be building out new Chat functionality&lt;/a&gt; using Matrix&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;a href="https://meta.discourse.org/t/matrix-protocol-for-chat/210780"&gt;Discourse&lt;/a&gt; is working on adding Matrix support&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;&lt;a href="https://www.theregister.com/2022/06/30/thunderbird_102"&gt;Thunderbird&lt;/a&gt; launched Matrix support.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Automattic is busy building &lt;a href="https://github.com/Automattic/chatrix"&gt;Matrix plugins for Wordpress&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Not as exciting&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...only a handful of these initiatives have resulted in funding reaching the core Matrix team. &lt;strong&gt;This is directly putting core Matrix development at risk.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In short: folks love the amazing decentralised encrypted comms utopia of Matrix. But organisations also love that they can use it without having to pay anyone to develop or maintain it. &lt;strong&gt;This is completely unsustainable&lt;/strong&gt;, and Element is now literally unable to fund the entirety of the Matrix Foundation on behalf of everyone else - and has had to lay off some of the folks working on the core team as a result.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In the interim, if you are an organisation who‚Äôs building on Matrix and you want the project to continue to flourish, please mail funding@matrix.org to discuss how you can support the foundations that you are depending on.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I'm looking forward to the future of the Matrix protocol, especially the &lt;a href="https://arewep2pyet.com/"&gt;P2P components&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/matrix-holiday-edition-2022?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/matrix-holiday-edition-2022</guid>
      <pubDate>2022-12-27 21:34</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Appropedia - Sustainability Wiki</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/appropedia-sustaibility-wiki?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/appropedia-sustaibility-wiki&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/appropedia-sustaibility-wiki?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/appropedia-sustaibility-wiki</guid>
      <pubDate>2022-12-27 14:54</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Jon Udell: Personal RSS aggregators</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/personal-rss-aggregators-udell?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/personal-rss-aggregators-udell&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/personal-rss-aggregators-udell?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/personal-rss-aggregators-udell</guid>
      <pubDate>2022-12-25 16:29</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Comprehensive Rust Course</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/comprehensive-rust-course?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/comprehensive-rust-course&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This is a four day Rust course developed by the Android team. The course covers the full spectrum of Rust, from basic syntax to advanced topics like generics and error handling. It also includes Android-specific content on the last day.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/comprehensive-rust-course?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/comprehensive-rust-course</guid>
      <pubDate>2022-12-22 11:37</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>PINE64 Announces the PineTab2 Linux Tablet</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/pine64-announces-pinetab-2-linux?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/pine64-announces-pinetab-2-linux&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;img src="https://c.tenor.com/R0d3sZ4fq6EAAAAC/money-dollars.gif" class="img-fluid" alt="Futurama Fry Take My Money GIF" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/pine64-announces-pinetab-2-linux?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/pine64-announces-pinetab-2-linux</guid>
      <pubDate>2022-12-21 01:20</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Norvig AlphaCode Review</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/norvig-pytudes-alphacode?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/norvig-pytudes-alphacode&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large language models have recently shown an ability to solve a variety of problems. In this notebook we consider programming problems (as solved by AlphaCode) and mathematics problems (as solved by Minerva). The questions we would like to get at are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the future, what role will these generative models play in assisting a programmer or mathematician?&lt;/li&gt;
&lt;li&gt;What will be a workflow that incorporates these models?&lt;/li&gt;
&lt;li&gt;How will other existing tools (such as programming languages) change to accomodate this workflow?&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/norvig-pytudes-alphacode?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/norvig-pytudes-alphacode</guid>
      <pubDate>2022-12-16 22:03</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Mastodon Hashtag RSS feeds</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mastodon-hashtag-rss-boffosocko?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mastodon-hashtag-rss-boffosocko&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Worked well for me. Thanks for sharing!&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mastodon-hashtag-rss-boffosocko?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mastodon-hashtag-rss-boffosocko</guid>
      <pubDate>2022-12-15 22:33</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>The GPT-3 Architecture, on a Napkin</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/gpt-3-architecture-napkin?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/gpt-3-architecture-napkin&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://dugas.ch/artificial_curiosity/img/GPT_architecture/fullarch.png" class="img-fluid" alt="Drawing of GPT-3 Architecture" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;dugas.ch&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/gpt-3-architecture-napkin?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/gpt-3-architecture-napkin</guid>
      <pubDate>2022-12-13 21:11</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Exploring Large Language Models with ChatGPT</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/twiml-exploring-llm-chatgpt?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/twiml-exploring-llm-chatgpt&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Today we're joined by ChatGPT, the latest and coolest large language model developed by OpenAl. In our conversation with ChatGPT, we discuss the background and capabilities of large language models, the potential applications of these models, and some of the technical challenges and open questions in the field. We also explore the role of supervised learning in creating ChatGPT, and the use of PPO in training the model. Finally, we discuss the risks of misuse of large language models, and the best resources for learning more about these models and their applications.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/twiml-exploring-llm-chatgpt?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/twiml-exploring-llm-chatgpt</guid>
      <pubDate>2022-12-12 08:22</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Glow | GitHub</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/glow-markdown-cli?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/glow-markdown-cli&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Render markdown on the CLI&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="https://camo.githubusercontent.com/bd591b74af8a6991894c8a84ab8d48f05ce7f66975b325d31f6954c836ddab27/68747470733a2f2f73747566662e636861726d2e73682f676c6f772f676c6f772d312e332d747261696c65722d6769746875622e676966" class="img-fluid" alt="Glow CLI Tool GIF" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;github.com/charmbracelet/glow&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/glow-markdown-cli?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/glow-markdown-cli</guid>
      <pubDate>2022-12-07 18:05</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>GitHub Copilot is generally available for businesses</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/gh-copilot-business-ga?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/gh-copilot-business-ga&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;GitHub Copilot for Business gives organizations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The power of AI&lt;/strong&gt;. Millions of developers have already used GitHub Copilot to build software faster, stay in the flow longer, and solve problems in new ways‚Äîall right from their editor of choice.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Simple license management&lt;/strong&gt;. Administrators can enable GitHub Copilot for their teams and select which organizations, teams, and developers receive licenses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Organization-wide policy management&lt;/strong&gt;. You can easily set policy controls to enforce user settings for public code matching on behalf of your organization.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Your code is safe with us&lt;/strong&gt;. With Copilot for Business, we won‚Äôt retain code snippets, store or share your code regardless if the data is from public repositories, private repositories, non-GitHub repositories, or local files.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/gh-copilot-business-ga?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/gh-copilot-business-ga</guid>
      <pubDate>2022-12-07 17:59</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Simulating the Wrapinator 5000</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/fsadvent-wrapinator-5000?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/fsadvent-wrapinator-5000&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Cool use of &lt;a href="https://diffsharp.github.io/"&gt;DiffSharp&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/fsadvent-wrapinator-5000?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/fsadvent-wrapinator-5000</guid>
      <pubDate>2022-12-07 16:19</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Microsoft Teams adds free communities feature</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/teams-communities-feature?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/teams-communities-feature&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;figure class="figure"&gt;
&lt;p&gt;&lt;img src="http://blogs.windows.com/wp-content/uploads/sites/2/2012/11/Family_5F00_Hub_5F00_16x9_5F00_753FF398.jpg" class="img-fluid" alt="Windows Phone Rooms Feature Screenshot" /&gt;&lt;/p&gt;
&lt;figcaption class="figure-caption"&gt;Source: &lt;em&gt;blogs.windows.com&lt;/em&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This reminds me so much of the &lt;a href="https://blogs.windows.com/windowsexperience/2012/11/29/how-i-use-rooms-on-windows-phone-8/"&gt;Rooms&lt;/a&gt; feature in Windows Phone. üò¢&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/teams-communities-feature?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/teams-communities-feature</guid>
      <pubDate>2022-12-07 15:50</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Waydroid</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/waydroid-linux?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/waydroid-linux&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A container-based approach to boot a full Android system on a regular GNU/Linux system like Ubuntu.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/waydroid-linux?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/waydroid-linux</guid>
      <pubDate>2022-12-06 22:29</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Geospatial is a function of your life </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/florence-fsharp-geospatial?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/florence-fsharp-geospatial&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Florence, a low-code geospatial library for everyone that ranks city places with (life-inspired) functions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Florence heavily laverages .NET Interactive/Polyglot Notebooks runtime to hide and execute code behind the scene (including breaking some language constraints).&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/florence-fsharp-geospatial?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/florence-fsharp-geospatial</guid>
      <pubDate>2022-12-06 10:43</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>EmacsConf 2022 Videos</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/emacsconf-2022-videos?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/emacsconf-2022-videos&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/emacsconf-2022-videos?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/emacsconf-2022-videos</guid>
      <pubDate>2022-12-06 10:25</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Microsoft Open-Sources Agricultural AI Toolkit FarmVibes.AI </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/msft-open-sources-farmvibes-ai?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/msft-open-sources-farmvibes-ai&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Microsoft Research recently open-sourced &lt;a href="https://www.microsoft.com/en-us/research/project/project-farmvibes/articles/farmvibes-ai/"&gt;FarmVibes.AI&lt;/a&gt;, a suite of ML models and tools for sustainable agriculture. FarmVibes.AI includes data processing workflows for fusing multiple sets of spatiotemporal and geospatial data, such as weather data and satellite and drone imagery.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/msft-open-sources-farmvibes-ai?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/msft-open-sources-farmvibes-ai</guid>
      <pubDate>2022-12-06 09:55</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Course: NLP Demystified</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nlp-demystified-course?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nlp-demystified-course&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nlp-demystified-course?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nlp-demystified-course</guid>
      <pubDate>2022-12-03 18:53</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Using Elfeed to view videos</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/elfeed-view-videos?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/elfeed-view-videos&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Slightly modified the original script to use &lt;a href="https://streamlink.github.io/"&gt;Streamlink&lt;/a&gt; and lower quality to 240p for bandwith and resource purposes.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-lisp"&gt;(require 'elfeed)

(defun elfeed-v-mpv (url)
  &amp;quot;Watch a video from URL in MPV&amp;quot; 
  (async-shell-command (format &amp;quot;streamlink -p mpv %s 240p&amp;quot; url)))

(defun elfeed-view-mpv (&amp;amp;optional use-generic-p)
  &amp;quot;Youtube-feed link&amp;quot;
  (interactive &amp;quot;P&amp;quot;)
  (let ((entries (elfeed-search-selected)))
    (cl-loop for entry in entries
     do (elfeed-untag entry 'unread)
     when (elfeed-entry-link entry) 
     do (elfeed-v-mpv it)) 
   (mapc #'elfeed-search-update-entry entries) 
   (unless (use-region-p) (forward-line)))) 

(define-key elfeed-search-mode-map (kbd &amp;quot;v&amp;quot;) 'elfeed-view-mpv)
&lt;/code&gt;&lt;/pre&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/elfeed-view-videos?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/elfeed-view-videos</guid>
      <pubDate>2022-12-02 21:58</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>ChatGPT: Optimizing Language Models for Dialogue</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/openai-chatgpt?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/openai-chatgpt&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests. ChatGPT is a sibling model to &lt;a href="https://openai.com/blog/instruction-following/"&gt;InstructGPT&lt;/a&gt;, which is trained to follow an instruction in a prompt and provide a detailed¬†response.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/openai-chatgpt?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/openai-chatgpt</guid>
      <pubDate>2022-12-01 11:12</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>How open source is revitalizing the payphone</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/philtel-open-source-payphone?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/philtel-open-source-payphone&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;PhilTel is a telephone collective based in Philadelphia, Pennsylvania, focusing on making communications accessible to everyone by installing free-to-use payphones. While you'll be able to make standard telephone calls through our phones, we're also focusing on offering interesting services or experiences. We don't want to only facilitate human-to-human interaction but also human-to-machine interaction and give people an environment where they can explore the telephone network and learn from it.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/philtel-open-source-payphone?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/philtel-open-source-payphone</guid>
      <pubDate>2022-11-30 10:04</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Coping strategies for the serial project hoarder</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/documentation-coping-serial-project-hoarder?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/documentation-coping-serial-project-hoarder&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Consider timezones: engineers in Madrid and engineers in San Francisco had almost no overlap in their working hours. Good asynchronous communication was essential.&lt;/p&gt;
&lt;p&gt;Over time, I noticed that the teams that were most effective at this scale were the teams that had a strong culture of documentation and automated testing.&lt;/p&gt;
&lt;p&gt;As I started to work on my own array of smaller personal projects, I found that the same discipline that worked for large teams somehow sped me up, when intuitively I would have expected it to slow me down.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/documentation-coping-serial-project-hoarder?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/documentation-coping-serial-project-hoarder</guid>
      <pubDate>2022-11-29 10:00</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>The Art of Command Line</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/art-of-command-line?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/art-of-command-line&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/art-of-command-line?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/art-of-command-line</guid>
      <pubDate>2022-11-24 20:43</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Stable Diffusion 2.0 Release</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/stable-diffusion-v2?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/stable-diffusion-v2&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Stable Diffusion 2.0 release includes robust text-to-image models trained using a brand new text encoder (OpenCLIP), developed by LAION with support from Stability AI, which greatly improves the quality of the generated images compared to earlier V1 releases.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Stable Diffusion 2.0 also includes an Upscaler Diffusion model that enhances the resolution of images by a factor of 4.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/stable-diffusion-v2?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/stable-diffusion-v2</guid>
      <pubDate>2022-11-24 20:39</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>The Perks of a High-Documentation, Low-Meeting Work Culture</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/perks-high-documentation-culture?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/perks-high-documentation-culture&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;the intangible, overarching benefit of practicing meeting mindfulness is this: you spend less of your day sort-of-listening and more of your day really thinking.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/perks-high-documentation-culture?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/perks-high-documentation-culture</guid>
      <pubDate>2022-11-23 22:12</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Database of nearly 500 million WhatsApp users‚Äô mobile phones is up for sale</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/whatsapp-data-leak-alternatives?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/whatsapp-data-leak-alternatives&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Just a friendly reminder, there are &lt;a href="/posts/alternatives-to-whatsapp"&gt;alternatives&lt;/a&gt;.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/whatsapp-data-leak-alternatives?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/whatsapp-data-leak-alternatives</guid>
      <pubDate>2022-11-23 15:00</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Tumblr to add support for ActivityPub</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tumblr-activitypub?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tumblr-activitypub&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Automattic CEO Matt Mullenweg ‚Äî whose company acquired Tumblr from Verizon in 2019 ‚Äî suggested...Tumblr... would soon add ...activitypub.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I'm perfectly happy using my site as the main place to post content but considering it's under new management and adopting open standards, it might be time to checkout out Tumblr again.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tumblr-activitypub?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tumblr-activitypub</guid>
      <pubDate>2022-11-23 14:49</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Digital Library of Amateur Radio &amp; Communications Surpasses 25,000 Items</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/digital-library-amateur-radio-25k?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/digital-library-amateur-radio-25k&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In the six weeks since announcing that Internet Archive has begun gathering content for the Digital Library of Amateur Radio and Communications (DLARC), the project has quickly grown to more than 25,000 items, including ham radio newsletters, podcasts, videos, books, and catalogs.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/digital-library-amateur-radio-25k?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/digital-library-amateur-radio-25k</guid>
      <pubDate>2022-11-21 23:06</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Telepathic technical writing</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/telepathic-tech-writing?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/telepathic-tech-writing&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...blog posts are always async, but they can lead to conversations and debates, once the reader is done reading. There's also the nature of blog posts being one-to-many, whereas chat is many-to-many, if done in a public channel. One-to-many forms of communication should generally be more formal, but can spread ideas and thoughts more coherently than many-to-many.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/telepathic-tech-writing?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/telepathic-tech-writing</guid>
      <pubDate>2022-11-21 10:29</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Digital Books wear out faster than Physical Books</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/digital-physical-books-wear?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/digital-physical-books-wear&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Our paper books have lasted hundreds of years on our shelves and are still readable. Without active maintenance, we will be lucky if our digital books last a decade.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/digital-physical-books-wear?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/digital-physical-books-wear</guid>
      <pubDate>2022-11-20 23:14</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Feta - Matrix Server Distribution</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/feta-matrix-raspi?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/feta-matrix-raspi&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Feta is a Matrix server distribution for the Raspberry Pi 3 and 4.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This looks like a great project to get started with self-hosting and join the Matrix network. Having something similar for the fediverse would be great as well. I've hosted Matrix and Mastodon servers on a Raspberry Pi before so I know it's up to the task.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/feta-matrix-raspi?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/feta-matrix-raspi</guid>
      <pubDate>2022-11-16 22:29</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Use RSS for privacy and efficiency</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/rss-privacy-efficiency?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/rss-privacy-efficiency&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/rss-privacy-efficiency?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/rss-privacy-efficiency</guid>
      <pubDate>2022-11-16 00:08</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Two Decades of Blogging, The Free As A Bird Edition</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/two-decades-blogging-free-bird-edition?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/two-decades-blogging-free-bird-edition&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/two-decades-blogging-free-bird-edition?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/two-decades-blogging-free-bird-edition</guid>
      <pubDate>2022-11-05 16:52</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>It looks like I‚Äôm moving to Mastodon</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/willison-mastodon?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/willison-mastodon&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/willison-mastodon?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/willison-mastodon</guid>
      <pubDate>2022-11-05 15:42</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>How to Leave Dying Social Media Platforms</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/how-to-leave-dying-social-platforms?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/how-to-leave-dying-social-platforms&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/how-to-leave-dying-social-platforms?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/how-to-leave-dying-social-platforms</guid>
      <pubDate>2022-10-30 12:18</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>alt.interoperability.adversarial</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/alt-interoperability-adversarial?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/alt-interoperability-adversarial&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/alt-interoperability-adversarial?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/alt-interoperability-adversarial</guid>
      <pubDate>2022-10-30 12:16</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>A Proposal for News Organization Mastodon Servers and More</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/proposal-news-mastodon-servers?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/proposal-news-mastodon-servers&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Here‚Äôs how it works in practice:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A news organization (or any organization, let‚Äôs just start with news) already asserts ownership of its domain e.g. via its certificate, so we piggyback trust off its domain.&lt;/li&gt;
&lt;li&gt;It stands up a Mastodon or other social server at a standard address. I‚Äôd propose follow.washingtonpost.com but there‚Äôs a bunch of reasons why you might do something else, see below, and uses the agreed well-known autodiscovery protocol to return the address for its Mastodon server (but I don‚Äôt see an entry for activitypub or Mastodon yet).&lt;/li&gt;
&lt;li&gt;It creates accounts for its staff on its Mastodon server. Nobody else gets an account; the public can‚Äôt sign up.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;What you get:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Verified accounts. Instead of relying on a third party to ‚Äúverify‚Äù your account by sticking a blue check against your display name, account provenance and ‚Äúverification‚Äù is inherited as a property of the Mastodon server....&lt;/li&gt;
&lt;li&gt;Ease of discovery...all a user would have to do, to find Washington Post accounts to follow, would be to know the washingtonpost.com domain. Autodiscovery would let your Mastodon client point itself to the appropriate server.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Not just news organizations...anyone can set up a Mastodon server...the federation means that ‚Äúofficial‚Äù accounts become ‚Äúmore official‚Äù when their server home is hung off the domain of the actual organization.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;You wouldn‚Äôt need Twitter (or anyone else, really) to verify that the UK Prime Minister‚Äôs account is official, because you‚Äôd have following.gov.uk as the Mastodon server, which means you can trust that server as much as you trust .gov.uk domains.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Your university or college wants you to have a social media account? Sure, you can have it hosted at following.ucla.edu.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;And yes, brands can get in on it. Sure. That way there‚Äôs a tiny chance you‚Äôre following the Proper Brand Account rather than a Parody Brand Account, which‚Ä¶ is probably for the best. Or it‚Äôs easier to see that a Parody Account is a Parody Account because you can look at the parent server.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/proposal-news-mastodon-servers?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/proposal-news-mastodon-servers</guid>
      <pubDate>2022-10-28 21:07</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>How to communicate effectively as a developer</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/communicating-effectively-developer?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/communicating-effectively-developer&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Communicating effectively as an engineer means empathically increasing the resolution of your writing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...‚Äúlow-resolution writing‚Äù...There is very little context, too much reliance on pronouns and unclear references. The writing is not emphatic ‚Äîthe reader has to spend extra energy to work out what is being said&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Longer-form writing gives you an opportunity to dive deeper into why you are saying what you are saying. It is a chance to educate, to teach, to help understand and to level up.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The quality of the API documentation will carry an astronomical amount of leverage. This leverage will work in both directions. Genuinely helpful documentation is the difference between being swamped by support requests from frustrated API users and significantly increasing the usage of your service. Happy users beget more happy users.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Spoken words get forgotten. Written words are shared, preserved, and become the basis of a company's culture. &lt;a href="https://twitter.com/david_perell/status/1584933536439881728"&gt;source&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;High resolution, empathic writing...You will have to spend more energy to make your writing easy to follow. You will have to grapple with your own confusion and holes in your understanding. You will have to figure out what the appropriate density for your writing is.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;It's not about you, though.. It's about them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Not only does a single recipient benefit from your extra effort, what if ten people read your good work? A hundred? A thousand? What if the company CEO reads it? Taking writing seriously at work or in your organisation and putting in the effort to delight the reader will, over time, compound into a massive body of quality writing that benefits everyone. It is a literal win-win-win&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Produce writing you would read with delight if you were on the other end.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/communicating-effectively-developer?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/communicating-effectively-developer</guid>
      <pubDate>2022-10-28 20:57</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Why I use Jellyfin for my home media library</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/jellyfin-home-media-library?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/jellyfin-home-media-library&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;It's also built with .NET üôÇ&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/jellyfin-home-media-library?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/jellyfin-home-media-library</guid>
      <pubDate>2022-10-27 20:13</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>What work looks like</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/what-work-looks-like?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/what-work-looks-like&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/what-work-looks-like?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/what-work-looks-like</guid>
      <pubDate>2022-10-25 23:01</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Suspension</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ott-suspension?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ott-suspension&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;write your most important thoughts on your own site. You can share the link on as many platforms as you like and have conversations with anyone who wants to connect with you and your work. But nobody can take it from you. You are in control. Forever.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ott-suspension?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ott-suspension</guid>
      <pubDate>2022-10-25 22:44</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Android 12L for Surface Duo is now available</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/surface-duo-android-12L?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/surface-duo-android-12L&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Downloading now.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/surface-duo-android-12L?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/surface-duo-android-12L</guid>
      <pubDate>2022-10-24 21:58</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>PyTorch Joins Linux Foundation as Top Level Project</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/pytorch-lf-top-level-project?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/pytorch-lf-top-level-project&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;PyTorch is moving to the Linux Foundation (LF) as a top-level project under the name PyTorch Foundation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The PyTorch Technical Governance now supports a hierarchical maintainer structure and clear outlining of processes around day to day work and escalations. This doesn‚Äôt change how we run things, but it does add discipline and openness that at our scale feels essential and timely.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/pytorch-lf-top-level-project?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/pytorch-lf-top-level-project</guid>
      <pubDate>2022-10-20 01:07</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Pocket Casts Mobile Apps Are Now Open Source</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/pocket-casts-mobile-open-source?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/pocket-casts-mobile-open-source&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We‚Äôve been eager to take this step since we joined Automattic last year...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Wow. Yet another company I didn't know was owned by Automaticc (parent company of WordPress). So not only do they have a stake in regular blogs and websites, but they also are in the microblogging space with Tumblr. With Pocket Casts they're now into podcasts too. Good for them.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;We believe that podcasting can not and should not be controlled by Apple and Spotify, and instead support a diverse ecosystem of third-party clients.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I couldn't agree more. Though to be fair to Apple, in all the years since podcasts have been a thing, despite them being one of the main indices, they didn't make any overt attempts to lock down the ecosystem. It's not until companies like Amazon and Spotify tried to make certain content platform exclusives that the ecosystem has started to feel more closed.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/pocket-casts-mobile-open-source?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/pocket-casts-mobile-open-source</guid>
      <pubDate>2022-10-20 00:23</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>From Deep Learning Foundations to Stable Diffusion</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/fastai-from-dl-stable-diffusion?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/fastai-from-dl-stable-diffusion&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;In total, we‚Äôre releasing four videos, with around 5.5 hours of content, covering the following topics (the lesson numbers start at ‚Äú9‚Äù, since this is a continuation of Practical Deep Learning for Coders part 1, which had 8 lessons):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lesson 9 by Jeremy Howard: How to use Diffusers pipelines; What are the conceptual parts of Stable Diffusion&lt;/li&gt;
&lt;li&gt;Lesson 9A by Jonathan Whitaker: A deep dive into Stable Diffusion concepts and code&lt;/li&gt;
&lt;li&gt;Lesson 9B by Wasim Lorgat and Tanishq Abraham: The math of diffusion&lt;/li&gt;
&lt;li&gt;Lesson 10 by Jeremy Howard: Creating a custom diffusion pipeline; Starting ‚Äúfrom the foundations‚Äù&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/fastai-from-dl-stable-diffusion?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/fastai-from-dl-stable-diffusion</guid>
      <pubDate>2022-10-19 23:52</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Thoughts on Bluesky AT Protocol</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/staltz-thoughts-bluesky-at-protocol?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/staltz-thoughts-bluesky-at-protocol&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;My experience working on SSB (i.e. non-crypto fully decentralized protocol) is suitable for small world communication, and definitely not suitable for big world. Email proves that federation can do big world.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/staltz-thoughts-bluesky-at-protocol?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/staltz-thoughts-bluesky-at-protocol</guid>
      <pubDate>2022-10-19 09:56</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>A Brief History &amp; Ethos of the Digital Garden</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/appleton-garden-history?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/appleton-garden-history&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The conversational feed design of email inboxes, group chats, and InstaTwitBook is fleeting ‚Äì they're only concerned with self-assertive immediate thoughts that rush by us in a few moments...But streams only surface the Zeitgeisty ideas of the last 24 hours...Gardens present information in a richly linked landscape that grows slowly over time...The garden helps us move away from time-bound streams and into contextual knowledge spaces.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Six Patterns of Gardening:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Topography over Timelines - Gardens are organised around contextual relationships and associative links; the concepts and themes within each note determine how it's connected to others.&lt;/li&gt;
&lt;li&gt;Continuous Growth - Gardens are never finished, they're constantly growing, evolving, and changing.&lt;/li&gt;
&lt;li&gt;Imperfection &amp;amp; Learning in Public - Gardens are imperfect by design. They don't hide their rough edges or claim to be a permanent source of truth.&lt;/li&gt;
&lt;li&gt;Playful, Personal, and Experimental - Gardens are non-homogenous by nature. You can plant the same seeds as your neighbour, but you'll always end up with a different arrangement of plants.&lt;/li&gt;
&lt;li&gt;Intercropping &amp;amp; Content Diversity - Gardens are not just a collection of interlinked words...Podcasts, videos, diagrams, illustrations, interactive web animations, academic papers, tweets, rough sketches, and code snippets should all live and grow in the garden.&lt;/li&gt;
&lt;li&gt;Independent Ownership - Gardening is about claiming a small patch of the web for yourself, one you fully own and control... If you give it a bit of forethought, you can build your garden in a way that makes it easy to transfer and adapt. Platforms and technologies will inevitably change. Using old-school, reliable, and widely used web native formats like HTML/CSS is a safe bet.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/appleton-garden-history?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/appleton-garden-history</guid>
      <pubDate>2022-10-19 00:03</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Bluesky - The Authenticated Transport (AT) Protocol</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/bluesky-at-protocol?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/bluesky-at-protocol&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Bluesky was created to build a social protocol. In the spring, we released ‚ÄúADX,‚Äù the very first iteration of the protocol...ADX is now the ‚ÄúAuthenticated Transport Protocol&amp;quot;...more simply, the ‚ÄúAT Protocol.‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The ‚ÄúAT Protocol‚Äù is a new federated social network&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;What makes AT Protocol unique:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Account portability&lt;/li&gt;
&lt;li&gt;Algorithmic choice&lt;/li&gt;
&lt;li&gt;Interoperation&lt;/li&gt;
&lt;li&gt;Performance&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://atproto.com/"&gt;AT Procol website&lt;/a&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/bluesky-at-protocol?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/bluesky-at-protocol</guid>
      <pubDate>2022-10-18 17:23</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Overdue insights on daily blogging</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/overdue-insights-daily-blogging?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/overdue-insights-daily-blogging&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Here are a bunch of not-so-obvious lessons I‚Äôve internalized through writing each day:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Writing can be a starting point, not an ending one&lt;/li&gt;
&lt;li&gt;Write to think&lt;/li&gt;
&lt;li&gt;The power of DIFY: Do it for yourself, don‚Äôt think too much about what you want other people to get out of it.&lt;/li&gt;
&lt;li&gt;Think small&lt;/li&gt;
&lt;li&gt;Gain energy&lt;/li&gt;
&lt;li&gt;A lot of books are collections of blog posts&lt;/li&gt;
&lt;li&gt;Small is an unblocker&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/overdue-insights-daily-blogging?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/overdue-insights-daily-blogging</guid>
      <pubDate>2022-10-18 17:20</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Patterns for collaborative blogging</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/patterns-collaborative-blogging?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/patterns-collaborative-blogging&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/patterns-collaborative-blogging?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/patterns-collaborative-blogging</guid>
      <pubDate>2022-10-18 17:19</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Bidirectional Attention Flow (BiDAF) ML.NET ONNX Sample</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/bidaf-mlnet-onnx?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/bidaf-mlnet-onnx&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This sample shows how to use a pretrained Bidirectional Attention Flow (BiDAF) ONNX model in ML.NET for answering a query about a given context paragraph.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/bidaf-mlnet-onnx?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/bidaf-mlnet-onnx</guid>
      <pubDate>2022-10-18 17:16</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>The Illustrated Stable Diffusion</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/illustrated-stable-diffusion?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/illustrated-stable-diffusion&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/illustrated-stable-diffusion?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/illustrated-stable-diffusion</guid>
      <pubDate>2022-10-18 09:25</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>HTTP Archive State of the Web 2022 Report</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/http-archive-state-of-web-2022-almanac?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/http-archive-state-of-web-2022-almanac&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The Web Almanac is a comprehensive report on the state of the web, backed by real data and trusted web experts. The 2022 edition is comprised of 23 chapters spanning aspects of page content, user experience, publishing, and distribution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://almanac.httparchive.org/en/2022/seo"&gt;Chapter 10: SEO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://almanac.httparchive.org/en/2022/privacy"&gt;Chapter 13: Privacy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://almanac.httparchive.org/en/2022/pwa"&gt;Chapter 17: PWA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://almanac.httparchive.org/en/2022/jamstack"&gt;Chapter 19: Jamstack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://almanac.httparchive.org/en/2022/page-weight"&gt;Chapter 21: Page Weight&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/http-archive-state-of-web-2022-almanac?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/http-archive-state-of-web-2022-almanac</guid>
      <pubDate>2022-10-18 09:20</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>The Illustrated GPT-2</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/illustrated-gpt-2?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/illustrated-gpt-2&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/illustrated-gpt-2?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/illustrated-gpt-2</guid>
      <pubDate>2022-10-17 19:03</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Readium LCP - Digital Rights Management Technology</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/readium-lcp-drm-technology?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/readium-lcp-drm-technology&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Readium LCP was developed five years ago to protect digital files from unauthorized distribution. Unlike proprietary platforms, the technology is open to anyone who wants to look inside the codebase and make improvements. It is a promising alternative for libraries and users wanting to avoid the limitations of traditional DRM.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/readium-lcp-drm-technology?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/readium-lcp-drm-technology</guid>
      <pubDate>2022-10-16 11:14</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Low Earth Orbit Visualization</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/leo-visualization?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/leo-visualization&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/leo-visualization?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/leo-visualization</guid>
      <pubDate>2022-10-14 21:57</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Discu.eu - Blog comments on a static site via social networks</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/discueu-comments-static-page?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/discueu-comments-static-page&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;discu.eu, a fantastic service that you provide with an URL and then gives back results from various social networks. Places where that URL is discussed. Hackernews, Reddit, and/or Lobsters. It has an API, that I can call with some JavaScript and then insert in the page.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/discueu-comments-static-page?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/discueu-comments-static-page</guid>
      <pubDate>2022-10-14 15:41</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Matrix protocol added to ejabberd</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ejabberd-matrix-protocol?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ejabberd-matrix-protocol&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ejabberd-matrix-protocol?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ejabberd-matrix-protocol</guid>
      <pubDate>2022-10-14 15:39</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Identity - The Hell≈ç Cooperative</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/hello-decentralized-identity-coop?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/hello-decentralized-identity-coop&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/hello-decentralized-identity-coop?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/hello-decentralized-identity-coop</guid>
      <pubDate>2022-10-14 15:36</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Swiss Dwellings: A large dataset of apartment models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/swiss-dwelling-apartment-dataset?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/swiss-dwelling-apartment-dataset&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This dataset contains detailed data on 42,207 apartments (242,257 rooms) in 3,093 buildings including their geometries, room typology as well as their visual, acoustical, topological and daylight characteristics.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/swiss-dwelling-apartment-dataset?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/swiss-dwelling-apartment-dataset</guid>
      <pubDate>2022-10-14 15:33</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Understanding HTML with Large Language Models</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/understanding-html-large-language-models?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/understanding-html-large-language-models&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding...have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Out of the LLMs we evaluate, we show evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/understanding-html-large-language-models?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/understanding-html-large-language-models</guid>
      <pubDate>2022-10-14 15:29</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Kaggle - State of Data Science and Machine Learning 2022</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/kaggle-state-data-science-ml-2022?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/kaggle-state-data-science-ml-2022&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h2&gt;Development&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Python and SQL remain the two most common programming skills for data scientists&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;VSCode is now used by over 50% of working data scientists&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Notebooks are a popular environment as well.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Colab notebooks are the most popular cloud-based Jupyter notebook environment&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Makes sense especially since Kaggle is owned by Google.&lt;/p&gt;
&lt;h2&gt;Machine Learning&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Kaggle DS &amp;amp; ML Survey 2022 Scikit-learn is the most popular ML framework while PyTorch has been growing steadily year-over-year&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;LightGBM, XGBoost are also among the top frameworks.&lt;/p&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Transformer architectures are becoming more popular for deep learning models (both image and text data)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Cloud computing&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;All major cloud computing providers saw strong year over year growth in 2022&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Specialized hardware like Tensor Processing Units (TPUs) is gaining initial traction with Kaggle data scientists&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://storage.googleapis.com/kaggle-media/surveys/Kaggle%20State%20of%20Machine%20Learning%20and%20Data%20Science%20Report%202022.pdf"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/c/kaggle-survey-2022/data"&gt;Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/kaggle-state-data-science-ml-2022?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/kaggle-state-data-science-ml-2022</guid>
      <pubDate>2022-10-14 13:59</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Streaming services lost the plot</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/streaming-companies-lost-plot?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/streaming-companies-lost-plot&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I got serious about consolidating the media my family consumes; I decided to buy blu-ray and DVD copies of all the movies and TV shows we actually cared about, and rip them onto a NAS.&lt;/p&gt;
&lt;p&gt;I'm glad I've gone down this path, and become more intentional about my media consumption, especially as companies are deleting already-purchased content from users' media libraries! It's sickening (and, IMHO, should be illegal) they can show a 'buy' button for a DRM'ed digital downloads that the user never actually 'owns'.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Couldn't agree more. Although the focus of this post is on video, you can say the same for music and books.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/streaming-companies-lost-plot?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/streaming-companies-lost-plot</guid>
      <pubDate>2022-10-13 15:06</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Element - New app layout for Android</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/matrix-element-android-redesign-2022?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/matrix-element-android-redesign-2022&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/matrix-element-android-redesign-2022?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/matrix-element-android-redesign-2022</guid>
      <pubDate>2022-10-13 10:32</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Doc Quixote</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/doc-quixote?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/doc-quixote&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;I spent great time, energy and money, over many years to create the writing and programming environment I wanted to use and I wanted my peers to use, so we could work together to create species-saving communication tools, and just beauty...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...I read the story of David Bowie's last days, he did something amazing when he knew he had a short time to live. He stepped back and got out of the way. He understood this is no longer his world.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;When you're young, you think expansively, and as you get old reality sinks in and your imagination contracts. The horizon gets closer and closer. We don't get to mold the world, we are not gods, no matter how good or generous, smart of ruthless you may be, we all start out young and if we're lucky we get old and then we're gone.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/doc-quixote?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/doc-quixote</guid>
      <pubDate>2022-10-12 23:30</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>State of AI Report 2022 </title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/2022-state-of-ai-report?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/2022-state-of-ai-report&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Key takeaways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;New independent research labs are rapidly open sourcing the closed source output of major labs.&lt;/li&gt;
&lt;li&gt;Safety is gaining awareness among major AI research entities&lt;/li&gt;
&lt;li&gt;The China-US AI research gap has continued to widen&lt;/li&gt;
&lt;li&gt;AI-driven scientific research continues to lead to breakthroughs&lt;/li&gt;
&lt;/ol&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/2022-state-of-ai-report?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/2022-state-of-ai-report</guid>
      <pubDate>2022-10-12 19:07</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Advanced Programming in the UNIX Environment</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/cs631-advanced-programming-unix-env?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/cs631-advanced-programming-unix-env&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;In this course, students will learn to develop complex system-level software in the C programming language while gaining an intimate understanding of the Unix operating system (and all OS that belong to this family, such as Linux, the BSDs, and even Mac OS X) and its programming environment.&lt;/p&gt;
&lt;p&gt;Topics covered will include the user/kernel interface, fundamental concepts of Unix, user authentication, basic and advanced I/O, fileystems, signals, process relationships, and interprocess communication. Fundamental concepts of software development and maintenance on Unix systems (development and debugging tools such as &amp;quot;make&amp;quot; and &amp;quot;gdb&amp;quot;) will also be covered.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/cs631-advanced-programming-unix-env?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/cs631-advanced-programming-unix-env</guid>
      <pubDate>2022-10-12 18:29</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>How Wine works 101</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/how-wine-works?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/how-wine-works&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/how-wine-works?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/how-wine-works</guid>
      <pubDate>2022-10-12 18:25</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Sane - Infopunk's Digital Garden</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/sane-app?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/sane-app&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/sane-app?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/sane-app</guid>
      <pubDate>2022-10-11 22:42</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Adding fragmention links to my website</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/jamesg-fragmentation-links?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/jamesg-fragmentation-links&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/jamesg-fragmentation-links?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/jamesg-fragmentation-links</guid>
      <pubDate>2022-10-11 21:40</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Researchers Publish Results of NLP Community Metasurvey</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nlp-metasurvey-results?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nlp-metasurvey-results&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Relevant links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://nlpsurvey.net/"&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://nlpsurvey.net/nlp-metasurvey-results.pdf"&gt;Report&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nlp-metasurvey-results?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nlp-metasurvey-results</guid>
      <pubDate>2022-10-11 20:38</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled neural networks</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/impnet-undetectable-backdoors-compiled-nn?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/impnet-undetectable-backdoors-compiled-nn&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...backdoors can be added during compilation, circumventing any safeguards in the data preparation and model training stages.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;some backdoors, such as ImpNet, can only be reliably detected at the stage where they are inserted and removing them anywhere else presents a significant challenge.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;machine-learning model security requires assurance of provenance along the entire technical pipeline, including the data, model architecture, compiler, and hardware specification.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/impnet-undetectable-backdoors-compiled-nn?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/impnet-undetectable-backdoors-compiled-nn</guid>
      <pubDate>2022-10-11 20:22</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Indieweb Bookmark Post</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/indieweb-bookmark-post?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/indieweb-bookmark-post&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A bookmark (or linkblog) is a post that is primarily comprised of a URL, often title text from that URL, sometimes optional text describing, tagging, or quoting from its contents.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Bookmarks are useful for saving things to read later or build a recommended reading list.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/indieweb-bookmark-post?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/indieweb-bookmark-post</guid>
      <pubDate>2022-10-10 20:39</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Rediscover the Joy in Coding: learn F#!</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/manning-rediscover-joy-coding-fsharp?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/manning-rediscover-joy-coding-fsharp&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/manning-rediscover-joy-coding-fsharp?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/manning-rediscover-joy-coding-fsharp</guid>
      <pubDate>2022-10-10 18:28</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Can Indie Social Media Save Us?</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/indie-social-media-save-us?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/indie-social-media-save-us&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/indie-social-media-save-us?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/indie-social-media-save-us</guid>
      <pubDate>2022-10-09 22:09</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Handwritten Blog</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/handwrittern-blog?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/handwrittern-blog&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/handwrittern-blog?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/handwrittern-blog</guid>
      <pubDate>2022-10-09 14:25</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Things Learned Blogging</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/things-learned-blogging?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/things-learned-blogging&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Both this post as well as &lt;a href="https://macwright.com/2019/02/06/how-to-blog.html"&gt;Tom MacWright's &amp;quot;How to Blog&amp;quot;&lt;/a&gt; resonate. I've been posting more consistently to my microblog feeds. These posts are more informal, but just the aspect of producing something even if it's just a short snippet is gratifying. The informality of it makes the posts significantly shorter but the pace at which I publish content and share ideas is faster.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/things-learned-blogging?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/things-learned-blogging</guid>
      <pubDate>2022-10-06 15:50</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Jim Nielsen: My Favorite RSS Feeds</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nielsen-favorite-rss-feeds?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nielsen-favorite-rss-feeds&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;It's always fun to stumble upon these lists and finding more interesting people and websites to follow.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nielsen-favorite-rss-feeds?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nielsen-favorite-rss-feeds</guid>
      <pubDate>2022-10-06 15:35</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Numerical F#nalysis 01:Introduction</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/numerical-fnalysis-intro?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/numerical-fnalysis-intro&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/numerical-fnalysis-intro?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/numerical-fnalysis-intro</guid>
      <pubDate>2022-10-06 09:24</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>F# with Python - TensorFlow Binding</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/fsharp-python-tf-binding?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/fsharp-python-tf-binding&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/fsharp-python-tf-binding?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/fsharp-python-tf-binding</guid>
      <pubDate>2022-10-06 09:21</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Unredacted Magazine - Issue 004</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/unredacted-mag-issue-4?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/unredacted-mag-issue-4&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/unredacted-mag-issue-4?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/unredacted-mag-issue-4</guid>
      <pubDate>2022-10-05 20:13</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Locations/Venues - IndieWebCamp Berlin 2022</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/indiewebcamp-berlin-2022-location-venues?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/indiewebcamp-berlin-2022-location-venues&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Interesting discussion at about 21:30 on a federated wiki / review aggregator.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/indiewebcamp-berlin-2022-location-venues?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/indiewebcamp-berlin-2022-location-venues</guid>
      <pubDate>2022-10-05 15:52</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>The OG Social Network: Other People's Websites</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/og-social-network-other-peoples-websites?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/og-social-network-other-peoples-websites&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Google+ was Google trying to mimic the walled garden of Facebook ‚Äî their ‚Äúhow‚Äù of extracting value from the people of the internet. But they already had an answer for Facebook‚Äôs News Feed in front of them: Blogger / Google Reader, the read / write of the internet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;They provide the tools ‚Äì Reader, Blogger, Search ‚Äî we provide the personal websites. The open, accessible, indexable web as The Next Great Social Network.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/og-social-network-other-peoples-websites?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/og-social-network-other-peoples-websites</guid>
      <pubDate>2022-10-05 15:25</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Patching the open web</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/patching-open-web?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/patching-open-web&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I've been doing something similar to readlists, except with RSS feeds. I create a &lt;a href="https://manual.calibre-ebook.com/news.html"&gt;custom recipe in Calibre&lt;/a&gt; which pulls and aggregates the 50 latest articles for each feed in my recipe. I limit it to only look at articles published in the past day since I do this every evening. Think of it like the evening paper. The result is an EPUB file which I then import into my e-book reader.&lt;/p&gt;
&lt;p&gt;A few advantages I've found to doing this are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I take a break from the computer.&lt;/li&gt;
&lt;li&gt;Since the e-book reader is offline, I focus on reading the article and don't have the option to click on links and get distracted browsing the internet.&lt;/li&gt;
&lt;li&gt;Since I'm already using the e-book reader, it's easy to transition to reading a book.&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/patching-open-web?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/patching-open-web</guid>
      <pubDate>2022-10-05 10:46</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Minimalism as Narcissism</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/minimalism-narcissism?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/minimalism-narcissism&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/minimalism-narcissism?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/minimalism-narcissism</guid>
      <pubDate>2022-10-03 20:56</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Stellarium 1.0 Released</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/stellarium-1-0?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/stellarium-1-0&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Can't believe this is the first time I'd heard of this desktop app. Usually using mobile apps like Sky Guide is convenient when on the go, but Stellarium not only seems to have lots of information, but also cross-platform and web-based.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/stellarium-1-0?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/stellarium-1-0</guid>
      <pubDate>2022-10-03 20:28</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Note-taking Apps: Bear and Joplin</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/bear-joplin?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/bear-joplin&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/bear-joplin?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/bear-joplin</guid>
      <pubDate>2022-10-02 18:16</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>FlyLo - Terrifier 2</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/flylo-terrifier-2?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/flylo-terrifier-2&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This ‚òùÔ∏è. If you know, you know. üî•&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/flylo-terrifier-2?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/flylo-terrifier-2</guid>
      <pubDate>2022-09-29 12:32</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Stadia Shuts Down</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/google-stadia-shutdown?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/google-stadia-shutdown&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;img src="https://c.tenor.com/pGORhlFQBx0AAAAC/dj-khaled-another-one.gif" class="img-fluid" alt="DJ Khaled Another One GIF" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/google-stadia-shutdown?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/google-stadia-shutdown</guid>
      <pubDate>2022-09-29 12:24</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>How to - Build a personal webpage from scratch</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/how-to-build-personal-webpage-scratch?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/how-to-build-personal-webpage-scratch&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/how-to-build-personal-webpage-scratch?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/how-to-build-personal-webpage-scratch</guid>
      <pubDate>2022-09-29 11:42</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Appeal of Small Computers</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/appeal-small-pc?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/appeal-small-pc&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/appeal-small-pc?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/appeal-small-pc</guid>
      <pubDate>2022-09-28 21:42</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>MNT Pocket Reform - Open Source Pocket PC</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mnt-pocket-reform?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mnt-pocket-reform&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;&lt;img src="https://c.tenor.com/R0d3sZ4fq6EAAAAC/money-dollars.gif" class="img-fluid" alt="Take My Money GIF" /&gt;&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mnt-pocket-reform?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mnt-pocket-reform</guid>
      <pubDate>2022-09-28 21:38</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>DALL-E Available Without Waitlist</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/dalle-no-waitlist?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/dalle-no-waitlist&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/dalle-no-waitlist?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/dalle-no-waitlist</guid>
      <pubDate>2022-09-28 11:36</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Federated Wiki</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/fed-wiki-udell?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/fed-wiki-udell&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/fed-wiki-udell?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/fed-wiki-udell</guid>
      <pubDate>2022-09-28 10:44</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>GitHub for English Teachers</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/github-english-teachers?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/github-english-teachers&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/github-english-teachers?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/github-english-teachers</guid>
      <pubDate>2022-09-28 10:42</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>FOSDEM 2023 Dates (Feb 4 &amp; 5 2023)</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/fosdem-2023-dates?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/fosdem-2023-dates&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;It feels like the 2022 conference was just yesterday. In any case, save the date February 4-5,2023.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/fosdem-2023-dates?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/fosdem-2023-dates</guid>
      <pubDate>2022-09-26 21:48</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>The story behind Joplin</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/joplin-interview?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/joplin-interview&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Good read. Other than Emacs, &lt;a href="https://joplinapp.org/"&gt;Joplin&lt;/a&gt; is my go-to notetaking application.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/joplin-interview?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/joplin-interview</guid>
      <pubDate>2022-09-26 20:44</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Launch of Proton Drive - Encrypted Cloud Storage</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/proton-drive-launch?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/proton-drive-launch&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/proton-drive-launch?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/proton-drive-launch</guid>
      <pubDate>2022-09-26 20:15</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Web-Based Matrix Rain</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/web-matrix-rain?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/web-matrix-rain&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Matrix Rain sample with different variations.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/web-matrix-rain?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/web-matrix-rain</guid>
      <pubDate>2022-09-25 22:36</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>MLPerf - Modular ML for Benchmarks Working Group</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mlperf-modular-ml-workgroup?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mlperf-modular-ml-workgroup&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mlperf-modular-ml-workgroup?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mlperf-modular-ml-workgroup</guid>
      <pubDate>2022-09-23 10:04</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Narrative-Commitment-Task (NCT) - Goal Setting</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/nct-goal-setting?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/nct-goal-setting&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Alternative to OKRs&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/nct-goal-setting?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/nct-goal-setting</guid>
      <pubDate>2022-09-23 09:57</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Moving with Prototypes</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/moving-prototypes?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/moving-prototypes&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/moving-prototypes?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/moving-prototypes</guid>
      <pubDate>2022-09-22 21:48</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Mutant Standard Emojis</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/mutant-standard-emojis?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/mutant-standard-emojis&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;an alternative...emoji set.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/mutant-standard-emojis?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/mutant-standard-emojis</guid>
      <pubDate>2022-09-22 20:33</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Testing Web Design Color Contrast</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/testing-web-design-color-contrast?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/testing-web-design-color-contrast&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;W3C‚Äôs Web Accessibility Initiative provides strategies, standards, and resources to ensure that the internet is accessible for as many people as possible. The guidelines that underpin these standards are called the Web Content Accessibility Guidelines, or WCAG.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Color contrast is an important piece of the puzzle for accessibility on the web, and adhering to it makes the web more usable for the greatest number of people in the most varied situations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Apps to test contrast:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pika&lt;/li&gt;
&lt;li&gt;VisBug&lt;/li&gt;
&lt;li&gt;Chrome Dev Tools&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/testing-web-design-color-contrast?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/testing-web-design-color-contrast</guid>
      <pubDate>2022-09-22 20:06</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>ARM Support for VS Extensions</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/vs-extension-arm-support?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/vs-extension-arm-support&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/vs-extension-arm-support?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/vs-extension-arm-support</guid>
      <pubDate>2022-09-22 20:02</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>TensorStore</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/tensorstore?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/tensorstore&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;High-perf, scalable array storage that can be used for scenarios like language models.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/tensorstore?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/tensorstore</guid>
      <pubDate>2022-09-22 19:56</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Open AI Whisper</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/open-ai-whisper?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/open-ai-whisper&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/open-ai-whisper?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/open-ai-whisper</guid>
      <pubDate>2022-09-22 19:44</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>5 Git configurations I make on Linux</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/five-git-configurations-linux?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/five-git-configurations-linux&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;ol&gt;
&lt;li&gt;Create global configuration&lt;/li&gt;
&lt;li&gt;Set default name&lt;/li&gt;
&lt;li&gt;Set default email address&lt;/li&gt;
&lt;li&gt;Set default branch name&lt;/li&gt;
&lt;li&gt;Set default editor&lt;/li&gt;
&lt;/ol&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/five-git-configurations-linux?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/five-git-configurations-linux</guid>
      <pubDate>2022-09-22 19:44</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>An app can be a home-cooked meal</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/app-home-cooked?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/app-home-cooked&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;When you liberate programming from the requirement to be general and professional and scalable, it becomes a different activity altogether, just as cooking at home is really nothing like cooking in a commercial kitchen.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Same goes for websites and self-hosting.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/app-home-cooked?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/app-home-cooked</guid>
      <pubDate>2022-09-21 22:28</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Welcome to La Terminal</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/welcome-to-la-terminal?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/welcome-to-la-terminal&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;I'm not an Apple user, but this is cool.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/welcome-to-la-terminal?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/welcome-to-la-terminal</guid>
      <pubDate>2022-09-21 19:48</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Resources for navigating complex leadership work</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/resources-navigating-complex-leadership-work?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/resources-navigating-complex-leadership-work&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Resource on:&lt;/p&gt;
&lt;p&gt;Influence &amp;amp; managing up: Enact positive change for yourself, your team, or the whole organization.&lt;/p&gt;
&lt;p&gt;Leading through crises: Strengthen your support network, meet your team where they‚Äôre at, and weather the tough times.&lt;/p&gt;
&lt;p&gt;Cross-functional relationships: Strengthen relationships by creating role clarity and creatively supporting one another.&lt;/p&gt;
&lt;p&gt;One-on-ones: Set your teammates up for success during your one-on-one meetings!&lt;/p&gt;
&lt;p&gt;Hiring: Build consistent, repeatable, and equitable interviews and onboarding plans.&lt;/p&gt;
&lt;p&gt;Meetings: Support participants, hone the content, and nail the meeting goal.&lt;/p&gt;
&lt;p&gt;Feedback &amp;amp; performance reviews: Everyone deserves clear, actionable feedback!&lt;/p&gt;
&lt;p&gt;Communication &amp;amp; team dynamics: Plan ahead, facilitate well, and create clarity.&lt;/p&gt;
&lt;p&gt;Adapting your approach: As your work context and team evolves, your leadership approach will need to evolve, too.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/resources-navigating-complex-leadership-work?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/resources-navigating-complex-leadership-work</guid>
      <pubDate>2022-09-21 19:19</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>What are Distance Metrics in Vector Search?</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/distance-metrics-vector-search?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/distance-metrics-vector-search&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Metrics covered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cosine&lt;/li&gt;
&lt;li&gt;Dot Product&lt;/li&gt;
&lt;li&gt;L2-Squared&lt;/li&gt;
&lt;li&gt;Manhattan&lt;/li&gt;
&lt;li&gt;Hamming&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/distance-metrics-vector-search?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/distance-metrics-vector-search</guid>
      <pubDate>2022-09-21 12:44</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>1% Rule</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/1-percent-rule?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/1-percent-rule&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...about 1% of Internet users create content, while 99% are just consumers of that content&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/1-percent-rule?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/1-percent-rule</guid>
      <pubDate>2022-09-21 12:39</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>What I've Learned From Users</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/what-ive-learned-from-users?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/what-ive-learned-from-users&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;What have I learned from YC's users, the startups we've funded?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...most startups have the same problems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the batch that broke YC was a powerful demonstration of how individualized the process of advising startups has to be.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...founders can be [bad] at realizing what their problems are. Founders will sometimes come in to talk about some problem, and we'll discover another much bigger one in the course of the conversation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Often founders know what their problems are, but not their relative importance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Focus is doubly important for early stage startups, because not only do they have a hundred different problems, they don't have anyone to work on them except the founders. If the founders focus on things that don't matter, there's no one focusing on the things that do.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Speed defines startups. Focus enables speed. YC improves focus.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Why are founders uncertain about what to do? Partly because startups almost by definition are doing something new, which means no one knows how to do it yet, or in most cases even what &amp;quot;it&amp;quot; is.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/what-ive-learned-from-users?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/what-ive-learned-from-users</guid>
      <pubDate>2022-09-20 21:21</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>How to Ditch Facebook Without Losing Your Friends</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/ditch-facebook-without-losing-friends?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/ditch-facebook-without-losing-friends&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;disgruntled Facebook users keep using the service because they don‚Äôt want to leave behind their friends, family, communities and customers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;‚ÄúHow to Ditch Facebook Without Losing Your Friends‚Äù explains the rationale behind these proposals - and offers a tour of what it would be like to use a federated, interoperable Facebook, from setting up your account to protecting your privacy and taking control of your own community‚Äôs moderation policies, overriding the limits and permissions that Facebook has unilaterally imposed on its users.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://archive.org/details/interoperable-facebook"&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://archive.org/details/interoperable-facebook-fictional-scenarios"&gt;Highlight reel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.eff.org/files/2022/09/12/interoperablefacebook-2022_0.pdf"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/ditch-facebook-without-losing-friends?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/ditch-facebook-without-losing-friends</guid>
      <pubDate>2022-09-20 20:30</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Firefox Profile Maker</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/firefox-profile-maker?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/firefox-profile-maker&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This tool will help you to create a Firefox profile with the defaults you like.&lt;/p&gt;
&lt;p&gt;You select which features you want to enable and disable and in the end you get a download link for a zip-file with your profile template.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/firefox-profile-maker?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/firefox-profile-maker</guid>
      <pubDate>2022-09-20 20:27</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>RSS at 20 still going string</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/rss-20-at-20?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/rss-20-at-20&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;20 years and still going strong.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/rss-20-at-20?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/rss-20-at-20</guid>
      <pubDate>2022-09-18 10:20</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Test Response</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/webmention-test-1?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/webmention-test-1&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This is my comment 2&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/webmention-test-1?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/webmention-test-1</guid>
      <pubDate>2022-09-16 14:37:00</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>WordPress+IndieWeb as the OS of the Open Social Web</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/wordpress-indieweb-os-open-social-web?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/wordpress-indieweb-os-open-social-web&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...2022 Netherlands WordCamp edition in Arnhem [presentation] on turning all WordPress sites into fully IndieWeb enabled sites. Meaning turning well over a third of the web into the open social web. Outside all the silos.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/wordpress-indieweb-os-open-social-web?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/wordpress-indieweb-os-open-social-web</guid>
      <pubDate>2022-09-16 11:44</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>The Notecard System</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/rh-notecard-system?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/rh-notecard-system&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/rh-notecard-system?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/rh-notecard-system</guid>
      <pubDate>2022-09-14 09:29</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Lisp is a chameleon</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/lisp-is-a-chameleon?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/lisp-is-a-chameleon&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;When Lisp adopts a new paradigm it not only replicates existing practice, but goes beyond it to become a testbed for advancing the state of the art. Why has Lisp been able to adapt so easily when other languages have not? One reason is that Lisp is a programmable programming language.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/lisp-is-a-chameleon?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/lisp-is-a-chameleon</guid>
      <pubDate>2022-09-14 09:17</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>MOOC - 850+ Free Courses</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/850-free-mooc-courses?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/850-free-mooc-courses&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Subjects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Science&lt;/li&gt;
&lt;li&gt;Programming&lt;/li&gt;
&lt;li&gt;Education &amp;amp; Teaching&lt;/li&gt;
&lt;li&gt;Health &amp;amp; Medicine&lt;/li&gt;
&lt;li&gt;Social Sciences&lt;/li&gt;
&lt;li&gt;Business&lt;/li&gt;
&lt;li&gt;Information Security (InfoSec)&lt;/li&gt;
&lt;li&gt;Mathematics&lt;/li&gt;
&lt;li&gt;Computer Science&lt;/li&gt;
&lt;li&gt;Data Science&lt;/li&gt;
&lt;li&gt;Personal Development&lt;/li&gt;
&lt;li&gt;Humanities&lt;/li&gt;
&lt;li&gt;Engineering&lt;/li&gt;
&lt;li&gt;Art &amp;amp; Design&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/850-free-mooc-courses?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/850-free-mooc-courses</guid>
      <pubDate>2022-09-14 09:14</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Open Building Institute</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/open-building-institute?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/open-building-institute&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The OBI system is open source, collaborative and distributed.&lt;/p&gt;
&lt;p&gt;Our focus is on low cost and rapidly-built structures that are modular, ecological, and energy efficient.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/open-building-institute?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/open-building-institute</guid>
      <pubDate>2022-09-11 11:58</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Metablogging</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/metablogging-ideaspace?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/metablogging-ideaspace&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...internally public blogs written by members of the...squad detailing what they‚Äôre working on and thinking about.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/metablogging-ideaspace?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/metablogging-ideaspace</guid>
      <pubDate>2022-09-09 09:49</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Coding for Economists</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/coding-for-economists?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/coding-for-economists&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...a guide for economists on what programming is, why it‚Äôs useful, and how to do it.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/coding-for-economists?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/coding-for-economists</guid>
      <pubDate>2022-09-08 19:22</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title> google/haskell-trainings</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/google-haskell-trainings?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/google-haskell-trainings&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;This repository contains the source for the slides and the exercises used in the Haskell trainings at Google.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/google-haskell-trainings?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/google-haskell-trainings</guid>
      <pubDate>2022-09-06 20:16</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Open Source Alternative To</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/open-source-alternativeto?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/open-source-alternativeto&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Discover 350+ popular open source alternatives to your proprietary SaaS.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/open-source-alternativeto?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/open-source-alternativeto</guid>
      <pubDate>2022-09-06 18:51</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Open Library</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/open-library?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/open-library&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Open Library is an initiative of the Internet Archive, a 501(c)(3) non-profit, building a digital library of Internet sites and other cultural artifacts in digital form.&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/open-library?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/open-library</guid>
      <pubDate>2022-09-05 18:29</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>A Deep Dive into Transformers with TensorFlow and Keras: Part 1</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/deep-dive-transformers-tf-keras-pt1?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/deep-dive-transformers-tf-keras-pt1&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/deep-dive-transformers-tf-keras-pt1?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/deep-dive-transformers-tf-keras-pt1</guid>
      <pubDate>2022-09-05 12:05</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>The Feed Directory - Pine.blog</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/pine-blog-feed-directory?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/pine-blog-feed-directory&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Find feeds for all of your favorite sites and keep up with everything they post!&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/pine-blog-feed-directory?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/pine-blog-feed-directory</guid>
      <pubDate>2022-09-04 22:27</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Taking Note: How to Create Commomplace with Evernote</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/how-to-create-commonplace-book-evernote?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/how-to-create-commonplace-book-evernote&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h2&gt;Why commonplace?&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Commonplace is an ideal medium for the curation and cultivation of intellectual ideas, thoughts, and knowledge. It‚Äôs also a proven and timeless system, tested by folks from a spectrum of backgrounds including authors, professors, and scientists.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Remember things&lt;/li&gt;
&lt;li&gt;Write to recall&lt;/li&gt;
&lt;li&gt;Understand reading&lt;/li&gt;
&lt;li&gt;Personal reference system&lt;/li&gt;
&lt;li&gt;Filter ideas&lt;/li&gt;
&lt;li&gt;Unleash creativity&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Reflecting on your commonplace&lt;/h2&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;The most profound power of your commonplace is being able to thumb through reading and review material. Your commonplace book isn‚Äôt just a filing cabinet‚Äîit‚Äôs an evolving record of your life and observations.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/how-to-create-commonplace-book-evernote?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/how-to-create-commonplace-book-evernote</guid>
      <pubDate>2022-09-04 20:18</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Every User a Neo</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/every-user-neo?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/every-user-neo&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/every-user-neo?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/every-user-neo</guid>
      <pubDate>2022-09-04 18:12</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Buck Rogers in the 25th Century A.D.</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/buck-rogers-25-century-ad?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/buck-rogers-25-century-ad&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/buck-rogers-25-century-ad?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/buck-rogers-25-century-ad</guid>
      <pubDate>2022-09-04 18:01</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Small Web</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/small-web-jayless-wiki?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/small-web-jayless-wiki&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...the kind of web they define themselves against; that kind of bloated, corporate, algorithm-ruled and ad-ridden mess that constitutes the majority of highly-trafficked websites these day.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...for me, the term ‚ÄúSmall Web‚Äù refers to a couple of main things: independence from tech giants, and websites that are lightweight and high-performance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Related concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A late 90s-style, hand-crafted web&lt;/li&gt;
&lt;li&gt;Alternative protocols, like Gemini&lt;/li&gt;
&lt;li&gt;An independent web&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/small-web-jayless-wiki?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/small-web-jayless-wiki</guid>
      <pubDate>2022-09-02 00:03</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Blot - Blogging Platform</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/blot-blogging-platform?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/blot-blogging-platform&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Blot is a blogging platform with no interface. It turns a folder into a website.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/blot-blogging-platform?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/blot-blogging-platform</guid>
      <pubDate>2022-08-30 22:20</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>osmlab/awesome-openstreetmap</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/osmlab-awesome-osm?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/osmlab-awesome-osm&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;Curated list of awesome OpenSteetMap-projects&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/osmlab-awesome-osm?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/osmlab-awesome-osm</guid>
      <pubDate>2022-08-12 08:12</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>The Yesterweb</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/yesterweb?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/yesterweb&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;This site is dedicated to a community (and a larger movement) about the internet how it's changed. We are creating, discovering and enjoying websites and digital spaces.&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/yesterweb?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/yesterweb</guid>
      <pubDate>2022-08-03 22:24</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Reimagine the internet</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/columbia-reimagine-internet?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/columbia-reimagine-internet&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;A virtual conference exploring what the internet could become over the next decade&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sessions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pioneering Alternative Models for Community on the Internet&lt;/li&gt;
&lt;li&gt;Misinformation, Disinformation, and Media Literacy in a Less-Centralized Social Media Universe&lt;/li&gt;
&lt;li&gt;Interoperability and Alternative Social Media&lt;/li&gt;
&lt;li&gt;Lessons from Experiments in Local Community-Building&lt;/li&gt;
&lt;li&gt;Deplatforming and Innovation&lt;/li&gt;
&lt;li&gt;New Directions in Social Media Research&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/columbia-reimagine-internet?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/columbia-reimagine-internet</guid>
      <pubDate>2022-08-03 21:21</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>MasterWiki</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/masterwiki?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/masterwiki&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;...masterWiki is the direct adaptation of MasterClass' video courses translated into wikiHow-style how-to guides...&lt;/p&gt;
&lt;/blockquote&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/masterwiki?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/masterwiki</guid>
      <pubDate>2022-08-03 18:48</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>The Optimal iPhone Settings and Best Apps for Productivity, Focus, and Health</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/iphone-settings-focus-productivity-health?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/iphone-settings-focus-productivity-health&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;blockquote class="blockquote"&gt;
&lt;p&gt;Here‚Äôs how to read the post:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Level 1 ‚Äî Casual. Read the headlines ‚Äî figure out the details yourself. Most of this isn‚Äôt rocket science.&lt;/li&gt;
&lt;li&gt;Level 2 ‚Äî Tutorial. Read the steps underneath the headline. I‚Äôve spelled out every step so that you can save your brain power for something else.&lt;/li&gt;
&lt;li&gt;Level 3 ‚Äî Productivity Nerd. Below the tutorial steps, I‚Äôve included discussion of the behavior design implications. This is for true productivity nerds, i.e. the readers of Better Humans.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Optimize First for Single Tasking&lt;/p&gt;
&lt;p&gt;#1. Turn OFF (almost) all notifications&lt;br /&gt;
#2. Hide social media slot machines&lt;br /&gt;
#3. Hide messaging slot machines&lt;br /&gt;
#4. Disable app review requests&lt;br /&gt;
#5. Turn on Do Not Disturb&lt;br /&gt;
#6. Be strategic about your wallpaper&lt;br /&gt;
#7. Turn off Raise to Wake&lt;br /&gt;
#8. Add the Screen Time widget&lt;br /&gt;
#9. Add Content Restrictions&lt;br /&gt;
#10. (Optional) Use Restrictions to turn off Safari&lt;br /&gt;
#11. Organize your Apps and Folders alphabetically&lt;/p&gt;
&lt;p&gt;Switch to Google Cloud to Work Faster&lt;/p&gt;
&lt;p&gt;#12. Choose GMail&lt;br /&gt;
#13. Choose Google Calendar&lt;br /&gt;
#14. Replace Apple Maps with Google Maps&lt;br /&gt;
#15. Install the GBoard keyboard for faster typing&lt;br /&gt;
#16. Switch to Google Photos&lt;/p&gt;
&lt;p&gt;Install These Apps for Productivity&lt;/p&gt;
&lt;p&gt;#17. Use Evernote for all note taking, to-do lists, everything&lt;br /&gt;
#18. The Case for Calm as your go-to meditation app&lt;br /&gt;
#19. Install the right goal tracker for you&lt;br /&gt;
#20. Store all your passwords in a password manager, probably LastPass&lt;br /&gt;
#21. Use Numerical as your default calculator&lt;br /&gt;
#22. Put the Camera app in your toolbar&lt;br /&gt;
#23. Use this Doppler Radar app&lt;br /&gt;
#24. Use this Pomodoro app&lt;br /&gt;
#25. Use Brain.fm for background noise&lt;/p&gt;
&lt;p&gt;Use These Apps and Configurations for Deep Learning&lt;/p&gt;
&lt;p&gt;#26. Subscribe to these podcasts&lt;br /&gt;
#27. Install the Kindle app but never read it in bed&lt;br /&gt;
#28. Use Safari this way&lt;br /&gt;
#29. Organize your home screen for deep learning over shallow learning&lt;/p&gt;
&lt;p&gt;Use These Apps and Configurations for Longevity&lt;/p&gt;
&lt;p&gt;#30. Track steps this way&lt;br /&gt;
#31. Prefer Time Restricted Eating Over Calorie Counting&lt;br /&gt;
#32. Schedule Night Shift&lt;br /&gt;
#33. Set up Medical ID&lt;/p&gt;
&lt;p&gt;Make The Finishing Touches with These Configurations&lt;/p&gt;
&lt;p&gt;#34. Change Siri to a man&lt;br /&gt;
#35. Change your phone‚Äôs name&lt;br /&gt;
#36. Turn off advertising tracking&lt;br /&gt;
#37. Set auto-lock to the maximum time&lt;br /&gt;
#38. Set your personal hotspot password to a random three word phrase&lt;br /&gt;
#39. Turn on control center everywhere&lt;br /&gt;
#40. Turn on Background App Refresh&lt;br /&gt;
#41. Delete Garage Band&lt;br /&gt;
#42. Develop verbal memory for talking to Siri&lt;br /&gt;
#43. Set up these text replacement shortcuts&lt;br /&gt;
#44. Set your address&lt;br /&gt;
#45. Backup this way&lt;/p&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/iphone-settings-focus-productivity-health?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/iphone-settings-focus-productivity-health</guid>
      <pubDate>2022-08-01 17:50</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Open Source and Organization Workshop</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/open-source-organization-workshop?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/open-source-organization-workshop&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;h2&gt;Workshop objectives&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Discover and learn to use the basic functions of the following software:
&lt;ul&gt;
&lt;li&gt;Joplin&lt;/li&gt;
&lt;li&gt;Zotero&lt;/li&gt;
&lt;li&gt;Nextcloud&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learn how to use these different software programs together to manage information using the following methods:&lt;/li&gt;
&lt;li&gt;The Zettelkasten method&lt;/li&gt;
&lt;li&gt;The P.A.R.A method&lt;/li&gt;
&lt;li&gt;The inbox method&lt;/li&gt;
&lt;/ul&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/open-source-organization-workshop?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/open-source-organization-workshop</guid>
      <pubDate>2022-07-27 22:01</pubDate>
      <category>#untagged</category>
    </item>
    <item>
      <title>Practical Deep Learning for Coders 2022</title>
      <description>&lt;![CDATA[&lt;p&gt;See the original post at &lt;a href="https://www.luisquintanilla.me/feed/practical-deep-learning-coders-2022?utm_medium=feed"&gt;https://www.luisquintanilla.me/feed/practical-deep-learning-coders-2022&lt;/a&gt;&lt;/p&gt;&lt;br&gt;&lt;br&gt;&lt;p&gt;A free course designed for people with some coding experience, who want to learn how to apply deep learning and machine learning to practical problems.&lt;/p&gt;
&lt;p&gt;Lessons&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Getting started&lt;/li&gt;
&lt;li&gt;Deployment&lt;/li&gt;
&lt;li&gt;Neural net foundations&lt;/li&gt;
&lt;li&gt;Natural Language (NLP)&lt;/li&gt;
&lt;li&gt;From-scratch Model&lt;/li&gt;
&lt;li&gt;Random forests&lt;/li&gt;
&lt;li&gt;Collaborative filtering and embeddings&lt;/li&gt;
&lt;li&gt;Convolutions (CNNs)&lt;/li&gt;
&lt;/ol&gt;
]]&gt;</description>
      <link>https://www.luisquintanilla.me/feed/practical-deep-learning-coders-2022?utm_medium=feed</link>
      <guid>https://www.luisquintanilla.me/feed/practical-deep-learning-coders-2022</guid>
      <pubDate>2022-07-26 21:11</pubDate>
      <category>#untagged</category>
    </item>
  </channel>
</rss>