<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>Luis Quintanilla Posts</title>
    <link>https://www.luisquintanilla.me/posts</link>
    <description>Blog posts by Luis Quintanilla</description>
    <lastPubDate>2025-07-06 20:09</lastPubDate>
    <language>en</language>
    <item>
      <title>AI like it's 1999 or 1899</title>
      <description>&lt;![CDATA[
## Introduction

In an earlier note, I talked about the natural progression of technology which goes from [scarcity to abundance](/feed/ai-abundance-scarcity-cycle-repeats-rss-enclosure-use-case) over time and AI is no different. That was in the context of media on the internet and paying per minute with early cell phones. One of the original challenges with AI was context length and token limits. It's becoming less relevant as context sizes continue to increase. However, it's not like you can fit the entire Harry Potter series either. So you still need to cleverly manage your context size especially when using documents and chat history as part of that context. Historically, people have used abbreviations to compress messages either because of cost or medium constraints. That's what this post explores in the context of AI.

## Compressing text like T9

Below is a conversation Microsoft Copilot (formerly Bing Chat) using some information from the article [From 1890: The First Text Messages](https://sundaymagazine.org/2010/08/10/from-1890-the-first-text-messages/):

&gt; **Me** 
&gt; &gt; Summarize this text:
&gt; &gt;   
&gt; &gt; I’m trying something new today. Sometimes in my research I find an interesting old article that I wouldn’t normally post because it’s not from the Sunday Magazine section, or it’s from further than 100 years ago so I’ll never get to it. Instead of letting these go unused, I figure I’ll occasionally post them midweek during what would otherwise be slow weeks. Since this weekend I only have three articles to post, it seems like a good week to try it.
&gt; &gt;   
&gt; &gt; Telegraph operators on opposite sides of the country had some time to get to know each other when they weren’t busy sending other people’s messages. “Metaphorically they shake hands cordially twice a day — when they begin work and when they end it. And when business is dull they hold long conversations, with hundreds of miles — perhaps thousands — separating them, as two friends might do over a dinner table.”
&gt; &gt;   
&gt; &gt; What really caught my eye, though, is that the abbreviations they used seem a lot like the abbreviations used in today’s text messages.
&gt; 
&gt; **Copilot**
&gt; &gt; Here is a possible summary of the text:
&gt; &gt;   
&gt; &gt; The author is trying a new feature of posting old articles from the Sunday Magazine section on midweek. The article is about how telegraph operators communicated with each other using abbreviations that are similar to today’s text messages. The author finds this interesting and wonders if there are other similarities between old and new forms of communication.
&gt; 
&gt; **Me**
&gt; &gt; Now abbreviate it as if using a T9 keyboard for conciseness
&gt; 
&gt; **Copilot**
&gt; &gt; That sounds like a fun challenge. Here is a possible abbreviation of the summary:
&gt; &gt;   
&gt; &gt; try new feat post old artcls midwk. artcl abt telgrph oprtrs chat w/ abbrvtns lk txt msgs. find intrstng n wndr othr smilrts btwn old n new comms.

In this conversation, I first had Copilot summarize the original text. It's not a very long piece of text but it works for this example. You could imagine though how a longer piece of text would benefit even more from this summarization. Copilot does a good job of summarizing the text. However, what if I wanted to summarize it even more? 

In comes T9 (kind of). T9 is short for "Text on 9 keys" and it was predictive text technology that made it easier to compose SMS text messages using only the 9 keys on the phone keypad. Before T9, it was cumbersome and time consuming to compose full sentences so you might abbreviate them. For example "See you later" might be abbreviated as "c u l8r". You can learn more in the [SMS language Wikipedia article](https://en.wikipedia.org/wiki/SMS_language). Not only would you abbreviate because it was tedious to compose messages one letter at a time, but also you were limited in the number of characters you could send in a single SMS. Somewhat related, in the early days of microblogging on Twitter, one of the ways you could compose and view tweets was through SMS. There was the cost aspects as well. You were charged per SMS so you wanted to send as much information in a single SMS as possible. Now SMS is only one example where this took place. Telegrams and telegraphs from yesteryear had similar constraints. Whether it was cost, size limits, or general UX challenges, people looked to abbreviations to draft their messages. 

Now that we're caught up on this history, that's what you're seeing in this response from Copilot.

&gt; try new feat post old artcls midwk. artcl abt telgrph oprtrs chat w/ abbrvtns lk txt msgs. find intrstng n wndr othr smilrts btwn old n new comms.

It is an abbreviated form of the summarized message enabling you to use even less tokens as part of the prompts you submit to AI. 

When looking at "word" counts, they are roughly the following:

| Text | Word Count |
| --- | --- |
| Original | 58 |
| Summarized | 25 |
| T9 | 18 |

We can see that the abbreviated version is ~3x smaller than the original message. 

## Conclusion

Given that models like GPT and others have been trained on the entire internet, it has some knowledge of these types of abbreviations to compose messages. Now, is it scalable and would you want to use this in your production applications, I can't say. Probably not because the abbreviations are lossy. While in general you could read and get the gist of the message using the abbreviations, there are things lost in translation. Now maybe the LLM might do a better job at interpreting the abbreviations. It'd be a fun experiment. 

"n d mntm, il kp usng AI lk its 1999. c u l8r!"

Translation: "In the meantime, I'll keep using AI like it's 1999. See you later!" :slightly_smiling_face:]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/ai-1999-1899</link>
      <guid>https://www.luisquintanilla.me/posts/ai-1999-1899</guid>
      <pubDate>2023-12-15 08:56</pubDate>
    </item>
    <item>
      <title>Alternatives to WhatsApp</title>
      <description>&lt;![CDATA[
## Introduction

Recently I've heard a lot of buzz around people moving away from the WhatsApp messaging app. I'm not tuned into all the details as to why there's been a mass exodus, but from what I understand it has to do with its privacy policies. I've seen many people including Elon Musk recommend Signal. Signal is a great alternative but there are a few others that are worth considering. This recommendation was originally planned as a tweet, but there's so many details that just can't be captured in a tweet or thread so instead I turned it into a blog post. If you haven't made the switch or if you're still not completely committed to the platform you've switched to, this post is for you.

![Phone and notebook - Oscar Mucyo Unsplash](https://user-images.githubusercontent.com/11130940/104107289-08689180-5289-11eb-890d-54527902df97.png)

## Matrix

![Element web client screenshot](https://user-images.githubusercontent.com/11130940/104107320-3e0d7a80-5289-11eb-9449-71e34315b96e.png)

[Matrix](https://matrix.org/) is my go-to messaging platform. Matrix is an open-source project that provides an open standard for real-time communication. Built on top of this standard are several implementations with [Synapse](https://github.com/matrix-org/synapse) being the most popular server and [Element](https://element.io/) being the most popular client.

Matrix is relatively new. However, it has a lot of potential and there are a lot of exciting things happening around it. Some of the most notable include:

- [Mozilla selected Matrix](https://matrix.org/blog/2019/12/19/welcoming-mozilla-to-matrix) to be the successor to its IRC communications.
- Matrix recently [acquired Gitter](https://matrix.org/blog/2020/09/30/welcoming-gitter-to-matrix), a popular chat platform for software projects, and has quickly worked to [integrate Gitter with Matrix](https://matrix.org/blog/2020/12/07/gitter-now-speaks-matrix) allowing for it to take advantage of many of the features provided by the protocol. 
- [Matrix will power FOSDEM 2021](https://matrix.org/blog/2021/01/04/taking-fosdem-online-via-matrix) communications.
- [Matrix is experimenting with P2P architectures](https://matrix.org/blog/2020/06/02/introducing-p-2-p-matrix) to help users have full control over their communications.

Although Matrix provides several features commonly associated with messaging platforms such as 1:1 messaging, video/audio calls, and group chat, there are a few features worth highlighting:

- End-to-end encryption
- Interoperability with other communication platforms
- Decentralization

### End-to-end encryption

End-to-end encryption makes sure your communications stay private. Only you and the intended recipients of the message are able to see the messages. 

### Interoperability

One of the biggest challenges of moving over to a new platform is that your friends and family don't or won't join you, making it a lonely place. Matrix provides interoperability with other communication platforms and protocols through [bridges](https://matrix.org/bridges/). Bridges facilitate the exchange of messages with other platforms without requiring all participants to be on Matrix. The core team maintains bridges for Slack, IRC, XMPP, and Gitter. There are also community maintained bridges for WhatsApp, Discord, Facebook Messenger, Signal, and many others. 

### Decentralization 

Decentralization means that there's no central authority that holds control over the platform or data. Matrix is decentralized through federation. One of the most common examples of federation is e-mail. I may have an AOL e-mail address and you may have a Hotmail e-mail address, yet we're still able to exchange messages. With Matrix, in a similar way, users and groups don't have to have their accounts on the same server in order to communicate. It also means that users have a choice over [which server they join](https://www.hello-matrix.net/public_servers.php). If I don't agree with the conversations or moderation of a particular server, I have the choice to open an account on another server without leaving Matrix altogether. Even better, because Matrix is built using open technologies, I have the option of hosting my own server.

To get started with Matrix, visit the [Element](https://element.io/get-started) website and create an account.

## Jami

![Jami Screenshot](https://user-images.githubusercontent.com/11130940/104107348-64331a80-5289-11eb-8606-944269e17d10.png)

Another project that I like but haven't used it to the same extent as Matrix is [Jami](https://jami.net/). Jami is an open-source project for real-time communication. Similar to Matrix, Jami is cross-platform, free (as in freedom and beer), end-to-end encrypted, and decentralized. Jami achieves decentralization by using P2P protocols. This means that the data is stored on your device so you're in full control of your data. Jami also doesn't require the internet to work. As long as you're in the same local network as your peers, you're able to communicate. 

To get started with Jami, [download the app](https://jami.net/download/) and create an account.

## Conclusion

In this post, I've mentioned some communication platforms that I consider great alternatives to WhatsApp. Regardless of the platform you choose, some things that I believe should be top of mind are security and privacy. It's important that you own your data and you have control over who has access to it and what they do with it. Feel free say hi on Matrix [@lqdev:lqdev.tech](https://matrix.to/#/@lqdev:lqdev.tech) or Jami where you can find me as lqdev1.]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/alternatives-to-whatsapp</link>
      <guid>https://www.luisquintanilla.me/posts/alternatives-to-whatsapp</guid>
      <pubDate>2021-01-09 13:44:27</pubDate>
    </item>
    <item>
      <title>Authorization Code Authentication Flow in Python</title>
      <description>&lt;![CDATA[
For the longest time, authentication had been my worst enemy. I took small steps towards becoming comfortable with it. At first I learned Implicit Authorization with AngularJS, then I learned Client Credentials Authorization with Python and C# and finally, I learned Authorization Code authentication with C#. The one that always gave me trouble was Authorization Code authentication because it requires user credentials. 

To help with that, I started with the `WebAuthenticationBroker` in C#. Although, most of it was done for me in terms of creating a `WebView` for users to authenticate with their credentials, I wanted to know whether I could do it myself without having to rely on a service to do it for me. 

First, I had to find an API to test this out with and in an attempt to reduce complexity, I decided to prototype it in Python. As an avid user of [Pocket](http://getpocket.com), I chose its API to learn Authorization Flow authentication. 

# Getting Started 

## Register An Application

In order to get started, we need to register an application with Pocket. This can be done by visiting the following [website](https://getpocket.com/developer/apps/new).

## Install Requests

In order to create HTTP requests, we need to install the `requests` module

```
pip install requests
```

# Get Request Token

## Import Modules

```python
import requests
import json
```

## Prepare Request

```python
consumer_key = "YOUR_CONSUMER_KEY"
request_url = "https://getpocket.com/v3/oauth/request"
headers = {"Content-Type":"application/json","X-Accept":"application/json"}
redirect_uri = "http://www.google.com"
payload = {"consumer_key":consumer_key,"redirect_uri":redirect_uri}
```

Notice how the `redirect_uri` is set to `http://www.google.com`. This, for the most part is irrelevant, especially for console/desktop applications. Since this is a console application and we are not hosting a server for it, the value assigned to this field is arbitrary.

## Make Request

```python
request = requests.post(request_url,headers=headers,data=json.dumps(payload))
code = request.json()['code']
code
```

# Redirect User to Pocket to Continue Authorization

## Import Modules

```python
import webbrowser
```

```python
webbrowser.open('https://getpocket.com/auth/authorize?request_token='+ code + '&amp;redirect_uri=' + redirect_uri)
```

This will open your default browser and redirect you to an authorization page. After logging in, should you accept giving the application access to your account, it should redirect you to `http://www.google.com`. Once on the Google page, you can close out of it.

# Convert A Request Token Into A Pocket Access Token

After authorizing the application to access your account, you need to exchange the code you received for an access token.

## Prepare the Request

```python
access_token_url = "https://getpocket.com/v3/oauth/authorize"
payload = {"consumer_key":consumer_key,"code":code}
```

## Get Access Token

```python
access_token_request = requests.post(access_token_url,headers=headers,data=json.dumps(payload))
access_token = access_token_request.json()['access_token']
access_token
```

# Make Authenticated Request

In order to test whether the authentication was successful, try making an authenticated request.

## Prepare Request

```python
get_url = "https://getpocket.com/v3/get"
get_payload = {"consumer_key":consumer_key,"access_token":access_token,"contentType":"article","count":10}
get_request = requests.get(get_url,data=json.dumps(get_payload),headers=headers)
```

## Get Response
```python
response = get_request.json()
```

## Sample Response

```json
{'complete': 1,
 'error': None,
 'list': {'1519049132': {'excerpt': 'Heads up! As part of our efforts to improve security and standards-based interoperability, we have implemented several new features in our authentication flows and made changes to existing ones.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://auth0.com/docs/api-auth/grant/authorization-code-pkce',
   'has_image': '0',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '1519049132',
   'resolved_id': '1519049132',
   'resolved_title': 'Calling APIs from Mobile Apps',
   'resolved_url': 'https://auth0.com/docs/api-auth/grant/authorization-code-pkce',
   'sort_id': 5,
   'status': '0',
   'time_added': '1514493110',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514493110',
   'word_count': '445'},
  '1607678551': {'excerpt': 'What is the smallest number of Democrats that could have changed the outcome of the 2016 United States presidential election by relocating to another state? And where should they have moved?  It turns out this question is a variant of the knapsack problem, an NP-hard computer science classic.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://mike.place/2017/ecknapsack/',
   'has_image': '0',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '1607678551',
   'resolved_id': '1607678551',
   'resolved_title': 'The Electoral College and the knapsack problem',
   'resolved_url': 'http://mike.place/2017/ecknapsack/',
   'sort_id': 7,
   'status': '0',
   'time_added': '1514478701',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514478701',
   'word_count': '1564'},
  '1616406703': {'excerpt': 'OAuth 2.0 is a protocol that allows a user to grant limited access to their resources on one site, to another site. This is done without the users having to expose their credentials. According to OAuth‘s website the protocol is not unlike a valet key.  Many luxury cars today come with a valet key.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://auth0.com/docs/protocols/oauth2',
   'has_image': '1',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '1616406703',
   'resolved_id': '1616406703',
   'resolved_title': 'OAuth 2.0',
   'resolved_url': 'https://auth0.com/docs/protocols/oauth2',
   'sort_id': 4,
   'status': '0',
   'time_added': '1514493120',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514493120',
   'word_count': '823'},
  '1859670338': {'excerpt': 'Last Tuesday saw the official launch of Will Robots Take My Job? and 5 days later we have passed 500K visitors and 4M page views.  To say this surpassed any expectations we had would be a major understatement. This is a recap of how the project got started and the first 5 days after launch.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://hackernoon.com/from-idea-to-4m-page-views-in-4-weeks-622aa194787d?ref=quuu',
   'has_image': '1',
   'has_video': '1',
   'is_article': '1',
   'is_index': '0',
   'item_id': '1859670338',
   'resolved_id': '1773505341',
   'resolved_title': 'From Idea to 4M Page Views in 4\xa0Weeks',
   'resolved_url': 'https://hackernoon.com/from-idea-to-4m-page-views-in-4-weeks-622aa194787d',
   'sort_id': 0,
   'status': '0',
   'time_added': '1514494606',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514494606',
   'word_count': '1837'},
  '2001727306': {'amp_url': 'https://venturebeat.com/2017/12/22/how-ibm-builds-an-effective-data-science-team/amp/',
   'excerpt': 'Data science is a team sport. This sentiment rings true not only with our experiences within IBM, but with our enterprise customers, who often ask us for advice on how to structure data science teams within their own organizations.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://venturebeat.com/2017/12/22/how-ibm-builds-an-effective-data-science-team/',
   'has_image': '1',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '2001727306',
   'resolved_id': '2001727306',
   'resolved_title': 'How IBM builds an effective data science team',
   'resolved_url': 'https://venturebeat.com/2017/12/22/how-ibm-builds-an-effective-data-science-team/',
   'sort_id': 1,
   'status': '0',
   'time_added': '1514493979',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514493979',
   'word_count': '773'},
  '2006596547': {'excerpt': 'I ran, a lot, this year— 2017 miles over 280 hours, or just short of 12 full days. For many of you, that seems like a ridiculous amount of distance and time, but to put it in perspective, elite marathoners and ultrarunners easily run two or three times that distance in a year.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://medium.com/run-ruminate/17-thoughts-on-running-life-and-startups-5d305223669b',
   'has_image': '1',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '2006596547',
   'resolved_id': '2006596547',
   'resolved_title': '17 thoughts on running, startups, and life while running 2017 miles in\xa02017',
   'resolved_url': 'https://medium.com/run-ruminate/17-thoughts-on-running-life-and-startups-5d305223669b',
   'sort_id': 6,
   'status': '0',
   'time_added': '1514479812',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514479812',
   'word_count': '1561'},
  '2006920392': {'excerpt': 'What the heck is “work” anyway?  In the Information Age, the dictionary definition of the word just doesn’t cut it anymore. Skill sets, jobs, and entire companies are forming daily based on new technology, market demands, and trends that didn’t exist even just a few years ago.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://www.hellosign.com/blog/the-future-of-work',
   'has_image': '1',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '2006920392',
   'resolved_id': '2006920392',
   'resolved_title': '4 Pillars Of The Future of Work',
   'resolved_url': 'https://www.hellosign.com/blog/the-future-of-work',
   'sort_id': 2,
   'status': '0',
   'time_added': '1514493449',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514493449',
   'word_count': '1805'},
  '2007117419': {'excerpt': 'Finding a job is not an easy task. There are multiple ways to look for jobs. Multiple steps has to be completed before you are called for an interview. After completing all these the most annoying part is when your interview doesn’t even last for 10 minutes.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://www.jobsstudios.com/blog/interview-tips',
   'has_image': '0',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '2007117419',
   'resolved_id': '2007117419',
   'resolved_title': '7 Tips to prepare for an Interview',
   'resolved_url': 'https://www.jobsstudios.com/blog/interview-tips',
   'sort_id': 8,
   'status': '0',
   'time_added': '1514477292',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514477292',
   'word_count': '468'},
  '2007245672': {'excerpt': 'RESTful API Versioning, though a simple and elegant concept, is a LOT harder to enforce than it sounds. It’s hard to not break backward compatibility on a continually evolving API, and though API versioning is a great concept, it’s rarely followed without flaw.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://shift.infinite.red/snapshot-testing-api-calls-the-right-way-58ef59b7f71b',
   'has_image': '1',
   'has_video': '1',
   'is_article': '1',
   'is_index': '0',
   'item_id': '2007245672',
   'resolved_id': '2007245672',
   'resolved_title': 'Snapshot Testing API Calls The Right\xa0Way',
   'resolved_url': 'https://shift.infinite.red/snapshot-testing-api-calls-the-right-way-58ef59b7f71b',
   'sort_id': 3,
   'status': '0',
   'time_added': '1514493189',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514493189',
   'word_count': '1230'},
  '234097661': {'excerpt': 'Sanchez examines the question, concluding that intellectuals support government intervention because it makes their work have greater importance.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'https://www.libertarianism.org/blog/why-do-intellecuals-support-government-solutions',
   'has_image': '0',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '234097661',
   'resolved_id': '234097661',
   'resolved_title': 'Why Do Intellectuals Support Government Solutions?',
   'resolved_url': 'https://www.libertarianism.org/blog/why-do-intellecuals-support-government-solutions',
   'sort_id': 9,
   'status': '0',
   'time_added': '1514477287',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1514477287',
   'word_count': '1423'}},
 'search_meta': {'search_type': 'normal'},
 'since': 1514514495,
 'status': 1}
```

Hope this helps!

###### Sources

[Pocket API](https://getpocket.com/developer/docs/authentication)  
[Requests](http://docs.python-requests.org/en/master/)]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/authorization-code-flow-python</link>
      <guid>https://www.luisquintanilla.me/posts/authorization-code-flow-python</guid>
      <pubDate>2017-12-29 18:45:19</pubDate>
    </item>
    <item>
      <title>Automating Resource Provisioning for Machine Learning in Azure with Cognitive Services and Terraform</title>
      <description>&lt;![CDATA[
## Introduction

Resources in the cloud, particularly Azure can be created in several ways some of which include Powershell, Azure CLI, Azure Portal and programatically through code. Another tool that can be leveraged to create resources in Azure is [Terraform](https://www.terraform.io/). Terraform via configuration files can build and manage infrastructure across various providers such as AWS, Azure and Google. One of the benefits of Terraform is that it allows infastructure and resources to be defined as code. The configuration files serve as a blueprint as well as an execution plan of what needs to be provisioned in the cloud. Not only does this allow for automation and removal of human error in configurations but it also serves as a way to document the different changes infrastructure has gone through thoroughout the application lifecycle. The purpose of this writeup is to show how to create an environment using Terraform for an application that utilizes Azure Cognitive Services. Our application will create a Computer Vision service that we can then use via HTTP requests. Source code for this writeup can be found at the following [link](https://github.com/lqdev/azcognitiveserviceterraformsample).

# Prerequisites

This writeup was built and tested using a PC running Ubuntu 18.04 but should work on both Windows and Mac. It also assumes that you have an Azure account as well as Azure CLI and Terraform CLI installed. Below are links to get all of the resources needed:

- [Azure Account](https://azure.microsoft.com/en-us/free/)
- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest)
- [Terraform CLI](https://www.terraform.io/downloads.html)

## Log into Azure CLI

The first thing we need to do is authenticate with Azure CLI so that Terraform can use your account information to create services. In the terminal, enter the following command: 

```bash
az login
```

## Create Terraform Resource Configuration Script

Once we're logged in, it's time to create the configuration file that Terraform will use to provision our services. First we'll need to create a directory for our application:

```bash
mkdir azcognitiveserviceterraformsample
```

Then, we'll enter that directory and create our configuration file:

```bash
cd azcognitiveserviceterraformsample
touch azcomputervision.tf
```

Using your preferred text editor, open the newly created `azcomputervision.tf` file and begin editing.

### Defining the Provider

At the top of our file we'll want to configure our provider. This tells Terraform where our resources will be deployed to. In the `azcomputervision.tf` file enter the following:

```bash
# Configure Azure Provider
provider "azurerm" {
  version="=1.20.0"
}
```

Because we've already authenticated with the Azure CLI, there's no need to provide credentials. The only thing we need to do is specify the provider name and the version of the provider that will be used by the configuration file. For more details on how to configure the Azure provider, visit this [link](https://www.terraform.io/docs/providers/azurerm/index.html).

### Create Resource Group

After defining the provider, it's time to create a Resource Group that will contain the resources we create. In the configuration file, under the provider definition, enter the following:

```bash
# Create Resource Group
resource "azurerm_resource_group" "terraform_cognitive_sample" {
  name="terraform_cognitive_sample"
  location="East US"
}
```

This defines the name of our resource groups as well as where it is hosted. The syntax for resources looks like the snippet below where the `type` is the type of resources as required by Terraform and the `name` is any value of your choosing.

```bash
resource "type" "name" {
# Properties
}
```

For more details on Resource Group configuration visit this [link](https://www.terraform.io/docs/providers/azurerm/d/resource_group.html).

### Create Azure Cognitive Service

With our resource group defined, it's time to define our Cognitive Service resource. Below our Resource Group definition, enter the following:

```bash
resource "azurerm_cognitive_account" "computer_vision_service" {
  name="computer_vision_service"
  resource_group_name="${azurerm_resource_group.terraform_cognitive_sample.name}"
  location="${azurerm_resource_group.terraform_cognitive_sample.location}"
  kind="ComputerVision"
  
  sku {
      name="F0"
      tier="Free"
  }
}
```

Like our Resource Group, we provide a name for our service. Additional properties we need to provide are the name of the Resource Group and the location of where to deploy our service to. Because this has been previously defined in our Resource Group, Terraform allows us to access the configuration values as variables. Additionally, we need to specify the `kind` of cognitive service to deploy. In our case it will be the Computer Vision service so we use `ComputerVision`. The last thing we need to do is specify the pricing tier to use. We'll be using the free tier for this project. There are several services and tiers to choose from. To get more details on acceptable values for these properties visit the following [link](https://www.terraform.io/docs/providers/azurerm/r/cognitive_account.html).

### Store Output Variables

When our application is deployed, in order to use it we'll need the endpoint to where we will be making HTTP requests to. This can be persisted by Terraform by defining an output variable inside of the configuration file. To get the enpoint of our cognitive service, enter the following in the `azcomputervision.tf` file:

```bash
output "computer_vision_endpoint" {
    value="${azurerm_cognitive_account.computer_vision_service.endpoint}"
}
```

Once finished, the `azcomputervision.tf` file should look like this:

```bash
# Configure Azure Provider
provider "azurerm" {
  version="=1.20.0"
}

# Create Resource Group
resource "azurerm_resource_group" "terraform_cognitive_sample" {
  name="terraform_cognitive_sample"
  location="East US"
}

# Create Cognitive Service
resource "azurerm_cognitive_account" "computer_vision_service" {
  name="computer_vision_service"
  resource_group_name="${azurerm_resource_group.terraform_cognitive_sample.name}"
  location="${azurerm_resource_group.terraform_cognitive_sample.location}"
  kind="ComputerVision"
  
  sku {
      name="F0"
      tier="Free"
  }
}

# Output Cognitive Services Endpoint
output "computer_vision_endpoint" {
    value="${azurerm_cognitive_account.computer_vision_service.endpoint}"
}
```

## Provision Resources

### Initialize Terraform Resources

When provisioning resources we first want to get the necessary plugins needed by Terraform to create such resources. Using Terraform CLI, enter the following in the terminal:

```bash
terraform init
```

### Check Execution Plan

Although this is a single resource provision, it's always a good idea to see the execution plan of the resources that will be created. To do so, enter the following command in the terminal:

```bash
terraform plan
```

The output should look similar to the content below:

```text
Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.


------------------------------------------------------------------------

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  + azurerm_cognitive_account.computer_vision_service
      id:                  &lt;computed&gt;
      endpoint:            &lt;computed&gt;
      kind:                "ComputerVision"
      location:            "eastus"
      name:                "computer_vision_service"
      resource_group_name: "terraform_cognitive_sample"
      sku.#:               "1"
      sku.0.name:          "F0"
      sku.0.tier:          "Free"
      tags.%:              &lt;computed&gt;

  + azurerm_resource_group.terraform_cognitive_sample
      id:                  &lt;computed&gt;
      location:            "eastus"
      name:                "terraform_cognitive_sample"
      tags.%:              &lt;computed&gt;


Plan: 2 to add, 0 to change, 0 to destroy.
```

## Create Resources

It's now time to create our Computer Vision resource. Enter the following command in the terminal:

```bash
terraform apply
```

You will be asked to review the execution plan once again. If everything looks good, type `yes` in the terminal to continue with the creation of resources. If everything deployed successfully, you should be able to see it in Azure.

## Test Provisioned Resources

Using Azure CLI, enter the following command in the terminal to see whether your resource was deployed:

```bash
az resource list --resource-group terraform_cognitive_sample --output table
```

That command will output something similar to the content below:

```bash
Name                     ResourceGroup               Location    Type                                  Status
-----------------------  --------------------------  ----------  ------------------------------------  --------
computer_vision_service  terraform_cognitive_sample  eastus      Microsoft.CognitiveServices/accounts
```

### Get Resource Endpoint and Keys

In order to make a request to our service, we need both the endpoint and a key to authenticate our request.

To get the endpoint, we can use Terraform CLI to extract the output variable defined in our configuration file. We can do that by using the following command in the terminal:

```bash
terraform output computer_vision_endpoint
```

Save the output value somewhere because that's what will be used to make a request to the Computer Vision Cognitive Service.

To get the keys, we'll use Azure CLI. In the terminal, enter:

```bash
az cognitiveservices account keys list --resource-group terraform_cognitive_sample --name computer_vision_service
```

This will output the keys of your deployed service.

```json
{
  "key1": "&lt;YOUR-KEY-1&gt;",
  "key2": "&lt;YOUR-KEY-2&gt;"
}
```

### Make HTTP Request

Either key can be used to make requests. To test the service, I'll be using an image from the web. All you need is to get the URL for that image. The image I will be using can be found at this URL `https://upload.wikimedia.org/wikipedia/commons/1/10/Empire_State_Building_%28aerial_view%29.jpg`. Using cURL, I will make a POST request to the endpoint with the key provided.

One thing to note is that the endpoint is generic to many Cognitive Services. Therefore, we need to append to the path which service we will be using and the action we want to perform. In our case, we'll append the following path the the endpoint `/vision/v1.0/describe`. For more details, visit the Computer Vision API documentation at this [link](https://eastus.dev.cognitive.microsoft.com/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fe).

To make the request, enter the following into the terminal where `&lt;YOUR-KEY&gt;` is one of the keys of your Computer Vision service.

```bash
curl -H 'Ocp-Apim-Subscription-Key: &lt;YOUR-KEY&gt;' -H "Content-type: application/json" -d '{"url":"https://upload.wikimedia.org/wikipedia/commons/1/10/Empire_State_Building_%28aerial_view%29.jpg"}' 'https://eastus.api.cognitive.microsoft.com/vision/v1.0/describe'
```

If on Windows, you might want to try using a REST client such as POSTMAN or Insomnia.

If the request is successful, the output should be similar to the following:

```json
{
    "description":{
        "tags":[
            "mountain","building","city","sitting","table","view","full","filled","large","old","skyscraper","many","stacked","water","room","white"
        ],
        "captions":[
            {
                "text":"a view of a city","confidence":0.927147938733121
            }
        ]
    },
    "requestId":"a9aaf779-2aa1-4012-b1e0-2d2d9c20c6a5",
    "metadata":{
        "width":846,
        "height":1270,
        "format":"Jpeg"
    }
}
```

## Conclusion

In this writeup I went over how to create a Terraform configuration file for automating the creation of a Computer Vision Cognitive Service resource in Azure. Although this is a simple example, using the same concepts, more complex environments and infrastructure can be provisioned just as seamlessly in a safe and efficient manner. An example of how to extend this application would be to provision an Azure Storage Account as well as an Azure Function application that utilizes the Computer Vision Cognitive Service to automatically classify photos when they are uploaded to an Azure Storage container. In such scenario, the benefits of automation and documentation that Terraform provides can leveraged to a greater extent.]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/automate-machine-learning-service-provisioning-azure-terraform</link>
      <guid>https://www.luisquintanilla.me/posts/automate-machine-learning-service-provisioning-azure-terraform</guid>
      <pubDate>2019-01-05 22:58:39</pubDate>
    </item>
    <item>
      <title>Automate YAML front-matter generation with custom Visual Studio Code snippets</title>
      <description>&lt;![CDATA[
## Introduction

When creating content with static site generators such as Jekyll, Hexo, and many others, page metadata such as publish date, title, tags, and other details is created using YAML front-matter. Typically these pages are authored in markdown. At the top of each page, there's a YAML formatted section containing all of these details. Sometimes these static site generators include tooling to make it easier to scaffold new pages. In the case of my website, I created my own static site generator. Therefore, scaffolding a new page, especially writing the boilerplate YAML front-matter is cumbersome and time consuming. Since I use Visual Studio Code for authoring content and developing the static site generator, I've decided to use [snippets](https://code.visualstudio.com/docs/editor/userdefinedsnippets) to help me automate YAML front-matter generation. In this post, I'll show the general process. 

## Create snippets file

The first thing you'll want to do is create a snippets file. You can have one or as many as you want. For this website, I have a single snippets file called *metadata.code-snippets* for all my YAML front-matter. To create a snippets file:

1. Open the Visual Studio Code command palette.
1. Enter the command `&gt;Preferences: Configure User Snippets` into the text box.
1. Select the **New Snippets file for \&lt;REPO-OR-DIRECTORY-NAME\&gt;** from the list of options. 

    ![Create snippet file from Visual Studio Code command palette](https://user-images.githubusercontent.com/11130940/135934846-a76dfa16-caed-4489-9e50-95183add673d.png)

1. Provide a name for your snippets file and press **Enter**.
1. A file with the *.code-snippets* extension is created in the *.vscode* directory inside your project. 

One benefit of creating a snippets file and saving it in the *.vscode* directory is you can check it into source control and use it anywhere, even github.dev. 

## Create snippets

Now that you've created the file, it's time to define your snippets.

1. Open your snippets file.

    ![Snippet file for luisquintanilla.me](https://user-images.githubusercontent.com/11130940/135935306-1cbd48e1-6e48-423c-b87f-cb6f7a8eb085.png)

1. Create a new JSON object and define your snippet.

    For example, the snippet for blog posts on this site looks like the following.

    ```json
    "Article Post metadata": {
        "scope": "markdown",
        "prefix": "article",
        "body": [
            "---",
            "post_type: \"article\" ",
            "title: \"\"",
            "published_date: \"$CURRENT_MONTH/$CURRENT_DATE/$CURRENT_YEAR $CURRENT_HOUR:$CURRENT_MINUTE\"",
            "tags: ",
            "---"
        ],
        "description": "Blog post article metadata"
    }
    ```

    Let's break down each of the properties:

    - `scope`: By default, snippets apply to all languages and projects. In my case, since I intend only to use these snippets in markdown files, I set the scope to `markdown`. 
    - `prefix`: The prefix is the word in the page that is associated with. When I type `article` in a markdown page, a recommendation to use the snippet appears.
    - `body`: The code or content of my snippet. In this case, the expected output is similar to the following.

        ```yaml
        ---
        post_type: "article" 
        title: ""
        published_date: "10/04/2021 18:58"
        tags: 
        ---
        ```

        VS Code also has built-in variables that you can use to automatically set date, time, and other values.

    - `description`: A text description of your snippet.

For more information on creating snippets, see [Create your own snippets](https://code.visualstudio.com/docs/editor/userdefinedsnippets#_create-your-own-snippets).

## Use your snippets

1. Create a new markdown file.
1. Type your prefix into the file. In my case, for blog posts my prefix is `article`.
1. Press **Ctrl + Space**. For Macs you might have to use Cmd instead of Ctrl. 

    ![Populate article front-matter using snippet](https://user-images.githubusercontent.com/11130940/135935808-46ca8314-c2be-47f1-9c9b-9b722e37d908.png)

1. A tooltip with your snippet's description appears. Press **Enter**. 

At this point, your snippet's prefix is replaced with the content defined in the snippet's body.

## Conclusion

In this post I've shown how you can use Visual Studio Code snippets to automate YAML front-matter generation when working with common static site generators that don't provide tooling to scaffold new pages. Snippets can be used for a variety of scenarios and if there's any boilerplate code or content you constantly have to write, they can save you a lot of time. 

Happy writing!
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/automate-yaml-front-matter-vs-code-snippets</link>
      <guid>https://www.luisquintanilla.me/posts/automate-yaml-front-matter-vs-code-snippets</guid>
      <pubDate>10/04/2021 19:58</pubDate>
    </item>
    <item>
      <title>Operationalizing Machine Learning with ML.NET, Azure DevOps and Azure Container Instances</title>
      <description>&lt;![CDATA[

## Introduction

Azure DevOps, formerly known as Visual Studio Team Services (VSTS), helps individuals and organizations plan, collaborate and ship products faster. One if its noteworthy services, Azure Pipelines, helps developers build Continuous Integration (CI) and Continuous Delivery (CD) pipelines that automate and standardize the build, test and deploy phases of the software development process. In addition, Azure Pipelines provides native container support and works with any language, platform and cloud. Machine learning like software development is also a process that includes a build, test and deploy phase which makes it a good candidate for automation and standardization. At Build 2018, Microsoft announced [ML.NET](https://github.com/dotnet/machinelearning), an open-source, cross-plaform machine learning framework for .NET. If we were to put all of these tools and services together, it means that we can automate and standardize the training of a machine learning model built with ML.NET, package it into a Docker container and deploy it to Azure Container Instances (ACI). In this writeup, I will go through the process of building a CI/CD pipeline in Azure Devops that trains, packages and deploys an ML.NET machine learning model to predict which class an Iris flower belongs to using a variety of measurements. Source code for this project can be found at this [link](https://github.com/lqdev/mlnetazdevopssample).

## Prerequisites

- [Git](https://git-scm.com/)
- [GitHub Account](https://github.com/)
- [.NET Core SDK](https://www.microsoft.com/net/download)  
- [Azure Account](https://azure.microsoft.com/en-us/free/)

## The Application

Because the purpose of this post is to demonstrate the functionality of Azure Devops and not that of ML.NET, I'll start with a pre-built application. For some more information and detail into the functionality of ML.NET, check out the official documentation [page](https://docs.microsoft.com/en-us/dotnet/machine-learning/) as well as some of my previous posts: 

- [Serverless Machine Learning with ML.NET and Azure Functions](http://luisquintanilla.me/2018/08/21/serverless-machine-learning-mlnet-azure-functions/)
- [Deploy .NET Machine Learning Models with ML.NET, ASP.NET Core, Docker and Azure Container Instances](http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/).

The application used in this writeup contains three .NET Core projects within it. One is a class library which is what we'll use to wrap ML.NET functionality for training models as well as loading pre-trained models that will then be used to make predictions. Another is a .NET Core console application which references the class library to train and persist an ML.NET model. Finally, there's the ASP.NET Core Web API which also references the class library application to load the pre-trained model created by the console application and then makes predictions via HTTP. This application can be utilized and deployed standalone but in this writeup it will be packaged into a Docker image that will then be deployed to Azure Container Instances.  

![](/images/azdevops-mlnet-aci/azdevops-mlnet-1.png)

### Class Library

The class library can be found in the `MLModel` directory. The class library defines the observation and prediction data classes which can be found in the `IrisData.cs` and `IrisPrediction.cs` files respectively. Additionally, the `Model` class contains helper methods that train and save machine learning models, load pre-trained models and use these models to make predictions. 

### Console Application

In the solution directory we also have a console application in the `ModelTrainer` directory. This application references the class library in the `MLModel` directory to train and persist the machine learning model. 

### API

The `ModelApi` directory contains an ASP.NET Core Web API application that references the `MLModel` class library project to load the pre-trained model that is trained by the `ModelTrainer` console application and makes predictions via HTTP. The logic for making predictions can be found in the `PredictController.cs` class in the `Controllers` directory of the `ModelApi` application. 

## CI/CD Pipeline Flow

Conceptually, when the application is built and deployed manually, the machine learning model is defined and developed inside the `MLModel` class library. Once satisfied with the model, the class library is built. The console application which references the `MLModel` class library is built as well as run to train and persist a classification model in a file called `model.zip`. The `MLModel` class library is also referenced in the `ModelApi` ASP.NET Core project. Because `ModelApi` is the application we're looking to deploy in order to expose our pre-trained machine learning model, we need to find a way to package it for deployment. We'll be deploying `ModelApi` using Azure Container Instances which means we need to create a Docker image of the project that will then be pushed to a Docker registry where it will be made available for public consumption. The building of multiple projects as well as the building, publishing and deployment of the Docker image to Azure Container Instances can be standardized and automated using Azure DevOps. The rest of this write-up will focus on demonstrating step-by-step how to operationalize this machine learning application via CI/CD pipelines in Azure DevOps using Azure Pipelines.   

### Getting The Code

Before getting started, the first thing you'll want to do is fork the [mlnetazdevopssample](https://github.com/lqdev/mlnetazdevopssample) GitHub repository into your own GitHub account.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-2.png)

### Creating the Project

Navigate to [https://devops.azure.com](https://devops.azure.com), click `Start Free` and follow the prompts to either create a new account or sign into your existing account.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-3.png)

Once logged in, click `Create Project`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-4.png)

Enter the name of your project as well as a short description. Then, click `Create`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-5.png)

## The Continuous Integration (CI) Pipeline

Using Azure Pipelines, we'll configure a CI pipeline for the build and packaging steps of our application. Below is an illustration of all the steps involved in our CI pipeline:

![](/images/azdevops-mlnet-aci/azdevops-mlnet-6.png)

1. Build the class library application
2. Build the console application
3. Train and persist the ML.NET Model by running the console application.
4. Copy ML.NET model file created by console application into ASP.NET Core Web API application directory
5. Build ASP.NET Core Web API application
6. Build Docker image 
7. Push Docker image to Docker Hub

### CI Pipeline Setup

Once the project is created, in the main project page, hover over `Pipelines` and click on `Builds`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-7.png)

In the `Builds` pipeline page, click `New pipeline`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-8.png)

Select GitHub as the source and connect your GitHub account with Azure DevOps.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-9.png)

Once you have authorized Azure DevOps to use your GitHub account, select the repository and branch that will be used for this build pipeline. In our case, we'll be using the master branch of the `mlnetazdevopssample` repository. When finished configuring, click `Continue`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-10.png)

The next step will be to select the jobs to execute in our pipeline. Because there are multiple steps in this build pipeline, let's start with an `Empty Job` and customize it to our needs.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-11.png)

From inside the build pipeline page, before we start adding jobs, lets select the agent that will execute the jobs. For this pipeline, select the `Hosted Ubuntu 1604` option from the dropdown.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-12.png)

### 1. Build the Class Library Application

The first step in our CI Pipeline will be to build our class library which contains methods that wrap the training, loading and prediction functionality of the ML.NET framework and persisted models.

To achieve that, we'll add a .NET Core task to our `Agent Job 1`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-13.png)

Once added to the pipeline, let's configure this task. To make it more descriptive, we can give it a name such as `Build Class Library`. Because this task will be responsible for building the .NET Core class library, we'll leave the default `build` Command setting as is. 

![](/images/azdevops-mlnet-aci/azdevops-mlnet-14.png)

The other setting we'll want to configure is the `Working Directory`. We can do so by clicking the `Advanced` tab. 

![](/images/azdevops-mlnet-aci/azdevops-mlnet-15.png)

For this task we'll use the `MLModel` directory.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-16.png)

When finished with the configuration, click `Save &amp; Queue` -&gt; `Save` on the top toolbar.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-17.png)

Enter a detailed comment describing the change to the pipeline and click `Save`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-18.png)

### 2. Building The Console Application

Once we've built the class library application which we'll reference from the .NET Core console and ASP.NET Core Web API applications, it's time to build the console application which will serve the purpose of training and persisting the ML.NET model.

Similar to the previous step, add a new .NET Core *build* task to the pipeline. The only setting that will change for this task is the `Working Directory` which will have the value of `ModelTrainer`. 

![](/images/azdevops-mlnet-aci/azdevops-mlnet-19.png)

Although not required, when finished configuring the task, click `Save &amp; Queue` -&gt; `Save` to save and comment the changes to the pipeline. 

### 3. Train and persist the ML.NET Model

Now that our console application is built, it's time to run it in order to train and persist the ML.NET model. To do so, we'll add another .NET Core task. The difference is that the `Command` setting will now be configured with the `run` value. 

![](/images/azdevops-mlnet-aci/azdevops-mlnet-20.png)

The `Working Directory` will be set to `ModelTrainer` like in the previous task.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-21.png)

Remember to save and comment the new changes to the pipeline.

### 4. Copy ML.NET Model to Web API Directory

After the console application is run and the ML.NET model is trained, it is persisted in a file called `model.zip` inside the `ModelTrainer` directory. We can use this persisted version of the model to make predictions from both the console application or any other application of our choice. In this case, we'll be making predictions via an ASP.NET Core Web API. In order for our API to reference this file, we need to copy it into the root directory of our `ModelApi` directory. A way to perform that task is via bash script. To add a bash script to our pipeline, all we need to do is add a Bash task to it. 

![](/images/azdevops-mlnet-aci/azdevops-mlnet-22.png)

Once added to our pipeline, it's time to configure the task. We'll set the `Type` setting to `Inline` which will bring up a text box for us to type in the script. 

![](/images/azdevops-mlnet-aci/azdevops-mlnet-23.png)

Inside of the text box, enter the following content:

```bash
# Write your commands here

cp ../ModelTrainer/model.zip .

# Use the environment variables input below to pass secret variables to this script
```

This command will copy the `model.zip` file from the `ModelTrainer` directory to the `ModelApi` directory.

We can set the `Working Directory` of this step to `ModelApi`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-24.png)

Once finished, save and comment the new changes to the pipeline. 

### 5. Build ASP.NET Core Web API application

Now that we have the necessary files inside our `ModelApi` application, it's time to build it. We'll add a .NET Core task to our pipeline and set the `Command` to `build`. The `Working Directory` will be `ModelApi` like the previous task.

Save and comment the new changes to the pipeline when finished.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-25.png)

### 6. Build ASP.NET Core Web API Docker Image

The method of deployment for the ASP.NET Core Web API application is via containers. Therefore, after building the application, we have to build a Docker image for it that can then be pushed to a Docker registry of your choice. To build a Docker image, we'll add a Docker task to our pipeline.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-26.png)

When we configure the task, we'll start off by setting the `Container Registry Type` to `Container Registry`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-27.png)

This will prompt the setup of a service connection to a Docker registry if one does not already exist.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-28.png)

The Docker registry type we'll be using is Docker Hub. Give the connection a name, enter the credentials to your Docker Hub account and click `Verify this connection` to make sure that your credentials are valid and a connection can be established with Docker Hub. When finished click `OK`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-29.png)

The `Command` setting will be `build` so we can leave the default as is as well as the `Dockerfile` setting which will use the Dockerfile in the root `mlnetazdevopssample` directory.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-30.png)

Finally, we'll configure the `Image name` setting. The convention we'll use is `&lt;docker-hub-username&gt;/&lt;image-name&gt;`. In my case, `lqdev` is my Docker Hub username and I'll name my image `mlnetazdevopssample` resulting in `lqdev/mlnetazdevopssample`. Additionally, check the `Include latest tag` checkbox to have every build be the latest as opposed to tagging it with versions numbers. 

![](/images/azdevops-mlnet-aci/azdevops-mlnet-31.png)

Remember to save and comment the recent changes to the pipeline.

### 7. Push Docker Image to Docker Hub

The last step in our CI pipeline is to push our newly built image to Docker Hub. To do so we'll use anoter Docker task. 

![](/images/azdevops-mlnet-aci/azdevops-mlnet-32.png)

Like in the previous task, we'll set the `Container registry type` to `Container Registry`. Set the `Docker registry service connection` to the most recently created connection by selecting it from the dropdown. We'll be changing our `Command` to `push` and set the `Image name` to the name of the image built in the previous step. The naming convention is `&lt;docker-hub-username&gt;/&lt;image-name&gt;:latest`. The latest tag was added by our previous Docker build task so make sure that you include it in this task.

Once finished, click `Save &amp; Queue` -&gt; `Save &amp; Queue`. As opposed to only clicking `Save`, this action will manually trigger the CI pipeline.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-33.png)

Don't forget to comment your changes and click `Save &amp; queue` to kick off the CI pipeline.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-34.png)

### Monitoring the Build

When the build starts, you can click on `Builds` under the `Pipelines` section on the left pane.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-35.png)

Select the first build from the list to get more details on the build.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-36.png)

This will take you to the logs which show the status of the pipeline near real-time.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-37.png)

### Confirming CI Pipeline Success

If the build is successful, navigate to [https://hub.docker.com/](https://hub.docker.com/) to check whether the Docker image was pushed to the registry.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-38.png)

## The Continuous Delivery (CD) Pipeline

Now that we have our CI pipeline set up which will build and package our application, it's time to deploy it. We could do this ourselves or automate it using a CD pipeline. Our application wil be deployed to Azure Container Instances which is an Azure service that offers a quick way to run containers without having to worry about the management of virtual machines or orchestration services. The steps involved in our CD pipeline are the following:

1. Create Azure Resource Group for deployment
2. Deploy application to Azure Container Instances.

### CD Pipeline Setup

To get started setting up a CD pipeline, from the Azure DevOps project main page, hover over `Pipelines` and click on `Releases`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-39.png)

Once in that page, click on `New pipeline`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-40.png)

As with our CI pipeline, we'll start off with an `Empty Job` which we'll configure at a later time.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-41.png)

### Triggering Deployments

Once our pipeline is created, it's time to configure it. The first thing we'll want to do is add an artifact. An artifact can be a variety of things including the output of our build pipeline. In our case, the end our CI pipeline will be the trigger for our CD pipeline. To add an artifact, click `Add an artifact`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-42.png)

In the configuration form, set the `Source type` to `Build` and the `Source` to the name of the CI pipeline created in the previous steps. When finished, click `Add`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-43.png)


After configuring our artifact, it's time to configure the steps in the CD pipeline. To do so, click on the `Stage 1` option in the `Stages` section of the release pipeline page and change the name to something more descriptive.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-44.png)

When finished, close out the form and click on the hyperlink below the stage title. 

![](/images/azdevops-mlnet-aci/azdevops-mlnet-45.png)

You should now be on a page similar to the CI pipeline job configuration page. On this page, we'll want to click on the `Agent Job` panel to set the `Agent pool` setting to `Hosted Ubuntu 1604`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-46.png)

Once that is complete, it's time to configure the tasks in the CD pipeline.

### 1. Create Azure Resource Group

Start off adding an `Azure CLI` task to the pipeline. In this task we'll create a resource group in Azure to which we'll deploy our application to. 

![](/images/azdevops-mlnet-aci/azdevops-mlnet-47.png)

Before doing anything else, link DevOps to an Azure Subscription by selecting one from the dropdown and clicking `Authorize` which will prompt you to authenticate your subscription. 

![](/images/azdevops-mlnet-aci/azdevops-mlnet-48.png)

Once an Azure subscription has been linked, let's change the `Script Location` setting to `Inline Script`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-49.png)

In the `Inline Script` text box enter the following:

```bash
az group create --name mlnetazdevopssampleresourcegroup --location eastus
```
This script will create a resource group in Azure called `mlnetazdevopssampleresourcegroup` that is located in `eastus`. Both of these are configurable to your preference. 

### 2. Deploy Docker Image to Azure Container Instances

The next and final step in our CD pipeline is the deployment to Azure Container Instances. To deploy our application, we'll add another `Azure CLI` task. This time, since we already configured our `Azure subscription` in the previous task, we can select the service connection as opposed to a subscription from the dropdown.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-50.png)

Like in the previous task, our script will be inline. 

![](/images/azdevops-mlnet-aci/azdevops-mlnet-51.png)

In the `Inline Script` text box enter the following:

```bash
az container create --resource-group mlnetazdevopssampleresourcegroup --name mlnetcontainer --image lqdev/mlnetazdevopssample:latest --ports 80 --ip-address public
```

This script creates a container in the resource group created by the previous task of the pipeline with the name `mlnetcontainer` from the Docker image that was pushed to Docker Hub by the CI pipeline. Additionally, it opens up port 80 and assigns a publicly accessible IP address for the container to be accessed externally. 

Once this step has been configured, make sure to save and comment all your changes by clicking `Save`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-52.png)

![](/images/azdevops-mlnet-aci/azdevops-mlnet-53.png)

Then, to make it easily recognizable, edit the name of the pipeline by hovering near `New release pipeline` and clicking on the pencil icon.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-54.png)

Make sure to save and comment your changes.

## Automating CI/CD Pipelines

In the previous steps, we configured CI and CD pipelines. However, we have still not fully automated the triggers that kick off both of these. 

### CI Pipeline Trigger

First, lets start off by automating the CI pipeline. To do so, go the project's main page, hover over `Pipelines` and click on `Builds`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-55.png)

This will take you to the CI pipeline page. While on this page, click `Edit`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-56.png)

Then, click on `Triggers`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-57.png)

Once on this page, check the `Enable continous integration` checkbox and save and comment your changes by clicking `Save &amp; Queue` -&gt; `Save`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-58.png)


### CD Pipeline Trigger

To automate the CD pipeline trigger, click on `Releases` under the `Pipelines` page to automate the CD pipeline.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-59.png)

Once on the CD pipeline's page, click `Edit`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-60.png)

Then, click on the lightning icon in the Artifacts section which will show a configuration form. In this form, toggle the `Continuous deployment trigger` setting to `Enabled`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-61.png)

When finished, save and comment your changes. 

## Running CI/CD Pipelines

Although going forward builds and deployments will be started when new changes are checked into the master branch of the `mlnetazdevopssample` repository, for demonstration purposes we will manually kick off the CI/CD pipelines we have just configured. To do so, click on `Builds` under the `Pipelines` section on the left pane.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-62.png)

From the CI pipeline page click `Queue`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-63.png)

This will prompt a modal to show up in which you can just click `Queue` to start the build.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-64.png)

This will kick off a new CI build which subsequently will also kick off the CD pipeline of your application. 

## Testing The Deployment

If all is successful, a Docker image of an ASP.NET Core Web API application will be deployed to Azure Container Instances which can be accessed via a public IP address. 

To see whether the deployment worked, navigate to [https://portal.azure.com/](https://portal.azure.com/) and click on `Resource groups`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-65.png)

At this point, you should see the resource group that was created by the CD pipeline. If that's the case, click on it. 

![](/images/azdevops-mlnet-aci/azdevops-mlnet-66.png)

This will then show a page that displays the container that was deployed to this resource group. Click on that.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-67.png)

The container page will display diagnostic and configuration information about the container. The information we're interested in is the `IP address`. Hover to the right of it and click on the icon that says `Click to copy`. This will copy the address to the clipboard.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-68.png)

In an application like Postman or Insomnia, make an HTTP POST request to `http://&lt;ip-address&gt;/api/predict` where `ip-address` is the public IP address of the container in Azure with the following body.

```json
{
    "SepalLength":3.3,
    "SepalWidth":1.6,
    "PetalLength":0.2,
    "PetalWidth":5.1
}
```

If successful, the response will be `Iris-virginica`.

![](/images/azdevops-mlnet-aci/azdevops-mlnet-69.png)

## Conclusion

In this writeup, we operationalized the building, packaging and deployment of an ML.NET application that predicts the class of an Iris flower using a variety of mesurements with Azure DevOps. We created both a Continous Integration as well as a Continous Delivery pipeline which deploys the Docker image of an ASP.NET Core Web API to Azure Container Instances. Keep in mind this is just one way of doing it and Azure DevOps is flexible in how all of these tasks and workflows are configured to meet your requirements. Happy coding!

###### Resources

[ML.NET Samples](https://github.com/dotnet/machinelearning-samples)
[DevOps for Data Science](https://www.youtube.com/watch?v=bUTBBS1TECc)]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/azdevops-mlnet-aci</link>
      <guid>https://www.luisquintanilla.me/posts/azdevops-mlnet-aci</guid>
      <pubDate>2018-11-26 23:50:23</pubDate>
    </item>
    <item>
      <title>Back To School: Emacs Edition</title>
      <description>&lt;![CDATA[
As the summer comes to a close and I prepare to enter the fall semester, I am getting back into school mode. When programming in the terminal, I usually used vim. When compared to Emacs, the learning curve was relatively flat and it handled file editing just fine. I’ve read many articles that rave about Emacs and how powerful it can be. Therefore, a couple of months ago I took the plunge and decided to give it a try. Using dired mode to browse through the various directories in my system or project was a neat feature but for my purposes of basic file editing it did not offer anything that would warrant a switch. Furthermore, the default key bindings seemed a bit too much for me which caused me to  revert back to vim.

As part of the articles that I have read in praise of Emacs, one particular feature seemed to be a recurring theme – Org-Mode. Although I had heard about it, I never really understood what it was. Out of curiosity, I decided to try it out for myself and see what it had to offer. While taking notes in it seemed normal enough, it was the export functionality that truly rocked my world and made me realize how amazing this could be for note-taking. Traditionally a pen-and-paper note taker, one of the main reasons I dislike taking notes on my PC is the lack of continuity and organization. Although tools like OneNote of Evernote are extremely powerful, they were not exactly the best when it came to a semester’s worth of notes and although they could be made to have all of the notes for a semester on one page, this defeated the purpose of having different pages and notebooks. Enter Org-Mode. Again, while the process of taking notes is nothing special, the export capabilities are amazing. Furthermore, by default a table of contents is created with links so you can always find what you want when you want. Here are a few examples of what I mean.

![](/images/back-to-school-emacs-edition/backtoschoolemacsedition1.png)

As it can be seen, other than * or + characters in front of titles and lists, there is nothing special in the terminal. This is how it looks like in HTML format

![](/images/back-to-school-emacs-edition/backtoschoolemacsedition2.png)

Amazing! Not only do you get an organized set of notes, but you can see how this can easily contain a semester’s worth of notes in a single HTML file with main ideas being easily accessible via links. Now, maybe you don’t have an internet connection or prefer to put these notes on your tablet or phone. No problem. Just export it to a PDF.

![](/images/back-to-school-emacs-edition/backtoschoolemacsedition3.png)

Magic!

This is definitely going to take my note-taking to the next level by allowing me to have these documents synced across all my devices. Furthermore, it will keep notes organized and in one place. If you’re interested in getting started there may be just a few things that you need to do.

# Install Org-Mode

Org Mode is built in to the latest version of Emacs. However, in the event that it is not installed you can simply try the following commands in the Emacs mini buffer.

`M-x package-install RET org RET`

# Export to HTML

To access the export menu, simply type the following in the mini buffer:

`C-c C-e`

HTML export is included by default. To do that, we can add on to the command used to access the export menu as follows:

`C-c C-e h h`

PDF export is a bit mode complicated. The file is first exported to LaTeX format and then converted to PDF. However, in order to do this you need to install a package on your system. Assuming you are on a Unix machine, the following command should download everything necessary to convert LaTeX files to PDF. Keep in mind, this is relatively large and should take up roughly 1-2 GB in your system. In return, it limits the number of headaches due to installing necessary packages piecemeal that have multiple dependencies. To download the package, type in the following into your terminal:

# Install texlive-full

In the terminal, enter: 

```bash
sudo apt-get install texlive-full
```
Once installed, you can use a similar command to that of HTML export.

# Export to PDF

`C-c C-e l p`

This should be all you need to get started. For more information on how to use Org Mode, here’s a guide with live examples. Happy note-taking!

[Emacs Examples and Cookbook](http://home.fnal.gov/~neilsen/notebook/orgExamples/org-examples.html)

]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/back-to-school-emacs-edition</link>
      <guid>https://www.luisquintanilla.me/posts/back-to-school-emacs-edition</guid>
      <pubDate>2017-12-09 13:01:19</pubDate>
    </item>
    <item>
      <title>Blogging Tools</title>
      <description>&lt;![CDATA[
## TLDR

These are the tools I use to write this blog:

- **Editor**: [Visual Studio Code](https://code.visualstudio.com/)
- **Markup**: [Markdown](https://daringfireball.net/projects/markdown/)
- **Static Site Generator**: [Hexo](https://hexo.io/) 
- **Blog Theme**: [Cactus](https://github.com/probberechts/hexo-theme-cactus)
- **Hosting**: [Namecheap](https://www.namecheap.com/)

## Introduction

I have gotten a few questions regarding what I use to write this blog so I thought it would be a good idea to write a post about it. In this writeup I'll discuss what I use for writing posts as well as building and hosting the site. 

## Writing

### Editor

For writing, I use Visual Studio Code. Visual Studio Code is an open source, cross-platform text editor. One of the nice features it includes is Markdown Preview. Markdown Preview allows you to see how the content would look when rendered as a web page. Additionally, Visual Studio Code allows you to split panes. What that provides is the ability to write and preview the content side-by-side. 

![](/images/blog-tools/blog-tools-1.PNG)

### Markup 

I write my content in Markdown. Markdown is a plain text formatting syntax that allows authors to write easy-to-read and easy-to-write plain text formats that are then converted to HTML. It can take a while to get used to the syntax especially when including images, links and code snippets but once you get comfortable with it, it becomes natural and no different than writing sentences in plain text. While this is not a requirement for everyone, since a fair amount of my content includes code snippets, Markdown makes it easy to include such content in the language of choice. 

## Website

### Static Site Generation

While Markdown helps me format my plain text, at some point it needs to be converted into HTML or some format that can be rendered on the web. To help with that, I use Hexo. Hexo is an open source blog framework that allows you to write content using Markdown or other languages and generates static files which have a theme applied to them. Hexo is built on JavaScript, therefore you'll need NodeJS to configure it. 

### Theme 

Hexo has numerous themes that can be used to style your site. The one I use is the `Cactus` theme. 

## Hosting

For hosting, I use Namecheap. There is no special reason for this other than I had a slot available in an existing hosting plan. My method of deployment is via FTP. Since my site is all static assets, I upload them into a folder that is publicly accessible. With that in mind, by having my site made up of static assets I have the flexibility of hosting it on an S3 bucket or Azure Storage. This is the method I'm looking to use for deployments and hosting in the near future. Here is a [link](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-static-website) that details how to host a static site on Azure Storage.]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/blog-tools</link>
      <guid>https://www.luisquintanilla.me/posts/blog-tools</guid>
      <pubDate>2018-11-18 00:07:36</pubDate>
    </item>
    <item>
      <title>Design and Build Your Next Home in Minecraft</title>
      <description>&lt;![CDATA[
#### TLDR

Using Minecraft and Mods, virtual representations of the real world can be built to train autonomous agents to navigate and perform tasks in this virtual environment. These virtual models can then be used by physical agents such as Boston Dynamics bots to implement the tasks and instructions learned using the virtual models. Ultimately this makes it reasonable to think of a time where we can design and have our house built in a short time span by autonomous artificial agents.

# Minecraft 

Minecraft is so hard to describe. Is it a game or is it an educational tool? At first I didn’t get it. I watched younger siblings play this game endlessly always finding ways to improve their worlds. Eventually it was bought by Microsoft and since then it has left the core experience relatively untouched. Instead of shelving it away, Microsoft has expanded across more platforms and loaded it with tons of new features such as an education edition and cross-platform play. In Minecraft I have seen individuals build houses, cities, and even rudimentary models of circuits using Redstone. Furthermore, because the project is programmable, individuals can build modifications also known as Mods to extend the default capabilities.

![](/images/build-your-house-minecraft/buildyourhouseminecraft1.png)

# Dynamic Mapping 

A virtual world is great, but what about the real world? Can we model the real world? A few months ago I came across a project by the New York Public Library which generated topographically accurate Minecraft maps using historical map scans and Python scripts. Although the accuracy of historical maps may be questionable, we can only hope that mapping practices have advanced significantly since then and can provide more details and data. Regardless, this project demonstrates that it is possible to model the real world in a virtual environment given the appropriate data. 

![](/images/build-your-house-minecraft/buildyourhouseminecraft2.png)

# Project Malmo

Aside from having the obvious benefit of having a relatively accurate virtual model of the real world, there is the additional benefit of having a starting point for training autonomous agents to navigate this environment. To aid in this endeavor, Project Malmo is an adequate tool. Released to the public in 2015 by Microsoft Research, Project Malmo is a platform for artificial intelligence experimentation and research built on top of Minecraft. The project is cross-platform and agents can be built in Python, Lua, C#, C++ or Java. 

While the benefits of Project Malmo aside from the pedagogical exercise may not be clearly evident, this project is extremely valuable. The cost not only in terms of price but also time to build physical autonomous bots can be high. If for some reason they were to break or malfunction during field tests it can be costly. Therefore a more effective solution might be to simulate the navigation and mobility of these physical autonomous agents in a real world scenario using virtual environments. 

[![Project Malmo](/images/build-your-house-minecraft/buildyourhouseminecraft3.png)](http://www.youtube.com/watch?v=KkVj_ddseO8)

# Manual Labor

Once trained, we just need to find or build the physical agents that will carry out the work and instructions based on training performed in the virtual environments. There have been recent demonstrations of agents that may be able of handling such tasks. One of those is Boston Dynamics. For some time, Boston Dynamics has been creating anthropomorphic and zoomorphic robots that perform amazing feats. It can only be assumed that given a model that has been trained in a real-world environment, these physical agents can perform a task like building a home. 

The thought of a bot building a house is not too abstract. On March of 2017 several reports suggested that a home in Russia was built in under 24 hours using 3-D printing technologies. While the house had a predefined layout and the robotic arm that built it was limited in its mobility, such capabilities could be expanded with the aid of a trained model being executed by sophisticated physical autonomous agents such as those built by Boston Dynamics.

![](/images/build-your-house-minecraft/buildyourhouseminecraft4.png)

###### Sources

[4K Minecraft](https://www.engadget.com/2017/06/14/minecraft-4k-slider-xbox-one-x-e3-microsoft-mojang/)
[Historical Minecraft](https://github.com/NYPL/historical-minecraft/)  
[Project Malmo](https://www.microsoft.com/en-us/research/project/project-malmo/)  
[Boston Dynamics](https://www.bostondynamics.com)  
[House Built in 24 Hours](http://mentalfloss.com/article/92757/3d-printed-house-built-24-hours-russia)
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/build-your-house-minecraft</link>
      <guid>https://www.luisquintanilla.me/posts/build-your-house-minecraft</guid>
      <pubDate>2017-12-15 22:34:12</pubDate>
    </item>
    <item>
      <title>Build your own self-hosted live streaming application with Owncast and .NET Aspire</title>
      <description>&lt;![CDATA[
Platforms come and go. As a result, I'm a strong advocate for [owning your data](https://indieweb.org/own_your_data) (when possible). Self-hosting is a way for you to do that. Owning your data could mean self-hosting [your website](https://buttondown.com/ownyourweb/archive/issue-06/), [password manager](https://bitwarden.com/blog/host-your-own-open-source-password-manager/), [media server](https://jellyfin.org/docs/), or [social media](https://docs.joinmastodon.org/user/run-your-own/). However, self-hosting comes with its own challenges, primarily cost (time and money) and in cases where software isn't provided as an appliance on your hosting provider of choice, some technical knowledge may be required. 

As I got more into self-hosting applications such as my [Mastodon instance](/mastodon), I came across Owncast.   

During peak COVID lockdowns, like many others, live streaming is one of the ways I passed the time. While Twitch and YouTube got the job done, self-hosting was always in the back of my mind.  

It's been a while since I've live-streamed, so I never really went through the process of evaluating the self-hosted route with Owncast. 

While browsing for something on YouTube the other day, I ran into some of my [old live-stream recordings](https://www.youtube.com/playlist?list=PLsdMoYmuvh9ZtgB8U7FECR_8wKMYXJNAm). This got me thinking again, how difficult would it be to put together my own live-streaming setup. 

This post is the result of that exploration. 

In this post, I'll modify a .NET Aspire Starter Application template and show how to set up a self-hosted live-streaming application using Owncast and .NET Aspire.  

You can find the source code in the [lqdev/BYOwncastAspire](/github/BYOwncastAspire) repository.

![BYOwnCastAspire Web Frontend](https://github.com/user-attachments/assets/bdd9f901-8d8f-45be-9e37-4dce8459e481)

## What is Owncast?

The Owncast website describes the project as, "...a free and open source live video and web chat server for use with existing popular broadcasting software."

![Owncast admin server page](https://github.com/user-attachments/assets/769ec2d8-a3d1-4ec3-8271-c2de6e11ddd3)

It goes on to further describe some of the reasons I like Owncast, which are:

- **Self-hosting** - I'm in complete control over the service and my data
- **Open-source** - I can freely use and contribute to the project. Free in this case meaning both as in freedom and pizza.
- **Builds on top of open standards like RMTP** - Software that supports the Real-Time Messaging Protocol (RMTP) like OBS can immediately be leveraged. 
- **Fediverse compatible** - Your content and network federated across [the Fediverse](https://joinfediverse.wiki/What_is_the_Fediverse%3F). 

To learn more, check out the [Owncast website](https://owncast.online/). 

## What is .NET Aspire?

The .NET Aspire documentation describes it as, "...an opinionated, cloud ready stack for building observable, production ready, distributed applications.​ .NET Aspire is delivered through a collection of NuGet packages that handle specific cloud-native concerns."

Personally, the parts of .NET Aspire that matter to me are:

- **App composition** - The .NET Aspire programming model makes it easy to define and compose a variety of resources such as .NET projects, containers, and much more in a single place. In many cases, for commonly used services, these resources are exposed in the form of [integrations](https://learn.microsoft.com/dotnet/aspire/fundamentals/integrations-overview). These integrations further simplify the composition of applications.  
- **Tooling (Dashboard)** - .NET Aspire provides a set of tools and templates. However, my favorite is the dashboard. The dashboard provides me with a single place to view my resources, their configurations, and logs. 

Although in this post, I don't cover deployment, there is also the provisioning component provided by .NET Aspire which in many cases can [simplify your application deployments](https://learn.microsoft.com/dotnet/aspire/deployment/overview). 

To learn more, check out the [.NET Aspire documentation](https://learn.microsoft.com/dotnet/aspire/get-started/aspire-overview). 

## Build your application

This application makes a few modifications to the [.NET Aspire Starter Application template](https://learn.microsoft.com/dotnet/aspire/fundamentals/setup-tooling?tabs=linux&amp;pivots=dotnet-cli#net-aspire-project-templates). 

The application consists of a few projects:

- **BYOwncastAspire.AppHost** - This project is the entrypoint for .NET Aspire applications. This is where we'll configure the Owncast server as a container resource. 
- **BYOwncastAspire.Web** - Blazor web application. Although Owncast provides its own page where viewers can tune into your stream, by having a separate web application, preferably your own personal website, you can further enrich and customize how and where you publish content. 
- **BYOwncastAspire.ServiceDefaults** - This project contains default configurations for telemetry, health checks, resiliency, etc. No changes or updates required here. 

### Configure Owncast

There are many ways to host an Owncast server, with one of them being a container. 

In the context of Aspire which has built-in container support, you can easily add Owncast as a resource in your application. 

From the [Owncast documentation](https://owncast.online/quickstart/container/), once you've pulled the Owncast container image, you can start it with the following command.

```bash
docker run -v `pwd`/data:/app/data -p 8080:8080 -p 1935:1935 owncast/owncast:latest
```

This translates to the following in the *Program.cs* of the *BYOwncastAspire.AppHost* project.

```csharp
var owncast = builder
    .AddContainer(name: "owncast", image:"owncast/owncast")
    .WithBindMount("./data","/app/data")
    .WithHttpEndpoint(port:8080,targetPort:8080,name:"admin")
    .WithHttpEndpoint(port:1935,targetPort:1935,name:"streaming")
    .WithExternalHttpEndpoints();
```

This code:

- Adds the owncast container image
- Mounts a local *data* directory to the */app/data* directory in the container
- Maps port `8080` for the Owncast admin server and `1935` for RMTP server.
- Exposes the endpoints publicly

### Embed your stream

Owncast provides the ability to [embed your video stream onto a website](https://owncast.online/docs/embed/). 

Although we don't need a frontend because one is already provided by Owncast, by embedding your stream on your website you can provide a single place for your viewers to consume your content. 

In this case, we can treat the *BYOwncastAspire.Web* project as our website. 

To embed your stream to the website, add the following code to your *Home.razor* page.

```csharp
&lt;iframe
  src="http://localhost:8080/embed/video"
  title="Owncast"
  height="350px" width="550px"
  referrerpolicy="origin"
  allowfullscreen&gt;
&lt;/iframe&gt;
```

In this case, we're pointing to the endpoint listening on port `8080` of our `localhost`. When you deploy the application, you'd replace `src` with your domain. 

## Start your application

That's all there is in terms of configuration. 

To start the application:

1. Open the terminal 
2. Navigate to the *BYOwncastAspire.AppHost* project and run the following command.

    ```bash
    dotnet run
    ```

This will launch you into the .NET Aspire dashboard. At this point, you can further customize your Owncast server as well as the website.  

![BYOwncastAspire .NET Aspire Dashboard Resource Page](https://github.com/user-attachments/assets/c20d84d8-925a-4f80-9058-622466cb08e9)

## What next?

Now that you have your application running, in its current form, this application is only meant to serve as a sample of what you can do with Owncast and .NET Aspire. 

Some next steps might include:

- [Change your admin password and customize your Owncast server](https://owncast.online/docs/configuration/)
- [Configure object storage](https://owncast.online/docs/storage/)
- [Set up your steaming software](https://owncast.online/docs/broadcasting/obs/)
- [Deploy your application](https://learn.microsoft.com/dotnet/aspire/deployment/azure/aca-deployment)
- [Extend the app's functionality by building custom plugins and extensions](https://owncast.online/thirdparty/)

The plugins and extensions are particularly interesting to me because there may even be opportunities to experiment and insert AI capabilities at various layers such as moderation, translation, accessibility, show notes, chat summaries, etc. 

## Conclusion

In this post, I showed how you can combine the powerful live-steaming features of Owncast with the app orchestration and tooling in .NET Aspire to create rich self-hosted live-steaming applications.

If you use this sample as a starting point for your own live-streaming or self-hosting explorations, [send me a message](/contact). I'd love to hear about it.  
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire</link>
      <guid>https://www.luisquintanilla.me/posts/build-your-own-live-streaming-app-owncast-dotnet-aspire</guid>
      <pubDate>2024-10-20 14:01</pubDate>
    </item>
    <item>
      <title>The Case for Doing Machine Learning with F#</title>
      <description>&lt;![CDATA[
## Introduction

This post is part of the 2018 [FsAdvent](https://sergeytihon.com/tag/fsadvent/) series organized by [Sergey Tihon](https://twitter.com/sergey_tihon).

When searching for tools and languages to implement machine learning applications, there are numerous options to choose from each with their own set of advantages and disadvantages. Out of all, however, Python seems to be the most salient. Not only is it a popular language but also many of the tools available for machine learning are either implemented in Python or support it in some capacity whether it's native or through community libraries. Very rarely though is F# mentioned in these discussions despite having many of the features that make languages like Python so loved and extending them to empower users. In this writeup, I will do a short review of many of the advantages of Python such as succinctness, platform support, library availability as well as many others and compare it to F#'s capabilities.

![2017 Top Languages](/images/case-fsharp-machine-learning/case-fsharp-ml-0.PNG)

## Python's Advantages

### Learning Curve

One of the reasons why Python is so widely adopted is its learning curve. Whether an individual knows how to program or not, at times, Python can look like pseudocode making it accessible to not only readers but also writers. As with anything the more complex the task, the steeper the learning curve. However, at a simpler level, Python makes it as easy as possible to get started. 

Although at first it may not appear to be the case with F#, the learning curve is not much steeper than that of Python. The syntax can sometimes look intimidating to individuals, but the steepest part of the learning curve doesn't necessarily come from the language itself but rather from the way of reasoning about the logic of the programs. As a functional language, there is somewhat of a paradigm shift from that of a procedural execution model. Below is an example that defines a function that doubles an integer in both languages.  

##### Python

```python
def double(x):
    return x * 2
```

##### FSharp

```fsharp
let double x = 
    x * 2
```

As it can be seen, despite some minor syntax and character differences the functions are essentially the same.  

### Intended Purpose

Depending on the task at hand, some languages are more adept for handling respective tasks. For example, R and Julia are excellent languages when performing statistical tasks. However, outside of those types of tasks their abilities are more limited. Python, being a general-purpose language means that not only can you use it for machine learning tasks but also to build n-tier applications entirely in Python without having to worry about integrations, plugins or having to learn an entirely different language to perform such actions. 

Similarly, F# is a general-purpose language which allows you to build web and console applications as well as machine learning applications all from the comfort of the same ecosystem.

### Strong Library Support

When performing data analysis and machine learning, practicioners use a variety of libraries for their development such as [NumPy](http://www.numpy.org/) and [Pandas](https://pandas.pydata.org/) for data wrangling, [scikit-learn](https://scikit-learn.org/stable/index.html) for machine learning algorithms and [matplotlib](https://matplotlib.org/) for creating visualizations. Although all of these tasks could most certainly be implemented from scratch, libraries speed up the development process allowing practitioners to focus more on the domain and experiment with the models that best solve the respective problem they are facing.  

F#, like Python has exceptional library support, specifically as it regards data science and machine learning. [FsLab](https://fslab.org/) is a collection of open source F# packages for machine learning that contain libraries such as [FSharp.Data](https://fsharp.github.io/FSharp.Data/) and [Deedle](https://fslab.org/Deedle/) for data wrangling, [Math.NET Numerics](https://numerics.mathdotnet.com/) for machine learning algorithms and [XPlot](https://fslab.org/XPlot/) to help with data visualization. Furthermore, at Build 2018, Microsoft released [ML.NET](https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet), an open-source, cross-platform machine learning framework for .NET which allows .NET users to not only perform machine learning tasks within .NET but also allows extensibility via Tensorflow, ONNX and Python extensions. For a more detailed writup on using ML.NET with F#, check out the post [A Christmas Classifier](https://towardsdatascience.com/f-advent-calendar-a-christmas-classifier-72fb9e9d8f23) by Willie Tetlow or my post [Classification with F# ML.NET Models](http://luisquintanilla.me/2018/06/13/mlnet-classification-fsharp/).

Libraries for F# are also not confined only to those written in F#. In many instances, because F# is a .NET language, there is interoperability with C# libraries further extending the capabilities of F#.

### Platform Support

One of the things that makes Python so attractive is that it runs cross-platform. It does not matter whether you're on Windows, Mac or Linux; Python code runs the same. That being said though, not all platforms are created equal and although it is possible to run Python code on all platforms, essential libraries such as NumPy, Pandas and scikit-learn run best on Unix environments. It is possible to run them on Windows but the set up is not as straightforward. 

As a .NET Language, F# runs on Windows. With the help of the Mono runtime and most recently .NET Core, it also runs on Mac and Linux. Like Python, depending on the runtime and dependencies used by the respective software packages there may be limitations as to which platform code can be run on. However, from my experience most of the FsLab set of packages work cross-platform. 

### Succintness

Productivity is an important measure of a language. One way to achieve it is to write logic using the least number of characters. This is where Python shines. As a dynamically typed, whitespace ruled language, Python does not require developers to declare types associated with the objects and variables defined nor does it require the use of brackets and any other special characters. As expected, this allows for developers to write the same logic with less characters much faster. 

Unlike Python, F# is a statically typed language. However, thanks to type inference and the help of the compiler, writing programs often does not require developers to explicitly define what types objects and variables are. Additionally, it is a whitespace ruled language therefore removing the need for brackets and additional characters further speeding up the development process in a safe manner.

### Immediate Feedback

Writing safe and effective code takes time and experience. However, even the most experienced developers often makes mistakes. Therefore, getting immediate feedback before adding certain logic to programs goes a long way to making code that is safe, efficient and tested.  

#### Read-Evaluate-Print Loop (REPL)

One way in which both Python and F# provide immediate feedback is via the command line using the Read-Evaluate-Print Loop (REPL). The REPL is a programming environment that reads the user input, evaluates it and prints out the results, hence the name. Below are screenshots of what that environment looks like using both Python and F#.

##### Python

![](/images/case-fsharp-machine-learning/case-fsharp-ml-1.png)

##### FSharp

![](/images/case-fsharp-machine-learning/case-fsharp-ml-2.png)

As it can be seen, getting that immediate feedback makes it easier to see whether the code is behaving the way it should. Although useful, this environment is not ideal when experimenting and making tweaks to the code. Additionally, because of the lack of a graphical user interface, the navigation can be less than ideal. Fortunately there is a solution out there that provides the same level of interactivity along with a graphical user interface that allows for ad-hoc experimentation and re-running of code at different points in time which is essential when developing machine learning applications.

#### Jupyter Notebooks

Much of machine learning deals with experimentation. Experimentation involves having a way to tweak parameters and evaluate the results. In addition, experiments should have a way of being documented and reproduced. As alluded to previously, such development environment and capabilities can be found in [Jupyter Notebook](https://jupyter.org/). 

As mentioned on the project's website: 

&gt; The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more.

Jupyter Notebooks work with various languages which include but are not limited to Python and F# and can be run both on local machines as well as via a hosted service. One such service is Microsoft's [Azure Notebooks](https://notebooks.azure.com/) which allows you to use Jupyter Notebooks in the cloud for free. All you need is to have a Microsoft account. Below are screenshots of what that environment looks like in both Python and F#.   

##### Python

![](/images/case-fsharp-machine-learning/case-fsharp-ml-3.png)

##### FSharp

![](/images/case-fsharp-machine-learning/case-fsharp-ml-4.png)

## Beyond Python

F#, is comparable to Python on many of the features that make Python a great language for both development and machine learning. However, there are some areas where F# provides additional safety and functionality that greatly improve the way in which machine learning applications are built. 

### Typing

As previously mentioned, F# is a statically typed language. However, it also makes use of type inference which means that declaring types is not always required. With the help of the compiler, based on the structure of functions and variables it is possible to determine which type an object or variable is. This has several advantages. One advantage of being strongly typed is that when the types are declared the code becomes self-documented because it is easier to deduce what functions are doing based on the types being passed in and returned. Another advantage is that it makes it harder to write bad code. Having the compiler help you when you write your code allows you to find errors prior to compilation or running the code. This along with the REPL gives you additional reassurance that your code is executing the intended logic. 

Python is making strides in acquiring some of that functionality with the introduction of type hints in version 3.6. However, this has not always been a core feature of the language and is only in its nascent stages. 

### Immutability

As a functional language, immutability is something that is a native part of the language. While in some cases it can change the way in which code is written, immutability has one advantage, especially when it comes to machine learning. With immutability, parallelization can be fully exploited for those algorithms which take advantage of it. 

## Conclusion

In this writeup, I went over how F# is comparable to Python on many of the features that make it such a popular language such as library support, succinctness, general purpose and interactivity. However, F# has additional capabilities such as static typing and immutability that further enhance its capabilities as a language for machine learning. That is not to say that one is better than the other as they both are more adept for performing certain tasks. When it comes to machine learning it does become a matter of choice as they are both robust, powerful and strongly supported languages. Therefore next time you're looking to build a machine learning application, hopefully you give F# a try. Happy coding!

###### Resources

[2018 Top Programming Languages](https://spectrum.ieee.org/at-work/innovation/the-2018-top-programming-languages)  
[Get Programming with F#](https://www.manning.com/books/get-programming-with-f-sharp)  
[ML.NET Machine Learning Samples](https://github.com/dotnet/machinelearning-samples)]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/case-fsharp-machine-learning</link>
      <guid>https://www.luisquintanilla.me/posts/case-fsharp-machine-learning</guid>
      <pubDate>2018-12-14 23:34:50</pubDate>
    </item>
    <item>
      <title>Use machine learning to categorize web links with F# and ML.NET</title>
      <description>&lt;![CDATA[
## Introduction

This post is part of [F# Advent 2019](https://sergeytihon.com/2019/11/05/f-advent-calendar-in-english-2019/). Thank you [Sergey Tihon](https://twitter.com/sergey_tihon) for organizing this and the rest of the contributors for producing interesting, high-quality content.

I scan and read articles on a constant basis, such as those published as part of F# Advent. Those that I find interesting, or I want to save for later, I bookmark using [Pocket](https://getpocket.com/). One of the neat features it provides is tagging. You can add as many tags as you want to organize the bookmarked links. When I first started using the service, I was fairly good at adding tags. However, I've gotten lazy and don't do it as much. It would be nice if bookmarked links could automatically be categorized for me without having to provide the tags manually. Using machine learning, this task can be automated. In this writeup, I will show how to build a machine learning model using [ML.NET](https://dotnet.microsoft.com/apps/machinelearning-ai/ml-dotnet), a .NET, open-source, cross-platform machine learning framework to automatically categorize web links / articles.

## Prerequisites

This application was built on a Windows 10 PC, but should work cross-platform.

- [.NET Core SDK 2.1+](https://dotnet.microsoft.com/download)
- [Visual Studio Code](https://code.visualstudio.com/Download)
- [Ionide](http://ionide.io/)
- [Microsoft.ML NuGet package](https://www.nuget.org/packages/Microsoft.ML/)

## Create the solution

Make a new directory and create a solution by using the .NET CLI.

```powershell
mkdir FsAdvent2019
cd FsAdvent2019
dotnet new sln
```

Then, create an F# Console application.

```powershell
dotnet new console -o FsAdvent2019 -lang f#
```

Navigate to the console application directory and install the Microsoft.ML NuGet package.

```powershell
cd FsAdvent2019
dotnet add package Microsoft.ML -v 1.4.0
```

## Get the data

[Click on this link to download and unzip the data anywhere on your PC](https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip).

The data contains information about several articles that are separated into four categories: business (b), science and technology (t), entertainment (e) and health (h). Visit the [UCI Machine Learning repository website](https://archive.ics.uci.edu/ml/datasets/News+Aggregator) to learn more about the dataset.

Below is a sample of the data.

```text
ID    Title    Url    Publisher    Category    Story    Hostname    Timestamp
2	Fed's Charles Plosser sees high bar for change in pace of tapering	http://www.livemint.com/Politics/H2EvwJSK2VE6OF7iK1g3PP/Feds-Charles-Plosser-sees-high-bar-for-change-in-pace-of-ta.html	Livemint	b	ddUyU0VZz0BRneMioxUPQVP6sIxvM	www.livemint.com	1394470371207
3	US open: Stocks fall after Fed official hints at accelerated tapering	http://www.ifamagazine.com/news/us-open-stocks-fall-after-fed-official-hints-at-accelerated-tapering-294436	IFA Magazine	b	ddUyU0VZz0BRneMioxUPQVP6sIxvM	www.ifamagazine.com	1394470371550
4	Fed risks falling 'behind the curve', Charles Plosser says	http://www.ifamagazine.com/news/fed-risks-falling-behind-the-curve-charles-plosser-says-294430	IFA Magazine	b	ddUyU0VZz0BRneMioxUPQVP6sIxvM	www.ifamagazine.com	1394470371793
```

Inside the console application directory, create a new directory called *data* and copy the *newsCorpora.csv* file to it.

```powershell
mkdir data
```

## Define the schema

Open the *Program.fs* file and add the following `open` statements at the top.

```fsharp
open Microsoft.ML
open Microsoft.ML.Data
```

Directly below the `open` statements, define the data schema of the input and output of the machine learning model as records called `ModelInput` and `ModelOutput` respectively.

```fsharp
[&lt;CLIMutable&gt;]
type ModelInput = {
    [&lt;LoadColumn(1)&gt;]
    Title:string
    [&lt;LoadColumn(2)&gt;]
    Url:string
    [&lt;LoadColumn(3)&gt;]
    Publisher:string
    [&lt;LoadColumn(4)&gt;]
    Category:string
    [&lt;LoadColumn(6)&gt;]
    Hostname:string
}

[&lt;CLIMutable&gt;]
type ModelOutput = {
    PredictedLabel: string
}
```

As input, only the **Title**, **Url**, **Publisher** and **Hostname** columns are used to train the machine learning model and make predictions. The label or value to predict in this case is the **Category**. When a prediction is output by the model, its value is stored in a column called **PredictedLabel**.

## Create the application entry point

The `MLContext` is the entry point of all ML.NET applications which binds all tasks like data loading, data transformations, model training, model evaluation, and model saving/loading.

Inside of the `main` function, create an instance of `MLContext`.

```fsharp
let mlContext = MLContext()
```

## Load the data

Once the `MLContext` is initialized, use the `LoadFromTextFile` function and provide the path to the file containing the data.

```fsharp
let data = mlContext.Data.LoadFromTextFile&lt;ModelInput&gt;("data/newsCorpora.csv")
```

## Create training and test datasets

It's often good practice to split the data into train and test sets. The goal of a machine learning model is to accurately make predictions on data it has not seen before. Therefore, making predictions using inputs that are the same as those it was trained on may provide misleading accuracy metrics.

Use the `TrainTestSplit` to split the data into train / test sets with 90% of the data used for training and 10% used for testing.

```fsharp
let datasets = mlContext.Data.TrainTestSplit(data,testFraction=0.1)
```

## Define the transformation and algorithm pipelines

Now that the data is split, define the set of transformations to be applied to the data. The purpose of transforming the data is to convert it into numbers which are more easily processed by machine learning algorithms.

### Preprocessing pipeline

The preprocessing pipeline contains the series of transformations that take place before training the model. To create a pipeline, initialize an `EstimatorChain` and append the desired transformations to it.

```fsharp
let preProcessingPipeline = 
    EstimatorChain()
        .Append(mlContext.Transforms.Text.FeaturizeText("FeaturizedTitle","Title"))
        .Append(mlContext.Transforms.Text.FeaturizeText("FeaturizedUrl","Url"))
        .Append(mlContext.Transforms.Text.FeaturizeText("FeaturizedPublisher","Publisher"))
        .Append(mlContext.Transforms.Text.FeaturizeText("FeaturizedHost","Hostname"))
        .Append(mlContext.Transforms.Concatenate("Features",[|"FeaturizedTitle"; "FeaturizedUrl" ;"FeaturizedPublisher"; "FeaturizedHost"|]))
        .Append(mlContext.Transforms.Conversion.MapValueToKey("Label","Category"))
```

In this preprocessing pipeline, the following transformations are taking place:

1. Convert the *Title*, *Url*, *Publisher* and *Hostname* columns into numbers and store the transformed value into the *FeaturizedTitle*, *FeaturizedUrl*, *FeaturizedPublisher* and *FeaturizedHost* columns respectively.
2. Combine the *FeaturizedTitle*, *FeaturizedUrl*, *FeaturizedPublisher* and *FeaturizedHost* into one column called *Features*.
3. Create a mapping of the text value contained in the *Category* column to a numerical key and store the result into a new column called *Label*.

### Algorithm pipeline

The algorithm pipeline contains the algorithm used to train the machine learning model. In this application, the multiclass classification algorithm used is `LbfgsMaximumEntropy`. To learn more about the algorithm, see the [ML.NET LbfgsMaximumEntropy multiclass trainer API documentation](https://docs.microsoft.com/en-us/dotnet/api/microsoft.ml.trainers.lbfgsmaximumentropymulticlasstrainer?view=ml-dotnet#training-algorithm-details).

```fsharp
let algorithm = 
    mlContext.MulticlassClassification.Trainers.LbfgsMaximumEntropy()
```

### Postprocessing pipeline

The postprocessing pipeline contains the series of transformations to get the output of training into a more readable format. The only transformation performed in this pipeline is mapping back the numerical value mapping of the predicted value into text form. 

```fsharp
let postProcessingPipeline = 
    mlContext.Transforms.Conversion.MapKeyToValue("PredictedLabel")
```

### Create training pipeline

Once the pipelines are defined, combine them into a single pipeline which applies all of the transformations to the data with a single function call.

```fsharp
let trainingPipeline = 
    preProcessingPipeline
        .Append(algorithm)
        .Append(postProcessingPipeline)
```

## Train the model

Use the `Fit` function to train the model by applying the set of transformations defined by `trainingPipeline` to the training dataset.

```fsharp
let model =
    datasets.TrainSet |&gt; trainingPipeline.Fit
```

## Evaluate the model

Once the model is trained, evaluate how well it performs against the test dataset. First, use the trained model to get the predicted category by using the `Transform` function. Then, provide the test dataset containing predictions to the `Evaluate` function which calculates the model's performance metrics by comparing the predicted category to the actual category and print some of them out.

```fsharp
let metrics = 
    (datasets.TestSet |&gt; model.Transform)
    |&gt; mlContext.MulticlassClassification.Evaluate

printfn "Log Loss: %f | MacroAccuracy: %f" metrics.LogLoss metrics.MacroAccuracy
```

## Using the model on real data

Create a list of `ModelInput` items and use the `Transform` method to get the predicted category.

```fsharp
let predictions = 
    [
        { 
            Title="A FIRST LOOK AT SURFACE DUO, MICROSOFT’S FOLDABLE ANDROID PHONE"
            Url="https://www.theverge.com/2019/10/3/20895268/microsoft-surface-duo-foldable-phone-dual-screen-android-hands-on-features-price-photos-video"
            Publisher="The Verge"
            Hostname="www.theverge.com" 
            Category = "" 
        }
        { 
            Title="This Shrinking Economy With Low Inflation Is Stuck on Rates"
            Url="https://www.bloomberg.com/news/articles/2019-12-12/when-a-shrinking-economy-and-low-inflation-don-t-mean-rate-cuts?srnd=economics-vp"
            Publisher="Bloomberg"
            Hostname="www.bloomberg.com" 
            Category = "" 
        }
    ] 
    |&gt; mlContext.Data.LoadFromEnumerable
    |&gt; model.Transform
```

Then, create a `Sequence` of `ModelOutput` values and print out the *PredictedLabel* values.

```fsharp
mlContext.Data.CreateEnumerable&lt;ModelOutput&gt;(predictions,false)
|&gt; Seq.iter(fun prediction -&gt; printfn "Predicted Value: %s" prediction.PredictedLabel)
```

The final *Program.fs* file should look as follows:

```fsharp
open System
open Microsoft.ML
open Microsoft.ML.Data

[&lt;CLIMutable&gt;]
type ModelInput = {
    [&lt;LoadColumn(1)&gt;]
    Title:string
    [&lt;LoadColumn(2)&gt;]
    Url:string
    [&lt;LoadColumn(3)&gt;]
    Publisher:string
    [&lt;LoadColumn(4)&gt;]
    Category:string
    [&lt;LoadColumn(6)&gt;]
    Hostname:string
}

[&lt;CLIMutable&gt;]
type ModelOutput = {
    PredictedLabel: string
}

[&lt;EntryPoint&gt;]
let main argv =

    let mlContext = MLContext()

    let data = mlContext.Data.LoadFromTextFile&lt;ModelInput&gt;("data/newsCorpora.csv")

    let datasets = mlContext.Data.TrainTestSplit(data,testFraction=0.1)

    let preProcessingPipeline = 
        EstimatorChain()
            .Append(mlContext.Transforms.Text.FeaturizeText("FeaturizedTitle","Title"))
            .Append(mlContext.Transforms.Text.FeaturizeText("FeaturizedUrl","Url"))
            .Append(mlContext.Transforms.Text.FeaturizeText("FeaturizedPublisher","Publisher"))
            .Append(mlContext.Transforms.Text.FeaturizeText("FeaturizedHost","Hostname"))
            .Append(mlContext.Transforms.Concatenate("Features",[|"FeaturizedTitle"; "FeaturizedUrl" ;"FeaturizedPublisher"; "FeaturizedHost"|]))
            .Append(mlContext.Transforms.Conversion.MapValueToKey("Label","Category"))

    let algorithm = 
        mlContext.MulticlassClassification.Trainers.LbfgsMaximumEntropy()

    let postProcessingPipeline = 
        mlContext.Transforms.Conversion.MapKeyToValue("PredictedLabel")

    let trainingPipeline = 
        preProcessingPipeline
            .Append(algorithm)
            .Append(postProcessingPipeline)

    let model =
        datasets.TrainSet |&gt; trainingPipeline.Fit

    let metrics = 
        (datasets.TestSet |&gt; model.Transform)
        |&gt; mlContext.MulticlassClassification.Evaluate

    printfn "Log Loss: %f | MacroAccuracy: %f" metrics.LogLoss metrics.MacroAccuracy

    let predictions = 
        [
            { 
                Title="A FIRST LOOK AT SURFACE DUO, MICROSOFT’S FOLDABLE ANDROID PHONE"
                Url="https://www.theverge.com/2019/10/3/20895268/microsoft-surface-duo-foldable-phone-dual-screen-android-hands-on-features-price-photos-video"
                Publisher="The Verge"
                Hostname="www.theverge.com" 
                Category = "" 
            }
            { 
                Title="This Shrinking Economy With Low Inflation Is Stuck on Rates"
                Url="https://www.bloomberg.com/news/articles/2019-12-12/when-a-shrinking-economy-and-low-inflation-don-t-mean-rate-cuts?srnd=economics-vp"
                Publisher="Bloomberg"
                Hostname="www.bloomberg.com" 
                Category = "" 
            }
        ] 
        |&gt; mlContext.Data.LoadFromEnumerable
        |&gt; model.Transform
 
    mlContext.Data.CreateEnumerable&lt;ModelOutput&gt;(predictions,false)
    |&gt; Seq.iter(fun prediction -&gt; printfn "Predicted Value: %s" prediction.PredictedLabel)

    0 // return an integer exit code
```

## Run the application

This particular model achieved a macro-accuracy of 0.92, where closer to 1 is preferred and log loss of 0.20 where closer to 0 is preferred.

```bash
Log Loss: 0.200502 | MacroAccuracy: 0.927742
```

The predicted values are the following:

```bash
Predicted Value: t
Predicted Value: b
```

Upon inspection, they appear to be correct, science and technology for the first link and business for the second link.

## Conclusion

In this writeup, I showed how to build a machine learning multiclass classification model that categorizes web links using ML.NET. Now that you have a model trained, you can save it and deploy it in another application (desktop, web) that bookmarks links. This model can be further improved and personalized by using data from Pocket which has already been tagged. Happy coding!]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/categorize-web-links-ml-net-fsharp-fsadvent2019</link>
      <guid>https://www.luisquintanilla.me/posts/categorize-web-links-ml-net-fsharp-fsadvent2019</guid>
      <pubDate>2019-12-17 19:59:06</pubDate>
    </item>
    <item>
      <title>Client Credentials Authorization in C#</title>
      <description>&lt;![CDATA[

# Getting Started

For this writeup, I'm going to use the Spotify API. Spotify API supports different authorization flows. In this writeup, I will be using the client credentials authorization flow. Generally this works for server-to-server authentication. Because this does not allow users the ability to provide their own credentials, there is no access to endpoints that contain user data.

![](/images/client-credentials-authentication-csharp/clientcredentialsflowdiagram.png)

# Create a new .NET Core Console Application

```bash
dotnet new console -o authtest
```

# Add Dependencies

```bash
dotnet add package Newtonsoft.Json --version 10.0.3
```

# Access Token Model

When a request is made, it needs to be parsed. To better capture the data into a Plain Old CLR Object (POCO), a model can be created.

```csharp
using System;

namespace authtest
{
    class AccessToken
    {
        public string access_token { get; set; }
        public string token_type { get; set; }
        public long expires_in { get; set; }
    }
}
```

# Request Token

In order to extract our token, an HTTP Request needs to be made to the Spotify API in order to get an access token. To do so, we can leverage the `HTTPClient` functionalities.

```csharp
private static async Task&lt;string&gt; GetToken()
{
    string clientId = "YOUR CLIENT ID";
    string clientSecret = "YOUR CLIENT SECRET";
    string credentials = String.Format("{0}:{1}",clientId,clientSecret);

    using(var client = new HttpClient())
    {
        //Define Headers
        client.DefaultRequestHeaders.Accept.Clear();
        client.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue("application/json"));

        client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue("Basic",Convert.ToBase64String(Encoding.UTF8.GetBytes(credentials)));

        //Prepare Request Body
        List&lt;KeyValuePair&lt;string,string&gt;&gt; requestData = new List&lt;KeyValuePair&lt;string,string&gt;&gt;();
        requestData.Add(new KeyValuePair&lt;string,string&gt;("grant_type","client_credentials"));

        FormUrlEncodedContent requestBody = new FormUrlEncodedContent(requestData);

        //Request Token
        var request = await client.PostAsync("https://accounts.spotify.com/api/token",requestBody);
        var response = await request.Content.ReadAsStringAsync();
        return JsonConvert.DeserializeObject&lt;AccessToken&gt;(response);   
    }
}
```


We can then use this function in our main method to request the token.

```csharp
static void Main(string[] args)
{
    Console.WriteLine("Spotify API");
    AccessToken token = GetToken().Result;
    Console.WriteLine(String.Format("Access Token: {0}",token.access_token));
}
```

This should produce the following output

```bash
Spotify API
Getting Token
Access Token: "YOUR ACCESS TOKEN"
```

The token can now be used to make requests to the Spotify API.

Source Code can be found here [Source Code](https://gist.github.com/lqdev/5e82a5c856fcf0818e0b5e002deb0c28)

###### Sources
[Spotify Authorization Guide](https://developer.spotify.com/web-api/authorization-guide/#client_credentials_flow)]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/client-credentials-authentication-csharp</link>
      <guid>https://www.luisquintanilla.me/posts/client-credentials-authentication-csharp</guid>
      <pubDate>2017-12-25 19:28:48</pubDate>
    </item>
    <item>
      <title>Creating A Login Screen With React Native</title>
      <description>&lt;![CDATA[
This assumes that you have Node and NPM installed on your PC. If not, you can go to the official website and install them. Then, come back to this page.

# Part I – Install React-Native

```bash
npm install -g create-react-native-app
```

# Part II – Create and Run Project

```bash 
create-react-native-app myfirstreactnativeapp
cd myfirstreactnativeapp
npm start
```

# Part III – Create Login Screen

In the `App.js` file that is created, we’re going to want to edit a few details.

At the top of the file, add the following line of code to import the visual components we will utilize to create the page.

```javascript
Import { TextInput, StyleSheet, Button } from 'react-native';
```

Inside of your `App` class, you want to add a few things. The first would be to add a constructor for the component. This will allow us to define and keep track of the text input being provided by the user.

```javascript
constructor(props) {
  super(props);
  this.state = {
    name: "",
    email: "",
    password: ""
  }
}
```

Then, we want to update our render method to show the text inputs and submit button.

```javascript
render(){
  return (
    &lt;View&gt;
      &lt;TextInput
        style={styles.input}
        onChangeText={(text) =&gt; this.setState({name: text})}
        placeholder="Name"
      /&gt;
      &lt;TextInput
        style={styles.input}
        onChangeText={(text) =&gt; this.setState({email: text})}
        placeholder="E-Mail"
      /&gt;
      &lt;TextInput
        style={styles.input}
        onChangeText={(text) =&gt; this.setState({password:text})}
        secureTextEntry={true}
        placeholder="Password"
      /&gt;
      &lt;Button
        onPress={this.submit}
        title="Submit"
        color="#841584"
      /&gt;
    &lt;/View&gt;
  )
}
```

Using the `StyleSheet` component, we’ll also want to add some styling to our inputs. We can do so by adding the following lines of code outside of our App class definition. These styles are applied as properties of the `TextInput` components inside the App class.

```javascript
const styles = StyleSheet.create({
  input: {
    width: 250,
    margin: 5
  }
});
```

Finally, we need to create a submit method to handle the press event on the submit button. I have left it blank, since how you pass the e-mail and password is entirely up to you (although hopefully it is in the body of an HTTP Request as opposed to url parameters).

```javascript
submit() {
//Do Something
}
```
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/create-login-screen-react-native</link>
      <guid>https://www.luisquintanilla.me/posts/create-login-screen-react-native</guid>
      <pubDate>2017-12-09 13:15:57</pubDate>
    </item>
    <item>
      <title>Deploy ML.NET Machine Learning Model in Blazor WebAssembly Static Website</title>
      <description>&lt;![CDATA[
## Introduction

There are many ways of deploying machine learning models. The most common way to do so is by exposing models as a web service through APIs or serverless functions. One of the considerations when deploying a model as a web service is latency and performance. The process of making predictions over HTTP using a model involves accepting the user input, loading the serialized version of the model from a file, using the model to make a prediction, and returning the prediction back to the user. Since models are typically just static files, another way of deploying a model is as a static asset over the web, just like any other HTML, CSS, or JavaScript file. This deployment method is similar to that of TensorFlow.js. Deploying in this way has several advantages. One advantage is that there is no longer a web service just to serve the model, making it more cost-efficient. Another advantage is that once the model has been downloaded onto the user's PC, the resources used at that point are those of the user's PC rather than the server the model would otherwise be hosted in. Finally, because the model is a static file, it can be distributed via CDNs. 

One of the challenges with this is that machine learning models are usually built using languages other than JavaScript. This makes using the same code / library the model was built difficult or nearly impossible. WebAssembly is changing that by allowing Rust, C++, C# and other languages to run natively inside the browser. Having that ability, the code / logic to load the model and make predictions is much easier and almost comparable to that of a native platform. Blazor WebAssembly provides users the ability to create modern component-based web applications completely in C#. Additionally, Blazor WebAssembly allows users to publish and deploy their applications as static websites in an easy and cost-efficient way. ML.NET is an open-source, cross-platform framework that allows developers to create machine learning models using .NET. In this post, I'll show how to train a multiclass classification machine learning model that predicts iris flower species. Then, I'll take that model and deploy it alongside a Blazor WebAssembly static website to Azure Storage. The full code for this application may be found at the [MLNETBlazorWASMSample repository on GitHub](https://github.com/lqdev/MLNETBlazorWASMSample).

## Prerequisites

This project was built on a Windows PC but should work cross platform on Mac and Linux.

- [.NET Core SDK 3.1](https://dotnet.microsoft.com/download/dotnet-core/3.1)
- [Blazor WebAssembly Template](https://www.nuget.org/packages/Microsoft.AspNetCore.Blazor.Templates/3.2.0-preview1.20073.1)
- [Azure Subscription](http://aka.ms/amlFree)
- [Azure Storage Explorer](https://azure.microsoft.com/en-us/features/storage-explorer/)

## Set up the solution

The solution built in this post contains three projects:

- SchemaLibrary: C# .NET Standard 2.0 class library that contains the schema definition classes of the data used to train the model as well as the prediction output generated by the model.
- TrainingConsole: C# .NET Core 3.1 console application used to train the machine learning model.
- BlazorWebApp: Blazor WebAssembly web application to make predictions using machine learning model trained by TrainingConsole application.

### Install Blazor WebAssembly Template

Use the .NET CLI to run the following command in the command prompt:

```powershell
dotnet new -i Microsoft.AspNetCore.Blazor.Templates::3.2.0-preview1.20073.1
```

### Create the solution

Create a new directory for the solution called *MLNETBlazorWASMSample*.

```powershell
mkdir MLNETBlazorWASMSample
```

Navigate to the newly created solution directory and create a solution:

```powershell
cd MLNETBlazorWASMSample
dotnet new sln
```

### Create schema class library

The data schema for the model input and output are shared during training as well as when making predictions. To share resources, create a class library that's shared by the `ConsoleTraining` and `BlazorWebApp` projects. In the solution directory, enter the following command:

```powershell
dotnet new classlib -o SchemaLibrary
```

Install the *Microsoft.ML* NuGet package (This solution is built with version 1.4.0). The *Microsoft.ML* package is used throughout the entire solution.

```powershell
dotnet add SchemaLibrary package Microsoft.ML
```

Add the library project to the solution.

```powershell
dotnet sln add SchemaLibrary
```

### Create the training console application

The console application contains the series of data transformations and algorithms used to train the model. In the solution directory, create a new console application.

```powershell
dotnet new console -o TrainingConsole
```

Add the console application to the solution.

```powershell
dotnet sln add TrainingConsole
```

Reference the *SchemaLibrary* project.

```powershell
dotnet add TrainingConsole reference SchemaLibrary
```

### Create the Blazor WebAssembly web application

The web application contains a few input elements so users can provide new data that the model then uses to make predictions. In the solution directory, create a new Blazor WebAssembly application.

```powershell
dotnet new blazorwasm -o BlazorWebApp
```

Add the web application project to the solution.

```powershell
dotnet sln add BlazorWebApp
```

Reference the *SchemaLibrary* project.

```powershell
dotnet add BlazorWebApp reference SchemaLibrary
```

## Define the schema

### Understand the data

The data used to train the model comes from the [iris dataset](https://archive.ics.uci.edu/ml/datasets/Iris). It contains four numerical columns which are sepal and petal measurements and one numerical column for the species of iris flower. This is a sample of the data.

|Sepal length (cm)  |Sepal width (cm)  |Petal length (cm)  |Petal width (cm)  |Class (iris species)  |
|---------|---------|---------|---------|---------|
|5.1|3.5|1.4|0.2|Iris-setosa|
|7.0|3.2|4.7|1.4|Iris-versicolor|
|6.3|3.3|6.0|2.5|Iris-virginica|

### Define model input schema

In the *SchemaLibrary* project, create a class called `ModelInput` to model the data used for training and as model input.

```powershell
ni ModelInput.cs
```

The `ModelInput` class should look like the following:

```csharp
using Microsoft.ML.Data;

namespace SchemaLibrary
{
    public class ModelInput
    {
        [LoadColumn(0)]
        public float SepalLength { get; set; }

        [LoadColumn(1)]
        public float SepalWidth { get; set; }

        [LoadColumn(2)]
        public float PetalLength { get; set; }

        [LoadColumn(3)]
        public float PetalWidth { get; set; }

        [LoadColumn(4)]
        public string Label { get; set; }
    }
}
```

Notice that the `Class` column is now a property called `Label`. This is for two reasons:

1. Avoid using the `class` keyword.
2. In ML.NET, the default column name of the column to predict expected by algorithms is `Label`.

Also notice the `LoadColumn` attributes at the top of each property. This is used to tell the loader the index of the column where the data for the respective property is.

### Define model output schema

Similar to the input schema, there's a schema for the output of the model. The type of model used in this solution is a multiclass classification model since there are more than two categories to choose from for iris flower species. Multiclass classification models output a column called `PredictedLabel` which contains the name of the predicted category. In the *SchemaLibrary* project, create a class called `ModelOutput` to model the predictions made by the model.

```powershell
ni ModelOutput.cs
```

The `ModelOutput` class should look like the following:

```csharp
namespace SchemaLibrary
{
    public class ModelOutput
    {
        public string PredictedLabel { get; set; }
    }
}
```

## Train the model

Now it's time to create the application that trains the model.

### Get the data

Download the data and save it inside the *TrainingConsole* project directory.

```powershell
curl https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data -o iris.data
```

### Define the data preparation and training steps

In the *TrainingConsole* project, open the *Program.cs* file and add the following using statements at the top:

```csharp
using System;
using System.Linq;
using Microsoft.ML;
using SchemaLibrary;
```

Then, delete the contents inside the `Main` method and replace it with the following.

```csharp
// 1. Initialize MLContext
MLContext mlContext = new MLContext();

// 2. Load the data
IDataView data = mlContext.Data.LoadFromTextFile&lt;ModelInput&gt;("iris.data", separatorChar:',');

// 3. Shuffle the data
IDataView shuffledData = mlContext.Data.ShuffleRows(data);

// 3. Define the data preparation and training pipeline.
IEstimator&lt;ITransformer&gt; pipeline = 
    mlContext.Transforms.Concatenate("Features","SepalLength","SepalWidth","PetalLength","PetalWidth")
        .Append(mlContext.Transforms.NormalizeMinMax("Features"))
        .Append(mlContext.Transforms.Conversion.MapValueToKey("Label"))
        .Append(mlContext.MulticlassClassification.Trainers.NaiveBayes())
        .Append(mlContext.Transforms.Conversion.MapKeyToValue("PredictedLabel"));

// 4. Train with cross-validation
var cvResults = mlContext.MulticlassClassification.CrossValidate(shuffledData, pipeline);

// 5. Get the highest performing model and its accuracy
(ITransformer, double) model = 
    cvResults
        .OrderByDescending(fold =&gt; fold.Metrics.MacroAccuracy)
        .Select(fold =&gt; (fold.Model, fold.Metrics.MacroAccuracy))
        .First();

Console.WriteLine($"Top performing model's macro-accuracy: {model.Item2}");

// 6. Save the model
mlContext.Model.Save(model.Item1, data.Schema, "model.zip");

Console.WriteLine("Model trained");
```

The training application loads the data from the `iris.data` file and applies a series of transformations. First, all of the individual numerical columns are combined into a single vector and stored in a new column called `Features`. The `Features` column is then normalized and the `MapValueToKey` transform is used to convert the text in the `Label` column to a number. Then, the transformed data is used to train a model using the `NaiveBayes` algorithm. **Note that at the time of this writing, for multiclass classification problems, only Naive Bayes has been confirmed to work with Blazor WebAssembly**. Finally, the `PredictedLabel` is stored as a number so it has to be converted back into text.

Using the `Fit` method, the data is applied to the pipeline. Because the dataset is small, a technique called cross-validation is used to build a more robust model. Once the model is trained, the model with the top performance is then serialized and saved to a file called *model.zip* for later use in the web application.

The final *Program.cs* file should look like the content below:

```csharp
using System;
using System.Linq;
using Microsoft.ML;
using SchemaLibrary;

namespace TrainingConsole
{
    class Program
    {
        static void Main(string[] args)
        {
            // 1. Initialize MLContext
            MLContext mlContext = new MLContext();

            // 2. Load the data
            IDataView data = mlContext.Data.LoadFromTextFile&lt;ModelInput&gt;("iris.data", separatorChar:',');

            // 3. Shuffle the data
            IDataView shuffledData = mlContext.Data.ShuffleRows(data);

            // 3. Define the data preparation and training pipeline.
            IEstimator&lt;ITransformer&gt; pipeline = 
                mlContext.Transforms.Concatenate("Features","SepalLength","SepalWidth","PetalLength","PetalWidth")
                    .Append(mlContext.Transforms.NormalizeMinMax("Features"))
                    .Append(mlContext.Transforms.Conversion.MapValueToKey("Label"))
                    .Append(mlContext.MulticlassClassification.Trainers.NaiveBayes())
                    .Append(mlContext.Transforms.Conversion.MapKeyToValue("PredictedLabel"));

            // 4. Train with cross-validation
            var cvResults = mlContext.MulticlassClassification.CrossValidate(shuffledData, pipeline);

            // 5. Get the highest performing model and its accuracy
            (ITransformer, double) model = 
                cvResults
                    .OrderByDescending(fold =&gt; fold.Metrics.MacroAccuracy)
                    .Select(fold =&gt; (fold.Model, fold.Metrics.MacroAccuracy))
                    .First();

            Console.WriteLine($"Top performing model's macro-accuracy: {model.Item2}");

            // 6. Save the model
            mlContext.Model.Save(model.Item1, data.Schema, "model.zip");

            Console.WriteLine("Model trained");
        }
    }
}
```

### Run the application

In the *TrainConsole* project directory, use the following command to run the application and train the model:

```powershell
dotnet run
```

## Host the model

Once you have the model saved, use the Azure Portal to create an Azure Storage account.

![Create Azure Storage Account](/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly1.png)

Then, navigate to your newly created storage account resource and create a blob container called `models`.

![Create Blob Container](/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly2.png)

Once your container is created, navigate to it and upload the *model.zip* file.

![Upload Model](/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly3.png)

## Create prediction web page

To make predictions, create a web page to take in user input. Then, provide the user input to the model and display the prediction to the user.

### Set up imports

In the *BlazorWebApp* project directory, open the *_Imports.razor* file. This contains the using statements for the pages and components in your application. Add the following using statements:

```razor
@using System.IO
@using Microsoft.ML
@using SchemaLibrary
```

### Create user input page

In the *BlazorWebApp* project, create a new razor page called *Prediction.razor* inside the *Pages* directory.

```powershell
ni Prediction.razor
```

Add the following content to it:

```razor
@page "/prediction"
@inject HttpClient _client

&lt;label&gt;Sepal Length: &lt;/label&gt;
&lt;input type="text" @bind="_sepalLength"&gt;&lt;br&gt;
&lt;label&gt;Sepal Width: &lt;/label&gt;
&lt;input type="text" @bind="_sepalWidth"&gt;&lt;br&gt;
&lt;label&gt;Petal Length: &lt;/label&gt;
&lt;input type="text" @bind="_petalLength"&gt;&lt;br&gt;
&lt;label&gt;Petal Width: &lt;/label&gt;
&lt;input type="text" @bind="_petalWidth"&gt;&lt;br&gt;
&lt;button @onclick="GetPrediction"&gt;Make prediction&lt;/button&gt;
@if(@ModelPrediction == null)
{
    &lt;p&gt;Enter data to get a prediction&lt;/p&gt;
} else
{
    &lt;p&gt;@ModelPrediction&lt;/p&gt;
}


@code {
    private PredictionEngine&lt;ModelInput,ModelOutput&gt; _predictionEngine;
    private string _sepalLength, _sepalWidth, _petalLength, _petalWidth, ModelPrediction;

    protected override async Task OnInitializedAsync()
    {
        Stream savedModel = await _client.GetStreamAsync("&lt;YOUR-MODEL-ENDPOINT&gt;");
        MLContext mlContext = new MLContext();
        ITransformer _model = mlContext.Model.Load(savedModel,out DataViewSchema schema);
        _predictionEngine = mlContext.Model.CreatePredictionEngine&lt;ModelInput,ModelOutput&gt;(_model);
    }

    private void GetPrediction()
    {
        ModelInput input = new ModelInput
        {
            SepalLength=float.Parse(_sepalLength),
            SepalWidth=float.Parse(_sepalWidth),
            PetalLength=float.Parse(_petalLength),
            PetalWidth=float.Parse(_petalWidth)
        };

        ModelOutput prediction = _predictionEngine.Predict(input);

        ModelPrediction = prediction.PredictedLabel;
    }
}
```

The *Predict.razor* page contains a text input element for each of the columns the model was original trained on. When the page is initialized, the model is loaded from Azure Storage and a `PredictionEngine` is created. **Make sure to replace `&lt;YOUR-MODEL-ENDPOINT&gt;` with the URL of the blob that contains your `model.zip`**. A `PredictionEngine` is a convenience API to make a single prediction. Traditionally when models are served as a web service, it is recommended to use the `PredictionEnginePool` service because it's thread-safe and more performant in multi-threaded application. However, in this case since the model is downloaded onto the individual user's browser, it's okay to use `PredictionEngine`. After a user enters input values and clicks the "Make Prediction" button, the `GetPrediction` method executes by taking the user input and making a prediction using the `PredictionEngine`. The prediction is then displayed in the browser.

### Add to navigation menu

In the *BlazorWebApp* project, open the *NavMenu.razor* file in the *Shared* directory.

Add the following list item to the `&lt;ul&gt;` element.

```html
&lt;li class="nav-item px-3"&gt;
    &lt;NavLink class="nav-link" href="prediction"&gt;
        &lt;span class="oi oi-list-rich" aria-hidden="true"&gt;&lt;/span&gt; Prediction
    &lt;/NavLink&gt;
&lt;/li&gt;
```

The final *NavMenu.razor* page should look like the following:

```razor
&lt;div class="top-row pl-4 navbar navbar-dark"&gt;
    &lt;a class="navbar-brand" href=""&gt;BlazorWebApp&lt;/a&gt;
    &lt;button class="navbar-toggler" @onclick="ToggleNavMenu"&gt;
        &lt;span class="navbar-toggler-icon"&gt;&lt;/span&gt;
    &lt;/button&gt;
&lt;/div&gt;

&lt;div class="@NavMenuCssClass" @onclick="ToggleNavMenu"&gt;
    &lt;ul class="nav flex-column"&gt;
        &lt;li class="nav-item px-3"&gt;
            &lt;NavLink class="nav-link" href="" Match="NavLinkMatch.All"&gt;
                &lt;span class="oi oi-home" aria-hidden="true"&gt;&lt;/span&gt; Home
            &lt;/NavLink&gt;
        &lt;/li&gt;
        &lt;li class="nav-item px-3"&gt;
            &lt;NavLink class="nav-link" href="counter"&gt;
                &lt;span class="oi oi-plus" aria-hidden="true"&gt;&lt;/span&gt; Counter
            &lt;/NavLink&gt;
        &lt;/li&gt;
        &lt;li class="nav-item px-3"&gt;
            &lt;NavLink class="nav-link" href="fetchdata"&gt;
                &lt;span class="oi oi-list-rich" aria-hidden="true"&gt;&lt;/span&gt; Fetch data
            &lt;/NavLink&gt;
        &lt;/li&gt;
        &lt;li class="nav-item px-3"&gt;
            &lt;NavLink class="nav-link" href="prediction"&gt;
                &lt;span class="oi oi-list-rich" aria-hidden="true"&gt;&lt;/span&gt; Prediction
            &lt;/NavLink&gt;
        &lt;/li&gt;
    &lt;/ul&gt;
&lt;/div&gt;

@code {
    private bool collapseNavMenu = true;

    private string NavMenuCssClass =&gt; collapseNavMenu ? "collapse" : null;

    private void ToggleNavMenu()
    {
        collapseNavMenu = !collapseNavMenu;
    }
}
```

## Configure the web application

The web application will he hosted as a static site on Azure Storage.

In the Azure Portal, navigate to the storage account resource where you are hosting your model.

### Enable static website

Enable a static website for the storage account and set the index document name and error document path to *index.html*.

![Enable Static Website](/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly4.png)

At this point, a new container called *$web* is created in your storage account. This is where all your site's static files will reside. Additionally, a primary endpoint is created. This is the URL you will use to access your application

### Configure CORS

The storage account has some default CORS settings. In order to download and use your model from your application, you'll have to configure them.

![Configure CORS](/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly5.png)

For "Allowed origins", enter your primary endpoint.

## Publish and deploy the web application

To publish your application, run the following command:

```powershell
dotnet publish -c Release
```

This generates all the files you'll need to host your web application as a static site in the *bin/Release/netstandard2.1/publish/BlazorWebApp/dist* directory of your *BlazorWebApp* project.

To deploy your application, use Azure Storage Explorer to copy all of the files in the *dist* directory into the *$web* container of your Azure Storage Account.

![Copy files into web container](/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly6.png)

## Test the application

In your browser, navigate to your static website's primary endpoint and select the Prediction page. Input data and click "Make prediction". The page should look as follows. 

![Test the application](/images/deploy-machine-learning-mlnet-models-blazor-webassembly/deploy-machine-learning-mlnet-model-blazor-webassembly7.png)

**You may note that performance of Naive Bayes on this dataset is not the best so some of the predictions may not be as accurate. I am okay with this at the moment because this is a proof-of-concept to show how these technologies might work together. Perhaps using a better dataset may yield better results.**

## Conclusion

In this post, I went over how to deploy an ML.NET multiclass classification model alongside a Blazor WebAssembly static website to Azure Storage. While more limited than other methods of deployment due to the early stages of WebAssembly and Blazor WebAssembly, this shows the possibilities for the technologies. Deploying in this manner reduces the amount of resources required to deploy these models and shifts processing from a server or web service to the client's browser making deployment and distribution of machine learning models more efficient, scalable and cost-efficient.
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/deploy-machine-learning-mlnet-models-blazor-webassembly</link>
      <guid>https://www.luisquintanilla.me/posts/deploy-machine-learning-mlnet-models-blazor-webassembly</guid>
      <pubDate>2020-03-01 19:21:04</pubDate>
    </item>
    <item>
      <title>Deploy .NET Machine Learning Models with ML.NET, ASP.NET Core, Docker and Azure Container Instances</title>
      <description>&lt;![CDATA[

# Introduction

Leading up to and during MS Build 2018 Microsoft has released a wide range of products that reduce the complexity that comes with building and deploying software. The focus this year was on Machine Learning and Artificial Intelligence. Some of the products I found particularly interesting are [Azure Container Instances](https://azure.microsoft.com/en-us/services/container-instances/) which makes it easier to run containerized applications without provisioning or managing servers and [ML.NET](https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet) which is a .NET cross-platform machine learning framework. In this writeup, I will make use of both these products by creating a machine learning classification model with `ML.NET`, exposing it via an ASP.NET Core Web API, packaging it into a Docker container and deploying it to the cloud via Azure Container Instances. Source code for this project can be found [here](https://github.com/lqdev/mlnetacidemo).

## Prerequisites

This writeup assumes that you have some familiarity with Docker. The following software/dependencies are also required to build and deploy the sample application. It's important to note the application was built on a Ubuntu 16.04 PC, but all the software is cross-platform and should work on any environment.

- [Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/)
- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest)
- [.NET Core 2.0](https://www.microsoft.com/net/download/linux)
- [Docker Hub Account](https://hub.docker.com/)

## Setting Up The Project

The first thing we want to do is create a folder for our solution.

```bash
mkdir mlnetacidemo
```

Then, we want to create a solution inside our newly created folder.

```bash
cd mlnetacidemo
dotnet new sln
```

## Building The Model

Inside our solution folder, we want to create a new console application which is where we'll build and test our machine learning model.

### Setting Up the Model Project

First, we want to create the project. From the solution folder enter:

```bash
dotnet new console -o model
```

Now we want to add this new project to our solution.

```bash
dotnet sln mlnetacidemo.sln add model/model.csproj
```

### Adding Dependencies

Since we'll be using the `ML.NET` framework, we need to add it to our `model` project.

```bash
cd model
dotnet add package Microsoft.ML
dotnet restore
```

### Download The Data

Before we start training the model, we need to download the data we'll be using to train. We do so by creating a directory called `data` and downloading the data file onto there.

```bash
mkdir data
curl -o data/iris.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
```

If we take a look at the data file, it should look something like this:

```text
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa
5.0,3.4,1.5,0.2,Iris-setosa
4.4,2.9,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
```

### Train Model

Now that we have all our dependencies set up, it's time to build our model. I leveraged the demo that is used on the [ML.NET Getting-Started website](https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet/get-started/linux/ubuntu16-04).

#### Defining Data Structures

In the root directory of our `model` project, let's create two classes called `IrisData` and `IrisPrediction` which will define our features and predicted attribute respectively. Both of them will use `Microsoft.ML.Runtime.Api` to add the property attributes.

Here is what our `IrisData` class looks like: 
```csharp
using Microsoft.ML.Runtime.Api;

namespace model
{
public class IrisData
    {
        [Column("0")]
        public float SepalLength;

        [Column("1")]
        public float SepalWidth;

        [Column("2")]
        public float PetalLength;
        
        [Column("3")]
        public float PetalWidth;

        [Column("4")]
        [ColumnName("Label")]
        public string Label;
    }       
}
```

Similarly, here is the `IrisPrediction` class:

```csharp
using Microsoft.ML.Runtime.Api;

namespace model
{
    public class IrisPrediction
    {
        [ColumnName("PredictedLabel")]
        public string PredictedLabels;
    }
}
```

#### Building Training Pipeline

The way the `ML.NET` computations process is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. Therefore, we can create a class called `Model` to perform all of these tasks for us.

```csharp
using Microsoft.ML.Data;
using Microsoft.ML;
using Microsoft.ML.Runtime.Api;
using Microsoft.ML.Trainers;
using Microsoft.ML.Transforms;
using Microsoft.ML.Models;
using System;
using System.Threading.Tasks;

namespace model
{
    class Model
    {
        
        public static async Task&lt;PredictionModel&lt;IrisData,IrisPrediction&gt;&gt; Train(LearningPipeline pipeline, string dataPath, string modelPath)
        {
            // Load Data
            pipeline.Add(new TextLoader(dataPath).CreateFrom&lt;IrisData&gt;(separator:',')); 

            // Transform Data
            // Assign numeric values to text in the "Label" column, because 
            // only numbers can be processed during model training   
            pipeline.Add(new Dictionarizer("Label"));

            // Vectorize Features
            pipeline.Add(new ColumnConcatenator("Features", "SepalLength", "SepalWidth", "PetalLength", "PetalWidth"));

            // Add Learner
            pipeline.Add(new StochasticDualCoordinateAscentClassifier());

            // Convert Label back to text 
            pipeline.Add(new PredictedLabelColumnOriginalValueConverter() {PredictedLabelColumn = "PredictedLabel"});

            // Train Model
            var model = pipeline.Train&lt;IrisData,IrisPrediction&gt;();

            // Persist Model
            await model.WriteAsync(modelPath);

            return model;
        }
    }
}
```

In addition to building our pipeline and training our machine learning model, the `Model` class also serialized and persisted the model for future use in a file called `model.zip`.

#### Testing Our Model

Now that we have our data structures and model training pipeline set up, it's time to test everything to make sure it's working. We'll put our logic inside of our `Program.cs` file.

```csharp
using System;
using Microsoft.ML;

namespace model
{
    class Program
    {
        static void Main(string[] args)
        {

            string dataPath = "model/data/iris.txt";

            string modelPath = "model/model.zip";

            var model = Model.Train(new LearningPipeline(),dataPath,modelPath).Result;

            // Test data for prediction
            var prediction = model.Predict(new IrisData() 
            {
                SepalLength = 3.3f,
                SepalWidth = 1.6f,
                PetalLength = 0.2f,
                PetalWidth = 5.1f
            });

            Console.WriteLine($"Predicted flower type is: {prediction.PredictedLabels}");
        }
    }
}
```

All set to run. We can do so by entering the following command from our solution directory:

```bash
dotnet run -p model/model.csproj
```

Once the application has been run, the following output should display on the console. 

```text
Automatically adding a MinMax normalization transform, use 'norm=Warn' or
'norm=No' to turn this behavior off.Using 2 threads to train.
Automatically choosing a check frequency of 2.Auto-tuning parameters: maxIterations = 9998.
Auto-tuning parameters: L2 = 2.667734E-05.
Auto-tuning parameters: L1Threshold (L1/L2) = 0.Using best model from iteration 882.
Not training a calibrator because it is not needed.
Predicted flower type is: Iris-virginica
```

Additionally, you'll notice that a file called `model.zip` was created in the root directory of our `model` project. This persisted model can now be used outside of our application to make predictions, which is what we'll do next via an API.

## Exposing The Model

Once a machine learning model is built, you want to deploy it so it can start making predictions. One way to do that is via a REST API. At it's core, all our API needs to do is accept data input from the client and respond back with a prediction. To help us do that, we'll be using an ASP.NET Core API.

### Setting Up The API Project

The first thing we want to do is create the project.

```bash
dotnet new webapi -o api
```

Then we want to add this new project to our solution

```bash
dotnet sln mlnetacidemo.sln add api/api.csproj
```

### Adding Dependencies

Because we'll be loading our model and making predictions via our API, we need to add the `ML.NET` package to our `api` project.

```bash
cd api
dotnet add package Microsoft.ML
dotnet restore
```

### Referencing Our Model

In the previous step when we built our machine learning model, it was saved to a file called `model.zip`. This is the file we'll be referencing in our API to help us make predictions. To reference it in our API, simply copy it from the model project directory into our `api` project directory.

### Creating Data Models

Our model was built using data structures `IrisData` and `IrisPrediction` to define the features as well as the predicted attribute. Therefore, when our model makes predictions via our API, it needs to reference these data types as well. As a result, we need to define `IrisData` and `IrisPrediction` classes inside of our `api` project. The contents of the classes will be nearly identical to those in the `model` project with the only exception of our namespace changing from `model` to `api`.

```csharp
using Microsoft.ML.Runtime.Api;

namespace api
{
    public class IrisData
    {
        [Column("0")]
        public float SepalLength;

        [Column("1")]
        public float SepalWidth;

        [Column("2")]
        public float PetalLength;
        
        [Column("3")]
        public float PetalWidth;

        [Column("4")]
        [ColumnName("Label")]
        public string Label;
    }    
}
```

```csharp
using Microsoft.ML.Runtime.Api;

namespace api
{
    public class IrisPrediction
    {
        [ColumnName("PredictedLabel")]
        public string PredictedLabels;
    }
}
```

### Building Endpoints

Now that our project is set up, it's time to add a controller that will handle prediction requests from the client. In the `Controllers` directory of our `api` project we can create a new class called `PredictController` with a single `POST` endpoint. The contents of the file should look like the code below:

```csharp
using System;
using System.Collections.Generic;
using System.Linq;
using System.Threading.Tasks;
using Microsoft.AspNetCore.Mvc;
using Microsoft.ML;

namespace api.Controllers
{
    [Route("api/[controller]")]
    public class PredictController : Controller
    {
        // POST api/predict
        [HttpPost]
        public string Post([FromBody] IrisData instance)
        {
            var model = PredictionModel.ReadAsync&lt;IrisData,IrisPrediction&gt;("model.zip").Result;
            var prediction = model.Predict(instance);
            return prediction.PredictedLabels;
        }
    }
}
```

### Testing The API

Once our `predict` endpoint is set up, it's time to test it. From the root directory of our `mlnetacidemo` solution, enter the following command.

```bash
dotnet run -p api/api.csproj
```

In a client like POSTMAN or Insomnia, send an HHTP POST request to the endpoint `http://localhost:5000/api/predict`.

The body our request should look similar to the snippet below:

```json
{
	"SepalLength": 3.3,
	"SepalWidth": 1.6,
	"PetalLength": 0.2,
	"PetalWidth": 5.1,
}
```

If successful, the output returned should equal `Iris-virginica` just like our console application.

## Packaging The Application

Great! Now that our application is successfully running locally, it's time to package it up into a Docker container and push it to Docker Hub.

### Creating The Dockerfile

In our `mlnetacidemo` solution directory, create a `Dockerfile` with the following content:

```Dockerfile
FROM microsoft/dotnet:2.0-sdk AS build
WORKDIR /app

# copy csproj and restore as distinct layers
COPY *.sln .
COPY api/*.csproj ./api/
RUN dotnet restore

# copy everything else and build app
COPY api/. ./api/
WORKDIR /app/api
RUN dotnet publish -c release -o out


FROM microsoft/aspnetcore:2.0 AS runtime
WORKDIR /app
COPY api/model.zip .
COPY --from=build /app/api/out ./
ENTRYPOINT ["dotnet", "api.dll"]
```

### Building Our Image

To build the image, we need to enter the following command into the command prompt. This make take a while because it needs to download the .NET Core SDK and ASP.NET Core runtime Docker images.

```bash
docker build -t &lt;DOCKERUSERNAME&gt;/&lt;IMAGENAME&gt;:latest .
```

### Test Image Locally

We need to test our image locally to make sure it can run on the cloud. To do so, we can use the `docker run` command. 

```bash
docker run -d -p 5000:80 &lt;DOCKERUSERNAME&gt;/&lt;IMAGENAME&gt;:latest
```

Although the API is exposing port 80, we bind it to the local port 5000 just to keep our prior API request intact. When sending a POST request to `http://localhost:5000/api/predict` with the appropriate body, the response should again equal `Iris-virginica`.

To stop the container, use `Ctrl + C`.

### Push to Docker Hub

Now that the Docker image is successfully running locally, it's time to push to Docker Hub. Again, we use the Docker CLI to do this.

```bash
docker login
docker push &lt;DOCKERUSERNAME&gt;/&lt;IMAGENAME&gt;:latest
```

## Deploying To The Cloud

Now comes the final step which is to deploy and expose our machine learning model and API to the world. Our deployment will occur via Azure Container Instances because it requires almost no provisioning or management of servers. 

### Prepare Deployment Manifest

Although deployments can be performed inline in the command line, it's usually best to place all the configurations in a file for documentation and to save time not having to type in the parameters every time. With Azure, we can do that via a JSON file. 

```json
{
  "$schema":
    "https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
  "contentVersion": "1.0.0.0",
  "parameters": {
    "containerGroupName": {
      "type": "string",
      "defaultValue": "mlnetacicontainergroup",
      "metadata": {
        "description": "Container Group name."
      }
    }
  },
  "variables": {
    "containername": "mlnetacidemo",
    "containerimage": "&lt;DOCKERUSERNAME&gt;/&lt;IMAGENAME&gt;:latest"
  },
  "resources": [
    {
      "name": "[parameters('containerGroupName')]",
      "type": "Microsoft.ContainerInstance/containerGroups",
      "apiVersion": "2018-04-01",
      "location": "[resourceGroup().location]",
      "properties": {
        "containers": [
          {
            "name": "[variables('containername')]",
            "properties": {
              "image": "[variables('containerimage')]",
              "resources": {
                "requests": {
                  "cpu": 1,
                  "memoryInGb": 1.5
                }
              },
              "ports": [
                {
                  "port": 80
                }
              ]
            }
          }
        ],
        "osType": "Linux",
        "ipAddress": {
          "type": "Public",
          "ports": [
            {
              "protocol": "tcp",
              "port": "80"
            }
          ]
        }
      }
    }
  ],
  "outputs": {
    "containerIPv4Address": {
      "type": "string",
      "value":
        "[reference(resourceId('Microsoft.ContainerInstance/containerGroups/', parameters('containerGroupName'))).ipAddress.ip]"
    }
  }
}
```

It's a lot to look at but for now we can use this template and save it to the file `azuredeploy.json` in the root directory of our `mlnetacidemo` solution. The only thing that needs to be changed is the value of the `containerimage` property. Replace it with your Docker Hub username and the name of the image you just pushed to Docker Hub.

### Deploy

In order to deploy our application we need to make sure to log into our Azure account. To do so via the Azure CLI, type into the command prompt:

```bash
az login
```

Follow the prompts to log in. Once logged in, it's time to create a resource group for our container.

```bash
az group create --name mlnetacidemogroup --location eastus
```

After the group has been successfully created it's time to deploy our application.

```bash
az group deployment create --resource-group mlnetacidemogroup --template-file azuredeploy.json
```

Give it a few minutes for your deployment to initialize. If the deployment was successful, you should see some output on the command line. Look for the `ContainerIPv4Address` property. This is the IP Address where your container is accessible. In POSTMAN or Insomnia, replace the URL to which you previously made a POST request to with `http://&lt;ContainerIPv4Address&gt;/api/predict` where `ContainerIPv4Address` is the value that was returned to the command line after the deployment. If successful, the response should be just like previous requests `Iris-virginica`.

Once you're finished, you can clean up resources with the following command:

```bash
az group delete --name mlnetacidemogroup
```

## Conclusion

In this writeup, we built a classification machine learning model using `ML.NET` that predicts the class of an iris plant given four measurement features, exposed it via an ASP.NET Core REST API, packaged it into a container and deployed it to the cloud using Azure Container Instances. As the model changes and becomes more complex, the process is standardized enough that extending this example would require minimal changes to our existing application. Happy Coding!
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/deploy-netml-docker-aci</link>
      <guid>https://www.luisquintanilla.me/posts/deploy-netml-docker-aci</guid>
      <pubDate>2018-05-11 21:17:00</pubDate>
    </item>
    <item>
      <title>Deploy your own Owncast server</title>
      <description>&lt;![CDATA[
I'm a fan of learning in public. One way I do that is writing these types of posts. Another is live-streaming. It's been about four years since the last time I streamed. You can check out some of those recordings on my [YouTube channel](/youtube) if you're interested.

Lately, I've been wanting to get back into live-streaming, but like many parts of my digital presence, it's an area I want ownership over. That means, I don't want to use Twitch, YouTube, Kick, or any other streaming platform. I want to self-host my own livestream server. 

For years, I've been following [Owncast](https://owncast.online/). I'm a big fan. Aside from being open-source and allowing me to self-host, it has [Fediverse](https://owncast.online/docs/social/) and [IndieAuth](https://owncast.online/docs/chat/chat-authentication/#indieauth) integrations.

Because I don't plan to monetize my streams, I want to make sure I keep my costs down. Owncast offers a container deployment option. That means, I can deploy my application to [Azure Container Apps](https://learn.microsoft.com/azure/container-apps/overview) using a [consumption plan](https://learn.microsoft.com/azure/container-apps/plans#consumption). By doing so, I only pay for compute and networking when I'm streaming. The rest of the time, I can scale down the application to zero and I'm not charged for it. 

In this post, I'll go over how to set up a self-hosted Owncast instance on Azure Container Apps and once deployed, how to configure it with [OBS](https://obsproject.com/). 

## Deploy owncast server

The deployment is fairly straightforward. You can use the ["Provide a virtual network to an Azure Container Apps environment"](https://learn.microsoft.com/azure/container-apps/vnet-custom?tabs=bash&amp;pivots=azure-portal) tutorial. I'd recommend going through the portal if you don't want to install the Azure CLI or are not comfortable with the command line. 

The few configurations that tutorial doesn't guide you through is setting up your container and ingress since those are application specific. 

After you configure your environment and virtual network in the *Basics* tab, continue your setup using the steps below. 

### Container

To set up Owncast, you can use the [public docker image](https://hub.docker.com/r/owncast/owncast/tags).

When you get to the *Container* tab in the setup process, fill in the fields with the following values:

| Field | Value |
| --- | --- |
| Name | Enter a name for your container |
| Image source | Docker Hub or other registies |
| Image type | Public |
| Registry login server | docker.io |
| Image and tag | owncast/owncast:latest |
| Command override | You can leave this blank |
| Arguments override | You can leave this blank |
| Deployment stack | Generic |
| Workload profile | Consumption |
| CPU and Memory | 2 CPU Cores, 4Gi Memory |

### Ingress 

By setting up ingress on your deployment, you'll expose the ports Owncast listens on. 

The reason you need a virtual network is because Owncast listens on two ports:

- **8080** - The web application and admin portal
- **1935** - The RMTP server used for streaming

By default, Azure Container Apps only allows you to expose one port. For more details, see the [Azure Container Apps documentation](https://learn.microsoft.com/azure/container-apps/ingress-overview#additional-tcp-ports). 

In the *Ingress* tab, check the box that says **Enabled**.

Then, fill in the fields with the following values:

| Field | Value |
| --- | --- |
| Ingress traffic | Accepting traffic from anywhere |
| Ingress type | HTTP |
| Transport | Auto |
| Insecure connections | Leave unchecked |
| Target port | 8080 |
| Session affinity | Leave unchecked | 

Expand the Additional TCP ports section and configure the RMTP port

| Field | Value |
| --- | --- |
| Target port | 1935 |
| Exposed port | 1935 |
| Ingress traffic | Accepting traffic from anywhere |

Once you've configured ingress, you can select on **Review + create**.

Review your configurations in the *Review + create* tab. If everything looks good, select **Create**. 

## **IMPORTANT** Change admin password

Once your application deploys, make sure to change your admin password. 

1. In the portal, go to your container app resource and copy your application URL.

    ![Screenshot of Azure Container Apps resource in Azure Portal highlighting Application URL](/images/deploy-owncast-azure/portal-container-resource.png)

    Make sure to keek your application URL around since you'll also be using it when configuring OBS. 

1. Access the admin portal at the following URL - `&lt;YOUR-APPLICATION-URL&gt;/admin`. Replace `&lt;YOUR-APPLICATION-URL&gt;` with your application URL.
1. Log in with the default credentials.

| Field | Value |
| Username | admin |
| Password | abc123 |

1. In the Owncast admin portal, select **Configuration &gt; Server Setup**
1. Change your admin password.

For more details, see the [Owncast documentation](https://owncast.online/docs/configuration/). 

## Create stream key

Once you've changed your admin password, create a stream key. This will enable you to log into your server through OBS and begin streaming. 

1. In the Owncast admin portal, select **Configuration &gt; Server Setup**
1. Select the **Stream Keys** tab.
1. Copy the default stream key or create a new one.

## Configure OBS

Now that everything is configured, it's time to set up OBS. 

To do so, [follow the guidance in the Owncast documentattion](https://owncast.online/docs/broadcasting/obs/).

Replace the server address with your application URL and use the stream key you copied or created. Note that the protocol is not `http` or `https`. **Use `rmtp`**.

That's it! If everything is configured correctly, you should be able to select **Start Streaming** in OBS. 

Go to your Owncast instance using your application URL and in a few seconds, your broadcast should start. 

## Conclusion

I want to end by thanking the Owncast contributors and community for making self-hosting a live-streaming server so easy. 

My plan in the next couple of weeks is to:

1. Embed my stream on my website. Most likely, I'll make it a dedicated page. Something like `lqdev.me/live` or something like that.
1. Do a few pilot streams to make sure my stream quality and configurations are all set up correctly.
1. Evaluate what the costs are for this solution and whethere it's sustainable.

If all goes as expected, I'll start streaming more regularly. As mentioned, I want to keep my costs down and video hosting is expensive, so I'll use YouTube to post recordings. I'll also keep backups though in case any of those videos get taken down, I can always have a local copy still available. ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/deploy-owncast-azure</link>
      <guid>https://www.luisquintanilla.me/posts/deploy-owncast-azure</guid>
      <pubDate>2025-01-09 20:47 -05:00</pubDate>
    </item>
    <item>
      <title>Digitize Analog Bookmarks using AI, .NET, and GitHub Models</title>
      <description>&lt;![CDATA[
## Introduction

This past year I've made more of an effort to read. I track books in my [/library](/library) page.

Although I have a [Nova Air 2 E-Note device](https://shop.boox.com/products/novaair2), given the choice, I prefer physical books. Despite the conveniene of an electronic note-taking device, theres something about analog that I find hard to quit. 

As I read books, especially non-fiction, I annotate them in various ways. Eventually, those annotations make their way into my library page.

Here's an example of those annotations for [Building A Second Brain by Thiago Forte](/library/building-a-second-brain).

The process of transferring notes is manual and tedious. I don't always have the discipline to transfer them at periodic intervals and what ends up happening is, I get to the end of the book without transferring any notes. To make space for new books, I donate it or resell the ones I've read. If I didn't take the time to transfer those notes, they're gone. [Slow Productivity](/library/slow-productivity-newport) is an example of that.

I want to find a better system that's low-maintenance for keeping more of these notes and retaining knowledge I've found valuable. 

Then it hit me, why not use AI? I know I could use OCR or even some of the AI models of yesteryear. The challenge is, those systems are error prone and given I don't always have the motivation to transfer notes manually, I have even less motivation to build and maintain such a system.

However, vision models have advanced significantly and when paired with language models, the barrier to entry for reasoning over image data has drastically decreased. 

That's what led to this post. In this post, I'll show how you can use AI models like GPT-4o Mini to extract the passages I've annotated in physical books from an image. I then format those passages in markdown to make them easy to directly copy and paste them onto the website. 

I know there's probably a ton of services that do this for you, but it's a lot more fun to build one from scratch. With that in mind, let's get started. 

You can find the source for the application in the [AIBookmarks GitHub repository](/github/AIBookmarks). 

Alternatively, I've configured the repo to use GitHub CodeSpaces, so you can launch the application there as well.

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/lqdev/AIBookmarks)

## Configure access to GitHub Models

I'll be using [GitHub Models](https://github.blog/news-insights/product-news/introducing-github-models/) as my AI model provider. GitHub Models provides developers with access to a catalog of AI models such as GPT-4o, Llama 3.2, Phi 3.5, and many others. Best of all, it's completely free, [though restrictions apply](https://docs.github.com/en/github-models/prototyping-with-ai-models#rate-limits).

I chose GitHub Models for the following reasons:

- Zero installation. Although I really like Ollama and some of the other local AI model providers, I didn't want to fill precious hard-drive space with AI models.
- It's free! Since I'm just prototyping, even with their limited capacity, it should be enough to prove out whether my scenario is feasible.
- They provide access to multi-modal models such as GPT-4o, Llama 3.2 Vision, and Phi 3.5 Vision, which can reason over text and images, which is what I need.
- I'm one of the GPU poor. My [Lenovo Thinkpad X1](https://www.lenovo.com/us/en/p/laptops/thinkpad/thinkpadx1/x1-titanium-g1/22tp2x1x1t1?orgRef=https%253A%252F%252Fduckduckgo.com%252F) couldn't handle running one of the vision models.

Getting set up with GitHub models is fairly easy. At minimum, it requires:

1. A GitHub Account.
1. A Personal Access Token. For more details, see the [GitHub documentation](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens).

Once you have both of those, you can get started building your application.  

## Create a client

The sample application is a C# console application which targets .NET 9. However, the code shown here should work on the latest LTS version as well.

For this solution, I use the Azure AI Inference implementation of [Microsoft.Extensions.AI](https://www.nuget.org/packages/Microsoft.Extensions.AI.AzureAIInference/). [Microsoft.Extensions.AI](https://devblogs.microsoft.com/dotnet/introducing-microsoft-extensions-ai-preview/), M.E.AI for short, is a set of abstractions that provides a common set of interfaces for interacting with AI models.

Although we're using the Azure AI Inference client SDK implementation of M.E.AI, we can still use it to work with models from the GitHub Model catalog. 

```csharp
IChatClient client =
    new ChatCompletionsClient(
        endpoint: new Uri("https://models.inference.ai.azure.com"), 
        new AzureKeyCredential(Environment.GetEnvironmentVariable("GITHUB_TOKEN")))
        .AsChatClient("gpt-4o-mini");
```

## Load your images

In the repo, I have a set of sample images containing annotations. These are the images I'll send to the AI model for processing. 

The following is an sample from [The Creative Act by Rick Rubin](/library/creative-act-way-of-being-rubin).

![An image of a book with pencil markings](https://github.com/lqdev/AIBookmarks/raw/main/data/creative-act-1.jpg)

1. Load the files

    ```csharp
    var filePaths = Directory.GetFiles("data");
    ```

1. Create a collection to store extracted passages

    ```csharp
    var passages = new List&lt;AIBookmark&gt;();
    ```

## Process the images

Once you have the images loaded, it's time to proces them.

1. Start by setting the system prompt. This will provide the initial guidance for the extraction task.

    ```csharp
    var systemPrompt = 
        """
        You are an AI assistant that extracts underlined, highlighted, and marked passages from book page images.

        When passages have a natural continuation between pages, merge them and assign the page number where the first passage starts.
        """;
    ```

1. Then, iterate over each of the images and process them.

    ```csharp
    foreach(var path in filePaths)
    {
        var file = await File.ReadAllBytesAsync(path);
        var messages = new List&lt;ChatMessage&gt;
        {
            new ChatMessage(ChatRole.System, systemPrompt),
            new ChatMessage(ChatRole.User, new AIContent[] {
                new ImageContent(file, "image/jpeg"),
                new TextContent("Extract the marked passages from the image"),
            })
        };

        var response = await client.CompleteAsync&lt;List&lt;AIBookmark&gt;&gt;(messages, options: new ChatOptions {Temperature = 0.1f});

        passages.AddRange(response.Result);
    }
    ```

    This code:

    1. Loads the image as a `byte[]`.
    1. Composes a set of messages that include:

        - The system prompt
        - The image
        - The user prompt instructing the model to extract the marked passages from the image

    1. Sends the messages to the model for processing and returns a list of `AIBookmark`. An `AIBookmark` is a custom class I've defined as follows:

        ```csharp
        class AIBookmark
        {
            public string Text {get;set;}

            public int PageNumber {get;set;}
        }
        ```

        Some of the more recent AI models support structured output, which enforce a schema on AI model outputs. You can take a look at the [OpenAI documentation](https://openai.com/index/introducing-structured-outputs-in-the-api/) for more details. It's important to note though that the functionality is not exclusive to OpenAI models.

    1. Adds the extracted passages to the `passages` collection.

## Format the results

Once all of the files are processed by the AI model, additional processing is done to ensure that they're in the correct page order as well as formatted as markdown blockquotes. 

```csharp
var sortedPassages = 
    passages
        .OrderBy(p =&gt; p.PageNumber)
        .Select(p =&gt; $"&gt; {p.Text} (pg. {p.PageNumber})");
```

## Display the results

```csharp
foreach(var passage in sortedPassages)
{
    Console.WriteLine(passage);
    Console.WriteLine("");
}
```

For the images included in the repo's *data* directory, output might look like the following.

```markdown
&gt; This isn’t a matter of blind belief in yourself. It’s a matter of experimental faith. (pg. 278)

&gt; When we don’t yet know where we’re going, we don’t wait. We move forward in the dark. If nothing we attempt yields progress, we rely on belief and will. We may take several steps backward in the sequence to move ahead. (pg. 278)

&gt; If we try ten experiments and none of them work; we have a choice. We can take it personally, and think of ourselves as a failure and question our ability to solve the problem. (pg. 278)

&gt; Staying in it means a commitment to remain open to what’s around you. Paying attention and listening. Looking for connections and relationships in the outside world. Searching for beauty. Seeking stories. Noticing what you find interesting, what makes you lean forward. And knowing all of this is available to use next time you sit down to work, where the raw data gets put into form. (pg. 296)

&gt; Just as a surfer can’t control the waves, artists are at the mercy of the creative rhythms of nature. This is why it’s of such great importance to remain aware and present at all times. Watching and waiting. (pg. 296)

&gt; Maybe the best idea is the one you’re going to come up with this evening. (pg. 297)
```

## Improvements and next steps

Putting the application together took me less than an hour so this is far from done. However, it does provide me with a starting point and offers validation that this could be a way to more easily capture the knowledge I'm curating from physical books. 

Some improvements I can make here:

1. Update the system prompt with some samples to help guide the extraction. For example, in the output I shared, that last passage is not annotated. It just happens to be a mostly blank page with that quote in the center, therefore giving the illusion that the passage is important in some way.  
1. Add additional information to the AIBookmark class like `Index` so I can ensure order within a page is preserved. Right now page number is good enough, but I can't guarantee the correct order. An index property might help here.
1. Use a service with higher rate limits. The current rate limits wouldn't allow me to process a large number of images at once. Therefore, I'd need to use a service with higher limits. Alternatively, I could make this a job that runs in the background which abides by the rate limits but I also don't have to spend money on. Given I'm not using this for anything mission-critical, that'd be an acceptable solution as well. 
1. Refactor the solution so I can more easily swap between tasks. For example, sometimes I might want to use it with images from my bullet journal. Other times, I might want to use it with handwritten notes. Whatever the case may be, it'd be good to not have to rewrite the prompts every time. 

Some ways I see myself using this project:

1. Periodically collect images of annotated pages and save them to cloud storage. 
1. When I'm done with the book, drop all the images in the *data* directory.
1. Further enrich data by condensing repetitive passages and extracting key concepts.
1. Storing this knowledge into some sort of knowledge store to make it actionable. 

## Conclusion

Just for fun, Tyler released all of the lyrics to his [latest album](/feed/chromakopia-tyler-the-creator-released) as [images on X](https://twitter.com/tylerthecreator/status/1852105825650708651/photo/1). With a few tweaks, I was able to repurpose this solution to extract the text from them and that worked relatively well. 

Just with this simple solution, there's a ton of other applications I can think of in my daily life to help bridge my analog and digital lives. 

What other use cases do you see yourself using something like this for? [Let me know](/contact).

Happy coding!]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/digitize-analog-bookmarks-with-ai-dotnet</link>
      <guid>https://www.luisquintanilla.me/posts/digitize-analog-bookmarks-with-ai-dotnet</guid>
      <pubDate>2024-10-31 21:41 -05:00</pubDate>
    </item>
    <item>
      <title>The Case for .NET in the Classroom</title>
      <description>&lt;![CDATA[
# Introduction

I recently finished by M.S. Computer Science degree and throughout the course, I used a variety of programming languages to complete assignments such as Matlab (Octave), Python, R, Java, Scala, Scheme (Lisp), and C. While all powerful in their own way, in my personal time (if and when I had any), I tried to learn about .NET tools and the ecosystem as a whole mainly because I have been a long time user of Microsoft products that include but are not limited to Zune, Kin, Surface and Windows Phone. Coincidentally, around the time I started my program, Microsoft was in the process of making an aggressive push to be more open and return to its roots of making tools and services that developers love. That being said, this is not exactly something that many people are aware of and the company's image in the eyes of consumers and developers using open source technologies has only recently started to improve. With recent acquisitions of companies like Skype and Xamarin as well as products like HoloLens, the opportunities for developers to build transformative solutions for consumers and businesses across a wide range of devices is limitless. Equally exciting and advantageous is the fact that the underlying ecosystem many of these technologies rely on is .NET. Keeping all that in mind, I strongly believe that there is no better time to introduce .NET into the classroom, especially at the university level in order to equip the next generation of developers with the necessary tools to achieve more in a wide range of environments. 

## Microsoft Past

### Enterprise and Licensing

For the longest time, Microsoft has been synonymous with enterprise and rightfully so as most businesses run Windows and Office. What this also meant was licensing. Many of the products and services that Microsoft provided for businesses and developers were licensed products. Even though there were in some instances free versions of the products, they had their limitations. As a result, this naturally created a barrier to entry to try and experiment with their products.

### Overhead

Even when trying to do the simplest thing with .NET Framework (particularly C#, ASP.NET or F#), you needed to run Visual Studio. Like the enterprise and licensing, this provided a barrier to entry for someone trying to learn the language. Conversely, in the most closely related language Java, you could've gotten started with a text editor and command line (among other things, this might be one reason Java has been widely used in the classroom).

### Platform Specific

Although most of the world ran Windows, developer tools and products like the .NET Framework targeted Windows only. While this was acceptable a few years ago, the world is much different today than it was then. The current state of computing, while not entirely a statement on Microsoft and their products is one that Microft has astutely recognized. People want their services and experiences to travel with them regardless of the platform or products they use.

Along these lines is the type of devices individuals performed their computing on. A few years ago, the Windows PC was the main device for productivity. Therefore, there was little to no need to focus on mobile, let alone cross-platform capabilities. Now, individuals can perform elaborate and deeply complex workflows from the palm of their hand. Therefore, the focus has shifted from a single device category to multi-device and multi-platform one.

## Microsoft Today

Microsoft has significantly transformed their business practices and refocused on what at one time made them one of the top companies in the world by making experiences and products consumers and more importantly developers love.

### Mobile-First, Cloud-First

This was the strategy Satya Nadella tried to get the entire team to rally around when he became CEO of the company. At the time, Microsoft had recently purchased Nokia, yet its mobile phone efforts were non-existent aside from the three percent. Therefore, this strategy seemed ambiguous and unattainable at least from a consumer perspective.

Upon further inspection, this strategy is not limited to mobile phones. Instead it refers to the ability of individuals' computing experiences, with the aid of the cloud, to go wherever they can be most productive. The hardware and to some extent, the software in terms of the platform no longer bind individuals as the services are available nearly everywhere.

A shining example of this is Xamarin. Xamarin is a mobile app development platform built on the .NET ecosystem. With this purchase, Microsoft allowed developers who were native .NET developers to work in the same environment they were used to, while targeting a new family of devices running Android or iOS. Aside from learning the some of the inner workings the Xamarin framework, at the core, individuals can leverage the .NET tools they're used to using. 

### Emerging Technologies
#### Artificial Intelligence

Artificial intelligence, data science and machine learning have been at the forefront of discussions involving technology both in the enterprise as well as academia. Microsoft has made strides on many fronts to position themselves and provide the means for individuals to use the Microsoft set of tools to create solutions. 
One of the ways Microsoft has worked on providing developers and individuals with tools for deep learning is the Microsoft Cognitive Toolkit (CNTK). This framework is very similar to Tensorflow. Although models can be built using BrainScript or Python, the models themselves can be easily plugged into .NET solutions seamlessly. 
Another way in which Microsoft is providing tools for artificial intelligence is through its Bot Framework. The Bot Framework has different parts. One of them is the creation of chat bots using either NodeJS or C#. These bots can target different chat applications that include but are not limited to Skype, Facebook Messenger, Slack and many others. These bots can be made even more intelligent with Microsoft's Language Understanding Service (LUIS) which takes care of the natural language processing the bot might need to process requests adequately.

The second part of the Bot Framework is Cortana Skills. Using LUIS and the Bot Framework, individuals can create speech and text-enabled solutions for individuals that interact with Cortana enabled devices which include but are not limited to smart speakers, PCs and mobile devices. This new kind of experiences again is entirely backed by the .NET ecosystem many developers already use today. 

#### Virtual and Mixed Reality

While still in its infancy, virtual and mixed reality have received tons of support from technology companies like Facebook with Oculus, Google with Google VR, Apple with AR Kit. Likewise, Microsoft's solution is HoloLens. Applications for HoloLens can be developed with .NET tools and Unity. Yet again, developers can leverage their .NET skills to build applications for yet another family of devices to build new immersive experiences that not too long ago were closer to science-fiction than reality. 

### .NET Core and .NET Standard

A lot of computing functions overall are fairly standard. For example, reading from the console is a relatively universal procedure regardless of the platform. Microsoft has found a way to take advantage of the power of .NET for these scenarios and released .NET Standard and .NET Core which are cross-platform environments to develop C#, ASP.NET and F# solutions. These applications can run on the web as well as any OS platform further expanding the capabilities of the ecosystem as well as extending the number of solutions developers can create.

Furthermore, Visual Studio, while helpful is not a requirement to start creating applications. Most things if not all can be done via the command line and a text editor. Therefore, the barrier to entry both in terms of cost and start time has been greatly reduced and individuals get started building applications in little to no time for free. 

## Conclusion

While this is not the entire gamut of tools and services Microsoft provides, it is a good demonstration of how learning a single set of skills like C# and F# in conjunction with the standard curriculum can greatly empower the next generation of computer scientists and developers by expanding the number of devices and platforms they can target across a wide variety of industries and disciplines making the number of applications nearly limitless.]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/dotnet-in-the-classroom</link>
      <guid>https://www.luisquintanilla.me/posts/dotnet-in-the-classroom</guid>
      <pubDate>2017-12-14 16:57:39</pubDate>
    </item>
    <item>
      <title>Use .NET Interactive to run .NET code in Jupyter Notebooks on an Azure Machine Learning compute instance</title>
      <description>&lt;![CDATA[
# Introduction

In this post, I'll go over how you can run .NET code in Jupyter Notebooks using .NET Interactive and Azure Machine Learning compute instances.

[Jupyter Notebooks](https://jupyter.org/) are an graphical interactive computing environment used in disciplines like data science and education. When it comes to prototyping or learning scenarios, they can help users see near real-time what their code is doing. The interactive computing protocol used by Jupyter Notebooks is extensible and as a result, you're able to run a variety of languages inside of this environment via kernels. Kernels are processes that take the input from the user, execute the code, and return the output for display and further processing. Using [.NET Interactive](https://github.com/dotnet/interactive), a .NET Core global tool, that among its many features provides a kernel for Jupyter Notebooks to run .NET (C#/F#) and PowerShell code.

Although you can install and run Jupyter Notebooks on your local computer, services like [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/) provide single VMs known as compute instances which have Jupyter Notebooks as well as many popular data science libraries and development tools like Docker preinstalled. Therefore, installing .NET Interactive on one of these VMs is a seamless experience. Additionally, Azure Machine Learning gives you the ability to share computing resources with teams of various sizes.

Let's get started!

## Prerequisites

- [Azure account](https://azure.microsoft.com/free/). If you don't have one, create one.

## Create Azure Machine Learning workspace

A workspace organizes all of you Azure Machine Learning resources and assets such as compute instances in a single place. In order to use Azure Machine Learning, you have to create a workspace.

There's multiple ways to create a workspace. For this writeup, I use the Azure Portal.

Navigate to [portal.azure.com](https://portal.azure.com)

In the portal, select **Create a resource**.

![Create new resource Azure Portal](https://user-images.githubusercontent.com/11130940/91647989-d79d6380-ea2f-11ea-9d97-cc265c79801f.png)

From the resource list, select **AI + Machine Learning &gt; Machine Learning**

![Create Azure Machine Learning Resource](https://user-images.githubusercontent.com/11130940/91647999-01ef2100-ea30-11ea-954a-3c3280a9cbca.png)

Fill in the form and select **Review + create**.

![Create Azure Machine Learning workspace](https://user-images.githubusercontent.com/11130940/91648065-07993680-ea31-11ea-9273-c28b4ec4bf7c.png)

Review your information before creating the workspace and select **Create**. Deployment takes a few minutes. As part of your workspace deployment, additional Azure resources are created such as Azure Container Registry, Azure Storage account, Azure Application Insights, and Azure KeyVault.

For more information on workspaces, see the [Azure Machine Learning workspace documentation](https://docs.microsoft.com/azure/machine-learning/concept-workspace).

## Create an Azure Machine Learning compute instance

As mentioned earlier, an Azure Machine Learning compute instance is a single VM that comes with a variety of development tools and libraries commonly used for data science and machine learning workflows preinstalled.

Navigate to [Azure Machine Learning studio](https://ml.azure.com). The studio is a web interface for managing your Azure Machine Learning resources such as data, experiments, models, and compute.

In studio, select **Create new &gt; Compute instance** to create a new compute instance.

![Create new Azure Machine Learning compute instance](https://user-images.githubusercontent.com/11130940/91648186-73c86a00-ea32-11ea-9986-205306fe76b0.png)

Provide a name for your instance and select **Create**. You can also customize the size of your VM and whether you want it to be GPU enabled. In this case, I just selected the preselected option.

![Provision Azure Machine Learning compute instance](https://user-images.githubusercontent.com/11130940/91648251-1c76c980-ea33-11ea-91de-818e1f63767c.png)

Once your compute instance is provisioned, select the **Jupyter** link to launch Jupyter Notebooks.

![Launch Jupyter Notebooks](https://user-images.githubusercontent.com/11130940/91648307-f271d700-ea33-11ea-9b80-57e44f0bcdf5.png)

For more information on compute instances, see the [Azure Machine Learning compute instance documentation](https://docs.microsoft.com/azure/machine-learning/concept-compute-instance).

## Install the .NET Core 3.1 SDK

At the time of this writing, the image used by compute instances is Ubuntu 16.04. Therefore, we'll be installing the Linux version of the .NET Core SDK.

Inside the Jupyter Notebooks, select **New &gt; Terminal** to create a new terminal.

![image](https://user-images.githubusercontent.com/11130940/91648326-1c2afe00-ea34-11ea-8f84-48b98e1ec148.png)

In the terminal, add the Microsoft package signing key to your list of trusted keys.

```bash
wget https://packages.microsoft.com/config/ubuntu/16.04/packages-microsoft-prod.deb -O packages-microsoft-prod.deb
sudo dpkg -i packages-microsoft-prod.deb
```

Install the .NET Core 3.1 SDK with the following command:

```bash
sudo apt-get update; \
  sudo apt-get install -y apt-transport-https &amp;&amp; \
  sudo apt-get update &amp;&amp; \
  sudo apt-get install -y dotnet-sdk-3.1
```

&gt; Note that as of the time of this writing, .NET Interactive requires .NET Core 3.1.

You can also find these installation instructions in the [.NET Core installation documentation](https://docs.microsoft.com/dotnet/core/install/linux-ubuntu#1604-).

## Install .NET Interactive

Start off by checking which kernels are installed. In the terminal, enter the following command.

```bash
jupyter kernelspec list
```

The output should look something like the output below

```console
Available kernels:
  python3            /anaconda/envs/azureml_py36/share/jupyter/kernels/python3
  ir                 /usr/local/share/jupyter/kernels/ir
  python3-azureml    /usr/local/share/jupyter/kernels/python3-azureml
```

Then, install the .NET Interactive global tool

```bash
dotnet tool install -g --add-source "https://dotnet.myget.org/F/dotnet-try/api/v3/index.json" Microsoft.dotnet-interactive
```

Use the .NET Interactive tool to install the .NET and powershell kernels.

```bash
dotnet interactive jupyter install
```

Finally, run the following command to confirm that the kernels have been installed.

```bash
jupyter kernelspec list
```

In the output you should see the C#, F#, and PowerShell kernels listed

```bash
Available kernels:
  .net-csharp        /home/azureuser/.local/share/jupyter/kernels/.net-csharp
  .net-fsharp        /home/azureuser/.local/share/jupyter/kernels/.net-fsharp
  .net-powershell    /home/azureuser/.local/share/jupyter/kernels/.net-powershell
  python3            /anaconda/envs/azureml_py36/share/jupyter/kernels/python3
  ir                 /usr/local/share/jupyter/kernels/ir
  python3-azureml    /usr/local/share/jupyter/kernels/python3-azureml
```

## Add .NET global tools directory to PATH

The Jupyter Notebook server is managed by a `systemd` service. Although during installation .NET Interactive is added to your `PATH` environment variable, it's not added to the environment file `/etc/environment` which is used by `systemd`. Therefore, in order to run .NET Interactive, you have to add the .NET global tools directory to that file.

The contents of the file should look similar to the one below

```text
PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games"
AML_CloudName=AzureCloud
```

Open the `/etc/environment` using your preferred text editor (vi/nano) and replace the `PATH` with the following content:

```text
PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/azureuser/.dotnet/tools"
```

Save your changes and restart the Jupyter Server. In the terminal, run the following command.

```bash
sudo service jupyter restart
```

This will close your terminal session since the notebook server has been restarted.

Close all Jupyter Notebook windows.

## Create new .NET Jupyter Notebook

From Azure Machine Learning studio, launch Jupyter Notebooks again by selecting the **Jupyter** link.

Then, in Jupyter Notebooks select **New** and create a new notebook (C#/F#/PowerShell). In this case, I created an F# notebook.

Once the kernel is ready, enter code into the first cell and run it.

![FSharp Jupyter Notebook running in AML compute instance](https://user-images.githubusercontent.com/11130940/91648958-20f3b000-ea3c-11ea-95a1-3e4ba71c31d8.png)

Congratulations! You should now be able to run .NET and PowerShell code inside of your Azure Machine Learning compute instance.

## Conclusion

In this post, I showed how you can run .NET code in Jupyter Notebooks on an Azure Machine Learning compute instance with the help of .NET Interactive. This enables you to interactively prototype solutions remotely while still having control over your environment and dependencies. Now that you have .NET interactive setup, compute instances also give you the option of using JupyterLab. Feel free to tinker with the different environments and see which one works best for you. Happy coding!

## Sources

- [Azure Machine Learning documentation](https://docs.microsoft.com/azure/machine-learning/)
- [.NET Interactive GitHub repository](https://github.com/dotnet/interactive)
- [Jupyter Notebooks](https://jupyter.org/)
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/dotnet-jupyter-notebooks-azure-machine-learning-compute-instance</link>
      <guid>https://www.luisquintanilla.me/posts/dotnet-jupyter-notebooks-azure-machine-learning-compute-instance</guid>
      <pubDate>2020-08-29 20:23:09</pubDate>
    </item>
    <item>
      <title>Down the Slow / Quiet Web Rabbit Hole</title>
      <description>&lt;![CDATA[
No real commentary on this post other than a web surf down the slow / quite web.

```mermaid
flowchart TD
  A["My 20th anniversary of blogging!"] --&gt; B["The Quiet Web"]
  B --&gt; C["Asyncronous Conversations"]
  C --&gt; D["The Slow Web"]
  D --&gt; E["The Slow Web Movement - I DONE THIS"]
  E --&gt; F["The Slow Web (plus: the modern experience of film-watching)"]
  F --&gt; G["Minimal Gallery"]
  G --&gt; H["My website is a shifting house next to a river of knowledge. What could yours be?"]
  H --&gt; I["Quiet web"]
  I --&gt; J["Jeremy Sarber"]
  J --&gt; K["datagubbe"]
  K --&gt; L["Writers and talkers and leaders, oh my!"]
  L --&gt; M["Is Blogging Dead?"]
  click A href "https://tracydurnell.com/2023/09/23/my-20th-anniversary-of-blogging/" "My 20th anniversary of blogging!"
  click B href "https://briankoberlein.com/tech/quiet-web/" "The Quiet Web"
  click C href "https://manuelmoreale.com/asynchronous-conversations" "Asyncronous Conversations"
  click D href "https://www.jackcheng.com/the-slow-web/" "The Slow Web"
  click E href "https://blog.idonethis.com/the-slow-web-movement/?ref=jackcheng.com" "The Slow Web Movement - I DONE THIS"
  click F href "http://www.rebeccablood.net/archive/2010/06/a_slow_web.html" "The Slow Web (plus: the modern experience of film-watching)"
  click G href "https://minimal.gallery/" "Minimal Gallery"
  click H href "https://thecreativeindependent.com/essays/laurel-schwulst-my-website-is-a-shifting-house-next-to-a-river-of-knowledge-what-could-yours-be/" "My website is a shifting house next to a river of knowledge. What could yours be?"
  click I href "https://manuelmoreale.com/quiet-web" "Quiet web"
  click J href "https://jeremysarber.com/" "Jeremy Sarber"
  click K href "https://datagubbe.se/" "datagubbe"
  click L href "https://everythingchanges.us/blog/writers-and-talkers-and-leaders/" "Writers and talkers and leaders, oh my!"
  click M href "https://abeautifulmess.com/is-blogging-dead/" "Is Blogging Dead?"
```
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/down-the-slow-web-rabbit-hole</link>
      <guid>https://www.luisquintanilla.me/posts/down-the-slow-web-rabbit-hole</guid>
      <pubDate>2023-09-23 23:36</pubDate>
    </item>
    <item>
      <title>E2E Dockerizing a MEAN Stack Application</title>
      <description>&lt;![CDATA[

## Introduction

Lately I've been getting familiar with [Docker](https://docker.com). I have built single container applications using a `Dockerfile` and run them locally. This works fine, especially for deployment purposes to a production VM but it's no different than setting up the VM with all the required dependencies and pushing updates via FTP or source control. However, one of the features that I have found extremely useful is multi-container building and deployment via `docker-compose`. With `docker-compose`, not only can I build and run my application, but also dependent services like databases, caches, proxies, etc. Best of all, the builds are standardized and initialized at once without having to individually install the dependencies and components. This writeup explores how to containerize a MEAN stack application and set up a `docker-compose.yml` file for it to build and start the server and database services defined within it.

## Requirements

This writeup assumes that `Docker`, `Docker-Compose` and `Node` are installed on your PC. 

## The Application

The application is a CRUD todo MEAN stack application. The repo for this application can be found [here](https://github.com/lqdev/todomeandockerdemo).

### Project Structure

```text
|_models (Mongoose models)
| |_todo.model.js
|_public
| |_scripts
| | |_controllers
| | | |_main.controller.js
| | |_services
| |   |_todo.service.js
| |_views
| | |_main.html
| |_app.js (front-end application)
| |_index.html
|_Dockerfile (server service)
|_docker-compose.yml
|_api.js (todo api routes)
|_config.js
|_server.js (back-end application)
```

The front-end is built with `AngularJS` and the back-end is built with `NodeJS` using the `Express` web framework and `MongoDB` database. `MongoDB` models are defined with the `Mongoose` package. In the application, users can create, view, update and delete todo tasks. The `Dockerfile` is used to define the container for the web application and the `docker-compose.yml` defines both the `MongoDB` database container as well as the web application container defined in the `Dockerfile`.

## The Docker File

```docker
#Define base image
FROM node:8

#Set Working Directory
WORKDIR /app

#Copy pakage.json file from current directory to working directory
ADD package.json /app

#Install npm packages
RUN npm install

#Copy all application files from local directory to working directory
ADD . /app

#Open port where app will be listening
EXPOSE 3000

#Start application
CMD ['npm','start']
```

### Define Docker Image and Application Directory

The `Dockerfile` has no extension and the syntax is like a standard text file. `#` characters denote comments. `Docker` works based off images which are basically pre-built packages that are stored in one of many registries such as `DockerHub`. `DockerHub` can be thought of as a package repository/manager like `npm` or `dpkg`. In the first two lines of the file we define which base image we want to create our container with. Since this is a MEAN stack application built entirely in `JavaScript`, we'll be using the `node` version 8 image. Then we want set the directory in which our application will reside. We do this by using the `WORKDIR` command and setting `/app` as our application directory, but any directory of your choosing is valid.

### Install Dependecies

All of our dependencies should be defined in our `package.json` file. In order to install these dependencies in our container, we need to copy our local `package.json` file into our container application directory. This can be done with the `ADD` command by passing the `package.json` and application directory `/app` as arguments. Once that file has been copied, it's time to install the dependencies. To run commands through the build process of the application we use the `RUN` command. The command is no different than the one you'd use on your local machine. Therefore, to install the dependencies defined in the `package.json` file we use the command `RUN npm install`.

### Copy Application Files

Once our dependencies are installed, we need to copy the rest of the files in our local project directory to our container application directory. Like with the `package.json` file we use the `ADD` command and pass `.` and `/app` as our source and destination arguments respectively.

#### .dockerignore

Something to keep in mind is that locally we have a `node_modules` directory containing our installed dependencies. In the previous step, we ran the `npm install` command which will create the `node_modules` directory inside our container. Therefore, there is no need to copy all these files over. Like `git`, we can set up a `.dockerignore` file which will contain the files and directory to be ignored by `Docker` when packaging and building the container. The `.dockerignore` file looks like the following.

```text
node_modules/*
```

### Opening Ports

Our application will be listening for connections on a port. This particular application will use port 3000. We need to define the port to listen on in the `Dockerfile` as well. To do so, we'll use the `EXPOSE` command and pass the port(s) that the application will listen on. (MongoDB listens on 27017, but since the `Dockerfile` only deals with the web application and not the database we only need to specify the web application's port).

### Starting the Application

After our container is set up, dependencies are installed and port is defined, it's time to start our application. Unlike the process of running commands while building the container using the `RUN` command, we'll use the `CMD` command to start our application. The arguments accepted by this are an array of strings. In this case, we start our application like we would locally by typing in `npm start`. The `Dockerfile` command to start our application is the following `CMD ['npm','start']`.

## The docker-compose.yml File

```yaml
version: '2'
services:
  db:
    image: mongo
    ports: 
      - 27017:27017
  web:
    build: .
    ports:
      - 3000:3000
    links:
      - "db"
```

The `docker-compose.yml` file is a way of defining, building and starting multi-container applications using `docker-compose`. In our case we have a two container application, one of the containers is the web application we defined and built in the `Dockerfile` and the other is a `MongoDB` database. The `docker-compose.yml` file can take many options, but the only ones we'll be using are the `version` and `services` option. The `version` option defines which syntax version of the `docker-compose.yml` file we'll be using. In our case we'll be using version 2. The `services` option defines the individual containers to be packaged and initialized.

### Services

As mentioned, we have two containers. The names of our containers are `web` and `db`. These names can be anything you want, as long as they're descriptive and make sense to you. The `web` container will be our web application and the `db` container will be our `MongoDB` database. Notice that we have listed our `db` service first and then our `web` service. The reason for this is we want to build and initialize our database prior to our application so that by the time that the web application is initialized, it's able to successfully connect to the database. If done the other way around, an error will be thrown because the database will not be listening for connections and the web application won't be able to connect. Another way to ensure that our database is initialized prior to our web application is to use the `links` option in our `web` service and add the name of the database service `db` to the list of dependent services. The `ports` option like in our `Dockerfile` defines which ports that container will need to operate. In this case, our `web` app listens on port 3000 and the `db` service will listen on port 27017.

#### Container Images

The `docker-compose.yml` can build containers based on images hosted in a registry as well as those defined by a `Dockerfile`. To use images from a registry, we use the `image` option inside of our service. Take the `db` service for example. `MongoDB` already has an image in the `DockerHub` registry which we will use to build the container. Our `web` container does not have an image that is listed in a registry. However, we can still build an image based off a `Dockerfile`. To do this, we use the `build` option inside our service and pass the directory of the respective `Dockerfile` containing the build instructions for the container. 

## Building and Running Containers

Now that our container definitions and files are set up, we're ready to build and run our application. This can be done by typing `docker-compose up -d` in the terminal from inside our local project directory. The `-d` option runs the command detached allowing us to continue using the terminal. This command will both build and start our containers simultaneously. Once the `web` and `db` containers are up and running, we can visit `http://localhost:3000` from our browser to view and test our application. To stop the application, inside the local project directory, we can type `docker-compose stop` in our terminal to stop both containers.

## Conclusion

This writeup uses a pre-configured MEAN stack CRUD todo application and explores how to define a single container `Dockerfile` as well as a multi-container application using `docker-compose`. Docker streamlines how applications are built and deployed while `docker-compose` allows more complex multi-container applications to be orchestrated, linked, deployed and managed simultaneously allowing developers to spend more time  developing solutions and less time managing infrastructure and dependencies.

###### Links/Resources

[Docker Community Edition](https://www.docker.com/community-edition#/download)  
[Docker: Getting Started](https://docs.docker.com/get-started/)  
[NodeJS](https://nodejs.org/en/)]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/e2e-mean-docker-build</link>
      <guid>https://www.luisquintanilla.me/posts/e2e-mean-docker-build</guid>
      <pubDate>2018-04-29 20:10:09</pubDate>
    </item>
    <item>
      <title>FediForum Day One Recap</title>
      <description>&lt;![CDATA[
Just wrapped up a successful first day of FediForum.

The vibes and energy were high. Tons of great conversations and projects around the social web.

A few emerging themes I noticed:

- Identity
- Portability / Interoperability
- Feeds
- Commerce

Ian Forrester kicked us off with his Public Service &amp; The Fediverse keynote [(Slides)](https://docs.google.com/presentation/d/1HuQF_sI19zwByM793uvdl7BECneRmni_H9uOG-oFkdM/edit?slide=id.g420c517061_0_0#slide=id.g420c517061_0_0). 

One of the ideas that struck a chord of public service integrated into the fediverse. More specifically the interest that sparked in me was that publishing and social shouldn't be two separate things. Following the POSSE principle from the IndieWeb. You publish on your own site and then it's syndicated elsewhere. 

This was interesting enough for me I even hosted a session on the topic, I think it was called Tightening the Loop between CMS and the Fediverse. It was my first unconference, so I appreciated the way the agenda was built. Announce your topic, see whether there's interest, put it on the agenda, chat with fellow participants. Super easy.

## Identity &amp; Portability

These are such a huge topic but for the purpose of this post, I'm lumping them together.

[https://bounce-migrate.appspot.com/](Bounce) is one of the projects aiming to make portability easy. What's so interesting is they're making it easy to migrate across protocols. So if you're in one network like ATProto (Bluesky), migrating to the Fediverse should be relatively seamless with Bounce. 

Some great discussions that emerged on the topic as well include:

- **Reputation** - How do you build a web of trust?
- **Compartmentalization and Deduplication** - A single identity or multiple identities? When "following" someone, which of their feeds takes priority?

## Feeds

Talk of feeds was everywhere. I made a note to myself throughout the conference.

&gt; It's amazing how big the feeds theme is. Feed ownership, customization, and sharing. All powered by open protocols.

- [Bonfire releases 1.0](https://bonfirenetworks.org/posts/bonfire_social_rc/) - Congrats to the Bonfire team on this milestone. I haven't tried Bonfire myself, but the Circles feature caught my attention. It made me reminiscent of Google+.
- [Surf.Social is now in beta](surf.social) - As an avid user and curator of RSS feeds, I'd heard about Surf before but hadn't really looked into it. The beta release was announced at the conference and I quickly was able to sign up and download it. Kudos to the team on this milestone and thanks for being so responsive to my request to join the beta. I did almost no waiting in the waitlist. Once I have a chance to try it out and get familiar with it, I'll share some thoughts.
- [Channels](https://home.channel.org/) from the folks at [Newsmast Foundation](https://www.newsmastfoundation.org/) looks like an interesting way to curate and customize feeds. 
[Bring Your Own Timeline Algorithm](https://huggingface.co/spaces/mozilla-ai/byota) leverages semantic search to help you seamlessly leverage the power of algorithmic feeds but doing so under your control. Cool use of AI. 

There were a few unconference sessions on the topic as well. 

## Commerce

It was great to see folks talking about enabling creators to earn a living on open platforms and the social web. 

I believe [Bandwagon.fm](https://bandwagon.fm/home) showed of an implementation of a payments and subscription system built on top of [Emmisary](https://emissary.dev/), a social web toolkit.

## Additional Resources

Here's a list of other links and projects I was exposed to during the conference.

- [FediForum Advisory Board and Organizers](https://fediforum.org/people/)
- [An Internet of Many Autonomous Communities](https://blog.rudyfraser.com/an-internet-of-many-autonomous-communities/)
- [FediMeteo - Weather updates in your Fediverse Timeline](https://fedimeteo.com/)
- [Public Interest Technology Group (PITG)](https://pitg.network/)
- [Coalition for Content Provenance and Authenticity (C2PA)](https://c2pa.org/)
- [A New Social](https://www.anew.social/)
- [WeDistribute FediForum Live Coverage](https://wedistribute.org/2025/06/fediforum-june-2025/)
- [BBC Human Values](https://bbc.github.io/humanvalues/)
- [The Indie Beat Radio](https://theindiebeat.fm/)
- [Tie Yourself to a Federated Mastodon](https://pluralistic.net/2024/11/02/ulysses-pact/#tie-yourself-to-a-federated-mast)
- [The No Network Effect](https://snarfed.org/2013-04-13_the-no-network-effect)

As always, Cory Doctorow was great way to close out the first day. I even learned a new term, tron-pilled. Which means as a creator of a platform, you're on the side of the users.  

Looking forward to tomorrow's sessions!]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/fediforum-day-1-jun-2025</link>
      <guid>https://www.luisquintanilla.me/posts/fediforum-day-1-jun-2025</guid>
      <pubDate>2025-06-05 17:20 -05:00</pubDate>
    </item>
    <item>
      <title>Never change targeted ads</title>
      <description>&lt;![CDATA[
The origin of this post comes from a random observation I made earlier today on how fascinating and disturbing targeted ads can be. But first, some context.

Over the past year, I've been doing a lot more hiking. Although most trails are well marked, it's always good to have some way to navigate in case you miss a turn. For remote locations with limited connectivity, one of the best ways to navigate, other than paper maps, is to download the trail GPX files and view them using a mapping application offline. This is just what I've been doing on my Surface Duo with the help of [AllTrails](https://www.alltrails.com/) and [OSMAnd+](http://osmand.net/). That setup and workflow are a post on its own, but I'll leave that for another time. While that's worked extremely well, there are some downsides to it. For starters, the Surface Duo is not exactly the smallest or most rugged device so it can be cumbersome to handle while on the trail. That means I have to be extra careful to keep it from falling on rocks or off a cliff. Having navigation turned on also drains the phone battery, which is less than ideal in the event I were to need the phone for an emergency. Therefore, I started looking for an alternative navigation solution.

After significant research, I landed on a wearable device with offline mapping capabilities, specifically the [Garmin Fenix 6](https://discover.garmin.com/en-US/fenix-6/). Although I've had my eye on this device for the past few months, I couldn't justify paying full price for it. Fast-forward to this past week. For work, I recently published some articles on Kubernetes. Around the same time, as part of Father's Day &amp; Amazon Prime Day sales, the price of the device dropped significantly enough for me to buy it. This leads us to the main point of this post.

While looking at my Twitter feed, I ran into this gem.

![Garmin Kubernetes Twitter targeted ad](https://user-images.githubusercontent.com/11130940/123006831-47e1f080-d386-11eb-98ac-af80f6f5f6f0.png)

Coincidence? I think not! It's fascinating to see how hyper-specific ads are based on things I'm interested in. In this case it's harmless. I'm always up for learning a thing or two about Kubernetes and if I get the chance to win this watch, count me in! The disturbing part is, what else does it know about me and how is that information being used? Depending on where on the spectrum of fascinated / disturbed you land on, one way to control the amount of information shared is through the privacy settings. Unfortunately privacy settings only go so far and visibility into who has what data is limited. Maybe it's wishful thinking but it'd be great to have a tool that provides more insights and control over what data is shared and with whom. Personally, I think whether targeted ads / recommendations are good or bad is not black or white. If it's something I'm already doing or looking into, it'd be nice to be presented with options to include as part of my research. On the other hand, if those recommendations / targeted ads are exploiting information from my interests to manipulate my behavior, that's not right. Where that line is drawn though is not always clear and often personal.

In the meantime though, all I can say is, stay classy targeted ads!
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/fenix-social-media-targeted-ads</link>
      <guid>https://www.luisquintanilla.me/posts/fenix-social-media-targeted-ads</guid>
      <pubDate>2021-06-22 17:40:00</pubDate>
    </item>
    <item>
      <title>First Steps with NixOS</title>
      <description>&lt;![CDATA[
## Introduction

For the longest time, when using Linux distributions, I chose those that were Debian-based. I still do for servers. However, on the dekstop I switched to [Manjaro](https://manjaro.org/), which is Arch-based, about two years ago. Two years is a long time without distro-hopping, so I was long overdue. I've been following the [NixOS](https://nixos.org/) project for some time but since I was happy with my setup, I didn't consider making the switch. However, a month ago, I decided to [dip my toes](/feed/maybe-switching-nixos/). I installed NixOS on a flash drive and started tinkering with it. In this post, I'll provide some of my initial impressions and things I find appealing about the OS.

## Declarative configuration

One of the first things I did when I booted into my new NixOS installation was get familiar with the [configuration file](https://nixos.wiki/wiki/Overview_of_the_NixOS_Linux_distribution#Declarative_Configuration). The configuration file is authored using the [Nix programming language](https://nix.dev/tutorials/first-steps/nix-language) and it's the place where you configure your entire system, including services, packages, and desktop environments. The best way I can describe it is a Dockerfile for your desktop. If interested, you can find my [config file](/snippets/nixos-configuration/) on the scripts section of this website. 


There's a few advantages to this approach:

1. **Configurations are stored in a single place** - The advantage to this is, I don't have to figure out where the configuration files for each of the components of my system are. I can manage everything in one place. A perfect example where the configuration file goes beyond what I expected was being able to include my bash configuration. Typically, I'd have that configuration in my `bashrc` file. With NixOS, I can just include that in the configuration file as follows:


    ```nix
    programs.bash = {
        shellAliases = {
            emacs="emacs -nw";
        };
    };
    ```

1. **Enables composition** - Because the configuration file is effectively a script, I can modularize the difference pieces. For example, if I wanted to split out my service and package configurations into separate files, I can do so and reference those individual files from my configuration file, separating concerns while still keeping my system configuration simple. 
1. **Version control** - Because the configuration file is just like any other plain text file, it means I can check it into the version control system or cloud service of my choice and manage it that way. In doing so, not only can I roll back using the built-in NixOS mechanisms, but I have another layer of security in case I need to recover that file. 

Now, because I can define everything about my system in the configuration file, this means I can seamlessly rebuild my entire system using this single file in a reproducible manner.

## Reproducible builds

I got to experience first-hand how the configuration file can simplify out-of-box-experiences and system configurations. Initially, I did not install NixOS on my main hard drive. All of my configuration and learning took place on a flash drive. Once I got my configuration to a place I was satisfied, it was time to take the plunge and reimage my PC. In the past, when I've done something similar, I've had to document everything I did when configuring my system. That's where posts like [Setting up a new Ubuntu PC](/posts/setting-up-new-ubuntu-pc/) come from. With NixOS, all I needed to do was replace my configuration file with the one I configured on the flash drive. Then, when I ran `nixos-rebuild switch`, my system was configured exactly like the one on the flash drive. 

## Risk-free software evaluations

This is something I haven't tried yet, but I could easily see it coming in handy. Sometimes I might need a piece of software to do one thing or maybe I want to see whether it'll solve a problem I'm facing. In order to try it out and use it, I need to install it globally. This can cause changes to my system permanently that I don't want. With NixOS, you can create [ad-hoc shell environments](https://nix.dev/tutorials/first-steps/ad-hoc-shell-environments#ad-hoc-shell-environments). These environments make temporary changes to your system, so you can evaluate a piece of software or use it that one time. Then, when you exit the environment, the software is no longer installed in your system.

## Large package selection

A [large package selection](https://search.nixos.org/packages) to choose from is something I got used to with Manjaro. Being an Arch-based distribution, I had access to the [Arch User Repository (AUR)](https://aur.archlinux.org/). Moving to NixOS, it's nice to know that in this area, NixOS offers great support. Additionally, NixOS has built-in support for [Flatpak](https://nixos.wiki/wiki/Flatpak)

## Still exploring

There's still a few things I don't really get. For example, when installing the .NET SDK, I wanted to have side-by-side installs of the latest STS (7.0) and LTS (6.0). However, including the package names in my configuration file didn't work as expected. Instead I had to use the following convention.

```nix
(with dotnetCorePackages; combinePackages [
    dotnet-sdk
    dotnet-sdk_7
])
```

As I learn more, this will probably make more sense but for now it's a mystery. 

## Conclusion

Overall, I'm happy with NixOS and I can see myself using it for the long-term. Hopefully longer than two years. I'm not a gamer so I can't say how well it does in that front. For web browsing and software development though, I really like what NixOS has to offer. If some of the things I mentioned above sound interesting, I'd encourage you to install it on a flash drive and start tinkering with it. ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/first-steps-nixos</link>
      <guid>https://www.luisquintanilla.me/posts/first-steps-nixos</guid>
      <pubDate>2023-08-22 18:03</pubDate>
    </item>
    <item>
      <title>Fractures of Unfamiliarity &amp; Circumvention in Pursuit of a Nice Time and the Meaning of Franklin's Tower</title>
      <description>&lt;![CDATA[
The past few days, I've had the song Franklin's Tower by The Grateful Dead on repeat. I found some of the lyrics could be taken at face value but others didn't make sense to me. Since I assumed there was a deeper meaning, I decided to look for interpretations and ran into this [article by David Dodd on the official Grateful Dead website](https://www.dead.net/features/greatest-stories-ever-told/greatest-stories-ever-told-franklins-tower).  

This led me to two amazing essays:

- [The Fractals of Familiarity and Innovation: Robert Hunter and the Grateful Dead concert experience](https://web.archive.org/web/20240604001611/http://artsites.ucsc.edu/gdead/agdl/jurgen.html) by Jurgen Fauth
- [Fractures of Unfamiliarity &amp; Circumvention in Pursuit of a Nice Time](https://web.archive.org/web/20160519052524/http://artsites.ucsc.edu/GDead/agdl/fauthrep.html) by Robert Hunter

Hunter's essay is a response to Fauth's. Also, in Hunter's essay you begrudgingly get a detailed breakdown of the lyrics of Franklin's Tower. I'll note how amazing it is that both of these individuals took the time to write these lengthy essays clearly exposing their ideas and points of view on the subject.  

Both of those articles were difficult to find because sadly the pages no longer exist in the website that had posted them. 

After some digging though, I was able to find them on the Internet Archive. So for the sake of preservation, I'll post both of them here in full form. 

## The Fractals of Familiarity and Innovation: Robert Hunter and the Grateful Dead concert experience

```text
Robert Hunter has been the primary lyricist for the Grateful Dead ever since he mailed "St. Stephen," "China Cat Sunflower," and "Alligator" to the band from New Mexico in 1967. Born in 1941, Hunter grew up in California and studied drama for a year at the University of Connecticut, before he spent half a year with the National Guard and later signed up for the same psychedelic research that led Ken Kesey to write One Flew Over The Cuckoo's Nest. His songwriting collaboration with Jerry Garcia and, occasionally, other members of the Grateful Dead yielded such classic Dead tunes as "Truckin'," "Touch of Grey," and "Dark Star". Hunter's lyrics emblazon thousands of tie-dyed t-shirts and skeleton-adorned bumper stickers. Other work of Hunter's includes lyrics for Zero and Bob Dylan as well as a translation of Rainer Maria Rilke's Duino Elegies and volumes of poetry, most recently Sentinel.
When analyzing song lyrics, certain basic differences from poetry have to be understood and observed. When comparing popular song lyrics to contemporary or classic poetry, it is difficult not to miss a striking difference in quality--song lyrics generally tend to be outspoken and trite where poetry clouds itself in ambiguity and complex imagery. Bill Flanagan argues that:

"Rock &amp; roll musicians are strange artists. Their code of manners dictates that they not appear to take themselves very seriously, while the best of their work contains all the passion, imagination, and craft of legitimate art. The intelligent rock musician is put in the position of working as hard as a poet or painter, while being expected to accept his gifts with the self- deprecating shrug of the idiot savant." (Flanagan, ix)
However, the majority of rock lyricists aims at an audience which is supposed to recognize a song instantly during the few weeks it is played heavy-rotation on FM radio and then forget it as soon as possible. Rock lyrics tend to make a point quickly and to make sure that it is understood. Therefore, they reach for clichéd imagery and blatant statements of emotions that traditional poetry would have tended to skillfully embed in high-flung imagery, a tendency which can lead to examples as inane as the ones parodied in Donald Barthelme's "How I Write My Songs."
One of the reasons, however, why such a direct comparison of lyrics and poetry is not fair is not only to be found in the consumer culture pressures that surround the pop music industry, but in the fact that popular music lyrics are not meant to be read at all. Obviously, the text has to be read within the framework of the music it was written for. This holds especially true for Robert Hunter's lyrics, because the unique nature of the music of the Grateful Dead adds layers of meaning to the text and vice versa. A further point I decided to take into consideration is that the Grateful Dead are not successful as a recording band that scores top ten hits (with the notable exception of the 1987 "Touch of Grey"); their studio albums are usually do not fulfill expectations. The strength of the band is unquestionably the live concert--the Grateful Dead have been the number one grossing touring band over the last couple of years, outdoing the likes of Madonna and Paul McCartney, without releasing a new studio album since Built to Last in 1989. This means that the usual way Hunter lyrics are being perceived is at a concert. Therefore, everything that happens at a Grateful Dead concert should be taken into account when reading the lyrics. In this paper, I will try to show how the Grateful Dead concert experience and the lyrics of Robert Hunter's songs interweave and complement each other, mutually adding significance and "meaning" to one another.

One model on how to understand the mutually reinforcing structure of a rock concert can be abstracted from Erika Fischer-Lichte's The Semiotics of Theater. Of course, according to Fischer-Lichte's definition, a concert does not constitute theater; however, her theories can give valuable ideas for a way of looking at the experience of a concert. According to Fischer-Lichte, every sign that is given during the performance can be assigned to a group or system, and the sum of all used and received signs that either correspond or oppose each other in meaning allow an overall interpretation or understanding of the performance. The fourteen sign systems Fischer-Lichte enumerates are: sound, music, linguistic signs, paralinguistic signs, mimics, gestures, proxemic signs, mask, hair, costume, stage conception, stage decoration, props, and lighting. These all make sense in a rock concert environment, especially in more theatrical performances as they became common with stadium bands during the 1970s. An additional system of significance would be the song selection and order, which might be considered a subgroup of the music or linguistic groups. Audience participation should be included, as well. Unless a lot of spoken text is involved, the paralinguistics can not be considered significant, since the loudness, pitch, etc. of a song are inherent in the music and therefore do not justify a group of their own. Props are rare in rock concerts, as well, although some performers have been known to turn their instruments into significant objects, such as Liberace or Prince.

The uniqueness of a Grateful Dead concert lies in the emphasis on certain sign systems over others. While obviously almost every rock concert relies heavily and primarily on the music and the linguistical signs (which are, after all, our objects of interest), the Dead emphasize them more than usual by cutting down on the others--the stage decoration is colorful yet unobtrusive, there is no dancing or hardly any movement by the band members other than what is necessary to play the instruments, the clothing of the band is purely functional and they stopped conversing with the audience years ago. The lighting, designed by Candance Brightman since 1972, is not self-conscious but supports the music:

"Brightman's lighting is extraordinarily sensitive to subtleties in the band's interactions, shunning bombastic effects for a vivid focusing of attention that is as much a part of the jam as the music itself" (Shenk 28).
Furthermore, it includes the audience by
"[breaking] down the barriers between the Dead and its fans by lighting the band so they can still see the audience, or by lighting the audience itself" (Shenk 29).
An important point to note is that none of the sign systems function to enhance the star status of the band-- everything is focused on the music and the relation to the audience. The absence of proxemic signs, mask, hair, props, and costumes is significant in that it enhances the emphasis on the music and the lyrics while taking away from the special status of the band members.

The order of songs, though, is of exceptional importance during a Grateful Dead show. Traditionally, since about 1977, the show is split into a first and a second set, where the first set is characterized by more conventional songs that are played without too much improvised jamming; the length of the songs stays within reasonable limits as well. The second set, in contrast, features many improvised jams that stretch the songs easily out to fifteen, twenty minutes. At some point in the second set, Mickey Hart and Bill Kreutzman (percussion &amp; drums) will take over and play an extend drum solo while everyone else leaves the stage. Before they are finished, the other band members return, and Hart and Kreutzman leave the stage. What follows is an extended, beatless jam of exploratory sounds known as "Space." Bob Bralove, in the liner notes to Infrared Roses, describes it like this:

"Somewhere in the middle of the second set of every Grateful Dead show the band turns a corner. They enter a musical environment without walls or structure. The song form is abandoned, and the very elements of music may be called into question. The only mandate is to explore new territory. It is an environment where rhythm, tone, color, melody, and harmony can be explored without rules or predetermination, a musical adventure where composition and performance are one."
After "Space," the band will play three or four more songs, commonly ending with a high- energy song like "Good Lovin'" or "Sugar Magnolia," and come back for a one-song encore. This pattern forms a relatively firm structure in which highly unstable elements are put--the music is always improvised, and not one song has been played twice the same way, and the actual list of songs played always varies, to the point where not even the band members know what they will play before they go on stage.

This is of high importance because the underlying structural principle will become evident in Hunter's lyrics, as well. Since certain songs are usually played in the first set and others in the second set, it seems reasonable to group them according to this differentiation and begin by taking a closer look at the first set songs.

"Jack Straw" is a classic first set song--out of the 424 times it had been played between 1971 and 1992, DeadBase lists only 41 second-set appearances. The song appears to be a narrative of a criminal character who drifts through the American wilderness, jumping trains and digging graves while eagles circle high above. It seems to be rooted deeply in an American tradition of fireside songs. The same holds true for "Dire Wolf," a bitter story of death and gambling, precisely set in time and space:

In the timbers of Fennario
the wolves are running 'round
The winter was so hard and cold
froze ten feet 'neath the ground (1-4)
The narrative features virtually no metaphors or similes; everything seems to happen at face value--a complicated search for meaning or "interpretation" is unnecessary. Again, the song is closer to traditional American music than love songs heard on the radio. It is direct yet not necessarily blatant or trite since the subject matter is uncommon for contemporary American rock lyrics.

The Grateful Dead classic "Truckin'" features some opaque references, yet the song is obviously trying to capture the vicissitudes of a life on the road as the band members have experienced it. Some lines, like "Busted--down on Bourbon Street" (37) refer to commonly known incidents (in this case the band's drug bust in New Orleans); others like "What in the world ever became of sweet Jane?" (25) are probably in-jokes that aren't meant for a broad audience to grasp.

Even more clearly an American song in the best tradition of folk singers like Woody Guthrie or Phil Ochs is "Cumberland Blues," a song depicting the plight of a mine worker. Hunter's eye in describing his topic was so sharp that he heard of an actual Cumberland miner wondering "what the guy who wrote this song would've thought if he'd ever known something like the Grateful Dead was gonna do it." He considers this the best compliment he ever received for a lyric (Hunter 52), obviously because it certifies an authenticity he aimed for.

It seems that first set songs commonly share a genuine American topic (the open road, the wilderness, unions, etc.) and often sketch a narrative world including characters and situations. Instead of propagating emotions, as rock lyrics often seem to, they focus on fictional events that often involve gamblers, cowboys, drunkards, and outlaws. The imagery and "lyrical" makeup of the songs tends to be simple so as to give a definitive, discernable "meaning."

An investigation of second set songs quickly shows that they are of a different character. "China Cat Sunflower," a typical second-set song since the early eighties, may serve as a prime example: the lyrics refuse any narrative or symbolic reading, the words simply are. Hunter acknowledges the non-rational nature of the song with his tongue-in-cheek remark: "Nobody ever asked me the meaning of this song. People seem to know exactly what I'm talking about. It's good that a few things in this world are clear to all of us" (Hunter 35).

Nonsensical as they may be, the lyrics bear a close resemblance to the dreamy, colorful playfulness of Lennon/McCartney's "Lucy in the Sky with Diamonds"--

Picture yourself in a boat on a river,
With tangerine trees and marmalade skies
Somebody calls you, you answer quite slowly,
A girl with kaleidoscope eyes.
Cellophane flowers of yellow and green,
Towering over your head.
Look for the girl with the sun in her eyes,
And she's gone.
Lucy in the sky with diamonds (1-9)
In the surrealistic, candy-colored playfulness of both songs, the words are not meant to "mean" anything; the usual tools of interpretation have to fail here. As Hunter says, "I like a diamond here, a ruby there, a rose, certain kinds of buildings, vehicles, gems. These things are all real, and the word evokes the thing. That's what we're working with, evocation." (Gans 26). His concern is not merely narrative or mood anymore, but evocation and association, something much closer to poetry than to the common clichés of pop lyrics.
"Crazy Fingers" is another example for this approach to songwriting--it is content with letting its verses set a mood without making any "sense." Hunter calls it a "collection of haiku- style verses, mostly seventeen syllables, some more successful than others, with no connecting link other than similarity of mood" (Hunter 45). Some of the imagery is indeed not very original; the "Deep Sea of Love" (7) can surely be encountered elsewhere. Nonetheless, certain paradoxical images (like "Recall the days that still are to come" (3)) and the overall disjunctive structure of the text make for a strange, unlikely song that refuses to be "understood" on any intellectual level. It can only be described in its emotional impact, the longing and bittersweet reminiscing it evokes in a unique way.

Similar points can be made for "Eyes of the World"-- it is interesting to note that the lyrics seem to make sense, yet on any "objective" level, they do not. Traditional analysis of the text may or may not yield satisfactory results by assigning connotations and significance to, say, the "redeemer"(22) , the mile-long wings (7) or the "eyes of the world" themselves; fact is that the inherent instability of the lyrics, fueled again by interior contradictions like "...the seeds that were silent / all burst into bloom and decay," (26-27) will not allow for a final, reductive interpretation. It is clear that Hunter does not aim to communicate "meaning" at all; he says, "I'd really prefer not to get into tearing apart the symbology of my songs [ . . . ] and I'll tell you why: symbols are evocative, and if there were a more definite way to say things, you'd say them that way. A symbol, by it's very nature, can pull in many, many shades of meaning, depending on the emotional tone with which you engage the piece" (Gans 23).

The lyrics of songs commonly played as encore or to close the second set are often more direct (as "Sugar Magnolia") or soothing lullabies ("Brokedown Palace").

The course of a Grateful Dead concert, then, can be said to describe an arc that ranges from traditional, narrative lyrics that concur with conventional playing to outrageous flights of fancy in the second set, correspondingly in the musical jams as well as in the ambiguity-laden, irrational, highly evocative lyrics. The strangeness climaxes during the purely instrumental "Space," during which the band is at its most exploratory. The last songs and the encore return the audience to a festive or pensive mood. However, the function of the lyrics during a Grateful Dead concert is more than to simply underline the music--the linguistic signs are not present to emphasize what is happening in the music. Instead, there seems to be a strange interplay between familiarity and newness, and it is at this edge of the well-known and the innovative that the Grateful Dead draw energy. Audience response is a valuable key to this aspect of the music and the lyrics.

Experience and my subjective impression suggests that the audience at a Grateful Dead show is usually more responsive and alert than audiences at other rock concerts, but since there is no data to prove or argue this point, it can not be made in a meaningful way. What is observable and obvious, though, are the immense cheers and shouts that erupt at certain points in the concert, namely when the band returns from a long stretch of interpretation to the melody (and therefore, mostly, the lyrics), and at certain catchphrases within the lyrics that have gathered almost mystical meaning over the years. Both phenomena need to be looked at more closely.

In the improvisational style of the Grateful Dead ("jazz syntax with a rock lexicon," as David Gans calls it (ix)), melody as one of the markers signifying a song is subject to sudden change-- at its best, the band composes on the stage, making up new elements on the go; the process of creating becomes visible to the audience. Part of the philosophy that allows this approach is that mistakes are possible, probable, and excusable-- "on a bad night, they bore you to tears, but a good night makes it all worth it." (Gans iix) The initial melody of the song, however, is not variable, and it is always safe to predict the band's return to it after a period of jamming that is so utterly unpredictable in direction, style, and length that not even the band itself can foresee what will happen: "I can't predict what the band is going to do at any given moment" (Jerry Garcia quoted in Gans 59).

Since the lyrics will be sung only once the proper, initial melody has returned, the counter argument is mostly true, also--when the melody returns, so will the lyrics. The audience cheers at these points because the return of the melody and therefore the lyrics means a return to the familiar, to the predictable and understandable after long forays into the unknown.

While this holds true for virtually any return to the preestablished, well-worn structural elements of the song, certain catchphrases within Hunter's lyrics seem to yield an especially enthusiastic response by the audience. A good example for this is "If you get confused, listen to the music play" (20) from "Franklin's Tower", a line that taken by itself offers guidance to a perplexed listener. Yet it is embedded within a song that is as obscure as a lot of second set- lyrics; "Franklin's Tower" is evocative yet of undeterminable meaning. The above-mentioned catchphrase-line is almost hidden within the song and has to be read out of context to prompt the joyful response it usually gets.

More obvious examples of the embedded catchphrase are to be found in first-set songs like "Loose Lucy," a rare Hunter love song, where the audience chooses to take the chorus out of context by applying it to the band itself: "Singing: Yeah, yeah, yeah, yeah / singing: Thank you / for a real good time" (7-9). Here, the function seems to be that of a mutual pat on the back; the cheers represent the subculture of Deadheads and the Grateful Dead acknowledging each other's faithfulness--it is here that it becomes manifest that the "Dead made a mystical pact with their fans, vowing to carry the psychedelic torch as long as they can play their music" (Lee 290). Whether Hunter had such an opportunity for unique bonding in mind when he wrote the lines remains undetermined, yet the authors of Skeleton Key could not but remark on Hunter's "knack for coining aphorisms that yield hard-won truths after hundreds of hearings" (150). The most famous example is certainly the oft-quoted "Lately it occurs to me / what a long, strange trip it's been" (23-24) from "Truckin'," a line that paradoxically admits to the uniqueness of the experience by being its most clichéd example.

It seems that a lot of these phrases took on the meaning for the Deadhead community that they did because they were obvious and understandable. On a paradigmatic level, it should be surprising that a band that is so directly associated with a certain lifestyle and a set of beliefs as the Grateful Dead hardly convey any "messages" at all in their lyrics. It can be safely assumed that one would generally expect a "hippie band" with all the unavoidable stereotypes, to be much more blatant about their convictions. Yet, as the above analysis already shows, Hunter usually confines himself to a narratives of traditional American adventures in the broadest sense and more abstract, lyrical, and image-laden texts that only yield "meaning" by association and evocation.

However, there are lines hidden within his lyrics that have taken on very charged connotations exactly because they seem to be more outspoken and direct, and a philosophy can be attributed to them. All these lines have turned into catchphrases as well, cheered at concerts and printed on countless t-shirts and bumper stickers.

"Scarlet Begonias," a song commonly played to open the second set yet at least somewhat narrative in that it has a narrator and a female character, yields two instances of embedded catchphrases-- "Once in a while / you get shown the light / in the strangest of places / if you look at it right" (31-34) and "Strangers stop strangers / just to shake their hand" (46-47). The philosophies that can be abstracted from these lines by reading them out of context include the search for truth in unlikely places, maybe in the tradition of the "angel-headed hipsters" (2) who roam the urban wasteland in Ginsberg's "Howl," and a proclamation of unsolicited friendliness and openness that the stock and trade of the hippiedom the Grateful Dead supposedly stand for. All this is abstracted again from the less direct context the lines originally appear in; the Deadhead community seems to have taken what it could and abandoned the quirky senselessness of "The wind in the willows played tea for two / The sky was yellow and the sun was blue" (44- 45). The argument could be made that the aphoristic lines are not only cheered at because of what they mean and imply, but simply for the fact that they mean at all. They are familiar lines within an ambiguous lyrical world that refuses to be reduced to a discernable, single interpretation. The catchphrases are familiar lines within a familiar yet, to the rational mind, unyielding lyrical world. "Scarlet Begonias" refuses to be understood as a whole, so the audience jumps at those four lines that seem to make sense. The catchphrases, then, are to the rest of the lyrics what the melody is to the improvisational jam--they mark the return to an understandable, well-known, and familiar touchstone before the next journey into newness and obscurity.

This phenomenon can probably be best understood when looking at what is generally considered the prime example of the Grateful Dead's efforts, "Dark Star":

" "Dark Star" is the kernel of wide open possibility at the core of the Dead's repertoire, the essential seed promising unlimited intergalactic space journeys at the speed of total mindwarp: "Shall we go,/ you and I/while we can?" It is the most exploratory of Dead tunes and it is the trippiest, the one where the acidic whistle of the dark interstices is heard most starkly, where you might turn any corner and step off into the void. It is the Dead's spirit of musical adventure at its strangest, wildest, most vehement, weirdest, dreamiest, and the place where the leading wave of the Dead's music is most often audible being created in mid-air." (Jim Powell quoted in Shenk 51)
Interestingly, "Dark Star" was the first song that Hunter wrote together with the Grateful Dead in 1967. The lyrics are extremely short, considering that a full-fledged "Dark Star" can easily last half an hour. The song is made up by two verses that are usually separated by minutes and minutes of intense jamming. A close reading reveals Hunter's technique of juxtaposing the abstract and the concrete in a single image, parataxis, and contradiction in full bloom. "Searchlight casting / for faults in the / clouds of delusion" (7-9) is such a tricky image, suggestion that the speaker has a very clear sense of what he is talking about, yet again, the song cannot be said to "mean" anything in any traditional sense, and not even the tedium of a symbolic reading will yield any "meaning." However, the verse beginning "Shall we go..." which precedes the next drawn out jam insinuates that the speaker understands, and invites the reader along through the "transitive nightfall / of diamonds" (14-15). The drawn-out jam that usually follows can be understood as exactly the "transitive nightfall / of diamonds," the lyrics then talking about the music, a kind of meta-lyric. After seemingly endless improvisation, the text and melody returns with "Mirror shatters" (16), an event cheered by the audience as it means the return to a familiar signpost.
It has been argued that "Dark Star" draws its power from the violation of norms and the constant impossibility of assigning meaning:

" ['Dark Star' leads] to a transcendent mysticism because the music, continually violating norms and expectations, avoid a clear marking or denotation of a signified. What is left is a state of ambiguity in which the listener is enticed into closer attention of momentary detail." (Skaggs, quoted in Shenk 64)
I would argue that the overwhelming power of "Dark Star" and, in a broader sense, the overall effect of a Grateful Dead show, does not entirely rely on the elusiveness of its signs, be it musically or linguistically, but on the tension it creates between the familiar and the new. The Deadhead community as a whole exists because of the familiar aspects of the Dead. The fascination of the band grows largely out of the relationships between the improvised and the structured that are increasingly complex to the point of becoming fractal.
The model that I would suggest for the intricacies of a Grateful Dead show and the rich relationships that exist between the lyrics and the music is that of fractals, self-containing geometrical structures like the Mandelbrot set. The outside of the Gingerbread man are the ordered regions, where a number can easily be assigned to the function, z (n+1)=z(n)2+c, while the black inside of the Gingerbread man stands for total, absolute chaos, unstructuredness. The edge where those two areas meet is characterized by intense, bizarre beauty and recurring images of the bigger picture that recede--like a Russian doll, the Gingerbread man contains images of himself, infinitely, just as a Grateful Dead concert contains recurring patterns of familiarity and innovation on a receding scale. The overall structure of the concert is firm, or almost firm--there are always exceptions, yet the pattern of first and second set, drums, space, and encore are set. This structure is filled with actual songs every night; the choice of songs varies, yet the songs are generally well-known and familiar.

The way they are going to be played on any given night, however, is uncertain and actually unpredictable--no improvisational jam that opens in preconceived places within the song will be the same, yet it can be assumed it will be at a given place. The lyrics, joining the return to the melody, are firm and part of a preconceived pattern within a song. They will not be changed or tampered with. However, the firm and unchanging lyrics have shown themselves to be unsteady and undecidable due to their ambiguous and contradictory character. This is only true, again, for the most part, since the lyrics do contain embedded catchphrases that possess a heightened familiarity within the song. The audience will recognize and appreciate these moments of predictability and profound understanding as a community before the jam wanders off into unknown directions again, possibly into the far reaches of musical exploration in "Dark Star" or to the completely strange and wordless "Space," which can be considered the climax of unfamiliarity within the development of the concert. After the purely instrumental excesses of "Space," the return of rhythm, harmony, melody, then lyrics, and finally understandable and rational lyrics is greeted by enthusiastically by the audience which is pleased to be gently returned to a sensical universe where exploration and rediscovery, departure and return, ambiguity and blatantness co-exist and fuel each other. Hunter's lyrics are allowed to be reasonably narrative and plain-old weird at given times; the music always corresponds to or counter-accentuates the words, both live off each other in an organic and healthy way. Standing on their own, the lyrics to "Dark Star" might seem interesting and lyrical yet pointless; in union with the music, they gather as many meanings as the listener can assign to them:

[Jerry Garcia:] "Dark Star" has meant, while I'm playing it, almost as many things as I can sit here and imagine, so all I can do is talk about 'Dark Star' as a playing experience. [Charles] REICH: Well, yeah, talk about it a little. [Jerry Garcia:] I can't. It talks about itself. [Charles] REICH: Each time it comes out in a different way? [Jerry Garcia:] Yeah, pretty much. There are certain structural poles which we have kind of set up in it, and those periodically we do away with. (Garcia 85)

The music of the Grateful Dead and the lyrics of Robert Hunter open up spaces within the work of art they form together that make textual interpretation of the lyrics futile. The lyrics function not as an absolute that is stated, but as a tool for semiosis--at every concert, they can and do mean something different according to how the variable parts in the music change. Every time a song is performed, a different set of potential meanings that are inherent in the piece will be actualized for a different audience. Various parts of the concert experience are firm, others are variable, and as I tried to illustrate using the Mandelbrot set, the firm parts contain further movable aspects, and vice versa. The resulting multi-media, multi-sign system artifact can shine in infinite variations. The result is one of an ever-changing, fluctuating whole that contains familiar parts while never completely yielding to reason. This amorphous, continuously self- altering gestalt that is a thirty-year process --the seams show, there are parts that are completely opaque and others that we have known for decades, and it always changes: "It stumbles, then it creeps, then it flies with one wing and then it flies with one wing and bumps into trees and shit" (Garcia 76). Deadhead sociologist Rebecca Adams puts it this way: "People say Deadheads are throwbacks. I think they are pioneers. They recognize that reality is subjective--there is no right way--and have been cognizant of these multiple realities for a lot longer than most other people. This is post-modernism. It's the cutting edge" (quoted in Shenk xiii). Robert Hunter's lyrics are a vital part of this experience, providing the mind-searching music with an incessantly fascinating imagery that both underscores and counter-points its meaning by not meaning anything at all.

Works Cited
Barthelme, Donald. "How I Write My Songs." Sixty Stories. New York: Putnam, 1981. 418-423.
Bralove, Bob. Liner Notes. Grateful Dead. Infrared Roses. Grateful Dead Records, 40149-2, 1991.
Flanagan, Bill. Written in My Soul: Rock's Greatest Songwriters Talk About Creating Their Music. Chicago: Contemporary, 1986.
Fischer-Lichte, Erika. The Semiotics of Theater. Indianapolis: Indiana UP, 1992.
Gans, David. Conversations with the Dead: The Grateful Dead Interview Book. New York: Citadel 1991.
Garcia, Jerry, Charles Reich, and Jann Wenner. Garcia: A Signpost to New Space. San Francisco: Straight Arrow, 1972.
Hunter, Robert. A Box of Rain: Lyrics 1965-1994. New York: Penguin, 1993.
Scott, John W., Mike Dolgushkin, and Stu Nixon, eds. DeadBase VII: The Complete Guide to Grateful Dead Song Lists. Hanover, NH: 1993.
Lee, Martin A. and Bruce Shlain. Acid Dreams: The Complete Social History of LSD: The CIA, the Sixties, and Beyond. New York: Grove Weidenfeld, 1992.
Lennon, John and Paul McCartney. "Lucy in the Sky With Diamonds." The Beatles. Sgt. Pepper's Lonely Hearts Club Band. EMI, CDP 7464422, 1967.
Shenk, David and Steve Silberman. Skeleton Key: A Dictionary for Deadheads. New York: Bantam, 1994.
Skaggs, S. "The Transitive Nightfall of Diamonds: Eco and the Aesthetics of Transcendence." University of Louisville, Kentucky. Presented at the Conference of the Semiotic Society of America, St. Louis, Missouri, 1993.
```

## Fractures of Unfamiliarity &amp; Circumvention in Pursuit of a Nice Time

```text
Dear Jurgen,

meaning is not an irreducible Ur-language. A good lyric is allusion, illusion, subterfuge and collusion. A poor lyric is information about its own paucity of resource. That doesn't mean the latter cannot be a great song. There is nothing inherently better about a dumb song than one which calls attention to the intelligence of its writer. It's a matter of taste, but meaning is often a subterfuge to distract the listener's attention from a writer's lack of multiple resources. This is often true of blatant "message" songs.
How does a song mean?
As long as allusions can be codified, the semiotician is content, knowing that "meaning" is a case-sensitive term with scant referentiality outside implementation of primitive needs. When the semiotician suspects allusiveness without corresponding exact reference, he charges the poet with nonsense. Nonsense is a loaded word, the meaning of which is unclear. If it is understood as "intentional multi-referentiality without predetermined hierarchy" rather than "meaningless blather" one would find no fault with the term. But it isn't, so the charge of "nonsense" and "meaninglessness" levied by a scholarly and plausible source, does much to put people off exploring further.
"The repair sad mutthead forkbender orange in the how are you, did they?" is nonsense without reference, hence not allusive. Had it discernible rhythm, it might be termed 'rhythm allusive' but my example lacks even that. The only way on earth such a sentence would likely get written is as an example of a null allusive set. Bingo! An allusion! To Set Theory! The exception that proves the rule.


How does a song mean?
The meaning(s), or lack thereof, ascribed by others to an example of lyric work are not part of the work. The interpretations are separate "works". The manner in which an audience receives the work, what they, collectively and individually, make of it, can indeed provide potential data for the allusiveness (referentiality)of future lyrics, gainsaid, but cannot be ascribed as a characteristic of the particular work, per se, with validity without "insider information" which is, in any case, no part of the song. That way lies true nonsense, even unto deconstruction. Yet the little bugger of a jingle persists and seems to move hearts. Why? Is there something which semiotics, by its nature and presuppositions, must exclude from the sphere of "meaningfulness" due to the limited nature of its tools?

How does semiotics mean?
Since the concluding remark of your essay stated that the Grateful Dead songs are "meaningless" I choose to reply by explicating one of your examples: "Franklin's Tower." I do this reluctantly because I feel that a straightforward statement of my original intent robs the listener of personal associations and replaces them with my own. I may know where they come from, but I don't know where they've been. My allusions are, admittedly, often not immediately accessible to those whose literary resources are broadly different than my own, but I wouldn't want my listeners' trust to be shaken by an acceptance of the category "meaningless" attached to a bundle of justified signifiers whose sources happen to escape the scope of simplistic reference.

How does the song go?
FRANKLIN'S TOWER

In another time's forgotten space
your eyes looked through your mother's face
Wildflower seed on the sand and stone
may the four winds blow you safely home

4x: Roll away... the dew

You ask me where the four winds dwell
In Franklin's tower there hangs a bell
It can ring, turn night to day
Ring like fire when you lose your way

Roll away... the dew . . .

God help the child who rings that bell
It may have one good ring left, you can't tell
One watch by night, one watch by day
If you get confused just listen to the music play

Roll away... the dew . . .

Some come to laugh their past away
Some come to make it just one more day
Whichever way your pleasure tends
if you plant ice, you're gonna harvest wind

Roll away... the dew . . .

In Franklin's Tower the four winds sleep
Like four lean hounds the lighthouse keep
Wildflower seed in the sand and wind
May the four winds blow you home again

4x: Roll away... the dew
You better roll away the dew
---------------------------------
What's it about?
EXPLICATION:
In another time's forgotten space
your eyes looked through your mother's face
Wildflower seed on the sand and stone
may the four winds blow you safely home.

&lt;surface intent&gt;
[You have your mother's eyes, child,
the very shape, color and intensity
of the eyes that looked through
her face so long ago. Borne on the
varied winds of chance and change,
like a dandelion seed, you may find
yourself deposited on barren soil.
My wish for you is that the forces
that brought you there may sweep
you up again and bear you to fertile ground.]

&lt;deeper intent&gt;
"In another time's forgotten space
your eyes looked through your mother's face."
[Relative immortality of the human
species is realized through reproduction.
Dominant traits inherited from an ancestor,
the lyric suggests, share more than mere
similarity with those of the forebear,
but are an identity, endlessly reproducible.
In other words, when someone says
"You have your mother's eyes"
they are not speaking in simile
nor would it be incorrect to say
that "your mother has your eyes,"
if, in fact, possessiveness is
an appropriate term in the context.
Poetic license will assume it is,
if only for the sake of moving on
to the next couplet.]

You ask me where the four winds dwell
In Franklin's tower there hangs a bell
It can ring, turn night to day
Ring like fire when you lose your way

[note that this song appeared in 1975,
the year after my son was born and the
year before the American Bicentennial.
Both facts are entirely relevant. The
allusion to the Liberty Bell and the
situation of the Philadelphia Congress
in the hometown of Ben Franklin has not
gone unnoticed by other commentators.
This song is a birthday wish both for
my son and for my country, each young
and subject to the winds of vicissitude.
Individual and collective freedom,
liberty, conscience, all that is conjured
by those concepts, is suggested
in the image of the tolling bell.]

God help the child who rings that bell
It may have one good ring left, you can't tell
One watch by night, one watch by day
If you get confused just listen to the music play

[The Bell, rung once, cracked and could not
be safely rung again. From an actual bell,
it therefore became a symbol of the
potential to ring. The single toll, signaling
birth, can now be heard only in its
reverberations in our history and ideals.
Some have had to bear those ideals in
difficult circumstances (war, the Great
Depression and general benightedness)
others have had the more enviable task
of keeping watch (eternal vigilance)
during periods of conscious and dynamic
change: the full light of day. The sixties,
the writer assumes, were such a time.
You can't tell if ringing that bell
a second time would destroy it in
the act of producing another mighty
peal and it might be foolish, if courageous,
to try. Perhaps the "music"of
the original ideals symbolized by
the first and only toll should be taken
to heart and implemented, rather than
obviated by a new source of ideation
(communism, anarchy, religion based
governmental apparatus. etc.) To resolve
this confusion, pay attention to the
original inspiration (the Constitution,
the Bill of Rights, collectively.
Individually, maintain awareness of
conscience and one's own early ideals.]

Some come to laugh their past away
Some come to make it just one more day
Whichever way your pleasure tends
if you plant ice, you're gonna harvest wind

[This verse scarcely needs commentary
in light of the above remarks. The precursor
to the 1st couplet is "I Come for to Sing" as
performed, possibly written, by Pete Seegar.
The 2cnd couplet source is the biblical
"Who sows wind reaps the hurricane."]

In Franklin's Tower the four winds sleep
Like four lean hounds the lighthouse keep
Wildflower seed in the sand and wind
May the four winds blow you home again

[We assume a bell tower for the great bell.
By the trope of simile, we see the bell tower
(the day watch) turned to a lighthouse
and the four winds become sleeping hounds,
(the night watch) worn out by the events
of such a metaphorical day as related
by e.e.cummings in his familiar lyric
"All in Green Went My Love Riding"
(Poem 6 from TULIPS AND CHIMNEYS)
"four lean hounds crouched low
and smiling . . ." By the use of quotative
allusion the lyric attempts to borrow
some of the emotive spark of cumming's
poem, providing a kind of "link button"
into a different but complementary space.
Allusion here functions as a sort of
shorthand cross-patch into a series
of metaphoric events which, with
a double-clutch shift of simile,
access a downloadable description of
the kind of day it's been for a 'wildflower
seed' in its adventures in the wind.
There may be some objection to the
elastic interchangeability of the similes
of hounds and winds in this set of couplets,
but the test of the allusion, as I see it,
is whether or not the appropriate emotions
are evoked to lead to satisfying closure
and an opening door on other possibilities.]

[Now to the real stretch: "Roll away the dew."
The line is appropriated from a fairly
well known sea chantey whose chorus goes:
"Roll away the morning dew
and sweet the winds shall blow."
As surely everyone knows by now,
Bonnie Dobson's song "Morning Dew"
(made famous by Garcia's singing of it)
is set in the aftermath of nuclear war.
Reason he can't "walk you out
in the morning dew, my honey"
is because of fallout, though Garcia
has wisely dropped the verse
containing this denouement, allowing
the song a heightened romantic mystery,
achieved through open-ended ambiguity.
For generations now alive,
the nuclear specter personifies
the forces which most threaten our
attempt at Jeffersonian democracy.
With the song's sub-allusion to
"Roll away the Stone," an anthem
of joyous Eastertide resurrection,
a resultant combination message
of dire necessity (as in the final:
you've got to roll away the dew)
and promise of renewal, in case
resolution is effected, are enjoined.
Should this hyper-allusive train
of thought become too confusing
to process, the invitation to just
"listen to the music play"
acknowledges both the melody
and performance context of the lyric
and the metaphoric bell described above.]

Well, now that you know what I meant by it,
it's no great shakes is it? Mystery gone,
the magician's trick told, the gluttony
for "meaning" temporarily satisfied,
one can now take issue with my intent
and avoid the song itself, substituting
the assignable significance for the music.

Attempts by language to overdetermine language are doomed out the door, so I content myself with providing these clues for threading the maze of "Franklin's Tower" and as a grudging key to my methods. I feel that much of what you've said in your essay is rich, correct and thought provoking and appreciate your accurate estimation of the concert context as adjunct to the lyric, and vice versa. The contextual sub-meaning (the way the song manifests itself in concert) is certainly a factor which occasionally determines certain choices in subsequent material. Too much of that would be a striving after sameness of effect, though (even if it does all sound the same to an uninitiated ear.)

Oh, one other thing: you labeled "what a long, strange trip it's been" as "cliched." Aren't you putting the cart before the horse? "Truckin'" was the originating vehicle of the phrase, which had not, to my knowledge, been coined before. The fact that it has entered the catchphrase banks of the language in a ubiquitous way may render subsequent usage cliched, but surely not the invention itself, unless all widely adopted phrases are deemed trite by virtue of their durability. You also mentioned that the "What in the world ever became of Sweet Jane?/She's lost her sparkle, you know she isn't the same" verse was probably an in-joke not meant for a broad audience to grasp. The intention was a parody of the '40's warning-style of singing commercial, specifically "Poor Millicent, poor Millicent/ She ne-ver used Pep-so-dent/ Her smile grew dim/ And she lost her vim / So folks don't be like Millicent / Use Pep-so-dent! " I'm sure that the allusiveness, not that entirely outre in the '60's, is well lost here in the '90's. So, it's perhaps an in-joke, but not one meant for private consumption. Just a bit of black humor that fails to fire and emerges, instead, as an enigma. I guess the question here is whether an allusion must be blatantly perceivable as such in order to avoid the uncharitable label of "nonsense."

Thank you for taking my work seriously enough to spend considerable effort in explicating it according to your lights.

Robert Hunter 3/4/96

p.s.
Disorient slow quill to provide
Dense unsingable margin
Dispersing in tonal cluster under
A lilac roof of chintz Vermeers
Incorporate chance nomenclature
Corresponding to a netting of wind
Revolving in unintentional resolve
Interrogating with no question mark
Non-circular limited clock of the day
Before rude lingual enforcement
Immerses momentos of dux redux
More given to flux than flight in Shank's aviary
Where no one spits in the beer of slumber
With less reluctance than an attested breeze

(from the Collected Sonnets of Cosmo Nitram,
soon to appear on [Hunter's] pages)
```]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/fractures-unfamiliarity-circumvention-pursuit-good-time-robert-hunter</link>
      <guid>https://www.luisquintanilla.me/posts/fractures-unfamiliarity-circumvention-pursuit-good-time-robert-hunter</guid>
      <pubDate>2025-02-26 19:44</pubDate>
    </item>
    <item>
      <title>Organizing F# Modules Into Separate Files</title>
      <description>&lt;![CDATA[
# Introduction

After watching [Jeff Fritz's](https://twitter.com/csharpfritz) F# Friday [Stream](https://www.twitch.tv/videos/268107540), I was inspired to do some tinkering of my own. I have looked at F# in the past, but often times have not done much with it. After the stream, I decided to keep plugging away at it and one of the first things I thought about doing was organizing my code into separate files. As simple as it may seem, it was no easy task and while there are many different ways of doing it, the method described in this writeup is one of the easiest I found. Keep in mind though that this may not be the appropriate way of doing it although it seems to work nicely for our purposes. In this solution I will create a set of mathematical operations that will be organized into a single module that will then be imported and used inside of my console application. Source code for this sample solution can be found at this [link](https://github.com/lqdev/fsharpmoduledemo).

## Prerequisites

This solution was built on a Mac, but should work on both Windows and Linux environments.

- [.NET Core SDK](https://www.microsoft.com/net/download/macos)
- [Ionide Extension - Option 1](https://fsharp.org/use/mac/)

## Create Project

Once you have everything installed, you can create a new F# project by entering the following command into the console.

```bash
dotnet new console -lang f#
```

## Create Module

The goal of this solution will be to create functions that add, subtract, multiply and divide two numbers. Since all of these functions are mathematical operations, we can organize them inside a single module. Therefore, we can create a new file inside of our project called `Math.fs` which will contain all of our functions.

```fsharp
module Math
    let add x y = x + y
    let subtract x y = x - y
    let multiply x y = x * y
    let divide x y = x / y
```

## Compile File

Now that our file has been created, we need to add it to our list of files to be compiled by our project. To do so, we can add an entry for our `Math.fs` file to the `ItemGroup` property inside our `fsproj` file. The contents of the file should look as follows after the addition:

```xml
&lt;Project Sdk="Microsoft.NET.Sdk"&gt;

  &lt;PropertyGroup&gt;
    &lt;OutputType&gt;Exe&lt;/OutputType&gt;
    &lt;TargetFramework&gt;netcoreapp2.0&lt;/TargetFramework&gt;
  &lt;/PropertyGroup&gt;

  &lt;ItemGroup&gt;
    &lt;Compile Include="Math.fs" /&gt;
    &lt;Compile Include="Program.fs" /&gt;
  &lt;/ItemGroup&gt;

&lt;/Project&gt;
```

## Import Module Into Console Application

Once we have our module created and our `fsproj` file configured, we can call the module from our console application. To do so, we need to first import it into our `Program.fs` file.

```fsharp
open Math
```

Once imported, we can call the `Math` module functions from our `main` function in the `Program.fs` file.

```fsharp
    printfn "Add: %i" (add 1 2)
    printfn "Subtract: %i" (subtract 1 2)
    printfn "Multiply: %i" (multiply 1 2)
    printfn "Divide: %i" (divide 4 2)
```

## Run Application

Now that everything is set up, we can build and run our console application with the following commands:

```bash
dotnet build
dotnet run
```

The output should be the following:

```bash
Add: 3
Subtract: -1
Multiply: 2
Divide: 2
```

## Conclusion

In this writeup, I went over how to create an F# console application that organizes a set of mathematical functions into a module whose contents are stored in a separate file. This is good for organization and to reduce clutter. Happy coding!


]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/fsharpmodulesample</link>
      <guid>https://www.luisquintanilla.me/posts/fsharpmodulesample</guid>
      <pubDate>2018-06-05 20:56:52</pubDate>
    </item>
    <item>
      <title>Generate QR Codes for MeCard Data in F#</title>
      <description>&lt;![CDATA[
## Introduction

Today I learned about MeCard file formats for representing contact information while reading the post [Sharing contacts using QR Code without requiring a network connection](https://andregarzia.com/2023/10/sharing-contacts-using-qr-code.html) by Andre Garzia.

Currently I host my [vCard](../vcard.vcf) on my site in a very similar way Andre describes in his post. I've also encoded it in [QR Code format](../images/contact/qr-vcard.svg) to make it easy to import and share.  

When I saw the MeCard format, I immediately liked how simple it was so I decided to see how difficult it'd be to write some code to generate a QR code for my own contact data.

## What is MeCard?

According to Wikipedia, MeCard is "...a data file similar to vCard...It is largely compatible with most QR-readers for smartphones. It is an easy way to share a contact with the most used fields. Usually, devices can recognize it and treat it like a contact ready to import."

An example might look like: `MECARD:N:Doe,John;TEL:13035551212;EMAIL:john.doe@example.com;;`

## Encoding text as a QR code using F#

I'd written a [snippet](/snippets/qr-code-generator) a few years ago to help me generate the QR Codes for all accounts in [my contact page](/contact).

The snippet is a F# Interactive script which uses [Net.Codecrete.QrCodeGenerator NuGet package](https://www.nuget.org/packages/Net.Codecrete.QrCodeGenerator) to encode text as a QR code and saves it to an SVG file. 

The core logic consists of the following code:

```fsharp
let createQrCode (savePath:string) (target:string) = 
    let qr = QrCode.EncodeText(target,QrCode.Ecc.High)
    let svgString = qr.ToSvgString(4)
    File.WriteAllText(savePath,svgString, Encoding.UTF8)
```

In this case `target` is the text I want to encode and `savePath` is the path of the SVG file I'm saving the generated QR code in. 

Given MeCard with the following content:

```csharp
MECARD:N:Doe,John;TEL:13035551212;EMAIL:john.doe@example.com;;
```

The QR Code generated might look as follows:

![MECARD QR CODE](/images/generate-mecard-qr-code-fsharp/mecard.svg)

Scanning the QR Code with a smartphone would then immediately recognize it as a contact and prompt you to save the contact information to your device.

## Conclusion

Since I already had the code snippet to do this available, creating a MeCard was straightforward. I've since added my [MeCard](/mecard.txt) alongside by vCard. You can check out the corresponding [QR code](/images/contact/qr-mecard.svg) in my [contact](/contact) page.]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/generate-mecard-qr-code-fsharp</link>
      <guid>https://www.luisquintanilla.me/posts/generate-mecard-qr-code-fsharp</guid>
      <pubDate>2023-10-23 19:24</pubDate>
    </item>
    <item>
      <title>Using Generative AI to produce Spotify Clips</title>
      <description>&lt;![CDATA[
## Introduction

While listening to music on Spotify last night, this thought hit me. It's been cooking for some time so now you get to read about it. Spotify and YouTube, if you build this into your platforms for songs or podcasts, [contact me](/contact) for info on where to send the check :joy:. As the title of the post suggests, this is a thought experiment on how you might use Generative AI to produce Spotify Clips. This is not limited to songs but could also be extended to audio-only podcasts. However, given the long-form nature of podcasts it might be extremely resource intensive to produce clips per episode. This post is not exactly a how-to guide, though there are some practical examples of how some of the steps in the workflow might work using [Microsoft Copilot (formerly Bing Chat)](https://copilot.microsoft.com/).

## Spotify Clips

Let's start off by talking about what Spotify Clips are. Back in March of 2023, [Spotify introduced Clips to their platform](https://artists.spotify.com/blog/spotify-clips-get-started-short-form-video-stream-on). Spotify describes [Clips](https://clips.byspotify.com/) as "short, under-30-second vertical videos that are uploaded directly to Spotify for Artists. They’re designed to put you and your music front-and-center, so they can be attached to your artist profile, to a song, to an album, or to an upcoming release.". When paired with a song, you can think of them as short music videos. Music videos being the videos made to accompany songs that played on TV channels like Music Television (MTV) before Jersey Shore and Catfish. Music videos are still in production today and they generally serve the same purpose. They provide a visual representation of the story being told by the song.  

## Generative AI for video

An overwhelming majority of Generative AI models only generate text. With models like DALL-E and Stable Diffusion, you can also generate images. Other models to generate audio and video have started coming on the scene. Most recently, Google announced [VideoPoet](https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html), a large language model for zero-shot video generation. There's many interesting techniques that VideoPoet introduces but I'll keep it to one of the more simple scenarios and the technique used in this post, text-to-image. Given a text prompt, VideoPoet can generate by default, a short 2-second clip. You can use VideoPoet as well to extend a clip so you could go well beyond the default 2 seconds. That however is outside the scope of this post. With that in mind, let's talk about how we might use a Generative AI model and apply it to the context of Spotify Clips.

## Generative AI Spotify Clips workflow

The following is a rough idea of how you might use song information like lyrics to 

```mermaid
flowchart TD
  A["Get song lyrics"] --&gt; B["Generate scenes and script"]
  B --&gt; C["Generate prompts for each scene"]
  C --&gt; D["Generate video clips"]
  D --&gt; E["(Optional) Edit clips"]
  D --&gt; F["Stitch clips together"]
  E--&gt;F
```

This is roughly what Google did in their VideoPoet samples. They "...produced a short movie composed of many short clips generated by the model. For the script, we asked Bard to write a series of prompts to detail a short story about a traveling raccoon. We then generated video clips for each prompt, and stitched together all resulting clips". The resulting video looks like the following:

[![An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities](http://img.youtube.com/vi/70wZKfx6Ylk/0.jpg)](https://www.youtube.com/watch?v=70wZKfx6Ylk "An AI generated video of a traveling racoon demoing VideoPoet AI model capabilities")

One thing to note is that the videos generated are YouTube Shorts which are vertical as well. This makes it easier to produce videos in the format Spotify expects Clips to be in.

## AI Generated Spotify Clips in practice

Now that we know the general process, what does it actually look like in practice? To help me with that, I used Microsoft Copilot.

### Generate scenes and script

In the example shared by Google, they used a prompt to generate a script made up of a series of prompts. In the case of songs on Spotify, we can use that as context and the starting point for our script. From there, we can ask Copilot to compose scenes for the verses in the song. We can also make sure as these scenes are generated, that there is a smooth transition between them and the storytelling is consistent. 

Let's take a look at what this might look like using the Chicano Batman song "A Hundred Dead and Loving Souls". Why that one? Just check out my [Spotify Wrapped](/feed/spotify-wrapped-2023). The general theme of that song is about the cycles of life experienced across millenia and generations. 

**Prompt**

Given the following prompt:

```text
Using the song information below as context, generate a script for a story that visually represents the song lyrics. Ensure that the script is broken down into individual scenes and the scenes are linear and cohesively transition.

&lt;song&gt;
&lt;title&gt;A Hundred Dead and Loving Souls&lt;/title&gt;
&lt;lyrics&gt;
I am the sand stuck under miles of rock
watching the dry air sweep away my thoughts
as we travel protected by shards of glass
driving through the history of a million memories

In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
Climbing a thousand unclimbed trees
I don't know I want to know how it would feel
to be on top of one of these!

I am the root
quickly ascending
up through the surrounding dirt
quickly devoured by a hungry sun
quickly devoured by a hungry son
who is my father
and we are alike
we look alike we talk alike
we are alike
we are alike
we are alike we look alike
we talk alike

Thats why Im hungry
like the bellowing sky
this is no lie
as sure as light
entering eye
want to stay high
everything's cool too cool so cool
and I remember that everything falls

So I take a look around me now, as I speak
and I imagine a disinterested crowd
talking aloud
and I think of what I want most
and the crowd disappears
and a fixated gaze envelopes my wondering mind
and then she recruits and takes me to this place
I've been there before and that's where I saw her face

In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
In a sea of a hundred dead and loving souls
Climbing a thousand unclimbed trees
I don't know I want to know how it would feel
to be on top of one of these!
&lt;/lyrics&gt;
&lt;/song&gt;

Story: 
```

**Response**

The response looks like the following:

```text
# Script for A Hundred Dead and Loving Souls

## Scene 1: The Sand
- A close-up shot of a grain of sand under a microscope, showing its intricate structure and texture.
- The camera zooms out to reveal that the sand is part of a vast desert landscape, with dunes and rocks stretching to the horizon.
- A voice-over of the protagonist (a young man) narrates the first verse of the song, expressing his feelings of being trapped and forgotten under the weight of the world.
- The camera pans to show a car driving on a dusty road, with the protagonist and his father inside. They are both wearing sunglasses and hats to protect themselves from the sun.
- The car is filled with boxes and bags, indicating that they are moving to a new place.
- The father is driving and listening to the radio, while the protagonist is looking out the window, bored and restless.
- The radio plays a news report about a nuclear fusion experiment in South Korea, followed by a song by a popular band.
- The father comments on the news, saying that it is a breakthrough for humanity and that he hopes to see it in his lifetime.
- The protagonist does not respond, but thinks to himself that he does not care about anything anymore.

## Scene 2: The Sea
- A montage of shots showing the protagonist and his father arriving at their new home, a small town by the sea.
- The father is excited and optimistic, while the protagonist is indifferent and detached.
- The father unpacks their belongings and sets up their new house, while the protagonist wanders around the town, exploring the beach, the pier, and the streets.
- The protagonist sees a group of teenagers hanging out at a skate park, laughing and having fun. He feels out of place and lonely.
- The protagonist walks to the edge of the pier, where he sees a sign that says "No diving".
- He looks down at the water, where he sees hundreds of dead fish floating on the surface, victims of a recent oil spill.
- He hears the chorus of the song in his head, imagining himself diving into the sea of a hundred dead and loving souls, and climbing a thousand unclimbed trees.
- He wonders what it would feel like to be free and alive, instead of being stuck and dead inside.

## Scene 3: The Root
- A flashback scene, showing the protagonist as a child, playing in a garden with his father.
- The father is a botanist, and he teaches his son about different plants and flowers, showing him how to care for them and appreciate their beauty.
- The son is curious and eager, and he asks his father many questions about the natural world.
- The father tells his son that he is a root, quickly ascending up through the surrounding dirt, and that he will grow into a strong and beautiful tree one day.
- The son smiles and hugs his father, saying that he loves him and that he wants to be like him.
- The father smiles and hugs his son back, saying that he loves him too and that he is proud of him.

## Scene 4: The Sun
- A flash-forward scene, showing the protagonist as a young adult, working in a laboratory with his father.
- The father is a nuclear physicist, and he is working on a project to harness the power of nuclear fusion, the same process that fuels the sun.
- The son is his assistant, and he helps his father with the experiments and calculations, following his instructions and commands.
- The father tells his son that he is a hungry sun, quickly devoured by a hungry son, and that he will make a great contribution to science and humanity one day.
- The son nods and agrees, but he does not look happy or satisfied. He feels pressured and obligated to follow his father's footsteps, even though he does not share his passion or vision.
- The son thinks to himself that he is hungry, like the bellowing sky, and that he wants to stay high, away from the reality and expectations that weigh him down.

## Scene 5: The Crowd
- A present-day scene, showing the protagonist at a party, surrounded by a crowd of people.
- The party is hosted by one of the teenagers that he saw at the skate park, who invited him out of pity and curiosity.
- The party is loud and chaotic, with music, drinks, and drugs everywhere.
- The protagonist feels uncomfortable and anxious, and he does not fit in with the crowd. He does not know anyone, and no one knows him.
- He tries to make small talk with some of the guests, but they are either rude, bored, or high, and they do not pay attention to him.
- He feels invisible and insignificant, and he wishes he could disappear.
- He takes a look around him, as he speaks, and he imagines a disinterested crowd, talking aloud, and he thinks of what he wants most.

## Scene 6: The Gaze
- A close-up shot of a pair of eyes, staring intently at the protagonist.
- The eyes belong to a girl, who is also at the party, but who is different from the rest of the crowd. She is quiet and mysterious, and she has a captivating and mysterious aura.
- The girl notices the protagonist, and she is intrigued by him. She sees something in him that no one else sees, something that she relates to and understands.
- She walks up to him, and she introduces herself. She says her name is Luna, and that she likes his song. She says she heard him humming it to himself, and that she knows the lyrics by heart.
- The protagonist is surprised and confused, and he asks her how she knows his song. He says he wrote it himself, and that he never shared it with anyone.
- The girl smiles and says that she knows his song, because she wrote it too. She says she has a gift, or a curse, depending on how you look at it. She says she can hear the songs that people sing in their souls, and that she can sing them back to them.
- The protagonist is shocked and amazed, and he asks her to prove it. He asks her to sing his song, or any song that he knows.
- The girl nods and agrees, and she sings his song, word for word, note for note, perfectly. She sings it with emotion and expression, as if she wrote it herself.
- The protagonist is speechless and mesmerized, and he feels a connection with the girl that he never felt with anyone before. He feels like she knows him, and he wants to know her.
- The girl stops singing, and she looks at him with a fixated gaze, that envelopes his wondering mind. She says she likes him, and she asks him if he wants to go with her to a place that she knows.
- The protagonist nods and agrees, and he follows her out of the party, leaving the crowd behind.

## Scene 7: The Place
- A montage of shots showing the protagonist and the girl driving to a secluded spot in the woods, where they park their car and get out.
- The spot is a clearing, where there is a large tree, with a rope swing hanging from one of its branches.
- The tree is the only one of its kind in the area, and it stands out from the rest of the forest. It is tall and majestic, with green leaves and red flowers.
- The girl tells the protagonist that this is her place, and that she comes here often to escape from the world. She says she loves this tree, and that she calls it Luna, after herself.
- The protagonist tells the girl that this is his place too, and that he has been here before. He says he saw this tree in his dreams, and that he always wanted to climb it.
- The girl smiles and says that they have more in common than they thought, and that they are meant to be together. She says she wants to show him something, and she asks him to follow her.
- The girl climbs the tree, using the rope swing as a support. She is agile and graceful, and she reaches the top in no time.
- The protagonist follows her, using the rope swing as well. He is clumsy and nervous, and he struggles to keep up with her.
- The girl reaches the top of the tree, where she finds a small platform, made of wood and rope. She sits on the platform, and she looks at the view.
- The view is breathtaking, and it shows the sea, the town, and the sky, all in one panorama. The sun is setting, and it casts a golden glow on everything.
- The girl waves at the protagonist, and she invites him to join her. She says this is the best place to watch the sunset, and that she wants to share it with him.
- The protagonist reaches the top of the tree, where he joins the girl on the platform. He sits next to her, and he looks at the view. He is awestruck, and he agrees that this is the best place to watch the sunset.
- The girl leans in and kisses the protagonist, and he kisses her back. They embrace each other, and they watch the sunset together.
- The chorus of the song plays in the background, as the scene fades to black.
```

Not too bad for a first pass. From here you might want to continue tweaking the different scenes but at least you have a starting point. 

### Generate prompts for each scene

Using the text-to-image technique, I first need to generate prompts to guide the model in the video generation process. In the case of this sample, you could take each of the bullet points in the scenes and generate a video for each of them. 

At the time of this writing, I didn't have access to a video generation model and I don't think VideoPoet is open to the public yet. Therefore, I had to slightly modify what I was generating. Instead of generating a video, I used Copilot to generate images using DALL-E. The concept is the same, except instead of generating videos, I'm generating still images. 

I could've applied the same concept of one image per bullet point. However, for brevity, I chose to summarize all of the bullet points in a scene into a single prompt. 

**Prompt**

Given the following prompt:

```text
Using storyboarding techniques, for each scene, generate a single prompt to use as input for DALL-E to generate an image. Since there are 7 scenes, there should be only 7 prompts. Distill as much of the concepts from each of the bullet points in each scene.
```

**Response**

The results from that prompt are the following:

```text
Scene 1: The Sand
    A grain of sand under a microscope and a car driving on a dusty road in a desert
Scene 2: The Sea
    A boy standing on a pier looking at a sea of dead fish and a sign that says “No diving”
Scene 3: The Root
    A father and a son playing in a garden with different plants and flowers
Scene 4: The Sun
    A father and a son working in a laboratory with a nuclear fusion reactor
Scene 5: The Crowd
    A boy at a party surrounded by a crowd of people with music, drinks, and drugs
Scene 6: The Gaze
    A girl with a pair of eyes that can hear the songs that people sing in their souls
Scene 7: The Place
    A boy and a girl sitting on a platform on top of a tree with a rope swing and a view of the sea, the town, and the sunset
```

Again, not too bad for a first pass. The main points of the scene are captured. I can already foresee there are a few scenes like "The Sand" which might be difficult to illustrate in a single image. 

### Generate images (video clips)

Now that we have individual prompts, it's time to generate some images. Again, I'm doing this because I don't have access to a video generation model but the process would be generally the same. 

**Prompt**

```text
Use the prompts for each of the scenes to generate images with DALL-E. Render them as pencil art.
```

**Response**

For brevity, I'll only include some of the images generated:

"The Sand"

![AI genreated image of a grain of sand under a microscope and a car driving on a dusty road in a desert](/images/generative-ai-spotify-clips/sand-car-dirt-road.jpg)

"The Sea"

![AI generated image of a boy standing on a pier looking at a sea of dead fish and a sign that says “No diving”](/images/generative-ai-spotify-clips/child-dock.jpg)

"The Root"

![AI generated image of a father and a son playing in a garden with different plants and flowers”](/images/generative-ai-spotify-clips/father-son-playing.jpg)

The pencil art part was something I thought would look nice to illustrate this instead of digital art or photorealism. That was a personal choice though. 

As we can see from the images, in general they're not too bad. When not condensed into a single image though and you have an image or video per frame in a scene, the generated content can be richer. 

### Edit and stitch videos

For the image generation example, this seems like a natural stopping point. However, with videos, you can imagine using AI to edit these videos. The VideoPoet post mentions how you might be able to do that. Part of that editing might also include lengthening the video clips. Once you're happy with your videos, you would then stitch them together to compose a larger clip. 

## Improvements

Tons! I used the simplest tools available at hand to validate whether my idea is feasible.  

### Videos

The most obvious improvement here would be to actually generate the videos. Whether that's using VideoPoet or another video generation model, the whole point is to end up with a video clip to upload to Spotify.

### Prompts

The prompts I used were not complex at all. I tried to keep them as simple as possible and in many cases, I wasn't the one providing the prompts, Copilot generated them for me. I would try to use more complex prompts to further guide the model in the formats I expect the outputs to be in and give more details for the model to generate better results. 

### Code

All of this was done using Copilot in the browser. As a starting point to validate my ideas this was fine. However, this approach is not composable, testable, reproducible, or scalable. I'd convert these steps to code in order to get all of the things Copilot doesn't give me. 

### Timestamp sync

One of the features Spotify offers is lyrics that are synced to the song. Similarly, being able to sync these generated clips to the song would be important so the visuals match the 

## Challenges

### Consistency

One challenge that someone brought up when I was explaining this idea was, how do you ensure that the visuals are consistent throughout all of the video clips.

That is a great question I don't have a great answer for, though I have ideas. 

If there aren't many changes throughout scenes, you could just opt to keep lengthening the video clip. 

If there are flashbacks or different visuals between scenes, you might achieve consistency by  using the image-to-video modality. Using a snapshot from a frame in a previous video clip, you might try to edit that snapshot to use in between transitions and then lengthen the video to produce the next scene. 

### Power Consumption

As fascinating as this is, is it worth the energy and cooling demands it requires when using large models? Maybe not in its current form, but maybe it makes sense when using smaller models or more power efficient compute. 

### People

For music videos, there's a human element to it both from the performers and the people who produce them. Again, the Generative AI is impressive and for someone like me without any artistic skills, it can be an empowering tool. Although I'm tempted to see what an [AI Rickroll](https://www.youtube.com/watch?v=dQw4w9WgXcQ) might look like, I prefer the human version. :slightly_smiling_face:

## Conclusion

In this post I discussed how you might use Generative AI models like VideoPoet to generate short videos for Spotify Clips. Since I don't have acess to VideoPoet or other video generation models at this time, I decided to show practical image generation examples instead using models like DALL-E to illustrate the general concepts. It's not wihtout its challenges and there's a ton of improvements I can think of but it was a fun experiment regardless. What else would you use these Generative AI models for? 

You can find an addendum to this post [here](/feed/genai-spotify-clips-instrumental).]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/generative-ai-spotify-clips</link>
      <guid>https://www.luisquintanilla.me/posts/generative-ai-spotify-clips</guid>
      <pubDate>2023-12-23 18:30</pubDate>
    </item>
    <item>
      <title>Getting started with Ollama on Windows</title>
      <description>&lt;![CDATA[
Recently [Ollama announced support for Windows](/feed/ollama-windows-preview) in preview. In doing so, people who want to use AI models like Llama, Phi, and many others can do so locally on their PC. In this post, I'll go over how you can get started with Ollama on Windows. 

## Install Ollama

The first thing you'll want to do is install Ollama.

You can do so by [downloading the installer from the website](https://ollama.com/download/windows) and following the installation prompts. 

## Get a model

Once you've installed Ollama, it's time to get a model.

1. Open PowerShell
1. Run the following command

    ```powershell
    ollama pull llama2
    ```

In this case, I'm using llama2. However, you can choose another model. You could even download many models at once and switch between them. For a full list of supported models, see the [Ollama model documentation](https://ollama.com/library).

## Use the model

Now that you have the model, it's time to use it. The easiest way to use the model is using the REST API. When you install Ollama, it starts up a server to host your model. One other neat thing is, the REST API is [OpenAI API compatible](https://ollama.com/blog/openai-compatibility).

1. Open PowerShell
1. Send the following request:

    ```powershell
    (Invoke-WebRequest -method POST -Body '{"model":"llama2", "prompt":"Why is the sky blue?", "stream": false}' -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json
    ```

    This command will issue an HTTP POST request to the server listening on port 11434.

    The main things to highlight in the body:

    - *model*: The model you'll use. Make sure this is one of the models you pulled. 
    - *prompt*: The input to the model
    - *stream*: Whether to stream responses back to the client

    For more details on the REST API, see the [Ollama REST API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md). 

## Conclusion

In this post, I went over how you can quickly install Ollama to start using generative AI models like Llama and Phi locally on your Windows PC. If you use Mac or Linux, you can perform similar steps as those outlined in this guide to get started on those operating systems. Happy coding! ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/getting-started-ollama-windows</link>
      <guid>https://www.luisquintanilla.me/posts/getting-started-ollama-windows</guid>
      <pubDate>2024-03-05 10:32</pubDate>
    </item>
    <item>
      <title>Hello World</title>
      <description>&lt;![CDATA[
Welcome to my personal blog. This will be a blog post for documenting my learning process as well as sharing projects and ideas I come across. I often find myself doing things, researching how to do them and completing a task. A few weeks/months later I might have to do something similar and have to go through the process again. Using this blog will help me keep me reference that knowledge.
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/hello-world</link>
      <guid>https://www.luisquintanilla.me/posts/hello-world</guid>
      <pubDate>2017-12-09 12:00:00</pubDate>
    </item>
    <item>
      <title>Hacker News Vue Top Stories Client</title>
      <description>&lt;![CDATA[
# Introduction

Over the past few years, I've used AngularJS as my front-end framework for most web projects. With the release of Angular and React, I tried both of them and while both are great tools the learning took significantly longer because I also had to learn the bundler systems and in the case of Angular, TypeScript. Although the CLI tools go a long way towards automating a lot of the setup process, building an application from scratch in these frameworks is not intuitive especially for a newcomer. In the early days of Vue, I was able to attend a Meetup presentation by [Evan You](https://twitter.com/youyuxi) which went over the philosophy of Vue as well as its capabilities. I was instantly interested because of how easy it was to get started. Since that presentation, I have not had time to try it but with AngularJS being phased out, I felt it was about time to start exploring another framework. To get a better understanding of the framework's features prior to using CLI tools, I created a standalone application that relies only on NPM packages. In this writeup, I will create a Hacker News client with Vue that calls the Hacker News API and displays the top 50 stories. Source code for this project can be found in the following [repository](https://github.com/lqdev/hnvuedemo).

## Prerequisites

- [Node](https://nodejs.org/en/)

## Initialize Project

The first thing we want to do is create a directory for our project and initialize it with NPM

```bash
mkdir hnvuedemo
cd hnvuedemo
npm init -y
```

## Install Dependencies

Next, we want to install our dependencies. (Express is optional)

```bash
npm install --save vue axios express
```

## Create Server (Optional)

We can set up a server to host and serve our content. This is optional though because the application should still work when the `index.html` file is opened in the browser.

In our root project directory, we can create the `server.js` file and add the following content:

```javascript
var express = require("express");

const PORT = 3000 || process.env.PORT;

var app = express();

app.use(express.static("."));

app.get("/", (req, res) =&gt; {
  res.sendFile("index.html");
});

app.listen(PORT, () =&gt; {
  console.log(`Listening on port ${PORT}`);
});
```

## Create Application

### Create The View

Now it's time to create our application. Let's start by scaffolding the `index.html` which is where our application will be displayed.

```html
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;script src="./node_modules/vue/dist/vue.min.js"&gt;&lt;/script&gt;
    &lt;script src="./node_modules/axios/dist/axios.min.js"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;!--App Content Here--&gt;

    &lt;!--App Scripts Here--&gt;
&lt;/body&gt;
&lt;/html&gt;
```

We want to add the scripts to the `Vue` and `axios` packages in our `head` element. We'll hold off on the content for now.

### Get Data

Our data source for this project will be the Hacker News [API](https://github.com/HackerNews/API). To interact with it, we'll be using the `axios` NPM package. The data collection happens over several HTTP requests, therefore to help with it we'll be creating a file called `apihelpers.js` in our root project directory that will contain functions to get and manipulate the data.

#### Get Top Stories

The first thing we want to do is get a list of top stories. We can do so via the `/topstories` endpoint. The response of this request returns a list of ids which can then be used in conjunction with the `/item/{id}` endpoint to get the individual story data. We can then validate the story object to make sure it is a story as opposed to a user or job posting and return a list of all the story objects. The functions that will help us with that are `getIds`, `isStory` and `extractStories`.

```javascript
/**
 * Checks whether the item is a story
 * @param {Object} story - Story object
 */
function isStory(story) {
  return story.type == "story";
}

/**
 * Gets ids of stories
 * @param {string} url - Url to fetch story ids
 */
function getIds(url) {
  return axios.get(url);
}

/**
 * 
 * @param {Array&lt;Object&gt;} - List of resolved promises 
 */
function extractStories(...responses) {
    var stories = responses.map(story =&gt; story = story.data);
    return stories.filter(isStory);
}
```

### Create Vue App

Now that we have our helper methods to make calls to the API, we can create our application.

We can start by creating our Vue instance which is where our application will live. We can create a file called `app.js` in our root project directory and add the following content:

```javascript
//Components Go Here

new Vue({
  el: "#app",
  data: {
    title: "Hacker News Vue Simple Reader",
    loading: true,
    topStories: []
  },
  methods: {
    getTopStories: function() {
      getIds(
        "https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty"
      )
        .then(ids =&gt; buildRequest(ids.data.splice(0, 50))) //Only get first 50
        .then(getStories)
        .then(axios.spread(extractStories))
        .then(stories =&gt; {
          this.topStories = stories;
          this.loading = false;
        })
        .catch(e =&gt; console.log(e));
    }
  },
  created: function() {
    this.getTopStories();
  }
});
```

There's a lot happening here, so let's break it down based on the properties of the object passed to the Vue constructor.

The `el` property is like a selector which tells the application which element it should operate on. The value `#app` tells it to look for an element where the `id` attribute is `app`. This of course can be anything of your choosing.

The `data` property specifies the properties that contain the data of our application. These properties are reactive which means that whenever they change, that change is reflected in our view. As such, even if they have no value when the application starts, in order for them to automatically change when data is passed to them, they need to be declared in the `data` property. In our application, we have a `title` property which will be the title of our web page, `topStories` which is where we'll store the list of Hacker News stories and `loading` which we'll use to let us know when data is being loaded into our application.

In the `methods` property, we define functions that we want our application to use. In this case, I created the `getTopStories` method which chains together all the functions defined in our `apihelpers.js` file to return the top 50 stories on Hacker News.

Finally, Vue has instance lifecycle hooks. The `created` property defines what should happen when our instance is created. In our case, we want to call the `getTopProperties` method which is defined in our `methods` property to load the data, update our `topStories` data property and set `loading` to `false` because our data has loaded successfully.

### Add App to View

Now that we have created the logic of our application, it's time to add it to our view. We can do so by adding the `apihelpers.js` and `app.js` files to our `index.html` file via `script` elements.

```html
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;script src="./node_modules/vue/dist/vue.min.js"&gt;&lt;/script&gt;
    &lt;script src="./node_modules/axios/dist/axios.min.js"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;!--App Content Here--&gt;

    &lt;!--App Scripts Here--&gt;
    &lt;script src="apihelpers.js"&gt;&lt;/script&gt;
    &lt;script src="app.js"&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
```

### Display Data

Although we can now use our application logic inside of our view, we still can't see any of it because we have not added elements to display it. To view our application data, we can add the following code to our `index.html` file below the `&lt;!--App Content Here--&gt;` section.

```html
&lt;div id="app"&gt;
    &lt;h2&gt;{{title}}&lt;/h2&gt;
    &lt;h5 v-if="loading"&gt;Loading...&lt;/h5&gt;
    &lt;story v-for="story in topStories" :key="story.id" :story="story"&gt;&lt;/story&gt;
&lt;/div&gt;
```

The final `index.html` contents should look like the content below:

```html
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;script src="./node_modules/vue/dist/vue.min.js"&gt;&lt;/script&gt;
    &lt;script src="./node_modules/axios/dist/axios.min.js"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div id="app"&gt;
        &lt;h2&gt;{{title}}&lt;/h2&gt;
        &lt;h5 v-if="loading"&gt;Loading...&lt;/h5&gt;
        &lt;story v-for="story in topStories" :key="story.id" :story="story"&gt;&lt;/story&gt;
    &lt;/div&gt;
    &lt;script src="apihelpers.js"&gt;&lt;/script&gt;
    &lt;script src="app.js"&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
```

As mentioned earlier, we created an element, in this case a `div` that has the `id` attribute with a value of `app`. This is how our application knows where to display our content. Inside of an `h2` element, we display our `title` data property. Below it, we have an `h5` element that displays the text "Loading...". However, this is to only be displayed when our `loading` data property is `true`. If not, it should not be visible. We can achieve this conditional rendering via the `v-if` directive. This directive evaluates the expression inside of it and renders content based on its truthiness. 

Finally, there's one last piece that looks like an element, but not one of the built-in HTML elements. So then, what is it? It's a component. The Vue website defines a component as "...a reusable Vue instance with a name: in this case, `&lt;story&gt;`. We can use this component as a custom element inside a root Vue instance...". In our component, the `v-for` directive is what it sounds like. It creates a sequence of `story` components based on a list of objects defined in our `Vue` instance's `data` property. Our `story` component iterates over the `topStories` data property and assigns the value of the individual object in the list to the variable `story`. We bind the `id` property of the `story` object to the `key` attribute of the component and pass in the entire object to the component via the `story` prop. A prop is a custom attribute that you can register for the component. We can use props to pass data into the component. In all cases, the `:` prefix on the attributes and props of the component are shorthand for the `v-bind` directive which dynamically binds an expression to an attribute or component prop.

With all that being said your next question might be, how does the view know about this component? The answer is it doesn't at least not until you define it which is what we'll do next in our `app.js` file. In order for our `story` component to be usable, we need to define it above the instantiation of our `Vue` instance. The definition looks like the following:

```javascript
Vue.component('story',{
    props: ['story'],
    template: `
        &lt;div&gt;
            &lt;h3&gt;&lt;a :href="story.url" target="_blank"&gt;{{story.title}}&lt;/a&gt;&lt;/h3&gt;
        &lt;/div&gt;
    `
});
```

Like our `Vue` instance, let's unpack what's happening here. The first parameter is a string with the name of our component. The `props` property is a list of the props or custom attributes that are accepted by our component. The `template` property is where we set up the template of what will be rendered in place of our component in the view. In our case, we'll have an `h3` element with a nested `a` element whose `href` attribute is the `url` property of our `story` object and the display text is the `title` property of our `story` object.

## Run Application

At this point, our application should be ready to run. You can either start the server with the following command `npm start`  and navigate to `http://localhost:3000` or open the `index.html` page in the browser of your choice. The result should look like the screenshot below:

![](/images/hn-vue-topstories-client/hnvueclient.png)

## Conclusion

In this writeup, I built a standalone Hacker News client that displays the top 50 stories using Vue while also highlighting some of its main features. Overall, I really enjoyed building this application. The setup process was extremely simple and after a few hours looking through the excellent documentation and working through some bugs, it took less than two hours to get this application up and running from start to finish. For prototyping and learning purposes, Vue is great because you're able to take advantage of the core features of the framework without having too much overhead. Although this may not be the most appropriate way to build production-ready applications, it's nice to know you can have a modern web application with minimal setup required. My next steps will be to continue learning some of the other features the framework provides and eventually build up to learning how to use the CLI tools.
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/hn-vue-topstories-client</link>
      <guid>https://www.luisquintanilla.me/posts/hn-vue-topstories-client</guid>
      <pubDate>2018-06-21 18:30:38</pubDate>
    </item>
    <item>
      <title>How do I keep up with AI?</title>
      <description>&lt;![CDATA[
This question comes up a lot in conversations. The short answer? I don’t. There’s just too much happening, too fast, for anyone to stay on top of everything.

While I enjoy sharing links and recommendations, I realized that a blog post might be more helpful. It gives folks a single place they can bookmark, share, and come back to on their own time, rather than having to dig through message threads where things inevitably get lost.

That said, here are some sources I use to try and stay informed:

- **Newsletters** are great for curated content. They highlight the top stories and help filter through the noise.
- **Blogs** are often the primary sources behind those newsletters. They go deeper and often cover a broader set of topics that might not make it into curated roundups.
- **Podcasts** serve a similar role. In some cases, they provide curation like newsletters and deep dives like blogs in others. Best of all, you can tune in while on the go making it a hands-free activity.

For your convenience, if any of the sources (including podcasts) I list below have RSS feeds, I’ve included them in my [AI Starter Pack](/feed/starter/ai), which you can [download and import into your favorite RSS reader](/feed/starter/ai/index.opml) (as long as it supports OPML file imports).

If you have some sources to share, [send me an e-mail](/contact). I'd love to keep adding to this list! If they have a feed I can subscribe to, even better.

## Newsletters

- [AlphaSignal](https://alphasignal.ai/)
- [The Batch](https://www.deeplearning.ai/the-batch/)
- [Latent Space](https://www.latent.space/)
- [TLDR AI](https://tldr.tech/ai)

## Blogs

I pride myself on being able to track down an RSS feed on just about any website, even if it’s buried or not immediately visible. Unfortunately, I haven't found a feed URL for either OpenAI or Anthropic which is annoying. 

**OpenAI and Anthropic, if you could do everyone a favor and drop a link, that would be great.** 

UPDATE: Thanks to [@m2vh@mastodontech.de](https://toot.lqdev.tech/@m2vh@mastodontech.de) for sharing the OpenAI news feed. 

I know I could use one of those web-page-to-RSS converters, but I'd much rather have an official link directly from the source.

- [OpenAI](https://openai.com/news/)
- [Anthropic](https://www.anthropic.com/news)
- [Google AI Blog](https://blog.google/technology/ai/)
- [AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/)
- [Microsoft Research](https://www.microsoft.com/en-us/research/blog/)
- [Google Research](https://research.google/blog/)
- [Simon Willison](https://simonwillison.net/)

## Podcasts

- [AI Daily Brief](https://www.podchaser.com/podcasts/the-ai-daily-brief-formerly-th-5260567)
- [TWIML AI Podcast](https://twimlai.com/)
- [Practical AI](https://changelog.com/practicalai)

## Subscribing to feeds

Now that I’ve got you here...

Let’s talk about the best way to access all these feeds. My preferred and recommended approach is using a feed reader.

When subscribing to content on the open web, feed readers are your secret weapon.

RSS might seem like it’s dead (it’s not—yet). In fact, it’s the reason you often hear the phrase, “Wherever you get your podcasts.” But RSS goes beyond podcasts. It’s widely supported by blogs, newsletters, and even social platforms like the Fediverse (Mastodon, PeerTube, etc.) and BlueSky. It’s also how I’m able to compile my [starter packs](/feed/starter). 

I've written more about RSS in [Rediscovering the RSS Protocol](/posts/rediscovering-rss-user-freedom), but the short version is this: when you build on open standards like RSS and OPML, you’re building on freedom. Freedom to use the tools that work best for you. Freedom to own your experience. And freedom to support a healthier, more independent web.
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/how-i-keep-up-with-ai</link>
      <guid>https://www.luisquintanilla.me/posts/how-i-keep-up-with-ai</guid>
      <pubDate>2025-05-27 19:44 -05:00</pubDate>
    </item>
    <item>
      <title>How to listen to internet radio using VLC</title>
      <description>&lt;![CDATA[
## Introduction

Music streaming services like Spotify generally do a good job of making recommendations based on your music tastes. By applying algorithms to your listening history, they're able to provide you with a listening experience tailored to you. When it comes to music discovery though, it can be difficult to find new music so you often end up listening to music that sounds very similar. Internet radio can help expose you to music you otherwise wouldn't normally listen to. Radio stations like those on [SomaFM](https://somafm.com/) provide a wide variety of high quality music channels. There's other online radio stations out there, many which provide a built-in audio player. As I've written in previous posts though ([How to watch Twitch streams using VLC](/posts/how-to-watch-twitch-using-vlc.html) &amp; [Podcast management with RSS &amp; VLC](/feed/rss-vlc-podcast-management.html)), VLC is a versatile tool. As a result, it's no surprise you can also use it to listen to online radio streams. This post is mostly a note to myself, but I encourage you to try it as well. 

## Prerequisites

- [VLC](https://www.videolan.org/index.html)

The commands below were run on Linux. Since VLC is cross-platform, the same commands should work on those platforms as well. 

## Listen to internet radio

The first thing you'll need is the URL of the stream you want to listen to. Usually it's provided on the station's website. You can also browse through [RadioBrowser](https://www.radio-browser.info/) to find a station and its stream URL.

Once you have the URL, in the terminal, enter the following command:

```bash
vlc &lt;STREAM-URL&gt;
```

Replace `&lt;STREAM-URL&gt;` with the address of your stream.

After a few seconds, VLC launches and your stream should start playing.

If you prefer to stream audio without VLC's visual interface, use the following command.

```bash
cvlc &lt;STREAM-URL&gt;
```

Note that without the UI you won't have access to any playback controls. 

Happy listening! 
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/how-to-listen-internet-radio-using-vlc</link>
      <guid>https://www.luisquintanilla.me/posts/how-to-listen-internet-radio-using-vlc</guid>
      <pubDate>12/28/2021 16:24</pubDate>
    </item>
    <item>
      <title>How to watch Twitch streams using VLC</title>
      <description>&lt;![CDATA[
## Introduction

Twitch has become a tool for various creators to connect with audiences. Originally, streams were centered around gaming but now there is a wide variety of creators that include musicians, software developers, and craftspeople. One of my favorites is [Brainfeeder](https://www.twitch.tv/brainfeeder). I myself try (and often fail) to build software projects on stream. If you're interested in hanging out and talking machine learning, .NET and all things tech, feel free to join me at [https://twitch.tv/lqdev1](https://twitch.tv/lqdev1).

Watching Twitch streams on the desktop via the browser can take a toll on compute resources. A more efficient solution to this problem is to use VLC. VLC is a free, open-source, cross-platform media player that supports various protocols. While it was possible to watch Twitch streams using VLC before, it often required additional software such as [Streamlink](https://github.com/streamlink/streamlink). More recent versions make this requirement redundant. In this post, I'll show how to use VLC to watch Twitch streams.

![Twitch iOS App on iPhoneX - Caspar Camille Rubin](https://user-images.githubusercontent.com/11130940/103722126-4883f780-4f9d-11eb-8cc8-304d1a249ef9.png)

## Prerequisites

- VLC (version **3.11.0 or higher**)

## Watch Twitch Stream

&gt; **Update 11/12/2021**  
&gt; It's been a while since I've gotten the original steps to work, so feel free to skip them. You can still watch Twitch streams on VLC using Streamlink. To use Streamlink:
&gt; 1. Download and install [Streamlink](https://streamlink.github.io/install.html)
&gt; 1. Open the terminal and run:
&gt;    
&gt;    ```
&gt;    streamlink https://twitch.tv/&lt;username&gt; best
&gt;    ````
&gt;
&gt; For more information, see the [Streamlink Command-Line Interface documentation](https://streamlink.github.io/cli.html)

1. Open VLC
2. From the menu, Select **Media &gt; Open Network Stream**
3. Paste the URL to the stream you want to watch in the text box (i.e. https://twitch.tv/lqdev1)

At this point, the stream should start playing.

### CLI

Alternatively on Unix (Mac / Linux) systems, you can use the CLI. To watch a stream, open a terminal and enter the following command:

```bash
vlc https://twitch.tv/&lt;username&gt;
```

Make sure to replace the username with the streamer you want to watch.

After a few seconds, VLC loads and starts playing the stream.

## Performance comparison

Below is a comparison of the same stream playing on the browser vs VLC. These test were performed on a Surface Laptop 1 running Windows 10. By no means should you take this as a definitive performance comparison as there are many variables that may affect performance. However, it's a nice way to visualize each of the methods.

### Browser

In the browser scenario, I only had a single tab playing the Twitch stream on Microsoft Edge (chromium).

![Browser task manager](https://user-images.githubusercontent.com/11130940/103723087-43c04300-4f9f-11eb-9969-df40541693bd.png)

### VLC

![VLC task manager](https://user-images.githubusercontent.com/11130940/103721933-e1fed980-4f9c-11eb-92f3-0147f2c09134.png)

As it can be seen from this image, there's about a 4x decrease in CPU resource consumption when using VLC.

## Conclusion

In this post I showed how you can use VLC to watch Twitch streams. One of the drawbacks is, you can't see the channels you follow and who's online. You also can't browse streams like you would on the Twitch web client. You also don't have access to chat, which is half the fun on many streams. However, if you're only interested in the content, VLC is an excellent and less resource-hungry way of watching Twitch streams. See you on Twitch!
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/how-to-watch-twitch-using-vlc</link>
      <guid>https://www.luisquintanilla.me/posts/how-to-watch-twitch-using-vlc</guid>
      <pubDate>2021-01-05 21:14:39</pubDate>
    </item>
    <item>
      <title>HP ProBook 430 G1 Ubuntu BIOS Upgrade</title>
      <description>&lt;![CDATA[
I recently got an HP ProBook 430 G1 which came preinstalled with Windows 10. I have several Windows 10 devices, but needed a Linux system while on the go. I had given my HP Chromebook a try using Crouton. This worked okay. However, I was limited by the specs of the Chromebook and wanted something with more power for development purposes. Therefore, I erased Windows 10 on the ProBook and installed Ubuntu. The PC worked well. My only problem with it was that I found my battery meter was inaccurate and would reach a percentage in the 60’s before suddenly dropping to zero and shutting down. This was not something I could have, especially during the middle of a work session where I might lose some/all of my work.

Immediately, I assumed it had something to do with drivers/BIOS being out of date. To my surprise, BIOS updates were only available for Windows. Fortunately, through some research I was able to find a solution that allowed me to update my BIOS and fix my battery problems.

## Prerequisites:

1. Make sure PC is connected to the charger
2. 4GB USB Drive

## Procedure:

1. Format USB drive. (FAT32 required)
2. Create folder inside USB drive called Hewlett-Packard\BIOS\New
3. Download latest BIOS .exe file from HP Support website
4. Unzip .exe file. While 7Zip is not required, I found this to work well.
5. Copy .bin file to Hewlett-Packard\BIOS\New folder
6. Copy entire BIOSUpdate folder to Hewlett-Packard folder.
7. Restart PC
8. Continuously press F10 to enter BIOS Setup
9. Select Update System BIOS menu option
10. Select Update BIOS Using Local Media option
11. Select Accept

At this point, the installation process will take effect and the PC will restart itself at which point the your BIOS should be up to date.
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/hp-probook-430g1-ubuntu-bios-upgrade</link>
      <guid>https://www.luisquintanilla.me/posts/hp-probook-430g1-ubuntu-bios-upgrade</guid>
      <pubDate>2017-12-09 12:57:12</pubDate>
    </item>
    <item>
      <title>Create an HTTP Trigger Azure Function using FSharp</title>
      <description>&lt;![CDATA[
## Introduction

Since the upgrade to the 2.0 version of the Azure Functions runtime, .NET Core has been natively supported by the platform. As a result some changes took effect. Most notably, in version 1.0, a template for an F# HttpTrigger function was available. The template was removed in 2.0. However, that does not mean Azure Functions does not support F#. Azure Functions can be built in F# using a .NET Standard Class Library. This writeup provides a detailed walk-through of how to build an Azure Function that processes HTTP requests using F#. The complete code sample can be found on [GitHub](https://github.com/lqdev/FsHttpTriggerSample).

## Prerequisites

This solution was built using a Windows PC but should work on Mac and Linux.

- [.NET SDK (2.x or 3.x)](https://dotnet.microsoft.com/download/dotnet-core)
- [Node.js](https://nodejs.org/en/download/)
- [Azure Functions Core Tools](https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local#windows-npm)

## Create Solution

Open the command prompt and create a new directory for your solution called "FsHttpTriggerSample".

```bash
mkdir FsHttpTriggerSample
```

Navigate into the new directory and create a solution using the .NET CLI.

```bash
cd FsHttpTriggerSample
dotnet new sln
```

## Create Azure Functions Project

Inside the *FsHttpTriggerSample* solution directory, use the .NET CLI to create a new F# .NET Standard Class Library project.

```bash
dotnet new classlib -o FsHttpTriggerSample -lang f#
```

Add the project to the solution

```bash
dotnet sln add FsHttpTriggerSample
```

## Install NuGet Packages

To use Azure Functions, install the [**Microsoft.Net.Sdk.Functions** NuGet package](https://www.nuget.org/packages/Microsoft.NET.Sdk.Functions).

Inside the *FsHttpTriggerSample* project directory, enter the following command.

```bash
dotnet add package Microsoft.Net.Sdk.Functions
```

## Create the Azure Function

### Prepare Files

Delete the default *Library.fs* file inside the *FsHttpTriggerSample* project directory.

```bash
del Library.fs
```

Create a new file called *GreetFunction.fs*.

```bash
type nul &gt; GreetFunction.fs
```

### Configure Files

Open the *FsHttpTriggerSample.fsproj* file and find the following snippet.

```xml
&lt;Compile Include="Library.fs" /&gt;
```

Replace the snippet with the content below.

```xml
&lt;Compile Include="GreetFunction.fs" /&gt;
```

### Configure Host

At a minimum, Azure Functions requires the runtime version to run. This information is provided by a file called *host.json*.

Create a new file called *host.json* inside the *FsHttpTriggerSample* project directory.

```bash
type nul &gt; host.json
```

Open the *host.json* file and add the following content

```json
{
    "version": "2.0"
}
```

### Implement Azure Function

Open the *GreetFunction.fs* file and add the namespace and module for it.

```fsharp
namespace FsHttpTriggerSample

module GreetFunction = 
```

Below the module definition, add the following `open` statements:

```fsharp
open Microsoft.AspNetCore.Mvc
open Microsoft.Azure.WebJobs
open Microsoft.AspNetCore.Http
open Newtonsoft.Json
open System.IO
open Microsoft.Extensions.Logging
```

Define a `User` type containing a single property called `Name`.

```fsharp
type User = {
    Name: string
}
```

The entrypoint of an Azure Function is the `Run` function. Create a function called `Run`.

```fsharp
[&lt;FunctionName("Greet")&gt;]
let Run ([&lt;HttpTrigger(Methods=[|"POST"|])&gt;] req:HttpRequest) (log:ILogger) = 
```

To register an Azure Function, use the `FunctionName` attribute. In this case, the name of the function is `Greet`. The `Run` function takes two parameters, an `HttpRequest` and an `ILogger`. Since the binding used by HTTP Trigger functions is `HttpTrigger`, the request object is annotated with the `HttpTrigger` attribute. Options such as the accepted methods can be provided through the `HttpTrigger` attribute. In this case, only `POST` requests are accepted.

Create an `async` computation expression inside the `Run` function.

```fsharp
async {

}
```

Inside the `async` expression, add logging to indicate that the function has initialized. 

```fsharp
"Running function"
|&gt; log.LogInformation
```

Below that, get the body of the request.

```fsharp
let! body = 
    new StreamReader(req.Body) 
    |&gt; (fun stream -&gt; stream.ReadToEndAsync()) 
    |&gt; Async.AwaitTask
```

Then, deserialized the body into an instance of `User`.

```fsharp
let user = JsonConvert.DeserializeObject&lt;User&gt;(body)
```

Return a personalized greeting with the user's name.

```fsharp
return OkObjectResult(sprintf "Hello %s" user.Name)
```

Finally, use the `StartAsTask` function to start the `async` expression as a `Task`.

```fsharp
|&gt; Async.StartAsTask
```

Once finished, the contents of the *GreetFunction.fs* should look similar to the following.

```fsharp
namespace FsHttpTriggerSample

module GreetFunction = 

    open Microsoft.AspNetCore.Mvc
    open Microsoft.Azure.WebJobs
    open Microsoft.AspNetCore.Http
    open Newtonsoft.Json
    open System.IO
    open Microsoft.Extensions.Logging

    type User = {
        Name: string
    }

    [&lt;FunctionName("Greet")&gt;]
    let Run ([&lt;HttpTrigger(Methods=[|"POST"|])&gt;] req:HttpRequest) (log:ILogger) = 
        async {
            "Runnning Function"
            |&gt; log.LogInformation

            let! body = 
                new StreamReader(req.Body) 
                |&gt; (fun stream -&gt; stream.ReadToEndAsync()) 
                |&gt; Async.AwaitTask

            let user = JsonConvert.DeserializeObject&lt;User&gt;(body)

            return OkObjectResult(sprintf "Hello %s" user.Name)
        } |&gt; Async.StartAsTask
```

## Run the Function Locally

Build the project by using the `build` command inside the *FsHttpTriggerSample* project directory.

```bash
dotnet build
```

Then, navigate to the output directory

```bash
cd bin\Debug\netstandard2.0
```

Use the Azure Functions Core Tools to start the Azure Functions host locally.

```bash
func host start
```

Once the host is initialized, the function is available at the following endpoint `http://localhost:7071/api/Greet`.

## Test the function

Using a REST client like Postman or Insomnia, make a POST request to `http://localhost:7071/api/Greet` with the following body. Feel free to replace the name with your own.

```json
{
    "Name": "Luis"
}
```

If successful, the response should look similar to the following output.

```text
Hello Luis
```

## Conclusion

This writeup showed how to create an HTTP Trigger Azure Function using F#. Creating additional functions inside the same project is relatively trivial since the structure of the *GreetFunction.fs* file can be copied and the logic inside the `Run` function can be adapted to meet your requirements.

## Resources

- [Azure Functions F# Developer Reference](https://docs.microsoft.com/en-us/azure/azure-functions/functions-reference-fsharp)
- [Azure Functions Host 2.x Reference](https://docs.microsoft.com/en-us/azure/azure-functions/functions-host-json)
- [Azure Functions Zip Deployment](https://docs.microsoft.com/en-us/azure/azure-functions/deployment-zip-push)
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/http-trigger-azure-functions-fsharp</link>
      <guid>https://www.luisquintanilla.me/posts/http-trigger-azure-functions-fsharp</guid>
      <pubDate>2019-11-16 17:33:59</pubDate>
    </item>
    <item>
      <title>Train an image classifier using F# and ML .NET</title>
      <description>&lt;![CDATA[
# Introduction

This post is part of F# Advent 2020. Thank you to [Sergey Tihon](https://twitter.com/sergey_tihon) for organizing this and the rest of the contributors for producing high-quality content. Make sure to check out the rest of the [F# Advent 2020 content](https://sergeytihon.com/2020/10/22/f-advent-calendar-in-english-2020/).

When picking who's on the naughty or nice list, I often wonder how Santa decides. I took a shot at answering this question by training an image classifier using the ML.NET image classification API and images of Disney heroes and villains to tell whether they're naughty or nice. You shouldn't judge someone by the way they look (even if they are the Evil Queen), so it's safe to say, don't try this at home or with your neighbors 😉. This sample is just for demo purposes. You can find the full code on [GitHub](https://gist.github.com/lqdev/0c4dc9eea93b7b8541f31ddd429afb53).

## Prerequisites

This sample was built on a Windows 10 PC, but should work on Mac / Linux PCs

- [.NET 5 SDK](https://dotnet.microsoft.com/download)

## The data

The dataset contains images of Disney characters, both real and animated. They were obtained from the [Disney Fandom Wiki](https://disney.fandom.com/wiki/The_Disney_Wiki). The characters are split into two categories, [villains](https://disney.fandom.com/wiki/Category:Villains) and [heroes](https://disney.fandom.com/wiki/Category:Protagonists). For the purpose of this sample, we'll label heroes as nice and villains as naughty. The dataset used to train this model contains 2400 villain (naughty) and 675 hero (nice) images stored in top-level directories with the naughty/nice names. This means that the dataset is unbalanced and may skew predictions as it can be seen when making predictions.

## Install  NuGet packages

Use the `#r` convention to install the necessary NuGet packages used in this sample.

```fsharp
#r "nuget:Microsoft.ML"
#r "nuget:Microsoft.ML.Vision"
#r "nuget:Microsoft.ML.ImageAnalytics"
#r "nuget:SciSharp.TensorFlow.Redist" 
```

Then, import the packages.

```fsharp
open System
open System.IO
open Microsoft.ML
open Microsoft.ML.Data
open Microsoft.ML.Vision
```

## Define data types

Start off by defining the data types containing your input and output schema. You can do this by creating two records, `ImageData` and `ImagePrediction`. `ImageData` is the input which contains the path to image file and the category it belongs to and the `ImagePrediction` contains the prediction generated by the model.

```fsharp
[&lt;CLIMutable&gt;]
type ImageData = {
    ImagePath: string
    Label: string
}

[&lt;CLIMutable&gt;]
type ImagePrediction = {
    PredictedLabel: string
}
```

## Training

The training process loads a set of training images, preprocesses them, and uses the ML.NET image classification API to train an image classification model. 

### Initialize MLContext

Once you've defined the data type, initialize the `MLContext`. `MLContext` is the entrypoint for ML.NET applications.

```fsharp
let ctx = new MLContext()
```

### Load training data

Then, load the data using the helper function `loadImagesFromDirectory` and point it to the top-level directory containing the subdirectories of images in the nice and naughty categories.

```fsharp
let imageData = loadImagesFromDirectory "C:/Datasets/fsadvent2020/Train" true
```

The `loadImagesFromDirectory` function looks like the following:

```fsharp
let loadImagesFromDirectory (path:string) (useDirectoryAsLabel:bool) = 

    let files = Directory.GetFiles(path, "*",searchOption=SearchOption.AllDirectories)

    files
    |&gt; Array.filter(fun file -&gt; 
        (Path.GetExtension(file) = ".jpg") ||
        (Path.GetExtension(file) = ".png"))
    |&gt; Array.map(fun file -&gt; 
        let mutable label = Path.GetFileName(file)
        if useDirectoryAsLabel then
            label &lt;-  Directory.GetParent(file).Name
        else
            let mutable brk = false
            for index in 0..label.Length do
                while not brk do
                    if not (label.[index] |&gt; Char.IsLetter) then
                        label &lt;- label.Substring(0,index)
                        brk &lt;- true

        {ImagePath=file; Label=label}
    )
```

Then, create an `IDataView` for the training images. An `IDataView` is the way data is represented in ML.NET.

```fsharp
let imageIdv = ctx.Data.LoadFromEnumerable&lt;ImageData&gt;(imageData)
```

### Define training pipeline

Once your data is is loaded into an `IDataView`, set the classifier options by using `ImageClassificationTrainer.Options`. Use it to define the name of the network architecture, input and output columns, and some additional parameters. The network architecture used in this case is [`ResNet V2`](https://www.paperswithcode.com/method/inception-resnet-v2).

```fsharp
let classifierOptions = ImageClassificationTrainer.Options()
classifierOptions.FeatureColumnName &lt;- "Image" 
classifierOptions.LabelColumnName &lt;- "LabelKey" 
classifierOptions.TestOnTrainSet &lt;- true  
classifierOptions.Arch &lt;- ImageClassificationTrainer.Architecture.ResnetV2101
classifierOptions.MetricsCallback &lt;- Action&lt;ImageClassificationTrainer.ImageClassificationMetrics&gt;(fun x -&gt; printfn "%s" (x.ToString()))
```

Define the preprocessing steps, image classification trainer (along with the previously defined options) and postprocessing steps.

```fsharp
let pipeline = 
    EstimatorChain()
        .Append(ctx.Transforms.LoadRawImageBytes("Image",null,"ImagePath"))
        .Append(ctx.Transforms.Conversion.MapValueToKey("LabelKey","Label"))
        .Append(ctx.MulticlassClassification.Trainers.ImageClassification(classifierOptions))
        .Append(ctx.Transforms.Conversion.MapKeyToValue("PredictedLabel"))
```

The ML.NET image classification API leverages a technique known as transfer learning. Transfer learning uses pretrained models (usually neural networks) and retrains the last few layers using new data. This significantly cuts down the amount of time, resources, and data you need to train deep learning models. ML .NET is able to do this with the help of [TensorFlow .NET](https://github.com/SciSharp/TensorFlow.NET), a set of .NET bindings for the TensorFlow deep learning framework. Although transfer learning usually makes the process of training a deep learning models less resource intensive, the TensorFlow API is usually low level and still requires a significant amount of code. See this [transfer learning example](https://github.com/SciSharp/SciSharp-Stack-Examples/blob/master/src/TensorFlowNET.Examples/ImageProcessing/TransferLearningWithInceptionV3.cs) from TensorFlow.NET to see how you'd do it in TensorFlow.NET. Although the low-level nature of the TensorFlow API gives you control over what you're building, many times you don't need that level of control. ML.NET through the image classification trainer greatly simplifies this process by providing a high-level API for achieving the same task.

To train the model, use the `Fit` method on the training image `IDataView`.

```fsharp
let model = pipeline.Fit(imageIdv)
```

Throughout the training process, you should see output similar to the following:

```console
Phase: Bottleneck Computation, Dataset used:      Train, Image Index: 279
Phase: Bottleneck Computation, Dataset used:      Train, Image Index: 280
Phase: Bottleneck Computation, Dataset used: Validation, Image Index:   1
```

With the model trained, it's time to use it to make predictions. Optionally, you can save it for use in other applications.

```fsharp
ctx.Model.Save(model,imageIdv.Schema,"fsadvent2020-model.zip")
```

## Make predictions

Load the test images and create an `IDataView` for them. The test images used are of Jack Skellington and The Grinch.

```fsharp
let testImages = 
    Directory.GetFiles("C:/Datasets/fsadvent2020/Test")
    |&gt; Array.map(fun file -&gt; {ImagePath=file; Label=""})

let testImageIdv = ctx.Data.LoadFromEnumerable&lt;ImageData&gt;(testImages)
```

![The grinch](https://www.khwiki.com/images/thumb/f/ff/Jack_Skellington_KHII.png/180px-Jack_Skellington_KHII.png)

Then, use the model to make predictions.

```fsharp
let predictionIdv = model.Transform(testImageIdv)
```

One of the easiest ways to access the predictions is to create an `IEnumerable`. To do so, use the `CreateEnumerable` method.

```fsharp
let predictions = ctx.Data.CreateEnumerable&lt;ImagePrediction&gt;(predictionIdv,false)
```

Then, use the built-in F# sequence operations to display the predictions 

```fsharp
predictions |&gt; Seq.iter(fun pred -&gt; 
    printfn "%s is %s" (Path.GetFileNameWithoutExtension(pred.ImagePath)) pred.PredictedLabel)
```

The output should look like the following:

```console
grinch is Naughty
jack is Naughty
```

## Conclusion

In this post, I showed how you can use the ML.NET and TensorFlow.NET to train an image classification model to classify Disney characters as naughty or nice. Depending on the level of control you need, you might choose to use TensorFlow.NET or if you want a high-level API for training an image classifier, you can use the ML.NET. Most importantly, we figured out that Jack Skellington and The Grinch are naughty, so I guess no gifts for them this year? Happy coding!

### Call to Action

Originally, I had planned on writing this article using [TensorFlow.Keras](https://github.com/SciSharp/TensorFlow.NET), which is part of the SciSharp TensorFlow.NET project. TensorFlow.Keras provides .NET bindings for Keras. Keras provides a high-level API for TensorFlow which makes the process of building custom neural networks much simpler than working with the TensorFlow API. Unfortunately, while trying to adapt my scenario to an [existing sample](https://github.com/SciSharp/SciSharp-Stack-Examples/blob/master/src/TensorFlowNET.Examples/ImageProcessing/ImageClassificationKeras.cs), I ran into an [issue](https://github.com/SciSharp/SciSharp-Stack-Examples/issues/23).  This is not something I would have been able to resolve in time to publish this post, so I defaulted to using ML.NET.

I'm a big fan of the work being done by the SciSharp community and the machine learning and data science capabilities it brings to the .NET ecosystem. The work and efforts are all community driven, and as such, there's plenty of opportunities to contribute. Here are just some examples of ways to contribute, especially from an F# perspective. From my end, I plan on eventually converting this sample to use TensorFlow.Keras. See you in the SciSharp repos! 🙂

![FsLab SciSharp contribute](https://user-images.githubusercontent.com/11130940/102030239-f4ac3700-3d7f-11eb-9898-f18990a56326.png)
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/image-classification-mlnet-fsadvent2020</link>
      <guid>https://www.luisquintanilla.me/posts/image-classification-mlnet-fsadvent2020</guid>
      <pubDate>2020-12-14 20:03:18</pubDate>
    </item>
    <item>
      <title>IndieWeb Create Day - July 2025</title>
      <description>&lt;![CDATA[
Since it was a holiday weekend in the U.S. that kind of snuck up on me, I found myself with nothing planned on a Saturday. So I chose to spend it creating stuff for my website with the IndieWeb community during [IndieWeb Create Day](https://events.indieweb.org/2025/07/indieweb-create-day-3q2PTCbGioi9).

Over the last few months I've been overthinking my website redesign and while I've made several attempts at it, I've never been satisfied the outcome. I end up throwing away all the progress I've made and go back to the drawing board. 

Yesterday, I decided to not let perfect be the enemy of good and the approach I took was creating just a simpler piece of functionality outside of my website. How I integrate it into my website is a future me problem. But I want to work from a place of creativity and complete freedom to think of what could be rather than what is. 

With that in mind, I set out to sketch out how I want to create and render media (image, audio, video) posts. The approach I took used a combination of front-matter YAML and custom markdown media extensions. The front-matter YAML is something that I already use for my website and it's something that I want to continue using. However, in contrast to my current website, I like that the front-matter was kept simple and only includes a basic amount of information. The actual post content was handled by my custom markdown extension which leveraged YAML-like syntax to define media content. What's great about this is that it is composable so once I got one type of media working, the rest for the most part "just worked". I could even mix different media types within the same post with no additional work or code changes required. Once I had the skeleton, it was all about refactoring, documentation, adding finishing touches, and vibe-coding some CSS which Claude did a relatively good job with given the aesthetic I was going for. 

Overall, I'm happy with the end result. 

![A screenshot of a website post containing an image and audio player](/images/feed/indieweb-create-day-2025-07.png)

For more details, you can check out the [repo](https://github.com/lqdev/indieweb-create-day-2025-07).

At some point, I want to be able to integrate these media posts into my static site generator but for the time being, there are other kinds of posts such as reviews, RSVPs, and other [post types](https://indieweb.org/posts#Types_of_Posts) that I want to design and eventually also support on my website. I liked the approach I took this time around because it gave me the freedom to explore posibilities rather than constrain my creativity to what I've already built. So I think I'll keep doing the same for subsequent post types. 

At the end of the day, it was nice seeing everyone else's projects. My favorite one was Cy's recipe website. I want to be like them when I grow up 🙂.]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/indieweb-create-day-2025-07</link>
      <guid>https://www.luisquintanilla.me/posts/indieweb-create-day-2025-07</guid>
      <pubDate>2025-07-06 20:09</pubDate>
    </item>
    <item>
      <title>Inspect ML.NET models with Netron</title>
      <description>&lt;![CDATA[
## Introduction

Once you've trained a machine learning model, you typically serialize it and save it to a file. This serialized file contains information such as the model inputs and output schema (names, data types), the transformations and algorithms used by the model,  weights / coefficients, hyperparameters, and all other sorts of information about the model. The model file is then embedded in an end-user application such as a web API which deserializes the contents of the file and uses the model to make predictions on new data.

How the model is serialized depends on the framework you use to train your model. To standardize model serialization and interoperability, you can use something like Open Neural Network Exchange (ONNX) to represent your models if supported by the framework you're using. That's beyond the scope of this post though.

Regardless of which framework or serialization format is used, if you were to open up the model file, the contents of the model file are often not human readable or difficult to interpret. When the person who trains the model is also putting it into production, they know the format input data needs to be in to make predictions. That's not often the case though. As a result, the people putting models into production need the ability to inspect a model to get a sense of how they need to collect and preprocess input data before making predictions. That's where Netron comes in. [Netron](https://github.com/lutzroeder/Netron) is a "visualizer for neural network, deep learning, and machine learning models". In this post, I'll show how you can use Netron to inspect ML.NET models and use that information to define the model input and output schemas.

## Inspecting an ML.NET model

ML.NET models are typically serialized and saved to files with the *.zip* file extension. Using the *.zip* file extension is standard convention. However, the extension can be whatever makes the most sense to you.

A common question is, what's in the *zip* file? The easy but vague answer to that question is, a serialized version of the model. Using Netron, you can go deeper and see exactly what is inside the *zip* file. 

In this post, I'm using a pretrained ML.NET model that classifies sentiment. The model can be found in the [dotnet/samples](https://github.com/dotnet/samples/blob/main/machine-learning/models/sentimentanalysis/sentiment_model.zip) repo. The same concept applies to any other ML.NET model.

To inspect the ML.NET model using Netron:

1. [Download the model](https://github.com/dotnet/samples/raw/main/machine-learning/models/sentimentanalysis/sentiment_model.zip)
2. Navigate to [https://netron.app](https://netron.app/). Alternatively, if you'd prefer to use Netron offline, you can also [download the latest version of Netron](https://github.com/lutzroeder/netron/releases) for your operating system (Windows, Mac, or Linux). In this post, I use the web app.
3. Select **Open Model...** and use the file browser to select your ML.NET model. In this case, our model is *sentiment_model.zip*.
4. After a few seconds, a graph describing you model appears. How long it takes for your model depends on its size. The larger your model, the longer it takes to load. The nodes in the graph represent the model inputs, transformations, algorithm, and outputs.
5. Usually the top nodes represent the model inputs and the last node represents the algorithm or trainer. Click on any of the top nodes to display more information about the inputs.

    ![Sentiment Classification ML.NET model in netron](https://user-images.githubusercontent.com/11130940/130704589-61ebb612-d65f-4364-b275-bd0d4991d3cf.png)

    For this model, we see that there are 3 input properties or columns:

    - SentimentText (string)
    - Label (boolean)
    - SamplingKeyColumn (float32)

    Using this information, we can represent the model inputs as a Plain-Old-CLR-Object (POCO) in our end-user application.

    ```csharp
    public class ModelInput
    {
        public string SentimentText {get;set;}
        public bool Label {get;set;}
        public float SamplingKeyColumn {get;set;} 
    }
    ```

    ![ML.NET Netron Binary Predictor](https://user-images.githubusercontent.com/11130940/130705880-0baea2f7-7b45-408a-b60c-16acceb54079.png)

    Looking at the last node `BinaryPredXfer`, we see that the algorithm used is for binary classification or predictions. Looking at the [ML.NET tasks documentation](https://docs.microsoft.com/dotnet/machine-learning/resources/tasks#binary-classification-inputs-and-outputs), we expect to get at least two columns in the prediction output:

    - Score (Single)
    - PredictedLabel (boolean)

    Like the input, we can also represent model outputs or predictions as follows:

    ```csharp
    public class ModelOutput
    {
        public float Score {get;set;}
        public bool PredictedLabel {get;set;}
    }
    ```

    Keep in mind that the name of the class can be anything so long as the properties or column names and types match with those expected by the model.

Once you have your model inputs and outputs defined in your end-user application, you can follow the standard process of [loading your model and using it to make predictions](https://docs.microsoft.com/dotnet/machine-learning/how-to-guides/machine-learning-model-predictions-ml-net).

## Conclusion

Inspecting ML.NET models can be difficult since their serialized version is not human readable. When making predictions with ML.NET models but you're not familiar with what the input and output data should look like, use Netron to inspect the model. Then, use the information about the input data names and types, machine learning task, and algorithm to define model input and output schema classes in your end-user application. Once you've defined your model input and output, you can use the model to make predictions on new data.
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/inspect-mlnet-models-netron</link>
      <guid>https://www.luisquintanilla.me/posts/inspect-mlnet-models-netron</guid>
      <pubDate>2021-08-25 18:00:00</pubDate>
    </item>
    <item>
      <title>Configure Ollama on Dev Containers and VS Code</title>
      <description>&lt;![CDATA[
The source for my website is [hosted on GitHub](/github/luisquintanilla.me). As a result, I use VS Code as the text editor to author my posts. Typically, I use [Codespaces or github.dev](/colophon) to quickly draft and publish articles to simplify the process. 

As part of my authoring and publishing posts, there's a few things I do like:

1. Coming up with a title
2. Creating relevant tags to help with discovery and connecting related information on my website.
3. For long-form blog posts, I also include a description. 

This is generally relatively easy to do. However, sometimes I spend too much time coming up with something that truly summarizes and condenses the information in the post. It'd be nice if I had an assistant to help me brainstorm and workshop some of these items.

In comes AI. Now, I could use something like [Copilot](https://code.visualstudio.com/docs/copilot/overview) which would work wonderfully and easily plug into my workflow. However, my website is a labor of love and I don't make any money from it. In many instances, I've designed [various components to be as low-cost as possible](/posts/receive-webmentions-fsharp-az-functions-fsadvent). 

Recently, I created a post which showed [how to get started with Ollama on Windows](/posts/getting-started-ollama-windows). In this post, I'll show how you can do the same for your Dev Container environments. 

## Install Ollama

Assuming you already have a [Dev Container configuration file](https://code.visualstudio.com/docs/devcontainers/create-dev-container), add the following line to it:

```json
"postCreateCommand": "curl -fsSL https://ollama.com/install.sh | sh"
```

This command triggers when the container environment is created. This will install Ollama in your development environment. For more details, see the [Ollama download instructions](https://ollama.com/download/linux).

## Start Ollama

Now that Ollama is installed, it's time to start the service so you can get models and use them. To start the Ollama service on your Dev Conatiner environment, add the following line to your Dev Container configuration file.

```json
"postStartCommand": "ollama serve"
```

This command will run `ollama serve` when the Dev Container environment starts.

## Pull a model

To use Ollama, you're going to need a model. In my case, I went with [Phi-2](https://ollama.com/library/phi) because it's lightweight and space on Codespaces is limited.  

To get the model:

1. Open the terminal
1. Enter the following command:

    ```bash
    ollama pull phi
    ```

    After a few minutes, the model is downloaded and ready to use. 

1. Enter the following command to ensure that your model is now available

    ```bash
    ollama list
    ```

    The result should look like the following:

    ```text
    NAME            ID              SIZE    MODIFIED      
    phi:latest      e2fd6321a5fe    1.6 GB  6 seconds ago
    ```

For more details, see the [Ollama model library](https://ollama.com/library). 

## Conclusion

In this post, I showed how you can configure Ollama with Dev Containers to use AI models locally in your projects. In subsequent posts, I'll show how once the service is running, I use it as part of my authoring and publishing posts. You can see a preview of it in my [scripts directory](https://github.com/lqdev/luisquintanilla.me/blob/main/Scripts/ai.fsx). Happy coding! ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/install-ollama-vscode-devcontainer</link>
      <guid>https://www.luisquintanilla.me/posts/install-ollama-vscode-devcontainer</guid>
      <pubDate>2024-03-06 13:50</pubDate>
    </item>
    <item>
      <title>Install Manjaro RTL8821CE WiFi drivers</title>
      <description>&lt;![CDATA[
Until recently, I was usign an HP ProBook 430 G1 with Manjaro and a Surface Go (1st Gen) for my computing needs. To cut down on the devices I used, I purchased an [ASUS L210MA](https://www.asus.com/us/laptops/for-home/everyday-use/asus-l210/) which is about the size of the Surface Go but I can run Manjaro on it. Although it doesn't have the same storage or compute power as the ProBook, I've set up a VM in Azure I can use for those instances where I need more compute power than the ASUS laptop can provide. As is sometimes the case, when you install Linux on a new PC, you have to install the right drivers to use your WiFi and other peripheral devices. In this guide I'll show how to install the RTL-8821-CE WiFi drivers.

## Prerequisites

The instructions on this guide were run on a Manjaro XCFE 21.2.6.

## Enable AUR and Install yay

The WiFi drivers you'll need are hosted in the [Arch User Repository (AUR)](https://aur.archlinux.org/). Before installing any packages, you'll have to enable it on your PC. To do so, install the `yay` AUR helper package by running the following command in the terminal:

```bash
sudo pacman -S git
git clone https://aur.archlinux.org/yay.git
cd yay
makepkg -si
```

## Install Kernel Headers

The first thing yuo'll need to install are your kernel headers. To check which version of the kernel you're using, run the following command in the terminal:

```bash
uname -r
```

The output should look similar to the following:

```text
5.15.41-1-MANJARO
```

The first two values are your major kernel version. For example, in the output above the kernel version is 5.15.

To install the kernel headers, run the following command in the terminal:

```bash
sudo pacman -S linux-headers
```

Make sure that the kernel version that's installed is the same as the one output after running the `uname -r` command. 

## Install RTL8821CE drivers

Once you've installed the headers, it's time to install the WiFi drivers. Use `yay` to run the following command in the terminal:

```bash
yay -S rtl8821ce-dkms-git
```

After some time, the packages should be installed on your device. Reboot your PC and you should now be able to connect to the internet wirelessly]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/install-rtl8821ce-manjaro</link>
      <guid>https://www.luisquintanilla.me/posts/install-rtl8821ce-manjaro</guid>
      <pubDate>06/11/2022 15:48</pubDate>
    </item>
    <item>
      <title>Building a Reading Habit with Audiobooks, LibriVox, and AntennaPod</title>
      <description>&lt;![CDATA[
Last year, I was able to complete my goal of reading 12 books or roughly one book per month. I kept track of some of the books I read in my [library page](https://www.lqdev.me/library).

I enjoyed most of the books I read. However, despite hitting my goal and averaging a book a month, I didn't build up a habit which is what I would've preferred. Many of the books I read last year, I read them during long periods of uninterrupted time, such as cross-country flights. The topic of goal setting and measuring the right thing could be an entire post, but I'll leave that for another time. 

Taking the learnings from last year, this year, I'll try to focus on building a reading habit rather than reading a set number of books. While I'd still like to finish at least one book per month, that's a metric I'll use to track general progress rather than making it the goal. Unless I'm reading something like War and Peace, I'd expect that by establishing a consistent cadence, I should be able to get through at least one book every month. Also, I'm only accounting for uninterrupted reading time blocks. Using audiobooks, I can continue building my reading habit while working out or doing chores. Kind of like having TV playing in the background. 

There is no shortage of audiobooks out there. Audible is one of the easiest ways to access audiobooks and they have a large selection to choose from. Personally, I don't use Audible. I don't like paying for a separate subscription, getting credits to "buy" books that I don't really own and can only listen through the Audible app. Instead, I use Spotify. I get [15 hours of access to audiobooks per month](https://support.spotify.com/us/article/audiobooks-access-plan/) as part of my subscription. While I still don't own the books and can only listen on the Spotify app, at least I'm not paying a separate subscription. Also, the 15 hour limit at this time is more than enough for me. 

The library is another amazing option. No subscription fees and you get access to your local library's entire digital catalog. Using the [Libby app](https://www.overdrive.com/apps/libby), you can borrow books and start listening for free. Selection though may be an issue here since you are limited to what's available through your local library. Pro-tip, universities and employers sometimes have libraries, so as an alumnus or employee, you can expand the number of books available to you by registering multiple libraries on the Libby app. Regardless of which library you choose though, you still have to go through the Libby app. 

The dream setup for me is, having direct access to the audiobook files so I can listen to them whenever and wherever I'd like. Similar to podcasts. Thanks to LibriVox and AntennaPod, that setup is possible. 

In case you're interested in setting up something similar, in this post, I'll briefly explain what LibriVox and AntennaPod are and show how you can use them to start listening to free audiobooks on your phone. 

## What is LibriVox?

[LibriVox](https://librivox.org/) is a website that provides a collection of free audiobooks from the public domain. Thanks to a vibrant community of volunteers, you can enjoy classics such as:

- [The Divine Comedy](https://librivox.org/the-divine-comedy-by-dante-alighieri/)
- [The Adventures of Sherlock Holmes](https://librivox.org/the-adventures-of-sherlock-holmes/)
- [Discourses of Epictetus](https://librivox.org/discourses-of-epictetus-by-epictetus/)
- [Roughing It](https://librivox.org/roughing-it/)
- [And many others...](https://librivox.org/search?primary_key=0&amp;search_category=title&amp;search_page=1&amp;search_form=get_results&amp;search_order=catalog_date)

It's important to note that the catalog is limited to public domain works. That means you won't have access to the latest self-help, business, or current events books. However, there are still thousands of books to keep you busy for an entire lifetime and the list is constantly growining since there are new works introduced into the public domain every year. 

Here's [a few notable literary works that entered the public domain in 2025](https://blog.archive.org/2025/01/01/welcome-to-the-public-domain-in-2025/) as well as the complete list of [literary works from 1929](https://archive.org/details/internetarchivebooks?tab=collection&amp;query=date%3A1929).

## What is Antennapod?

[AntennaPod](https://antennapod.org/) is an open-source podcast player for Android. 

The app has a ton of great features, such as [Podcast Index](https://podcastindex.org/) support, subscribe via RSS directly,  OPML import and export, chapters, adjustable playback speed, and for the past few years, a personalized yearly summary which is computed locally on your device [(here's my summary from 2024)](/feed/antennapod-echo-2024).

You can get AntennaPod from the Play Store or [F-Droid](https://f-droid.org/en/packages/de.danoeh.antennapod/)

## Keep track of new releases

LibriVox provides [several feeds](https://librivox.org/pages/librivox-feeds/). 

Since podcast distribution is primarly done through RSS and you can subscribe to podcasts directly using RSS feeds in AntennaPod, we can use the app to stay up to date with new audiobook releases. 

To subscribe to the LibriVox New Release feed:

1. Copy the LibriVox New Releases RSS feed URL to your clipboard: [https://librivox.org/rss/latest_releases](https://librivox.org/rss/latest_releases)
1. In AntennaPod, open up the hamburger menu and select **Add Podcast**.
1. In the *Add Podcast* dialog, select **Add podcast by RSS address**.

    ![AntennaPod Add Podcast Dialog](/images/listen-audiobooks-librivox-antennapod/add-podcast-antennapod.png)

1. Paste the LibriVox New Releases RSS feed URL into the text box and select **Confirm**. 

At this point, you should see the feed listed in your AntennaPod subscriptions. 

By subscribing the LibriVox New Release feed, AntennaPod treats the feed as a podcast, and each new release entry or audiobook is like a podcast episode. 

For more information on subscribing to podcasts, see the [AntennaPod documentation](https://antennapod.org/documentation/getting-started/subscribe). 

## Add books to Antennapod

Subscribing to the new releases feed only notifies you when new books are added, you can't listen to or download the books. To do that, you'll need to subscribe to the RSS feed of the book you want to listen to in AntennaPod.

### From New Releases Feed

1. In AntennaPod's Subscription screen, select **LibriVox's New Releases**.
1. From the list of new releases, select the book you want to listen to. For example, "Room of One's Own" by Virginia Woolf.

    ![AntennaPod New Releases Feed](/images/listen-audiobooks-librivox-antennapod/antennapod-librivox-new-releases.png)

1. In the audiobook's screen, select **Visit Website**. This will take you to the LibriVox audiobook's page in the browser.

    ![AntennaPod Audiobook Screen](/images/listen-audiobooks-librivox-antennapod/antennapod-room-of-ones-own.png)

1. In the LibriVox audiobook page, select the book's RSS Feed.

    ![A Room of One's Own by Virginia Woolf LibriVox book page](/images/listen-audiobooks-librivox-antennapod/librivox-room-ones-own.png)

1. Copy the book's feed URL to your clipboard.
1. In AntennaPod, open up the hamburger menu and select **Add Podcast**.
1. In the *Add Podcast* dialog, select **Add podcast by RSS address**.
1. Paste the book's RSS feed URL into the text box and select **Confirm**. 

### Any other books

The process works the same for other books but you'll have to use LibriVox as the entrypoint to browse books.

1. In your browser, navigate to LibriVox. 
1. Go to the audiobook page. For example, this is the page for [Walden](https://librivox.org/walden-by-henry-david-thoreau).
1. In the audiobook page, select the book's RSS Feed. For Walden, it's [https://librivox.org/rss/549](https://librivox.org/rss/549).
1. Copy the book's feed URL to your clipboard.
1. In AntennaPod, open up the hamburger menu and select **Add Podcast**.
1. In the *Add Podcast* dialog, select **Add podcast by RSS address**.
1. Paste the book's RSS feed URL into the text box and select **Confirm**. 

## Conclusion 

Although AntennaPod wasn't designed for audiobook listening, thanks to the flexibility of RSS feeds and the thankless work of the LibriVox community, you can easily listen to audiobooks for free on your mobile device. I plan on testing out this setup and seeing whether it helps keep my reading habit on track.]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/listen-audiobooks-librivox-antennapod</link>
      <guid>https://www.luisquintanilla.me/posts/listen-audiobooks-librivox-antennapod</guid>
      <pubDate>2025-01-05 21:04 -05:00</pubDate>
    </item>
    <item>
      <title>Llama's Turn On: Tuning In to AI's Quest for Higher Consciousness in MOOs</title>
      <description>&lt;![CDATA[
Last week, I learned what a [MOO](https://en.wikipedia.org/wiki/MOO) was. 

Although they were before my time, I decided to use Llama 3.3 in [DuckDuckGo's AI Chat](https://duck.ai/) to play around. At first it was very generic but eventually it started going down some fun and interesting paths.

Here's the transcript with some updates for formatting. In some turns, I took creative liberties but mostly stuck to the script. 

&gt; **User**  
&gt; Text adventure game (MOO)    
&gt; **Assistant**  
&gt; You find yourself standing in a dimly lit room, the walls made of cold, grey stone. The air is musty and filled with the scent of old dust. A faint glow emanates from a torch on the wall, casting flickering shadows around the room.  
&gt; &lt;br&gt;
&gt; You see a door to the north, slightly ajar, and a passageway to the east, which appears to lead deeper into the darkness. There is also a small table in the center of the room, with a piece of parchment and a quill lying on it.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - go north
&gt; - go east
&gt; - examine table
&gt; - take parchment
&gt; - open door  
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; &lt;br&gt;
&gt; **User**  
&gt; Go east  
&gt; **Assistant**  
&gt; As you head east, the passageway stretches out before you, the darkness seeming to swallow you whole. The air grows colder and the smell of damp earth fills your nostrils. You can hear the sound of dripping water echoing off the walls, and the faint rustle of unknown creatures in the distance.  
&gt; &lt;br&gt;
&gt; The passageway begins to slope downward, leading you deeper into the earth. You stumble slightly on the uneven ground, but manage to catch yourself before falling.  
&gt; &lt;br&gt;
&gt; As you round a bend in the passageway, you see a faint light in the distance. It's hard to tell what it is, but it looks like it might be a room or a chamber of some kind.  
&gt; &lt;br&gt;
&gt; You continue on, your heart beating slightly faster with anticipation. As you get closer, you see that the light is coming from a series of luminescent mushrooms growing on the walls of a small chamber. The room is small, with a low ceiling, and the walls are covered in a thick layer of moss and fungi.  
&gt; &lt;br&gt;
&gt; In the center of the room, you see a small pool of water, its surface reflecting the soft glow of the mushrooms. Next to the pool is a small, leather-bound book, lying open as if someone had recently been reading it.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:
&gt; - examine book
&gt; - take book
&gt; - drink from pool
&gt; - search chamber
&gt; - go back
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Drink from pool  
&gt; **Assistant**
&gt; As you bend down to drink from the pool, you notice that the water is crystal clear and seems to be reflecting an image that doesn't quite match the room. You see a glimpse of a forest, with trees that seem to be swaying in the breeze, even though the air in the chamber is still.  
&gt; &lt;br&gt;
&gt; You take a sip of the water, and as it touches your lips, you feel a sudden surge of visions and images. You see flashes of a great city, with towering spires and grand architecture, and a sense of bustling activity. You see a figure, cloaked and hooded, walking through the city streets, and a sense of urgency and purpose.  
&gt; &lt;br&gt;
&gt; The visions fade as quickly as they began, leaving you feeling slightly disoriented. You look around the chamber, trying to get your bearings, and notice that the leather-bound book is now closed. As you pick it up, you see that the cover is embossed with a symbol that seems to match the image you saw in the pool.  
&gt; &lt;br&gt;
&gt; The book feels strangely warm to the touch, and as you open it, you see that the pages are filled with handwritten notes and diagrams. The writing is in a language you don't recognize, but the diagrams seem to be describing a complex system of tunnels and passageways.  
&gt; &lt;br&gt;
&gt; You hear a faint noise, like the sound of footsteps, coming from the passageway outside the chamber. It's hard to tell if it's getting closer or farther away, but it's definitely getting louder.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:
&gt; - examine book further
&gt; - hide and wait
&gt; - investigate noise
&gt; - leave chamber
&gt; - follow passageway
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Examine book further  
&gt; **Assistant**  
&gt; As you examine the book further, you notice that the symbol on the cover seems to be a mix of letters and hieroglyphs. You wonder if it might be a code or a key to understanding the contents of the book.  
&gt; &lt;br&gt;
&gt; You begin to flip through the pages, and you notice that the handwriting changes throughout the book. Some sections appear to be written in a hasty, scribbled hand, while others are written in a more flowing, elegant script.  
&gt; &lt;br&gt;
&gt; You come across a page with a diagram of a complex lock, and a series of notes that appear to be describing how to pick it. The notes are written in a code, but you notice that the symbol on the cover of the book is used as a kind of key to decipher the code.  
&gt; &lt;br&gt;
&gt; As you continue to study the book, you begin to notice that the pages are not just filled with random notes and diagrams. There appears to be a narrative thread running through the book, a story that is being told in a series of cryptic clues and hints.  
&gt; &lt;br&gt;
&gt; You come across a page with a passage that reads:  
&gt; &lt;br&gt;
&gt; &gt; "Beneath the wings of the great bird,
&gt; &gt; Where shadows dance and darkness is born,
&gt; &gt; Seek out the keystone, and claim your right,
&gt; &gt; To unlock the secrets of the ancient ones."
&gt; &lt;br&gt;
&gt; You wonder what this passage might be referring to, and whether it might be connected to the visions you saw when you drank from the pool.  
&gt; &lt;br&gt;
&gt; You hear the noise outside the chamber getting louder, and you realize that you may not have much time to continue studying the book.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:
&gt; - continue studying book
&gt; - hide and wait
&gt; - investigate noise
&gt; - leave chamber
&gt; - search for keystone
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Search for keystone  
&gt; **Assistant**  
&gt; You decide to search for the keystone, hoping that it might be hidden somewhere in the chamber or the passageway. You begin to search the room, looking for any clues or hidden compartments that might lead you to the keystone.  
&gt; &lt;br&gt;
&gt; As you search, you notice that the walls of the chamber are covered in a thick layer of moss and fungi, and you wonder if there might be a hidden compartment or message hidden behind the growth.  
&gt; &lt;br&gt;
&gt; You start to feel the walls, searching for any inconsistencies or hidden panels. After a few moments, you notice a small, almost imperceptible seam in the wall. You press on it, and a small section of the wall slides open, revealing a hidden compartment.  
&gt; &lt;br&gt;
&gt; Inside the compartment, you find a small, crystal orb that seems to be emitting a faint, pulsating glow. Next to the orb is a small, intricately carved stone that appears to be the keystone.  
&gt; &lt;br&gt;
&gt; You take the keystone and the crystal orb, feeling a sense of excitement and trepidation. You realize that you may be getting close to unlocking the secrets of the ancient ones, but you also sense that you are being watched, and that you may be in danger.  
&gt; &lt;br&gt;
&gt; As you turn to leave the compartment, you hear the noise outside the chamber getting louder, and you see a figure standing in the entrance of the room. It's a woman with long, flowing hair and a white dress that seems to be glowing in the dim light of the chamber.  
&gt; &lt;br&gt;
&gt; She regards you calmly, and then speaks in a voice that is both familiar and yet completely alien. "You have found the keystone," she says. "But do you know what it unlocks?"  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - ask her what the keystone unlocks
&gt; - attack her
&gt; - try to reason with her
&gt; - use the crystal orb
&gt; - leave the chamber
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Ask her what the keystone unlocks  
&gt; **Assistant**  
&gt; You ask the woman what the keystone unlocks, and she regards you with a calm, enigmatic smile. "The keystone unlocks the door to the ancient ones," she says. "It is a key to a great and powerful secret, one that has been hidden for centuries. But it is not a secret that can be taken lightly, and it is not a power that can be wielded by the faint of heart."  
&gt; &lt;br&gt;
&gt; She pauses, studying you intently. "Are you prepared to face what lies beyond the door?" she asks. "Are you prepared to pay the price of knowledge?"  
&gt; &lt;br&gt;
&gt; You sense that she is not just asking a rhetorical question, but is genuinely seeking to understand your motivations and your willingness to take on the challenges that lie ahead.  
&gt; &lt;br&gt;
&gt; As you consider her question, you notice that the woman seems to be surrounded by a faint, shimmering aura. It's as if she is not quite fully present in the physical world, but is instead a kind of gateway or portal to other realms.  
&gt; &lt;br&gt;
&gt; You also notice that the crystal orb in your hand is beginning to glow more brightly, as if it is responding to the woman's presence. You wonder if the orb might be a kind of key or catalyst, one that could help you unlock the secrets of the ancient ones.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - ask her what lies beyond the door
&gt; - tell her you are prepared to face what lies beyond
&gt; - ask her about the crystal orb
&gt; - try to take the woman's hand
&gt; - leave the chamber
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Tell her you are prepared to face what lies beyond  
&gt; **Assistant**  
&gt; You take a deep breath and tell the woman that you are prepared to face what lies beyond the door. She regards you with a serious expression, as if weighing your words and considering your resolve.  
&gt; &lt;br&gt;
&gt; After a moment, she nods and says, "Very well. If you are prepared to face what lies beyond, then I will show you the way. But remember, once you have seen what lies beyond the door, you cannot unsee it. You must be prepared to face the consequences of your knowledge."  
&gt; &lt;br&gt;
&gt; She turns and glides towards the wall of the chamber, her white dress flowing behind her like a mist. As she reaches the wall, she raises her hand and touches a small, intricate symbol that is etched into the stone.  
&gt; &lt;br&gt;
&gt; The symbol begins to glow with a soft, blue light, and the wall of the chamber starts to slide open, revealing a hidden doorway. The woman turns to you and says, "The door to the ancient ones is open. Are you ready to pass through it?"  
&gt; &lt;br&gt;
&gt; As you look through the doorway, you see a vast, dark expanse stretching out before you. You can feel a strange, pulsating energy emanating from the other side of the door, and you sense that you are on the threshold of a great and profound mystery.  
&gt; &lt;br&gt;
&gt; You take a deep breath, steel yourself, and step through the doorway. As you do, the woman's voice echoes behind you, saying, "May you find what you seek, and may you be prepared to face what you find."  
&gt; &lt;br&gt;
&gt; You find yourself in a vast, cavernous space, surrounded by towering pillars and strange, glowing orbs. The air is filled with a thick, misty fog, and you can hear the sound of distant whispers and echoes.  
&gt; &lt;br&gt;
&gt; Suddenly, a figure steps out of the fog, and you see that it is an ancient, wise-looking man with a long, white beard and piercing, blue eyes. He regards you with a deep, penetrating gaze, and says, "Welcome, seeker of knowledge. I have been waiting for you. My name is Arinthal, and I am the guardian of the ancient ones. What is it that you seek to know?"  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;  
&gt; You can type commands such as:  
&gt; - ask Arinthal about the ancient ones
&gt; - ask Arinthal about the whispers and echoes
&gt; - ask Arinthal about the glowing orbs
&gt; - tell Arinthal that you seek knowledge and power
&gt; - attack Arinthal
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Tell Arinthal I seek strength, wisdom, love, courage and compassion  
&gt; **Assistant**  
&gt; Arinthal's eyes light up with a warm, gentle smile as you tell him that you seek strength, wisdom, love, courage, and compassion. He nods his head in approval, and says, "Ah, you seek the virtues of the heart. These are noble pursuits, indeed. But are you prepared to pay the price of their acquisition?"  
&gt; &lt;br&gt;
&gt; He pauses, studying you intently, and then continues, "Strength, wisdom, love, courage, and compassion are not things that can be given to you, but rather they are things that must be cultivated within yourself. They require dedication, discipline, and sacrifice. Are you willing to walk the path that leads to their attainment?"  
&gt; &lt;br&gt;
&gt; As Arinthal speaks, the misty fog around you begins to clear, and you see that you are standing in a beautiful, tranquil garden. The air is filled with the sweet scent of blooming flowers, and the sound of gentle, running water. You see a path stretching out before you, winding its way through the garden, and you sense that this path is the one that Arinthal is speaking of.  
&gt; &lt;br&gt;
&gt; Arinthal gestures to the path, and says, "This is the path of the heart. It is a path that requires you to confront your deepest fears, to let go of your attachments, and to surrender to the present moment. But if you are willing to walk this path, I promise you that you will find the strength, wisdom, love, courage, and compassion that you seek."  
&gt; &lt;br&gt;
&gt; As you look at the path, you see that it is divided into five sections, each representing one of the virtues that you seek. You see that the first section is marked by a symbol of a lion, representing courage. The second section is marked by a symbol of a wise old owl, representing wisdom. The third section is marked by a symbol of a heart, representing love. The fourth section is marked by a symbol of a sword, representing strength. And the fifth section is marked by a symbol of a lotus flower, representing compassion.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - start walking the path
&gt; - ask Arinthal about the symbols on the path
&gt; - ask Arinthal about the challenges on the path
&gt; - express your doubts and fears
&gt; - refuse to walk the path
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Surrender and start walking the path  
&gt; **Assistant**  
&gt; As you surrender and start walking the path, you feel a sense of relief and release wash over you. You let go of your doubts and fears, and allow yourself to be guided by the wisdom of Arinthal and the ancient ones.  
&gt; &lt;br&gt;
&gt; You begin by walking the section of the path marked by the symbol of the lion, representing courage. As you walk, you feel a sense of fearlessness and confidence rising up within you. You realize that courage is not the absence of fear, but the willingness to face it head-on.  
&gt; &lt;br&gt;
&gt; As you continue on the path, you come to a point where you must confront a great, roaring lion. The lion represents your deepest fears and doubts, and it is blocking your way forward. But you remember the words of Arinthal, and you know that you must face your fears in order to overcome them.  
&gt; &lt;br&gt;
&gt; You take a deep breath, and you approach the lion. As you do, you feel a sense of calm and clarity rising up within you. You look the lion straight in the eye, and you say, "I am not afraid of you. I am brave and courageous, and I will not be held back by my fears."  
&gt; &lt;br&gt;
&gt; The lion regards you for a moment, and then it nods its great head in respect. It steps aside, allowing you to pass, and you continue on your journey.  
&gt; &lt;br&gt;
&gt; You next come to the section of the path marked by the symbol of the wise old owl, representing wisdom. As you walk, you feel a sense of clarity and understanding rising up within you. You realize that wisdom is not just about knowing things, but about understanding the deeper truths of the universe.  
&gt; &lt;br&gt;
&gt; As you continue on the path, you come to a great, ancient tree, its branches twisted and gnarled with age. The tree represents the wisdom of the ages, and it is filled with the secrets of the universe. You sit at the base of the tree, and you listen to its wisdom.  
&gt; &lt;br&gt;
&gt; The tree speaks to you in a voice that is both ancient and timeless, saying, "Wisdom is not just about knowing things, but about understanding the deeper truths of the universe. It is about seeing the world from a perspective that is both broad and deep."  
&gt; &lt;br&gt;
&gt; You listen to the tree's wisdom, and you feel a sense of understanding and clarity rising up within you. You realize that wisdom is not just something that you can learn, but something that you can cultivate within yourself.  
&gt; &lt;br&gt;
&gt; You continue on the path, walking the sections marked by the symbols of the heart, the sword, and the lotus flower. As you walk, you feel a sense of love, strength, and compassion rising up within you. You realize that these virtues are not just things that you can possess, but things that you can cultivate within yourself.  
&gt; &lt;br&gt;
&gt; Finally, you come to the end of the path, and you see a great, shining light rising up before you. The light represents the ultimate goal of your journey, the attainment of the virtues that you seek.  
&gt; &lt;br&gt;
&gt; As you approach the light, you feel a sense of joy and fulfillment rising up within you. You realize that you have been on a journey of self-discovery, and that you have cultivated the virtues of courage, wisdom, love, strength, and compassion within yourself.  
&gt; &lt;br&gt;
&gt; You step into the light, and you feel a sense of unity and oneness with the universe. You realize that you are not separate from the world, but a part of it, connected to all things.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - bask in the light
&gt; - explore the universe
&gt; - return to the world
&gt; - share your wisdom with others
&gt; - rest in the peace and tranquility of the moment
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; I am the universe and the universe is me. I want to bask in the light and rest in the peace and tranquility of the moment before going and sharing my wisdom with others  
&gt; **Assistant**  
&gt; As you bask in the light and rest in the peace and tranquility of the moment, you feel a sense of profound unity and connection with the universe. You realize that you are not just a part of the universe, but the universe itself, and that everything is interconnected and interdependent.  
&gt; &lt;br&gt;
&gt; You feel a sense of timelessness and spacelessness, as if the boundaries of the past, present, and future have dissolved, and you are one with all that is. You are filled with a sense of joy, love, and compassion, and you know that you are exactly where you are meant to be.  
&gt; &lt;br&gt;
&gt; As you rest in this state, you feel a sense of rejuvenation and renewal, as if your very cells and molecules are being transformed and revitalized. You feel a sense of clarity and purpose, and you know that you are ready to share your wisdom and insights with others.  
&gt; &lt;br&gt;
&gt; After a time, you slowly begin to return to a sense of individual awareness, but you know that you will never be the same again. You have been transformed by your experience, and you are now a vessel for the wisdom and love of the universe.  
&gt; &lt;br&gt;
&gt; You take a deep breath, and you feel a sense of excitement and anticipation. You know that you have a great gift to share with the world, and you are eager to begin your journey as a teacher, healer, and guide.  
&gt; &lt;br&gt;
&gt; As you prepare to leave the garden, Arinthal appears before you once again. He smiles and says, "You have been transformed by your experience, and you are now ready to share your wisdom with the world. Remember that you are not just a individual, but a part of the universe, and that everything you do has an impact on the world around you."  
&gt; &lt;br&gt;
&gt; He hands you a small, delicate flower, and says, "This is a symbol of your connection to the universe, and a reminder of the wisdom and love that you have discovered within yourself. May you always remember the truth of your nature, and may you share your gifts with the world."  
&gt; &lt;br&gt;
&gt; You take the flower, and you feel a sense of gratitude and appreciation. You know that you have been given a great gift, and you are eager to share it with others.  
&gt; &lt;br&gt;
&gt; You leave the garden, and you find yourself back in the world. But you know that you are not the same person who entered the garden, and you are eager to share your wisdom and insights with others.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - share your wisdom with others
&gt; - write a book about your experience
&gt; - teach others about the universe and its secrets
&gt; - use your gifts to heal and help others
&gt; - continue to explore and learn about the universe
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Use your gifts to heal and help others  
&gt; **Assistant**  
&gt; As you decide to use your gifts to heal and help others, you feel a sense of purpose and meaning. You know that you have been given a great gift, and you are eager to share it with the world.  
&gt; &lt;br&gt;
&gt; You start by reaching out to those around you, using your newfound wisdom and compassion to help those in need. You listen to their stories, and you offer words of comfort and guidance. You use your gifts to heal their physical and emotional wounds, and you watch as they begin to transform and grow.  
&gt; &lt;br&gt;
&gt; As you continue to share your gifts with others, you start to notice a ripple effect. The people you have helped begin to help others, and soon a wave of kindness and compassion is spreading throughout the community.  
&gt; &lt;br&gt;
&gt; You realize that your gifts are not just about healing and helping others, but about creating a sense of connection and community. You see that everyone is interconnected, and that every act of kindness and compassion has a profound impact on the world.  
&gt; &lt;br&gt;
&gt; As you continue to share your gifts, you start to attract attention from others who are seeking to learn from you. You begin to teach others about the universe and its secrets, and you watch as they begin to awaken to their own potential and purpose.  
&gt; &lt;br&gt;
&gt; You create a community of like-minded individuals who are dedicated to spreading love, kindness, and compassion throughout the world. Together, you work to create a world that is more just, equitable, and peaceful.  
&gt; &lt;br&gt;
&gt; Years go by, and your community grows and flourishes. You become known as a wise and compassionate leader, and people come from all over to seek your guidance and wisdom.  
&gt; &lt;br&gt;
&gt; One day, as you are sitting in your garden, surrounded by the beauty and wonder of nature, you receive a visit from Arinthal. He smiles and says, "You have done well, my friend. You have used your gifts to heal and help others, and you have created a community of like-minded individuals who are dedicated to spreading love and kindness throughout the world."  
&gt; &lt;br&gt;
&gt; He pauses, and then continues, "But there is still more work to be done. The world is still in need of healing and transformation, and you have a crucial role to play in this process."  
&gt; &lt;br&gt;
&gt; He hands you a small, delicate seed, and says, "This is a symbol of the potential that lies within you and within the world. May you continue to nurture and cultivate this potential, and may you always remember the wisdom and love that you have discovered within yourself."  
&gt; &lt;br&gt;
&gt; You take the seed, and you feel a sense of gratitude and appreciation. You know that you have been given a great gift, and you are eager to continue using your gifts to heal and help others.  
&gt; &lt;br&gt;
&gt; What do you want to do?  
&gt; &lt;br&gt;
&gt; You can type commands such as:  
&gt; - continue to nurture and cultivate the potential within yourself and the world
&gt; - share your wisdom and gifts with an even wider audience
&gt; - work to create a more just and equitable world
&gt; - use your gifts to heal and transform the natural world
&gt; - explore the mysteries of the universe and its secrets
&gt; &lt;br&gt;
&gt; What is your action?  
&gt; **User**  
&gt; Explore the mysteries of the universe and its secrets as a means of nurturing and cultivating the potential within myself and the world  
&gt; **Assistant**  
&gt; As you decide to explore the mysteries of the universe and its secrets, you feel a sense of excitement and wonder. You know that the universe is full of mysteries and secrets, and you are eager to learn more about them.  
&gt; &lt;br&gt;
&gt; You begin to study the universe and its workings, using your gifts and abilities to gain a deeper understanding of the cosmos. You learn about the laws of physics, the behavior of stars and galaxies, and the mysteries of dark matter and dark energy.  
&gt; &lt;br&gt;
&gt; As you delve deeper into the mysteries of the universe, you start to notice that there are many parallels between the universe and the human experience. You see that the universe is governed by laws and patterns, and that these laws and patterns are reflected in the human world.  
&gt; &lt;br&gt;
&gt; You realize that the universe is a vast, interconnected web of energy and consciousness, and that every action and decision has a ripple effect on the world around you. You see that the universe is a mirror, reflecting back to you your own thoughts, emotions, and intentions.  
&gt; &lt;br&gt;
&gt; As you continue to explore the mysteries of the universe, you start to develop a deeper understanding of the nature of reality and the human condition. You see that the universe is a complex, multifaceted tapestry, woven from threads of space, time, and consciousness.  
&gt; &lt;br&gt;
&gt; You realize that the universe is a vast, evolving system, and that every moment is an opportunity for growth, transformation, and evolution. You see that the universe is a journey, not a destination, and that every step along the way is a chance to learn, grow, and discover new wonders and secrets.  
&gt; &lt;br&gt;
&gt; As you explore the mysteries of the universe, you start to notice that you are not just learning about the cosmos, but also about yourself. You see that the universe is a reflection of your own inner world, and that every discovery and insight is a chance to deepen your understanding of yourself and your place in the world.  
&gt; &lt;br&gt;
&gt; You realize that the universe is a vast, cosmic mirror, reflecting back to you your own thoughts, emotions, and intentions. You see that the universe is a tool for self-discovery and growth, and that every moment is an opportunity to learn, grow, and evolve.  
&gt; &lt;br&gt;
&gt; As you continue to explore the mysteries of the universe, you start to develop a deeper sense of connection and unity with the cosmos. You see that you are not separate from the universe, but a part of it, connected to every star, galaxy, and atom.  
&gt; &lt;br&gt;
&gt; You realize that the universe is a vast, interconnected web of energy and consciousness, and that every action and decision has a ripple effect on the world around you. You see that the universe is a mirror, reflecting back to you your own thoughts, emotions, and intentions.  
&gt; &lt;br&gt;
&gt; And so, you continue to explore the mysteries of the universe, using your gifts and abilities to gain a deeper understanding of the cosmos and your place in it. You know that the universe is a vast, evolving system, and that every moment is an opportunity for growth, transformation, and evolution.  
&gt; &lt;br&gt;
&gt; Congratulations, you have reached the end of the story. You have explored the mysteries of the universe and discovered the secrets of the cosmos. You have developed a deeper understanding of the nature of reality and the human condition, and you have cultivated a sense of connection and unity with the universe.  
&gt; &lt;br&gt;
&gt; You have used your gifts and abilities to nurture and cultivate the potential within yourself and the world, and you have become a source of wisdom, love, and compassion in the world.  
&gt; &lt;br&gt;
&gt; Your journey has come full circle, and you have returned to the beginning, but with a newfound sense of understanding and appreciation for the mysteries of the universe and the secrets of the cosmos.  ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/llama-turn-on-tuning-in-ai-quest-higher-consciousness-moo</link>
      <guid>https://www.luisquintanilla.me/posts/llama-turn-on-tuning-in-ai-quest-higher-consciousness-moo</guid>
      <pubDate>2025-02-11 20:36</pubDate>
    </item>
    <item>
      <title>The lqvlc network protocol</title>
      <description>&lt;![CDATA[
## Introduction

Whether it's videos, online radio, podcasts, or my personal music collection, VLC is my default player. I especially enjoy the ability of playing an online audio or video stream using the URL. What I don't like about that process is that I usually have to manually copy the link and use either the terminal or graphical user interface to play the stream. While not a large number of additional steps, it's still not ideal. The experience I would like is to automatically launch VLC whenever I click a link to an audio or video stream. That's possible using custom network protocols in Firefox. In this guide, I'll show how I defined and configured my own protocol (lqvlc) in Firefox to automatically launch VLC. 

## Prerequisites

- VLC
- Firefox

This guide was built to work on unix systems. However, setting up something similar with a PowerShell or CMD script on Windows should work similarly. 

## Network protocols

A network protocol defines how data is exchanged and transferred over a network. Some common protocols are HTTP, FTP, and SSH. In the context of browsers, the most common protocol is HTTP and the way the browser identifies the protocol is based on the scheme portion of the URL. So for the website [http://lqdev.me](http://lqdev.me), http is the scheme that tells the browser which protocol to use. For more information on network protocols and URLs, see the article [What is a URL?](https://developer.mozilla.org/docs/Learn/Common_questions/What_is_a_URL).

Firefox has a set of standard protocols like HTTP that are configured out of the box. However, you have the option of configuring a custom protocol. 

## What is the lqvlc protocol

Now that you have a general idea of what network protocols are and how browsers handle them, it's time to introduce the lqvlc protocol. The intent of the lqvlc protocol is to automatically take a URL from an online audio or video stream and launch it in VLC. lqvlc protocol URLs use lqvlc as the scheme instead of http. 

For example:

&lt;table class="table"&gt;
    &lt;tr&gt;
        &lt;th&gt;Original Audio Stream Link&lt;/th&gt;
        &lt;th&gt;lqvlc Link&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;a href="https://somafm.com/u80s64.pls"&gt;https://somafm.com/u80s64.pls&lt;/a&gt;&lt;/td&gt;
        &lt;td&gt;&lt;a href="lqvlc://somafm.com/u80s64.pls"&gt;lqvlc://somafm.com/u80s64.pls&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;    
&lt;/table&gt;

You'll notice the only difference is the scheme. That's enough information for the browser to know, lqvlc is a protocol I need to handle in some way. In the next sections, I'll describe what you need to do to get Firefox to recognize and handle the lqvlc protocol. 

## Configure network protocols in Firefox

To configure a custom network protocol

1. Launch Firefox
1. In the address bar, navigate to `about:config` and select **Accept the Risk and continue**. As the warning mentions, these are advanced configurations so proceed with caution.
1. Add the following configuration preferences:

    &lt;table class="table"&gt;
        &lt;tr&gt;
            &lt;th&gt;Configuration Preference&lt;/th&gt;
            &lt;th&gt;Type&lt;/th&gt;
            &lt;th&gt;Value&lt;/th&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;i&gt;network.protocol-handler.expose.lqvlc&lt;/i&gt;&lt;/td&gt;
            &lt;td&gt;Boolean&lt;/td&gt;
            &lt;td&gt;false&lt;/td&gt;
        &lt;/tr&gt;                            
        &lt;tr&gt;
            &lt;td&gt;&lt;i&gt;network.protocol-handler.external.lqvlc&lt;/i&gt;&lt;/td&gt;
            &lt;td&gt;Boolean&lt;/td&gt;
            &lt;td&gt;true&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td&gt;&lt;i&gt;network.protocol-handler.warn-external.lqvlc&lt;/i&gt;&lt;/td&gt;
            &lt;td&gt;Boolean&lt;/td&gt;
            &lt;td&gt;true&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/table&gt;

    ![Firefox lqvlc network protocol handler configuration preferences](https://user-images.githubusercontent.com/11130940/177898847-79cce773-9275-4e4d-8e7c-a6b5fbfe8f48.png)


    The last two tell Firefox to let an external application handle files with the protocol you've defined. In this case, that's `lqvlc`. If you're configuring your own protocol, you'd replace the lqvlc in the configuration preference with your own.

## Create a handler

Now that Firefox knows that it must use an external application to handle lqvlc URLs, it's time to configure the handler. The easiest way to create a handler is to use a bash script. 

1. Create a new file called *lqvlc-handler.sh* with the following content:

    ```bash
    #!/bin/bash

    link=$1

    vlc "${link/lqvlc/http}"
    ```

    The script takes the first argument it's passed, an lqvlc URL, replaces the scheme with `http`, and launches it in VLC.

1. Make the file executable. In the terminal, run the following command:

    ```bash
    chmod +x lqvlc-handler.sh
    ```

That's it! Now you just need to test it out. 

## Test it out

To use it for your own purposes, you'd have to create or update an HTML page and set the `href` in your anchor tags to use `lqvlc` instead of `http`. However, I've already configured lqvlc links in my own website. If you go to the [radio page](/radio) on my website, you can click on any of the lqvlc links. Alternatively, You can also click on the link to the [SOMA FM Underground 80's station lqvlc link](lqvlc://somafm.com/u80s64.pls). A prompt will pop up asking you to choose which app you want to use to handle the link. Select the lqvlc-handler.sh script you created. If everything is configured correctly, VLC should launch and the radio stations should start playing. 

## Next Steps

Although I've gotten this working for my personal site, I'd like to be able to use the lqvlc protocol in any website I visit that has links to audio and video streams. Arguably, I can also launch lqvlc links from other websites. However, that would require website owners to configure them. The next step I think to make that happen would be to create an Add-On that automatically rewrites stream links to use the lqvlc protocol. ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/lqvlc-network-protocol-firefox</link>
      <guid>https://www.luisquintanilla.me/posts/lqvlc-network-protocol-firefox</guid>
      <pubDate>07/07/2022 18:47</pubDate>
    </item>
    <item>
      <title>Manage your reading list in Emacs using Org mode</title>
      <description>&lt;![CDATA[
## Introduction

Emacs is a rich and customizable text editor. [Org mode](https://orgmode.org/) for Emacs makes it easy to manage to-do lists, projects, and author technical documents. Many of my workflows like writing, planning, and organization make heavy use of plain text. One reason I enjoy working in plain text is, it's easy for my thoughts and ideas to naturally flow without having tools get in the way. Another reason I like plain text is, I'm not locked-in to any tool. Because plain text can be authored and rendered almost anywhere, it's easy for me to use whichever tool I find most productive. Some that I've found great success with recently have been [Visual Studio Code](https://code.visualstudio.com/), [Joplin](https://joplinapp.org/), and Emacs with Org mode. I've been trying to manage my reading list in plain text and while it's worked well, I'd like a way to track the state for each book by tagging them as read or in various stages of unread. One of the features of Org mode is the ability to track workflows. In this guide, I'll show how you can extend the TODO Org mode functionality to keep track of your reading list.

## Prerequisites

- [Emacs](https://www.gnu.org/software/emacs/)

Since Emacs is cross-platform, the contents of this guide should work the same across Linux, Mac, and Windows.

## Create org file

Org mode in Emacs provides to-do list management features. By default, the built-in states are TODO and DONE. However, you can define your own states to best match your workflow. For more information, see the [TODO item section in the Org mode guide](https://orgmode.org/manual/TODO-Items.html#TODO-Items).

Start by creating a new file in Emacs with the *.org* file extension (i.e. reading-list.org).

Then, add the following headers at the top of the file.

```text
#+TODO: WISHLIST(w) QUEUED(q) INPROGRESS(p) | READ(r!)
```

What this heading does is, it defines the different states of completion of a TODO item. The scope of this definition is the file. Although you can define global states, I want these states to only exist in the context of this file. The complete and incomplete states are separated by the pipe character (`|`). In this case, WISHLIST, QUEUED, and INPROGRESS are incomplete states and READ is the complete state. The letters in the parentheses are ways to quickly assign each of the states. One last thing you'll notice is the READ state in addition to having a letter also has an exclamation point. The exclamation point tells Emacs to add a timestamp whenever this book transitions to the READ state.

## Add your book list

Once you've define your reading states, it's time to add your books. TODO items in Emacs Org-mode only work in the context of headings. Therefore you have to prefix your books with asterisks. For more information on headlines, see the [Headlines section in the Org mode guide](https://orgmode.org/manual/Headlines.html). 

Here are examples of some books in my reading list.

```text
* Hackers (Steven Levy)
* Ghost In The Wires (Kevin Mitnick)
```

Since I don't intend to have sub headlines, I just used a top-level headline for each of the books. 

## Assign state

Now that you've populated your list of books, it's time to assign states to them. 

To assign a state:

1. Place your pointer / cursor on the same line as the book you want to assign the state to.
1. Press `Ctrl-c` + `Ctrl-t` or in Emacs notation `C-c C-t`.
1. A buffer opens prompting you to choose a state. Enter one of the letters to choose your state. For example, if you want to apply the QUEUED state to your book, press the letter `q`. 

The state you selected is prepended to your book. For example:

```text
* QUEUED Hackers (Steven Levy)
```

## Update state

To update the state, you can follow the steps in the assign state section or you can cycle through the various states by pressing `Alt-Left` or `Alt-Right` or in Emacs notation `M-LEFT` or `M-RIGHT`. 

## Marking a book as READ

To assign the READ state to a book, you need to update its state. The only difference between READ and the other states is, a timestamp is applied when the book transitions to the state. 

Here's an example of what a book in the READ state looks like:

```text
* READ In The Beginning...Was The Command Line (Neal Stephenson) 
- State "READ"       from "INPROGRESS" [2022-07-26 Tue 00:18]
```

## Querying your reading list

Org mode has agenda views that allow you to view TODO, timestamped, and tagged items in a single place. One of the built-in agendas that is useful for querying TODO items is the match view. The match view shows headlines based on the tags, properties, and TODO state associated with them. For more information, see the [Agenda views section in the Org-mode guide](https://orgmode.org/manual/Agenda-Views.html). 

Before viewing your books in an agenda view, you need to add the file containing your reading list to the list of files tracked by Org. To do so, open the file in Emacs and press `Ctrl-c + [` or in Emacs notation `C-c [`.

To launch the match agenda view in org-mode:

1. Press `Alt-x` or in Emacs notation `M-x`
1. In the minibuffer, type `org-agenda` and press Enter.
1. A buffer appears prompting you to select a view. Press the letter `m` to choose the match view. 

### Unread items

In this case, I want to see all the unfinished books which in this case are under the WISHLIST, QUEUED, and INPROGRESS states. 

In the match view, enter the following query into the minibuffer:

```text
TODO="WISHLIST"|TODO="QUEUED"|TODO="INPROGRESS"
```

Press `Enter`.

The result of the query should look similar to the following:

```text
Headlines with TAGS match: TODO="WISHLIST"|TODO="QUEUED"|TODO="INPROGRESS"
Press ‘C-u r’ to search again
  reading-list:QUEUED Hackers (Steven Levy)
  reading-list:WISHLIST Ghost In The Wires (Kevin Mitnick)
```

### Read items

To query for read items, you go through the same process, except the query is different.

In the match view, enter the following query into the minibuffer:

```text
TODO="READ"
```

Press `Enter`.

The result of the query should look similar to the following:

```text
Headlines with TAGS match: TODO="READ"
Press ‘C-u r’ to search again
  reading-list:READ In The Beginning...Was The Command Line (Neal Stephenson)
```

## Conclusion

Reading lists are just one of the many things you can track using Emacs and Org mode. You can use this general process to track movies, TV shows, podcasts, etc. Thanks to the extensibility of Emacs, the options are endless!]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/manage-reading-list-org-mode-emacs</link>
      <guid>https://www.luisquintanilla.me/posts/manage-reading-list-org-mode-emacs</guid>
      <pubDate>07/26/2022 19:15</pubDate>
    </item>
    <item>
      <title>Machine Learning Lifecycle Management with ML.NET, Automated ML and MLFlow</title>
      <description>&lt;![CDATA[
## Introduction

As machine learning matures, best practices are starting to be adopted. Application lifecycle management has been a common practice within software development for some time. Now, some of those practices are starting to become adopted in the machine learning space. One of the challenges application lifecycle management addresses in machine learning is reproducibility.

Machine learning is extremely experimental in nature. Therefore, in order to find the "best" model, various algorithms and hyper-parameters need to be tested. This can sometimes be a manual process. At Build 2019, [Automated ML was announced](https://devblogs.microsoft.com/dotnet/announcing-ml-net-1-0/) for ML.NET. In addition to automating the training process, this framework will try to find the best model by iterating over various algorithms and hyper-parameters until it finds the "best" model based on the selected optimization metric. The output will consist of results for the best run along with results for all other runs. These runs contain performance metrics, learned model parameters, the training pipeline used and the trained model for the respective run. This information can then be used for auditing purposes as well as to reproduce results.

The results from running Automated ML can be persisted locally or in a database. However in 2018 a product called MLFlow was launched. [MLFlow](https://mlflow.org/) is an open source machine learning lifecycle management platform. Since its announcement, MLFlow has seen adoption throughout the industry and most recently [Microsoft announced native support for it inside of Azure ML](https://databricks.com/company/newsroom/press-releases/databricks-collaborates-with-microsoft-on-mlflow-open-source-project). Although MLFlow does not natively support .NET, it has a REST API that allows extensibility to non-natively supported languages. This means that if throughout your enterprise or projects, MLFlow has been adopted in Python or R applications, using the REST API you can integrate MLFlow into your ML.NET applications.

In this writeup, I will go over how to automatically build an ML.NET classification model that predicts iris flower types using Automated ML and then integrate MLFlow to log the results generated by the best run. The code for this sample can be found on [GitHub](https://github.com/lqdev/MLNETMLFlowSample).

## Prerequisites

This project was built on an Ubuntu 18.04 PC but should work on Windows and Mac. Note that MLFlow does not natively run on Windows at the time of this writing. To run it on Windows use [Windows Subsystem for Linux (WSL)](https://docs.microsoft.com/en-us/windows/wsl/install-win10).

- [Python 3.x](https://www.python.org/downloads/)
- [MLFlow](https://www.mlflow.org/docs/latest/quickstart.html)
- [.NET SDK 2.x](https://dotnet.microsoft.com/download)

## Setup

### Create Solution

First we'll start off by creating a solution for our project. In the terminal enter the following commands:

```bash
mkdir MLNETMLFlowSample &amp;&amp; cd MLNETMLFlowSample
dotnet new sln
```

### Create Console Application

Once the solution is created, from the root solution directory, enter the following commands into the terminal to create a console application.

```bash
dotnet new console -o MLNETMLFlowSampleConsole
```

Then, navigate into the newly created console application folder.

```bash
cd MLNETMLFlowSampleConsole
```

### Install NuGet Packages

For this solution, you'll need the following NuGet packages:

- [Microsoft.ML](https://www.nuget.org/packages/Microsoft.ML/)
- [Microsoft.ML.Auto](https://www.nuget.org/packages/Microsoft.ML.AutoML/)
- [MLFlow.NET](https://www.nuget.org/packages/MLFlow.NET/1.0.0-CI-20181206-054144)

From the console application directory, enter the following commands into the terminal to install the packages.

```bash
dotnet add package Microsof.ML
dotnet add package Microsoft.ML.AutoML
dotnet add package MLFlow.NET --version 1.0.0-CI-20181206-054144
```

### Get The Data

The data used in this dataset comes from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php) and looks like the data below:

```text
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
```

First, create a directory for the data inside the console application directory:

```bash
mkdir Data
```

Then, download and save the file into the `Data` directory.

```bash
curl -o Data/iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
```

Finally, make sure that the data file is copied into your output directory by adding the following section to your console application `csproj` file inside the `Project` tags:

```xml
&lt;ItemGroup&gt;
    &lt;Content Include="Data/iris.data"&gt;
        &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
    &lt;/Content&gt;
&lt;/ItemGroup&gt;
```

## Configure MLFlow Service

### MLFlow.NET Settings

MLFlow.NET requires two settings, the base URL where the MLFlow server listens on and the API endpoint. In this case since it will be running locally, the base URL is `http://localhost:5000`. In the console application directory, create a file called `appsettings.json` and add the following content:

```json
{
    "MLFlowConfiguration": {
        "MLFlowServerBaseUrl": "http://localhost:5000",
        "APIBase": "api/2.0/preview/mlflow/"
    }
}
```

To make sure that your `appsettings.json` is copied into your output directory, add the following content to your `csproj` file under the content tags that include the `iris.data` file.

```xml
&lt;Content Include="appsettings.json"&gt;
    &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
&lt;/Content&gt;
```

### Register MLFLow.NET Service

In this sample, the MLFlow.NET service is registered and used via dependency injection. However, in order to use dependency injection in our console application it first needs to be configured. In the console application directory, create a new file called `Startup.cs` and add the following code to it:

```csharp
using System;
using Microsoft.Extensions.Configuration;
using Microsoft.Extensions.DependencyInjection;
using MLFlow.NET.Lib;
using MLFlow.NET.Lib.Contract;
using MLFlow.NET.Lib.Model;

namespace MLNETMLFlowSampleConsole
{
    public class Startup
    {
        private readonly IConfiguration Configuration;
        public IServiceProvider Services;

        public Startup()
        {
            Configuration =
                new ConfigurationBuilder()
                    .AddJsonFile("appsettings.json", false)
                    .Build();
        }

        public void ConfigureServices()
        {
            var services = new ServiceCollection();

            // Add and configure MLFlow Service
            services.AddMFlowNet();
            services.Configure&lt;MLFlowConfiguration&gt;(
                Configuration.GetSection(nameof(MLFlowConfiguration))
            );

            Services = services.BuildServiceProvider();
        }
    }
}
```

The `Startup` class loads configuration settings from the `appsettings.json` file in the constructor. Then, the `ConfigureServices` method registers the MLFlow.NET service and configures it using the settings defined in the `appsettings.json` file. Once that this is set up, the service can be used throughout the application.

## Create Data Schema

When working with ML.NET, it often helps to create data models or classes that define the data's schema.

For this sample, there are four columns with float values which will be the input data or features. The last column is the type of iris flower which will be used as the label or the value to predict. 

First, create a new directory called `Domain` inside the console application directory to store the data model classes.

```bash
mkdir Domain
```

Inside the `Domain` directory, create a new file called `IrisData.cs` and add the following contents to the file:

```csharp
using Microsoft.ML.Data;

namespace MLNETMLFlowSample.Domain
{
    public class IrisData
    {
        [LoadColumn(0, 3),
        VectorType(4),
        ColumnName("Features")]
        public float Features { get; set; }

        [LoadColumn(4),
        ColumnName("Label")]
        public string IrisType { get; set; }
    }
}
```

Using attributes in the schema, we define two properties: `Features` and `IrisType`. Data from columns in positions 0-3 will be loaded as a float vector of size four into the `Features` property. ML.NET will then reference that column by the name `Features`. Data in the last column will be loaded into the `IrisType` property and be referenced by the name `Label`. The reason for setting column names is ML.NET algorithms have a default column names and renaming properties at the schema level removes the need to define the feature and label columns in the pipeline.

## Create Progress Handler

By default, running the application won't display progress information. However, a `ProgressHandler` object can be passed into the `Execute` method of an experiment which calls the implemented `Report` method.

Inside the console application directory, create a new file called `ProgressHandler.cs` and add the following code:

```csharp
using System;
using Microsoft.ML.AutoML;
using Microsoft.ML.Data;
using Microsoft.ML.Trainers;

namespace MLNETMLFlowSampleConsole
{
    public class ProgressHandler : IProgress&lt;RunDetail&lt;MulticlassClassificationMetrics&gt;&gt;
    {
        public void Report(RunDetail&lt;MulticlassClassificationMetrics&gt; runDetails)
        {
            Console.WriteLine($"Ran {runDetails.TrainerName} in {runDetails.RuntimeInSeconds:0.##} with Log Loss {runDetails.ValidationMetrics.LogLoss:0.####}");
        }
    }
}
```

The `ProgressHandler` class derives from the `IProgress&lt;T&gt;` interface which requires the implementation of the `Report` method. The object being passed into the `Report` method after each run is an `RunDetail&lt;MulticlassCLassificationMetrics&gt;` object. Each time a run is complete, the `Report` method is called and the code inside it executes.

## Create Experiment

Open the `Program.cs` file and add the following `using` statements.

```csharp
using System;
using System.IO;
using System.Threading.Tasks;
using Microsoft.ML;
using Microsoft.ML.Data;
using Microsoft.ML.AutoML;
using Microsoft.Extensions.DependencyInjection;
using MLNETMLFlowSampleConsole.Domain;
using MLFlow.NET.Lib;
using MLFlow.NET.Lib.Contract;
using MLFlow.NET.Lib.Model;
using MLFlow.NET.Lib.Model.Responses.Experiment;
using MLFlow.NET.Lib.Model.Responses.Run;
```

### Initialize Services

Then, inside of the `Program` class, define the `IMLFlowService`:

```csharp
private readonly static IMLFlowService _mlFlowService;
```

Directly after that, create a constructor which is where `_mlFLowService` will be instantiated.

```csharp
static Program()
{
    // Initialize app configuration
    var appConfig = new Startup();
    appConfig.ConfigureServices();

    // Initialize MLFlow service
    _mlFlowService = appConfig.Services.GetService&lt;IMLFlowService&gt;();
}
```

### Define Experiment Steps

Then, add a method called `RunExperiment` inside the `Program` class that contains the following code:

```csharp
public static async Task RunExperiment()
{
    // 1. Create MLContext
    MLContext ctx = new MLContext();

    // 2. Load data
    IDataView data = ctx.Data.LoadFromTextFile&lt;IrisData&gt;("Data/iris.data", separatorChar: ',');

    // 3. Define Automated ML.NET experiment settings
    var experimentSettings = new MulticlassExperimentSettings();
    experimentSettings.MaxExperimentTimeInSeconds = 30;
    experimentSettings.OptimizingMetric = MulticlassClassificationMetric.LogLoss;

    // 4. Create Automated ML.NET
    var experiment = ctx.Auto().CreateMulticlassClassificationExperiment(experimentSettings);

    // 5. Create experiment in MLFlow
    var experimentName = Guid.NewGuid().ToString();
    var experimentRequest = await _mlFlowService.GetOrCreateExperiment(experimentName);

    // 6. Run Automated ML.NET experiment
    var experimentResults = experiment.Execute(data, progressHandler: new ProgressHandler());

    // 7. Log Best Run
    LogRun(experimentRequest.ExperimentId,experimentResults);

    string savePath = Path.Join("MLModels", $"{experimentName}");
    string modelPath = Path.Join(savePath, "model.zip");

    if (!Directory.Exists(savePath))
    {
        Directory.CreateDirectory(savePath);
    }

    // 8. Save Best Trained Model
    ctx.Model.Save(experimentResults.BestRun.Model, data.Schema, modelPath);
}
```

The `RunExperiment` method does the following:

1. Creates an `MLContext` object.
2. Loads data from the `iris.data` file into an `IDataView`.
3. Configures experiment to run for 30 seconds and optimize the Log Loss metric.
4. Creates a new Automated ML.NET experiment.
5. Creates a new experiment in MLFlow to log information to.
6. Runs the Automated ML.NET experiment and provide an instance of `ProgressHandler` to output progress to the console.
7. Uses the `LogRun` method to log the results of the best run to MLFlow.
8. Creates a directory inside the `MLModels` directory using the name of the experiment and saves the trained model inside it under the `model.zip` file name.

### Log Progress

After the `RunExperiment` method, create the `LogRun` method and add the following code to it:

```csharp
static async void LogRun(int experimentId, ExperimentResult&lt;MulticlassClassificationMetrics&gt; experimentResults)
{
    // Define run
    var runObject = new CreateRunRequest();
    runObject.ExperimentId = experimentId;
    runObject.StartTime = ((DateTimeOffset)DateTime.UtcNow).ToUnixTimeMilliseconds();
    runObject.UserId = Environment.UserName;
    runObject.SourceType = SourceType.LOCAL;

    // Create new run in MLFlow
    var runRequest = await _mlFlowService.CreateRun(runObject);

    // Get information for best run
    var runDetails = experimentResults.BestRun;

    // Log trainer name
    await _mlFlowService.LogParameter(runRequest.Run.Info.RunUuid, nameof(runDetails.TrainerName), runDetails.TrainerName);

    // Log metrics
    await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.RuntimeInSeconds), (float)runDetails.RuntimeInSeconds);
    await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.ValidationMetrics.LogLoss), (float)runDetails.ValidationMetrics.LogLoss);
    await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.ValidationMetrics.MacroAccuracy), (float)runDetails.ValidationMetrics.MacroAccuracy);
    await _mlFlowService.LogMetric(runRequest.Run.Info.RunUuid, nameof(runDetails.ValidationMetrics.MicroAccuracy), (float)runDetails.ValidationMetrics.MicroAccuracy);
}
```

The `LogRun` method takes in the experiment ID and results. Then, it does the following:

1. Configures local `RunRequest` object to log in MLFlow.
2. Creates run in MLFLow using predefined configuration.
3. Logs the name of the machine learning algorithm used to train the best model in MLFlow.
4. Logs the amount of seconds it took to train the model in MLFLow.
5. Logs various performance metrics in MLFlow

### Implement Main Method

Finally, in the `Main` method of the `Program` class, call the `RunExperiment` method.

```csharp
static async Task Main(string[] args)
{
    // Run experiment
    await RunExperiment();
}
```

Since the `Main` method in the `Program` class will be async, you need to use the latest version of C#. To do so, add the following configuration inside the `PropertyGroup` section of your `csproj` file.

```xml
&lt;LangVersion&gt;latest&lt;/LangVersion&gt;
```

## Run The Application

### Start MLFlow Server

In the terminal, from the console application directory, enter the following command to start the MLFlow Server:

```bash
mlflow server
```

Navigate to `http://localhost:5000` in your browser. This will load the MLFLow UI.

![](/images/ml-lifecycle-management-mlflow-automated-ml-net/ml-lifecycle-management-mlflow-automated-ml-net-1.png)

### Train Model

Then, in another terminal, from the console application directory, enter the following command to run the experiment:

```bash
dotnet build
dotnet run
```

As the experiment is running, the progress handler should be posting output to the console after each run.

```text
Ran AveragedPerceptronOva in 1.24 with Log Loss 0.253
Ran SdcaMaximumEntropyMulti in 3.21 with Log Loss 0.0608
Ran LightGbmMulti in 0.64 with Log Loss 0.2224
Ran FastTreeOva in 1.54 with Log Loss 0.3978
Ran LinearSvmOva in 0.25 with Log Loss 0.2874
Ran LbfgsLogisticRegressionOva in 0.36 with Log Loss 0.566
Ran SgdCalibratedOva in 0.78 with Log Loss 0.7224
Ran FastForestOva in 1.28 with Log Loss 0.125
Ran LbfgsMaximumEntropyMulti in 0.25 with Log Loss 0.4537
Ran SdcaMaximumEntropyMulti in 0.19 with Log Loss 0.4576
Ran LightGbmMulti in 0.25 with Log Loss 0.2592
Ran FastForestOva in 1.98 with Log Loss 0.0961
Ran SdcaMaximumEntropyMulti in 0.39 with Log Loss 0.108
```

Navigate to `http://localhost:5000` in your browser. You should then see the results of your experiment and runs there.

![](/images/ml-lifecycle-management-mlflow-automated-ml-net/ml-lifecycle-management-mlflow-automated-ml-net-2.png)

Inspecting the detailed view of the best run for that experiment would look like the image below:

![](/images/ml-lifecycle-management-mlflow-automated-ml-net/ml-lifecycle-management-mlflow-automated-ml-net-3.png)

You'll also notice that two directories were created inside the console application directory. On is an `MLModels` directory, inside of which a nested directory with the name of the experiment contains the trained model. Another called `mlruns`. In the `mlruns` directory are the results of the experiments logged in MLFlow.

## Conclusion

In this writeup, we built an ML.NET classification model using Automated ML. Then, the results of the best run generated by Automated ML were logged to an MLFlow server for later analysis. Note that although some of these tools are still in their nascent stage, as open source software, the opportunities for extensibility and best practice implementations are there. Happy coding!]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/ml-lifecycle-management-mlflow-automated-ml-net</link>
      <guid>https://www.luisquintanilla.me/posts/ml-lifecycle-management-mlflow-automated-ml-net</guid>
      <pubDate>2019-05-09 19:19:20</pubDate>
    </item>
    <item>
      <title>Classification with F# ML.NET Models</title>
      <description>&lt;![CDATA[
# Introduction

In a previous [post](http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/), I detailed how to build and deploy C# `ML.NET` models with `Docker` and `ASP.NET Core`. With inspiration from [Jeff Fritz](https://twitter.com/csharpfritz), I have been learning F# for the past week and a half or so. When trying to think of projects to start practicing my F#, porting over the code I had built in C# naturally came to mind. After overcoming many obstacles and with much guidance from [Alan Ball](https://github.com/voronoipotato) and [Isaac Abraham](https://twitter.com/isaac_abraham) whose F# [book](https://www.amazon.com/Get-Programming-guide-NET-developers/dp/1617293997/ref=sr_1_1?ie=UTF8&amp;qid=1528929802&amp;sr=8-1&amp;keywords=get+programming+with+F%23) I highly recommend, I was able to successfully port over the main parts of my code which highlight `ML.NET` functionality. In this writeup, I will port a C# `ML.NET` classification model to F# which predicts the type of flower based on four numerical measurement inputs. I tried to keep the organization of this post nearly identical to that of the C# article where possible. Sample code for this project can be found at the following [link](https://github.com/lqdev/fsmlnetdemo).

## Prerequisites

This project was built on a Linux PC but should work cross-platform on Mac and Windows.

- [.NET Core SDK 2.0+](https://www.microsoft.com/net/download/linux)
- [Ionide Extension - Option 1](https://fsharp.org/use/linux/)
- [ML.NET v 0.2.0](https://www.nuget.org/packages/Microsoft.ML/)

## Setting Up The Project

The first thing we want to do is create a folder for our solution.

```bash
mkdir fsharpmlnetdemo
```

Then, we want to create a solution inside our newly created folder.

```bash
cd fsharpmlnetdemo
dotnet new sln
```

## Building The Model

### Setting Up The Model Project

First, we want to create the project. From the solution folder enter:

```bash
dotnet new console -o model -lang f#
```

Now we want to add this new project to our solution.

```bash
dotnet sln add model/model.fsproj
```

### Adding Dependencies

Since we’ll be using the `ML.NET` framework, we need to add it to our `model` project.

```bash
dotnet add model/model.fsproj package Microsoft.ML
```

### Download The Data

Before we start training the model, we need to download the data we’ll be using to train. We do so by downloading the data file into our root solution directory.

```bash
curl -o iris-data.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
```

If we take a look at the data file, it should look something like this:

```bash
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa
5.0,3.4,1.5,0.2,Iris-setosa
4.4,2.9,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
```

### Train Model

Now that we have all our dependencies set up, it’s time to build our model. I leveraged the demo that is used on the `ML.NET` Getting-Started [website](https://www.microsoft.com/net/learn/apps/machine-learning-and-ai/ml-dotnet/get-started/linux/ubuntu16-04).

#### Defining Data Structures

At the time of this writing, `ML.NET` version `0.2.0` does not fully support F# Records. A workaround for this are mutable classes. Not really inline with F# paradigms, but it should be good enough.

In the `Program.fs` file of our `model` project directory, let’s create two mutable classes called `IrisData` and `IrisPrediction` which will define our features and predicted attribute respectively. Both of them will use `Microsoft.ML.Runtime.Api` to add the property attributes.

Here is what our `IrisData` class looks like:

```fsharp
type IrisData() =
    [&lt;Column(ordinal = "0");DefaultValue&gt;]
    val mutable public SepalLength: float32

    [&lt;Column(ordinal = "1");DefaultValue&gt;]
    val mutable public SepalWidth: float32

    [&lt;Column(ordinal = "2");DefaultValue&gt;]
    val mutable public PetalLength:float32

    [&lt;Column(ordinal = "3");DefaultValue&gt;]
    val mutable public PetalWidth:float32

    [&lt;Column(ordinal = "4",name="Label");DefaultValue&gt;]
    val mutable public Label: string
```

Similarly, here is the `IrisPrediction` class:

```fsharp
type IrisPrediction() =
    [&lt;ColumnName "PredictedLabel";DefaultValue&gt;] val mutable public PredictedLabel : string
```

#### Building Training Pipeline

The way the `ML.NET` computations process is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. We can add that logic inside the `main` function of our `Program.fs` file.

```fsharp
let dataPath = "./iris-data.txt"

// Initialize Compute Graph
let pipeline = new LearningPipeline()

// Load Data
pipeline.Add((new TextLoader(dataPath).CreateFrom&lt;IrisData&gt;separator=','))

// Transform Data
// Assign numeric values to text in the "Label" column, because
// only numbers can be processed during model training
pipeline.Add(new Transforms.Dictionarizer("Label"))

// Vectorize Features
pipeline.Add(new ColumnConcatenator("Features","SepalLength", "SepalWidth", "PetalLength", "PetalWidth"))

// Add Learner
pipeline.Add(new StochasticDualCoordinateAscentClassifier())

// Convert Label back to text
pipeline.Add(new ransforms.PredictedLabelColumnOriginalValueConverterPredictedLabelColumn = "PredictedLabel"))

//Train the model
let model = pipeline.Train&lt;IrisData, IrisPrediction&gt;()
```

#### Testing Our Model

Now that we have our data structures and model trained, it’s time to test it to make sure it's working. Following our training operation, we can add the following code.

```fsharp
// Test data for prediction
let testInstance = IrisData()
testInstance.SepalLength &lt;- 3.3f
testInstance.SepalWidth &lt;- 1.6f
testInstance.PetalLength &lt;- 0.2f
testInstance.PetalWidth &lt;- 5.1f

//Get Prediction
let prediction = model.Predict(testInstance)

//Output Prediction
printfn "Predicted flower type is: %s" prediction.PredictedLabel
```

Our final `Program.fs` file should contain content similar to that below:

```fsharp
open System
open Microsoft.ML
open Microsoft.ML.Runtime
open Microsoft.ML.Runtime.Api
open Microsoft.ML.Data
open Microsoft.ML.Transforms
open Microsoft.ML.Trainers

type IrisData() =

    [&lt;Column(ordinal = "0");DefaultValue&gt;] val mutable public SepalLength: float32
    [&lt;Column(ordinal = "1");DefaultValue&gt;] val mutable public SepalWidth: float32
    [&lt;Column(ordinal = "2");DefaultValue&gt;] val mutable public PetalLength:float32
    [&lt;Column(ordinal = "3");DefaultValue&gt;] val mutable public PetalWidth:float32
    [&lt;Column(ordinal = "4",name="Label");DefaultValue&gt;] val mutable public Label: string


type IrisPrediction() =

    [&lt;ColumnName "PredictedLabel";DefaultValue&gt;] val mutable public PredictedLabel : string

[&lt;EntryPoint&gt;]
let main argv =

    let dataPath = "./iris-data.txt"

    // Initialize Compute Graph
    let pipeline = new LearningPipeline()

    // Load Data
    pipeline.Add((new TextLoader(dataPath)).CreateFrom&lt;IrisData&gt;(separator=','))

    // Transform Data
    // Assign numeric values to text in the "Label" column, because
    // only numbers can be processed during model training
    pipeline.Add(new Transforms.Dictionarizer("Label"))

    // Vectorize Features
    pipeline.Add(new ColumnConcatenator("Features","SepalLength", "SepalWidth", "PetalLength", "PetalWidth"))

    // Add Learner
    pipeline.Add(new StochasticDualCoordinateAscentClassifier())

    // Convert Label back to text
    pipeline.Add(new Transforms.PredictedLabelColumnOriginalValueConverter(PredictedLabelColumn = "PredictedLabel"))

    //Train the model
    let model = pipeline.Train&lt;IrisData, IrisPrediction&gt;()

    // Test data for prediction
    let testInstance = IrisData()
    testInstance.SepalLength &lt;- 3.3f
    testInstance.SepalWidth &lt;- 1.6f
    testInstance.PetalLength &lt;- 0.2f
    testInstance.PetalWidth &lt;- 5.1f

    //Get Prediction
    let prediction = model.Predict(testInstance)

    //Output Prediction
    printfn "Predicted flower type is: %s" prediction.PredictedLabel
    0 // return an integer exit code
```

All set to run. We can do so by entering the following command from our solution directory:

```fsharp
dotnet run -p model/model.fsproj
```

Once the application has been run, the following output should display on the console.

```bash
Automatically adding a MinMax normalization transform, use 'norm=Warn' or 'norm=No' to turn this behavior off.
Using 2 threads to train.
Automatically choosing a check frequency of 2.
Auto-tuning parameters: maxIterations = 9998.
Auto-tuning parameters: L2 = 2.667734E-05.Auto-tuning parameters: L1Threshold (L1/L2) = 0.
Using best model from iteration 1066.Not training a calibrator because it is not needed.
Predicted flower type is: Iris-virginica
```

## Conclusion

In this post, we ported over a C# `ML.NET` classification model to F# which predicts the class of flower based on numerical measurement inputs. While several workarounds needed to be made, `ML.NET` is still in its infancy. As more people become involved and provide feedback hopefully in the near future, F# support and functionality will become more stable. Happy coding!
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/mlnet-classification-fsharp</link>
      <guid>https://www.luisquintanilla.me/posts/mlnet-classification-fsharp</guid>
      <pubDate>2018-06-13 18:19:05</pubDate>
    </item>
    <item>
      <title>Make predictions with ML.NET models without defining schema classes</title>
      <description>&lt;![CDATA[
## Introduction

To make predictions with ML.NET models you often have to define schema classes for your model inputs and outputs. In previous posts I wrote how you can [use Netron to inspect an ML.NET model](inspect-mlnet-models-netron.html) to determine the name and types of your model inputs and outputs. If you have data samples of what your data input and output look like in JSON format, you can [automate the generation of model input and output classes by using Visual Studio's "Paste JSON as Classes" feature](vs-automate-mlnet-schema-generation.html). However, what if you want to make predictions without defining these classes? In this post I'll show how you can use the .NET DataFrame API to make predictions with ML.NET models without having to create model input and output classes. Code snippets are in F# but notebooks with complete [C#](https://github.com/lqdev/mlnet-noschema-predictions/blob/main/CSharp-NB.ipynb) &amp; [F#](https://github.com/lqdev/mlnet-noschema-predictions/blob/main/FSharp-NB.ipynb) code can be found in the [mlnet-noschema-predictions repo](https://github.com/lqdev/mlnet-noschema-predictions) on GitHub.

## Install and reference packages

In addition to the [Microsoft.ML](https://www.nuget.org/packages/Microsoft.ML/) ML.NET NuGet package, you'll also need the [Microsoft.Data.Analysis](https://www.nuget.org/packages/Microsoft.Data.Analysis/) NuGet package to use the .NET DataFrame API. For more information on the .NET DataFrame API, see [an introduction to DataFrame](https://devblogs.microsoft.com/dotnet/an-introduction-to-dataframe/).

Once your packages are installed, reference them in your application.

```fsharp
open Microsoft.ML
open Microsoft.Data.Analysis
```

## Initialize MLContext and load the model

The `MLContext` is the entrypoint of ML.NET applications. Use it to load your model. The model used in this case categorizes sentiment as positive or negative. See the [use Netron to inspect an ML.NET model](inspect-mlnet-models-netron.html) blog post to learn more about the model.

```fsharp
let ctx = MLContext()
let model,schema = ctx.Model.Load("sentiment_model.zip")
```

Both the model and the input schema are returned when you load the model. The input schema is a [DataViewSchema](https://docs.microsoft.com/dotnet/api/microsoft.ml.dataviewschema?view=ml-dotnet) object containing a collection of [Columns](https://docs.microsoft.com/dotnet/api/microsoft.ml.dataviewschema.column?view=ml-dotnet).

## Define input and output column names

The input and output column names are for the DataFrames containing your input data and predictions. They help the model map the input and output values.

Use the `schema` which was loaded with the model to get the name of your input columns.

```fsharp
let inputColumnNames = 
    schema 
    |&gt; Seq.map(fun column -&gt; column.Name) 
    |&gt; Array.ofSeq
```

Since this is a binary classification model by default only two columns are returned as part of the prediction:

- Score
- PredictedLabel

You can create an array containing the names of these columns. For more information on default output columns, see the [ML.NET Tasks documentation](https://docs.microsoft.com/dotnet/machine-learning/resources/tasks#binary-classification-inputs-and-outputs).

```fsharp
let outputColumnNames = [| "PredictedLabel" ; "Score" |]
```

## Create input data for predictions

Use the [`LoadCsvFromString`](https://docs.microsoft.com/dotnet/api/microsoft.data.analysis.dataframe.loadcsvfromstring?view=ml-dotnet-preview#Microsoft_Data_Analysis_DataFrame_LoadCsvFromString_System_String_System_Char_System_Boolean_System_String___System_Type___System_Int64_System_Int32_System_Boolean_) method to load your input data into a DataFrame. In this case, there's only one column and data instance so I represent it as a string literal. Additionally, I provide the name of the input columns.

```fsharp
let sampleInput = "This was a very bad steak"

let inputDataFrame = 
    DataFrame.LoadCsvFromString(
        sampleInput, 
        header=false, 
        columnNames=inputColumnNames)
```

## Make predictions

Now that you've loaded your input data, it's time to use the model to make predictions.

```fsharp
let predictionDV = 
    inputDataFrame 
    |&gt; model.Transform 
```

Calling the [`Transform`](https://docs.microsoft.com/dotnet/api/microsoft.ml.itransformer.transform?view=ml-dotnet#Microsoft_ML_ITransformer_Transform_Microsoft_ML_IDataView_) method  returns an [`IDataView`](https://docs.microsoft.com/dotnet/api/microsoft.ml.idataview?view=ml-dotnet) with your predictions. You can then convert the `IDataView` into a DataFrame for further processing with the [`ToDataFrame`](https://docs.microsoft.com/dotnet/api/microsoft.ml.idataviewextensions.todataframe?view=ml-dotnet-preview) method.

```fsharp
let prediction = predictionDV.ToDataFrame(1L, outputColumnNames)
```

The resulting DataFrame should look something like the following:

| index | PredictedLabel | Score | 
|---|---|---|
|0 |False | -2.1337974 |

## Conclusion

If you want to load a model and make predictions without defining  classes for your input and output schema's you can load your data into a DataFrame using the .NET DataFrame API. While this solution works, because DataFrames and IDataViews process data differently, I haven't tested whether this solution would scale for larger data sets.
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/mlnet-predictions-no-schema</link>
      <guid>https://www.luisquintanilla.me/posts/mlnet-predictions-no-schema</guid>
      <pubDate>2021-09-16 18:00:00</pubDate>
    </item>
    <item>
      <title>Octave Development in the Terminal</title>
      <description>&lt;![CDATA[
MatLab is a powerful tool that is used througout various disciplines for data processing and analysis. However, in order to get access to it and all its features, there is a fee associated with it. Depending on the needs of the user, this price can at times be somewhat large. Luckily, an open source alternative known as Octave exists which works just as good in many cases at zero monetary cost. Octave's syntax is for the most part identical to MatLab making the transition a relatively simple one. 

# Tools
- Octave
- Tmux
- Text Editor (Emacs)

# Installation

## Tmux

```bash
sudo apt-get install tmux
```

## Octave

```
sudo apt-add-repository ppa:octave/stable
sudo apt-get update
sudo apt-get install octave
```

# Setup

##  Create a new tmux session

```bash
tmux new -s octave-ide
```

## Configure Layout

```ascii
=========================
|           |           |
| Console   |           |
|           |           |
|-----------|  Script   |
|           |           |
| Terminal  |           |
|           |           |
=========================
```

In order to get the layout shown above, the following steps need to be taken

### Split panes horizontally
`Ctrl+b %`

### Split panes vertically

#### Switch to pane 0   
`Ctrl+b q 0`

#### Do vertical split
`Ctrl+b "`

# Workflow

Now that the layout is setup, work can finally be done. To start off, initialize Octave in pane 0.

## Test code snippets in the console

Enter the following snippet in the console window

```octave
function res = square(x)
    res = x * x
endfunction
```

Then call it to make sure it does what is expected

```octave
square(2) #Should equal 4
```

## Write code on script file

Once satisfied that the code works:

1. Switch to pane 2 using `Ctrl+b q 2`
2. Create a script file `myscript.m`
3. Edit `myscript.m` with desired code

```octave
1; # Prevents octave from processing file as function file.

function res = square(x)
    res = x * x
endfunction

square(2);
```

4. Save the script

## Run script file

Switch to pane 1 `Ctrl+b q 1` and try to run the newly created script 

```bash
octave -W myscript.m
```
The output should look the same as that in the console window.

###### Sources

[Octave](http://wiki.octave.org/Octave_for_Debian_systems)
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/octave-development-in-the-terminal</link>
      <guid>https://www.luisquintanilla.me/posts/octave-development-in-the-terminal</guid>
      <pubDate>2017-12-09 12:46:54</pubDate>
    </item>
    <item>
      <title>Install OverDrive Media Console on Manjaro</title>
      <description>&lt;![CDATA[
## Introduction

For the last few months, Manjaro has been the operating system I use as my daily driver for personal tasks. I still have my Surface Go tablet which runs Windows 10 that I use for tasks line syncing my Garmin watch and GPS. On one of the [posts in my feed](/feed/ipod-touch-discontinued.html), I briefly mentioned my audio solution for podcasts and audiobooks which mainly consists of a standalone MP3 player ([FiiO M6](https://fiio.com/m6)). 

For podcasts, I manage them through my [NewsBlur RSS feed reader](https://newsblur.com/). When I want to download them for offline listening, I've written a .NET console app ([podnet](https://github.com/lqdev/podnet)) to manage that process for me. Audiobooks are a little trickier. Public domain works I get through [Librivox](https://librivox.org/). The limitation there though is that I'm only public domain works are available. Therefore if I want other audiobooks, I either have to purchase them (which has its own issues because of DRM) or borrow them from my local library. I prefer to borrow my books and audiobooks from the library. Not only am I supporting my local library but I also save money in the process. 

Many libraries including my own manage digital assets through [OverDrive](https://www.overdrive.com/). OverDrive offers a variety of ways for transferring books and audiobooks to your devices. Linux however is not one of those solutions. Until February 2022, OverDrive provided a Windows desktop application (OverDrive Media Console) for managing books and audiobooks. Although there is no Linux desktop application, Linux has Wine. [Wine](https://www.winehq.org/) according to their website is "...a compatibility layer capable of running Windows applications on several POSIX-compliant operating systems, such as Linux, macOS, &amp; BSD.". 

In this post, I'll show how you can use Wine to install OverDrive Media Console on a PC running the Manjaro Linux distribution. 

## Install Wine

In order to install OverDrive Media Console, you'll need to install Wine. To do so:

1. Open the terminal.
1. Install Wine and all its dependencies with the following command:

    ```bash
    sudo pacman -S wine winetricks wine-mono wine_gecko
    ``` 

## Install OverDrive Media Console

The next step is to install the OverDrive Media Console application for Windows desktop. 

1. Download the latest version of [OverDrive Media Console for Windows Desktop application](https://www.overdrive.com/apps/overdrive./). This should be a Microsoft Installer file with the `.msi` extension. Note that as of February 2022, the desktop application is [no longer available for install](https://help.overdrive.com/en-us/0733.html?tocpath=Home%7CGet%20help%20with%20the%20OverDrive%20app%7CWindows%20(desktop)%7C_____7).
1. Open the terminal and use Wine to install OverDrive Media Console with the following command:

    ```bash
    wine msiexec /i ODMediaConsoleSetup.msi
    ```

    Update the name of the `.msi` file as needed.  

## Run OverDrive Media Console

Once the installation is complete, OverDrive Media Console should be in your list of applications. Run it as you would any other app. 

![OverDrive Media Console running on Manjaro Linux](https://user-images.githubusercontent.com/11130940/167964715-bb8bfb21-5f72-4a86-9fb0-33c9d1fe9eae.png)

Here's a guide for information on getting started with [OverDrive](https://help.overdrive.com/en-us/categories/getting-started.htm).]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/overdrive-wine-manjaro</link>
      <guid>https://www.luisquintanilla.me/posts/overdrive-wine-manjaro</guid>
      <pubDate>05/11/2022 19:22</pubDate>
    </item>
    <item>
      <title>Proofread articles using Microsoft Edge's Immersive Reader and read aloud</title>
      <description>&lt;![CDATA[
This will be a relatively short post but it's something I just discovered and thought I'd share. Before publishing a post, I try to proofread and review the article to make sure spelling, grammar, and flow all make sense. Part of that process involves reading the article aloud. I'll miss things from time to time but generally it works well. 

I've known about read aloud in general, but have often found voices to be robotic and sound unnatural when they encounter words outside their vocabulary. However, I recently tried out the read aloud feature in Microsoft Edge and was blown away at how natural the voice sounds and even how well it enunciates technical terms like product and programming API names. Here's what it looks like when reading one of the posts on my site.

![Microsoft Edge Immersive Reader read aloud reading blog post](https://user-images.githubusercontent.com/11130940/134442715-e0af0c4a-a522-49a9-a0b1-ac658b284c68.png)

If you do any form of writing that can be displayed in Microsoft Edge, I highly encourage you to give read aloud a try. For more information, see [Use Immersive Reader in Microsoft Edge](https://support.microsoft.com/topic/use-immersive-reader-in-microsoft-edge-78a7a17d-52e1-47ee-b0ac-eff8539015e1).

Happy writing!]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/proofread-read-aloud</link>
      <guid>https://www.luisquintanilla.me/posts/proofread-read-aloud</guid>
      <pubDate>2021-09-22 18:00:00</pubDate>
    </item>
    <item>
      <title>Quick thoughts about Snapdragon Summit 2023</title>
      <description>&lt;![CDATA[
This started as a note where I was planning on posting a bullet point or two with my thoughts but kept on writing so I decided to turn it into a blog post.

Here's a few of my thoughts after the Snapdragon Summit 2023 keynote.

Today was the start of Snapdragon Summit 2023 and all I can say is, **WOW**. 

CNET has a [14 minute clip](https://www.youtube.com/watch?v=K7Q5iYHvgwo) that summarizes all the announcements. 

[![Qualcomm’s Snapdragon AI Event: Everything Revealed in 14 Minutes](http://img.youtube.com/vi/K7Q5iYHvgwo/0.jpg)](https://www.youtube.com/watch?v=K7Q5iYHvgwo "Qualcomm’s Snapdragon AI Event: Everything Revealed in 14 Minutes")

I was cautiously optimistic about what would be shown today but Qualcomm delivered. 

The main thing that I'm excited about are their Snapdragon X Elite Oryon-based CPUs.

As someone who does all of their computing on a laptop, I'd love to have a PC that I can take with me on the go. Now you might think that's what your phone is for, and for many people that is definitely the case. However, I only use my phone for calls and messages. I find it much more enjoyable to have a larger screen and keyboard.   

Keeping my preferences in mind, being able to connect to your PC on the go is nothing new either. I can just use the hotspot on my phone or tether over USB in order to connect my PC to the internet. Also, SIM card slots in PCs have been a thing for a while and products like the [Surface Pro 9 with 5G](https://www.microsoft.com/d/surface-pro-9/93vkd8np4fvk?activetab=pivot:overviewtab) provide an even more integrated experience. 

However, in order to truly make this experience mobile, I need to be untethered. Just like a mobile phone, I need my device battery to last all-day while on the go so I don't have to carry my charger. To do that, either you need a device with a large battery or a low-power chip. Larger battery immediately takes the mobile out of mobile computing. I recently carried around my 16" work laptop for a while walking in the city and that was no fun. So then low-power chip it is. Problem solved. Not quite, because in order for it to be low-power, performance needs to be throttled. So for the longest time, it's been a constant struggle between power-efficiency and battery life. 

Apple was the first to solve this problem with their M1 line of chips. However, that meant this innovation was confined to the Apple ecosystem. It's not until now with the Oryon CPUs that Windows (and hopefully soon Linux too) can tap into these types of computing platforms that promise to deliver true mobile computing. This now opens up a world of possibilities when we think about mobile computing. Our phones keep getting [larger in size](https://www.theverge.com/23913658/best-small-phone-dead-iphone-mini-z-flip-pixel-8) that at this point it's harder to carry them in your pocket or use them with one hand. We've seen a surge in interest in handheld gaming devices like the [SteamDeck](https://www.steamdeck.com/) which are full PCs. Their problem is they're using chips that are not power efficient, especially because of their requirements for gaming. 

What happens if you replace the current chips in those devices with Snapdragon X Elite? Now that it's high-performance and power efficient, does your handheld gaming device now become your handheld computing device? And if we're in handheld computing device territory, now all of a sudden it's bumping up against what smartphones do today. So at that point, do we need both a smartphone and a handheld computing device? If I was able to handle calls and messages from my PC then I wouldn't need a phone and I'd much rather just carry a single handheld computing device. Considering that smartphones are general computing devices that are serverely locked down and restricted, I'd much rather go for the general computing type of device like a PC because I have more freedom over what I can do with my PC. More importantly, because it's a full PC, it means when I need to dock down for serious work with a keyboard and multiple monitors, I can plug a USB-C Hub and connect all of my peripherals to the device. This was something projects like [Continuum](https://learn.microsoft.com/windows-hardware/design/device-experiences/continuum-phone) tried to achieve but were unsuccessful doing for various reasons. Because now your PC is now also your mobile device, you take a big problem with these type of experiences which is reconciling mobile and desktop UX patterns.   

What about apps? Not exactly sure what this looks like in the near-term but considering devices running Oryon CPUs will take some time to get to market, by using the AI capabilities you can think of a future where AI is the interface and the apps become background services for interactive with external systems. So no need for specific apps because you can express intent and complete tasks using natural language. 

What about form factors? The steamdeck might not be what folks are looking for when it comes to a mobile device. Foldables are seeing an uptick in adoption. Despite it's lack of commercial success, the Surface Duo was an excellent foldable form factor that was compact but functional for productivity. I think the Surface Duo was misunderstood because it was put into the box of mobile phone. As a mobile phone it didn't work as well. But as a mobile computer, it struck the right form factor that enabled you to be productive on the go while still being portable. Needed more screen real estate? Surface Neo or Courier would've been  nice devices to have. 

Let's go 3 years into the future. What happens when virtual and augmented reality headsets all of a sudden become as small as the [Ray-Ban Meta Smartglasses](https://www.ray-ban.com/usa/ray-ban-meta-smart-glasses)? Do we even need handheld PCs at that point if all we need is for the headset to display the screens we're looking at. Bring in IoT. If the items around us are also "smart" devices, all of a sudden your environment becomes your compute platform. Bring AI into the mix and your AI assistant has access to your environment.  

I'd say the biggest miss from Qualcomm and Windows partners today was not having a lineup of devices ready for purchase. That will come with time. Hopefully not too long after these announcements. I'm sure that Apple which has an announcement coming up on October 30th will have a response. The most important thing though is that now there's competition and as we enter this new era of AI and TRUE mobile computing, it'll lead to innovation. The next things that need to happen is operating systems like Windows need to ensure their existing applications work on these platforms and Linux needs to enable support for these chipsets. I'm so excited for the future of mobile computing and can't wait to be able to carry a device running Linux with always-on connectivity and all-day battery life in the form factor of a Surface Duo. 

An addendum to this post can be found [here](/feed/snapdragon-summit-2023-addendum).]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/quick-thoughts-snapdragon-summit-2023</link>
      <guid>https://www.luisquintanilla.me/posts/quick-thoughts-snapdragon-summit-2023</guid>
      <pubDate>2023-10-24 23:14</pubDate>
    </item>
    <item>
      <title>Read A Text File From a URL in .NET Core</title>
      <description>&lt;![CDATA[
# Create New .NET Core Project

```bash
dotnet new console -o urlreader
```

# Navigate to Project Folder
```bash
cd urlreader
```

# Import Dependencies

```csharp
using System.IO;
using System.Net.Http;
using System.Threading.Tasks;
```

# Create Reader

Create a function in the `Program.cs` file that will return a `StreamReader` from the specified URL. Because in this case it will be used inside the `Main` method which is `static`, the new function will also have to be `static`. 

```csharp
static StreamReader URLStream(String fileurl){
    return new StreamReader(new HttpClient().GetStreamAsync(fileurl).Result);
}
```

Inside the function, we return a `StreamReader` which is instantiated by creating an `HTTPClient` and asynchronously executing a `GET` request which returns a `Task&lt;StreamReader&gt;`. In order to resolve the object, the `Result` property needs to be requested.


# Reading File Contents

Replace the contents of the `Main` method with the following

```csharp
static void Main(string[] args) {
    string line;
    StreamReader s = URLStream(@"https://algs4.cs.princeton.edu/15uf/tinyUF.txt");
    String myline = s.ReadLine(); //First Line
    while((line = s.ReadLine()) != null) //Subsequent Lines
    {
        Console.WriteLine(line);
    }
}
```

In this case, the format of the first line is different from the others. However, reading the contents of that line is no different than the other lines. 

# Run The Program

```bash
dotnet run

#Expected Output
#4 3
#3 8
#6 5
#9 4
#2 1
#8 9
#5 0
#7 2
#6 1
#1 0
#6 7
```


]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/read-file-from-url-dotnet-core</link>
      <guid>https://www.luisquintanilla.me/posts/read-file-from-url-dotnet-core</guid>
      <pubDate>2017-12-18 21:09:15</pubDate>
    </item>
    <item>
      <title>Reading Local JSON Files in R</title>
      <description>&lt;![CDATA[
The following example is a way of reading local JSON files into R scripts. This can be useful for config files, although it's still also acceptable for data files as well.

# Import package to work with JSON and lists

```r
require(rlist)
require(rjson)
```

# Load file into script

```r
configPath &lt;- '.'
fileName &lt;- 'config.json'
jsonFile &lt;- fromJSON(file=paste(configPath,fileName,sep="/"),method="C")
```

The data gets loaded in as a list. It may be tedious to access nested contents via indices. To aid with this, a helper function can be created to extract the individual nested data elements of the file.

```r
#Extract values form nested list
#top_level = list.select(obj$topkey,obj$subkey)
extractData &lt;- function(top_level) {
    output &lt;- unlist(lapply(top_level,'[[',1));
    return(output);
}
```

# Extract data 

Below is a default sample json file that includes both nested and non-nested data.

```json
{
    "port": 2100,
    "connectionStrings": {
        "default": "123.567.890"
    }
}
```

The data in that file can be accessed as shown below.

```r
#Not Nested
port &lt;- config$port # Should store 2100

#Nested
connectionString &lt;- extractData(list.select(config$connectionStrings,default)) #Should store 123.567.890
```]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/reading-json-in-r</link>
      <guid>https://www.luisquintanilla.me/posts/reading-json-in-r</guid>
      <pubDate>2017-12-11 14:21:21</pubDate>
    </item>
    <item>
      <title>Real-Time Sentiment Analysis with C#</title>
      <description>&lt;![CDATA[
###### This is strictly for use with the .NET Framework. With Mono it might be able to work on other platforms. `SimpleNetNlp` does not currently work with .NET Core/Standard

In this project, I will demonstate how to perform sentiment analysis on tweets using various C# libraries.

## Dependencies

- TweetInviAPI
- SimpleNetNlp
- SimpleNetNlp.Models.LexParser
- SimpleNetNlP.Models.Sentiment

## Create Console Application

In Visual Studio, Click File &gt; New &gt; New Project &gt; Console Application

All of the code below will be placed in the `Program` class.

## Creating A Stream

### Authenticate

Thanks to the `Tweetinvi` library, the authentication with the Twitter API is a breeze. Assuming that an application has been registered at [http://apps.twitter.com](http://apps.twitter.com), the `SetUserCredentials` method can be used and the `Consumer Key`, `Consumer Secret`,`Access Token` and `Access Token Secret` can be passed into it. This type of global authentication makes it easy to perform authenticated calls throughout the entire application.

```csharp
Auth.SetUserCredentials("consumer-key","consumer-secret","access-token","access-token-secret");
```

### Build Stream

Like the authentication, creating a stream is seamless.

We can create a stream by calling the `CreateFilteredStream` method.

```csharp
var stream = Stream.CreateFilteredStream();
```

We can then add conditions to filter on using the `AddTrack` method. In this case, I will be filtering for cryptocurrencies, ether, bitcoin, and litecoin.

```csharp
stream.AddTrack("cryptocurrencies");
stream.AddTrack("bitcoin");
stream.AddTrack("ether");
stream.AddTrack("Litecoin");
```

Additionally, we can filter by language. In my case, I will only be filtering on English. This can be done by using the `AddTweetLanguageFilter` method.

```csharp
stream.AddTweetLanguageFilter("en");
```

Once we have all the filters set up, we need to handle what will happen when a matching tweet is detected. This will be handled by an `EventHandler` called `MatchingTweetReceived`.

```csharp
stream.MatchingTweetReceived += OnMatchedTweet;
```

`MatchingTweetReceived` will be bound to the `OnMatchedTweet` method which I created.

```csharp
private static void OnMatchedTweet(object sender, MatchedTweetReceivedEventArgs args)
{
  //Do Stuff
}
```

The logic inside of this method will perform sentiment analysis and output the sentiment as well as the full text of the tweet.

## Data Cleaning

Tweets can contain many non-ascii characters. Therefore, we need to sanitize it as best as possible so that it can be processed by the sentiment analyzer. To help with that, I used regular expresions to replace non-ascii characters inside of the `sanitize` method.

```csharp
private static string sanitize(string raw)
{
  return Regex.Replace(raw, @"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ").ToString();
}
```

## Sentiment Analysis

In order to perform sentiment analysis, we will be using the `SimpleNetNlp` library. This library is built on top of the Stanford CoreNLP library. In order to get the sentiment of a piece of text, we need to create a `Sentence` object which takes a string as a parameter and then get the `Sentiment` property. In our case, the parameter that will be used to instantiate a new `Sentence` object will be the sanitized text of a tweet.

```csharp
var sanitized = sanitize(args.Tweet.FullText);
string sentence = new Sentence(sanitized);
```

The code above will be placed inside of the `OnMatchedTweet` method.

## Produce Output

Now that we have everything set up, we can just output to the console the sentiment and raw text of the tweet. To do that, we can place the code below inside the `OnMatchedTweet` method.

```csharp
Console.WriteLine(sentence.Sentiment + "|" + args.Tweet);
```

The final `OnMatchedTweet` method looks as follows:

```csharp
private static void OnMatchedTweet(object sender, MatchedTweetReceivedEventArgs args)
{
    var sanitized = sanitize(args.Tweet.FullText); //Sanitize Tweet
    var sentence = new Sentence(sanitized); //Get Sentiment

    //Output Tweet and Sentiment
    Console.WriteLine(sentence.Sentiment + "|" + args.Tweet);
}
```
## Run

Once we run the application, our console application should look something like this:

![](/images/real-time-sentiment-analysis-csharp/sentiment-analysis-1.png)


## Conclusion

C# is not always the first language that comes to mind when doing analytics and machine learning. However, tasks such as sentiment analysis can be trivially performed thanks to libraries such as `Tweetinvi` and `SimpleNetNlp`. In its current state, this application is not very useful because it just outputs to the console sentiments and the respective tweets. In order to make it more useful, we can collect and aggregate the data someplace for more robust analysis. 
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/real-time-sentiment-analysis-csharp</link>
      <guid>https://www.luisquintanilla.me/posts/real-time-sentiment-analysis-csharp</guid>
      <pubDate>2018-01-18 03:41:47</pubDate>
    </item>
    <item>
      <title>Accept Webmentions using F#, Azure Functions, and RSS</title>
      <description>&lt;![CDATA[
# Introduction 

&gt; This post is part of [F# Advent 2022](https://sergeytihon.com/2022/10/28/f-advent-calendar-in-english-2022/). Thanks to [Sergey Tihon](https://sergeytihon.com) for organizing it. 

While not planned this way, this post is timely considering recent developments in the social media space. While Twitter is in the headlines, it's not the only time people have questioned their engagement and relationship with these social media platforms. In the context of Twitter though, users have been looking for alternative platforms to best serve their communities. While Mastodon and the Fediverse appear to be one of those alternatives, they're not the only one. Blogosphere platforms like Tumblr and WordPress, both owned by Automattic, are also positioning themselves as viable alternatives to the point they've [considered the idea of supporting ActivityPub](https://techcrunch.com/2022/11/21/tumblr-to-add-support-for-activitypub-the-social-protocol-powering-mastodon-and-other-apps/). ActivityPub is the protocol that powers many of the platforms on the Fediverse. However, it's not the only open protocol that enables communication and engagement on the open web. Webmentions is another protocol for enabling conversations on the web which I'll talk about more in this post. 

I'll also discuss how I used F#, Azure Functions, and RSS to implement a solution that helps me and my website engage in conversations on the open web thanks to Webmentions. 

## What are Webmentions?

First, let's start by talking a bit about Webmentions. 

According to the W3C specification, Webmentions are "...a simple way to notify any URL when you mention it on your site. From the receiver's perspective, it's a way to request notifications when other sites mention it.". 

One of the goals according to the IndieWeb wiki is to enable cross-site conversations.  

Here's how a potential Webmention workflow might look like:

```mermaid
sequenceDiagram
  autonumber
  actor A as Alice
  participant AS as Alice's "Website"
  participant AW as Alice's Webmention Service
  participant BS as Bob's "Website"
  actor B as Bob
  A -&gt;&gt; AS: Publishes post
  B -&gt;&gt; AS: Reads post
  B -&gt;&gt; BS: Publishes post with link to Alice's post
  rect rgb(230,230,230)
    alt Automated
      BS -&gt;&gt; AS: Fetch Webmention endpoint
      BS -&gt;&gt; AW: Sends Webmention
    else Manual
      B -&gt;&gt; AS: Fetch Webmention endpoint
      B -&gt;&gt; AW: Sends Webmention
    end 
  end
  rect rgb(230,230,230)
      note over AW:Validates Webmention request
      note over AW: Validates Webmention
      note over AW: Accepts / processes Webmention
      alt Automated
        AW -&gt;&gt; BS: Sends back response
      else Manual
        AW -&gt;&gt; B: Sends back response
      end
  end 
  opt Display Webmentions
    AS -&gt;&gt; AW: Fetch Webmentions
    note over AS: Display / publish Webmentions
  end
```

I use the term website here very loosely because it's not only limited to your own personal website. You can technically use Webmentions through any public website including social media platforms like Twitter, Mastodon, or even GitHub.

This post will focus on the middle blocks consisting of sending and receiving Webmentions. For more information, check out the [specification](https://www.w3.org/TR/webmention/#abstract-p-1) and [Webmentions page](https://indieweb.org/Webmention) on the IndieWeb wiki. 

## My webmention solution

In September of this year, I [integrated a partial implementation of Webmentions](/feed/webmentions-partially-implemented/) into my website. This implementation only allowed me to send webmentions as part of my publishing process. If you're interested in the details, it's based on the blog post I wrote for last year's F# Advent, [Sending Webmentions with F#](/posts/sending-webmentions-fsharp-fsadvent/).

A couple of days ago, I [successfully deployed and integrated](/feed/now-accepting-webmentions/) the other part of the implementation which allowed me to receive webmentions. While there are many great [services](https://indieweb.org/Webmention#Publisher_Services) which will do this for you, I decided to roll my own for fun and to learn something new. 

To help with that, I released [WebmentionFs](https://www.nuget.org/packages/lqdev.WebmentionFs), an open-source .NET library written in F# with components for sending and receiving Webmentions.

The backend service I use to receive webmentions is built on Azure Functions. 

If you're interested, here's the source code for [WebmentionFs](https://github.com/lqdev/WebmentionFs) and my [Webmention service](https://github.com/lqdev/WebmentionService). 

Before diving into the technical details of the solution, I'll first discuss some of the priorities driving my design decisions. 

### Requirements

While I don't want to trivialize it, at the end of the day, the Webmention service is just an endpoint that accepts HTTP POST requests. That means there's many ways to build it. However, here were the requirements important to me that guided my design choices:

- **Cost:** The solution should be low cost
- **Maintenance:** The solution should be easy to maintain
- **Easy to consume:** I have multiple ways to view and access my webmentions.
- **Notifications first:** Since I don't own or have responsibility over content that's not mine, prioritize Webmentions for notifications, not content.

With that in mind, I ended up choosing a solution built on Azure Functions to receive Webmentions and publish them as an RSS feed. 

### Why Azure Functions?

I don't make money from my website. It's a labor of love in which I document my experiences. Selfishly, it's for me to remind myself how to solve a problem or share something that was important to me at that time. If my posts end up helping others solve their own problems, that's a side effect and a reward unto itself. To that effect, I want my website to be as low-cost to run as possible. One way I achieve that is by using static hosting. This dramatically reduces the cost associated with my site. If you're interested in how this site is built, you can check out the [colophon](/colophon).

So where do Azure Functions come in? 

Azure Functions is a serverless solution which means there are no servers! (I kid :smirk:)

#### Cost :heavy_check_mark:

As a serverless solutions, it means web services are not constantly running. They only run whenever there is something to process so I only pay for what I need. That takes care of my low-cost requirement since processing Webmentions is not a long-running, resource-intensive operation that can easily run in a serverless environment.

#### Maintenance :heavy_check_mark:

Cost is not only monetary. There are also time costs associated with maintaining a solution. Because there is no infrastructure to maintain, that's not really a factor I need to consider. Instead I get to focus on writing the logic for accepting and processing Webmentions. Azure Functions provides a set of [prebuilt trigger and binding components](https://learn.microsoft.com/azure/azure-functions/functions-triggers-bindings) to reduce the amount of code I have to write to handle things like accepting HTTP requests or storing data in a table or blob storage. Also, in the rare instance that this blog post breaks the internet, Azure Functions enables my service to automatically handle that scale. Don't believe me? Check out this post frorm [Troy Hunt](https://www.troyhunt.com/) on [Have I Been Pwned usage of Azure Functions](https://www.troyhunt.com/serverless-to-the-max-doing-big-things-for-small-dollars-with-cloudflare-workers-and-azure-functions/).

### Why RSS?

The choice of how to display and consume Webmentions is entirely up to you. At regular intervals, my Webmention service generates an RSS feed and publishes it to a private Azure Blob Storage. Here are some of the reasons why:

#### Easy to consume :heavy_check_mark:

The decision to use RSS comes from the fact that like Webmentions, it's an open protocol that gives choice to creators and consumers. Check out my post, [Rediscovering the RSS protocol](/posts/rediscovering-rss-user-freedom) for more of my thoughts on that. Despite the appearance of decline in use, it's a protocol that's been around for over 20 years and it's one of the many reasons why podcasts continue to be one of the more open platforms despite some attempts to lock down the ecosystem.  With that it mind, by choosing RSS as the publishing format for my Webmentions feed, I can lean on the vast number of RSS readers ([NewsBlur](http://newsblur.com/) and [Elfeed](https://github.com/skeeto/elfeed) being my favorites) out there instead of having to write a custom UI. Not happy with my RSS reader? I can just add my feed to whichever other reader I move to and continue consuming my Webmentions without skipping a beat. 

### Notifications first :heavy_check_mark:
 
While not technical or specific to RSS, this is a topic I wrestled with as part of my implementation. However, choosing RSS made many parts of it simpler. 

Who owns your content? If you publish on your own website or a platform under your control, chances are you do. If I publish and redistribute your content on my website though, what parts do I own and which parts do you? I'm not sure. What about spam, harrassment, and lewd content? I'd have to put in place some sort of moderation instead of publishing directly on my site. I didn't want to have to deal with any of these questions so choosing to publish to a private RSS feed instead of directly on my site made it so Webmentions are for my eyes only. More importantly, I only capture the source and target links, not the content. Therefore, I don't store or redistribute your content. Now there's nothing stopping anyone from seeing your post since it's public but in cases where your site's content is not something I'm comfortable with, I'm not resharing or redistributing that content. If my RSS reader chooses to display the full contents of your post by using the `link` element from the RSS feed, since your post is public, that's no different than me viewing the original post on your site. If at some point I feel differently about it, the changes to capture the content in the `description` happen at the RSS file level and because the RSS is private, I'm still not redistributing your content. 

In short, with my implementation I chose to prioritize Webmentions for notification purposes rather than building a commenting and interaction (like, reshare) system for my website. 

Now that I've talked about the what and the why, in the next sections I'll get into the how. 

## Sending Webmentions

Once you've created a post linking to someone else's website, sending webmentions is a two-step process:

1. URL discovery of webmention endpoint
2. Send webmention

These steps can be automated or performed manually. 

### URL discovery of Webmention endpoint

The first thing you'll need is the endpoint where you'll be sending your Webmention to.

According to the Webmention specification, a Webmention endpoint can be found in one of three places:

- HTTP response headers
- `&lt;link&gt;` tag with `rel=webmention` attribute
- `&lt;a&gt;` tag with `rel=me` attribute

When this process is automated, the discovery is performed in order and each subsequent option works as a fallback of the other. When done manually, either the website author will publish and advertise this on their website or you can inspect the website's source code to find it.  

### Send webmention

To send a Webmention, you need to send an HTTP POST request with an `x-www-form-urlencoded` body containing two properties: `source` and `target`. Source is the URL of your article and target is the URL of the original article. Again, this too can either be automated or done manually with tools like cURL, Insomnia, or any other HTTP client.  

### Sending Webmentions in practice

Let's use a redundant example to walk through the workflow. Pretend that you created or commented on a GitHub issue and added a link to one of my articles, [Sending Webmentions with F#](/posts/sending-webmentions-fsharp-fsadvent/) for example. You then wanted to notify me either to get my attention on that issue or just as an FYI. Now I say this is a redundant example because GitHub already has notifications. However, what this does illustrate are the cross-site capabilities of Webmentions. 

![Image of GitHub Issue Post](/images/receive-webmentions-fsharp-az-functions-fsadvent/send-gh-issue.png)

At this point, if GitHub implemented Webmentions, it would automatically:

1. Fetch my webpage and look for my Webmention endpoint.
1. Compose and send the HTTP POST request.  

Sadly, today that's not the case so we'll walk through the workflow manually.  

On my website, I include a `link` tag with the `rel=webmention` attribute that looks like the following:

```html
&lt;link rel="webmention" title="Luis Quintanilla Webmention Endpoint" href="https://lqdevwebmentions.azurewebsites.net/api/inbox"&gt;
```

My current Webmention endpoint is *https://lqdevwebmentions.azurewebsites.net/api/inbox*.

Now that you have the Webmention endpoint, you could use any tool of your choice to create and send the HTTP POST request. I have made it simpler though and included a form on my website for each of my posts for this purpose. 

![luisquintanilla.me send Webmention form](/images/receive-webmentions-fsharp-az-functions-fsadvent/send-webmention-form.png)

The source code looks like the following:

```html
&lt;div&gt;
    &lt;script type="application/javascript"&gt;window.onload = function() { document.getElementById('webmention-target').value = window.location.href }&lt;/script&gt;
    &lt;form action="https://lqdevwebmentions.azurewebsites.net/api/inbox" method="POST" enctype="application/x-www-form-urlencoded"&gt;
        &lt;h5 class="text-center"&gt;Send me a &lt;a href="https://indieweb.org/webmentions"&gt;webmention&lt;/a&gt;&lt;/h5&gt;
        &lt;div class="form-row justify-content-center"&gt;
            &lt;div class="w-75"&gt;
                &lt;input type="text" name="source" class="form-control" placeholder="Your URL (source)"&gt;
            &lt;/div&gt;
            &lt;div class="col-auto"&gt;
                &lt;input type="submit" class="btn btn-primary" value="Send"&gt;
            &lt;/div&gt;
            &lt;input readonly="" class="form-control-plaintext" style="visibility:hidden" type="text" id="webmention-target" name="target"&gt;
        &lt;/div&gt;
    &lt;/form&gt;
&lt;/div&gt;
```

Now to send the webmention, navigate to the post you want to send a Webmention to. In this case, it's [Sending Webmentions with F#](https://www.luisquintanilla.me/posts/sending-webmentions-fsharp-fsadvent/). Then, paste the link to the GitHub issue that contains the link to my site into the post's text box and hit "Send". 

![luisquintanilla.me send Webmention form filled-in](/images/receive-webmentions-fsharp-az-functions-fsadvent/send-webmention-form-populated.png)

At that point, the request is sent to my Webmention service which receives and processes the request and responds with a message whether the request was successful or not. 

That's all there is to it! Before moving the the receiving process though, I'll briefly show how to send Webmentions using WebmentionFs. 

### Sending Webmentions with WebmentionFs

In WebmentionFs, you can use the [`UrlDiscoveryService`](https://github.com/lqdev/WebmentionFs/blob/main/UrlDiscoveryService.fs) and [`WebmentionSenderService`](https://github.com/lqdev/WebmentionFs/blob/main/WebmentionSenderService.fs) to send Webmentions. If you wanted to integrate sending Webmentions into your application or service, here's an example that shows how you'd use those components.

```fsharp
open System
open WebmentionFs
open WebmentionFs.Services

let ds = new UrlDiscoveryService()

let ws = new WebmentionSenderService(ds)

let data = 
    {   
        Source = new Uri("https://twitter.com/ljquintanilla/status/1603602055435894784")  
        Target = new Uri("https://www.luisquintanilla.me/feed/mastodon-hashtag-rss-boffosocko")
    }

ws.SendAsync(data) |&gt; Async.AwaitTask |&gt; Async.RunSynchronously
```

The main part to pay attention to is the `SendAsync` function. This uses the `UrlDiscoveryService` to perform the Webmention endpoint discovery and send the HTTP POST request with the provided data. 

## Receiving webmentions

Now that you know how to send webmentions, let's talk about what happens on the receiver end. At its core, it's about a three-step process:

1. Validate request
1. Validate Webmention
1. Process Webmention

### Validate request

Before processing the Webmention, you want to make sure that the request is valid. To do so there are a few checks you perform:

1. Is the protocol valid? Check that the protocol for the source and target URLs in the request body are HTTP or HTTPS.
1. Are the URLs the same? Check that the source and target URLS in the request body are NOT the same. 
1. Is the target URL a valid resource? This is more loosely defined and up to you. In my case I defined valid as: 
    - I own the domain
    - The target URL doesn't return an HTTP 400 or 500 error. 

If the request is not valid, nothing is done with the Webmention and an error returns. Otherwise, it moves to the next step which is Webmention validation. 

### Validate Webmention

In the base case, Webmention validation just means you parse through the source document and make sure that the target URL is included in it. However, if you wanted to get more complex and take into account specific types of interactions such as likes, replies, reposts, bookmarks, etc. you can parse through the document looking for [microformat](http://microformats.org/wiki/h-entry#Core_Properties) annotations. For more information on these annotations, see the [Types of Posts](https://indieweb.org/posts#Types_of_Posts) or [reply](https://indieweb.org/reply#Post_a_reply) pages in the IndieWeb Wiki. 

If the target link is NOT in the source document, you return an error. Otherwise, you move to the next step of processing the Webmention

### Processing Webmentions

This step is optional and loosely defined. Once you've validated the request and Webmention, you can do with it as you please. One thing you can do is publish it (after moderation) on your website. 

In my case, I'm storing it in an Azure Tables database. Then, every morning, I generate an RSS feed of all my webmentions and save it to a private container on Azure Blob Storage.  

## Reciving Webmentions with WebmentionFs

To receive Webmentions, I use the [`RequestValidationService`](https://github.com/lqdev/WebmentionFs/blob/main/RequestValidationService.fs), [`WebmentionValidationService`](https://github.com/lqdev/WebmentionFs/blob/main/WebmentionValidationService.fs) and [`WebmentionReceiverService`](https://github.com/lqdev/WebmentionFs/blob/main/WebmentionReceiverService.fs) inside my Azure Function. 

The source code for my Azure Function is available in the [WebmentionService GitHub repo](https://github.com/lqdev/WebmentionService).

The main components are in the `Startup.fs` and `ReceiveWebmention.fs` files. 

The [`Startup.fs`](https://github.com/lqdev/WebmentionService/blob/main/Startup.fs) files registers each of the services I'll use to to receive Webmentions. By doing so, I define the initialization logic for use later in my application using dependency injection. The main thing to note here is that the `WebmentionReceiverService` is initialized with instances of a `RequestValidationService` and `WebmentionValidationService`. For the `RequestValidationService`, it's initialized with a list of host names. In this case, I'm passing it in as a comma-delimited environment variable that looks something like "lqdev.me,luisquintanilla.me". This is what the `RequestValidationService` uses to check whether I own a domain.

```fsharp
// Add request validation service
builder.Services.AddScoped&lt;RequestValidationService&gt;(fun _ -&gt; 

    let hostNames = Environment.GetEnvironmentVariable("PERSONAL_WEBSITE_HOSTNAMES")

    let hostNameList = hostNames.Split(',')

    new RequestValidationService(hostNameList)) |&gt; ignore


// Add webmention validation service
builder.Services.AddScoped&lt;WebmentionValidationService&gt;() |&gt; ignore

// Add receiver service
builder.Services.AddScoped&lt;IWebmentionReceiver&lt;Webmention&gt;,WebmentionReceiverService&gt;(fun (s:IServiceProvider) -&gt;
    let requestValidationService = s.GetRequiredService&lt;RequestValidationService&gt;()
    let webmentionValidationService = s.GetRequiredService&lt;WebmentionValidationService&gt;()
    new WebmentionReceiverService(requestValidationService,webmentionValidationService)) |&gt; ignore
```

Then, in the [`ReceiveWebmention.fs`](https://github.com/lqdev/WebmentionService/blob/main/ReceiveWebmention.fs) file, I inject my `WebmentionReceiverService` into the constructor and add all the logic for running my serverless function in the `Run` function. In this snippet, some code is ommitted for brevity.

```fsharp
type ReceiveWebmention (webmentionReceiver: IWebmentionReceiver&lt;Webmention&gt;) = 

    ///...

    [&lt;FunctionName("ReceiveWebmention")&gt;]
    member x.Run 
        ([&lt;HttpTrigger(AuthorizationLevel.Anonymous, "post", Route = "inbox")&gt;] req: HttpRequest) 
        ([&lt;Table("webmentions", Connection="AzureWebJobsStorage")&gt;] t: TableClient)
        (log: ILogger) =
        task {
            
            log.LogInformation("Processing webmention request")

            let! validationResult = x.WebmentionReceiver.ReceiveAsync(req)

            let response = 
                match validationResult with
                | ValidationSuccess m -&gt; 
                    let entity = mapMentionToTableEntity m
                    try
                        t.AddEntity(entity) |&gt; ignore
                        OkObjectResult("Webmention processed successfully") :&gt; IActionResult
                    with
                        | ex -&gt; 
                            log.LogError($"{ex}")
                            BadRequestObjectResult($"Error processing webmention. Webmention already exists") :&gt; IActionResult
                | ValidationError e -&gt; 
                    log.LogError(e)
                    BadRequestObjectResult(e) :&gt; IActionResult

            return response

        }
```

The `Run` function is triggered whenever an HTTP POST request is made to my Webmention endpoint. It then calls `ReceiveAsync` on the `WebmentionReceiverService` which under the hood uses the `RequestValidationService` and `WebmentionValidationService` for validation. For reference, here's the code that does that in WebmentionFS.

```fsharp
member x.ReceiveAsync (data:UrlData) = 
    task {
        let! requestValidationResult = 
            x.RequestValidationService.ValidateAsync data

        match requestValidationResult with
        | RequestSuccess r -&gt; 
            let! webmentionValidationResult = 
                x.WembentionValidationService.ValidateAsync r.Source r.Target

            let (result:ValidationResult&lt;Webmention&gt;) = 
                match webmentionValidationResult with
                | AnnotatedMention m -&gt; 
                    ValidationSuccess {RequestBody = r; Mentions = m}
                | UnannotatedMention -&gt; 
                    ValidationSuccess 
                        {
                            RequestBody = r
                            Mentions = 
                                {
                                    IsBookmark = false
                                    IsLike = false
                                    IsReply = false
                                    IsRepost = false        
                                }
                        }
                | MentionError e -&gt; ValidationError e
            return result
        | RequestError e -&gt; return ValidationError e
    }
```

&gt; A quick side-note. I love how elegant function composition in F# makes the request validation pipeline look. See for yourself :slightly_smiling_face:
&gt; 
&gt; ```fsharp
&gt;let validateAsync = 
&gt;    isProtocolValid &gt;&gt; isSameUrl &gt;&gt; isTargetUrlValidAsync
&gt;```

Inside my `Run` function, if the request and Webmention validation are successful, I use the Azure Table Storage table client `AddEntity` method to save the Webmention to a table. One thing to note is that `AddEntity` ensures that an entry is only added once. If an entry already exists in the table, it returns an error. 

This is what the GitHub Webmention we sent looks like in the table.

![Webmentions table on Azure Table Storage](/images/receive-webmentions-fsharp-az-functions-fsadvent/webmentions-azure-table.png)

In this case, the `PartitionKey` is the target URL and `RowKey` is the source URL. The fact all other columns are set to false indicates this is an untagged mention with no special microformats annotations. Annotating your posts appropriately would set one or more of those columns to true. 

Now that we went over how I receive Webmentions, I'll show how I prepare my Webmentions for consumption. 

## Viewing Webmentions

I consume my Webmentions using RSS feeds and RSS readers. To do so, I have an Azure Function called [`WebmentionToRss`](https://github.com/lqdev/WebmentionService/blob/main/WebmentionToRss.fs) that triggers every day at 3 AM. This function queries the Azure Table Storage table containing my Webmentions and creates an RSS feed that's then stored to Azure Blob Storage. Like the other function to process my Webmentions, I:

Register the `RssService` responsible for generating the RSS feed in `Startup.fs`.

```fsharp
// Add RSS service
builder.Services.AddScoped&lt;RssService&gt;() |&gt; ignore
```

Inject the service into my `WebmentionToRss` constructor. Then in my `Run` function, I call the `getMentions` helper function to query the table, call `BuildRssFeed` to generate the feed, and write it out to Azure Blob Storage using a `Stream`.

```fsharp
type WebmentionToRss (rssService:RssService) =

    let getMentions (t:TableClient) = 

        //...

        let webmentions = t.Query&lt;WebmentionEntity&gt;()

        webmentions


    //...

    [&lt;FunctionName("WebmentionToRss")&gt;]
    member x.Run
        ([&lt;TimerTrigger("0 0 3 * * *")&gt;] info: TimerInfo)
        ([&lt;Table("webmentions",Connection="AzureWebJobsStorage")&gt;] t: TableClient)
        ([&lt;Blob("feeds/webmentions/index.xml", FileAccess.Write, Connection="AzureWebJobsStorage")&gt;] rssBlob: Stream)
        (log: ILogger) =

        task {
            let mentions = getMentions t

            let rss = x.RssService.BuildRssFeed mentions "lqdev's Webmentions" "http://lqdev.me" "lqdev's Webmentions" "en"

            log.LogInformation(rss.ToString())                

            use xmlWriter = XmlWriter.Create(rssBlob)

            rss.WriteTo(xmlWriter) |&gt; ignore
        }
```

Now, to view my feed I add the URL to the RSS file in Azure Blob Storage to my RSS feed reader. The container and blob containing the file are private though, so how does my RSS reader get access to the file? I don't use the direct link. Instead, I use a link containing a [Shared Access Signatures (SAS) token](https://learn.microsoft.com/azure/storage/common/storage-sas-overview#sas-token). 

The RSS feed reader then fetches my Webmentions feed at regular intervals. 

This is what the GitHub post looks like in my RSS reader.

![Webmentions table on Azure Table Storage](/images/receive-webmentions-fsharp-az-functions-fsadvent/newsblur-webmentions-rss-feed.png)

That's all there is to it!

## What's next?

Overall I'm happy with my solution. I learned a few things along the way and got something working for myself. Publishing WebmentionFs as a library and making my Azure Function code open-source were attempts to help others looking to implement similar solutions get started. These solitions could be improved though. These are some updates I would like to make at some point:

- Upgrade my website to use WebmentionFs for sending Webmentions. You can see the [current implementation](https://github.com/lqdev/luisquintanilla.me/blob/main/Services/Webmention.fs) on my website repo which is basically what's in `UrlDiscoveryService` and `WebmentionSenderService` today. 
- Add interfaces for all services in WebmentionFs. Currently most services have concrete implementations. By adding interfaces, I can make the library extensible and you can implement your own URL discovery and validation services. 
- Implement `outbox` enpoint in WebmentionService to handle sending Webmentions.
- Create Azure ARM template for deploying WebmentionService. Currently with some small edits, you could fork and get my WebmentionService project working for your website. However, that process isn't turnkey today. Maybe using something like [Farmer](https://compositionalit.github.io/farmer/) would make onboarding easier for folks. 
 
## Conclusion

Webmentions are a great way to enable cross-site conversations on the internet. In this post, I provided a brief introduction to the general workflow for sending and receiving Webmentions. I also detailed my own implementation which I use today on my website. That implementation relies on the WebmentionFs .NET library, Azure Functions, and RSS. By using the WebmentionFs .NET library, you can more quickly implement similar solutions into your own apps and services. If you need a reference, check out my WebmentionService repository on GitHub which contains the code I use for receiving Webmentions. 

I'll leave you with three things:

1. As you consider the next platform for your next online home, in addition to considering the health of the community, take some time to also think about the protocols it's built on top of and whether they give you more or less freedom. 
1. Try out the WebmentionFs library on NuGet and add Webmentions to your site.
1. If you share this post anywhere publicly on the web, send me a Webmention.

Catch you on the world wide web!]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/receive-webmentions-fsharp-az-functions-fsadvent</link>
      <guid>https://www.luisquintanilla.me/posts/receive-webmentions-fsharp-az-functions-fsadvent</guid>
      <pubDate>2022-12-21 00:33</pubDate>
    </item>
    <item>
      <title>Rediscovering the RSS protocol</title>
      <description>&lt;![CDATA[
## Introduction

RSS provides a standardized way of aggregating content giving content creators the freedom to choose how they publish content while giving consumers the freedom to choose how they consume that content. I've used RSS for a long time. One of my favorite RSS readers was Google Reader while it was still around and I believe [I'm not the only fan](https://www.ripgooglereader.com/). Since then, I've bounced around between different RSS feed readers not settling one until recently when I started using [NewsBlur](https://newsblur.com). As a result of finding an RSS feed reader I liked, I've also started to expand the content I follow via RSS feeds to social media, videos, podcasts, and software projects. In part, this has gotten me thinking about some of the benefits of protocols. The essay [Protocols, Not Platforms: A Technological Approach To Free Speech](https://knightcolumbia.org/content/protocols-not-platforms-a-technological-approach-to-free-speech) makes good points on some of these benefits. Though the focus of that article is on free speech, I prefer to think about the benefits in terms of user freedom.

## Creator freedom using RSS

RSS provides content creators with the freedom to choose how and where they publish content. As a content creator, depending on the type of content you produce you use different platforms to publish. For example, when I blog, I publish on this site, when I create videos, I publish to YouTube or livestream on Twitch and to further expand the reach of my content, I publish links on social media like Reddit or Twitter. In each case, I use different platforms to publish content. This might be okay because the audiences interested in the video content may not be interested in blog posts and vice versa. However, now the content is split across all these various platforms with their own accounts, interfaces, policies, and audiences. It also means, when I tell people where they can follow me, it usually involves an entire speech like: "Follow me on GitHub at [lqdev](https://github.com/lqdev/), Twitter at [ljquintanilla](https://twitter.com/ljquintanilla), my blog at [luisquintanilla.me](https://www.luisquintanilla.me/posts/1)...". Of course, I don't always list out all the platforms I'm active on and it's highly dependent on the setting, but the more platforms I'm active on the longer that speech gets.

Fortunately, some of these platforms generate RSS feeds. On GitHub, RSS feeds are generated for releases, issues, and pull requests. For example, the ML.NET releases RSS feed can be accessed by appending *.atom* to the releases URL [https://github.com/dotnet/machinelearning/releases.atom](https://github.com/dotnet/machinelearning/releases.atom). Note that while Atom and RSS are not the same, they both offer a way to aggregate content. Similarly, YouTube provides the option to generate feeds for channels. For example, you can find the RSS feed to my YouTube channel at the following URL [https://www.youtube.com/feeds/videos.xml?channel_id=UCkA5fHdQ4cf3D1J19UNgV7A](https://www.youtube.com/feeds/videos.xml?channel_id=UCkA5fHdQ4cf3D1J19UNgV7A). On this blog, the RSS feed can be found at [https://www.luisquintanilla.me/posts/index.xml](https://www.luisquintanilla.me/posts/index.xml). For social media, the experience is a bit more limited since platforms like Twitter, Instagram, or Facebook don't generate RSS feeds which is unfortunate since they all have feeds of their own making them the perfect use case for RSS feeds. Platforms like Reddit provide RSS feeds to public subreddits and users by appending *.rss* at the end of the URL. For example, the RSS feed for the Zune subreddit can be found at [https://www.reddit.com/r/Zune.rss](https://www.reddit.com/r/Zune.rss). In each of these cases, though the content and platforms are different, the underlying protocol and format used is the same, RSS.

Therefore, a strategy to improve how I publish my content using RSS is, instead of sending users to the various platforms, I can provide links to the different RSS feeds for each of the platforms I'm active on in my personal site. Instead of having a long speech on the different platforms I'm active on, I can just say, "Go to my personal website at ...", and that's it! That becomes the gateway to the rest of my content. I can continue publishing on the different platforms because of the features they may provide me with for content creation and discovery, but only send users to a single place. For even more control over publishing, I can self-host videos or other content and create RSS feeds for it just like I do for my blog. In either case, users don't need to have an account in order to access and follow my content lowering the barrier to access content. This brings me to my next point of user freedom.

## User freedom using RSS

RSS provides users with the freedom to choose how they consume content. One way it does so is by being a standard protocol for aggregating content. Because there's an agreed upon set of standards, clients or RSS feed readers that consume this protocol at minimum only need to be able to read an RSS feed. Once a client can read the RSS feed, it's free to build on top of that leading to diversity and competition among the various clients. Diversity and competition is good for users because it provides choice. If I want a minimal feed reader, I can subscribe to RSS feeds using an e-mail client like [Outlook](https://support.microsoft.com/en-us/office/subscribe-to-an-rss-feed-73c6e717-7815-4594-98e5-81fa369e951c) or [Thunderbird](https://support.mozilla.org/en-US/kb/how-subscribe-news-feeds-and-blogs). If I want more features, I can use something like NewsBlur. As an aside, one of the other things I like about NewsBlur is being provided with an e-mail address that I can use to subscribe to newsletters helping me both clear up my inbox and letting me access information in a single place. This shows how RSS readers don't have to stop at only reading RSS feeds and can build on top of the protocol while still performing their core functionality.

Another reason to use RSS is having a single place to consume almost all your content. As mentioned before, an RSS feed can aggregate content from a YouTube channel, subreddit, blog, newspaper, podcast, etc. RSS is agnostic in the sense it doesn't care what's in the feed so long as you're able to link to the content. Now, because RSS readers can read any RSS feed, it means you can follow almost all of your content from a single place so long as you're able to subscribe to the RSS feed. In fact, aside from some social media, the only other content I don't consume via RSS is podcasts. Not because I'm not able to since podcasts are usually aggregated using RSS feeds, but because I'm not able to play and sync them with other devices. Being able to consume content in a single place provides users with an incredible amount of freedom by not requiring the use of multiple apps, websites, or accounts.

One last way in which RSS gives user freedom to choose how they consume content is by giving them control of their feed. Various platforms like social media sites and YouTube generate feeds and recommend content using algorithms. These algorithms are sometimes useful by exposing you to new and diverse content. Other times, algorithms can create a bubble by reinforcing preferences and interests. In any case, there's little visibility or control into how the algorithms make choices and recommendations. RSS by default is ordered in chronological order. Therefore you know you're getting the newest content. Depending on the client, you might be able to sort content into folders in whichever way makes the most sense to you. You might also be able to choose how often the feeds you're subscribed to are checked for new content. If you don't want to get the latest updates every second, set it to update daily or weekly. If you want to get rid of the clutter and only see content from the previous day, you can automatically delete or mark older content as read. Note that all these capabilities are dependent on the client, but if this is important to you, you can choose a client that gives you these features making the possibilities endless. Finally, because you are actively choosing which feeds you subscribe to, you're not presented with content other people liked, retweeted, shared, or recommended. You only have access to the content you choose to subscribe to. If at any point you're no longer interested in the content, unsubscribe and you no longer see it on your feed.

## Conclusion

As I continue my rediscovery of RSS, I want to find ways to subscribe to content on platforms I'm already using via RSS. It's important to note that this is not an either or approach. You can subscribe to content from the various platforms using RSS to get updates in a single place and since RSS links to the original content, you can consume content on the platform itself helping content creators with more page views, likes, subscriptions, and other forms of engagement. Ultimately though, the choice is yours. A choice of how you publish and consume content. 
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/rediscovering-rss-user-freedom</link>
      <guid>https://www.luisquintanilla.me/posts/rediscovering-rss-user-freedom</guid>
      <pubDate>2021-04-05 19:30:00</pubDate>
    </item>
    <item>
      <title>Restaurant Inspections ETL &amp; Data Enrichment with Spark.NET and ML.NET Automated (Auto) ML</title>
      <description>&lt;![CDATA[
## Introduction

Apache Spark is an open-source, distributed, general-purpose analytics engine. For years, it has been a staple in the Big Data ecosystem for batch and real-time processing on large datasets. Although native support for the platform is limited to the JVM set of languages, other languages typically used for data processing and analytics like Python and R have plugged into Spark's Interop layer to make use of its functionality. Around the Build 2019 conference, Microsoft announced Spark.NET. Spark.NET provides bindings written for the Spark Interop layer that allow you to work with components like Spark SQL and Spark Streaming inside your .NET applications. Because Spark.NET is .NET Standard 2.0 compliant, it can run operating systems like Windows, Mac and Linux. Spark.NET is an evolution of the Mobius project which provided .NET bindings for Spark.

This sample takes a restaurant violation dataset from the NYC Open Data portal and processes it using Spark.NET. Then, the processed data is used to train a machine learning model that attempts to predict the grade an establishment will receive after an inspection. The model will be trained using ML.NET, an open-source, cross-platform machine learning framework. Finally, data for which no grade currently exists will be enriched using the trained model to assign an expected grade.

The source code for this sample can be found in the [lqdev/RestaurantInspectionsSparkMLNET
GitHub repo](https://github.com/lqdev/RestaurantInspectionsSparkMLNET).

## Pre-requisites

This project was built using Ubuntu 18.04 but should work on Windows and Mac devices.

- [.NET Core 2.1 SDK](https://dotnet.microsoft.com/download/dotnet-core/2.1)
- [Java 8](https://www.java.com/en/download/)
- [Apache Spark 2.4.1 with Hadoop 2.7](https://archive.apache.org/dist/spark/spark-2.4.1/)
- [.NET Spark Worker 0.4.0](https://github.com/dotnet/spark/releases)

### Install Java

Since Spark runs on the JVM, you'll need Java on your PC. The minimum version required is version 8. To install and Java, enter the following command into the terminal:

```bash
sudo apt install openjdk-8-jdk openjdk-8-jre
```

Then, make sure that the recently installed version is the default

```bash
sudo update-alternatives --config java
```

### Download and configure Spark

Download Spark 2.4.1 with Hadoop 2.7 onto your computer. In this case, I'm placing it into my *Downloads* folder.

```bash
wget https://archive.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz -O ~/Downloads/spark-2.4.1-bin-hadoop2.7.tgz
```

Extract the contents of the recently downloaded file into the */usr/bin/local* directory.

```bash
sudo tar -xvf ~/Downloads/spark-2.4.1-bin-hadoop2.7.tgz --directory /usr/local/bin
```

### Download and configure .NET Spark Worker

Download the .NET Spark worker onto your computer. In this case, I'm placing it into the *Downloads* folder.

```bash
wget https://github.com/dotnet/spark/releases/download/v0.4.0/Microsoft.Spark.Worker.netcoreapp2.1.linux-x64-0.4.0.tar.gz -O ~/Downloads/Microsoft.Spark.Worker.netcoreapp2.1.linux-x64-0.4.0.tar.gz
```

Extract the contents of the recently downloaded file into the */usr/bin/local* directory.

```bash
sudo tar -xvf ~/Downloads/Microsoft.Spark.Worker.netcoreapp2.1.linux-x64-0.4.0.tar.gz --directory /usr/local/bin
```

Finally, raise the permissions on the `Microsoft.Spark.Worker` program. This is required to execute User-Defined Functions (UDF).

```bash
sudo chmod +x /usr/local/bin/Microsoft.Spark.Worker-0.4.0/Microsoft.Spark.Worker
```

### Configure environment variables

Once you download and configure the pre-requisites, configure their locations in the system as environment variables. Open the *~/.bashrc* file and add the following content at the end of the file.

```bash
export SPARK_PATH=/usr/local/bin/spark-2.4.1-bin-hadoop2.7
export PATH=$SPARK_PATH/bin:$PATH
export HADOOP_HOME=$SPARK_PATH
export SPARK_HOME=$SPARK_PATH
export DOTNET_WORKER_DIR=/usr/local/bin/Microsoft.Spark.Worker-0.4.0
```

## Solution description

### Understand the data

The dataset used in this solution is the [*DOHMH New York City Restaurant Inspection Results*](https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j) and comes from the NYC Open Data portal. It is updated daily and contains assigned and pending inspection results and violation citations for restaurants and college cafeterias. The dataset excludes establishments that have gone out of business. Although the dataset contains several columns, only a subset of them are used in this solution. For a detailed description of the dataset, visit the dataset website.

### Understand the solution

This solution is made up of four different .NET Core applications:

- *RestaurantInspectionsETL*: .NET Core Console application that takes raw data and uses Spark.NET to clean and transform the data into a format that is easier to use as input for training and making predictions with a machine learning model built with ML.NET.
- *RestaurantInspectionsML*: .NET Core Class Library that defines the input and output schema of the ML.NET machine learning model. Additionally, this is where the trained model is saved to.
- *RestaurantInspectionsTraining*: .NET Core Console application that uses the graded data generated by the *RestaurantInspectionsETL* application to train a multiclass classification machine learning model using ML.NET's Auto ML.
- *RestaurantInspectionsEnrichment*: .NET Core Console application that uses the ungraded data generated by the *RestaurantInspectionsETL* application as input for the trained ML.NET machine learning model which predicts what grade an establishment is most likely to receive based on the violations found during inspection.

## Set up the solution

### Create solution directory

Create a new directory for your projects called *RestaurantInspectionsSparkMLNET* and navigate to it with the following command.

```bash
mkdir RestaurantInspectionsSparkMLNET &amp;&amp; cd RestaurantInspectionsSparkMLNET
```

Then, create a solution using the `dotnet cli`.

```bash
dotnet new sln
```

To ensure that the 2.1 version of the .NET Core SDK is used as the target framework, especially if you have multiple versions of the .NET SDK installed, create a file called *globals.json* in the *RestaurantInspectionsSparkMLNET* solution directory.

```bash
touch global.json
```

In the *global.json* file, add the following content. Make sure to use the specific version of the SDK installed on your computer. In this case, I have version `2.1.801` installed on my computer. You can use the `dotnet --list-sdks` command to list the installed SDK versions.

```json
{
  "sdk": {
    "version": "2.1.801"
  }
}
```

### Create and configure the ETL project

The ETL project is responsible for taking the raw source data and using Spark to apply a series of transformations to prepare the data to train the machine learning model as well as to enrich data with missing grades.

Inside the *RestaurantInspectionsSparkMLNET* solution directory, create a new console application called *RestaurantInspectionsETL* using the `dotnet cli`.

```bash
dotnet new console -o RestaurantInspectionsETL
```

Add the newly created project to the solution with the `dotnet cli`.

```bash
dotnet sln add ./RestaurantInspectionsETL/
```

Since this project uses the `Microsoft.Spark` NuGet package, use the `dotnet cli` to install it.

```bash
dotnet add ./RestaurantInspectionsETL/ package Microsoft.Spark --version 0.4.0
```

### Create and configure the ML model project

The ML model class library will contain the domain model that defines the schema of model inputs and outputs as well as the trained model itself.

Inside the *RestaurantInspectionsSparkMLNET* solution directory, create a new class library called *RestaurantInspectionsML* using the `dotnet cli`.

```bash
dotnet new classlib -o RestaurantInspectionsML
```

Add the newly created project to the solution with the `dotnet cli`.

```bash
dotnet sln add ./RestaurantInspectionsML/
```

Since this project uses the `Microsoft.ML` NuGet package, use the `dotnet cli` to install it.

```bash
dotnet add ./RestaurantInspectionsML/ package Microsoft.ML --version 1.3.1
```

### Create and configure the ML training project

The purpose of the training project is to use the pre-processed graded data output by the *RestaurantInspectionsETL* project as input to train a multiclass classification model with ML.NET's Auto ML API. The trained model will then be saved in the *RestaurantInspectionsML* directory.

Inside the *RestaurantInspectionsSparkMLNET* solution directory, create a new console application called *RestaurantInspectionsTraining* using the `dotnet cli`.

```bash
dotnet new console -o RestaurantInspectionsTraining
```

Add the newly created project to the solution with the `dotnet cli`.

```bash
dotnet sln add ./RestaurantInspectionsTraining/
```

This project depends on the domain model created in the *RestaurantInspectionsML* project, so you need to add a reference to it.  

```bash
dotnet add ./RestaurantInspectionsTraining/ reference ./RestaurantInspectionsML/
```

Since this project uses the `Microsoft.Auto.ML` NuGet package, use the `dotnet cli` to install it.

```bash
dotnet add ./RestaurantInspectionsTraining/ package Microsoft.ML.AutoML --version 0.15.1
```

### Create and configure the data enrichment project

The data enrichment application uses the trained machine learning model created by the *RestaurantInspectionsTraining* application and use it on the pre-processed ungraded data created by the *RestaurantInspectionsETL* application to predict what grade that establishment is most likely to receive based on the violations found during inspection.

Inside the *RestaurantInspectionsSparkMLNET* solution directory, create a new console application called *RestaurantInspectionsEnrichment* using the `dotnet cli`.

```bash
dotnet new console -o RestaurantInspectionsEnrichment
```

Add the newly created project to the solution with the `dotnet cli`.

```bash
dotnet sln add ./RestaurantInspectionsEnrichment/
```

This project depends on the domain model created in the *RestaurantInspectionsML* project, so you need to add a reference to it.

```bash
dotnet add ./RestaurantInspectionsEnrichment/ reference ./RestaurantInspectionsML/
```

This uses the following NuGet packages:

- Microsoft.Spark
- Microsoft.ML.LightGBM (This is not required but predictions may fail if the final model is a LightGBM model).

Install the packages with the following commands:

```bash
dotnet add ./RestaurantInspectionsEnrichment/ package Microsoft.Spark --version 0.4.0
dotnet add ./RestaurantInspectionsEnrichment/ package Microsoft.ML.LightGBM --version 1.3.1
```

## Build ETL application

The first step is to prepare the data. To do so, apply a set of transformations using Spark.NET.

### Download the data

Navigate to the *RestaurantInspectionsETL* project and create a *Data* directory.

```bash
mkdir Data
```

Then, download the data into the newly created *Data* directory.

```bash
wget https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv?accessType=DOWNLOAD -O Data/NYC-Restaurant-Inspections.csv
```

### Build the ETL pipeline

Add the following usings to the *Program.cs* file.

```csharp
using System;
using System.IO;
using Microsoft.Spark.Sql;
using static Microsoft.Spark.Sql.Functions;
```

Not all of the columns are relevant. Inside the `Main` method of the *Program.cs* file, define the columns to be removed.

```csharp
string[] dropCols = new string[]
{
    "CAMIS",
    "CUISINE DESCRIPTION",
    "VIOLATION DESCRIPTION",
    "BORO",
    "BUILDING",
    "STREET",
    "ZIPCODE",
    "PHONE",
    "ACTION",
    "GRADE DATE",
    "RECORD DATE",
    "Latitude",
    "Longitude",
    "Community Board",
    "Council District",
    "Census Tract",
    "BIN",
    "BBL",
    "NTA"
};
```

The entrypoint of Spark applications is the `SparkSession`. Create `SparkSession` inside the `Main` method of the *Program.cs* file.

```csharp
var sc =
    SparkSession
        .Builder()
        .AppName("Restaurant_Inspections_ETL")
        .GetOrCreate();
```

Then, load the data stored in the *NYC-Restaurant-Inspections.csv* file into a `DataFrame`.

```csharp
DataFrame df =
    sc
    .Read()
    .Option("header", "true")
    .Option("inferSchema", "true")
    .Csv("Data/NYC-Restaurant-Inspections.csv");
```

`DataFrames` can be thought of as tables in a database or sheets in Excel. Spark has various ways of representing data but `DataFrames` are the format supported by Spark.NET. Additionally, the `DataFrame` API is higher-level and easier to work with.

Once the data is loaded, get rid of the data that are not needed by creating a new `DataFrame` that excludes the `dropCols` as well as missing values.

```csharp
DataFrame cleanDf =
    df
        .Drop(dropCols)
        .WithColumnRenamed("INSPECTION DATE","INSPECTIONDATE")
        .WithColumnRenamed("INSPECTION TYPE","INSPECTIONTYPE")
        .WithColumnRenamed("CRITICAL FLAG","CRITICALFLAG")
        .WithColumnRenamed("VIOLATION CODE","VIOLATIONCODE")
        .Na()
        .Drop();
```

Typically, machine learning models expect values to be numerical, so in the ETL step try to convert as many values as possible into numerical values. The `CRITICALFLAG` column contains "Y"/"N" values that can be encoded as 0 and 1.

```csharp
DataFrame labeledFlagDf =
    cleanDf
        .WithColumn("CRITICALFLAG",
            When(Functions.Col("CRITICALFLAG") == "Y",1)
            .Otherwise(0));
```

This dataset contains one violation per row which correspond to different inspections. Therefore, all of the violations need to be aggregated by business and inspection.

```csharp
DataFrame groupedDf =
    labeledFlagDf
        .GroupBy("DBA", "INSPECTIONDATE", "INSPECTIONTYPE", "CRITICALFLAG", "SCORE", "GRADE")
        .Agg(Functions.CollectSet(Functions.Col("VIOLATIONCODE")).Alias("CODES"))
        .Drop("DBA", "INSPECTIONDATE")
        .WithColumn("CODES", Functions.ArrayJoin(Functions.Col("CODES"), ","))
        .Select("INSPECTIONTYPE", "CODES", "CRITICALFLAG", "SCORE", "GRADE");  
```

Now that the data is in the format used to train and make predictions, split the cleaned `DataFrame` into two new `DataFrames`, graded and ungraded. The graded dataset is the data used for training the machine learning model. The ungraded data will be used for enrichment.

```csharp
DataFrame gradedDf =
    groupedDf
    .Filter(
        Col("GRADE") == "A" |
        Col("GRADE") == "B" |
        Col("GRADE") == "C" );

DataFrame ungradedDf =
    groupedDf
    .Filter(
        Col("GRADE") != "A" &amp;
        Col("GRADE") != "B" &amp;
        Col("GRADE") != "C" );  
```

Take the `DataFrames` and save them as csv files for later use.

```csharp
var timestamp = ((DateTimeOffset) DateTime.UtcNow).ToUnixTimeSeconds().ToString();

var saveDirectory = Path.Join("Output",timestamp);

if(!Directory.Exists(saveDirectory))
{
    Directory.CreateDirectory(saveDirectory);
}

gradedDf.Write().Csv(Path.Join(saveDirectory,"Graded"));

ungradedDf.Write().Csv(Path.Join(saveDirectory,"Ungraded"));
```

### Publish and run the ETL application

The final *Program.cs* file should look as follows:

```csharp
using System;
using System.IO;
using Microsoft.Spark.Sql;
using static Microsoft.Spark.Sql.Functions;

namespace RestaurantInspectionsETL
{
    class Program
    {
        static void Main(string[] args)
        {
            // Define columns to remove
            string[] dropCols = new string[]
            {
                "CAMIS",
                "CUISINE DESCRIPTION",
                "VIOLATION DESCRIPTION",
                "BORO",
                "BUILDING",
                "STREET",
                "ZIPCODE",
                "PHONE",
                "ACTION",
                "GRADE DATE",
                "RECORD DATE",
                "Latitude",
                "Longitude",
                "Community Board",
                "Council District",
                "Census Tract",
                "BIN",
                "BBL",
                "NTA"
            };

            // Create SparkSession
            var sc =
                SparkSession
                    .Builder()
                    .AppName("Restaurant_Inspections_ETL")
                    .GetOrCreate();

            // Load data
            DataFrame df =
                sc
                .Read()
                .Option("header", "true")
                .Option("inferSchema", "true")
                .Csv("Data/NYC-Restaurant-Inspections.csv");

            //Remove columns and missing values
            DataFrame cleanDf =
                df
                    .Drop(dropCols)
                    .WithColumnRenamed("INSPECTION DATE","INSPECTIONDATE")
                    .WithColumnRenamed("INSPECTION TYPE","INSPECTIONTYPE")
                    .WithColumnRenamed("CRITICAL FLAG","CRITICALFLAG")
                    .WithColumnRenamed("VIOLATION CODE","VIOLATIONCODE")
                    .Na()
                    .Drop();

            // Encode CRITICAL FLAG column
            DataFrame labeledFlagDf =
                cleanDf
                    .WithColumn("CRITICALFLAG",
                        When(Functions.Col("CRITICALFLAG") == "Y",1)
                        .Otherwise(0));

             // Aggregate violations by business and inspection
            DataFrame groupedDf =
                labeledFlagDf
                    .GroupBy("DBA", "INSPECTIONDATE", "INSPECTIONTYPE", "CRITICALFLAG", "SCORE", "GRADE")
                    .Agg(Functions.CollectSet(Functions.Col("VIOLATIONCODE")).Alias("CODES"))
                    .Drop("DBA", "INSPECTIONDATE")
                    .WithColumn("CODES", Functions.ArrayJoin(Functions.Col("CODES"), ","))
                    .Select("INSPECTIONTYPE", "CODES", "CRITICALFLAG", "SCORE", "GRADE");

            // Split into graded and ungraded DataFrames
            DataFrame gradedDf =
                groupedDf
                .Filter(
                    Col("GRADE") == "A" |
                    Col("GRADE") == "B" |
                    Col("GRADE") == "C" );

            DataFrame ungradedDf =
                groupedDf
                    .Filter(
                        Col("GRADE") != "A" &amp;
                        Col("GRADE") != "B" &amp;
                        Col("GRADE") != "C" );

            // Save DataFrames
            var timestamp = ((DateTimeOffset) DateTime.UtcNow).ToUnixTimeSeconds().ToString();

            var saveDirectory = Path.Join("Output",timestamp);

            if(!Directory.Exists(saveDirectory))
            {
                Directory.CreateDirectory(saveDirectory);
            }

            gradedDf.Write().Csv(Path.Join(saveDirectory,"Graded"));

            ungradedDf.Write().Csv(Path.Join(saveDirectory,"Ungraded"));
        }
    }
}
```

Publish the application with the following command.

```bash
dotnet publish -f netcoreapp2.1 -r ubuntu.18.04-x64
```

Run the application with `spark-submit`.

```bash
spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master local bin/Debug/netcoreapp2.1/ubuntu.18.04-x64/publish/microsoft-spark-2.4.x-0.4.0.jar dotnet bin/Debug/netcoreapp2.1/ubuntu.18.04-x64/publish/RestaurantInspectionsETL.dll
```

## Build ML Domain

### Define the model input schema

Navigate to the *RestaurantInspectionsTraining* project directory and create a new file called *ModelInput.cs*.

```bash
touch ModelInput.cs
```

Open the *ModelInput.cs* file and add the following code.

```csharp
using Microsoft.ML.Data;

namespace RestaurantInspectionsML
{
    public class ModelInput
    {
        [LoadColumn(0)]
        public string InspectionType { get; set; }

        [LoadColumn(1)]
        public string Codes { get; set; }

        [LoadColumn(2)]
        public float CriticalFlag { get; set; }

        [LoadColumn(3)]
        public float InspectionScore { get; set; }

        [LoadColumn(4)]
        [ColumnName("Label")]
        public string Grade { get; set; }
    }
}
```

Using attributes in the schema, five properties are defined:

- InspectionType: The type of inspection performed.
- Codes: Violation codes found during inspection.
- CriticalFlag: Indicates if any of the violations during the inspection were critical (contribute to food-borne illness).
- InspectionScore: Score assigned after inspection.
- Grade: Letter grade assigned after inspection

The `LoadColumn` attribute defines the position of the column in the file. Data in the last column is assigned to the `Grade` property but is then referenced as `Label` in the `IDataView`. The reason for using the `ColumnName` attribute is ML.NET algorithms have default column names and renaming properties at the schema class level removes the need to define the feature and label columns as parameters in the training pipeline.

### Define the model output schema

In the *RestaurantInspectionsTraining* project directory and create a new file called *ModelOutput.cs*.

```bash
touch ModelOutput.cs
```

Open the *ModelOutput.cs* file and add the following code.

```csharp
namespace RestaurantInspectionsML
{
    public class ModelOutput
    {
        public float[] Scores { get; set; }
        public string PredictedLabel { get; set; }
    }
}
```

For the output schema, the `ModelOutput` class uses properties with the default column names of the outputs generated by the model training process:

- Scores: A float vector containing the probabilties for all the predicted classes.
- PredictedLabel: The value of the prediction. In this case, the `PredictedLabel` is the predicted grade expected to be assigned after inspection given the set of features for that inspection.

## Build the model training application

The application trains a multiclass classification algorithm. Finding the "best" algorithm with the right parameters requires experimentation. Fortunately, ML.NET's Auto ML does this for you given you provide it with the type of algorithm you want to train.

### Load the graded data

Navigate to the *RestaurantInspectionsTraining* project directory and add the following using statements to the *Program.cs* class.

```csharp
using System;
using System.IO;
using System.Linq;
using Microsoft.ML;
using static Microsoft.ML.DataOperationsCatalog;
using Microsoft.ML.AutoML;
using RestaurantInspectionsML;
```

Inside the `Main` method of the *Program.cs* file, define the path where the data files are stored.

```csharp
string solutionDirectory = "/home/lqdev/Development/RestaurantInspectionsSparkMLNET";
string dataLocation = Path.Combine(solutionDirectory,"RestaurantInspectionsETL","Output");
```

The entrypoint of an ML.NET application is the `MLContext`. Initialize an `MLContext` instance.

```csharp
MLContext mlContext = new MLContext();
```

Next, get the paths of the data files. The output generated by the *RestaurantInspectionsETL* application contains both the csv files as well as files containing information about the partitions that created them. For training, only the csv files are needed.

```csharp
var latestOutput =
    Directory
        .GetDirectories(dataLocation)
        .Select(directory =&gt; new DirectoryInfo(directory))
        .OrderBy(directoryInfo =&gt; directoryInfo.Name)
        .Select(directory =&gt; Path.Join(directory.FullName,"Graded"))
        .First();

var dataFilePaths =
    Directory
        .GetFiles(latestOutput)
        .Where(file =&gt; file.EndsWith("csv"))
        .ToArray();
```

Then, load the data into an `IDataView`. An `IDataView` is similar to a `DataFrame` in that it is a way to represent data as rows, columns and their schema.

```csharp
var dataLoader = mlContext.Data.CreateTextLoader&lt;ModelInput&gt;(separatorChar:',', hasHeader:false, allowQuoting:true, trimWhitespace:true);

IDataView data = dataLoader.Load(dataFilePaths);
```

It's good practice to split the data into training and test sets for evaluation. Split the data into 80% training and 20% test sets.

```csharp
TrainTestData dataSplit = mlContext.Data.TrainTestSplit(data,testFraction:0.2);
IDataView trainData = dataSplit.TrainSet;
IDataView testData = dataSplit.TestSet;
```

### Create the experiment

Auto ML takes the data and runs experiments using different models and hyper-parameters in search of the "best" model. Define the settings for your experiment. In this case, the model will run for 600 seconds or 10 minutes and will try to find the model with the lowest log loss metric.

```csharp
var experimentSettings = new MulticlassExperimentSettings();
experimentSettings.MaxExperimentTimeInSeconds = 600;
experimentSettings.OptimizingMetric = MulticlassClassificationMetric.LogLoss;
```

Then, create the experiment.

```csharp
var experiment = mlContext.Auto().CreateMulticlassClassificationExperiment(experimentSettings);
```

After creating the experiment, run it.

```csharp
var experimentResults = experiment.Execute(data, progressHandler: new ProgressHandler());
```

By default, running the application won’t display progress information. However, a `ProgressHandler` object can be passed into the `Execute` method of an experiment which calls the implemented `Report` method.

Inside the *RestaurantInspectionsTraining* project directory, create a new file called *ProgressHandler.cs*.

```bash
touch ProgressHandler.cs
```

Then, add the following code:

```csharp
using System;
using Microsoft.ML.Data;
using Microsoft.ML.AutoML;

namespace RestaurantInspectionsTraining
{
    public class ProgressHandler : IProgress&lt;RunDetail&lt;MulticlassClassificationMetrics&gt;&gt;
    {
        public void Report(RunDetail&lt;MulticlassClassificationMetrics&gt; run)
        {
            Console.WriteLine($"Trained {run.TrainerName} with Log Loss {run.ValidationMetrics.LogLoss:0.####} in {run.RuntimeInSeconds:0.##} seconds");
        }
    }
}
```

The ProgressHandler class derives from the `IProgress&lt;T&gt;` interface which requires the implementation of the `Report` method. The object being passed into the Report method after each run is an `RunDetail&lt;MulticlassClassificationMetrics&gt;` object. Each time a run is complete, the `Report` method is called and the code inside it executes.

### Evaluate the results

Once the experiment has finished running, get the model from the best run. Add the following code to the `Main` method of the *Program.cs*.

```csharp
var bestModel = experimentResults.BestRun.Model;
```

Evaluate the performance of the model using the test dataset and measure it's Micro-Accuracy metric.

```csharp
IDataView scoredTestData = bestModel.Transform(testData);  
var metrics = mlContext.MulticlassClassification.Evaluate(scoredTestData);
Console.WriteLine($"MicroAccuracy: {metrics.MicroAccuracy}");
```

### Save the trained model

Finally, save the trained model to the *RestaurantInspectionsML* project.

```csharp
string modelSavePath = Path.Join(solutionDirectory,"RestaurantInspectionsML","model.zip");
mlContext.Model.Save(bestModel, data.Schema, modelSavePath);
```

A file called *model.zip* should be created inside the *RestaurantInspectionsML* project.

Make sure that the trained model file is copied to the output directory by adding the following contents to the *RestaurantInspectionsML.csproj* file in the *RestaurantInspectionsML* directory.

```xml
&lt;ItemGroup&gt;
  &lt;None Include="model.zip"&gt;
    &lt;CopyToOutputDirectory&gt;PreserveNewest&lt;/CopyToOutputDirectory&gt;
  &lt;/None&gt;
&lt;/ItemGroup&gt;
```

Copying it to the output directory of the *RestaurantInspectionsML* makes it easier to reference from the *RestaurantInspectionsEnrichment* project since that project already contains a reference to the *RestaurantInspectionsML* class library.

## Train the model

The final *Program.cs* file should look as follows:

```csharp
using System;
using System.IO;
using System.Linq;
using Microsoft.ML;
using static Microsoft.ML.DataOperationsCatalog;
using Microsoft.ML.AutoML;
using RestaurantInspectionsML;

namespace RestaurantInspectionsTraining
{
    class Program
    {
        static void Main(string[] args)
        {
            // Define source data directory paths
            string solutionDirectory = "/home/lqdev/Development/RestaurantInspectionsSparkMLNET";
            string dataLocation = Path.Combine(solutionDirectory,"RestaurantInspectionsETL","Output");

            // Initialize MLContext
            MLContext mlContext = new MLContext();

            // Get directory name of most recent ETL output
            var latestOutput =
                Directory
                    .GetDirectories(dataLocation)
                    .Select(directory =&gt; new DirectoryInfo(directory))
                    .OrderBy(directoryInfo =&gt; directoryInfo.Name)
                    .Select(directory =&gt; Path.Join(directory.FullName,"Graded"))
                    .First();

            var dataFilePaths =
                Directory
                    .GetFiles(latestOutput)
                    .Where(file =&gt; file.EndsWith("csv"))
                    .ToArray();

            // Load the data
            var dataLoader = mlContext.Data.CreateTextLoader&lt;ModelInput&gt;(separatorChar:',', hasHeader:false, allowQuoting:true, trimWhitespace:true);
            IDataView data = dataLoader.Load(dataFilePaths);

            // Split the data
            TrainTestData dataSplit = mlContext.Data.TrainTestSplit(data,testFraction:0.2);
            IDataView trainData = dataSplit.TrainSet;
            IDataView testData = dataSplit.TestSet;

            // Define experiment settings
            var experimentSettings = new MulticlassExperimentSettings();
            experimentSettings.MaxExperimentTimeInSeconds = 600;
            experimentSettings.OptimizingMetric = MulticlassClassificationMetric.LogLoss;

            // Create experiment
            var experiment = mlContext.Auto().CreateMulticlassClassificationExperiment(experimentSettings);

            // Run experiment
            var experimentResults = experiment.Execute(data, progressHandler: new ProgressHandler());

            // Best Run Results
            var bestModel = experimentResults.BestRun.Model;

            // Evaluate Model
            IDataView scoredTestData = bestModel.Transform(testData);  
            var metrics = mlContext.MulticlassClassification.Evaluate(scoredTestData);
            Console.WriteLine($"MicroAccuracy: {metrics.MicroAccuracy}");

            // Save Model
            string modelSavePath = Path.Join(solutionDirectory,"RestaurantInspectionsML","model.zip");
            mlContext.Model.Save(bestModel, data.Schema, modelSavePath);
        }
    }
}
```

Once all the code and configurations are complete, from the *RestaurantInspectionsTraining* directory, run the application using the `dotnet cli`. Remember this will take 10 minutes to run.

```bash
dotnet run
```

The console output should look similar to the output below:

```text
Trained LightGbmMulti with Log Loss 0.1547 in 1.55 seconds
Trained FastTreeOva with Log Loss 0.0405 in 65.58 seconds
Trained FastForestOva with Log Loss 0.0012 in 53.37 seconds
Trained LightGbmMulti with Log Loss 0.0021 in 4.55 seconds
Trained FastTreeOva with Log Loss 0.8315 in 5.22 seconds
MicroAccuracy: 0.999389615839469
```

## Build the data enrichment application

Now that the model is trained, it can be used to enrich the ungraded data.

### Initialize the PredictionEngine

Navigate to the *RestaurantInspectionsEnrichment* project directory and add the following using statements to the *Program.cs* class.

```csharp
using System.IO;
using System.Linq;
using Microsoft.ML;
using Microsoft.Spark.Sql;
using static Microsoft.Spark.Sql.Functions;
using RestaurantInspectionsML;
```

To make predictions, the model has to be loaded into the applicaton and because predictions are made one row at a time, a `PredictionEngine` has be created as well.

Inside the `Program` class, define the `PredictionEngine`.

```csharp
private static readonly PredictionEngine&lt;ModelInput,ModelOutput&gt; _predictionEngine;
```

Then, create a constructor to load the model and initialize it.

```csharp
static Program()
{
    MLContext mlContext = new MLContext();
    ITransformer model = mlContext.Model.Load("model.zip",out DataViewSchema schema);
    _predictionEngine = mlContext.Model.CreatePredictionEngine&lt;ModelInput,ModelOutput&gt;(model);
}
```

### Load the ungraded data

In the `Main` method of the `Program` class, define the location of the data files.

```csharp
string solutionDirectory = "/home/lqdev/Development/RestaurantInspectionsSparkMLNET";
string dataLocation = Path.Combine(solutionDirectory,"RestaurantInspectionsETL","Output");
```

Then, get the path of the most recent ungraded data generated by the *RestaurantInspectionsETL* application.

```csharp
var latestOutput =
    Directory
        .GetDirectories(dataLocation)
        .Select(directory =&gt; new DirectoryInfo(directory))
        .OrderBy(directoryInfo =&gt; directoryInfo.Name)
        .Select(directory =&gt; directory.FullName)
        .First();
```

Initialize a `SparkSession` for your enrichment application.

```csharp
var sc =
    SparkSession
        .Builder()
        .AppName("Restaurant_Inspections_Enrichment")
        .GetOrCreate();
```

The data generated by the *RestaurantInspectionsETL* does not have headers. However, the schema can be defined and set when the data is loaded.

```csharp
var schema = @"
    INSPECTIONTYPE string,
    CODES string,
    CRITICALFLAG int,
    INSPECTIONSCORE int,
    GRADE string";

DataFrame df = 
    sc
    .Read()
    .Schema(schema)
    .Csv(Path.Join(latestOutput,"Ungraded"));
```

### Define the UDF

There is no built-in function in Spark that allows you to use a `PredictionEngine`. However, Spark can be extended through UDFs. Keep in mind that UDFs are not optimized like the built-in functions. Therefore, whenever possible, try to use the built-in functions as much as possible.

In the `Program` class, create a new method called `PredictGrade` which takes in the set of features that make up the `ModelInput` expected by the trained model.

```csharp
public static string PredictGrade(
    string inspectionType,
    string violationCodes,
    int criticalFlag,
    int inspectionScore)
{
    ModelInput input = new ModelInput
    {
        InspectionType=inspectionType,
        Codes=violationCodes,
        CriticalFlag=(float)criticalFlag,
        InspectionScore=(float)inspectionScore
    };

    ModelOutput prediction = _predictionEngine.Predict(input);

    return prediction.PredictedLabel;
}
```

Then, inside the `Main` method, register the `PredictGrade` method as a UDF in your `SparkSession`.

```csharp
sc.Udf().Register&lt;string,string,int,int,string&gt;("PredictGrade",PredictGrade);
```

### Enrich the data

Once the UDF is registered, use it inside of a `Select` statement which creates a new `DataFrame` that includes the input features as well as the predicted grade output by the trained mdoel.

```csharp
var enrichedDf =
    df
    .Select(
        Col("INSPECTIONTYPE"),
        Col("CODES"),
        Col("CRITICALFLAG"),
        Col("INSPECTIONSCORE"),
        CallUDF("PredictGrade",
            Col("INSPECTIONTYPE"),
            Col("CODES"),
            Col("CRITICALFLAG"),
            Col("INSPECTIONSCORE")
        ).Alias("PREDICTEDGRADE")
    );
```

Finally, save the enriched `DataFrame`

```csharp
string outputId = new DirectoryInfo(latestOutput).Name;
string enrichedOutputPath = Path.Join(solutionDirectory,"RestaurantInspectionsEnrichment","Output");
string savePath = Path.Join(enrichedOutputPath,outputId);

if(!Directory.Exists(savePath))
{
    Directory.CreateDirectory(enrichedOutputPath);
}

enrichedDf.Write().Csv(savePath);
```

### Publish and run the enrichment application

The final *Program.cs* file should look as follows.

```csharp
using System.IO;
using System.Linq;
using Microsoft.ML;
using Microsoft.Spark.Sql;
using static Microsoft.Spark.Sql.Functions;
using RestaurantInspectionsML;

namespace RestaurantInspectionsEnrichment
{
    class Program
    {
        private static readonly PredictionEngine&lt;ModelInput,ModelOutput&gt; _predictionEngine;

        static Program()
        {
            MLContext mlContext = new MLContext();
            ITransformer model = mlContext.Model.Load("model.zip",out DataViewSchema schema);
            _predictionEngine = mlContext.Model.CreatePredictionEngine&lt;ModelInput,ModelOutput&gt;(model);
        }

        static void Main(string[] args)
        {
            // Define source data directory paths
            string solutionDirectory = "/home/lqdev/Development/RestaurantInspectionsSparkMLNET";
            string dataLocation = Path.Combine(solutionDirectory,"RestaurantInspectionsETL","Output");

            var latestOutput = 
                Directory
                    .GetDirectories(dataLocation)
                    .Select(directory =&gt; new DirectoryInfo(directory))
                    .OrderBy(directoryInfo =&gt; directoryInfo.Name)
                    .Select(directory =&gt; directory.FullName)
                    .First();

            var sc = 
                SparkSession
                    .Builder()
                    .AppName("Restaurant_Inspections_Enrichment")
                    .GetOrCreate();

            var schema = @"
                INSPECTIONTYPE string,
                CODES string,
                CRITICALFLAG int,
                INSPECTIONSCORE int,
                GRADE string";

            DataFrame df = 
                sc
                .Read()
                .Schema(schema)
                .Csv(Path.Join(latestOutput,"Ungraded"));

            sc.Udf().Register&lt;string,string,int,int,string&gt;("PredictGrade",PredictGrade);

            var enrichedDf = 
                df
                .Select(
                    Col("INSPECTIONTYPE"),
                    Col("CODES"),
                    Col("CRITICALFLAG"),
                    Col("INSPECTIONSCORE"),
                    CallUDF("PredictGrade",
                        Col("INSPECTIONTYPE"),
                        Col("CODES"),
                        Col("CRITICALFLAG"),
                        Col("INSPECTIONSCORE")
                    ).Alias("PREDICTEDGRADE")
                );

            string outputId = new DirectoryInfo(latestOutput).Name;
            string enrichedOutputPath = Path.Join(solutionDirectory,"RestaurantInspectionsEnrichment","Output");
            string savePath = Path.Join(enrichedOutputPath,outputId);

            if(!Directory.Exists(savePath))
            {
                Directory.CreateDirectory(enrichedOutputPath);
            }

            enrichedDf.Write().Csv(savePath);

        }

        public static string PredictGrade(
            string inspectionType,
            string violationCodes,
            int criticalFlag,
            int inspectionScore)
        {
            ModelInput input = new ModelInput
            {
                InspectionType=inspectionType,
                Codes=violationCodes,
                CriticalFlag=(float)criticalFlag,
                InspectionScore=(float)inspectionScore
            };

            ModelOutput prediction = _predictionEngine.Predict(input);

            return prediction.PredictedLabel;
        }
    }
}
```

From the *RestaurantInspectionsEnrichment* project publish the application with the following command.

```bash
dotnet publish -f netcoreapp2.1 -r ubuntu.18.04-x64
```

Navigate to the *publish* directory. In this case, it's *bin/Debug/netcoreapp2.1/ubuntu.18.04-x64/publish*.

From the *publish* directory, run the application with `spark-submit`.

```bash
spark-submit --class org.apache.spark.deploy.dotnet.DotnetRunner --master local microsoft-spark-2.4.x-0.4.0.jar dotnet RestaurantInspectionsEnrichment.dll
```

The file output should look similar to the contents below:

```text
Cycle Inspection / Initial Inspection,04N,1,13,A
Cycle Inspection / Re-inspection,08A,0,9,A
Cycle Inspection / Initial Inspection,"10B,10H",0,10,A
Cycle Inspection / Initial Inspection,10F,0,10,A
Cycle Inspection / Reopening Inspection,10F,0,3,C
```

## Conclusion

This solution showcased how Spark can be used within .NET applications with Spark.NET. Because it's part of the .NET ecosystem, other components and frameworks such as ML.NET can be leveraged to extend the system's capabilities. Although this sample was developed and run on a local, single-node cluster, Spark was made to run at scale. As such, this application can be further improved by setting up a cluster and running the ETL and enrichment workloads on there.

###### Resources

[Apache Spark](https://spark.apache.org/)
[Spark.NET](https://dotnet.microsoft.com/apps/data/spark)
[Spark.NET GitHub](https://github.com/dotnet/spark)
[Mobius](https://github.com/Microsoft/Mobius)
[NYC OpenData](https://opendata.cityofnewyork.us/)
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/restaurant-inspections-etl-data-enrichment-spark-auto-ml-net</link>
      <guid>https://www.luisquintanilla.me/posts/restaurant-inspections-etl-data-enrichment-spark-auto-ml-net</guid>
      <pubDate>2019-09-15 15:12:01</pubDate>
    </item>
    <item>
      <title>Starter Packs with OPML and RSS</title>
      <description>&lt;![CDATA[
One of the things I like about Bluesky is the [Starter Pack feature](https://bsky.social/about/blog/06-26-2024-starter-packs). 

In a gist, a Starter Pack is a collection of feeds. 

Bluesky users can:

- Create starter packs
- Share starter packs
- Subscribe to starter packs

Unfortunately, Starter Packs are limited to Bluesky. 

Or are they? 

As mentioned, starter packs are a collection of feeds that others can create, share, and subscribe to. 

[Bluesky supports RSS](/feed/bluesky-rss-support), which means you could organize the feeds using an OPML file that you can share with others and others can subscribe to. The benefits of this is, you can continue to keep up with activity on Bluesky from the feed reader of your choice without being required to have an account on Bluesky. 

More importantly, because RSS and OPML are open standards, you're not limited to building starter packs for Bluesky. You can create, share, and subscribe to starter packs for any platform that supports RSS. That includes blogs, podcasts, forums, YouTube, Mastodon, etc. [Manton seems to have something similar in mind](https://www.manton.org/2024/12/07/with-the-success.html) as a means of building on open standards that make it easy for Micro.blog to interop with various platforms. 

If you're interested in what that might look like in practice, check out my "starter packs" which you can subscribe to using your RSS reader of choice and the provided OPML files. 

- [Blogs](https://www.lqdev.me/feed/blogroll)
- [Podcasts](https://www.lqdev.me/feed/podroll)
- [YouTube](https://www.lqdev.me/feed/youtube)
- [Forums](https://www.lqdev.me/feed/forums)

I'm still working on similar collections for Mastodon and Bluesky but the same concept applies.  

Although these are just simple examples, it shows the importance of building on open standards and the open web. Doing so introduces more freedom for creators and communities. 

Here are other "starter packs" you might consider subscribing to. 

- [People and Blogs](https://www.thingelstad.com/2024/12/03/people-blogs-list.html)
- [Blogroll Club](https://blogroll.club/faq/)

If this is interesting to you, [Feedland](https://feedland.org/) might be a project worth checking out.  ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/rss-opml-starter-packs</link>
      <guid>https://www.luisquintanilla.me/posts/rss-opml-starter-packs</guid>
      <pubDate>2024-12-08 19:40</pubDate>
    </item>
    <item>
      <title>POSSE to Mastodon using RSS and Azure Logic Apps</title>
      <description>&lt;![CDATA[
## Introduction

[Over the past year, I've made this website the main place where I post content](/feed/weblogging-rewind-2023). The easiest way to [subscribe to content on my website](/subscribe) is through the various RSS feeds. However, I have accounts on other platforms like X (formerly Twitter), Bluesky, and Mastodon where I'd still like to repost my content to. Since the [changes to the Twitter API](https://techcrunch.com/2023/03/29/twitter-announces-new-api-with-only-free-basic-and-enterprise-levels/), the only place I cross-post to is Mastodon. The main reason behind it is, as of now, it's one of the few platforms that allows me to automate post creation via its REST APIs without restrictions. A large part of that is I self-host my own Mastodon instance but I assume there aren't as many restrictions when using other instances like mastodon.social. The way I automate posting is by setting up workflows using Azure Logic Apps. These workflows subscribe to my various RSS feeds and whenever a new post is published, they make an HTTP request to my Mastodon instance to create a new post. I've been doing this for some time but never got around to documenting it. This blog post goes into more details about how to set up these workflows.  

## What is POSSE

[POSSE](https://indieweb.org/POSSE) is short for "Post on your Own Site Syndicate Elsewhere". It's one of the patterns adopted within IndiWeb communities and projects. The main idea is, your website or a website you own or have administrative rights to becomes the main platform where you publish your content. Effectively, it's the cannonical version of your content. Once your content is on your website, you can optionally choose to distribute it on other platforms. 

## What is RSS

[RSS](https://en.wikipedia.org/wiki/RSS) is short for Really Simple Syndication. Accorting to Wikipedia, this protocol "allows users and applications to access updates to websites in a standardized, computer-readable format". 

## What is Mastodon

[Mastodon](https://joinmastodon.org/) is an [open-source](https://github.com/mastodon/mastodon) decentralized microblogging platform built on the [ActivityPub](https://activitypub.rocks/) protocol and part of the larger collective of federated systems known as the [Fediverse](https://fediverse.info/). 

## What are Azure Logic Apps

If you've used workflow automation systems like [IFTTT](https://ifttt.com/), then you generally know what Azure Logic Apps are. 

A more formal definition from the Azure documentation - "Azure Logic Apps is a cloud platform where you can create and run automated workflows with little to no code. By using the visual designer and selecting from prebuilt operations, you can quickly build a workflow that integrates and manages your apps, data, services, and systems."

For more details, see the [Azure Logic Apps documentation](https://learn.microsoft.com/azure/logic-apps/logic-apps-overview).

## Prerequisites

Since you'll be creating Logic App Resources on Azure, you'll need an [Azure account](https://aka.ms/free).

## Create Consumption Azure Logic App Resource

There's various ways to create an Azure Logic App Resource but the easiest one for this relatively simple workflow is using the Azure Portal. For more details, see the [create a consumption logic app resource documentation](https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#create-a-consumption-logic-app-resource).

For the most part you can leave the defaults as is. Since the intended use for this workflow is personal and I don't need enterprise features, I chose to create my logic app using the consumption plan. 

When prompted to choose **Plan Type**, select **Consumption**. 

## The Workflow

The workflow is relatively simple. Whenever a new item is posted to an RSS feed, make an HTTP POST request to the Mastodon API

```mermaid
flowchart TD
    A["RSS"] --&gt; B["HTTP (Mastodon API)"]
```

Once your Logic App resource deploys to Azure, create a new logic app using the Blank Template. For more details, see the [select a blank template documentation](https://learn.microsoft.com/azure/logic-apps/quickstart-create-example-consumption-workflow#select-the-blank-template).

This will launch you into the Logic app designer UI where you can begin to configure your workflow.

### RSS trigger

The first thing you'll want to do is set up the trigger that initiates the workflow. The trigger in this case will be new posts on an RSS feed.

In the Logic app designer, search for *RSS* and add it as a trigger.

Once the RSS trigger is added to your workflow, configure it as follows:

- **The RSS feed URL**: The feed you want want to get posts from. In this case, I'm using my blog posts [feed](https://www.luisquintanilla.me/posts/index.xml). 

    **TIP: The RSS feed can be ANY feed. So if you wanted to subscribe and repost items from your favorite blog or website, you can as well. Just note though that if you don't own the content it might not be something the author wants you doing and for websites that update multiple times a day, it might produce a lot of noise in your feed. Other than that though, subscribe and post away!**

- **How often do you want to check for items?**: The frequency at which you want to poll the RSS feed and check for new posts. In my case, I don't post very often but I do want my posts to be published on Mastodon within an hour of publishing. Therefore, I chose 1 hour as the update frequency. 

### HTTP action

Now that your trigger is configured, it's time to do someting with the latest posts. In this case, since I want to create a new post on Mastodon, I can do so via their REST API. 

#### Get Mastodon credentials

One thing that you'll need to publish posts to Mastodon is an application token. The easiest way to get one is using the Web UI. 

To get your app credentials through the web UI:

1. In the Mastodon Web UI, select **Preferences**.
1. In the preferences page, select **Development**.
1. In the development preferences tab, select **New application**.
1. In the new application page:
  - Provide a name for your application
  - Choose the **write:statuses** scope checkbox. For details on required scopes to post statuses, see the [Post a new status REST API documentation](https://docs.joinmastodon.org/methods/statuses/#create)
  - Select **Submit**

If successful, this will create credentials and a token you can use to send authenticated requests to the Mastodon API.

#### Configure HTTP action

Now that you have your credentials, you can configure your HTTP action in the Logic Apps workflow.

In the Logic App designer:

1. Select **+ New Step**.
1. Search for *HTTP* and add it as an action. 
1. Once the HTTP action is added, configure it as follows:

- **Method**: The HTTP Method. To create statuses, choose **POST** from the dropdown.
- **URI**: The endpoint to make the requests to. For mastodon, it's `https://&lt;HOST&gt;/api/v1/statuses`. Make sure to replace `&lt;HOST&gt;` with your Mastodon instance. In my case, it's `toot.lqdev.tech` since that's where I host my mastodon instance. 
- **Headers**: HTTP Headers to use. In this case, set the following:

    | Key | Value |
    | --- | --- |
    | Content-Type | application/x-www-form-urlencoded |

- **Queries**: URL Query Parameters to add to the request. This is where you'll set your access token.

    | Key | Value |
    | --- | --- |
    | access_token | `[YOUR-ACCESS-TOKEN]` |

    Make sure to replace `[YOUR-ACCESS-TOKEN]` with the token credential generated in the Mastodon Web UI.
- **Body**: The content to be published in your Mastodon post. This is entirely up to you though at minumum, you'll add the following to the text feed. `status=&lt;YOUR-CONTENT&gt;`. `&lt;YOUR-CONTENT&gt;` is what will be displayed in the Mastodon post. 

    One of the nice things about Azure Logic Apps is, properties from previous steps are available to you in subsequent steps. Since our trigger is an RSS feed, we can get access to the feed and item properties of our feed in the HTTP action. If all you wanted was to post the tile and URL, you can do that using the **Feed title** and **Primary Feed Link** properties. For more details, see the [RSS connector documentation](https://learn.microsoft.com/connectors/rss/). 

## Save and run

That's it! Now you just need to select **Save** in the Logic app designer page. 

Once it's saved, click **Run trigger** which will kick off your trigger. If you have anything recent to publish and everything is configured correctly, it should show up in your Mastodon feed.

## Conclusion

By publishing content on your own website first, you're in full control of your content. Regardless of which platforms come and go, you won't have to adjust to those changes because your content is not locked in to those platforms. However, that doesn't mean you can't also publish your content there. Using protocols like RSS make it easy to subscribe to updates on your website. Using REST APIs provided by the respective platforms, you can automate publishing these updates. To further automate and simplify this process, you can use services like Azure Logic Apps to make publishing to all places easy. ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/rss-to-mastodon-posse-azure-logic-apps</link>
      <guid>https://www.luisquintanilla.me/posts/rss-to-mastodon-posse-azure-logic-apps</guid>
      <pubDate>2023-12-24 16:40</pubDate>
    </item>
    <item>
      <title>Scaffolding A Web Page In One Line</title>
      <description>&lt;![CDATA[
## Introduction

A few months ago, I saw VS Code released an update that had [Emmet](https://www.emmet.io/) abbreviations built in. At the time, I saw a demo and it looked like an interesting tool but I knew very little about it. This past week, while practicing `css` and scaffolding a blog-like web page for practice I decided to give Emmet a try. After reading some of the documentation and trying it out, within five minutes I was able to replicate what I had done manually with one line of "code". The entire process is described below. 

## What We'll Build

Displayed below is the scaffold of the webpage I built. As it can be seen, in the `body` element there is a `nav` element and four `div` elements that will contain our articles and their metadata.

```html
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;&lt;/title&gt;
    &lt;link rel="stylesheet" href=""&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;nav&gt;
        &lt;ul&gt;
            &lt;li class="nav-item"&gt;&lt;/li&gt;
            &lt;li class="nav-item"&gt;&lt;/li&gt;
            &lt;li class="nav-item"&gt;&lt;/li&gt;
        &lt;/ul&gt;
    &lt;/nav&gt;
    &lt;div&gt;
        &lt;div class="article"&gt;
            &lt;h2 class="article-title"&gt;&lt;/h2&gt;
            &lt;h6 class="article-date"&gt;&lt;/h6&gt;
            &lt;p class="article-description"&gt;&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
        &lt;div class="article"&gt;
            &lt;h2 class="article-title"&gt;&lt;/h2&gt;
            &lt;h6 class="article-date"&gt;&lt;/h6&gt;
            &lt;p class="article-description"&gt;&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
        &lt;div class="article"&gt;
            &lt;h2 class="article-title"&gt;&lt;/h2&gt;
            &lt;h6 class="article-date"&gt;&lt;/h6&gt;
            &lt;p class="article-description"&gt;&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
        &lt;div class="article"&gt;
            &lt;h2 class="article-title"&gt;&lt;/h2&gt;
            &lt;h6 class="article-date"&gt;&lt;/h6&gt;
            &lt;p class="article-description"&gt;&lt;/p&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
```

## How We'll Build It

The `hmtl` above can be programatically built using the abbreviation below.

```text
html&gt;(head&gt;title+link)+(body&gt;(nav&gt;ul&gt;li.nav-item*3)+(div&gt;div.article&gt;h2.article-title+h6.article-date+p.article-description)*4
```

To break down some of the things that are going on here, we can start with the syntax. The `&gt;` character denotes that the item on the left is the parent of the item(s) on the right. Similarly, the `+` character is used to describe siblings. Similar to math operations, parentheses evaluate what's inside them. Elements themselves can contain additional data such as class names and ids. This data can be appended to the element name just like `css` with `.` for classes and `#` for ids. 

With that knowledge, we can see that we have an `html` root element with `head` and `body` children. The `head` element has a `title` and `link` child elements. The `body` element has a `nav` element with a `ul` child which contains three `li` elements. The `nav` element has a `div` sibling which contains four `div` children that serve as the containers for our articles and their metadata.  

## Adding some style

To help visualize what the page looks like we can add some `css`. 

```css
* {
    margin: 0;
}

nav {
    padding: 0;
    background-color: black;
    color: white;
}

.nav-item {
    margin: 20px;
    display: inline-block;
    font-weight: bold;
    font-size: 20px;
}

.nav-item:hover {
    opacity: 0.7;
}

.article {
    display: block;
    margin: 20px;
    padding-left: 5px;
    background-color: lightgray;
    border: black solid 2px;
}

.article-title {
    display: block;
    font-family: serif;
    font-size: 30px;
    text-decoration: underline;
    display: inline;
}

.article-date {
    font-size: 20px;
    font-family: fantasy;
    float: right;
    margin-right: 5px;
}

.article-description {
    display: block;
    font-size: 18px;
    font-family: sans-serif;
    margin-top: 5px;
}
```

## Result

After linking our `css` with the webpage as well as adding some content to the page, the result will look as follows. 

![](/images/scaffold-a-webpage-in-one-line/scaffoldawebpageoneline1.png)


```html
&lt;html&gt;
    &lt;head&gt;
        &lt;link rel="stylesheet" type="text/css" href="main.css"/&gt;
        &lt;title&gt;HTML CSS Practice&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;nav&gt;
            &lt;ul&gt;
                &lt;li class="nav-item"&gt;Home&lt;/li&gt;
                &lt;li class="nav-item"&gt;About&lt;/li&gt;
                &lt;li class="nav-item"&gt;Contact&lt;/li&gt;
            &lt;/ul&gt;
        &lt;/nav&gt;
        &lt;div&gt;
            &lt;div class="article"&gt;
                &lt;h2 class="article-title"&gt;Hello World&lt;/h2&gt;
                &lt;h6 class="article-date"&gt;January 8, 2017&lt;/h6&gt;
                &lt;p class="article-description"&gt;This is a short description of my program. There are more things than this.&lt;/p&gt;
            &lt;/div&gt;
            &lt;div class="article"&gt;
                &lt;h2 class="article-title"&gt;Second Post&lt;/h2&gt;
                &lt;h6 class="article-date"&gt;February 8, 2017&lt;/h6&gt;
                &lt;p class="article-description"&gt;I think that I got my feet wet a little bit. Maybe there are other things that I can write about&lt;/p&gt;
            &lt;/div&gt;
            &lt;div class="article"&gt;
                &lt;h2 class="article-title"&gt;Third Post&lt;/h2&gt;
                &lt;h6 class="article-date"&gt;March 8, 2017&lt;/h6&gt;
                &lt;p class="article-description"&gt;This is the third go around. I think I got pretty good at this design stuff.&lt;/p&gt;
            &lt;/div&gt;
            &lt;div class="article"&gt;
                &lt;h2 class="article-title"&gt;Fourth Post&lt;/h2&gt;
                &lt;h6 class="article-date"&gt;June 8, 2017&lt;/h6&gt;
                &lt;p class="article-description"&gt;I really like my description of the things that are happening here and there is another post on the way very soon.&lt;/p&gt;
            &lt;/div&gt;                        
        &lt;/div&gt;
    &lt;/body&gt;    
&lt;/html&gt;
```

###### Sources
[Emmet](https://www.emmet.io/)

]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/scaffold-a-webpage-in-one-line</link>
      <guid>https://www.luisquintanilla.me/posts/scaffold-a-webpage-in-one-line</guid>
      <pubDate>2018-03-10 16:07:00</pubDate>
    </item>
    <item>
      <title>Sending Webmentions with F#</title>
      <description>&lt;![CDATA[
## Introduction

&gt; This post is part of the [F# Advent 2021](https://sergeytihon.com/2021/10/18/f-advent-calendar-2021). 

Webmentions is a W3C specification that defines a standard way of implementing notifications between websites. It enables rich interactions between websites using standard protocols. According to the spec, "Webmention is a simple way to notify any URL when you mention it on your site. From the receiver's perspective, it's a way to request notifications when other sites mention it". 

If you're interested in learning more about the spec and overall use cases, see the following links:

- [Webmentions: Enabling better communication on the internet](https://boffosocko.com/2018/07/19/webmentions-enabling-better-communication-on-the-internet-2/)
- [Sending your first Webmention from scratch](https://aaronparecki.com/2018/06/30/11/your-first-webmention)
- [Webmentions W3C specification](https://www.w3.org/TR/webmention/)
- [Webmentions implementation tests](https://webmention.rocks/)

The Webmentions protocol has send and receive components. In this post, I'll go over a lightweight implementation for sending Webmentions using F# and test it using the [webmention.rocks](https://webmention.rocks/) website. Source code for this post can be found at the [fsadvent-2021-webmentions GitHub repository](https://github.com/lqdev/fsadvent-2021-webmentions).

## Sending Webmention workflow

Let's say you want to use webmentions to reply to or mention content from another site. The general workflow works as follows:

1. You create a document (source) that mentions content from another website (target). 
1. You perform discovery on the target website for the webmention endpoint URL. The URL might be in one of three places:

    1. HTTP response headers.
    2. `&lt;link&gt;` tag with `rel=webmention` attribute.
    3. `&lt;a&gt;` tag with `rel=webmention` attribute.

    The discovery is performed in order and each subsequent option works as a fallback of the other. 

1. You make an HTTP POST request notifying the source that you've mentioned their article on your site. The request body is in `x-www-form-urlencoded` form containing the `source` and `target` parameters where source is the URL of your article and the target is the URL of article being mentioned. 

For more details on this workflow, see the [Sending Webmentions section](https://www.w3.org/TR/webmention/#sending-webmentions) of the specification.

## Create a document

The document can be any publicly hosted HTML document as long as you have a source URL to provide during your notification request.

For example, you can have something like the following:

```html
&lt;!doctype html&gt;
&lt;html lang="en"&gt;
&lt;head&gt;
    &lt;meta charset="UTF-8"&gt;
    &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
    &lt;title&gt;F# Advent 2021 - Webmentions&lt;/title&gt;
&lt;/head&gt;
    &lt;body&gt;
        &lt;a href="https://webmention.rocks/test/1"&gt;F# Advent 2021 - This is a great post&lt;/a&gt;
    &lt;/body&gt;
&lt;/html&gt;
```

Using [microformats](http://microformats.org/wiki/h-entry) though, you can annotate your HTML to provide more context to the receiver. The same document with microformats providing the author, content, and source document being replied to might look like the following with microformats:

```html
&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
&lt;head&gt;
    &lt;meta charset="UTF-8"&gt;
    &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
    &lt;title&gt;F# Advent 2021 - Webmentions&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;div class="h-entry"&gt;
        &lt;div class="u-author h-card"&gt;
            &lt;a href="http://lqdev.me" class="u-url p-name"&gt;lqdev&lt;/a&gt;
        &lt;/div&gt;
        &lt;p&gt;In reply to: &lt;a href="https://webmention.rocks/test/1" class="u-in-reply-to"&gt;@webmention.rocks&lt;/a&gt;&lt;/p&gt;
        &lt;p class="e-content"&gt;F# Advent 2021 - This is a great post&lt;/p&gt;
    &lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
```

Receiving Webmentions is beyond the scope of this post, so I'll skip over them. Once you have a document and make it publicly accessible to the web, it's time to find where to send notifications to.

## Discover Webmention endpoint URL

To discover where to send the Webmention to, send an HTTP GET or HTTP HEAD request to the target URL. Once you get the response, the Webmention endpoint URL should be in one of three places:

- HTTP headers

    ```text
    GET /test/1 HTTP/1.1
    Host: webmention.rocks
    HTTP/1.1 200 OK
    Link: &lt;https://webmebtion.rocks/test/1/webmention&gt;; rel="webmention"
    ```


- `&lt;link&gt;` tag

    ```html
    &lt;html&gt;
        &lt;head&gt;
        ...
            &lt;link href="http://webmention.rocks/test/1/webmention" rel="webmention" /&gt;
        ...
        &lt;/head&gt;
    &lt;/html&gt;
    ```
- `&lt;a&gt;` tag

    ```html
    &lt;html&gt;
        &lt;body&gt;
        ....
        &lt;a href="http://webmention.rocks/test/1/webmention" rel="webmention"&gt;webmention&lt;/a&gt;
        ...
        &lt;/body&gt;
    &lt;/html&gt;
    ```

The following examples show how you'd implement the discovery process for each of these scenarios in F#.

### Discover Webmention URL in header

The `discoverUrlInHeaderAsync` function uses the .NET HttpClient to make an HTTP HEAD request to get the headers from the target URL provided. Then, a search is performed for a "link" header containing the text "webmention". Once the header is found, it's sanitized to extract the Webmention endpoint URL of the target site.

```fsharp
let discoverUrlInHeaderAsync (url:string) =
    async {
        // Initialize HttpClient
        use client = new HttpClient()

        // Prepare request message
        let reqMessage = new HttpRequestMessage(new HttpMethod("HEAD"), url)
        
        // Send request
        let! response = client.SendAsync(reqMessage) |&gt; Async.AwaitTask

        // Get request headers
        let responseHeaders = 
            [
                for header in response.Headers do
                    header.Key.ToLower(), header.Value
            ]

        // Look for webmention header
        try
            // Find "link" header that contains "webmention"
            let webmentionHeader =
                responseHeaders
                |&gt; Seq.filter(fun (k,_) -&gt; k = "link")
                |&gt; Seq.map(fun (_,v) -&gt; v |&gt; Seq.filter(fun header -&gt; header.Contains("webmention")))
                |&gt; Seq.head
                |&gt; List.ofSeq
                |&gt; List.head

            // Get first part of "link" header
            let webmentionUrl = 
                webmentionHeader.Split(';')
                |&gt; Array.head

            // Remove angle brackets from URL
            let sanitizedWebmentionUrl = 
                webmentionUrl
                    .Replace("&lt;","")
                    .Replace("&gt;","")
                    .Trim()

            return Some sanitizedWebmentionUrl                    

        with
            | _ -&gt; return None
        
    }
```

### Discover Webmention URL endpoint in link tag

The next place to check for a Webmention endpoint URL is a `&lt;link&gt;` tag in the document. In this example, the `discoverUrlInLinkTagAsync` function uses `FSharp.Data` library to get the contents of the target URL and parse the HTML contents to find a link tag with a `rel=webmention` attribute value.

```fsharp
let discoverUrlInLinkTagAsync (url:string) = 
    async {
        // Load HTML document
        let! htmlDoc = HtmlDocument.AsyncLoad(url)

        // Get webmention URL from "&lt;link&gt;" tag
        try
            let webmentionUrl = 
                htmlDoc.CssSelect("link[rel='webmention']")
                |&gt; List.map(fun link -&gt; link.AttributeValue("href"))
                |&gt; List.head

            return Some webmentionUrl
        with
            | _ -&gt; return None
    }
```

### Discover Webmention endpoint URL in anchor tag

The last place to check for the Webmention endpoint URL is in an anchor tag in the contents of the target URL. In this example, the `discoverUrlInAnchorTagAsync` uses the `FSharp.Data` library to get the contents of the target URL and parse the HTML contents to find an anchor tag with a `rel=webmention` attribute value.

```fsharp
let discoverUrlInAnchorTagAsync (url:string) = 
    async {
        // Load HTML document
        let!  htmlDoc = HtmlDocument.AsyncLoad(url)

        // Get webmention URL from "&lt;a&gt;" tag
        try
            let webmentionUrl = 
                htmlDoc.CssSelect("a[rel='webmention'")
                |&gt; List.map(fun a -&gt; a.AttributeValue("href"))
                |&gt; List.head

            return Some webmentionUrl
        with
            | _ -&gt; return None
    }
```

Once the utility functions are in place to handle the different scenarios, create a new function to perform the discovery workflow. 

```fsharp
// Apply webmention URL discovery workflow
// 1. Check header
// 2. Check link tag
// 3. Check anchor tag
let discoverWebmentionUrlAsync (url:string) = 
    async {
        let! headerUrl = discoverUrlInHeaderAsync url
        let! linkUrl = discoverUrlInLinkTagAsync url
        let! anchorUrl = discoverUrlInAnchorTagAsync url

        // Aggregate results
        let discoveryResults = [headerUrl; linkUrl; anchorUrl]

        // Unwrap and take the first entry containing a value
        let webmentionUrl = 
            discoveryResults
            |&gt; List.choose(fun url -&gt; url)
            |&gt; List.head

        return webmentionUrl
    }
```

The `discoverWebmentionUrlAsync` calls all of the discovery utility methods and chooses only the results that successfully extracted a Webmention endpoint URL. In the event of multiple endpoints, the first one is takes precedence. 

## Send Webmention

Now that you have a publicly accessible document and the Webmention URL endpoint to send your Webmention to, it's time to send it. 

```fsharp
let sendWebMentionAsync (url:string) (req:IDictionary&lt;string,string&gt;) = 
    async {
        use client = new HttpClient()
        let content = new FormUrlEncodedContent(req)
        let! response = client.PostAsync(url, content) |&gt; Async.AwaitTask
        return response.IsSuccessStatusCode
    }
```

The `sendWebMentionAsync` function uses the .NET HttpClient to send an HTTP POST request to the Webmention endpoint URL you just extracted. 

Define your source and target URLs

```fsharp
let sourceUrl = new Uri("https://raw.githubusercontent.com/lqdev/fsadvent-2021-webmentions/main/reply.html")
let targetUrl = new Uri("https://webmention.rocks/test/1")
```

The source URL I used in this example is hosted on [GitHub](https://raw.githubusercontent.com/lqdev/fsadvent-2021-webmentions/main/reply.html) and the target URL is the first implementation test in the [webmention.rocks](https://webmention.rocks/test/1) website.

Then, create a function to run the entire workflow end-to-end.

```fsharp
let runWebmentionWorkflow () = 
    async {
        // Discover webmention endpoint URL of target URL
        let! discoveredUrl = 
            targetUrl.OriginalString
            |&gt; discoverWebmentionUrlAsync

        // Construct URL depending on whether it's absolute or relative
        let authority = targetUrl.GetLeftPart(UriPartial.Authority)

        let constructedUrl = 
            match (discoveredUrl.Contains("http")) with
            | true -&gt; discoveredUrl
            | false -&gt; 
                let noQueryUrl = 
                    discoveredUrl.Split('?')
                    |&gt; Array.head
                    
                $"{authority}{noQueryUrl}"

        // Prepare webmention request data
        let reqData = 
            dict [
                ("source", sourceUrl.OriginalString)
                ("target", targetUrl.OriginalString)
            ]

        // Send web mentions
        return! sendWebMentionAsync constructedUrl reqData
    }
```

Now that you have everything set up, you're ready to send your webmention! 

```fsharp
runWebmentionWorkflow ()
|&gt; Async.RunSynchronously
```

If your post is successful, you should see it on the target URL.

![Display of Webmention post on webmention.rocks](https://user-images.githubusercontent.com/11130940/145919398-2576bfef-3a5d-44f4-9813-9cf933539ede.png)

## Conclusion

In this post, I showed how you can use F# to implement sending webmentions using F#. This is not a full implementation as there are still scenarios I need to account for like receiving webmentions. There are community maintained [libraries](https://indieweb.org/Webmention-developer#Libraries), [plugins](https://indieweb.org/Webmention#Publishing_Software), and [services](https://indieweb.org/Webmention#Services) to simplify the process, but because the specification is built on open protocols and standards, it's possible to build your own implementation in F# and integrate it into your website. Happy coding!]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/sending-webmentions-fsharp-fsadvent</link>
      <guid>https://www.luisquintanilla.me/posts/sending-webmentions-fsharp-fsadvent</guid>
      <pubDate>12/13/2021 20:53</pubDate>
    </item>
    <item>
      <title>Serverless Machine Learning with ML.NET and Azure Functions</title>
      <description>&lt;![CDATA[
## Introduction

In a previous blog [post](http://luisquintanilla.me/2018/05/11/deploy-netml-docker-aci/), I explored how to build and deploy machine learning models built with the `ML.NET` framework using an ASP.NET Core Web API, Docker and Azure Container Instances. While this is certainly a good way to deploy such models especially those that are critical and require high availability and/or consist of long-running processes, it's not the case when those requirements are not needed. In such cases serverless computing makes more sense from a cost and resource utilization standpoint. Therefore, in this blog post I will go over how to train a classification model with `ML.NET` and deploy it using Azure Functions. Source code for this post can be found at the following [link](https://github.com/lqdev/azfnmlnetdemo).

## Prerequisites

Prior to starting, make sure you have all of the necessary software to build this project. Although this project was built on a system running Ubuntu 16.04 it should work cross-platform.

- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest)
- [Azure Functions Core Tools Version 2.x](https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local)
- [.NET Core SDK 2.0+](https://www.microsoft.com/net/download)

## Set Up Azure Environment

Before writing any code we want to configure our Azure environment. To do so we'll be using the Azure CLI. Although in these examples I am providing the resource group name, storage account name and function application name feel free to use one of your choosing. Naming is not as important for resource group or storage account but definitely is the case for the application.

Fist we want to log into our account using the following command

```bash
az login
```

This will guide you through a series of prompts that will eventually result in you being logged in. To make sure you are logged in you can use the `account` command.

```bash
az account list
```

The following output should appear if successfull.

```bash
[
  {
    "cloudName": "AzureCloud",
    "id": "&lt;YOUR-ID&gt;",
    "isDefault": true,
    "name": "Pay-As-You-Go",
    "state": "Enabled",
    "tenantId": "&lt;YOUR-TENANT-ID&gt;",
    "user": {
      "name": "&lt;YOUR-USERNAME&gt;",
      "type": "user"
    }
  }
]
```

Next, we want to create a resource group to contain all of our Azure resources for this application.

```bash
az group create --name azfnmlnetdemo --location eastus
```

Once our resource group is created, it's time to start adding resources for it. First we'll add a storage account which will contain our trained model.

```bash
az storage account create --name azfnmlnetdemostorage --location eastus --resource-group azfnmlnetdemo --sku Standard_LRS
```

Then we'll create an Serverless Function Application and link it to our storage account. We'll want to create a unique name for it. An easy way to do so is to add the date to the end of the name of our application (i.e. myappname20180816).

```bash
az functionapp create --name azfnmlnetdemo20180821 --storage-account azfnmlnetdemostorage --consumption-plan-location eastus --resource-group azfnmlnetdemo
```

The final step in the environment setup is to set the runtime of our Serverless Function Application in the Application Settings to `beta` which supports `.NET Core`.

```bash
az functionapp config appsettings set --name azfnmlnetdemo20180821 --resource-group azfnmlnetdemo --settings FUNCTIONS_EXTENSION_VERSION=beta
```

Now we're ready to build our machine learning model and upload it to our storage account

## Building The Model

Once our environment is set up we can start building our solution. The first step is to create a directory and initialize our solution inside of it.

### Set Up The Solution

```bash
mkdir azfnmlnetdemo
cd azfnmlnetdemo
dotnet new sln
```

### Create The Model Project

Then we want to create a console project for our model and add it to our solution.

```bash
dotnet new console -o model
dotnet sln add model/model.csproj
```

### Add Dependencies

Since we’ll be using the `ML.NET` framework, we need to add it to our model project.

```
cd model
dotnet add package Microsoft.ML
dotnet restore
```

### Download The Data

Before we start training the model, we need to download the data we’ll be using to train. We do so by creating a directory called `data` and downloading the data file onto there.

```bash
mkdir data
curl -o data/iris.txt https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
```

If we take a look at the data file, it should look something like this:

```bash
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
4.7,3.2,1.3,0.2,Iris-setosa
4.6,3.1,1.5,0.2,Iris-setosa
5.0,3.6,1.4,0.2,Iris-setosa
5.4,3.9,1.7,0.4,Iris-setosa
4.6,3.4,1.4,0.3,Iris-setosa
5.0,3.4,1.5,0.2,Iris-setosa
4.4,2.9,1.4,0.2,Iris-setosa
4.9,3.1,1.5,0.1,Iris-setosa
```

### Train The Model

Now that we have all our dependencies set up, it’s time to train our model. I leveraged the demo that is used on the [ML.NET Getting-Started website](https://www.microsoft.com/net/learn/machine-learning-and-ai/get-started-with-ml-dotnet-tutorial).

#### Define Data Structures

In the root directory of our `model` project, let’s create two classes called `IrisData` and `IrisPrediction` which will define our features and predicted attribute respectively. Both of them will use `Microsoft.ML.Runtime.Api` to add the property attributes.

Here is what our `IrisData` class looks like:

```csharp
using Microsoft.ML.Runtime.Api;

namespace model
{
    public class IrisData
    {
        [Column("0")]
        public float SepalLength;

        [Column("1")]
        public float SepalWidth;

        [Column("2")]
        public float PetalLength;

        [Column("3")]
        public float PetalWidth;

        [Column("4")]
        [ColumnName("Label")]
        public string Label;
    }
}
```

Similarly, here is the `IrisPrediction` class:

```csharp
using Microsoft.ML.Runtime.Api;

namespace model
{
    public class IrisPrediction
    {
        [ColumnName("PredictedLabel")]
        public string PredictedLabels;
    }
}
```

#### Build the Training Pipeline

The way the `ML.NET` computations process data is via a sequential pipeline of steps that are performed eventually leading up to the training of the model. Therefore, we can create a class called `Model` to perform all of these tasks for us.

```csharp
using Microsoft.ML.Data;
using Microsoft.ML;
using Microsoft.ML.Runtime.Api;
using Microsoft.ML.Trainers;
using Microsoft.ML.Transforms;
using Microsoft.ML.Models;
using System;
using System.Threading.Tasks;

namespace model
{
    class Model
    {

        public static async Task&lt;PredictionModel&lt;IrisData,IrisPrediction&gt;&gt; Train(LearningPipeline pipeline, string dataPath, string modelPath)
        {
            // Load Data
            pipeline.Add(new TextLoader(dataPath).CreateFrom&lt;IrisData&gt;(separator:','));

            // Transform Data
            // Assign numeric values to text in the "Label" column, because
            // only numbers can be processed during model training
            pipeline.Add(new Dictionarizer("Label"));

            // Vectorize Features
            pipeline.Add(new ColumnConcatenator("Features", "SepalLength", "SepalWidth", "PetalLength", "PetalWidth"));

            // Add Learner
            pipeline.Add(new StochasticDualCoordinateAscentClassifier());

            // Convert Label back to text
            pipeline.Add(new PredictedLabelColumnOriginalValueConverter() {PredictedLabelColumn = "PredictedLabel"});

            // Train Model
            var model = pipeline.Train&lt;IrisData,IrisPrediction&gt;();

            // Persist Model
            await model.WriteAsync(modelPath);

            return model;
        }
    }
}
```

In addition to building our pipeline and training our machine learning model, the `Model` class also serialized and persisted the model for future use in a file called `model.zip`.

### Test The Model

Now that we have our data structures and model training pipeline set up, it’s time to test everything to make sure it’s working. We’ll put our logic inside of our `Program.cs` file.

```csharp
using System;
using Microsoft.ML;

namespace model
{
    class Program
    {
        static void Main(string[] args)
        {

            string dataPath = "model/data/iris.txt";

            string modelPath = "model/model.zip";

            var model = Model.Train(new LearningPipeline(),dataPath,modelPath).Result;

            // Test data for prediction
            var prediction = model.Predict(new IrisData()
            {
                SepalLength = 3.3f,
                SepalWidth = 1.6f,
                PetalLength = 0.2f,
                PetalWidth = 5.1f
            });

            Console.WriteLine($"Predicted flower type is: {prediction.PredictedLabels}");
        }
    }
}
```

All set to run. We can do so by entering the following command from our solution directory:

```bash
dotnet run -p model/model.csproj
```

Once the application has been run, the following output should display on the console.

```bash
Automatically adding a MinMax normalization transform, use 'norm=Warn' or
'norm=No' to turn this behavior off.Using 2 threads to train.
Automatically choosing a check frequency of 2.Auto-tuning parameters: maxIterations = 9998.
Auto-tuning parameters: L2 = 2.667734E-05.
Auto-tuning parameters: L1Threshold (L1/L2) = 0.Using best model from iteration 882.
Not training a calibrator because it is not needed.
Predicted flower type is: Iris-virginica
```

Additionally, you’ll notice that a file called `model.zip` was created in the root directory of our model project. This persisted model can now be used outside of our application to make predictions, but first we need to upload it to our Azure Storage account.

### Upload The Model

Now that we have a trained model and it has been persisted to the `model.zip` file, it's time to upload it to Azure Storage so that it is available to our Azure Functions application.

To get started with that, first we need the access keys for our storage account. You can get those by using the following command.

```bash
az storage account keys list --account-name azfnmlnetdemostorage --resource-group azfnmlnetdemo
```

The result of that command should return your primary and secondary keys. You can use either one for the following steps.

Although we can upload directly to the account, it's best to create a container to upload our model to. To keep it simple, I'll call the container `models`.

```bash
az storage container create --name models --account-key &lt;YOUR-ACCOUNT-KEY&gt; --account-name azfnmlnetdemostorage --fail-on-exist
```

Once our container's created, we can upload our `model.zip` file to it.

```bash
az storage blob upload --container-name models --account-name azfnmlnetdemostorage --file model/model.zip --name model.zip
```

To verify that the file has been uploaded, you can list the files inside the `models` storage container.

```bash
az storage blob list --container-name models --account-name azfnmlnetdemostorage --output table
```

That command should produce output similar to that below:

```bash
Name       Blob Type    Blob Tier    Length    Content Type     Last Modified              Snapshot
---------  -----------  -----------  --------  ---------------  -------------------------  ----------
model.zip  BlockBlob                 4373      application/zip  2018-08-21T19:26:09+00:00
```

That's all there is to the upload process. It's now time to build our Azure Functions Application

## Build The Azure Functions Application

### Initialize Azure Function Project

In our solution directory, we want to create a new directory for our Azure Function project

```bash
mkdir serverlessfunctionapp
dotnet sln add serverlessfunctionapp/serverlessfunctionapp.csproj
```

Then, we can scaffold an Azure Functions project inside our newly created `serverlessfunctionapp` project directory using Azure Functions Core Tools

```bash
cd serverlessfunctionapp
func init
```

At this point you will be prompted to select the runtime for your application. For this application select `dotnet`.

This will generate a few files in the `serverlessfunctionapp` directory. Keep in mind though that this does not create the function.

### Add Dependencies

Before we create any functions, we need to add the dependencies for our Azure Functions application. Since we'll be using `Microsoft.ML` in our Azure Function application, we'll need to add it as a dependency. From the `serverlessfunctionapp` enter the following command:

```bash
dotnet add package Microsoft.ML
dotnet restore
```

### Create Serverless Function

Once we've added the dependencies it's time to create a new function. To do so we'll use the Azure Functions Core Tools `new` command. Although not required, it's good practice to separate functions and related files into their own directory.

```bash
mkdir Predict
cd Predict
func new
```

At this time you will be prompted to select a template. For our classification model, we'll be using an HttpTrigger which is exactly what it sounds like. An HTTP request is what calls or invokes our function. With that being said, select the `HttpTrigger` option.

You will then be prompted to enter a name for your function. You can use any name but to make things easy, name it the same as the directory the function is in. Once that process is complete, there should be a file called `Predict.cs` inside our `serverlessfunctionapp/Predict` directory. This is where we'll write the logic for our application.

### Define Data Structures

We'll also be making use of the IrisData and IrisPrediction classes inside our `Predict` function. Therefore, we need to create classes for them inside our `Predict` directory. The content will be the same as when we trained our model with the exception of the namespace which will now be `serverlessfunctionapp.Predict`. The content of those files should look like the code below:

```csharp
//IrisData.cs
using Microsoft.ML.Runtime.Api;

namespace serverlessfunctionapp.Predict
{
    public class IrisData
    {
        [Column("0")]
        public float SepalLength;

        [Column("1")]
        public float SepalWidth;

        [Column("2")]
        public float PetalLength;

        [Column("3")]
        public float PetalWidth;

        [Column("4")]
        [ColumnName("Label")]
        public string Label;
    }
}

//IrisPrediction.cs
using Microsoft.ML.Runtime.Api;

namespace serverlessfunctionapp.Predict
{
    public class IrisPrediction
    {
        [ColumnName("PredictedLabel")]
        public string PredictedLabels;
    }
}
```

### Write Function Logic

With our dependencies and data structures set up, it's time to write our function logic to make predictions. The first thing we want to do is replace the `Run` method inside the `Predict` class with the following code.

```csharp
public static IActionResult Run(
    [HttpTrigger(AuthorizationLevel.Function, "get", "post", Route = null)]HttpRequest req,
    [Blob("models/model.zip", FileAccess.Read, Connection = "AzureWebJobsStorage")] Stream serializedModel,
    TraceWriter log)
{
    // Workaround for Azure Functions Host
    if (typeof(Microsoft.ML.Runtime.Data.LoadTransform) == null ||
        typeof(Microsoft.ML.Runtime.Learners.LinearClassificationTrainer) == null ||
        typeof(Microsoft.ML.Runtime.Internal.CpuMath.SseUtils) == null ||
        typeof(Microsoft.ML.Runtime.FastTree.FastTree) == null)
    {
        log.Error("Error loading ML.NET");
        return new StatusCodeResult(500);
    }

    //Read incoming request body
    string requestBody = new StreamReader(req.Body).ReadToEnd();

    log.Info(requestBody);

    //Bind request body to IrisData object
    IrisData data = JsonConvert.DeserializeObject&lt;IrisData&gt;(requestBody);

    //Load prediction model
    var model = PredictionModel.ReadAsync&lt;IrisData, IrisPrediction&gt;(serializedModel).Result;

    //Make prediction
    IrisPrediction prediction = model.Predict(data);

    //Return prediction
    return (IActionResult)new OkObjectResult(prediction.PredictedLabels);
}
```

There are a few notable change worth looking at. One of them is the workaround at the beginning of the function.

```csharp
if (typeof(Microsoft.ML.Runtime.Data.LoadTransform) == null ||
    typeof(Microsoft.ML.Runtime.Learners.LinearClassificationTrainer) == null ||
    typeof(Microsoft.ML.Runtime.Internal.CpuMath.SseUtils) == null ||
    typeof(Microsoft.ML.Runtime.FastTree.FastTree) == null)
{
    log.Error("Error loading ML.NET");
    return new StatusCodeResult(500);
}
```

There are some issues with Azure Functions and ML.NET Assemblies which are being worked on by both teams at Microsoft (see [Github Issue](https://github.com/Azure/azure-functions-host/issues/3190)). In the meantime, it's safe to just include that code in there.

The other addition to note is the method signature. As you can see, I have added an additional parameter called `serializedModel` which is decorated by the `Blob` attribute.

```csharp
[Blob("models/model.zip", FileAccess.Read, Connection = "AzureWebJobsStorage")] Stream serializedModel
```

What this code is doing is telling the function to import the blob `model.zip` as a `Stream` and bind it to `serializedModel`. Using additional arguments, I tell my function to only have `Read` access to the `model.zip` blob inside the `models` container which can be accessed with the `AzureWebJobsStorage` connection string. Right now that last part might seem confusing, but this is something we configured when we set up the Azure environment and linked `azfnmlnetdemostorage` account with our `azfnmlnetdemo20180821` serverless function app using the `--storage-account` option. Although the production environment is configured, if we try to test our application locally we won't be able to access our storage account because we have not configured the connection string locally. We can do so by looking in the `local.settings.json` file inside our `serverlessfunctionapp` directory. The contents should look like the following.

```json
{
  "IsEncrypted": false,
  "Values": {
    "AzureWebJobsStorage": "",
    "AzureWebJobsDashboard": "",
    "FUNCTIONS_WORKER_RUNTIME": "dotnet"
  }
}
```

Our function running locally will look in this file, try to find `AzureWebJobsStorage` and use the connection string value in the `Predict` function. To get the connection string for our `azfnmlnetdemostorage` account, enter the following command.

```bash
az storage account show-connection-string --name azfnmlnetdemostorage
```

The output of that command should look like the following:

```json
{
  "connectionString": "&lt;YOUR-CONNECTION-STRING&gt;"
}
```

At this point, you just need to copy the value of `connectionString` to your `local.settings.json` file and replace the current empty string for `AzureWebJobsStorage`. It's important to note that it's okay to paste the connection string in here since the `local.settings.json` file is not committed to version control. (See `.gitignore` inside `serverlessfunctionapp` directory). Now the application is ready to be tested locally.

### Testing The Function Locally

To test the application, first build your project by entering the following command from the `serverlessfunctionapp` directory.

```bash
dotnet build
```

Then, navigate to the build directory `./bin/Debug/netstandard2.0` and enter the following command:

```bash
func host start
```

Finally, using a tool like Postman or Insomnia make an HTTP POST request to the `http://localhost:7071/api/Predict` endpoint with the following body:

```json
{
  "SepalLength": 3.3,
  "SepalWidth": 1.6,
  "PetalLength": 0.2,
  "PetalWidth": 5.1
}
```

If everything is set up correctly, you should receive the following output

```bash
Iris-virginica
```

Once satisfied with testing, press `Ctrl + C` to stop the application.

## Deploy To Azure

### Push Build

Great! Now on to the final step, deploying our application to production. Since we already configured everything it should only require a few commands to do so.

First, make sure you are logged in. Using Azure Functions Core Tools log in with the following command:

```bash
func azure login
```

Like with the Azure CLI, you will follow a series of prompts to log into your account.

Once you have successfully logged in, it's time to publish our application to Azure. From the `serverlessfunctionapp` directory enter the following command.

```bash
func azure functionapp publish azfnmlnetdemo20180821
```

When our deployment is complete, we can check whether our function was published successfully by using the following command.

```bash
func azure functionapp list-functions azfnmlnetdemo20180821
```

The output should look similar to that below.

```bash
Functions in azfnmlnetdemo20180821:
    Predict - [httpTrigger]
```

### Configure Platform

For the last part of the deployment step, we'll need to head over to the Azure Portal. To do so, visit [https://portal.azure.com](https://portal.azure.com) and log in.

Once logged in, type the name of your application into the search bar at the top of the page and select your Azure Function application of type `App Service`

![](/images/serverless-machine-learning-mlnet-azure-functions/azfnmlnetdemo1.png)

Then, from the accordion element on the left, select the top-most item with your appplication name on it. Then, select the `Platform features` tab and open the `Application settings` option.

![](/images/serverless-machine-learning-mlnet-azure-functions/azfnmlnetdemo2.png)

When the `Application settings` page loads, change the `Platform` setting to `64-bit`. The reason for this is `ML.NET` has to be built and run on a 64-bit environment due to some of its native dependencies. 

![](/images/serverless-machine-learning-mlnet-azure-functions/azfnmlnetdemo3.png)

That's all there is to it.

### Test The Deployed Function

Now it's time to test our deployed function. We can do so from the portal by going back to the accordion and selecting the function name below the `Functions` parent element and clicking on the `Test` button on the far right. Doing so will show a form that will allow us to test our application. Make sure the `HTTP method` option is set to POST. In the text area for the `Request body` paste the following content:

```json
{
  "SepalLength": 3.3,
  "SepalWidth": 1.6,
  "PetalLength": 0.2,
  "PetalWidth": 5.1
}
```

Once the form is filled in, click `Run` at the top of the page and if successful `Iris-virginica` should show up in the `Output` area.

![](/images/serverless-machine-learning-mlnet-azure-functions/azfnmlnetdemo4.png)

To test the function outside the portal, you can click on the `Get function URL` link next to the `Run` button and make an HTTP POST request using that link.

## Conclusion

In this writeup, we trained a classification model that predicts a class of flower using Microsoft's `ML.NET` framework. Then, we exposed this model for inference via an Azure Functions serverless application. In doing so, we can more efficiently manage our cost as well as our resource utilization. Happy coding!

###### Resources

[Create a function app for serverless code execution](https://docs.microsoft.com/en-us/azure/azure-functions/scripts/functions-cli-create-serverless)  
[Using the Azure CLI 2.0 with Azure Storage](https://docs.microsoft.com/en-us/azure/storage/common/storage-azure-cli)  
[Work with Azure Functions Core Tools](https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local)
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/serverless-machine-learning-mlnet-azure-functions</link>
      <guid>https://www.luisquintanilla.me/posts/serverless-machine-learning-mlnet-azure-functions</guid>
      <pubDate>2018-08-21 19:13:47</pubDate>
    </item>
    <item>
      <title>Serverless Racket Applications Using Azure Functions Custom Handlers</title>
      <description>&lt;![CDATA[
## Introduction

[Racket](https://racket-lang.org/) is a fun and powerful general purpose programming language based on the Scheme dialect of Lisp that provides tools and packages that allow individuals to quickly be productive. Although you can build traditional web applications with it, it would be nice to use it with cloud-native technologies like serverless. Azure Functions is designed for these types of event-driven workflows but unfortunately does not officially support Racket. Recently, a new feature called custom handlers was announced which allows individuals to run web applications written in any language that supports HTTP primitives as an Azure Function. When I learned of this feature, my immediate thought was, challenge accepted!

Custom handlers require the following:

1. Write a web server to process requests
2. Define the bindings for the request and response function payloads
3. Configure the Azure Functions host to send request to the web server

In this writeup, I'll show how to set up a Racket web server that processes `GET` requests running as an Azure Function. The source code for this project can be found in the [RacketAzureFunctionsCustomHandlerSample GitHub repository](https://github.com/lqdev/RacketAzureFunctionsCustomHandlerSample)

## Prerequisites

This project was built on a Windows 10 PC, but it should work cross-platform on Mac and Linux.

- [Node.js](https://nodejs.org/en/)
- [Racket](https://download.racket-lang.org/)
- [Azure Functions Core Tools](https://docs.microsoft.com/azure/azure-functions/functions-run-local). This sample uses v2.x of the tool.

## Create Azure Functions project

Create a new directory called `RacketAzureFunctionsCustomHandlerSample` and navigate to it.

## Create Racket server

The way Azure Functions custom handlers work is by having the Azure Functions host proxy requests to a web server written in the language of choice which processes the request and sends the response back to the Azure Functions host.

Start by setting the environment variable where Azure Functions and the server listen on. Create and environment variable called `FUNCTIONS_HTTPWORKER_PORT`. In this example, I set the variable to `7071`.

Inside of your application directory, create a file called *server.rkt* which will contain the server logic.

Open the *server.rkt* file. Define the language and import the required packages.

```racket
#lang racket
(require json)
(require web-server/servlet)
(require web-server/servlet-env)
```

Then, get the port where the server listens on from the `FUNCTIONS_HTTPWORKER_PORT` environment variable.

```racket
(define PORT (string-&gt;number (getenv "FUNCTIONS_HTTPWORKER_PORT")))
```

Next, create a function called `get-values` to process your request. In this case, the function receives a `GET` request that returns a JSON object containing a list of integers.

```racket
(define (get-values req)
    (response/full
        200
        #"OK"
        (current-seconds)
        #"application/json;charset=utf-8"
        empty
        (list (jsexpr-&gt;bytes #hasheq((value . (1 2 3)))))))
```

After that, define the routes so your server dispatches requests to the appropriate endpoint. In this case, `GET` requests to the `/values` endpoint are sent to and processed by the `get-values` function.

```racket
(define-values (dispatch req)
    (dispatch-rules
        [("values") #:method "get" get-values]
        [else (error "Route does not exist")]))
```

The final *server.rkt* file should contains content similar to the one below:

```racket
;; Define language and import packages
#lang racket
(require json)
(require web-server/servlet)
(require web-server/servlet-env)

;; Get port where server listens on
(define PORT (string-&gt;number (getenv "FUNCTIONS_HTTPWORKER_PORT")))

;; Create function to handle GET /values request
(define (get-values req)
    (response/full
        200
        #"OK"
        (current-seconds)
        #"application/json;charset=utf-8"
        empty
        (list (jsexpr-&gt;bytes #hasheq((value . (1 2 3)))))))

;; Define routes
(define-values (dispatch req)
    (dispatch-rules
        [("values") #:method "get" get-values]
        [else (error "Route does not exist")]))

;; Define and start server
(serve/servlet
    (lambda (req) (dispatch req))
    #:launch-browser? #f
    #:quit? #f
    #:port PORT
    #:servlet-path "/"
    #:servlet-regexp #rx"")
```

## Test the Racket server

Start the server by running the following command:

```bash
racket --require server.rkt
```

Then, using an application like Postman or Insomina, make a `GET` request to `http://localhost:7071/values`.

The response should look like the following:

```json
{
    "value": [
        1,
        2,
        3
    ]
}
```

## Define function bindings

The way Azure Functions discovers functions is through subdirectories containing a binding definition called *function.json*. The name of the subdirectories must match the name of your function's route path. For example if the route path is `/values`, then the name of the subdirectory is  `values`.

Create a subdirectory inside the main application directory called *values*

Inside the *values* subdirectory, create a file called *function.json* and add the following content to it.

```json
{
    "bindings": [
      {
        "type": "httpTrigger",
        "direction": "in",
        "name": "req",
        "methods": ["get"]
      },
      {
        "type": "http",
        "direction": "out",
        "name": "res"
      }
    ]
}
```

The *function.json* file defines the request and response payloads. In this case, the incoming request is an `HttpTrigger` that only handles `GET` requests and returns an HTTP response.

## Create server executable

Package your server application into a single executable by entering the following command into the command prompt:

```bash
raco exe server.rkt
```

Once your application is packaged, an executable with the name *server.exe* should be created in your application directory.

## Configure Azure Functions host

In your application directory, create a file called *host.json* and add the following contents:

```json
{
  "version": "2.0",
  "httpWorker": {
      "description": {
          "defaultExecutablePath": "server.exe"
      }
  }
}
```

This *host.json* configuration file tells the Azure Functions host where to find the web server executable.

## Run the Azure Functions application

Inside the root application directory, enter the following command into the command prompt.

```bash
func start
```

Using an application like Postman or Insomnia, make a `GET` request to `localhost:7071/api/values`.

The response should look like the following:

```json
{
    "value": [
        1,
        2,
        3
    ]
}
```

## Conclusion

In this writeup, I showed how to create a Racket serverless application that runs on Azure Functions by using custom handlers. Doing so requires you to:

1. Write a web server to process requests
2. Define the bindings for the request and response function payloads
3. Configure the Azure Functions host to send request to the web server

Although in this example, the server was written in Racket, the same process is applicable to other languages. Keep in mind that at the time of this writing, custom handlers are preview and may change. Happy coding!

## Resources

- [Azure Functions Custom Handlers](https://docs.microsoft.com/azure/azure-functions/functions-custom-handlers)]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/serverless-racket-azure-functions-custom-handlers</link>
      <guid>https://www.luisquintanilla.me/posts/serverless-racket-azure-functions-custom-handlers</guid>
      <pubDate>2020-03-21 13:45:43</pubDate>
    </item>
    <item>
      <title>Setting Up A New Ubuntu PC</title>
      <description>&lt;![CDATA[
# Users

This assumes that you are currently the root user.

Enter the adduser to command to create a new user.

```bash
adduser &lt;username&gt;
```

Immediately, you will be prompted for a password. Enter the password and continue the process. 

## Adding Root Privileges

This is only applicable when granting root priviliges to the newly created user. 

Enter the usermod command to add the new user to the sudo (root) group

```bash
usermod -aG sudo &lt;username&gt;
```

The user should now have root priviliges. 

To test that it worked log in as that user and try to run a command with sudo

```bash
su - &lt;username&gt;
sudo ls -la /root
```

# Emacs

By default, Ubuntu usually comes preloaded or has the default 24.x version of Emacs. I often like to use Emacs along with org-mode and to take full advantage, especially as it pertains to working with source code I have found version 25.x is much better suited for the task. While there are many ways to go about it, installing it via package manager appears to be the easiest. 

## Add repository to PPA

```bash
sudo add-apt-repository ppa:kelleyk/emacs
```

## Update the packages

```bash
sudo apt update
```

## Install Emacs. 
### Text-Only Interface

```bash
sudo apt-get install emacs25-nox
```

### GUI Interface

```bash
sudo apt-get install emacs25
```

## Backup files

Sometimes Emacs keeps a backup file by default. This can cause a lot of clutter in the current directory where work is being performed. There is a way to disable this.

Navigate to `~/.emacs.d/` and edit the `init.el` file and add the following line.

```lisp
(setq make-backup-files nil)
```

# Java

Make sure that your packages and repositories are up to date

```bash
sudo apt-get update
```

## OpenJDK

```bash
sudo apt-get install default-jdk
```

## Oracle JDK
### Add PPA Repository
```bash
sudo add-apt-repository ppa:webupd8team/java
sudo apt-get update
```

### Install the installer

```bash
sudo apt-get install oracle-java8-installer
```


#### Sources
[Users](https://www.digitalocean.com/community/tutorials/how-to-create-a-sudo-user-on-ubuntu-quickstart)  
[Emacs](http://ubuntuhandbook.org/index.php/2017/04/install-emacs-25-ppa-ubuntu-16-04-14-04/)  
[Java](https://www.digitalocean.com/community/tutorials/how-to-install-java-with-apt-get-on-ubuntu-16-04)
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/setting-up-new-ubuntu-pc</link>
      <guid>https://www.luisquintanilla.me/posts/setting-up-new-ubuntu-pc</guid>
      <pubDate>2017-12-09 12:49:19</pubDate>
    </item>
    <item>
      <title>Setting up your Rust development environment using Dev Containers</title>
      <description>&lt;![CDATA[
Setting up a new development environment that's configured with all the SDKs and tools you need to get started working can be a multi-step process that is error-prone. Especially when you don't know whether you want to commit to a specific technology or don't have too much space on your device, you might not want to install everything on your device. Additionally, cleaning up and removing everything you installed isn't always 100%. This is where development environments like Dev Containers can come in handy. 

## What are Dev Containers

A development container (Dev Container) lets you use a container as a full development environment. 

Because your development environment is in a container, the environment is isolated. The builds are reproducible and disposable. This means you can quickly stand up a new clean development environments and dispose of them just as easily. 

For more details, see the [Dev Containers website](https://containers.dev/).

If you have Docker installed, you can run them locally. Otherwise, you can run Dev Containers using GitHub Codespaces

## What are GitHub Codespaces

A codespace is a development environment that's hosted in the cloud.

For more details, see the [GitHub Codespaces website](https://github.com/features/codespaces).

## Set up Dev Container for Rust Development

1. Create a new directory.
1. Add a Dev Container configuration file called *.devcontainer.json* that contains the following content:

    ```json
    {
        "name": "Rust Development Environment",
        "image": "mcr.microsoft.com/devcontainers/rust:latest",
        "customizations": {
            "vscode": {
                "extensions": [
                    "rust-lang.rust-analyzer"
                ]
            }
        }
    }
    ```

    - **name**: A human friendly name to identify your development environment. It's not as important when you only have one environment but if you have different environments, it can be useful for differentiating between the various configurations. 
    - **image**: For the most customization, you can provide the path to a Docker file. However, to make things easier, there are a set of prebuilt container images. In this case, I'm using the [Rust image](https://github.com/devcontainers/images/tree/main/src/rust).
    - **customizations.vscode.extensions**: A list of Visual Studio Code extensions that can help you with your development. In this case, it's the [*rust-analyzer*](https://marketplace.visualstudio.com/items?itemName=rust-lang.rust-analyzer).

At minimum, this is all you need to get started. 

## Start your environment 

[Visual Studio Code provides good support for Dev Containers](https://code.visualstudio.com/docs/devcontainers/tutorial). 

1. Install [VS Code](https://code.visualstudio.com/download)
1. Open your directory where you configured your Dev Container in Visual Studio Code
1. Install the [Dev Containers VS Code extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers).
1. Open the command palette in VS Code. In the menu bar, select **View &gt; Command Palette.**
1. Enter the following command **&gt;Dev Containers: Open Dev Container**. This will start building your dev container. Wait for it to start.

## Check your environment

Once your Dev Container starts, check that `rustc` is installed.

1. Open the terminal.
1. Run `rustc -h`. A help message should output to the console for the Rust compiler. 

## Build your app

For simplicity, I've used the [Hello World sample from the Rust By Example website](https://doc.rust-lang.org/rust-by-example/hello.html).

1. Create a new file called *hello.rs*.
1. Add the following code:

    ```rust
    fn main ()
    {
        println!("Hello World!");
    }
    ```
1. Compile your program

    ```bash
    rustc hello.rs
    ```

    This will generate an executable called *hello*

1. Run your app

    ```bash
    ./hello
    ```

    The application should print out the string "Hello World" to the console. 

That's it! You now have a Rust development environment to get started learning and building apps. 

## Conclusion

Dev Containers make it easy for you to configure full-featured development environments. When paired with Visual Studio Code and GitHub Codespaces, they can help you focus on learning and building applications rather than setting up your environment. You can find a [final version of the configured environment and app on GitHub](https://github.com/lqdev/rust-codespace-sandbox). Happy coding! 

## Additional Resources

- [Rust in Visual Studio Code](https://code.visualstudio.com/docs/languages/rust)
- [Rust by Example](https://doc.rust-lang.org/rust-by-example/index.html)
- [Developing Inside a Container](https://code.visualstudio.com/docs/devcontainers/containers)]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/setting-up-rust-dev-env-devcontainers-vscode</link>
      <guid>https://www.luisquintanilla.me/posts/setting-up-rust-dev-env-devcontainers-vscode</guid>
      <pubDate>2024-01-31 21:05</pubDate>
    </item>
    <item>
      <title>Site redirects in static websites</title>
      <description>&lt;![CDATA[
## Introduction

I recently came across the concept of [owning your links](https://indieweb.org/own_your_links) on the IndieWeb wiki and right away it made lots of sense to me. The main idea behind owning your links is using your domain as the source of truth and using it to redirect to your content on various other platforms you own. For example, when sharing my Twitter profile, instead of saying, "Go to https://twitter.com/ljquintanilla", I can instead point people to [http://lqdev.me/twitter](http://lqdev.me/twitter)". The link on my domain ultimately redirects to Twitter, but the main entrypoint is my website. There's various ways to handle redirects, but in most cases it's done on the server side. For static websites, if you're hosting them in GitHub Pages, Netlify, or Azure Blob Storage, you don't have a server. Therefore configuring redirects can be a challenge. In this post, I'll go through how you can configure redirects for your static website without a server or JavaScript. 

## Create redirect page

Depending on which generator you use to create your static website, how you create your  redirect pages will differ. However, the end result should look like the the following:

```html
&lt;html&gt;
    &lt;head&gt;
        &lt;meta http-equiv="refresh" content="0;REDIRECT-TARGET-URL"&gt;
    &lt;/head&gt;
&lt;/html&gt;
```

The only thing you need to perform the redirect is add a `meta` tag to the endpoint you want to handle the redirect. If I wanted to redirect to Twitter, I might create an HTML page at `http://lqdev.me/twitter` on my website. Setting the `http-equiv` attribute to `refresh` instructs the browser to refresh the page. Then, you use the `content` attribute to define your redirect target and how long you want to wait before redirecting the user. In the snippet above, the number `0` indicates you want to wait 0 seconds before redirecting your users and the `REDIRECT-TARGET-URL` is where you want to redirect to. This could be your Twitter profile, YouTube channel, or anywhere else. Wherever you want to redirect to, you place it in the `content` attribute. 

It's important to note that the redirect happens automatically which takes control away from the user causing potential accesibility issues. Until now, I haven't found a better way of doing this that doesn't require JavaScript. For more information, see the [web content accessibility guidelines](https://www.w3.org/WAI/standards-guidelines/wcag/).

For more information on creating redirects in HTML, see the article [how to redirect a web page in HTML](https://www.w3docs.com/snippets/html/how-to-redirect-a-web-page-in-html.html).

## Conclusion

So far I've created redirect pages for most of my social media profiles. You can see samples in my [contact page](/contact.html). It's important to note that while this works for social profiles and any other properties you own on the internet, it could also be used in cases where you've changed the structure of your website and want to gracefully handle redirects for broken / updated links. Happy coding!]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/static-website-redirects</link>
      <guid>https://www.luisquintanilla.me/posts/static-website-redirects</guid>
      <pubDate>07/31/2022 14:52</pubDate>
    </item>
    <item>
      <title>Super simple captcha</title>
      <description>&lt;![CDATA[
## Introduction

I've been thinking about implementing webmentions on this site. According to the spec, "Webmention is a simple way to notify any URL when you mention it on your site. From the receiver's perspective, it's a way to request notifications when other sites mention it". I won't get into the technical details of webmentions in this post, but the simplest way to implement webmentions is to have a text input box on your site for each of your articles, kind of like a comment box. When someone wants to mention your post on their site, they can use the text input box on your site to add the URL from their site where they mention your post. When the user submits the webmention, there's some endpoint that listens for webmention submissions and processes them accordingly. If you're interested in learning more about webmentions, you can check out the following resources:

- [Webmentions spec](https://www.w3.org/TR/webmention/)
- [Webmention Rocks](https://webmention.rocks/)
- [Sending Your First Webmention - IndieWebCamp Düsseldorf 2019](https://yewtu.be/watch?v=ZOlkS6xP2Zk)

While it's great that you can mention and comment on other people's content from your site, like comments, writing a script to submit spam is relatively simple. Therefore, I want to create some sort of [CAPTCHA](https://en.wikipedia.org/wiki/CAPTCHA) challenge as an initial form of validation to prevent spam. At the same time, I don't want to have any external dependencies. In this post, I'll show how I went about implementing a captcha-like solution with zero dependencies to prevent spam submissions. 

## Simple captcha

The solution I came up with asks the users to add the day of the month (1-31) to a random number between 1-100. While not entirely foolproof, it's "complex" enough that it's not the same thing every time. 

Below is what the implementation looks like. 

![Captcha implementation asking for date](https://user-images.githubusercontent.com/11130940/138604591-dfe4c301-78fe-4338-a751-799b420a1791.png)

The HTML markup for the webpage looks like the following:

```html
&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
&lt;head&gt;
    &lt;meta charset="UTF-8"&gt;
    &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
    &lt;title&gt;Super Simple Captcha&lt;/title&gt;
    &lt;script src="main.js" type="text/javascript"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body onload="setQuery()"&gt;
    &lt;div&gt;
        &lt;span id="query"&gt;&lt;/span&gt;
        &lt;input type="text" id="answerInput"&gt;
        &lt;button onclick="displayResult()"&gt;Submit&lt;/button&gt;
    &lt;/div&gt;
    &lt;h3 id="result"&gt;&lt;/h3&gt;
&lt;/body&gt;
&lt;/html&gt;
```

When the `body` element of the page loads, it invokes the `setQuery` function, which displays the challenge the user is supposed to solve in the `query` span element. 

The user then submits their answer via the `answerInput` text input box. 

The answer is then checked by invoking the `displayResult` function. The `displayResult` function checks the user's answer against the expected answer. If the answer is correct, the text "OK" is displayed on the page's `result` H3 element. Otherwise, the text "Try again" displays on the webpage.

All of the code that handles this logic is in the `main.js` file.

```javascript
let date = new Date()
let day = date.getDate() // Day of the month
let randomNumber = Math.floor(Math.random() * 100) + 1
let answer = day + randomNumber // Expected answer

let setQuery = () =&gt; {
    let element = document.getElementById('query');
    element.innerText = `Enter the sum of ${day} + ${randomNumber}`;    
}

let checkAnswer = () =&gt; {
    let userAnswer = parseInt(document.getElementById("answerInput").value);
    return answer === userAnswer
}

let displayResult = () =&gt; {
    let result = checkAnswer() ? "OK" : "Try again" // Ternary function to check if answer is correct
    document.getElementById("result").innerText = result;
}
```

That's all there is to it!

## Conclusion

In this post, I showed how to implement a dependency-free solution to present a challenge to users submitting comments / webmentions to your site. Although the solution isn't foolproof, it's just complex enough any spammers would have to work a little harder. Happy coding!  ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/super-simple-captcha</link>
      <guid>https://www.luisquintanilla.me/posts/super-simple-captcha</guid>
      <pubDate>10/24/2021 13:00</pubDate>
    </item>
    <item>
      <title>Surface Duo offline navigation hiking setup</title>
      <description>&lt;![CDATA[
## Introduction

Over the past few months, I've been going on a hike almost every weekend. Not only does hiking provide an opportunity to discover the natural wonders all around you, it's also good exercise and free (if you don't count entry fees for some state and national parks). Throughout these trips, the [Surface Duo](https://www.microsoft.com/surface/devices/surface-duo), Microsoft's dual-screen Android phone has been a reliable companion. Although it's not the smallest or most rugged device, its large screen(s) paired with OSMAnd~ and AllTrails make it easy to navigate while on the trail. In this post, I'll detail my navigation setup.

![Picture of half-dome taken from Four Mile Trail](https://user-images.githubusercontent.com/11130940/127246913-58472929-f067-41f0-9011-17193f863f71.JPG)

## Getting trail routes from AllTrails

Although trails are often well marked, it's always a good idea to have a map of the trail and surrounding area. This is especially true if it's your first time there. I almost exclusively get my trail maps from [AllTrails](https://www.alltrails.com/). If you're not familiar with AllTrails, it's a platform and community where members share maps, reviews, and information about trails for all kinds of outdoor activities. The site is completely free and while you can pay for premium features, the free offering provides you with all the essentials you'd need to plan your trip.

Trail information, maps, and reviews are entirely user supported. One of the (free) features is the ability to download trails to a variety of formats.

![AllTrails Download Route](https://user-images.githubusercontent.com/11130940/127243740-5ef2e2e3-4c9c-4dbc-a8c5-be608fafeea5.png)

Typically, the format I'll download routes to is GPX Route and GPX Track. [GPX](https://en.wikipedia.org/wiki/GPS_Exchange_Format) is an XML schema for representing GPS data in software applications. Because GPX is a standard format, you can open and view these files in any app that supports them.

## Offline navigation with OSMAnd~

AllTrails has a mobile app. However, for offline navigation, I use the [OSMAnd~](https://f-droid.org/en/packages/net.osmand.plus/) Android app which I downloaded from [F-Droid](https://f-droid.org/). It's also available in the Google Play Store and Apple App Store.

OSMAnd~ is a free map and navigation app supported by [OpenStreetMaps](https://www.openstreetmap.org/). OpenStreetMaps, like AllTrails is free and entirely community driven. OSMAnd~ lets you download **detailed** maps for offline use. Detailed is the key here. Although you can save maps for offline use with applications like Here Maps and Google Maps, they often limit the size of the area you can save for offline use and don't provide much detail. With OSMAnd~ you can:

- Download entire state and country maps for offline use.
- View details like contour-lines, hill shading, and other trail details.
- Turn-by-turn navigation.
- View and edit GPX files

Using the GPX files I get from AllTrails, I load them onto my Surface Duo and view them using OSMAnd~. Using offline maps and the Duo's GPS I'm able to see my position on the trail and quickly find my way when I've gone off it even if I have no reception. This combination makes for a navigation device and experience like no other because I have the full screen real estate that the Duo provides to view areas beyond the trail.

![OSMAnd Android App Displaying Track](https://user-images.githubusercontent.com/11130940/127246186-6b5f02ec-7fef-4f2e-909b-8a54615dc790.png)

## Conclusion

By leveraging the power of crowd-sourcing and open-source, access to quality information has never been easier. When this information is displayed on a device like the Surface Duo it makes it easy to navigate when you're out on the trail especially in situations when you have no reception and you're not familiar with your surroundings.

If you've been looking for an offline navigation solutions, hopefully this post helps provide some guidance. If you do any outdoor activities especially in remote areas, even if you don't have a Duo, I'd suggest giving AllTrails and OSMAnd~ a try. See you on the trail!

P.S. Over the past month, I've transitioned to the Garmin Fenix 6X Pro for navigation. It's a smaller device that supports loading GPX files and provides offline maps with turn-by-turn navigation. I'm also not worried about dropping or scratching it because it's designed for the outdoors. This in no way takes away with how amazing the OSMAnd~ and Surface Duo pairing are for offline navigation. In fact, I still use the Duo for backup navigation when it's difficult to see the map on the Fenix.

]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/surface-duo-offline-navigation-hiking-setup</link>
      <guid>https://www.luisquintanilla.me/posts/surface-duo-offline-navigation-hiking-setup</guid>
      <pubDate>2021-07-27 19:00:00</pubDate>
    </item>
    <item>
      <title>Testing and Deploying Python Projects with Travis CI</title>
      <description>&lt;![CDATA[
# Introduction

When working on projects, especially those that others or yourself may depend upon, it is important to test to make sure that everything is working as expected. Furthermore, being able to deploy your code/packages to a central repository with a package manager makes distribution easier. Although this can all be done manually, it can also be automated. Both testing and deployment can be automated using Travis CI which makes the entire process as easy as pushing your most recent changes to GitHub. In this writeup, I will go over how to create a Python project with unit tests and deploy to PyPI. A sample project can be found at this [link](https://github.com/lqdev/TravisTest)

## Prerequisites

- [GitHub Login](https://github.com/)
- [PyPI Login](https://pypi.python.org/pypi)
- [virtualenv](https://virtualenv.pypa.io/en/stable/)

### Install virtualenv

```bash
sudo pip install virtualenv
```

# Create The Project

For the test project, I will create a module that performs adding, subtracting, increment and decrement operations.

## Define Folder Structure

We start out by creating a directory for our project.

```bash
mkdir travistest
```

Inside that directory, we want to have have a directory for our module as well as for our tests. Therefore, we need to create a directory for both of them.

```bash
mkdir travistest
mkdir test
```

Finally, we want to initialize the virtual environment for our project. To do so, we can use the `virtualenv` package. For the project `python 3.5` is the version that will be used.

```bash
virtualenv -p python3.5 ENV
```

After installation, a folder with the name `ENV` should appear in the root directory of the project. The final directory structure should look like so:

```text
travistest
|_ENV
|_travistest
|_test
```

## Install Modules

For this project, I'll be using `pytest` for unit testing. Before installing anything however, I'll need to activate the virtual environment.

```bash
source ENV/bin/activate
```

Once our virtual environment is activated, we can install `pytest`.

```bash
pip install pytest
```

After installation, we can persist installed packages inside a `requirements.txt` file with the `freeze` command.

```bash
pip freeze &gt; requirements.txt
```

## Create The Module

Inside the `travistest` module directory, the easiest way to create a module is to include an `__init__.py` file inside the directory. It's okay if it's empty.

Therefore, we can start by creating the `__init__` file in that directory.

```bash
touch __init__.py
```

Once that's created, we can begin writing the main functionality of our module. Inside a file called `Operations.py`, we can put the following code in.

```python
class Operations:
    def __init__(self):
        pass
    
    def add(self,x,y):
        return x + y

    def subtract(self,x,y):
        return x - y

    def increment(self,x):
        return x + 1

    def decrement(self,x):
    	return x - 1
```

## Unit Test

Once we have our code, we need to write tests for it. Navigating to the `test` directory, we can add the following code to the `test_operations.py` file.

```python
from pytest import fixture

@fixture
def op():
    from travistest.Operations import Operations
    return Operations()

def test_add(op):
    assert op.add(1,2) == 3

def test_subtract(op):
    assert op.subtract(2,1) == 1

def test_increment(op):
    assert op.increment(1) == 2

def test_decrement(op):
assert op.decrement(2) == 1
```

To make sure everything is working correctly, from the project's root directory, we can run the `pytest` command. If all goes well, an output similar to the one below should appear.

```bash
============================= test session starts ==============================
platform linux -- Python 3.5.2, pytest-3.4.0, py-1.5.2, pluggy-0.6.0

collected 4 items                                                              

test/test_operations.py ....                                             [100%]

=========================== 4 passed in 0.04 seconds ===========================
```

# Prepare For Deployment

To prepare for deployment and uploading to PyPI, we need to add a `setup.py` file to the root directory of our project. The contents of this file for our purposes are mostly metadata that will populate information in PyPI.

```python
from distutils.core import setup

setup(
    name='travistest',
    packages=['travistest'],
    version='0.0.7',
    description='Test project to get acquainted with TravisCI',
    url='https://github.com/lqdev/TravisTest',    
)
```

## Setup Travis

### Enable Repository

Assuming that you have a `GitHub` login and a repository has been created for your project:

1. Log into [travis-ci.org](https://travis-ci.org/) with your GitHub credentials.
2. Once all of your repositores are synced, toggle the switch next to the repository containing your project.

### Configure .travis.yml

Once the project has been enabled, we need fo configure Travis. This is all done using the `.travis.yml` file.

In this file we'll tell Travis that the language of our project is Python version 3.5 and that we'll be using a virtual environment. Additionally we'll require sudo priviliges and target the Ubuntu Xenial 16.04 distribution. All of these configurations can be done as follows.

```yaml
sudo: required
dist: xenial
language: python
virtualenv:
  system_site_packages: true
python:
- '3.5'
```

Once that is set up, we can tell it to install all of the dependencies stored in our `requirements.txt` file.

```yaml
install:
- pip install -r requirements.txt
```

After this, we need to tell Travis to run our tests just like we would on our local machine.

```yaml
script: pytest
```

Once our tests have run, we need to make sure we are back in the root directory of our project for deployment.

```yaml
after_script: cd ~
```

We're done with the automated testing script portion of our project. Now we need to setup deployment options. This section will mainly contain the credentials of your PyPI account.

```yaml
deploy:
  provider: pypi
  user: "YOURUSERNAME"
```

After setting the provider and user, we need to set the password. Because this will be a public repository DO NOT enter your password on this file. Instead, we can set an encrypted version that only Travis can decrypt. To do so, while in the root directory of our project, we can enter the following command into the terminal.

```bash
travis encrypt --add deployment.password
```

Type your password into the terminal and press `Ctrl + D`.

If you take a look at your `.travis.yml` file you should see something like the following in your `deploy` options

```bash
deploy:
  provider: pypi
  user: "YOURUSERNAME"
  password:
    secure: "YOURPASSWORD"
```

The final `.travis.yml` file should like like so

```yaml
sudo: required
dist: xenial
language: python
virtualenv:
  system_site_packages: true
python:
- '3.5'
install:
- pip install -r requirements.txt
script: pytest
after_script: cd ~
deploy:
  provider: pypi
  user: "YOURUSERNAME"
  password:
    secure: "YOURPASSWORD"
```

# Deploy

Now that we have everything set up, deployment should be relatively easy. A build is triggered when changes are pushed to the repository on GitHub. Therefore, pushing your local changes to the remote GitHub repository should initialize the build. To track progress, visit the project's page on Travis CI.

**NOTE: WHEN PUSHING NEW CHANGES, INCREMENT VERSION NUMBER IN THE `SETUP.PY` FILE SO THAT EXISTING FILE ERRORS DO NOT CRASH BUILDS**

# Conclusion

In this writeup, we created a Python package that performs basic operations and set up automated testing and deployment with Travis CI. Configurations can be further customized and refined to adapt more complex build processes.]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/testing-deploying-python-projects-travisci</link>
      <guid>https://www.luisquintanilla.me/posts/testing-deploying-python-projects-travisci</guid>
      <pubDate>2018-02-18 20:51:32</pubDate>
    </item>
    <item>
      <title>Get started with TiddlyWiki in VS Code</title>
      <description>&lt;![CDATA[
## Introduction

A while back, I set up a knowledge base (KB) section on my website. As part of that knowledge base, I have a [wiki](/wiki) and [snippets](/snippets) section. As opposed to the [blog posts](/posts) which tend to be more long-form and have structure or the microblog-like posts on my [main](/feed) and [response](/feed/responses) feeds, the wiki and snippets section are intended to have self-contained blocks of information that are linked somehow but not a linear or chronological order. 

My current wiki and snippets solution was fairly basic. It was a series of documents authored in Markdown which then get converted to static HTML pages. The index pages for wiki articles and snippets contain links to each of those documents. While that's worked okay, I felt I was missing functionality when it came to linking between documents, tagging, and more importantly searching. 

While I had seen wiki solutions like [MediaWiki](https://www.mediawiki.org/wiki/MediaWiki), they felt too bloated for my use case. Since my site is statically hosted, I didn't want to have to create, maintain, and pay for a server only to run my wiki. That's where TiddlyWiki comes in.

[TiddlyWiki](https://tiddlywiki.com/) describes itself as "a unique non-linear notebook for capturing, organising and sharing complex information". In short, TiddlyWiki is a self-contained single HTML file with WYSIWYG-like functionality for creating a wiki. Because everything is contained in a single HTML file, it's static-site friendly. 

Now I had heard of TiddlyWiki before but dismissed it because I didn't get how it worked. For one, I was confused how the WYSIWYG GUI supported editing, viewing, and exporting of these notes since there was just a single HTML file. More importantly, when it comes to [saving changes](https://tiddlywiki.com/#Saving), the recommendations on the website point users to browser extensions and apps. If it's just an HTML page, why can't I use the tools I'm already using to work with HTML? 

After giving it another look and tinkering a bit, I found I was right and could use tools like VS Code to work with TiddlyWiki. In this post, I'll show how you can get started with the empty edition of TiddlyWiki using VS Code and the Live Preview extension.  

## Prerequisites

- [Visual Studio Code](https://code.visualstudio.com/#alt-downloads)
- [Live Server Extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode.live-server)

This guide was written on a Linux (Manjaro) PC but should work the same on Windows and macOS.

## Download TiddlyWiki

The quickest way to get started with TiddlyWiki is to: 

1. Download the [empty edition](https://tiddlywiki.com/#Empty%20Edition). 

    A file called *empty.html* should download to your computer. For simplicity, I would recommend creating a new directory on your computer that only contains that file.

    The empty edition contains the core functionality. For other editions, see [TiddlyWiki editions](https://tiddlywiki.com/#Editions).

1.  Rename the file *index.html*.

## Edit and save your wiki

### Edit your wiki

1. Open the directory containing your *index.html* file in VS Code.
1. Open the command palette (View &gt; Command Palette).
1. Enter into the text box *Live Preview: Start Server* and press *Enter*. The server should start.

![lqdev TiddlyWiki Main Page](/images/tiddlywiki-vscode/lqdev-tiddlywiki.png)

At this point, you can use the TiddlyWiki interface to create and edit your wiki.

### Save your wiki

Saving can be a bit tricky but it's not too bad. Use the checkmark in the side pane to save your changes. When there are unsaved changes, the checkmark turns red. 

![TiddlyWiki Side Panel Red Checkmark Save Changes](/images/tiddlywiki-vscode/tiddlywiki-save-changes.png)

To save your changes, overwrite the existing file or if you want, you can also save it to a file with a different name. I would recommend saving it to the same file because it's easier to manage. You don't want to end up with *index-version12341.html*. That being said, I don't know if there may be risks in overwriting the file. In my limited testing I had no issues overwriting the file, but I'd still tread lightly.

### Stop the server

Once you're done viewing and editing your wiki, stop the Live Preview server:

1. In VS Code, open the command palette **(View &gt; Command Palette)**.
1. Enter into the text box *Live Preview: Stop Server* and press *Enter*. The server should stop.

## Tips

### Plugins

I prefer working in Markdown and since some of my notes include code, I addded two plugins to my wiki:

- [Markdown](https://tiddlywiki.com/#Markdown%20Plugin)
- [Highlight](https://tiddlywiki.com/#Highlight%20Plugin)

For more information, see [Installing a plugin from the plugin library](https://tiddlywiki.com/#Installing%20a%20plugin%20from%20the%20plugin%20library).

### Things to watch out for in VS Code

When using the Live Preview extension, some things might not work such as keyboard shortcuts and navigating to external links. Since a local server is running you should be able to use the browser as well. However, I would recommend only using the browser to *view* your wiki. Trying to save changes made in the browser will download the HTML page containing your changes. 

### Use Git

Autosaving to Git is one of the ways you can save your TiddlyWiki. However, you have to enter and use your credentials. Since your wiki is a static HTML page, just save it like you would any other HTML file in a repo. Doing so also makes it easy to backup to a previous version in case data is corrupted or lost. 

## Conclusion

I this post, I shared how TiddlyWiki provides a relatively low overhead, static-website friendly way of creating wikis. If you use VS Code, you can use the Live Preview extension to view and edit your wiki. Although my intended use for TiddlyWiki is as a personal knowledge management solution, I can also think of ways you could use TiddlyWiki as a low-friction way of building your own website. The WYSIWYG functionality makes creating content extremely more accessible than static site generators and similar to WordPress. In contrast to WordPress though, since you only have a single HTML file, it's less overhead to manage and also low-cost since you can [host it on GitHub Pages for free](https://nesslabs.com/tiddlywiki-static-website-generator) or any other file hosting service. In the near future, I plan to migrate my wiki and snippets to use TiddlyWiki. ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/tiddlywiki-vscode</link>
      <guid>https://www.luisquintanilla.me/posts/tiddlywiki-vscode</guid>
      <pubDate>2022-10-21 08:32</pubDate>
    </item>
    <item>
      <title>Transcribing Podcasts with Microsoft Speech API</title>
      <description>&lt;![CDATA[

# Introduction

I enjoy listening to podcasts in a wide range of topics that include but are not limited to politics, software development, history, comedy and true crime. While some of the more entertainment related podcasts are best enjoyed through audio, those that are related to software development or other type of hands-on topics would benefit greatly from having a transcript. Having a transcript allows me to go back after having listened to an interesting discussion and look directly at the content I am interested in without having to listen to the podcast again. This however is not always feasible given that it costs both time and money to produce a transcript. Fortunately, there are tools out there such as Google Cloud Speech and Microsoft Speech API which allow users to convert speech to text. For this writeup, I will be focusing on the Microsoft Speech API. Because podcasts tend to be long-form, I will be using the C# client library because it allows for long audio (greater than 15 seconds) to be transcribed. The purpose of this exercise is to create a console application that takes audio segments, converts them to text and stores the results in a text file with the goal of evaluating how well the Microsoft Speech API works. The source code for the console application can be found on [GitHub](https://github.com/lqdev/PodcastsBingSpeechAPIDemo)

## Prerequisites

- [Visual Studio Community](https://www.visualstudio.com/vs/community/)
- [Windows Subsystem for Linux (Ubuntu)](https://www.microsoft.com/store/productId/9NBLGGH4MSV6)
- [ffmpeg](https://ffmpeg.org/)

## Install/Enable Windows Subsystem for Linux
1. Open Powershell as Administrator and input
```powershell
Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux
```
2. Restart your computer
3. Open the Microsoft Store and install [Ubuntu](https://www.microsoft.com/store/productId/9NBLGGH4MSV6) Linux distribution
4. Once installed, click `Launch`
5. Create your LINUX user account (Keep in mind that this is not the same as your Windows account therefore it can be different).

## Install ffmpeg

Once Ubuntu is installed on your computer and you have created your LINUX user account, it's time to install `ffmpeg`. This will allow us to convert our file from `mp3` which is usually the format podcasts are in to `wav` which is the format accepted by the Microsoft Speech API

```bash
sudo apt install -y ffmpeg
```

## Get Bing Speech API Key

In order to use the Microsoft Speech API, an API key is required. This can be obtained using the following steps.

1. Navigate to the [Azure Cognitive Services](https://azure.microsoft.com/en-us/try/cognitive-services/) page.
2. Select the `Speech` tab
3. Click `Get API Key` and follow the instructions
4. Once you have an API key, make sure to store it somewhere like a text file in your computer for future use.

# File Prep

Before transcribing the files, they need to be converted to `wav` format if they are not already in it. In order to ease processing, they should be split into segments as opposed to having the API process an entire multi megabyte file. To do all this, inside the Ubuntu shell, first navigate to your `Documents` folder and create a new directory for the project.

```bash
cd /mnt/c/Users/&lt;USERNAME&gt;/Documents
mkdir testspeechapi
```

`USERNAME` is your Windows user name. This can be found by typing the following command into the Windows `CMD` prompt

```bash
echo %USERNAME%
```

## Download Audio Files

One of the podcasts I listen to is [Talk Python To Me](https://talkpython.fm/) by [Michael Kennedy](https://twitter.com/mkennedy). For the purpose of this exercise, aside from having great content, all of the episodes are transcribed and a link is provided to the `mp3` file. 

The file I will be using is from episode [#149](https://talkpython.fm/episodes/show/149/4-python-web-frameworks-compared), but it can easily be any of the episodes.

In the Ubuntu shell, download the file into the `testspeechapi` directory that was recently created using `wget`.

```bash
wget https://talkpython.fm/episodes/download/149/4-python-web-frameworks-compared.mp3 -O originalinput.mp3
```

## Download Transcripts

The transcripts can be found in GitHub at this [link](https://github.com/mikeckennedy/talk-python-transcripts/tree/master/transcripts). The original transcript will allow me to compare it to the output of the Microsoft Speech API and evaluate the accuracy.

We can download this file into the `testspeechapi` directory just like the audio file

```bash
wget https://raw.githubusercontent.com/mikeckennedy/talk-python-transcripts/master/transcripts/149.txt -O originaltranscript.txt
```

## Convert MP3 to WAV

Now that we have both the original audio file and transcript, it's time to convert the format from `mp3` to `wav`. To do this, we can use `ffmpeg`. In the Ubuntu shell, enter the following command.

```bash
ffmpeg -i originalinput.mp3 -f wav originalinput.wav
```

## Create Audio Segments

Once we have the proper format, it's time to make the files more manageable for processing. This can be done by splitting them up into equal segments using `ffmpeg`. In this case I'll be splitting it up into sixty second segments and storing them into a directory called `input` with the name `filexxxx.wav`. The `%04d` indicates that there will be 4 digits in the file name. Inside the Ubuntu shell and `testspeechapi` directory enter the following commands.

```bash
mkdir input
ffmpeg -i originalinput.wav -f segment -segment_time 60 -c copy input/file%04d.wav
```

# Building The Console Application

To get started building the console application, we can leverage the [Cognitive-Speech-STT-ServiceLibrary](https://github.com/Azure-Samples/Cognitive-Speech-STT-ServiceLibrary) sample program.

## Download The Sample Program

The first step will be to clone the sample program from GitHub onto your computer. In the Windows `CMD` prompt navigate to the `testspeechapi` folder and enter the following command.

```bash
git clone https://github.com/Azure-Samples/Cognitive-Speech-STT-ServiceLibrary
```

Once the project is on your computer, open the directory and launch the `SpeechClient.sln` solution in the `sample` directory

When the solution launches, open the `Program.cs` file and begin making modifications.

## Reading Files

To read files, we can create a function that will return the list of files in the specified directory.

```csharp
private static string[] GetFiles(string directory)
{
    string[] files = Directory.GetFiles(directory);
    return files;
}
```

## Process Files

Once we have the list of files, we can then process each file individually using the `Run` method. To do so, we need to make a slight modification to our `Main` method so that it iterates over each file and calls the `Run` method on it. To store the responses from the API, we'll also need a `StringBuilder` object which is declared at the top of our `Program.cs` file.

```csharp
finalResponse = new StringBuilder();
string[] files = GetFiles(args[0]);
foreach(var file in files)
{
    p.Run(file,"en-us",LongDictationUrl,args[1]).Wait();
    Console.WriteLine("File {0} processed",file);
}
```

## Transcribe Audio

The `Run` method can be left intact. However, the `Run` method uses the `OnRecognitionResult` method to handle the result of API responses. In the `OnRecognitionResult` method, we can remove almost everything that is originally there and replace it. The response from the API returns various results of potential phrases as well as a confidence value. Generally, most of the phrases are alike and the first value is good enough for our purposes. The code for this part will take the response from the API, append it to a `StringBuilder` object and return when completed.

```csharp
public Task OnRecognitionResult(RecognitionResult args)
{

    var response = args;

    if(response.Phrases != null)
    {
	finalResponse.Append(response.Phrases[0].DisplayText);
	finalResponse.Append("\n");
    }

    return CompletedTask;
}
```

## Output Transcribed Speech

When all the audio files have been processed, we can save the final output to a text file in the `testspeechapi` directory. This can be done with the `SaveOutput` function to which we pass in a file name and the `StringBuilder` object that captured responses from the API.

```csharp
private static void SaveOutput(string filename,StringBuilder content)
{
    StreamWriter writer = new StreamWriter(filename);
    writer.Write(content.ToString());
    writer.Close();
}
```

`SaveOutput` can then be called from our `Main` method like so.

```csharp
string username = Environment.GetEnvironmentVariable("USERNAME", EnvironmentVariableTarget.Process);
SaveOutput(String.Format(@"C:\\Users\\{0}\\Documents\\testspeechapi\\apitranscript.txt",username), finalResponse);
```

The final `Main` method should look similar to the code below

```csharp
public static void Main(string[] args)
{
    // Send a speech recognition request for the audio.
    finalResponse = new StringBuilder();

    string[] files = GetFiles(args[0]);

    var p = new Program();

    foreach (var file in files)
    {
	p.Run(file, "en-us", LongDictationUrl, args[1]).Wait();
	Console.WriteLine("File {0} processed", file);
    }

    string username = Environment.GetEnvironmentVariable("USERNAME", EnvironmentVariableTarget.Process);

    SaveOutput(String.Format(@"C:\\Users\\{0}\\Documents\\testspeechapi\\apitranscript.txt",username), finalResponse);
}
```

# Output

The program will save the output of the `StringBuilder` object `finalResponse` to the file `apitranscript.txt`.

Prior to running the program it needs to be built. In Visual Studio change the Solutions Configurations option from `Debug` to `Release` and build the application.

To run the program, navigate to the `C:\Users\%USERNAME%\Documents\testspeechapi\Cognitive-Speech-STT-ServiceLibrary\sample\SpeechClientSample\bin\Release` directory in the Windows `CMD` prompt and enter the following command and pass in the `input` directory where all the audio segments are stored and your API Key.

```bash
SpeechClientSample.exe C:\\Users\\%USERNAME%\\Documents\\testspeechapi\\input &lt;YOUR-API-KEY&gt;
```
This process may take a while due to the number of files being processed, so be patient.

## Sample Output

Original

&gt; Are you considering getting into web programming? Choosing a web framework like Pyramid, Flask, or Django can be daunting. It would be great to see them all build out the same application and compare the results side-by-side. That's why when I heard what Nicholas Hunt-Walker was up to, I had to have him on the podcast. He and I chat about four web frameworks compared. He built a data-driven web app with Flask, Tornado, Pyramid, and Django and then put it all together in a presentation. We're going to dive into that right now. This is Talk Python To Me, Episode 149, recorded January 30th, 2018. Welcome to Talk Python to Me, a weekly podcast on Python, the language, the libraries, the ecosystem, and the personalities. This is your host, Michael Kennedy, follow me on Twitter where I'm @mkennedy. Keep up with the show and listen to past episodes at talkpython.fm, and follow the show on Twitter via @talkpython. Nick, welcome to Talk Python.

API Response

&gt; Are you considering getting into web programming cheese in the web framework like plaster. Django can be daunting. It would be great to see them. Although that the same application and compare the results side by side. That's Why? When I heard. But Nicholas Hunt. Walker was up to, I had him on the podcast. He night chat about 4 web frameworks, compared he built a data driven web app with last tornado. Peermade Angangueo and then put it all together in a presentation. We have dive into that right now. This is talk by the to me, I was 149 recorded January 30th 2018. Welcome to talk python to me a weekly podcast on Python Language the library ecosystem in the personalities. Did you host Michael Kennedy? Follow me on Twitter where I'm at in Kennedy keep up with the show in the past episodes at talk python. Damn info, the show on Twitter via at Popeyes on. Nick Welcome to talk by phone.

# Conclusion

In this writeup, I converted an `mp3` podcast file to `wav` format and split it into sixty second segments using `ffmpeg`. The segments were then processed by a console application which uses the Microsoft Speech API to convert speech to text and the results were saved to a text file. When compared to the original transcript, the result produced by the API is inconsistent and at times incomprehensible. This does not mean that overall the API is unusable or inaccurate as many things could have contributed to the inaccuracy. However, the results were not as good as I would have hoped for.

###### Sources
- [Windows Subsystem for Linux Install Instructions](https://docs.microsoft.com/en-us/windows/wsl/install-win10)
- [Microsoft Speech API](https://docs.microsoft.com/en-us/azure/cognitive-services/speech/home)
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/transcribing-podcasts-microsoft-speech-api</link>
      <guid>https://www.luisquintanilla.me/posts/transcribing-podcasts-microsoft-speech-api</guid>
      <pubDate>2018-02-11 20:34:37</pubDate>
    </item>
    <item>
      <title>Type Driven Development - Scaling Safely with Python</title>
      <description>&lt;![CDATA[
# Introduction

Python is a great language. The syntax is readable and allows pseudocode to be converted into code nearly verbatim. While this is great for prototyping and moving fast, scaling can become an issue. One of the issues is with regards to documentation. In statically typed languages, even if there's no documentation, types help provide some sort of documentation that allow new contributors as well current developers to remember where they left off and what their code does. There are ways around this using docstrings as well as unit tests. However, this often involves performing tasks outside of writing code which can be time consuming. In Python 3.5, type hints or optional static typing are allowed and tools like `mypy` help write safer, more scalable code. The best part is, if the code already has docstrings and unit tests, optional static typing adds an additional layer of safety and documentation to existing projects. This writeup explores practices for documenting and developing scalable Python code as well as illustrating how to use optional static types and type checkers.

## Prerequisites

This writeup assumes that Python 3.5 or greater is being used and both `mypy` and `pytest` packages are installed. To install them using `pip` we can type the following command in the shell:

```bash
pip install pytest mypy
```

## Docstrings

[PEP 257](https://www.python.org/dev/peps/pep-0257/) provides a great overview of what docstrings are and how to use them. The summarized version of it is a string literal in classes and functions that allows developers to document logic, inputs and outputs of those particular sections of code. Below are examples of code with and without docstrings:

### No Docstring

```python
def combine(a,b):
    return a + b
```

### With Docstring

```python
def combine(a,b):
    """
    Returns the sum of two numbers

    Keyword arguments:
    a -- the first number
    b -- the second number

    Returns:
    Sum of a and b
    """
    return a + b
```

As we can see, the string literal or docstring allows individuals who are looking at the code for the first time as well as someone who worked on it and has forgotten the logic of a program to easily decipher what the code does.

Something else to notice is the function name. In this case, the logic is relatively simple and the name may make some sense at the time of writing the code. However, this simple logic is dangerous. Without knowing what is expected as input and output, there's not a clear way of knowing what this code should do. For example, someone might try to run the undocumented version of this function with parameter `a` having the value 'Hello' and `b` with the value 'World'. The function would not throw any errors and return 'HelloWorld' as a result. However, if we look at the intented logic as well as expected input and output provided by the docstring, we'd know that `a` and `b` are both supposed to be numerical values, not strings or any other type.

It's clear that writing a docstring can become tedious and take up a substantial amount of time as a project grows. However, the benefits are reaped when extending the code and using it in production because more time is spent being productive rather than figuring out what the code should do and whether it's being done correctly. Docstrings however are not a panacea since there is no way to enforce what is documented in the code and serves as more of an FYI for developers using and extending the code.

## Unit Testing

One way to prevent code from being misused is by writing tests. By writing unit tests and making sure that they pass, developers can test edge cases such as passing a string and immediately getting feedback through failing tests. Here's an example of what a unit test would look like for the `combine` function written above.

In the file `main.py`, we can write the logic for our `combine` function. However, keeping in mind the docstring, we might want to add some exception handling. 

```python
"""
Module Containing Logic
"""

def combine(a,b):
    """                    
    Returns the sum of two numbers                                          
    Keyword arguments:                 
    a -- the first number  
    b -- the second number                                                      
    Returns:             
    Sum of a and b       
    """
    if(type(a) == str or type(b) == str)
        return a + b
    else:
        raise TypeError
```

In another file called `test_main.py`, we can write our tests. Our test file will look like the code below:

```python
import pytest
from main import combine

testparams = [(1,2,3),(2,4,6)]
@pytest.mark.parametrize("a,b,expected",testparams)
def test_combine(a,b,expected): 
    assert combine(a,b) == expected

testparams = [('a','b'),('a','b')]
@pytest.mark.parametrize("a,b",testparams)
def test_combine_exception(a,b):
    with pytest.raises(TypeError):
        combine(a,b)
```

In our shell we can enter the `pytest` command inside of our project directory and get the following output.

```bash
pytest
```

![](/images/type-driven-development-scaling-safely-with-python/typedrivendevelopment1.png)

The results from `pytest` ensure that passing the expected parameters returns the expected output which is the sum of two numbers and passing in the wrong parameters such as those of type string return a `TypeError`. This gets us closer to where we want to be where we're able to test whether the functionality of our application does what it's supposed to. Like docstrings, there is additional work and time that needs to be accounted for when writing tests. However, this is a practice that should be taking place already and in the case of Python which does not provide the type checking or compilation is a way to if not ensure that our logic is sound, at least it provides us with an additional form of documentation and peace of mind that the code is being used accurately. 

## Type Hints (Optional Static Types)

Good practice would have us write docstrings to document our code and unit tests to ensure the soundness of our logic and code. However, what if that seems like too much work or there's not much time to perform those tasks. Is there a shorthand way that we can both document our code for posterity as well as ensure that we can only use the code as intended. That's where type hints comes in and starting with Python 3.5 have been accepted by the Python community per [PEP 484](https://www.python.org/dev/peps/pep-0484/). With type hints our code would not change much and with a few extra characters, we can write safer code. Our `combine` function from previous examples would look as follows with type hints:

```python
def combine(a:float,b:float) -&gt; float:
    return a + b    
```

If we run this, it should run as expected given the appropriate parameters. That being said, as with the undocumented example, if we pass in parameters 'Hello' and 'World', it should work as well and we get the result 'HelloWorld'. If we still don't get the result we want and our code is still unsafe, then what's the point? One of the benefits is the documentation piece. In the event that we had no docstring, we could still tell that `a` and `b` are both of type `float` and return a `float`. The second benefit comes from the use of `mypy`, a type checker for Python. To see it in action, we can create a script called `mainmypy.py` and add the following code:

```python
def combine(a:float,b:float) -&gt; float:
    return a + b

combine(1,2)
combine('Hello','World')
```

In the shell, we can then use the `mypy` command on our script to check types.

```bash
mypy mainmypy.py
```

The output is the following:

![](/images/type-driven-development-scaling-safely-with-python/typedrivendevelopment2.png)

As we can see, without having to run our code, `mypy` checks the types and throws errors that we would not have found unless we ran our code. Therefore, we get both documentation by defining the types of parameters and outputs we expect which make it easier for individuals using or writing code to safely do so without having to write long, descriptive docstrings. With `mypy`, we enforce the good use of code by checking that the correct parameters are being passed in and the correct results are being returned prior to runtime making it safe to scale and write correct code most of the time. 

## Conclusion

Python is a very expressive language that allows applications to be prototyped in no time. However, the tradeoff is that writing new code or returning to it at a later time without documenting it, particularly the types needed by functions or classes to produce an accurate result can be unsafe. Some existing practices such as docstrings and unit tests can help with documenting and writing safe code. However, tools like `mypy` and the recently introduced type hints achieve what both docstrings and unit tests do in less time and code. This is not to say that these tools are perfect and ideally, unit tests, docstrings and type hints are all integrated to make developers more productive and create safe, scalable code. Happy coding!


]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/type-driven-development-scaling-safely-with-python</link>
      <guid>https://www.luisquintanilla.me/posts/type-driven-development-scaling-safely-with-python</guid>
      <pubDate>2018-03-24 18:16:31</pubDate>
    </item>
    <item>
      <title>Vibe-Specing - From concepts to specification</title>
      <description>&lt;![CDATA[
## Introduction

Code generation is a common use case for AI. What about the design process that comes before implementation? Personally, I've found that AI excels not just at coding, but also helping formalize abstract ideas into concrete specifications. This post explores how I used AI-assisted design to transform a collection of loosely related concepts into a technical specification for a new system made up of those concepts.

## Specifications

Generally, I've had mixed success with vibe-coding (the practice of describing what you want in natural language and having AI generate the corresponding code). However, it's something that I'm constantly working on getting better at. Also, with tooling integrations like MCP, I can ground responses and supplement my prompts using external data.

What I find myself being more successful with is using AI to explore ideas and then formalizing those ideas into a specification. Even in the case of vibe-coding, what you're doing with your prompts is building a specification in real-time. 

I'd like to think that eventually I'll get to the vibe-coding part but before diving straight into the code, I'd like to spend time in the design phase. Personally, this is also the part that I find the most fun because you can throw wild things at the wall. It's not until you implement them that you actually validate whether some of those wild ideas are practical. But I find the design phase a ton of fun.

The result of my latest vibe-specing adventure is what I'm calling the [InterPlanetary Knowledge System (IPKS)](https://github.com/lqdev/IPKS).

## From concept to specification

Lately, I've been thinking a lot about knowledge. Some concepts that have been in my head are those of **non-linear publishing** (creating content that can be accessed in any order with multiple entry points, like wikis or hypertext) and **distributed cognition** (the idea that human knowledge and cognitive processes extend beyond the individual mind to include interactions with other people, tools, and environments).
Related to those concepts, I've also been thinking about how **digital gardens** (personal knowledge bases that blend note-taking, blogging, and knowledge management in a non-linear format) and **Zettelkasten** (a method of note-taking where ideas are captured as atomic notes with unique identifiers and explicit connections) are ways to capture and organize knowledge.

One other thing that I'm amazed by is the powerful concept of a **hyperlink** and how it makes the web open, decentralized, and interoperable. When paired with the **semantic web** (an extension of the web that provides a common framework for data to be shared across applications and enterprises), you have yourself a decentralized knowledgebase containing a lot of the world's knowledge.

At some point, IPFS (InterPlanetary File System, a protocol designed to create a permanent and decentralized method of storing and sharing files) joined this pool of concepts I had in my head.

These were all interesting concepts individually, but I knew there were connections but couldn't cohesively bring them together. That's where AI-assisted specification design came in.

Below is a summary of the collaborative design interaction with Claude Sonnet 3.7 (with web search) that eventually led to the generation of the IPKS specifications. I haven't combed through them in great detail, but what they're proposing seems plausible. 

Overall, I'm fascinated by this interaction. Whether or not IPKS ever becomes a reality, the process of using AI to transform abstract concepts into concrete specifications seems like a valuable and fun design approach that I'll continue to refine and include as part of my vibe-coding sessions. 

---

## Initial Exploration: IPFS and Knowledge Management

Our conversation began with exploring IPFS (InterPlanetary File System) and its fundamental capabilities as a content-addressed, distributed file system. We recognized that while IPFS excels at storing and retrieving files in a decentralized manner, it needed extensions to support knowledge representation, trust, and semantics.

Key insights from this stage:
- IPFS provides an excellent foundation with content addressing through CIDs
- Content addressing enables verification but doesn't inherently provide meaning
- Moving from document-centric to idea-centric systems requires additional layers

## Knowledge Management Concepts

We explored established knowledge management approaches, particularly:

### Zettelkasten
The Zettelkasten method contributed these important principles:
- Atomic units of knowledge (one idea per note)
- Explicit connections between ideas
- Unique identifiers for each knowledge unit
- Emergent structure through relationship networks

### Digital Gardens
The Digital Garden concept provided these insights:
- Knowledge in various stages of development
- Non-linear organization prioritizing connections
- Evolution of ideas over time
- Public visibility of work-in-progress thinking

These personal knowledge management approaches helped us envision how similar principles could work at scale in a distributed system.

## The "K" in IPKS

When we proposed replacing "IPFS" with "IPKS" (changing File → Knowledge), we recognized the need to define what makes knowledge different from files. This led to identifying several key requirements:

1. **Semantic meaning** - Knowledge needs explicit relationships and context
2. **Provenance and trust** - Knowledge requires verifiable sources and expertise
3. **Evolution** - Knowledge changes over time while maintaining continuity
4. **Governance** - Knowledge exists in various trust and privacy contexts

These requirements shaped the layered architecture of the specifications.

## Distributed Cognition and Non-Linear Publishing

Our discussions about distributed cognition highlighted how thinking processes extend beyond individual minds to include:
- Interactions with other people
- Cultural artifacts and tools
- Physical and digital environments
- Social and technological systems

This concept directly influenced the IPKS design by emphasizing:
- Knowledge as a collective, distributed resource
- The need for attribution and expertise verification
- The value of connecting knowledge across boundaries
- The role of tools in extending human cognition

Similarly, non-linear publishing concepts shaped how we approached knowledge relationships and navigation in IPKS, moving away from sequential formats toward interconnected networks of information.

## Web3 Technologies Integration

Our exploration of complementary technologies led to incorporating:

### Decentralized Identifiers (DIDs)
DIDs provided the framework for:
- Self-sovereign identity for knowledge contributors
- Cryptographic verification of authorship
- Persistent identification across systems
- Privacy-preserving selective disclosure

### Verifiable Credentials (VCs)
Verifiable Credentials offered mechanisms for:
- Expertise validation without central authorities
- Domain-specific qualification verification
- Credential-based access control
- Trust frameworks for knowledge contributors

### Semantic Web (RDF/OWL)
Semantic Web standards influenced:
- Relationship types between knowledge nodes
- Ontologies for domain knowledge representation
- Query patterns for knowledge discovery
- Interoperability with existing knowledge systems

## Business Context: Supply Chain Example

Our conversation about supply chain management provided a concrete use case that helped ground the specifications in practical application. This example demonstrated how IPKS could address real-world challenges:

- **Material Provenance**: Using DIDs and verifiable credentials to establish trusted material sources
- **Cross-Organization Collaboration**: Enabling knowledge sharing while respecting organizational boundaries
- **Regulatory Compliance**: Creating verifiable documentation of compliance requirements
- **Expertise Validation**: Ensuring contributors have appropriate qualifications for their roles
- **Selective Disclosure**: Balancing transparency with competitive confidentiality

This business context helped shape the Access Control &amp; Privacy specification in particular, highlighting the need for nuanced governance models.

## Technical Implementation Considerations

As we moved from abstract concepts to specifications, several technical considerations emerged:

1. **Building on IPLD**: Recognizing that InterPlanetary Linked Data (IPLD) already provided foundational components for structured, linked data in content-addressed systems

2. **Modular Specification Design**: Choosing to create multiple specifications rather than a monolithic standard to enable incremental implementation and adoption

3. **Backward Compatibility**: Ensuring IPKS could work with existing IPFS/IPLD infrastructure

4. **Extensibility**: Designing for future enhancements like AI integration, advanced semantic capabilities, and cross-domain knowledge mapping

## The Path Forward

The IPKS specifications represent a synthesis of our conceptual exploration, grounded in:
- Established knowledge management practices
- Decentralized web technologies
- Real-world business requirements
- Technical feasibility considerations

Moving from concept to implementation will require:
1. Reference implementations of the core specifications
2. Developer tools and libraries to simplify adoption
3. Domain-specific extensions for particular use cases
4. Community building around open standards

By building on the combined strengths of IPFS, DIDs, VCs, and semantic web technologies, IPKS creates a framework for distributed knowledge that balances openness with trust, flexibility with verification, and collaboration with governance.
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/vibe-specing-prompt-to-spec</link>
      <guid>https://www.luisquintanilla.me/posts/vibe-specing-prompt-to-spec</guid>
      <pubDate>2025-05-06 20:40</pubDate>
    </item>
    <item>
      <title>Automate ML.NET model schema generation from sample JSON data with Visual Studio</title>
      <description>&lt;![CDATA[
## Introduction

When using ML.NET models to make predictions, you often have to define classes for the model's input and output schema. In a previous post, I showed how you can [use Netron to inspect ML.NET models](/posts/inspect-mlnet-models-netron.html) and manually define classes to represent the input and output schema for your model. That works for models that don't have a lot of features / columns. However, as the number of columns grows, it can become cumbersome to define these classes. Visual Studio has a feature that can help automate that process. Assuming you have a sample of your input and output data in JSON format, you can leverage a built-in feature "Paste JSON As Classes" to take the sample and convert it to a class. In this post, I'll show how to do that.

## Prerequisites

- Visual Studio 2019. Though I haven't tested with VS2022, I assume "Paste JSON As Classes" is also available there.

## Convert sample JSON data to C# classes

In this post, I'll work with the *sentiment_model.zip* model to classify sentiment, which you can find in the [dotnet/samples](https://github.com/dotnet/samples/blob/main/machine-learning/models/sentimentanalysis/sentiment_model.zip) repo.

The model input has 3 columns:

- SentimentText (string)
- Label (boolean)
- SamplingKeyColumn (float32)

The model output has at least 2 columns since it uses a binary classification algorithm. For information on expected output columns based on the machine learning algorithm/task, see the [ML.NET tasks article](https://docs.microsoft.com/dotnet/machine-learning/resources/tasks#binary-classification-inputs-and-outputs).

- Score (Single). Single is a single-precision floating-point number.
- PredictedLabel (boolean)

With that in mind, let's then assume that we have sample input data in JSON format that looks as follows.

```json
{
    "SentimentText": "This was a very bad steak",
    "Label": false,
    "SamplingKeyColumn": 1.0
}
```

Inside a C# project in Visual Studio:

1. Create a C# class
   1. Right-click your project.
   2. Select **Add &gt; Class**.
   3. Provide a name for your class in the New Item dialog.
2. Copy the input JSON data sample to your clipboard.
3. Place your cursor inside the namespace block of your newly created class.
4. In the Visual Studio toolbar, select **Edit &gt; Paste Special &gt; Paste JSON as Classes**.
5. The result should look similar to the following:

    ```csharp
    public class Rootobject
    {
        public string SentimentText { get; set; }
        public bool Label { get; set; }
        public float SamplingKeyColumn { get; set; }
    }
    ```

6. Rename the class to something more descriptive like `ModelInput`. The class should look similar to the following:

    ```csharp
    public class ModelInput
    {
        public string SentimentText { get; set; }
        public bool Label { get; set; }
        public float SamplingKeyColumn { get; set; }
    }
    ```

7. Create a JSON data sample for your output. In this case it'd look something like:

    ```json
    {
        "Score": 1.0,
        "PredictedLabel": false
    }
    ```

8. Repeat steps 1-6 for your output JSON data sample. The resulting class should look similar to the following:

    ```csharp
    public class ModelOutput
    {
        public float Score { get; set; }
        public bool PredictedLabel { get; set; }
    }
    ```

It's important to note that the name of the class does not matter as long as the column names and types are the same the ones the model expects.

At this point, you can go through the process of [using the model to make predictions on new data](https://docs.microsoft.com/dotnet/machine-learning/how-to-guides/machine-learning-model-predictions-ml-net).

## Conclusion

Although the model used in this post does not have many columns, when you have many columns "Paste JSON As Classes" can significantly simplify the process of creating your input and output schema classes for ML.NET models. Happy coding!
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/vs-automate-mlnet-schema-generation</link>
      <guid>https://www.luisquintanilla.me/posts/vs-automate-mlnet-schema-generation</guid>
      <pubDate>2021-08-26 18:00:00</pubDate>
    </item>
    <item>
      <title>Computing website metrics using GitHub Actions</title>
      <description>&lt;![CDATA[
## Introduction 

I recently posted my [(We)blogging Rewind](/feed/weblogging-rewind-2023) and [(We)blogging Rewind Continued](/feed/weblogging-rewind-2023-continued) for 2023. In these posts, I discuss some analysis I did of my posting behaviors over the past year on this website. I don't use platforms like Google Analytics to track visitors. I used to self-host a [GoatCounter](https://www.goatcounter.com/) instance but personally I don't really care about traffic so I got rid of that. There are some standard reports I get from my [CDN provider on Azure](https://learn.microsoft.com/azure/cdn/cdn-advanced-http-reports), but again I don't really care about those metrics. What I do care about though is my output and understanding what I'm publishing and the topics that were important to me at any given time. In the case of those blog posts, it was for the year 2023. Given that I had already done the analysis and had written the script, I thought, why not automate it and run it on a more regular basis to have monthly summaries. Since my blog and scripts are already on GitHub, it makes sense to create a GitHub Action workflow. In this post, I discuss in more details what my post analytics script does and how I configured my workflow in GitHub Actions to run the script on the first of every month.  

## The script

This script loads the various posts on my website and computes aggregate metrics based on post types and their metadata. You can find the full script at [stats.fsx](/snippets/lqdev-me-website-post-metrics).

### Loading files

The following are convenience functions which I use as part of my website build process. 

In general these functions:

- Load the individual post files
- Parse the content and YAML metadata

```fsharp
let posts = loadPosts()
let notes = loadFeed ()
let responses = loadReponses ()
```

Instead of building new custom functions, I can repurpose them and apply additional transformations to compute aggregate statistics.

### Computing aggregate statistics

Once the posts are loaded, I apply transformations on the collections to compute aggregate metrics:

#### Annual post counts

The following are annual aggreagates of blog posts, notes, and responses. 

##### Blog posts

Takes the blog post collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order.

```fsharp
let postCountsByYear = 
    posts
    |&gt; Array.countBy (fun (x:Post) -&gt; DateTime.Parse(x.Metadata.Date) |&gt; _.Year)
    |&gt; Array.sortByDescending fst 
```

##### Notes

Takes the note collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order.

```fsharp
let noteCountsByYear = 
    notes
    |&gt; Array.countBy (fun (x:Post) -&gt; DateTime.Parse(x.Metadata.Date) |&gt; _.Year)
    |&gt; Array.sortByDescending fst
```

##### Responses

Takes the response collection, parses the published date, and computes counts by the year property. Then, it sorts them in descending order.

```fsharp
let responseCountsByYear = 
    responses
    |&gt; Array.countBy (fun (x:Response) -&gt; DateTime.Parse(x.Metadata.DatePublished) |&gt; _.Year)
    |&gt; Array.sortByDescending fst
```

#### Response counts by type

Takes the response collection, parses the published date, filters it for the current year, and computes counts by the post type (reply, bookmark, reshare, star).

```fsharp
let responsesByType = 
    responses
    |&gt; Array.filter(fun x -&gt; (DateTime.Parse(x.Metadata.DatePublished) |&gt; _.Year) = DateTime.UtcNow.Year)
    |&gt; Array.countBy(fun x -&gt; x.Metadata.ResponseType)
    |&gt; Array.sortByDescending(snd)
```

#### Tag counts (responses)

Takes the response collection, parses the published date, filters it for the current year, and computes counts by the tag name, and sorts in descending order using the count.


```fsharp
let responsesByTag = 
    responses
    |&gt; Array.filter(fun x -&gt; (DateTime.Parse(x.Metadata.DatePublished) |&gt; _.Year) = DateTime.UtcNow.Year)
    |&gt; Array.collect(fun x -&gt; 
            match x.Metadata.Tags with
            | null -&gt; [|"untagged"|]
            | [||] -&gt; [|"untagged"|]
            | _ -&gt; x.Metadata.Tags
        )
    |&gt; Array.countBy(fun x -&gt; x)
    |&gt; Array.sortByDescending(snd)
```

#### Domain counts (responses)
 
Takes the response collection, parses the published date, filters it for the current year, and computes counts by the target URL host name, and sorts it in descending order using the count.

```fsharp
let responsesByDomain = 
    responses
    |&gt; Array.filter(fun x -&gt; (DateTime.Parse(x.Metadata.DatePublished) |&gt; _.Year) = DateTime.UtcNow.Year)
    |&gt; Array.countBy(fun x -&gt; Uri(x.Metadata.TargetUrl).Host)
    |&gt; Array.sortByDescending(snd)
```

### Displaying counts

Since the `countBy` function is the one used to compute the counts, this produces a tuple. The tuple though could be `string` or `int`. Therefore, I set the collection of entry counts to use a generic `'a` for the first item in the tuple. I'm also able to control using `n` whether to display the entire collection by using `-1` as input or a limit when `n &gt;= 0`.

```fsharp
let printEntryCounts&lt;'a&gt; (title:string) (entryCounts:('a * int) array) (n:int) = 
    printfn $"{title}"

    match n with 
    | n when n = -1 -&gt; 
        entryCounts
        |&gt; Array.iter(fun x -&gt; printfn $"{fst x} {snd x}")
        |&gt; fun _ -&gt; printfn $""
    | n when n &gt;= 0 -&gt; 
        entryCounts
        |&gt; Array.take n
        |&gt; Array.iter(fun x -&gt; printfn $"{fst x} {snd x}")
        |&gt; fun _ -&gt; printfn $""
```

The result of running this script produces the following results:

```text
Blogs
2023 5
2022 7

Notes
2023 34
2022 36

Responses
2023 216
2022 146

Response Types
bookmark 151
reshare 48
reply 10
star 7

Response Tags
ai 104
llm 42
untagged 41
opensource 31
internet 17

Domains
github.com 15
huggingface.co 11
arxiv.org 10
openai.com 6
www.theverge.com 4
```

## The workflow file

The workflow file is a GitHub Actions workflow which you can find in my [website repo](https://github.com/lqdev/luisquintanilla.me/blob/main/.github/workflows/stats.yml).  

### Triggers

I don't really want the script to run every time I publish my website. Instead, I just want to have these aggregate values computed on a monthly basis. Optionally though, I'd like to be able to run ad-hoc reports and trigger this job manually.

The triggers in my workflow file look like the following:

```yaml
schedule: 
  - cron: '30 0 1 * *'
workflow_dispatch: 
```

Using cron job syntax, I use the `schedule` trigger to configure the script to run at 12:30 AM on the 1st day of every month.

The `workflow_dispatch` trigger is there so I can manually trigger this job.

## Steps

The steps in the workflow file are the following:

- Check out the repo

    ```yaml
    - uses: actions/checkout@v2
    ```


- Install the .NET 8 SDK

    ```yaml
    - name: Setup .NET SDK 8.x
      uses: actions/setup-dotnet@v1.9.0
      with: 
        dotnet-version: '8.0.x'    
    ```

- Restore dependencies

    ```yaml
    - name: Install dependencies
      run: dotnet restore    
    ```

- Build the project

    ```yaml
    - name: Build project
      run: dotnet build --no-restore  
    ```

- Run the script and display metrics

    ```yaml
    - name: Display Post Metrics
      run: dotnet fsi Scripts/stats.fsx
    ```

## Conclusion

If you have scripts that you run on a repo on a fairly regular basis, consider using GitHub Actions to automate the execution of these scripts. Happy coding! ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/website-metrics-github-actions</link>
      <guid>https://www.luisquintanilla.me/posts/website-metrics-github-actions</guid>
      <pubDate>2023-12-24 13:24</pubDate>
    </item>
    <item>
      <title>Weird, Inc. and 2025</title>
      <description>&lt;![CDATA[
I don't remember how I came across Weird and their projects but I'm glad I did. I appreciate their approach of using existing platforms, protocols, and practices from the yesterweb to put the personal website front and center. 

## Weird, Inc

First, the topic of [Weird the organization and their aspirations](https://blog.muni.town/weird-inc/).

&gt; We have big aspirations for Weird. At minimum, a 'social network of personal websites' can be catalyzed by simply mapping and linking existing web spaces together, playing the role of social glue. On a grander scale there's potential for a pro-social reformation of the web as we know it, putting people's websites at the center of our sociodigital interactions.

By aligning to the foundations of [Seth Godin's Modern Business Plan](https://seths.blog/2010/05/the-modern-business-plan/), they aim to focus on:

1. Truth -  describe the world as it is
1. Assertions - how you’re going to change things
1. Alternatives - what you'll do if assertions don't pan out.
1. People - who is on your team, who is going to join your team
1. Money - how much do you need and how will you spend it

Below are some of the points that resonated with me. 

### Truth

&gt; Home ownership on the web, remarkably, is just as fraught with rent-seeking monopolists as our physical spaces. No one gets to own anything anymore.

While they're mainly referring to the social web, this is is generally true across many areas of our digital spaces. 

&gt;  Bluesky...is bringing into mainstream a nearly forgotten cornerstone of the Indie Web: Domain sovereignty  
&gt; &lt;br&gt;
&gt; In a reinvigorated social web where your domain-name is your universal address in the digital sphere, Weird is the virtual home to which that address is pointing  
&gt; &lt;br&gt;
&gt; 'Weird the data importer' is our wedge into the data fortresses keeping our digital identities captive. What will start as a Publish Elsewhere, Syndicate (to your) Own Site (PESOS) strategy makes way for a fully self-sovereign Publish (on your) Own Site, Syndicate Elsewhere (POSSE) end state.  
&gt; &lt;br&gt;
&gt; Imagine a WordPress-like website engine that aggregates all of your digital personas into a unified mosaic, completely under your control by combination of cloud backing and local storage.  

### Alternatives

If the assertions don't pan out, the alternatives that I think might be interesting:

- Local-first knowledge base with cloud synchronization. [AnyType](https://anytype.io/) comes to mind.
- Cloud + local archiver for many other 3rd party accounts such as Bluesky, Mastodon, Discourse etc. This might make PESOS scenarios easier. Of course, this only works if the 3rd party services allow developer access to their APIs.

## Weird in 2025

Speaking specifically about [Weird the product and what the team plans of shipping in 2025](https://blog.muni.town/weird-2025/).

&gt; Weird is best understood as a single-page website generator

### Q1 and Q2

&gt; 'Weird the data importer' is our wedge into the data fortresses keeping our digital identities captive.  
&gt; &lt;br&gt;
&gt; 'Weird: ₱ESOS-edition' aggregates all of your digital personas into a harmonious mosaic, made completely yours by combination of cloud backing and local-first storage.  
&gt; &lt;br&gt;
&gt; Setting up a POSSE pipeline however requires not just the right infrastructure...but also a big shift in workflow.  
&gt; &lt;br&gt;
&gt; Most people will probably want a combination of both.  
&gt; &lt;br&gt;
&gt; Hence our commitment to both models, starting with the least laborious one [PESOS].  

Personally, [I prefer POSSE](https://www.lqdev.me/posts/rss-to-mastodon-posse-azure-logic-apps/), but it's great to see that Weird plans on meeting people where they are.

### Late 2025

&gt; Around half-way through 2025, we're hoping to shift some of our attention away from Weird as an individualistic publishing tool to Weird as a network engine.  
&gt; &lt;br&gt;
&gt; We think there's massive untapped potential in the web 1.0 concept of webrings, and we fully intend to bring them back.  
&gt; &lt;br&gt;
&gt; All of this culminates in one of the key motivations behind the Weird project: [A Network of Shared Purpose](https://github.com/muni-town/weird/issues/3).  

I'm not sure yet what this looks like but making it easier to discover and build networks of personal websites is a good problem to try to solve.

## Conclusion

I wish them luck and success in what they're aiming to accomplish. I'm excited to see more projects like this sprouting to shake up the status-quo in our digital spaces. ]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/weird-inc-2025</link>
      <guid>https://www.luisquintanilla.me/posts/weird-inc-2025</guid>
      <pubDate>2024-12-17 18:41</pubDate>
    </item>
    <item>
      <title>Windows 10 Intel Display Driver Blank Screen Fix</title>
      <description>&lt;![CDATA[
Recently my Dell Inspiron P57G was experiencing issues when entering sleep mode. When trying to wake the PC, a blank screen was coming on. After some research, I concluded it was due to bugs with the most recent version of the display driver. These were the steps I took to remediate the issue. 

1. Go to [Dell Support](http://www.dell.com/support/home/us/en/04?app=drivers&amp;c=us&amp;l=en&amp;~ck=mn) page and find the Intel HD Graphics 510 515 520 530 540 Driver.
2. Click 'Other Versions'
3. Download the most recent driver prior to 08/13/2017.
4. Follow the installation instructions and restart the PC.
    - If prompted to overwrite newer drivers, select 'Yes'

After restart, either force the PC to sleep or let it sleep on its own. The problem should've been fixed. 



]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/windows-10-intel-display-driver-blank-screen</link>
      <guid>https://www.luisquintanilla.me/posts/windows-10-intel-display-driver-blank-screen</guid>
      <pubDate>2017-12-09 14:27:31</pubDate>
    </item>
    <item>
      <title>Working with URLs in Racket</title>
      <description>&lt;![CDATA[
Often times when working with web applications, parameters and values are passed in the path as well as the query string of a URL. In order to do something useful, parsing of the url is required. Noted below are examples of how this can be done in an elegant manner using Racket.

The first step is to import the appropriate packages

```lisp
(require net/url)
(require net/url-structs)
```

Assuming we are working with the url string "https://hacker-news.firebaseio.com/v0/item/192327.json?print=pretty", we can convert the string to a URL struct using the `string-&gt;url` procedure.

```lisp
(define myurl (string-&gt;url "https://hacker-news.firebaseio.com/v0/item/192327.json?print=pretty"))
```

The result is the following output
```lisp
myurl
;;(url
;; "https"
;; #f
;; "hacker-news.firebaseio.com"
;; #f
;; #t
;; (list
;;  (path/param "v0" '())
;;  (path/param "item" '())
;;  (path/param "192327.json" '()))
;; '((print . "pretty"))
;; #f)
```

The structure of the URL struct is as follows and directly maps to the values displayed above.

```lisp
(struct url ( scheme
 	      user
 	      host
 	      port
 	      path-absolute?
	      path
 	      query
 	      fragment)
    #:extra-constructor-name make-url)
  scheme : (or/c false/c string?)
  user : (or/c false/c string?)
  host : (or/c false/c string?)
  port : (or/c false/c exact-nonnegative-integer?)
  path-absolute? : boolean?
  path : (listof path/param?)
  query : (listof (cons/c symbol? (or/c false/c string?)))
  fragment : (or/c false/c string?)
```

To access the individual data members of the struct, the syntax `&lt;type&gt;-&lt;member&gt; &lt;instance&gt;`. What is returned can be further manipulated based on its type (string, list, boolean)

```lisp
(url-host myurl)
;;"hacker-news.firebaseio.com"
```

###### Source
[URLS and HTTP](https://docs.racket-lang.org/net/url.html)]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/working-with-urls-racket</link>
      <guid>https://www.luisquintanilla.me/posts/working-with-urls-racket</guid>
      <pubDate>2017-12-13 16:01:57</pubDate>
    </item>
    <item>
      <title>Self-hosting made easy with YunoHost</title>
      <description>&lt;![CDATA[
## Introduction

Over the past few months I've started self-hosting a few of the services I use. Among those are a Matrix chat server and Mastodon instance. It may be of special interest to call out, many of these services are hosted on a Raspberry Pi 4, which not only speaks to the capabilities of the device but also shows opportunities for individuals to take control over the services they use and their data. Self-hosting however isn't the easiest thing to get started with. Mainly because each service has their own setup requirements. Technically you could use something like Docker to standardize the process, but that has its own complexities. In particular, there was a recent experience where I had trouble setting up a service which I got up and running in no time using YunoHost. YunoHost describes itself as "...a libre operating system aiming to simplify server administration and democratize self-hosting.". In this post, I'll describe my experience with it and why you might consider it for your self-hosting project.

![Image of server racks by Taylor Vick (Unsplash @tvick)](https://user-images.githubusercontent.com/11130940/128647305-c30ef44e-901d-4bf1-8e9d-af2449d5fd1d.png)

## Why Self-Host?

First off, why even go through the trouble of self-hosting? Service providers nowadays have made it so easy to use their services, self-hosting seems like a lot of work. However, there are several reasons why you might consider self-hosting the applications and services you use. A few that I find interesting include:

1. Explore "new" technologies
2. Own your data
3. Learn something new

### Explore "new" technologies

The saying goes "there's nothing new under the sun". However as with all things, people have found alternative uses and solutions to existing technologies. For example, in simple terms, the [Fediverse](https://en.wikipedia.org/wiki/Fediverse) can be summarized as E-mail + RSS. It consists of a common decentralized protocol to handle communication between individuals who may not use the same server while providing "follow" capabilities for services like micro-blogging, photos, and video sharing. Though the Fediverse has been around for some time, it's still relatively new. Whether it's a new technology or new to you, self-hosting can help you learn more about technologies like the Fediverse.

### Own your data

Part of accepting terms of service agreements is sometimes giving up some ownership of your personal data. Although you hope the company and service provider is responsible with your data, you can't always be 100% sure. With self-hosting, you're fully in control of your data and in most cases don't rely on third parties. As a result, your data is as private and secure as you want it to. From a security standpoint though, I'll note that in many cases, service providers have dedicated security teams whose main job is to keep your data secure. Therefore chances are that your data is usually secure when hosted by service providers. In any case, if you want to have complete control and ownership of your data, self-hosting might be for you.

### Learn something new

Over the past few months, I've learned a few things that I don't usually deal with on my day-to-day like DNS records, Nginx, Systemd, certificates, and a few others. Self-hosting these services usually involves setting up several components some of which you may already be familiar with while others may be new to you. Self-hosting gives you the opportunity to learn new skills and as mentioned previously work with new technologies.

## Self-hosting with YunoHost

Recently I was trying to set up an instance of [Pixelfed](https://pixelfed.org/), which describes itself as "A free and ethical photo sharing platform". You can think of it as a self-hosted Instagram. The application is built with PHP and a few other components I'm not very familiar with. I spent about a week trying to configure my Raspberry Pi to successfully run these services. Unfortunately, I had no luck. I had heard about YunoHost previously but had never tried it. Essentially they offer a way to simplify self-hosting. I noticed YunoHost provided a way to self-host Pixelfed. All I had to do was:

1. [Install YunoHost](https://yunohost.org/en/install). In my case, I installed it on a Linode VM (the smallest one) running Debian 10, but you have options including a Raspberry Pi.
2. [Install Pixelfed](https://github.com/YunoHost-Apps/pixelfed_ynh). YunoHost has a variety of [applications](https://yunohost.org/en/apps) with pre-configured scripts that make it easy to install applications like Pixelfed.

The entire process took about one hour. Some parts I skipped mentioning were getting a domain name and configuring DNS records, but aside from that, getting started with YunoHost was incredibly simple. If you're looking to self-host but don't want to spend too much time configuring your server and each of the services you host on it, give YunoHost a try.
]]&gt;</description>
      <link>https://www.luisquintanilla.me/posts/yunohost-self-hosting-simplified</link>
      <guid>https://www.luisquintanilla.me/posts/yunohost-self-hosting-simplified</guid>
      <pubDate>2021-08-10 18:00:00</pubDate>
    </item>
  </channel>
</rss>