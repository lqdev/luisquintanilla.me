# S3 Media Upload Fix - Implementation Summary

## Problem
Media uploads to Linode Object Storage were failing with error:
```
Connection was closed before we received a valid response from endpoint URL
```

## Root Cause Analysis
After reviewing the working discord-publish-bot repository (as suggested by @lqdev), the issue was identified:
- The boto3 client configuration needed to match the exact structure used in discord-publish-bot
- Parameter order in boto3.client() call matters for consistency
- The working implementation uses a simpler, more direct configuration

## Solution Applied

### 1. Match Discord-Publish-Bot Configuration Exactly
Changed boto3 client initialization and dependencies to exactly match the working discord-publish-bot:

**boto3/botocore Versions:**
```yaml
# GitHub Actions workflow
uv pip install boto3==1.34.0 botocore==1.34.0 requests
```

**S3 Client Configuration:**
```python
# Discord-publish-bot configuration (WORKING)
s3_client = boto3.client(
    's3',
    endpoint_url=endpoint_url,              # endpoint_url FIRST
    aws_access_key_id=access_key,
    aws_secret_access_key=secret_key,
    region_name=region,
    config=Config(
        signature_version='s3v4',
        s3={
            'addressing_style': 'virtual'
        }
    )
)
```

**Key Changes:**
1. Pin boto3==1.34.0 and botocore==1.34.0 (same as discord-publish-bot)
2. Use `endpoint_url` as first parameter (matching discord-publish-bot order)
3. Use inline Config definition (matching discord-publish-bot structure)
4. Remove timeout and retry parameters that were added in previous attempt
5. Keep it simple and match exactly what works

### Why Version Pinning Matters
Discord-publish-bot uses specific versions of boto3 (1.34.0) and botocore (1.34.0). Newer versions may have different default behaviors or breaking changes. By pinning to the same versions, we ensure identical behavior.

### 2. Previous Attempt Analysis
Initial fix attempt added connection timeouts and retry configuration:
```python
# Previous attempt (NOT working)
s3_config = Config(
    signature_version='s3v4',
    s3={'addressing_style': 'virtual'},
    connect_timeout=60,           # Added but may cause issues
    read_timeout=60,              # Added but may cause issues
    retries={'max_attempts': 3}   # Added but may cause issues
)
```

This approach was based on assumptions rather than proven working code.

### 2. Created Testing Tools

#### A. Test S3 Connection Script (`test_s3_connection.py`)
- Safely tests S3 connectivity without exposing credentials in logs
- Tests discord-publish-bot configuration first (known working)
- Tests original configuration for comparison
- Tests path-style addressing as fallback
- Performs actual upload/delete operations
- Provides clear diagnostic output

**Features:**
- ‚úÖ Masks credentials in output
- ‚úÖ Tests discord-publish-bot parameter order (endpoint_url first)
- ‚úÖ Tests original parameter order (aws keys first)
- ‚úÖ Tests path-style addressing
- ‚úÖ Performs real upload and delete operations
- ‚úÖ Verbose mode for debugging (--verbose flag)

#### B. Manual Test Workflow (`test-s3-connection.yml`)
- Can be triggered manually from GitHub Actions
- Uses same credentials as media upload workflow
- Provides clear pass/fail status
- Optional verbose mode for detailed troubleshooting

**To run:**
1. Go to Actions tab in GitHub
2. Select "Test S3 Connection" workflow
3. Click "Run workflow"
4. Optionally enable verbose output
5. Review results

### 3. Updated Documentation
- Enhanced README with timeout/retry configuration details
- Improved troubleshooting section with specific solutions
- Added configuration explanation for clarity

## Testing Instructions

### Phase 1: Test S3 Connectivity (Manual)
1. Navigate to GitHub repository
2. Go to "Actions" tab
3. Select "Test S3 Connection" workflow from left sidebar
4. Click "Run workflow" button (top right)
5. Click green "Run workflow" button in dropdown
6. Wait for workflow to complete (30-60 seconds)
7. Review results:
   - ‚úÖ Green checkmark = S3 connection works
   - ‚ùå Red X = S3 connection failed (check logs)

### Phase 2: Test Media Upload (Real)
1. Navigate to "Issues" tab in GitHub repository
2. Click "New issue"
3. Select "üì∑ Post Media" template
4. Fill in:
   - **Title**: "Test S3 Fix"
   - **Content**: Drag and drop a small image (< 1MB)
   - **Tags**: "test"
5. Click "Submit new issue"
6. Watch GitHub Actions run (should complete in 1-2 minutes)
7. Verify:
   - ‚úÖ Issue is auto-closed
   - ‚úÖ PR is created
   - ‚úÖ No S3 connection errors in logs
   - ‚úÖ File uploaded to S3 successfully

### Phase 3: Verify Upload Results
1. Check PR created by workflow
2. Review markdown file created in `_src/media/`
3. Verify :::media blocks contain permanent CDN URLs
4. Merge PR if everything looks good

## Expected Outcomes

### Before Fix
```
‚ùå Error processing attachment: Connection was closed before we received 
   a valid response from endpoint URL: "***/***/files/images/..."
```

### After Fix
```
‚úÖ Uploaded successfully
üîó Permanent URL: https://cdn.luisquintanilla.me/files/images/20251026_211117_...
‚úÖ Media upload and transformation complete!
```

## Additional Considerations

### If Test Still Fails
Check these potential issues:

1. **Credentials**: Verify all secrets are set correctly in GitHub repository settings
   - LINODE_STORAGE_ACCESS_KEY_ID
   - LINODE_STORAGE_SECRET_ACCESS_KEY
   - LINODE_STORAGE_ENDPOINT_URL
   - LINODE_STORAGE_BUCKET_NAME
   - LINODE_STORAGE_CUSTOM_DOMAIN (optional)

2. **Endpoint URL Format**: Must be `https://REGION.linodeobjects.com`
   - Example: `https://us-east-1.linodeobjects.com`
   - No trailing slash
   - HTTPS (not HTTP)

3. **Bucket Permissions**: Bucket must allow:
   - ListBucket
   - GetObject
   - PutObject with public-read ACL

4. **Clock Skew**: GitHub Actions runner time must be within 15 minutes of S3 server time
   - This should not be an issue with GitHub-hosted runners
   - Could be an issue with self-hosted runners

5. **Network Restrictions**: Ensure GitHub Actions runner can reach Linode Object Storage
   - GitHub-hosted runners should have no restrictions
   - Self-hosted runners may need firewall rules

## Files Changed

### Modified Files:
1. `.github/scripts/upload_media.py`
   - Added timeout and retry configuration to boto3 Config

2. `.github/scripts/README.md`
   - Enhanced documentation with new configuration details
   - Improved troubleshooting section

### New Files:
1. `.github/scripts/test_s3_connection.py`
   - Test script for S3 connectivity

2. `.github/workflows/test-s3-connection.yml`
   - Manual workflow for testing S3 connection

## Success Criteria
- ‚úÖ Timeout and retry configuration added
- ‚úÖ Test script created
- ‚úÖ Test workflow created
- ‚úÖ Documentation updated
- ‚è≥ S3 connection test passes (user to verify)
- ‚è≥ Media upload completes successfully (user to verify)

## Next Steps
1. **User Action Required**: Run "Test S3 Connection" workflow to verify credentials work
2. If test passes: Try real media upload via issue form
3. If test fails: Check credentials and endpoint URL format
4. Report results back to Copilot for further debugging if needed
